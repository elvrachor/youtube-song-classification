{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FcVL-bkoHdWl"
   },
   "source": [
    "# Machine Learning and Pattern Recognition - Project 3\n",
    "Eleftheria Vrachoriti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPf3TPuSIOs2"
   },
   "source": [
    "Start by importing all the libraries that we are going to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uQRyajrZHQwU"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import ELU, Hardshrink, Hardsigmoid, Hardtanh, Hardswish, LeakyReLU, LogSigmoid\n",
    "from torch.nn import PReLU, ReLU, ReLU6, RReLU, SELU, CELU, GELU, Sigmoid, SiLU, Mish, Softplus, Softshrink, Softsign, Tanh, Tanhshrink\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8gnbhG6zreY",
    "outputId": "bbabe5b1-5cc2-4154-f873-c4d3ece9da5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/519.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/519.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
      "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.6.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.11.4\n"
     ]
    }
   ],
   "source": [
    "%pip install torchmetrics\n",
    "from torchmetrics.functional import f1_score\n",
    "from torchmetrics import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WSJ3NXVY4fDj"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adadelta, Adagrad, Adam, AdamW, SparseAdam, Adamax, ASGD, LBFGS, NAdam, RAdam, RMSprop, Rprop, SGD\n",
    "from torch.optim.lr_scheduler import LambdaLR, MultiplicativeLR, StepLR, MultiStepLR, ConstantLR\n",
    "from torch.optim.lr_scheduler import LinearLR, ExponentialLR, PolynomialLR, CyclicLR\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BUtDAgRKLda"
   },
   "source": [
    "Mount drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xShDXXhNKMyE",
    "outputId": "20c44605-be42-46c9-b807-b85ddff3da70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5ibslUvO3_v"
   },
   "source": [
    "Unzip data folder to root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Fk-CwjmfL9pQ"
   },
   "outputs": [],
   "source": [
    "data_path_zip = '/content/gdrive/MyDrive/data.zip'\n",
    "data_path = '/content/'\n",
    "\n",
    "data_zip = zipfile.ZipFile(data_path_zip, 'r')\n",
    "data_zip.extractall(data_path)\n",
    "data_zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Th-im1bHj9J"
   },
   "source": [
    "# Question 1 - Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NnbfO_SSiQm"
   },
   "source": [
    "### Step 1 - Load data (mfccs)\n",
    "\n",
    "Load train, test and validation data (mfccs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAu64mCIShxw"
   },
   "outputs": [],
   "source": [
    "mfcc_x_train = np.load('/content/music_genre_data_di/train/mfccs/X.npy')\n",
    "mfcc_y_train = np.load('/content/music_genre_data_di/train/mfccs/labels.npy')\n",
    "\n",
    "mfcc_x_test  = np.load('/content/music_genre_data_di/test/mfccs/X.npy')\n",
    "mfcc_y_test  = np.load('/content/music_genre_data_di/test/mfccs/labels.npy')\n",
    "\n",
    "mfcc_x_val   = np.load('/content/music_genre_data_di/val/mfccs/X.npy')\n",
    "mfcc_y_val   = np.load('/content/music_genre_data_di/val/mfccs/labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOHEfbjHTz3u"
   },
   "source": [
    "Find unique labels in train dataset.\n",
    "\n",
    "We are expecting 4 different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IgzR5iAyTSgt",
    "outputId": "8dea11d6-2d30-4e23-a221-10d2d85c887e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blues', 'classical', 'hiphop', 'rock_metal_hardrock'],\n",
       "      dtype='<U19')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mfcc_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zfox2AKPTCaA"
   },
   "source": [
    "Convert labels from strings <code>classical</code>, <code>blues</code>, <code>hiphop</code> and <code>rock_metal_hardrock</code> to integer values <code>0</code>, <code>1</code>, <code>2</code> and <code>3</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Sd-2bl-gOfSK"
   },
   "outputs": [],
   "source": [
    "labels_str_to_int = {'classical'          : 0,\n",
    "                     'blues'              : 1 ,\n",
    "                     'hiphop'             : 2,\n",
    "                     'rock_metal_hardrock': 3}\n",
    "\n",
    "def convert_labels(string_labels):\n",
    "  int_labels = []\n",
    "  for label in string_labels:\n",
    "    int_labels.append(labels_str_to_int[label])\n",
    "  return np.array(int_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWov5_pZ21lb"
   },
   "outputs": [],
   "source": [
    "mfcc_y_train = convert_labels(mfcc_y_train)\n",
    "mfcc_y_test  = convert_labels(mfcc_y_test)\n",
    "mfcc_y_val   = convert_labels(mfcc_y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUwrodiXcxdH"
   },
   "source": [
    "Use Pytorch Dataloaders to load data into the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZfborhZVIyM"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(list(zip(mfcc_x_train, mfcc_y_train)), batch_size=16, shuffle=True)\n",
    "test_dataloader  = DataLoader(list(zip(mfcc_x_test, mfcc_y_test)), batch_size=16, shuffle=True)\n",
    "val_dataloader   = DataLoader(list(zip(mfcc_x_val, mfcc_y_val)), batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-Gs6IqooN9u"
   },
   "source": [
    "Define device.\n",
    "\n",
    "This will be useful for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opZ5zR0X2Kyy",
    "outputId": "9baeb4c4-c598-4f76-c5ea-4d1cfa3e3d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrFoRbw-ObIS"
   },
   "source": [
    "### Step 2 - Definintion of the Neural Network\n",
    "\n",
    "The Neural Network that will be used consists of $4$ layers with $26$, $128$, $32$ and $4$ perceptrons respectively.\n",
    "\n",
    "The first layer is the input layer and it consists of $26$ perceptrons, because the input size is $26$.\n",
    "\n",
    "The next two layers that consist of $128$ and $32$ perceptrons respectively are the hidden layers.\n",
    "\n",
    "The last layer is the output layer and it consists of $4$ perceptrons, because the number of different labels is $4$.\n",
    "<br></br>\n",
    "\n",
    "The code shown in compimentary courses has been modified in order to implement the Neural Network that was described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jp6QKEDCdE0A"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # data is one-dimensional, there's no need to flatten it\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(26, 128), # input size = 26 -> number of perceptrons in 1st hidden layer = 128\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 32), # number of perceptrons in 1st hidden layer = 128 ->\n",
    "                            # number of perceptrons in 2nd hidden layer = 32\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 4)    # number of perceptrons in 2nd hidden layer = 32 ->\n",
    "                            # number of different labels = 4\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # data is one-dimensional, there's no need to flatten it\n",
    "    return self.linear_relu_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJhAfU5BjZuM"
   },
   "source": [
    "### Step 3 - Definition of the training procedure\n",
    "\n",
    "This function trains the given Neural Network for the given number of epochs using the given dataloader, optimizer and loss function.\n",
    "\n",
    "The Neural Network is returned after training it for the given number of epochs.\n",
    "<br></br>\n",
    "\n",
    "The code shown in compimentary courses has been modified in order to implement the training procedure was described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yD6nwi1l3iu"
   },
   "outputs": [],
   "source": [
    "def train_neural_network(epochs, optimizer, dataloader, loss_function, model):\n",
    "  size = len(dataloader.dataset)\n",
    "\n",
    "  for epoch in range(0, epochs):\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"-----------------------------\")\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(x.float())\n",
    "      loss = loss_function(prediction, y)\n",
    "\n",
    "      # backpropagation\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss, current = loss.item(), batch * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    print()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AKYMwBPq3Xw"
   },
   "source": [
    "### Step 4 - Definition of the testing procedure\n",
    "\n",
    "This function tests the given Neural Network on the dataset that can be retreived in batches using the given dataloader and returns the values of three metrics; loss, f1 macro averaged score and accuracy and the confusion matrix.\n",
    "<br></br>\n",
    "\n",
    "The code shown in compimentary courses has been modified in order to implement the testing procedure was described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi2tGcbIrPvS"
   },
   "outputs": [],
   "source": [
    "def test_neural_network(dataloader, loss_function, model):\n",
    "  size = len(dataloader.dataset)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  predictions = []\n",
    "  labels = []\n",
    "  with torch.no_grad():\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(x.float())\n",
    "      predictions.append(prediction.argmax(1))\n",
    "      test_loss += loss_function(prediction, y)\n",
    "      correct += (prediction.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "      # keep labels for later\n",
    "      labels.append(y)\n",
    "\n",
    "  test_loss /= size\n",
    "  accuracy = 100 * (correct / size)\n",
    "  predictions = torch.cat(predictions)\n",
    "  labels = torch.cat(labels)\n",
    "  f1_macro_avg = f1_score(predictions, labels, task='multiclass', num_classes=4, average='macro')\n",
    "\n",
    "  # move confusion matrix to GPU if GPU is in use\n",
    "  confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=4).to(device)\n",
    "  print(\"Test Error:\")\n",
    "  print(f\"Avg loss               : {test_loss:>8f}\")\n",
    "  print(f\"f1 macro averaged score: {f1_macro_avg:>8f}\")\n",
    "  print(f\"Accuracy               : {accuracy:>0.1f}%\")\n",
    "  print(f\"Confusion matrix       :\")\n",
    "  print(confusion_matrix(predictions, labels))\n",
    "\n",
    "  return test_loss, f1_macro_avg, accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9wWNZLXrbfV"
   },
   "source": [
    "### Step 5 - Training using CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhdkjX7B5CdM"
   },
   "source": [
    "Initialize model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0JtoLhdi5B1I",
    "outputId": "ad5e305e-e8d8-4ffe-e140-d8e49f0a969b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=26, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cpu_model = NeuralNetwork().to(device)\n",
    "print(cpu_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4r2pZuTXEsAC"
   },
   "source": [
    "Train model using $learning$ $rate = 0.002$, Stochastic Gradient Descent optimizer and Cross Entropy Loss function for $30$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNXJPOS8sMyf"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "optimizer = SGD(params=cpu_model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm8yJHvk2uIa"
   },
   "source": [
    "Train our Neural Network using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWluhvIO8XVj",
    "outputId": "34196b8e-1e1e-4291-e326-9ccdc4789333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 1.327188  [ 1200/ 3200]\n",
      "loss: 1.379151  [ 1216/ 3200]\n",
      "loss: 1.404680  [ 1232/ 3200]\n",
      "loss: 1.353718  [ 1248/ 3200]\n",
      "loss: 1.336174  [ 1264/ 3200]\n",
      "loss: 1.388414  [ 1280/ 3200]\n",
      "loss: 1.340556  [ 1296/ 3200]\n",
      "loss: 1.384500  [ 1312/ 3200]\n",
      "loss: 1.315806  [ 1328/ 3200]\n",
      "loss: 1.357095  [ 1344/ 3200]\n",
      "loss: 1.375440  [ 1360/ 3200]\n",
      "loss: 1.338940  [ 1376/ 3200]\n",
      "loss: 1.381393  [ 1392/ 3200]\n",
      "loss: 1.353540  [ 1408/ 3200]\n",
      "loss: 1.334444  [ 1424/ 3200]\n",
      "loss: 1.371490  [ 1440/ 3200]\n",
      "loss: 1.344838  [ 1456/ 3200]\n",
      "loss: 1.361484  [ 1472/ 3200]\n",
      "loss: 1.349878  [ 1488/ 3200]\n",
      "loss: 1.350991  [ 1504/ 3200]\n",
      "loss: 1.351029  [ 1520/ 3200]\n",
      "loss: 1.348270  [ 1536/ 3200]\n",
      "loss: 1.359521  [ 1552/ 3200]\n",
      "loss: 1.341205  [ 1568/ 3200]\n",
      "loss: 1.344490  [ 1584/ 3200]\n",
      "loss: 1.346338  [ 1600/ 3200]\n",
      "loss: 1.322726  [ 1616/ 3200]\n",
      "loss: 1.331240  [ 1632/ 3200]\n",
      "loss: 1.384868  [ 1648/ 3200]\n",
      "loss: 1.346686  [ 1664/ 3200]\n",
      "loss: 1.342937  [ 1680/ 3200]\n",
      "loss: 1.338222  [ 1696/ 3200]\n",
      "loss: 1.322824  [ 1712/ 3200]\n",
      "loss: 1.355165  [ 1728/ 3200]\n",
      "loss: 1.356347  [ 1744/ 3200]\n",
      "loss: 1.369758  [ 1760/ 3200]\n",
      "loss: 1.397385  [ 1776/ 3200]\n",
      "loss: 1.340540  [ 1792/ 3200]\n",
      "loss: 1.352220  [ 1808/ 3200]\n",
      "loss: 1.355802  [ 1824/ 3200]\n",
      "loss: 1.359085  [ 1840/ 3200]\n",
      "loss: 1.338726  [ 1856/ 3200]\n",
      "loss: 1.339803  [ 1872/ 3200]\n",
      "loss: 1.337467  [ 1888/ 3200]\n",
      "loss: 1.332569  [ 1904/ 3200]\n",
      "loss: 1.344567  [ 1920/ 3200]\n",
      "loss: 1.372841  [ 1936/ 3200]\n",
      "loss: 1.345863  [ 1952/ 3200]\n",
      "loss: 1.376998  [ 1968/ 3200]\n",
      "loss: 1.370893  [ 1984/ 3200]\n",
      "loss: 1.355761  [ 2000/ 3200]\n",
      "loss: 1.354367  [ 2016/ 3200]\n",
      "loss: 1.333577  [ 2032/ 3200]\n",
      "loss: 1.317679  [ 2048/ 3200]\n",
      "loss: 1.353344  [ 2064/ 3200]\n",
      "loss: 1.337327  [ 2080/ 3200]\n",
      "loss: 1.341996  [ 2096/ 3200]\n",
      "loss: 1.342767  [ 2112/ 3200]\n",
      "loss: 1.350815  [ 2128/ 3200]\n",
      "loss: 1.381368  [ 2144/ 3200]\n",
      "loss: 1.336068  [ 2160/ 3200]\n",
      "loss: 1.364887  [ 2176/ 3200]\n",
      "loss: 1.339269  [ 2192/ 3200]\n",
      "loss: 1.320726  [ 2208/ 3200]\n",
      "loss: 1.296713  [ 2224/ 3200]\n",
      "loss: 1.305073  [ 2240/ 3200]\n",
      "loss: 1.381904  [ 2256/ 3200]\n",
      "loss: 1.340328  [ 2272/ 3200]\n",
      "loss: 1.325977  [ 2288/ 3200]\n",
      "loss: 1.356212  [ 2304/ 3200]\n",
      "loss: 1.374900  [ 2320/ 3200]\n",
      "loss: 1.308463  [ 2336/ 3200]\n",
      "loss: 1.369331  [ 2352/ 3200]\n",
      "loss: 1.380595  [ 2368/ 3200]\n",
      "loss: 1.337841  [ 2384/ 3200]\n",
      "loss: 1.348307  [ 2400/ 3200]\n",
      "loss: 1.322416  [ 2416/ 3200]\n",
      "loss: 1.333935  [ 2432/ 3200]\n",
      "loss: 1.307328  [ 2448/ 3200]\n",
      "loss: 1.353440  [ 2464/ 3200]\n",
      "loss: 1.369501  [ 2480/ 3200]\n",
      "loss: 1.353023  [ 2496/ 3200]\n",
      "loss: 1.342515  [ 2512/ 3200]\n",
      "loss: 1.358745  [ 2528/ 3200]\n",
      "loss: 1.332778  [ 2544/ 3200]\n",
      "loss: 1.327437  [ 2560/ 3200]\n",
      "loss: 1.347487  [ 2576/ 3200]\n",
      "loss: 1.351760  [ 2592/ 3200]\n",
      "loss: 1.348053  [ 2608/ 3200]\n",
      "loss: 1.326678  [ 2624/ 3200]\n",
      "loss: 1.317313  [ 2640/ 3200]\n",
      "loss: 1.291119  [ 2656/ 3200]\n",
      "loss: 1.368634  [ 2672/ 3200]\n",
      "loss: 1.356337  [ 2688/ 3200]\n",
      "loss: 1.346892  [ 2704/ 3200]\n",
      "loss: 1.329101  [ 2720/ 3200]\n",
      "loss: 1.296575  [ 2736/ 3200]\n",
      "loss: 1.305427  [ 2752/ 3200]\n",
      "loss: 1.301959  [ 2768/ 3200]\n",
      "loss: 1.353184  [ 2784/ 3200]\n",
      "loss: 1.382543  [ 2800/ 3200]\n",
      "loss: 1.314443  [ 2816/ 3200]\n",
      "loss: 1.333191  [ 2832/ 3200]\n",
      "loss: 1.357451  [ 2848/ 3200]\n",
      "loss: 1.333406  [ 2864/ 3200]\n",
      "loss: 1.357893  [ 2880/ 3200]\n",
      "loss: 1.357174  [ 2896/ 3200]\n",
      "loss: 1.336701  [ 2912/ 3200]\n",
      "loss: 1.317212  [ 2928/ 3200]\n",
      "loss: 1.344596  [ 2944/ 3200]\n",
      "loss: 1.323920  [ 2960/ 3200]\n",
      "loss: 1.322575  [ 2976/ 3200]\n",
      "loss: 1.311402  [ 2992/ 3200]\n",
      "loss: 1.319391  [ 3008/ 3200]\n",
      "loss: 1.351017  [ 3024/ 3200]\n",
      "loss: 1.342051  [ 3040/ 3200]\n",
      "loss: 1.355234  [ 3056/ 3200]\n",
      "loss: 1.346267  [ 3072/ 3200]\n",
      "loss: 1.340086  [ 3088/ 3200]\n",
      "loss: 1.315315  [ 3104/ 3200]\n",
      "loss: 1.358218  [ 3120/ 3200]\n",
      "loss: 1.359586  [ 3136/ 3200]\n",
      "loss: 1.315067  [ 3152/ 3200]\n",
      "loss: 1.319778  [ 3168/ 3200]\n",
      "loss: 1.334550  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 7\n",
      "-----------------------------\n",
      "loss: 1.366905  [    0/ 3200]\n",
      "loss: 1.336998  [   16/ 3200]\n",
      "loss: 1.304801  [   32/ 3200]\n",
      "loss: 1.343471  [   48/ 3200]\n",
      "loss: 1.373013  [   64/ 3200]\n",
      "loss: 1.363879  [   80/ 3200]\n",
      "loss: 1.301764  [   96/ 3200]\n",
      "loss: 1.349694  [  112/ 3200]\n",
      "loss: 1.383382  [  128/ 3200]\n",
      "loss: 1.355250  [  144/ 3200]\n",
      "loss: 1.362653  [  160/ 3200]\n",
      "loss: 1.326205  [  176/ 3200]\n",
      "loss: 1.372903  [  192/ 3200]\n",
      "loss: 1.348983  [  208/ 3200]\n",
      "loss: 1.310240  [  224/ 3200]\n",
      "loss: 1.384793  [  240/ 3200]\n",
      "loss: 1.340195  [  256/ 3200]\n",
      "loss: 1.356422  [  272/ 3200]\n",
      "loss: 1.339496  [  288/ 3200]\n",
      "loss: 1.306238  [  304/ 3200]\n",
      "loss: 1.298197  [  320/ 3200]\n",
      "loss: 1.340968  [  336/ 3200]\n",
      "loss: 1.318600  [  352/ 3200]\n",
      "loss: 1.310008  [  368/ 3200]\n",
      "loss: 1.328388  [  384/ 3200]\n",
      "loss: 1.305045  [  400/ 3200]\n",
      "loss: 1.323316  [  416/ 3200]\n",
      "loss: 1.353043  [  432/ 3200]\n",
      "loss: 1.334800  [  448/ 3200]\n",
      "loss: 1.272643  [  464/ 3200]\n",
      "loss: 1.380825  [  480/ 3200]\n",
      "loss: 1.358892  [  496/ 3200]\n",
      "loss: 1.337035  [  512/ 3200]\n",
      "loss: 1.356482  [  528/ 3200]\n",
      "loss: 1.333836  [  544/ 3200]\n",
      "loss: 1.361791  [  560/ 3200]\n",
      "loss: 1.315516  [  576/ 3200]\n",
      "loss: 1.309125  [  592/ 3200]\n",
      "loss: 1.346020  [  608/ 3200]\n",
      "loss: 1.340535  [  624/ 3200]\n",
      "loss: 1.387179  [  640/ 3200]\n",
      "loss: 1.281856  [  656/ 3200]\n",
      "loss: 1.392291  [  672/ 3200]\n",
      "loss: 1.326715  [  688/ 3200]\n",
      "loss: 1.320455  [  704/ 3200]\n",
      "loss: 1.288340  [  720/ 3200]\n",
      "loss: 1.335191  [  736/ 3200]\n",
      "loss: 1.324851  [  752/ 3200]\n",
      "loss: 1.349469  [  768/ 3200]\n",
      "loss: 1.370647  [  784/ 3200]\n",
      "loss: 1.297262  [  800/ 3200]\n",
      "loss: 1.314633  [  816/ 3200]\n",
      "loss: 1.349592  [  832/ 3200]\n",
      "loss: 1.276292  [  848/ 3200]\n",
      "loss: 1.301199  [  864/ 3200]\n",
      "loss: 1.350672  [  880/ 3200]\n",
      "loss: 1.259221  [  896/ 3200]\n",
      "loss: 1.422704  [  912/ 3200]\n",
      "loss: 1.332282  [  928/ 3200]\n",
      "loss: 1.413424  [  944/ 3200]\n",
      "loss: 1.343323  [  960/ 3200]\n",
      "loss: 1.288763  [  976/ 3200]\n",
      "loss: 1.309015  [  992/ 3200]\n",
      "loss: 1.269204  [ 1008/ 3200]\n",
      "loss: 1.396689  [ 1024/ 3200]\n",
      "loss: 1.309815  [ 1040/ 3200]\n",
      "loss: 1.333563  [ 1056/ 3200]\n",
      "loss: 1.320155  [ 1072/ 3200]\n",
      "loss: 1.314307  [ 1088/ 3200]\n",
      "loss: 1.290271  [ 1104/ 3200]\n",
      "loss: 1.358334  [ 1120/ 3200]\n",
      "loss: 1.311715  [ 1136/ 3200]\n",
      "loss: 1.300890  [ 1152/ 3200]\n",
      "loss: 1.333777  [ 1168/ 3200]\n",
      "loss: 1.338500  [ 1184/ 3200]\n",
      "loss: 1.311720  [ 1200/ 3200]\n",
      "loss: 1.342479  [ 1216/ 3200]\n",
      "loss: 1.344056  [ 1232/ 3200]\n",
      "loss: 1.306409  [ 1248/ 3200]\n",
      "loss: 1.338921  [ 1264/ 3200]\n",
      "loss: 1.256849  [ 1280/ 3200]\n",
      "loss: 1.323688  [ 1296/ 3200]\n",
      "loss: 1.313908  [ 1312/ 3200]\n",
      "loss: 1.304811  [ 1328/ 3200]\n",
      "loss: 1.345654  [ 1344/ 3200]\n",
      "loss: 1.379412  [ 1360/ 3200]\n",
      "loss: 1.323283  [ 1376/ 3200]\n",
      "loss: 1.403721  [ 1392/ 3200]\n",
      "loss: 1.281246  [ 1408/ 3200]\n",
      "loss: 1.344447  [ 1424/ 3200]\n",
      "loss: 1.391445  [ 1440/ 3200]\n",
      "loss: 1.395070  [ 1456/ 3200]\n",
      "loss: 1.383307  [ 1472/ 3200]\n",
      "loss: 1.305005  [ 1488/ 3200]\n",
      "loss: 1.314266  [ 1504/ 3200]\n",
      "loss: 1.320955  [ 1520/ 3200]\n",
      "loss: 1.306202  [ 1536/ 3200]\n",
      "loss: 1.307783  [ 1552/ 3200]\n",
      "loss: 1.405430  [ 1568/ 3200]\n",
      "loss: 1.335813  [ 1584/ 3200]\n",
      "loss: 1.404115  [ 1600/ 3200]\n",
      "loss: 1.333778  [ 1616/ 3200]\n",
      "loss: 1.336354  [ 1632/ 3200]\n",
      "loss: 1.365880  [ 1648/ 3200]\n",
      "loss: 1.326414  [ 1664/ 3200]\n",
      "loss: 1.367570  [ 1680/ 3200]\n",
      "loss: 1.292659  [ 1696/ 3200]\n",
      "loss: 1.369394  [ 1712/ 3200]\n",
      "loss: 1.307483  [ 1728/ 3200]\n",
      "loss: 1.285728  [ 1744/ 3200]\n",
      "loss: 1.311623  [ 1760/ 3200]\n",
      "loss: 1.332637  [ 1776/ 3200]\n",
      "loss: 1.325752  [ 1792/ 3200]\n",
      "loss: 1.312127  [ 1808/ 3200]\n",
      "loss: 1.343857  [ 1824/ 3200]\n",
      "loss: 1.313658  [ 1840/ 3200]\n",
      "loss: 1.349165  [ 1856/ 3200]\n",
      "loss: 1.375919  [ 1872/ 3200]\n",
      "loss: 1.380900  [ 1888/ 3200]\n",
      "loss: 1.378246  [ 1904/ 3200]\n",
      "loss: 1.311998  [ 1920/ 3200]\n",
      "loss: 1.245610  [ 1936/ 3200]\n",
      "loss: 1.344893  [ 1952/ 3200]\n",
      "loss: 1.377797  [ 1968/ 3200]\n",
      "loss: 1.321003  [ 1984/ 3200]\n",
      "loss: 1.313549  [ 2000/ 3200]\n",
      "loss: 1.325989  [ 2016/ 3200]\n",
      "loss: 1.378357  [ 2032/ 3200]\n",
      "loss: 1.346952  [ 2048/ 3200]\n",
      "loss: 1.387964  [ 2064/ 3200]\n",
      "loss: 1.354270  [ 2080/ 3200]\n",
      "loss: 1.303596  [ 2096/ 3200]\n",
      "loss: 1.341401  [ 2112/ 3200]\n",
      "loss: 1.328021  [ 2128/ 3200]\n",
      "loss: 1.358671  [ 2144/ 3200]\n",
      "loss: 1.348056  [ 2160/ 3200]\n",
      "loss: 1.348040  [ 2176/ 3200]\n",
      "loss: 1.330006  [ 2192/ 3200]\n",
      "loss: 1.346057  [ 2208/ 3200]\n",
      "loss: 1.318791  [ 2224/ 3200]\n",
      "loss: 1.346799  [ 2240/ 3200]\n",
      "loss: 1.317720  [ 2256/ 3200]\n",
      "loss: 1.328556  [ 2272/ 3200]\n",
      "loss: 1.328728  [ 2288/ 3200]\n",
      "loss: 1.294622  [ 2304/ 3200]\n",
      "loss: 1.347156  [ 2320/ 3200]\n",
      "loss: 1.349303  [ 2336/ 3200]\n",
      "loss: 1.374182  [ 2352/ 3200]\n",
      "loss: 1.351452  [ 2368/ 3200]\n",
      "loss: 1.344140  [ 2384/ 3200]\n",
      "loss: 1.341773  [ 2400/ 3200]\n",
      "loss: 1.350278  [ 2416/ 3200]\n",
      "loss: 1.342515  [ 2432/ 3200]\n",
      "loss: 1.365350  [ 2448/ 3200]\n",
      "loss: 1.352436  [ 2464/ 3200]\n",
      "loss: 1.325881  [ 2480/ 3200]\n",
      "loss: 1.357235  [ 2496/ 3200]\n",
      "loss: 1.335311  [ 2512/ 3200]\n",
      "loss: 1.318631  [ 2528/ 3200]\n",
      "loss: 1.322817  [ 2544/ 3200]\n",
      "loss: 1.326508  [ 2560/ 3200]\n",
      "loss: 1.328811  [ 2576/ 3200]\n",
      "loss: 1.338240  [ 2592/ 3200]\n",
      "loss: 1.358591  [ 2608/ 3200]\n",
      "loss: 1.324889  [ 2624/ 3200]\n",
      "loss: 1.349394  [ 2640/ 3200]\n",
      "loss: 1.345053  [ 2656/ 3200]\n",
      "loss: 1.329145  [ 2672/ 3200]\n",
      "loss: 1.307721  [ 2688/ 3200]\n",
      "loss: 1.334018  [ 2704/ 3200]\n",
      "loss: 1.342467  [ 2720/ 3200]\n",
      "loss: 1.381136  [ 2736/ 3200]\n",
      "loss: 1.337770  [ 2752/ 3200]\n",
      "loss: 1.337525  [ 2768/ 3200]\n",
      "loss: 1.345758  [ 2784/ 3200]\n",
      "loss: 1.312281  [ 2800/ 3200]\n",
      "loss: 1.323640  [ 2816/ 3200]\n",
      "loss: 1.347714  [ 2832/ 3200]\n",
      "loss: 1.348186  [ 2848/ 3200]\n",
      "loss: 1.342278  [ 2864/ 3200]\n",
      "loss: 1.341583  [ 2880/ 3200]\n",
      "loss: 1.351319  [ 2896/ 3200]\n",
      "loss: 1.327599  [ 2912/ 3200]\n",
      "loss: 1.355697  [ 2928/ 3200]\n",
      "loss: 1.310275  [ 2944/ 3200]\n",
      "loss: 1.324775  [ 2960/ 3200]\n",
      "loss: 1.391557  [ 2976/ 3200]\n",
      "loss: 1.331271  [ 2992/ 3200]\n",
      "loss: 1.333619  [ 3008/ 3200]\n",
      "loss: 1.316945  [ 3024/ 3200]\n",
      "loss: 1.331967  [ 3040/ 3200]\n",
      "loss: 1.352294  [ 3056/ 3200]\n",
      "loss: 1.280385  [ 3072/ 3200]\n",
      "loss: 1.355639  [ 3088/ 3200]\n",
      "loss: 1.326260  [ 3104/ 3200]\n",
      "loss: 1.343658  [ 3120/ 3200]\n",
      "loss: 1.306145  [ 3136/ 3200]\n",
      "loss: 1.364281  [ 3152/ 3200]\n",
      "loss: 1.277696  [ 3168/ 3200]\n",
      "loss: 1.314755  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 1.337415  [    0/ 3200]\n",
      "loss: 1.276264  [   16/ 3200]\n",
      "loss: 1.333288  [   32/ 3200]\n",
      "loss: 1.339448  [   48/ 3200]\n",
      "loss: 1.322574  [   64/ 3200]\n",
      "loss: 1.363814  [   80/ 3200]\n",
      "loss: 1.295529  [   96/ 3200]\n",
      "loss: 1.381159  [  112/ 3200]\n",
      "loss: 1.382792  [  128/ 3200]\n",
      "loss: 1.344844  [  144/ 3200]\n",
      "loss: 1.367868  [  160/ 3200]\n",
      "loss: 1.343042  [  176/ 3200]\n",
      "loss: 1.340788  [  192/ 3200]\n",
      "loss: 1.355667  [  208/ 3200]\n",
      "loss: 1.298456  [  224/ 3200]\n",
      "loss: 1.392492  [  240/ 3200]\n",
      "loss: 1.351744  [  256/ 3200]\n",
      "loss: 1.313175  [  272/ 3200]\n",
      "loss: 1.359535  [  288/ 3200]\n",
      "loss: 1.305736  [  304/ 3200]\n",
      "loss: 1.312752  [  320/ 3200]\n",
      "loss: 1.356035  [  336/ 3200]\n",
      "loss: 1.310535  [  352/ 3200]\n",
      "loss: 1.318845  [  368/ 3200]\n",
      "loss: 1.378773  [  384/ 3200]\n",
      "loss: 1.356166  [  400/ 3200]\n",
      "loss: 1.363856  [  416/ 3200]\n",
      "loss: 1.362151  [  432/ 3200]\n",
      "loss: 1.351364  [  448/ 3200]\n",
      "loss: 1.298155  [  464/ 3200]\n",
      "loss: 1.306732  [  480/ 3200]\n",
      "loss: 1.320885  [  496/ 3200]\n",
      "loss: 1.387365  [  512/ 3200]\n",
      "loss: 1.315668  [  528/ 3200]\n",
      "loss: 1.322814  [  544/ 3200]\n",
      "loss: 1.323816  [  560/ 3200]\n",
      "loss: 1.268708  [  576/ 3200]\n",
      "loss: 1.332435  [  592/ 3200]\n",
      "loss: 1.360482  [  608/ 3200]\n",
      "loss: 1.342796  [  624/ 3200]\n",
      "loss: 1.337333  [  640/ 3200]\n",
      "loss: 1.313780  [  656/ 3200]\n",
      "loss: 1.290456  [  672/ 3200]\n",
      "loss: 1.370908  [  688/ 3200]\n",
      "loss: 1.360906  [  704/ 3200]\n",
      "loss: 1.333382  [  720/ 3200]\n",
      "loss: 1.342580  [  736/ 3200]\n",
      "loss: 1.309200  [  752/ 3200]\n",
      "loss: 1.313428  [  768/ 3200]\n",
      "loss: 1.318753  [  784/ 3200]\n",
      "loss: 1.346768  [  800/ 3200]\n",
      "loss: 1.373024  [  816/ 3200]\n",
      "loss: 1.347816  [  832/ 3200]\n",
      "loss: 1.309335  [  848/ 3200]\n",
      "loss: 1.333544  [  864/ 3200]\n",
      "loss: 1.315101  [  880/ 3200]\n",
      "loss: 1.317813  [  896/ 3200]\n",
      "loss: 1.369943  [  912/ 3200]\n",
      "loss: 1.318718  [  928/ 3200]\n",
      "loss: 1.309986  [  944/ 3200]\n",
      "loss: 1.299816  [  960/ 3200]\n",
      "loss: 1.343484  [  976/ 3200]\n",
      "loss: 1.293523  [  992/ 3200]\n",
      "loss: 1.336602  [ 1008/ 3200]\n",
      "loss: 1.383126  [ 1024/ 3200]\n",
      "loss: 1.305415  [ 1040/ 3200]\n",
      "loss: 1.297810  [ 1056/ 3200]\n",
      "loss: 1.275681  [ 1072/ 3200]\n",
      "loss: 1.376889  [ 1088/ 3200]\n",
      "loss: 1.348646  [ 1104/ 3200]\n",
      "loss: 1.319800  [ 1120/ 3200]\n",
      "loss: 1.334529  [ 1136/ 3200]\n",
      "loss: 1.328628  [ 1152/ 3200]\n",
      "loss: 1.366863  [ 1168/ 3200]\n",
      "loss: 1.338106  [ 1184/ 3200]\n",
      "loss: 1.328291  [ 1200/ 3200]\n",
      "loss: 1.314219  [ 1216/ 3200]\n",
      "loss: 1.320161  [ 1232/ 3200]\n",
      "loss: 1.339700  [ 1248/ 3200]\n",
      "loss: 1.301269  [ 1264/ 3200]\n",
      "loss: 1.305735  [ 1280/ 3200]\n",
      "loss: 1.361253  [ 1296/ 3200]\n",
      "loss: 1.352381  [ 1312/ 3200]\n",
      "loss: 1.311960  [ 1328/ 3200]\n",
      "loss: 1.278326  [ 1344/ 3200]\n",
      "loss: 1.332299  [ 1360/ 3200]\n",
      "loss: 1.372845  [ 1376/ 3200]\n",
      "loss: 1.308195  [ 1392/ 3200]\n",
      "loss: 1.339111  [ 1408/ 3200]\n",
      "loss: 1.322819  [ 1424/ 3200]\n",
      "loss: 1.307184  [ 1440/ 3200]\n",
      "loss: 1.361649  [ 1456/ 3200]\n",
      "loss: 1.324496  [ 1472/ 3200]\n",
      "loss: 1.341953  [ 1488/ 3200]\n",
      "loss: 1.333451  [ 1504/ 3200]\n",
      "loss: 1.353049  [ 1520/ 3200]\n",
      "loss: 1.354350  [ 1536/ 3200]\n",
      "loss: 1.347728  [ 1552/ 3200]\n",
      "loss: 1.309665  [ 1568/ 3200]\n",
      "loss: 1.306706  [ 1584/ 3200]\n",
      "loss: 1.342981  [ 1600/ 3200]\n",
      "loss: 1.326839  [ 1616/ 3200]\n",
      "loss: 1.275129  [ 1632/ 3200]\n",
      "loss: 1.303052  [ 1648/ 3200]\n",
      "loss: 1.282716  [ 1664/ 3200]\n",
      "loss: 1.375842  [ 1680/ 3200]\n",
      "loss: 1.293080  [ 1696/ 3200]\n",
      "loss: 1.351094  [ 1712/ 3200]\n",
      "loss: 1.416589  [ 1728/ 3200]\n",
      "loss: 1.346905  [ 1744/ 3200]\n",
      "loss: 1.293901  [ 1760/ 3200]\n",
      "loss: 1.363740  [ 1776/ 3200]\n",
      "loss: 1.328192  [ 1792/ 3200]\n",
      "loss: 1.323743  [ 1808/ 3200]\n",
      "loss: 1.278302  [ 1824/ 3200]\n",
      "loss: 1.280861  [ 1840/ 3200]\n",
      "loss: 1.358984  [ 1856/ 3200]\n",
      "loss: 1.378779  [ 1872/ 3200]\n",
      "loss: 1.316553  [ 1888/ 3200]\n",
      "loss: 1.296574  [ 1904/ 3200]\n",
      "loss: 1.292706  [ 1920/ 3200]\n",
      "loss: 1.338755  [ 1936/ 3200]\n",
      "loss: 1.322052  [ 1952/ 3200]\n",
      "loss: 1.345589  [ 1968/ 3200]\n",
      "loss: 1.330654  [ 1984/ 3200]\n",
      "loss: 1.309256  [ 2000/ 3200]\n",
      "loss: 1.343719  [ 2016/ 3200]\n",
      "loss: 1.330421  [ 2032/ 3200]\n",
      "loss: 1.299994  [ 2048/ 3200]\n",
      "loss: 1.321328  [ 2064/ 3200]\n",
      "loss: 1.269982  [ 2080/ 3200]\n",
      "loss: 1.318875  [ 2096/ 3200]\n",
      "loss: 1.309524  [ 2112/ 3200]\n",
      "loss: 1.360958  [ 2128/ 3200]\n",
      "loss: 1.406525  [ 2144/ 3200]\n",
      "loss: 1.346737  [ 2160/ 3200]\n",
      "loss: 1.353546  [ 2176/ 3200]\n",
      "loss: 1.303876  [ 2192/ 3200]\n",
      "loss: 1.367056  [ 2208/ 3200]\n",
      "loss: 1.310105  [ 2224/ 3200]\n",
      "loss: 1.324004  [ 2240/ 3200]\n",
      "loss: 1.259197  [ 2256/ 3200]\n",
      "loss: 1.254694  [ 2272/ 3200]\n",
      "loss: 1.317028  [ 2288/ 3200]\n",
      "loss: 1.300678  [ 2304/ 3200]\n",
      "loss: 1.327945  [ 2320/ 3200]\n",
      "loss: 1.315323  [ 2336/ 3200]\n",
      "loss: 1.324427  [ 2352/ 3200]\n",
      "loss: 1.347871  [ 2368/ 3200]\n",
      "loss: 1.370843  [ 2384/ 3200]\n",
      "loss: 1.327789  [ 2400/ 3200]\n",
      "loss: 1.366857  [ 2416/ 3200]\n",
      "loss: 1.349750  [ 2432/ 3200]\n",
      "loss: 1.262789  [ 2448/ 3200]\n",
      "loss: 1.354677  [ 2464/ 3200]\n",
      "loss: 1.320438  [ 2480/ 3200]\n",
      "loss: 1.322867  [ 2496/ 3200]\n",
      "loss: 1.290355  [ 2512/ 3200]\n",
      "loss: 1.328220  [ 2528/ 3200]\n",
      "loss: 1.319951  [ 2544/ 3200]\n",
      "loss: 1.345242  [ 2560/ 3200]\n",
      "loss: 1.339633  [ 2576/ 3200]\n",
      "loss: 1.303624  [ 2592/ 3200]\n",
      "loss: 1.294553  [ 2608/ 3200]\n",
      "loss: 1.295237  [ 2624/ 3200]\n",
      "loss: 1.336196  [ 2640/ 3200]\n",
      "loss: 1.308651  [ 2656/ 3200]\n",
      "loss: 1.297362  [ 2672/ 3200]\n",
      "loss: 1.311259  [ 2688/ 3200]\n",
      "loss: 1.317767  [ 2704/ 3200]\n",
      "loss: 1.345327  [ 2720/ 3200]\n",
      "loss: 1.291138  [ 2736/ 3200]\n",
      "loss: 1.368964  [ 2752/ 3200]\n",
      "loss: 1.284331  [ 2768/ 3200]\n",
      "loss: 1.359347  [ 2784/ 3200]\n",
      "loss: 1.289083  [ 2800/ 3200]\n",
      "loss: 1.350636  [ 2816/ 3200]\n",
      "loss: 1.294896  [ 2832/ 3200]\n",
      "loss: 1.267120  [ 2848/ 3200]\n",
      "loss: 1.338244  [ 2864/ 3200]\n",
      "loss: 1.376445  [ 2880/ 3200]\n",
      "loss: 1.303901  [ 2896/ 3200]\n",
      "loss: 1.339007  [ 2912/ 3200]\n",
      "loss: 1.296327  [ 2928/ 3200]\n",
      "loss: 1.375345  [ 2944/ 3200]\n",
      "loss: 1.342882  [ 2960/ 3200]\n",
      "loss: 1.325645  [ 2976/ 3200]\n",
      "loss: 1.352350  [ 2992/ 3200]\n",
      "loss: 1.323507  [ 3008/ 3200]\n",
      "loss: 1.352227  [ 3024/ 3200]\n",
      "loss: 1.403311  [ 3040/ 3200]\n",
      "loss: 1.300625  [ 3056/ 3200]\n",
      "loss: 1.311041  [ 3072/ 3200]\n",
      "loss: 1.323338  [ 3088/ 3200]\n",
      "loss: 1.272837  [ 3104/ 3200]\n",
      "loss: 1.343295  [ 3120/ 3200]\n",
      "loss: 1.288357  [ 3136/ 3200]\n",
      "loss: 1.258336  [ 3152/ 3200]\n",
      "loss: 1.389179  [ 3168/ 3200]\n",
      "loss: 1.346455  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 1.343305  [    0/ 3200]\n",
      "loss: 1.350436  [   16/ 3200]\n",
      "loss: 1.320632  [   32/ 3200]\n",
      "loss: 1.285763  [   48/ 3200]\n",
      "loss: 1.331386  [   64/ 3200]\n",
      "loss: 1.294101  [   80/ 3200]\n",
      "loss: 1.345289  [   96/ 3200]\n",
      "loss: 1.279509  [  112/ 3200]\n",
      "loss: 1.336937  [  128/ 3200]\n",
      "loss: 1.344232  [  144/ 3200]\n",
      "loss: 1.365199  [  160/ 3200]\n",
      "loss: 1.335804  [  176/ 3200]\n",
      "loss: 1.318128  [  192/ 3200]\n",
      "loss: 1.282404  [  208/ 3200]\n",
      "loss: 1.338819  [  224/ 3200]\n",
      "loss: 1.290286  [  240/ 3200]\n",
      "loss: 1.304644  [  256/ 3200]\n",
      "loss: 1.359850  [  272/ 3200]\n",
      "loss: 1.340329  [  288/ 3200]\n",
      "loss: 1.353563  [  304/ 3200]\n",
      "loss: 1.350596  [  320/ 3200]\n",
      "loss: 1.303631  [  336/ 3200]\n",
      "loss: 1.314523  [  352/ 3200]\n",
      "loss: 1.309453  [  368/ 3200]\n",
      "loss: 1.316210  [  384/ 3200]\n",
      "loss: 1.364901  [  400/ 3200]\n",
      "loss: 1.299919  [  416/ 3200]\n",
      "loss: 1.296218  [  432/ 3200]\n",
      "loss: 1.271858  [  448/ 3200]\n",
      "loss: 1.359396  [  464/ 3200]\n",
      "loss: 1.324714  [  480/ 3200]\n",
      "loss: 1.338542  [  496/ 3200]\n",
      "loss: 1.296666  [  512/ 3200]\n",
      "loss: 1.328570  [  528/ 3200]\n",
      "loss: 1.337587  [  544/ 3200]\n",
      "loss: 1.331947  [  560/ 3200]\n",
      "loss: 1.332116  [  576/ 3200]\n",
      "loss: 1.288691  [  592/ 3200]\n",
      "loss: 1.303943  [  608/ 3200]\n",
      "loss: 1.309014  [  624/ 3200]\n",
      "loss: 1.264629  [  640/ 3200]\n",
      "loss: 1.320611  [  656/ 3200]\n",
      "loss: 1.321061  [  672/ 3200]\n",
      "loss: 1.341094  [  688/ 3200]\n",
      "loss: 1.264295  [  704/ 3200]\n",
      "loss: 1.272995  [  720/ 3200]\n",
      "loss: 1.336150  [  736/ 3200]\n",
      "loss: 1.383381  [  752/ 3200]\n",
      "loss: 1.359382  [  768/ 3200]\n",
      "loss: 1.336760  [  784/ 3200]\n",
      "loss: 1.321089  [  800/ 3200]\n",
      "loss: 1.331271  [  816/ 3200]\n",
      "loss: 1.263127  [  832/ 3200]\n",
      "loss: 1.273024  [  848/ 3200]\n",
      "loss: 1.317559  [  864/ 3200]\n",
      "loss: 1.313903  [  880/ 3200]\n",
      "loss: 1.297325  [  896/ 3200]\n",
      "loss: 1.320424  [  912/ 3200]\n",
      "loss: 1.308075  [  928/ 3200]\n",
      "loss: 1.296550  [  944/ 3200]\n",
      "loss: 1.323756  [  960/ 3200]\n",
      "loss: 1.333138  [  976/ 3200]\n",
      "loss: 1.331756  [  992/ 3200]\n",
      "loss: 1.302993  [ 1008/ 3200]\n",
      "loss: 1.351962  [ 1024/ 3200]\n",
      "loss: 1.304143  [ 1040/ 3200]\n",
      "loss: 1.282009  [ 1056/ 3200]\n",
      "loss: 1.302232  [ 1072/ 3200]\n",
      "loss: 1.389687  [ 1088/ 3200]\n",
      "loss: 1.305606  [ 1104/ 3200]\n",
      "loss: 1.330658  [ 1120/ 3200]\n",
      "loss: 1.284895  [ 1136/ 3200]\n",
      "loss: 1.343771  [ 1152/ 3200]\n",
      "loss: 1.372197  [ 1168/ 3200]\n",
      "loss: 1.326079  [ 1184/ 3200]\n",
      "loss: 1.348734  [ 1200/ 3200]\n",
      "loss: 1.333623  [ 1216/ 3200]\n",
      "loss: 1.305009  [ 1232/ 3200]\n",
      "loss: 1.346315  [ 1248/ 3200]\n",
      "loss: 1.261512  [ 1264/ 3200]\n",
      "loss: 1.338006  [ 1280/ 3200]\n",
      "loss: 1.315650  [ 1296/ 3200]\n",
      "loss: 1.337853  [ 1312/ 3200]\n",
      "loss: 1.291644  [ 1328/ 3200]\n",
      "loss: 1.405151  [ 1344/ 3200]\n",
      "loss: 1.321726  [ 1360/ 3200]\n",
      "loss: 1.317822  [ 1376/ 3200]\n",
      "loss: 1.286548  [ 1392/ 3200]\n",
      "loss: 1.304105  [ 1408/ 3200]\n",
      "loss: 1.304842  [ 1424/ 3200]\n",
      "loss: 1.301649  [ 1440/ 3200]\n",
      "loss: 1.329121  [ 1456/ 3200]\n",
      "loss: 1.363456  [ 1472/ 3200]\n",
      "loss: 1.347213  [ 1488/ 3200]\n",
      "loss: 1.361143  [ 1504/ 3200]\n",
      "loss: 1.307631  [ 1520/ 3200]\n",
      "loss: 1.312596  [ 1536/ 3200]\n",
      "loss: 1.320948  [ 1552/ 3200]\n",
      "loss: 1.364449  [ 1568/ 3200]\n",
      "loss: 1.361687  [ 1584/ 3200]\n",
      "loss: 1.313582  [ 1600/ 3200]\n",
      "loss: 1.331082  [ 1616/ 3200]\n",
      "loss: 1.328316  [ 1632/ 3200]\n",
      "loss: 1.285884  [ 1648/ 3200]\n",
      "loss: 1.302488  [ 1664/ 3200]\n",
      "loss: 1.342905  [ 1680/ 3200]\n",
      "loss: 1.327031  [ 1696/ 3200]\n",
      "loss: 1.319802  [ 1712/ 3200]\n",
      "loss: 1.326528  [ 1728/ 3200]\n",
      "loss: 1.319236  [ 1744/ 3200]\n",
      "loss: 1.312255  [ 1760/ 3200]\n",
      "loss: 1.316600  [ 1776/ 3200]\n",
      "loss: 1.301901  [ 1792/ 3200]\n",
      "loss: 1.337641  [ 1808/ 3200]\n",
      "loss: 1.301120  [ 1824/ 3200]\n",
      "loss: 1.316854  [ 1840/ 3200]\n",
      "loss: 1.304767  [ 1856/ 3200]\n",
      "loss: 1.349786  [ 1872/ 3200]\n",
      "loss: 1.268491  [ 1888/ 3200]\n",
      "loss: 1.280840  [ 1904/ 3200]\n",
      "loss: 1.351964  [ 1920/ 3200]\n",
      "loss: 1.291169  [ 1936/ 3200]\n",
      "loss: 1.315378  [ 1952/ 3200]\n",
      "loss: 1.332325  [ 1968/ 3200]\n",
      "loss: 1.264508  [ 1984/ 3200]\n",
      "loss: 1.314659  [ 2000/ 3200]\n",
      "loss: 1.245701  [ 2016/ 3200]\n",
      "loss: 1.363610  [ 2032/ 3200]\n",
      "loss: 1.336904  [ 2048/ 3200]\n",
      "loss: 1.297025  [ 2064/ 3200]\n",
      "loss: 1.332703  [ 2080/ 3200]\n",
      "loss: 1.273601  [ 2096/ 3200]\n",
      "loss: 1.291355  [ 2112/ 3200]\n",
      "loss: 1.305280  [ 2128/ 3200]\n",
      "loss: 1.333892  [ 2144/ 3200]\n",
      "loss: 1.277986  [ 2160/ 3200]\n",
      "loss: 1.329267  [ 2176/ 3200]\n",
      "loss: 1.380741  [ 2192/ 3200]\n",
      "loss: 1.289098  [ 2208/ 3200]\n",
      "loss: 1.305963  [ 2224/ 3200]\n",
      "loss: 1.295023  [ 2240/ 3200]\n",
      "loss: 1.327968  [ 2256/ 3200]\n",
      "loss: 1.350809  [ 2272/ 3200]\n",
      "loss: 1.264399  [ 2288/ 3200]\n",
      "loss: 1.260464  [ 2304/ 3200]\n",
      "loss: 1.316965  [ 2320/ 3200]\n",
      "loss: 1.359975  [ 2336/ 3200]\n",
      "loss: 1.326293  [ 2352/ 3200]\n",
      "loss: 1.254478  [ 2368/ 3200]\n",
      "loss: 1.309130  [ 2384/ 3200]\n",
      "loss: 1.265656  [ 2400/ 3200]\n",
      "loss: 1.264957  [ 2416/ 3200]\n",
      "loss: 1.308300  [ 2432/ 3200]\n",
      "loss: 1.321111  [ 2448/ 3200]\n",
      "loss: 1.327053  [ 2464/ 3200]\n",
      "loss: 1.274564  [ 2480/ 3200]\n",
      "loss: 1.279911  [ 2496/ 3200]\n",
      "loss: 1.291757  [ 2512/ 3200]\n",
      "loss: 1.346684  [ 2528/ 3200]\n",
      "loss: 1.315862  [ 2544/ 3200]\n",
      "loss: 1.324526  [ 2560/ 3200]\n",
      "loss: 1.259742  [ 2576/ 3200]\n",
      "loss: 1.347678  [ 2592/ 3200]\n",
      "loss: 1.282811  [ 2608/ 3200]\n",
      "loss: 1.288013  [ 2624/ 3200]\n",
      "loss: 1.263204  [ 2640/ 3200]\n",
      "loss: 1.286039  [ 2656/ 3200]\n",
      "loss: 1.355052  [ 2672/ 3200]\n",
      "loss: 1.318215  [ 2688/ 3200]\n",
      "loss: 1.329643  [ 2704/ 3200]\n",
      "loss: 1.260816  [ 2720/ 3200]\n",
      "loss: 1.330539  [ 2736/ 3200]\n",
      "loss: 1.322369  [ 2752/ 3200]\n",
      "loss: 1.359037  [ 2768/ 3200]\n",
      "loss: 1.294250  [ 2784/ 3200]\n",
      "loss: 1.341269  [ 2800/ 3200]\n",
      "loss: 1.410209  [ 2816/ 3200]\n",
      "loss: 1.287228  [ 2832/ 3200]\n",
      "loss: 1.297131  [ 2848/ 3200]\n",
      "loss: 1.332959  [ 2864/ 3200]\n",
      "loss: 1.278215  [ 2880/ 3200]\n",
      "loss: 1.295129  [ 2896/ 3200]\n",
      "loss: 1.346256  [ 2912/ 3200]\n",
      "loss: 1.349299  [ 2928/ 3200]\n",
      "loss: 1.288480  [ 2944/ 3200]\n",
      "loss: 1.296795  [ 2960/ 3200]\n",
      "loss: 1.258700  [ 2976/ 3200]\n",
      "loss: 1.325978  [ 2992/ 3200]\n",
      "loss: 1.317755  [ 3008/ 3200]\n",
      "loss: 1.362905  [ 3024/ 3200]\n",
      "loss: 1.361520  [ 3040/ 3200]\n",
      "loss: 1.278503  [ 3056/ 3200]\n",
      "loss: 1.337677  [ 3072/ 3200]\n",
      "loss: 1.351126  [ 3088/ 3200]\n",
      "loss: 1.376037  [ 3104/ 3200]\n",
      "loss: 1.280337  [ 3120/ 3200]\n",
      "loss: 1.325267  [ 3136/ 3200]\n",
      "loss: 1.302220  [ 3152/ 3200]\n",
      "loss: 1.275680  [ 3168/ 3200]\n",
      "loss: 1.260982  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 1.259421  [    0/ 3200]\n",
      "loss: 1.334388  [   16/ 3200]\n",
      "loss: 1.274172  [   32/ 3200]\n",
      "loss: 1.247793  [   48/ 3200]\n",
      "loss: 1.319327  [   64/ 3200]\n",
      "loss: 1.345493  [   80/ 3200]\n",
      "loss: 1.287917  [   96/ 3200]\n",
      "loss: 1.329072  [  112/ 3200]\n",
      "loss: 1.354397  [  128/ 3200]\n",
      "loss: 1.345115  [  144/ 3200]\n",
      "loss: 1.357749  [  160/ 3200]\n",
      "loss: 1.287996  [  176/ 3200]\n",
      "loss: 1.337341  [  192/ 3200]\n",
      "loss: 1.269778  [  208/ 3200]\n",
      "loss: 1.338666  [  224/ 3200]\n",
      "loss: 1.297493  [  240/ 3200]\n",
      "loss: 1.327601  [  256/ 3200]\n",
      "loss: 1.296978  [  272/ 3200]\n",
      "loss: 1.307454  [  288/ 3200]\n",
      "loss: 1.319602  [  304/ 3200]\n",
      "loss: 1.324621  [  320/ 3200]\n",
      "loss: 1.354258  [  336/ 3200]\n",
      "loss: 1.345856  [  352/ 3200]\n",
      "loss: 1.341024  [  368/ 3200]\n",
      "loss: 1.255485  [  384/ 3200]\n",
      "loss: 1.297577  [  400/ 3200]\n",
      "loss: 1.309956  [  416/ 3200]\n",
      "loss: 1.324994  [  432/ 3200]\n",
      "loss: 1.331866  [  448/ 3200]\n",
      "loss: 1.280628  [  464/ 3200]\n",
      "loss: 1.254205  [  480/ 3200]\n",
      "loss: 1.324978  [  496/ 3200]\n",
      "loss: 1.251959  [  512/ 3200]\n",
      "loss: 1.287988  [  528/ 3200]\n",
      "loss: 1.319052  [  544/ 3200]\n",
      "loss: 1.224331  [  560/ 3200]\n",
      "loss: 1.346585  [  576/ 3200]\n",
      "loss: 1.263103  [  592/ 3200]\n",
      "loss: 1.360698  [  608/ 3200]\n",
      "loss: 1.361793  [  624/ 3200]\n",
      "loss: 1.250238  [  640/ 3200]\n",
      "loss: 1.342113  [  656/ 3200]\n",
      "loss: 1.382313  [  672/ 3200]\n",
      "loss: 1.278035  [  688/ 3200]\n",
      "loss: 1.270082  [  704/ 3200]\n",
      "loss: 1.335533  [  720/ 3200]\n",
      "loss: 1.283318  [  736/ 3200]\n",
      "loss: 1.338589  [  752/ 3200]\n",
      "loss: 1.326600  [  768/ 3200]\n",
      "loss: 1.327892  [  784/ 3200]\n",
      "loss: 1.284450  [  800/ 3200]\n",
      "loss: 1.280996  [  816/ 3200]\n",
      "loss: 1.289450  [  832/ 3200]\n",
      "loss: 1.312092  [  848/ 3200]\n",
      "loss: 1.304082  [  864/ 3200]\n",
      "loss: 1.288112  [  880/ 3200]\n",
      "loss: 1.352013  [  896/ 3200]\n",
      "loss: 1.355477  [  912/ 3200]\n",
      "loss: 1.231087  [  928/ 3200]\n",
      "loss: 1.332079  [  944/ 3200]\n",
      "loss: 1.325858  [  960/ 3200]\n",
      "loss: 1.305550  [  976/ 3200]\n",
      "loss: 1.335979  [  992/ 3200]\n",
      "loss: 1.291762  [ 1008/ 3200]\n",
      "loss: 1.290291  [ 1024/ 3200]\n",
      "loss: 1.304660  [ 1040/ 3200]\n",
      "loss: 1.304494  [ 1056/ 3200]\n",
      "loss: 1.287738  [ 1072/ 3200]\n",
      "loss: 1.280597  [ 1088/ 3200]\n",
      "loss: 1.321893  [ 1104/ 3200]\n",
      "loss: 1.270226  [ 1120/ 3200]\n",
      "loss: 1.344716  [ 1136/ 3200]\n",
      "loss: 1.296776  [ 1152/ 3200]\n",
      "loss: 1.351180  [ 1168/ 3200]\n",
      "loss: 1.290159  [ 1184/ 3200]\n",
      "loss: 1.319356  [ 1200/ 3200]\n",
      "loss: 1.322137  [ 1216/ 3200]\n",
      "loss: 1.284943  [ 1232/ 3200]\n",
      "loss: 1.297791  [ 1248/ 3200]\n",
      "loss: 1.236513  [ 1264/ 3200]\n",
      "loss: 1.338218  [ 1280/ 3200]\n",
      "loss: 1.327179  [ 1296/ 3200]\n",
      "loss: 1.323486  [ 1312/ 3200]\n",
      "loss: 1.292780  [ 1328/ 3200]\n",
      "loss: 1.296528  [ 1344/ 3200]\n",
      "loss: 1.359252  [ 1360/ 3200]\n",
      "loss: 1.296656  [ 1376/ 3200]\n",
      "loss: 1.340912  [ 1392/ 3200]\n",
      "loss: 1.319949  [ 1408/ 3200]\n",
      "loss: 1.312052  [ 1424/ 3200]\n",
      "loss: 1.320342  [ 1440/ 3200]\n",
      "loss: 1.322746  [ 1456/ 3200]\n",
      "loss: 1.321888  [ 1472/ 3200]\n",
      "loss: 1.317477  [ 1488/ 3200]\n",
      "loss: 1.277520  [ 1504/ 3200]\n",
      "loss: 1.357292  [ 1520/ 3200]\n",
      "loss: 1.320018  [ 1536/ 3200]\n",
      "loss: 1.305156  [ 1552/ 3200]\n",
      "loss: 1.296900  [ 1568/ 3200]\n",
      "loss: 1.270643  [ 1584/ 3200]\n",
      "loss: 1.329482  [ 1600/ 3200]\n",
      "loss: 1.320388  [ 1616/ 3200]\n",
      "loss: 1.281757  [ 1632/ 3200]\n",
      "loss: 1.355408  [ 1648/ 3200]\n",
      "loss: 1.279283  [ 1664/ 3200]\n",
      "loss: 1.309811  [ 1680/ 3200]\n",
      "loss: 1.318715  [ 1696/ 3200]\n",
      "loss: 1.293322  [ 1712/ 3200]\n",
      "loss: 1.305500  [ 1728/ 3200]\n",
      "loss: 1.327609  [ 1744/ 3200]\n",
      "loss: 1.277414  [ 1760/ 3200]\n",
      "loss: 1.325099  [ 1776/ 3200]\n",
      "loss: 1.275659  [ 1792/ 3200]\n",
      "loss: 1.241041  [ 1808/ 3200]\n",
      "loss: 1.279953  [ 1824/ 3200]\n",
      "loss: 1.294618  [ 1840/ 3200]\n",
      "loss: 1.321004  [ 1856/ 3200]\n",
      "loss: 1.288183  [ 1872/ 3200]\n",
      "loss: 1.287747  [ 1888/ 3200]\n",
      "loss: 1.317317  [ 1904/ 3200]\n",
      "loss: 1.299389  [ 1920/ 3200]\n",
      "loss: 1.271548  [ 1936/ 3200]\n",
      "loss: 1.313993  [ 1952/ 3200]\n",
      "loss: 1.256942  [ 1968/ 3200]\n",
      "loss: 1.250269  [ 1984/ 3200]\n",
      "loss: 1.256307  [ 2000/ 3200]\n",
      "loss: 1.242919  [ 2016/ 3200]\n",
      "loss: 1.332754  [ 2032/ 3200]\n",
      "loss: 1.263362  [ 2048/ 3200]\n",
      "loss: 1.348009  [ 2064/ 3200]\n",
      "loss: 1.296248  [ 2080/ 3200]\n",
      "loss: 1.325369  [ 2096/ 3200]\n",
      "loss: 1.320708  [ 2112/ 3200]\n",
      "loss: 1.308514  [ 2128/ 3200]\n",
      "loss: 1.252004  [ 2144/ 3200]\n",
      "loss: 1.314552  [ 2160/ 3200]\n",
      "loss: 1.314128  [ 2176/ 3200]\n",
      "loss: 1.227599  [ 2192/ 3200]\n",
      "loss: 1.231429  [ 2208/ 3200]\n",
      "loss: 1.398919  [ 2224/ 3200]\n",
      "loss: 1.341608  [ 2240/ 3200]\n",
      "loss: 1.377170  [ 2256/ 3200]\n",
      "loss: 1.352175  [ 2272/ 3200]\n",
      "loss: 1.264891  [ 2288/ 3200]\n",
      "loss: 1.335027  [ 2304/ 3200]\n",
      "loss: 1.242092  [ 2320/ 3200]\n",
      "loss: 1.348291  [ 2336/ 3200]\n",
      "loss: 1.307988  [ 2352/ 3200]\n",
      "loss: 1.310398  [ 2368/ 3200]\n",
      "loss: 1.278968  [ 2384/ 3200]\n",
      "loss: 1.336624  [ 2400/ 3200]\n",
      "loss: 1.294045  [ 2416/ 3200]\n",
      "loss: 1.275814  [ 2432/ 3200]\n",
      "loss: 1.282119  [ 2448/ 3200]\n",
      "loss: 1.265865  [ 2464/ 3200]\n",
      "loss: 1.307240  [ 2480/ 3200]\n",
      "loss: 1.232959  [ 2496/ 3200]\n",
      "loss: 1.326717  [ 2512/ 3200]\n",
      "loss: 1.312218  [ 2528/ 3200]\n",
      "loss: 1.367743  [ 2544/ 3200]\n",
      "loss: 1.269022  [ 2560/ 3200]\n",
      "loss: 1.305603  [ 2576/ 3200]\n",
      "loss: 1.290966  [ 2592/ 3200]\n",
      "loss: 1.277514  [ 2608/ 3200]\n",
      "loss: 1.253167  [ 2624/ 3200]\n",
      "loss: 1.263499  [ 2640/ 3200]\n",
      "loss: 1.346406  [ 2656/ 3200]\n",
      "loss: 1.315892  [ 2672/ 3200]\n",
      "loss: 1.295385  [ 2688/ 3200]\n",
      "loss: 1.319572  [ 2704/ 3200]\n",
      "loss: 1.350781  [ 2720/ 3200]\n",
      "loss: 1.290334  [ 2736/ 3200]\n",
      "loss: 1.333909  [ 2752/ 3200]\n",
      "loss: 1.329188  [ 2768/ 3200]\n",
      "loss: 1.302623  [ 2784/ 3200]\n",
      "loss: 1.299978  [ 2800/ 3200]\n",
      "loss: 1.294493  [ 2816/ 3200]\n",
      "loss: 1.299011  [ 2832/ 3200]\n",
      "loss: 1.334251  [ 2848/ 3200]\n",
      "loss: 1.303731  [ 2864/ 3200]\n",
      "loss: 1.274693  [ 2880/ 3200]\n",
      "loss: 1.282901  [ 2896/ 3200]\n",
      "loss: 1.279825  [ 2912/ 3200]\n",
      "loss: 1.279034  [ 2928/ 3200]\n",
      "loss: 1.274517  [ 2944/ 3200]\n",
      "loss: 1.266808  [ 2960/ 3200]\n",
      "loss: 1.337034  [ 2976/ 3200]\n",
      "loss: 1.341775  [ 2992/ 3200]\n",
      "loss: 1.302862  [ 3008/ 3200]\n",
      "loss: 1.250951  [ 3024/ 3200]\n",
      "loss: 1.336283  [ 3040/ 3200]\n",
      "loss: 1.261976  [ 3056/ 3200]\n",
      "loss: 1.302679  [ 3072/ 3200]\n",
      "loss: 1.260455  [ 3088/ 3200]\n",
      "loss: 1.272794  [ 3104/ 3200]\n",
      "loss: 1.285974  [ 3120/ 3200]\n",
      "loss: 1.287634  [ 3136/ 3200]\n",
      "loss: 1.251942  [ 3152/ 3200]\n",
      "loss: 1.264763  [ 3168/ 3200]\n",
      "loss: 1.348875  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 1.271632  [    0/ 3200]\n",
      "loss: 1.216491  [   16/ 3200]\n",
      "loss: 1.229899  [   32/ 3200]\n",
      "loss: 1.397853  [   48/ 3200]\n",
      "loss: 1.252614  [   64/ 3200]\n",
      "loss: 1.375798  [   80/ 3200]\n",
      "loss: 1.311264  [   96/ 3200]\n",
      "loss: 1.250779  [  112/ 3200]\n",
      "loss: 1.262426  [  128/ 3200]\n",
      "loss: 1.318551  [  144/ 3200]\n",
      "loss: 1.241276  [  160/ 3200]\n",
      "loss: 1.317789  [  176/ 3200]\n",
      "loss: 1.318867  [  192/ 3200]\n",
      "loss: 1.265631  [  208/ 3200]\n",
      "loss: 1.232985  [  224/ 3200]\n",
      "loss: 1.236084  [  240/ 3200]\n",
      "loss: 1.375315  [  256/ 3200]\n",
      "loss: 1.317907  [  272/ 3200]\n",
      "loss: 1.293484  [  288/ 3200]\n",
      "loss: 1.316029  [  304/ 3200]\n",
      "loss: 1.279127  [  320/ 3200]\n",
      "loss: 1.279595  [  336/ 3200]\n",
      "loss: 1.255741  [  352/ 3200]\n",
      "loss: 1.347257  [  368/ 3200]\n",
      "loss: 1.306675  [  384/ 3200]\n",
      "loss: 1.322759  [  400/ 3200]\n",
      "loss: 1.253151  [  416/ 3200]\n",
      "loss: 1.290063  [  432/ 3200]\n",
      "loss: 1.300488  [  448/ 3200]\n",
      "loss: 1.343771  [  464/ 3200]\n",
      "loss: 1.262523  [  480/ 3200]\n",
      "loss: 1.236418  [  496/ 3200]\n",
      "loss: 1.416708  [  512/ 3200]\n",
      "loss: 1.273780  [  528/ 3200]\n",
      "loss: 1.291716  [  544/ 3200]\n",
      "loss: 1.301553  [  560/ 3200]\n",
      "loss: 1.211235  [  576/ 3200]\n",
      "loss: 1.289157  [  592/ 3200]\n",
      "loss: 1.319788  [  608/ 3200]\n",
      "loss: 1.309122  [  624/ 3200]\n",
      "loss: 1.245818  [  640/ 3200]\n",
      "loss: 1.204545  [  656/ 3200]\n",
      "loss: 1.338239  [  672/ 3200]\n",
      "loss: 1.352576  [  688/ 3200]\n",
      "loss: 1.308181  [  704/ 3200]\n",
      "loss: 1.259602  [  720/ 3200]\n",
      "loss: 1.283963  [  736/ 3200]\n",
      "loss: 1.261068  [  752/ 3200]\n",
      "loss: 1.268020  [  768/ 3200]\n",
      "loss: 1.364226  [  784/ 3200]\n",
      "loss: 1.339293  [  800/ 3200]\n",
      "loss: 1.307931  [  816/ 3200]\n",
      "loss: 1.322992  [  832/ 3200]\n",
      "loss: 1.315800  [  848/ 3200]\n",
      "loss: 1.330311  [  864/ 3200]\n",
      "loss: 1.268807  [  880/ 3200]\n",
      "loss: 1.296882  [  896/ 3200]\n",
      "loss: 1.303200  [  912/ 3200]\n",
      "loss: 1.233055  [  928/ 3200]\n",
      "loss: 1.386485  [  944/ 3200]\n",
      "loss: 1.209934  [  960/ 3200]\n",
      "loss: 1.277572  [  976/ 3200]\n",
      "loss: 1.246015  [  992/ 3200]\n",
      "loss: 1.348429  [ 1008/ 3200]\n",
      "loss: 1.327097  [ 1024/ 3200]\n",
      "loss: 1.315070  [ 1040/ 3200]\n",
      "loss: 1.245900  [ 1056/ 3200]\n",
      "loss: 1.344044  [ 1072/ 3200]\n",
      "loss: 1.297420  [ 1088/ 3200]\n",
      "loss: 1.283797  [ 1104/ 3200]\n",
      "loss: 1.393590  [ 1120/ 3200]\n",
      "loss: 1.239352  [ 1136/ 3200]\n",
      "loss: 1.344010  [ 1152/ 3200]\n",
      "loss: 1.315937  [ 1168/ 3200]\n",
      "loss: 1.269681  [ 1184/ 3200]\n",
      "loss: 1.321598  [ 1200/ 3200]\n",
      "loss: 1.225240  [ 1216/ 3200]\n",
      "loss: 1.293553  [ 1232/ 3200]\n",
      "loss: 1.298191  [ 1248/ 3200]\n",
      "loss: 1.318051  [ 1264/ 3200]\n",
      "loss: 1.286210  [ 1280/ 3200]\n",
      "loss: 1.347324  [ 1296/ 3200]\n",
      "loss: 1.234817  [ 1312/ 3200]\n",
      "loss: 1.294517  [ 1328/ 3200]\n",
      "loss: 1.318726  [ 1344/ 3200]\n",
      "loss: 1.325040  [ 1360/ 3200]\n",
      "loss: 1.322209  [ 1376/ 3200]\n",
      "loss: 1.294471  [ 1392/ 3200]\n",
      "loss: 1.351460  [ 1408/ 3200]\n",
      "loss: 1.275956  [ 1424/ 3200]\n",
      "loss: 1.263942  [ 1440/ 3200]\n",
      "loss: 1.269787  [ 1456/ 3200]\n",
      "loss: 1.317733  [ 1472/ 3200]\n",
      "loss: 1.294212  [ 1488/ 3200]\n",
      "loss: 1.362069  [ 1504/ 3200]\n",
      "loss: 1.325102  [ 1520/ 3200]\n",
      "loss: 1.288767  [ 1536/ 3200]\n",
      "loss: 1.238409  [ 1552/ 3200]\n",
      "loss: 1.264620  [ 1568/ 3200]\n",
      "loss: 1.287100  [ 1584/ 3200]\n",
      "loss: 1.226024  [ 1600/ 3200]\n",
      "loss: 1.282960  [ 1616/ 3200]\n",
      "loss: 1.342487  [ 1632/ 3200]\n",
      "loss: 1.194373  [ 1648/ 3200]\n",
      "loss: 1.284872  [ 1664/ 3200]\n",
      "loss: 1.253214  [ 1680/ 3200]\n",
      "loss: 1.215366  [ 1696/ 3200]\n",
      "loss: 1.321384  [ 1712/ 3200]\n",
      "loss: 1.248116  [ 1728/ 3200]\n",
      "loss: 1.349001  [ 1744/ 3200]\n",
      "loss: 1.311128  [ 1760/ 3200]\n",
      "loss: 1.229672  [ 1776/ 3200]\n",
      "loss: 1.255908  [ 1792/ 3200]\n",
      "loss: 1.203786  [ 1808/ 3200]\n",
      "loss: 1.263970  [ 1824/ 3200]\n",
      "loss: 1.333760  [ 1840/ 3200]\n",
      "loss: 1.265332  [ 1856/ 3200]\n",
      "loss: 1.223269  [ 1872/ 3200]\n",
      "loss: 1.313453  [ 1888/ 3200]\n",
      "loss: 1.346157  [ 1904/ 3200]\n",
      "loss: 1.291532  [ 1920/ 3200]\n",
      "loss: 1.301798  [ 1936/ 3200]\n",
      "loss: 1.264714  [ 1952/ 3200]\n",
      "loss: 1.322092  [ 1968/ 3200]\n",
      "loss: 1.356477  [ 1984/ 3200]\n",
      "loss: 1.262074  [ 2000/ 3200]\n",
      "loss: 1.272524  [ 2016/ 3200]\n",
      "loss: 1.283865  [ 2032/ 3200]\n",
      "loss: 1.231126  [ 2048/ 3200]\n",
      "loss: 1.325732  [ 2064/ 3200]\n",
      "loss: 1.342954  [ 2080/ 3200]\n",
      "loss: 1.322851  [ 2096/ 3200]\n",
      "loss: 1.302135  [ 2112/ 3200]\n",
      "loss: 1.298472  [ 2128/ 3200]\n",
      "loss: 1.282899  [ 2144/ 3200]\n",
      "loss: 1.209718  [ 2160/ 3200]\n",
      "loss: 1.241665  [ 2176/ 3200]\n",
      "loss: 1.295289  [ 2192/ 3200]\n",
      "loss: 1.261916  [ 2208/ 3200]\n",
      "loss: 1.263080  [ 2224/ 3200]\n",
      "loss: 1.310979  [ 2240/ 3200]\n",
      "loss: 1.247479  [ 2256/ 3200]\n",
      "loss: 1.296023  [ 2272/ 3200]\n",
      "loss: 1.271641  [ 2288/ 3200]\n",
      "loss: 1.267713  [ 2304/ 3200]\n",
      "loss: 1.300445  [ 2320/ 3200]\n",
      "loss: 1.232177  [ 2336/ 3200]\n",
      "loss: 1.384499  [ 2352/ 3200]\n",
      "loss: 1.334000  [ 2368/ 3200]\n",
      "loss: 1.304694  [ 2384/ 3200]\n",
      "loss: 1.272130  [ 2400/ 3200]\n",
      "loss: 1.305014  [ 2416/ 3200]\n",
      "loss: 1.304831  [ 2432/ 3200]\n",
      "loss: 1.345359  [ 2448/ 3200]\n",
      "loss: 1.281678  [ 2464/ 3200]\n",
      "loss: 1.278086  [ 2480/ 3200]\n",
      "loss: 1.281518  [ 2496/ 3200]\n",
      "loss: 1.310874  [ 2512/ 3200]\n",
      "loss: 1.355706  [ 2528/ 3200]\n",
      "loss: 1.284520  [ 2544/ 3200]\n",
      "loss: 1.205546  [ 2560/ 3200]\n",
      "loss: 1.344290  [ 2576/ 3200]\n",
      "loss: 1.279180  [ 2592/ 3200]\n",
      "loss: 1.316198  [ 2608/ 3200]\n",
      "loss: 1.257184  [ 2624/ 3200]\n",
      "loss: 1.349498  [ 2640/ 3200]\n",
      "loss: 1.278628  [ 2656/ 3200]\n",
      "loss: 1.256118  [ 2672/ 3200]\n",
      "loss: 1.225728  [ 2688/ 3200]\n",
      "loss: 1.334181  [ 2704/ 3200]\n",
      "loss: 1.291377  [ 2720/ 3200]\n",
      "loss: 1.309897  [ 2736/ 3200]\n",
      "loss: 1.260306  [ 2752/ 3200]\n",
      "loss: 1.252537  [ 2768/ 3200]\n",
      "loss: 1.268942  [ 2784/ 3200]\n",
      "loss: 1.266692  [ 2800/ 3200]\n",
      "loss: 1.348092  [ 2816/ 3200]\n",
      "loss: 1.306657  [ 2832/ 3200]\n",
      "loss: 1.383469  [ 2848/ 3200]\n",
      "loss: 1.246738  [ 2864/ 3200]\n",
      "loss: 1.235834  [ 2880/ 3200]\n",
      "loss: 1.322214  [ 2896/ 3200]\n",
      "loss: 1.248462  [ 2912/ 3200]\n",
      "loss: 1.289265  [ 2928/ 3200]\n",
      "loss: 1.330891  [ 2944/ 3200]\n",
      "loss: 1.284321  [ 2960/ 3200]\n",
      "loss: 1.296969  [ 2976/ 3200]\n",
      "loss: 1.311987  [ 2992/ 3200]\n",
      "loss: 1.324342  [ 3008/ 3200]\n",
      "loss: 1.271242  [ 3024/ 3200]\n",
      "loss: 1.321830  [ 3040/ 3200]\n",
      "loss: 1.279340  [ 3056/ 3200]\n",
      "loss: 1.187957  [ 3072/ 3200]\n",
      "loss: 1.331534  [ 3088/ 3200]\n",
      "loss: 1.318480  [ 3104/ 3200]\n",
      "loss: 1.256015  [ 3120/ 3200]\n",
      "loss: 1.318540  [ 3136/ 3200]\n",
      "loss: 1.340371  [ 3152/ 3200]\n",
      "loss: 1.299834  [ 3168/ 3200]\n",
      "loss: 1.188283  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 1.317186  [    0/ 3200]\n",
      "loss: 1.322269  [   16/ 3200]\n",
      "loss: 1.273413  [   32/ 3200]\n",
      "loss: 1.299977  [   48/ 3200]\n",
      "loss: 1.293541  [   64/ 3200]\n",
      "loss: 1.340420  [   80/ 3200]\n",
      "loss: 1.267276  [   96/ 3200]\n",
      "loss: 1.272899  [  112/ 3200]\n",
      "loss: 1.315672  [  128/ 3200]\n",
      "loss: 1.245269  [  144/ 3200]\n",
      "loss: 1.250408  [  160/ 3200]\n",
      "loss: 1.233875  [  176/ 3200]\n",
      "loss: 1.195748  [  192/ 3200]\n",
      "loss: 1.188439  [  208/ 3200]\n",
      "loss: 1.308879  [  224/ 3200]\n",
      "loss: 1.312924  [  240/ 3200]\n",
      "loss: 1.253435  [  256/ 3200]\n",
      "loss: 1.269605  [  272/ 3200]\n",
      "loss: 1.353094  [  288/ 3200]\n",
      "loss: 1.264207  [  304/ 3200]\n",
      "loss: 1.273607  [  320/ 3200]\n",
      "loss: 1.276061  [  336/ 3200]\n",
      "loss: 1.243087  [  352/ 3200]\n",
      "loss: 1.279613  [  368/ 3200]\n",
      "loss: 1.299082  [  384/ 3200]\n",
      "loss: 1.250880  [  400/ 3200]\n",
      "loss: 1.278161  [  416/ 3200]\n",
      "loss: 1.249560  [  432/ 3200]\n",
      "loss: 1.250703  [  448/ 3200]\n",
      "loss: 1.287025  [  464/ 3200]\n",
      "loss: 1.318031  [  480/ 3200]\n",
      "loss: 1.272949  [  496/ 3200]\n",
      "loss: 1.295690  [  512/ 3200]\n",
      "loss: 1.295931  [  528/ 3200]\n",
      "loss: 1.299987  [  544/ 3200]\n",
      "loss: 1.270719  [  560/ 3200]\n",
      "loss: 1.239095  [  576/ 3200]\n",
      "loss: 1.283772  [  592/ 3200]\n",
      "loss: 1.340203  [  608/ 3200]\n",
      "loss: 1.270595  [  624/ 3200]\n",
      "loss: 1.251769  [  640/ 3200]\n",
      "loss: 1.277372  [  656/ 3200]\n",
      "loss: 1.253074  [  672/ 3200]\n",
      "loss: 1.335197  [  688/ 3200]\n",
      "loss: 1.239918  [  704/ 3200]\n",
      "loss: 1.299021  [  720/ 3200]\n",
      "loss: 1.253480  [  736/ 3200]\n",
      "loss: 1.234375  [  752/ 3200]\n",
      "loss: 1.246747  [  768/ 3200]\n",
      "loss: 1.359332  [  784/ 3200]\n",
      "loss: 1.300993  [  800/ 3200]\n",
      "loss: 1.341055  [  816/ 3200]\n",
      "loss: 1.256843  [  832/ 3200]\n",
      "loss: 1.274660  [  848/ 3200]\n",
      "loss: 1.235132  [  864/ 3200]\n",
      "loss: 1.218143  [  880/ 3200]\n",
      "loss: 1.256308  [  896/ 3200]\n",
      "loss: 1.204334  [  912/ 3200]\n",
      "loss: 1.265472  [  928/ 3200]\n",
      "loss: 1.296950  [  944/ 3200]\n",
      "loss: 1.253122  [  960/ 3200]\n",
      "loss: 1.327135  [  976/ 3200]\n",
      "loss: 1.320735  [  992/ 3200]\n",
      "loss: 1.324933  [ 1008/ 3200]\n",
      "loss: 1.274895  [ 1024/ 3200]\n",
      "loss: 1.242936  [ 1040/ 3200]\n",
      "loss: 1.332202  [ 1056/ 3200]\n",
      "loss: 1.261106  [ 1072/ 3200]\n",
      "loss: 1.351960  [ 1088/ 3200]\n",
      "loss: 1.257575  [ 1104/ 3200]\n",
      "loss: 1.275375  [ 1120/ 3200]\n",
      "loss: 1.304025  [ 1136/ 3200]\n",
      "loss: 1.258386  [ 1152/ 3200]\n",
      "loss: 1.220410  [ 1168/ 3200]\n",
      "loss: 1.226578  [ 1184/ 3200]\n",
      "loss: 1.270374  [ 1200/ 3200]\n",
      "loss: 1.379185  [ 1216/ 3200]\n",
      "loss: 1.339145  [ 1232/ 3200]\n",
      "loss: 1.272572  [ 1248/ 3200]\n",
      "loss: 1.202165  [ 1264/ 3200]\n",
      "loss: 1.326167  [ 1280/ 3200]\n",
      "loss: 1.274016  [ 1296/ 3200]\n",
      "loss: 1.310319  [ 1312/ 3200]\n",
      "loss: 1.245385  [ 1328/ 3200]\n",
      "loss: 1.218840  [ 1344/ 3200]\n",
      "loss: 1.160089  [ 1360/ 3200]\n",
      "loss: 1.321748  [ 1376/ 3200]\n",
      "loss: 1.340334  [ 1392/ 3200]\n",
      "loss: 1.290500  [ 1408/ 3200]\n",
      "loss: 1.224025  [ 1424/ 3200]\n",
      "loss: 1.297245  [ 1440/ 3200]\n",
      "loss: 1.300325  [ 1456/ 3200]\n",
      "loss: 1.339214  [ 1472/ 3200]\n",
      "loss: 1.323236  [ 1488/ 3200]\n",
      "loss: 1.261364  [ 1504/ 3200]\n",
      "loss: 1.274472  [ 1520/ 3200]\n",
      "loss: 1.250884  [ 1536/ 3200]\n",
      "loss: 1.234632  [ 1552/ 3200]\n",
      "loss: 1.220378  [ 1568/ 3200]\n",
      "loss: 1.400717  [ 1584/ 3200]\n",
      "loss: 1.238181  [ 1600/ 3200]\n",
      "loss: 1.230654  [ 1616/ 3200]\n",
      "loss: 1.197392  [ 1632/ 3200]\n",
      "loss: 1.354343  [ 1648/ 3200]\n",
      "loss: 1.249312  [ 1664/ 3200]\n",
      "loss: 1.270257  [ 1680/ 3200]\n",
      "loss: 1.350589  [ 1696/ 3200]\n",
      "loss: 1.279055  [ 1712/ 3200]\n",
      "loss: 1.293890  [ 1728/ 3200]\n",
      "loss: 1.263945  [ 1744/ 3200]\n",
      "loss: 1.242194  [ 1760/ 3200]\n",
      "loss: 1.379970  [ 1776/ 3200]\n",
      "loss: 1.242993  [ 1792/ 3200]\n",
      "loss: 1.296603  [ 1808/ 3200]\n",
      "loss: 1.249379  [ 1824/ 3200]\n",
      "loss: 1.211076  [ 1840/ 3200]\n",
      "loss: 1.225151  [ 1856/ 3200]\n",
      "loss: 1.277594  [ 1872/ 3200]\n",
      "loss: 1.295309  [ 1888/ 3200]\n",
      "loss: 1.302492  [ 1904/ 3200]\n",
      "loss: 1.231024  [ 1920/ 3200]\n",
      "loss: 1.311583  [ 1936/ 3200]\n",
      "loss: 1.221522  [ 1952/ 3200]\n",
      "loss: 1.263437  [ 1968/ 3200]\n",
      "loss: 1.310100  [ 1984/ 3200]\n",
      "loss: 1.310251  [ 2000/ 3200]\n",
      "loss: 1.206040  [ 2016/ 3200]\n",
      "loss: 1.237728  [ 2032/ 3200]\n",
      "loss: 1.233099  [ 2048/ 3200]\n",
      "loss: 1.354351  [ 2064/ 3200]\n",
      "loss: 1.267745  [ 2080/ 3200]\n",
      "loss: 1.333760  [ 2096/ 3200]\n",
      "loss: 1.288791  [ 2112/ 3200]\n",
      "loss: 1.262604  [ 2128/ 3200]\n",
      "loss: 1.307223  [ 2144/ 3200]\n",
      "loss: 1.376233  [ 2160/ 3200]\n",
      "loss: 1.263705  [ 2176/ 3200]\n",
      "loss: 1.300747  [ 2192/ 3200]\n",
      "loss: 1.268894  [ 2208/ 3200]\n",
      "loss: 1.237076  [ 2224/ 3200]\n",
      "loss: 1.253688  [ 2240/ 3200]\n",
      "loss: 1.210288  [ 2256/ 3200]\n",
      "loss: 1.188196  [ 2272/ 3200]\n",
      "loss: 1.260348  [ 2288/ 3200]\n",
      "loss: 1.272837  [ 2304/ 3200]\n",
      "loss: 1.263538  [ 2320/ 3200]\n",
      "loss: 1.265744  [ 2336/ 3200]\n",
      "loss: 1.211198  [ 2352/ 3200]\n",
      "loss: 1.278170  [ 2368/ 3200]\n",
      "loss: 1.262210  [ 2384/ 3200]\n",
      "loss: 1.252338  [ 2400/ 3200]\n",
      "loss: 1.207236  [ 2416/ 3200]\n",
      "loss: 1.168024  [ 2432/ 3200]\n",
      "loss: 1.329339  [ 2448/ 3200]\n",
      "loss: 1.161648  [ 2464/ 3200]\n",
      "loss: 1.329250  [ 2480/ 3200]\n",
      "loss: 1.351854  [ 2496/ 3200]\n",
      "loss: 1.261639  [ 2512/ 3200]\n",
      "loss: 1.254909  [ 2528/ 3200]\n",
      "loss: 1.226039  [ 2544/ 3200]\n",
      "loss: 1.379252  [ 2560/ 3200]\n",
      "loss: 1.308284  [ 2576/ 3200]\n",
      "loss: 1.265622  [ 2592/ 3200]\n",
      "loss: 1.403551  [ 2608/ 3200]\n",
      "loss: 1.308665  [ 2624/ 3200]\n",
      "loss: 1.241858  [ 2640/ 3200]\n",
      "loss: 1.211193  [ 2656/ 3200]\n",
      "loss: 1.187589  [ 2672/ 3200]\n",
      "loss: 1.212002  [ 2688/ 3200]\n",
      "loss: 1.268838  [ 2704/ 3200]\n",
      "loss: 1.276497  [ 2720/ 3200]\n",
      "loss: 1.294733  [ 2736/ 3200]\n",
      "loss: 1.204729  [ 2752/ 3200]\n",
      "loss: 1.273880  [ 2768/ 3200]\n",
      "loss: 1.236497  [ 2784/ 3200]\n",
      "loss: 1.357455  [ 2800/ 3200]\n",
      "loss: 1.280923  [ 2816/ 3200]\n",
      "loss: 1.287127  [ 2832/ 3200]\n",
      "loss: 1.218432  [ 2848/ 3200]\n",
      "loss: 1.245193  [ 2864/ 3200]\n",
      "loss: 1.289700  [ 2880/ 3200]\n",
      "loss: 1.235660  [ 2896/ 3200]\n",
      "loss: 1.331834  [ 2912/ 3200]\n",
      "loss: 1.271182  [ 2928/ 3200]\n",
      "loss: 1.322463  [ 2944/ 3200]\n",
      "loss: 1.297447  [ 2960/ 3200]\n",
      "loss: 1.270222  [ 2976/ 3200]\n",
      "loss: 1.280712  [ 2992/ 3200]\n",
      "loss: 1.227520  [ 3008/ 3200]\n",
      "loss: 1.197316  [ 3024/ 3200]\n",
      "loss: 1.286586  [ 3040/ 3200]\n",
      "loss: 1.297895  [ 3056/ 3200]\n",
      "loss: 1.341245  [ 3072/ 3200]\n",
      "loss: 1.251829  [ 3088/ 3200]\n",
      "loss: 1.261384  [ 3104/ 3200]\n",
      "loss: 1.276301  [ 3120/ 3200]\n",
      "loss: 1.305935  [ 3136/ 3200]\n",
      "loss: 1.361839  [ 3152/ 3200]\n",
      "loss: 1.346666  [ 3168/ 3200]\n",
      "loss: 1.297082  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 1.342069  [    0/ 3200]\n",
      "loss: 1.247185  [   16/ 3200]\n",
      "loss: 1.328101  [   32/ 3200]\n",
      "loss: 1.307643  [   48/ 3200]\n",
      "loss: 1.318391  [   64/ 3200]\n",
      "loss: 1.177368  [   80/ 3200]\n",
      "loss: 1.294857  [   96/ 3200]\n",
      "loss: 1.268519  [  112/ 3200]\n",
      "loss: 1.257316  [  128/ 3200]\n",
      "loss: 1.310903  [  144/ 3200]\n",
      "loss: 1.304714  [  160/ 3200]\n",
      "loss: 1.226709  [  176/ 3200]\n",
      "loss: 1.177236  [  192/ 3200]\n",
      "loss: 1.297493  [  208/ 3200]\n",
      "loss: 1.251244  [  224/ 3200]\n",
      "loss: 1.219835  [  240/ 3200]\n",
      "loss: 1.293767  [  256/ 3200]\n",
      "loss: 1.307227  [  272/ 3200]\n",
      "loss: 1.255650  [  288/ 3200]\n",
      "loss: 1.281100  [  304/ 3200]\n",
      "loss: 1.288033  [  320/ 3200]\n",
      "loss: 1.332516  [  336/ 3200]\n",
      "loss: 1.247621  [  352/ 3200]\n",
      "loss: 1.325825  [  368/ 3200]\n",
      "loss: 1.236765  [  384/ 3200]\n",
      "loss: 1.258821  [  400/ 3200]\n",
      "loss: 1.177822  [  416/ 3200]\n",
      "loss: 1.286659  [  432/ 3200]\n",
      "loss: 1.325792  [  448/ 3200]\n",
      "loss: 1.254236  [  464/ 3200]\n",
      "loss: 1.209713  [  480/ 3200]\n",
      "loss: 1.211189  [  496/ 3200]\n",
      "loss: 1.248056  [  512/ 3200]\n",
      "loss: 1.314404  [  528/ 3200]\n",
      "loss: 1.293328  [  544/ 3200]\n",
      "loss: 1.241147  [  560/ 3200]\n",
      "loss: 1.264321  [  576/ 3200]\n",
      "loss: 1.299184  [  592/ 3200]\n",
      "loss: 1.216578  [  608/ 3200]\n",
      "loss: 1.365815  [  624/ 3200]\n",
      "loss: 1.206483  [  640/ 3200]\n",
      "loss: 1.298938  [  656/ 3200]\n",
      "loss: 1.270734  [  672/ 3200]\n",
      "loss: 1.314997  [  688/ 3200]\n",
      "loss: 1.330103  [  704/ 3200]\n",
      "loss: 1.292233  [  720/ 3200]\n",
      "loss: 1.277825  [  736/ 3200]\n",
      "loss: 1.250451  [  752/ 3200]\n",
      "loss: 1.172897  [  768/ 3200]\n",
      "loss: 1.281286  [  784/ 3200]\n",
      "loss: 1.239689  [  800/ 3200]\n",
      "loss: 1.371945  [  816/ 3200]\n",
      "loss: 1.215420  [  832/ 3200]\n",
      "loss: 1.282200  [  848/ 3200]\n",
      "loss: 1.234780  [  864/ 3200]\n",
      "loss: 1.172802  [  880/ 3200]\n",
      "loss: 1.213814  [  896/ 3200]\n",
      "loss: 1.175653  [  912/ 3200]\n",
      "loss: 1.252029  [  928/ 3200]\n",
      "loss: 1.338810  [  944/ 3200]\n",
      "loss: 1.191354  [  960/ 3200]\n",
      "loss: 1.229372  [  976/ 3200]\n",
      "loss: 1.158471  [  992/ 3200]\n",
      "loss: 1.313363  [ 1008/ 3200]\n",
      "loss: 1.282515  [ 1024/ 3200]\n",
      "loss: 1.227704  [ 1040/ 3200]\n",
      "loss: 1.277502  [ 1056/ 3200]\n",
      "loss: 1.258660  [ 1072/ 3200]\n",
      "loss: 1.255974  [ 1088/ 3200]\n",
      "loss: 1.257303  [ 1104/ 3200]\n",
      "loss: 1.232178  [ 1120/ 3200]\n",
      "loss: 1.275895  [ 1136/ 3200]\n",
      "loss: 1.200903  [ 1152/ 3200]\n",
      "loss: 1.212438  [ 1168/ 3200]\n",
      "loss: 1.273500  [ 1184/ 3200]\n",
      "loss: 1.281387  [ 1200/ 3200]\n",
      "loss: 1.290893  [ 1216/ 3200]\n",
      "loss: 1.227492  [ 1232/ 3200]\n",
      "loss: 1.262510  [ 1248/ 3200]\n",
      "loss: 1.194970  [ 1264/ 3200]\n",
      "loss: 1.289756  [ 1280/ 3200]\n",
      "loss: 1.283272  [ 1296/ 3200]\n",
      "loss: 1.317047  [ 1312/ 3200]\n",
      "loss: 1.282421  [ 1328/ 3200]\n",
      "loss: 1.211808  [ 1344/ 3200]\n",
      "loss: 1.156774  [ 1360/ 3200]\n",
      "loss: 1.287914  [ 1376/ 3200]\n",
      "loss: 1.217758  [ 1392/ 3200]\n",
      "loss: 1.233230  [ 1408/ 3200]\n",
      "loss: 1.210356  [ 1424/ 3200]\n",
      "loss: 1.350400  [ 1440/ 3200]\n",
      "loss: 1.242633  [ 1456/ 3200]\n",
      "loss: 1.210027  [ 1472/ 3200]\n",
      "loss: 1.296913  [ 1488/ 3200]\n",
      "loss: 1.207786  [ 1504/ 3200]\n",
      "loss: 1.172829  [ 1520/ 3200]\n",
      "loss: 1.234437  [ 1536/ 3200]\n",
      "loss: 1.248333  [ 1552/ 3200]\n",
      "loss: 1.310053  [ 1568/ 3200]\n",
      "loss: 1.273967  [ 1584/ 3200]\n",
      "loss: 1.260585  [ 1600/ 3200]\n",
      "loss: 1.187471  [ 1616/ 3200]\n",
      "loss: 1.204071  [ 1632/ 3200]\n",
      "loss: 1.228799  [ 1648/ 3200]\n",
      "loss: 1.262025  [ 1664/ 3200]\n",
      "loss: 1.251368  [ 1680/ 3200]\n",
      "loss: 1.316596  [ 1696/ 3200]\n",
      "loss: 1.308715  [ 1712/ 3200]\n",
      "loss: 1.240312  [ 1728/ 3200]\n",
      "loss: 1.206687  [ 1744/ 3200]\n",
      "loss: 1.259288  [ 1760/ 3200]\n",
      "loss: 1.305388  [ 1776/ 3200]\n",
      "loss: 1.226587  [ 1792/ 3200]\n",
      "loss: 1.215759  [ 1808/ 3200]\n",
      "loss: 1.325655  [ 1824/ 3200]\n",
      "loss: 1.323968  [ 1840/ 3200]\n",
      "loss: 1.263455  [ 1856/ 3200]\n",
      "loss: 1.146992  [ 1872/ 3200]\n",
      "loss: 1.206651  [ 1888/ 3200]\n",
      "loss: 1.256279  [ 1904/ 3200]\n",
      "loss: 1.157029  [ 1920/ 3200]\n",
      "loss: 1.229629  [ 1936/ 3200]\n",
      "loss: 1.327493  [ 1952/ 3200]\n",
      "loss: 1.289154  [ 1968/ 3200]\n",
      "loss: 1.302789  [ 1984/ 3200]\n",
      "loss: 1.226570  [ 2000/ 3200]\n",
      "loss: 1.308233  [ 2016/ 3200]\n",
      "loss: 1.215674  [ 2032/ 3200]\n",
      "loss: 1.223076  [ 2048/ 3200]\n",
      "loss: 1.291911  [ 2064/ 3200]\n",
      "loss: 1.324237  [ 2080/ 3200]\n",
      "loss: 1.329613  [ 2096/ 3200]\n",
      "loss: 1.242949  [ 2112/ 3200]\n",
      "loss: 1.361617  [ 2128/ 3200]\n",
      "loss: 1.309639  [ 2144/ 3200]\n",
      "loss: 1.233975  [ 2160/ 3200]\n",
      "loss: 1.369637  [ 2176/ 3200]\n",
      "loss: 1.279466  [ 2192/ 3200]\n",
      "loss: 1.209883  [ 2208/ 3200]\n",
      "loss: 1.225051  [ 2224/ 3200]\n",
      "loss: 1.258026  [ 2240/ 3200]\n",
      "loss: 1.203020  [ 2256/ 3200]\n",
      "loss: 1.246311  [ 2272/ 3200]\n",
      "loss: 1.181585  [ 2288/ 3200]\n",
      "loss: 1.189260  [ 2304/ 3200]\n",
      "loss: 1.306249  [ 2320/ 3200]\n",
      "loss: 1.163443  [ 2336/ 3200]\n",
      "loss: 1.301875  [ 2352/ 3200]\n",
      "loss: 1.266560  [ 2368/ 3200]\n",
      "loss: 1.271680  [ 2384/ 3200]\n",
      "loss: 1.170825  [ 2400/ 3200]\n",
      "loss: 1.308505  [ 2416/ 3200]\n",
      "loss: 1.298532  [ 2432/ 3200]\n",
      "loss: 1.208546  [ 2448/ 3200]\n",
      "loss: 1.249213  [ 2464/ 3200]\n",
      "loss: 1.300637  [ 2480/ 3200]\n",
      "loss: 1.218001  [ 2496/ 3200]\n",
      "loss: 1.277380  [ 2512/ 3200]\n",
      "loss: 1.188934  [ 2528/ 3200]\n",
      "loss: 1.245407  [ 2544/ 3200]\n",
      "loss: 1.152676  [ 2560/ 3200]\n",
      "loss: 1.296483  [ 2576/ 3200]\n",
      "loss: 1.286103  [ 2592/ 3200]\n",
      "loss: 1.307643  [ 2608/ 3200]\n",
      "loss: 1.261570  [ 2624/ 3200]\n",
      "loss: 1.276175  [ 2640/ 3200]\n",
      "loss: 1.312851  [ 2656/ 3200]\n",
      "loss: 1.203052  [ 2672/ 3200]\n",
      "loss: 1.205329  [ 2688/ 3200]\n",
      "loss: 1.222685  [ 2704/ 3200]\n",
      "loss: 1.159078  [ 2720/ 3200]\n",
      "loss: 1.277946  [ 2736/ 3200]\n",
      "loss: 1.278809  [ 2752/ 3200]\n",
      "loss: 1.177990  [ 2768/ 3200]\n",
      "loss: 1.262467  [ 2784/ 3200]\n",
      "loss: 1.307096  [ 2800/ 3200]\n",
      "loss: 1.299040  [ 2816/ 3200]\n",
      "loss: 1.239997  [ 2832/ 3200]\n",
      "loss: 1.211708  [ 2848/ 3200]\n",
      "loss: 1.283952  [ 2864/ 3200]\n",
      "loss: 1.294330  [ 2880/ 3200]\n",
      "loss: 1.260211  [ 2896/ 3200]\n",
      "loss: 1.307300  [ 2912/ 3200]\n",
      "loss: 1.199339  [ 2928/ 3200]\n",
      "loss: 1.153340  [ 2944/ 3200]\n",
      "loss: 1.169894  [ 2960/ 3200]\n",
      "loss: 1.316736  [ 2976/ 3200]\n",
      "loss: 1.359985  [ 2992/ 3200]\n",
      "loss: 1.310426  [ 3008/ 3200]\n",
      "loss: 1.319262  [ 3024/ 3200]\n",
      "loss: 1.119334  [ 3040/ 3200]\n",
      "loss: 1.337577  [ 3056/ 3200]\n",
      "loss: 1.184060  [ 3072/ 3200]\n",
      "loss: 1.293154  [ 3088/ 3200]\n",
      "loss: 1.255432  [ 3104/ 3200]\n",
      "loss: 1.282891  [ 3120/ 3200]\n",
      "loss: 1.277301  [ 3136/ 3200]\n",
      "loss: 1.167938  [ 3152/ 3200]\n",
      "loss: 1.266595  [ 3168/ 3200]\n",
      "loss: 1.209330  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 1.287575  [    0/ 3200]\n",
      "loss: 1.227286  [   16/ 3200]\n",
      "loss: 1.297976  [   32/ 3200]\n",
      "loss: 1.186335  [   48/ 3200]\n",
      "loss: 1.280227  [   64/ 3200]\n",
      "loss: 1.195426  [   80/ 3200]\n",
      "loss: 1.210035  [   96/ 3200]\n",
      "loss: 1.248662  [  112/ 3200]\n",
      "loss: 1.117126  [  128/ 3200]\n",
      "loss: 1.231142  [  144/ 3200]\n",
      "loss: 1.227298  [  160/ 3200]\n",
      "loss: 1.175707  [  176/ 3200]\n",
      "loss: 1.231790  [  192/ 3200]\n",
      "loss: 1.186695  [  208/ 3200]\n",
      "loss: 1.247929  [  224/ 3200]\n",
      "loss: 1.133354  [  240/ 3200]\n",
      "loss: 1.380419  [  256/ 3200]\n",
      "loss: 1.137095  [  272/ 3200]\n",
      "loss: 1.270456  [  288/ 3200]\n",
      "loss: 1.304067  [  304/ 3200]\n",
      "loss: 1.274316  [  320/ 3200]\n",
      "loss: 1.164693  [  336/ 3200]\n",
      "loss: 1.268020  [  352/ 3200]\n",
      "loss: 1.212972  [  368/ 3200]\n",
      "loss: 1.365313  [  384/ 3200]\n",
      "loss: 1.204851  [  400/ 3200]\n",
      "loss: 1.310180  [  416/ 3200]\n",
      "loss: 1.222161  [  432/ 3200]\n",
      "loss: 1.220285  [  448/ 3200]\n",
      "loss: 1.228322  [  464/ 3200]\n",
      "loss: 1.274547  [  480/ 3200]\n",
      "loss: 1.278162  [  496/ 3200]\n",
      "loss: 1.208601  [  512/ 3200]\n",
      "loss: 1.346918  [  528/ 3200]\n",
      "loss: 1.174471  [  544/ 3200]\n",
      "loss: 1.301754  [  560/ 3200]\n",
      "loss: 1.292918  [  576/ 3200]\n",
      "loss: 1.276131  [  592/ 3200]\n",
      "loss: 1.243475  [  608/ 3200]\n",
      "loss: 1.206748  [  624/ 3200]\n",
      "loss: 1.195688  [  640/ 3200]\n",
      "loss: 1.231499  [  656/ 3200]\n",
      "loss: 1.347192  [  672/ 3200]\n",
      "loss: 1.225556  [  688/ 3200]\n",
      "loss: 1.284569  [  704/ 3200]\n",
      "loss: 1.340052  [  720/ 3200]\n",
      "loss: 1.258157  [  736/ 3200]\n",
      "loss: 1.335494  [  752/ 3200]\n",
      "loss: 1.243054  [  768/ 3200]\n",
      "loss: 1.180083  [  784/ 3200]\n",
      "loss: 1.181694  [  800/ 3200]\n",
      "loss: 1.223548  [  816/ 3200]\n",
      "loss: 1.226392  [  832/ 3200]\n",
      "loss: 1.215759  [  848/ 3200]\n",
      "loss: 1.226686  [  864/ 3200]\n",
      "loss: 1.202797  [  880/ 3200]\n",
      "loss: 1.206300  [  896/ 3200]\n",
      "loss: 1.250800  [  912/ 3200]\n",
      "loss: 1.251660  [  928/ 3200]\n",
      "loss: 1.213677  [  944/ 3200]\n",
      "loss: 1.233675  [  960/ 3200]\n",
      "loss: 1.198010  [  976/ 3200]\n",
      "loss: 1.243781  [  992/ 3200]\n",
      "loss: 1.262403  [ 1008/ 3200]\n",
      "loss: 1.239299  [ 1024/ 3200]\n",
      "loss: 1.211923  [ 1040/ 3200]\n",
      "loss: 1.150204  [ 1056/ 3200]\n",
      "loss: 1.252819  [ 1072/ 3200]\n",
      "loss: 1.240299  [ 1088/ 3200]\n",
      "loss: 1.194021  [ 1104/ 3200]\n",
      "loss: 1.216399  [ 1120/ 3200]\n",
      "loss: 1.261318  [ 1136/ 3200]\n",
      "loss: 1.333907  [ 1152/ 3200]\n",
      "loss: 1.195483  [ 1168/ 3200]\n",
      "loss: 1.198140  [ 1184/ 3200]\n",
      "loss: 1.289123  [ 1200/ 3200]\n",
      "loss: 1.313141  [ 1216/ 3200]\n",
      "loss: 1.231264  [ 1232/ 3200]\n",
      "loss: 1.299562  [ 1248/ 3200]\n",
      "loss: 1.251902  [ 1264/ 3200]\n",
      "loss: 1.214392  [ 1280/ 3200]\n",
      "loss: 1.308620  [ 1296/ 3200]\n",
      "loss: 1.248778  [ 1312/ 3200]\n",
      "loss: 1.205217  [ 1328/ 3200]\n",
      "loss: 1.220337  [ 1344/ 3200]\n",
      "loss: 1.298500  [ 1360/ 3200]\n",
      "loss: 1.216870  [ 1376/ 3200]\n",
      "loss: 1.115419  [ 1392/ 3200]\n",
      "loss: 1.218010  [ 1408/ 3200]\n",
      "loss: 1.231689  [ 1424/ 3200]\n",
      "loss: 1.235368  [ 1440/ 3200]\n",
      "loss: 1.172571  [ 1456/ 3200]\n",
      "loss: 1.237874  [ 1472/ 3200]\n",
      "loss: 1.216622  [ 1488/ 3200]\n",
      "loss: 1.126226  [ 1504/ 3200]\n",
      "loss: 1.274869  [ 1520/ 3200]\n",
      "loss: 1.282782  [ 1536/ 3200]\n",
      "loss: 1.177989  [ 1552/ 3200]\n",
      "loss: 1.280824  [ 1568/ 3200]\n",
      "loss: 1.260192  [ 1584/ 3200]\n",
      "loss: 1.252460  [ 1600/ 3200]\n",
      "loss: 1.217812  [ 1616/ 3200]\n",
      "loss: 1.297842  [ 1632/ 3200]\n",
      "loss: 1.179108  [ 1648/ 3200]\n",
      "loss: 1.159847  [ 1664/ 3200]\n",
      "loss: 1.230304  [ 1680/ 3200]\n",
      "loss: 1.333008  [ 1696/ 3200]\n",
      "loss: 1.182667  [ 1712/ 3200]\n",
      "loss: 1.222087  [ 1728/ 3200]\n",
      "loss: 1.303995  [ 1744/ 3200]\n",
      "loss: 1.193142  [ 1760/ 3200]\n",
      "loss: 1.272975  [ 1776/ 3200]\n",
      "loss: 1.226771  [ 1792/ 3200]\n",
      "loss: 1.278892  [ 1808/ 3200]\n",
      "loss: 1.192628  [ 1824/ 3200]\n",
      "loss: 1.242206  [ 1840/ 3200]\n",
      "loss: 1.231549  [ 1856/ 3200]\n",
      "loss: 1.370101  [ 1872/ 3200]\n",
      "loss: 1.203105  [ 1888/ 3200]\n",
      "loss: 1.174281  [ 1904/ 3200]\n",
      "loss: 1.141890  [ 1920/ 3200]\n",
      "loss: 1.281468  [ 1936/ 3200]\n",
      "loss: 1.171923  [ 1952/ 3200]\n",
      "loss: 1.224617  [ 1968/ 3200]\n",
      "loss: 1.300105  [ 1984/ 3200]\n",
      "loss: 1.228505  [ 2000/ 3200]\n",
      "loss: 1.312173  [ 2016/ 3200]\n",
      "loss: 1.248193  [ 2032/ 3200]\n",
      "loss: 1.387694  [ 2048/ 3200]\n",
      "loss: 1.202334  [ 2064/ 3200]\n",
      "loss: 1.236165  [ 2080/ 3200]\n",
      "loss: 1.184409  [ 2096/ 3200]\n",
      "loss: 1.262272  [ 2112/ 3200]\n",
      "loss: 1.174226  [ 2128/ 3200]\n",
      "loss: 1.179419  [ 2144/ 3200]\n",
      "loss: 1.162572  [ 2160/ 3200]\n",
      "loss: 1.091332  [ 2176/ 3200]\n",
      "loss: 1.044805  [ 2192/ 3200]\n",
      "loss: 1.249943  [ 2208/ 3200]\n",
      "loss: 1.276234  [ 2224/ 3200]\n",
      "loss: 1.247116  [ 2240/ 3200]\n",
      "loss: 1.264906  [ 2256/ 3200]\n",
      "loss: 1.294550  [ 2272/ 3200]\n",
      "loss: 1.122796  [ 2288/ 3200]\n",
      "loss: 1.251048  [ 2304/ 3200]\n",
      "loss: 1.315775  [ 2320/ 3200]\n",
      "loss: 1.236255  [ 2336/ 3200]\n",
      "loss: 1.187471  [ 2352/ 3200]\n",
      "loss: 1.179545  [ 2368/ 3200]\n",
      "loss: 1.221326  [ 2384/ 3200]\n",
      "loss: 1.256634  [ 2400/ 3200]\n",
      "loss: 1.203886  [ 2416/ 3200]\n",
      "loss: 1.191656  [ 2432/ 3200]\n",
      "loss: 1.220006  [ 2448/ 3200]\n",
      "loss: 1.181181  [ 2464/ 3200]\n",
      "loss: 1.297052  [ 2480/ 3200]\n",
      "loss: 1.221170  [ 2496/ 3200]\n",
      "loss: 1.171383  [ 2512/ 3200]\n",
      "loss: 1.214907  [ 2528/ 3200]\n",
      "loss: 1.245733  [ 2544/ 3200]\n",
      "loss: 1.339503  [ 2560/ 3200]\n",
      "loss: 1.150368  [ 2576/ 3200]\n",
      "loss: 1.300322  [ 2592/ 3200]\n",
      "loss: 1.130687  [ 2608/ 3200]\n",
      "loss: 1.371442  [ 2624/ 3200]\n",
      "loss: 1.286282  [ 2640/ 3200]\n",
      "loss: 1.254974  [ 2656/ 3200]\n",
      "loss: 1.255471  [ 2672/ 3200]\n",
      "loss: 1.230559  [ 2688/ 3200]\n",
      "loss: 1.214625  [ 2704/ 3200]\n",
      "loss: 1.202222  [ 2720/ 3200]\n",
      "loss: 1.308944  [ 2736/ 3200]\n",
      "loss: 1.299086  [ 2752/ 3200]\n",
      "loss: 1.146701  [ 2768/ 3200]\n",
      "loss: 1.215308  [ 2784/ 3200]\n",
      "loss: 1.208378  [ 2800/ 3200]\n",
      "loss: 1.197959  [ 2816/ 3200]\n",
      "loss: 1.321416  [ 2832/ 3200]\n",
      "loss: 1.234442  [ 2848/ 3200]\n",
      "loss: 1.300269  [ 2864/ 3200]\n",
      "loss: 1.286560  [ 2880/ 3200]\n",
      "loss: 1.232086  [ 2896/ 3200]\n",
      "loss: 1.203910  [ 2912/ 3200]\n",
      "loss: 1.192257  [ 2928/ 3200]\n",
      "loss: 1.290348  [ 2944/ 3200]\n",
      "loss: 1.252686  [ 2960/ 3200]\n",
      "loss: 1.195730  [ 2976/ 3200]\n",
      "loss: 1.278481  [ 2992/ 3200]\n",
      "loss: 1.222558  [ 3008/ 3200]\n",
      "loss: 1.195373  [ 3024/ 3200]\n",
      "loss: 1.324087  [ 3040/ 3200]\n",
      "loss: 1.290728  [ 3056/ 3200]\n",
      "loss: 1.216005  [ 3072/ 3200]\n",
      "loss: 1.319709  [ 3088/ 3200]\n",
      "loss: 1.246963  [ 3104/ 3200]\n",
      "loss: 1.332631  [ 3120/ 3200]\n",
      "loss: 1.226741  [ 3136/ 3200]\n",
      "loss: 1.187288  [ 3152/ 3200]\n",
      "loss: 1.321239  [ 3168/ 3200]\n",
      "loss: 1.237592  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 1.187563  [    0/ 3200]\n",
      "loss: 1.162013  [   16/ 3200]\n",
      "loss: 1.178132  [   32/ 3200]\n",
      "loss: 1.221816  [   48/ 3200]\n",
      "loss: 1.195423  [   64/ 3200]\n",
      "loss: 1.313114  [   80/ 3200]\n",
      "loss: 1.218854  [   96/ 3200]\n",
      "loss: 1.270043  [  112/ 3200]\n",
      "loss: 1.177699  [  128/ 3200]\n",
      "loss: 1.269859  [  144/ 3200]\n",
      "loss: 1.247559  [  160/ 3200]\n",
      "loss: 1.369421  [  176/ 3200]\n",
      "loss: 1.270292  [  192/ 3200]\n",
      "loss: 1.268954  [  208/ 3200]\n",
      "loss: 1.093499  [  224/ 3200]\n",
      "loss: 1.264366  [  240/ 3200]\n",
      "loss: 1.200660  [  256/ 3200]\n",
      "loss: 1.167054  [  272/ 3200]\n",
      "loss: 1.132341  [  288/ 3200]\n",
      "loss: 1.217012  [  304/ 3200]\n",
      "loss: 1.170295  [  320/ 3200]\n",
      "loss: 1.243521  [  336/ 3200]\n",
      "loss: 1.300184  [  352/ 3200]\n",
      "loss: 1.129267  [  368/ 3200]\n",
      "loss: 1.173294  [  384/ 3200]\n",
      "loss: 1.194190  [  400/ 3200]\n",
      "loss: 1.201256  [  416/ 3200]\n",
      "loss: 1.248608  [  432/ 3200]\n",
      "loss: 1.324051  [  448/ 3200]\n",
      "loss: 1.117874  [  464/ 3200]\n",
      "loss: 1.268304  [  480/ 3200]\n",
      "loss: 1.184611  [  496/ 3200]\n",
      "loss: 1.228015  [  512/ 3200]\n",
      "loss: 1.260386  [  528/ 3200]\n",
      "loss: 1.279021  [  544/ 3200]\n",
      "loss: 1.296498  [  560/ 3200]\n",
      "loss: 1.175953  [  576/ 3200]\n",
      "loss: 1.170139  [  592/ 3200]\n",
      "loss: 1.091620  [  608/ 3200]\n",
      "loss: 1.245376  [  624/ 3200]\n",
      "loss: 1.292940  [  640/ 3200]\n",
      "loss: 1.215015  [  656/ 3200]\n",
      "loss: 1.402369  [  672/ 3200]\n",
      "loss: 1.221810  [  688/ 3200]\n",
      "loss: 1.266432  [  704/ 3200]\n",
      "loss: 1.128130  [  720/ 3200]\n",
      "loss: 1.232744  [  736/ 3200]\n",
      "loss: 1.141495  [  752/ 3200]\n",
      "loss: 1.101825  [  768/ 3200]\n",
      "loss: 1.101508  [  784/ 3200]\n",
      "loss: 1.169009  [  800/ 3200]\n",
      "loss: 1.194047  [  816/ 3200]\n",
      "loss: 1.249710  [  832/ 3200]\n",
      "loss: 1.226647  [  848/ 3200]\n",
      "loss: 1.305636  [  864/ 3200]\n",
      "loss: 1.208298  [  880/ 3200]\n",
      "loss: 1.286373  [  896/ 3200]\n",
      "loss: 1.172636  [  912/ 3200]\n",
      "loss: 1.140718  [  928/ 3200]\n",
      "loss: 1.235142  [  944/ 3200]\n",
      "loss: 1.229233  [  960/ 3200]\n",
      "loss: 1.173141  [  976/ 3200]\n",
      "loss: 1.206640  [  992/ 3200]\n",
      "loss: 1.297551  [ 1008/ 3200]\n",
      "loss: 1.244694  [ 1024/ 3200]\n",
      "loss: 1.220652  [ 1040/ 3200]\n",
      "loss: 1.181831  [ 1056/ 3200]\n",
      "loss: 1.185628  [ 1072/ 3200]\n",
      "loss: 1.210381  [ 1088/ 3200]\n",
      "loss: 1.095663  [ 1104/ 3200]\n",
      "loss: 1.258452  [ 1120/ 3200]\n",
      "loss: 1.248922  [ 1136/ 3200]\n",
      "loss: 1.145945  [ 1152/ 3200]\n",
      "loss: 1.174641  [ 1168/ 3200]\n",
      "loss: 1.270712  [ 1184/ 3200]\n",
      "loss: 1.247909  [ 1200/ 3200]\n",
      "loss: 1.190879  [ 1216/ 3200]\n",
      "loss: 1.147864  [ 1232/ 3200]\n",
      "loss: 1.298539  [ 1248/ 3200]\n",
      "loss: 1.352762  [ 1264/ 3200]\n",
      "loss: 1.198515  [ 1280/ 3200]\n",
      "loss: 1.216734  [ 1296/ 3200]\n",
      "loss: 1.265949  [ 1312/ 3200]\n",
      "loss: 1.209705  [ 1328/ 3200]\n",
      "loss: 1.188238  [ 1344/ 3200]\n",
      "loss: 1.160782  [ 1360/ 3200]\n",
      "loss: 1.185696  [ 1376/ 3200]\n",
      "loss: 1.127597  [ 1392/ 3200]\n",
      "loss: 1.247415  [ 1408/ 3200]\n",
      "loss: 1.265474  [ 1424/ 3200]\n",
      "loss: 1.307964  [ 1440/ 3200]\n",
      "loss: 1.231384  [ 1456/ 3200]\n",
      "loss: 1.296068  [ 1472/ 3200]\n",
      "loss: 1.188955  [ 1488/ 3200]\n",
      "loss: 1.213918  [ 1504/ 3200]\n",
      "loss: 1.257028  [ 1520/ 3200]\n",
      "loss: 1.272296  [ 1536/ 3200]\n",
      "loss: 1.248293  [ 1552/ 3200]\n",
      "loss: 1.264649  [ 1568/ 3200]\n",
      "loss: 1.170943  [ 1584/ 3200]\n",
      "loss: 1.277756  [ 1600/ 3200]\n",
      "loss: 1.239443  [ 1616/ 3200]\n",
      "loss: 1.244777  [ 1632/ 3200]\n",
      "loss: 1.178627  [ 1648/ 3200]\n",
      "loss: 1.188721  [ 1664/ 3200]\n",
      "loss: 1.139863  [ 1680/ 3200]\n",
      "loss: 1.259972  [ 1696/ 3200]\n",
      "loss: 1.278579  [ 1712/ 3200]\n",
      "loss: 1.255622  [ 1728/ 3200]\n",
      "loss: 1.221439  [ 1744/ 3200]\n",
      "loss: 1.164204  [ 1760/ 3200]\n",
      "loss: 1.167436  [ 1776/ 3200]\n",
      "loss: 1.305448  [ 1792/ 3200]\n",
      "loss: 1.156424  [ 1808/ 3200]\n",
      "loss: 1.182215  [ 1824/ 3200]\n",
      "loss: 1.200161  [ 1840/ 3200]\n",
      "loss: 1.276131  [ 1856/ 3200]\n",
      "loss: 1.265267  [ 1872/ 3200]\n",
      "loss: 1.246940  [ 1888/ 3200]\n",
      "loss: 1.134173  [ 1904/ 3200]\n",
      "loss: 1.224308  [ 1920/ 3200]\n",
      "loss: 1.138693  [ 1936/ 3200]\n",
      "loss: 1.105024  [ 1952/ 3200]\n",
      "loss: 1.207726  [ 1968/ 3200]\n",
      "loss: 1.161866  [ 1984/ 3200]\n",
      "loss: 1.254110  [ 2000/ 3200]\n",
      "loss: 1.281975  [ 2016/ 3200]\n",
      "loss: 1.227252  [ 2032/ 3200]\n",
      "loss: 1.173051  [ 2048/ 3200]\n",
      "loss: 1.150604  [ 2064/ 3200]\n",
      "loss: 1.133919  [ 2080/ 3200]\n",
      "loss: 1.236093  [ 2096/ 3200]\n",
      "loss: 1.159662  [ 2112/ 3200]\n",
      "loss: 1.228546  [ 2128/ 3200]\n",
      "loss: 1.130506  [ 2144/ 3200]\n",
      "loss: 1.305264  [ 2160/ 3200]\n",
      "loss: 1.115946  [ 2176/ 3200]\n",
      "loss: 1.228505  [ 2192/ 3200]\n",
      "loss: 1.233318  [ 2208/ 3200]\n",
      "loss: 1.324944  [ 2224/ 3200]\n",
      "loss: 1.367560  [ 2240/ 3200]\n",
      "loss: 1.176652  [ 2256/ 3200]\n",
      "loss: 1.268035  [ 2272/ 3200]\n",
      "loss: 1.142603  [ 2288/ 3200]\n",
      "loss: 1.230751  [ 2304/ 3200]\n",
      "loss: 1.311306  [ 2320/ 3200]\n",
      "loss: 1.165693  [ 2336/ 3200]\n",
      "loss: 1.278111  [ 2352/ 3200]\n",
      "loss: 1.412605  [ 2368/ 3200]\n",
      "loss: 1.258143  [ 2384/ 3200]\n",
      "loss: 1.211150  [ 2400/ 3200]\n",
      "loss: 1.041192  [ 2416/ 3200]\n",
      "loss: 1.282643  [ 2432/ 3200]\n",
      "loss: 1.077139  [ 2448/ 3200]\n",
      "loss: 1.285182  [ 2464/ 3200]\n",
      "loss: 1.132692  [ 2480/ 3200]\n",
      "loss: 1.183105  [ 2496/ 3200]\n",
      "loss: 1.188612  [ 2512/ 3200]\n",
      "loss: 1.352488  [ 2528/ 3200]\n",
      "loss: 1.292870  [ 2544/ 3200]\n",
      "loss: 1.293072  [ 2560/ 3200]\n",
      "loss: 1.113920  [ 2576/ 3200]\n",
      "loss: 1.377229  [ 2592/ 3200]\n",
      "loss: 1.289684  [ 2608/ 3200]\n",
      "loss: 1.145073  [ 2624/ 3200]\n",
      "loss: 1.157157  [ 2640/ 3200]\n",
      "loss: 1.216591  [ 2656/ 3200]\n",
      "loss: 1.191364  [ 2672/ 3200]\n",
      "loss: 1.217608  [ 2688/ 3200]\n",
      "loss: 1.238873  [ 2704/ 3200]\n",
      "loss: 1.113163  [ 2720/ 3200]\n",
      "loss: 1.224133  [ 2736/ 3200]\n",
      "loss: 1.219211  [ 2752/ 3200]\n",
      "loss: 1.300351  [ 2768/ 3200]\n",
      "loss: 1.243631  [ 2784/ 3200]\n",
      "loss: 1.287167  [ 2800/ 3200]\n",
      "loss: 1.099528  [ 2816/ 3200]\n",
      "loss: 1.250793  [ 2832/ 3200]\n",
      "loss: 1.257390  [ 2848/ 3200]\n",
      "loss: 1.097442  [ 2864/ 3200]\n",
      "loss: 1.256112  [ 2880/ 3200]\n",
      "loss: 1.162645  [ 2896/ 3200]\n",
      "loss: 1.179541  [ 2912/ 3200]\n",
      "loss: 1.125150  [ 2928/ 3200]\n",
      "loss: 1.183056  [ 2944/ 3200]\n",
      "loss: 1.100797  [ 2960/ 3200]\n",
      "loss: 1.327514  [ 2976/ 3200]\n",
      "loss: 1.105183  [ 2992/ 3200]\n",
      "loss: 1.200148  [ 3008/ 3200]\n",
      "loss: 1.224391  [ 3024/ 3200]\n",
      "loss: 1.214950  [ 3040/ 3200]\n",
      "loss: 1.187712  [ 3056/ 3200]\n",
      "loss: 1.201760  [ 3072/ 3200]\n",
      "loss: 1.299402  [ 3088/ 3200]\n",
      "loss: 1.198247  [ 3104/ 3200]\n",
      "loss: 1.303689  [ 3120/ 3200]\n",
      "loss: 1.196517  [ 3136/ 3200]\n",
      "loss: 1.235988  [ 3152/ 3200]\n",
      "loss: 1.221992  [ 3168/ 3200]\n",
      "loss: 1.234886  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 1.205558  [    0/ 3200]\n",
      "loss: 1.150549  [   16/ 3200]\n",
      "loss: 1.131254  [   32/ 3200]\n",
      "loss: 1.283481  [   48/ 3200]\n",
      "loss: 1.213295  [   64/ 3200]\n",
      "loss: 1.172185  [   80/ 3200]\n",
      "loss: 1.142262  [   96/ 3200]\n",
      "loss: 1.175573  [  112/ 3200]\n",
      "loss: 1.193266  [  128/ 3200]\n",
      "loss: 1.150731  [  144/ 3200]\n",
      "loss: 1.240379  [  160/ 3200]\n",
      "loss: 1.198457  [  176/ 3200]\n",
      "loss: 1.244411  [  192/ 3200]\n",
      "loss: 1.277711  [  208/ 3200]\n",
      "loss: 1.184286  [  224/ 3200]\n",
      "loss: 1.245007  [  240/ 3200]\n",
      "loss: 1.103691  [  256/ 3200]\n",
      "loss: 1.260730  [  272/ 3200]\n",
      "loss: 1.168201  [  288/ 3200]\n",
      "loss: 1.220363  [  304/ 3200]\n",
      "loss: 1.063851  [  320/ 3200]\n",
      "loss: 1.109991  [  336/ 3200]\n",
      "loss: 1.220420  [  352/ 3200]\n",
      "loss: 1.311024  [  368/ 3200]\n",
      "loss: 1.150856  [  384/ 3200]\n",
      "loss: 1.044617  [  400/ 3200]\n",
      "loss: 1.198981  [  416/ 3200]\n",
      "loss: 1.162948  [  432/ 3200]\n",
      "loss: 1.172062  [  448/ 3200]\n",
      "loss: 1.351383  [  464/ 3200]\n",
      "loss: 1.248272  [  480/ 3200]\n",
      "loss: 1.221902  [  496/ 3200]\n",
      "loss: 1.272877  [  512/ 3200]\n",
      "loss: 1.243293  [  528/ 3200]\n",
      "loss: 1.136309  [  544/ 3200]\n",
      "loss: 1.155003  [  560/ 3200]\n",
      "loss: 1.326785  [  576/ 3200]\n",
      "loss: 1.221938  [  592/ 3200]\n",
      "loss: 1.131545  [  608/ 3200]\n",
      "loss: 1.246242  [  624/ 3200]\n",
      "loss: 1.239448  [  640/ 3200]\n",
      "loss: 1.228533  [  656/ 3200]\n",
      "loss: 1.278287  [  672/ 3200]\n",
      "loss: 1.182929  [  688/ 3200]\n",
      "loss: 1.205602  [  704/ 3200]\n",
      "loss: 1.230825  [  720/ 3200]\n",
      "loss: 1.113543  [  736/ 3200]\n",
      "loss: 1.299582  [  752/ 3200]\n",
      "loss: 1.071894  [  768/ 3200]\n",
      "loss: 1.256464  [  784/ 3200]\n",
      "loss: 1.081080  [  800/ 3200]\n",
      "loss: 1.205134  [  816/ 3200]\n",
      "loss: 1.215522  [  832/ 3200]\n",
      "loss: 1.276705  [  848/ 3200]\n",
      "loss: 1.154406  [  864/ 3200]\n",
      "loss: 1.203874  [  880/ 3200]\n",
      "loss: 1.282729  [  896/ 3200]\n",
      "loss: 1.163551  [  912/ 3200]\n",
      "loss: 1.117749  [  928/ 3200]\n",
      "loss: 1.195965  [  944/ 3200]\n",
      "loss: 1.255029  [  960/ 3200]\n",
      "loss: 1.180263  [  976/ 3200]\n",
      "loss: 1.145800  [  992/ 3200]\n",
      "loss: 1.201404  [ 1008/ 3200]\n",
      "loss: 1.169338  [ 1024/ 3200]\n",
      "loss: 1.290573  [ 1040/ 3200]\n",
      "loss: 1.102118  [ 1056/ 3200]\n",
      "loss: 1.048142  [ 1072/ 3200]\n",
      "loss: 1.295086  [ 1088/ 3200]\n",
      "loss: 1.146509  [ 1104/ 3200]\n",
      "loss: 1.145861  [ 1120/ 3200]\n",
      "loss: 1.063156  [ 1136/ 3200]\n",
      "loss: 1.258662  [ 1152/ 3200]\n",
      "loss: 1.241751  [ 1168/ 3200]\n",
      "loss: 1.271395  [ 1184/ 3200]\n",
      "loss: 1.212872  [ 1200/ 3200]\n",
      "loss: 1.197920  [ 1216/ 3200]\n",
      "loss: 1.252573  [ 1232/ 3200]\n",
      "loss: 1.169812  [ 1248/ 3200]\n",
      "loss: 1.193450  [ 1264/ 3200]\n",
      "loss: 1.133948  [ 1280/ 3200]\n",
      "loss: 1.262462  [ 1296/ 3200]\n",
      "loss: 1.195336  [ 1312/ 3200]\n",
      "loss: 1.243213  [ 1328/ 3200]\n",
      "loss: 1.219459  [ 1344/ 3200]\n",
      "loss: 1.185902  [ 1360/ 3200]\n",
      "loss: 1.188027  [ 1376/ 3200]\n",
      "loss: 1.244304  [ 1392/ 3200]\n",
      "loss: 1.196996  [ 1408/ 3200]\n",
      "loss: 1.089444  [ 1424/ 3200]\n",
      "loss: 1.152547  [ 1440/ 3200]\n",
      "loss: 1.174364  [ 1456/ 3200]\n",
      "loss: 0.975514  [ 1472/ 3200]\n",
      "loss: 1.327460  [ 1488/ 3200]\n",
      "loss: 1.031219  [ 1504/ 3200]\n",
      "loss: 0.955157  [ 1520/ 3200]\n",
      "loss: 1.191843  [ 1536/ 3200]\n",
      "loss: 1.133004  [ 1552/ 3200]\n",
      "loss: 1.356978  [ 1568/ 3200]\n",
      "loss: 1.154856  [ 1584/ 3200]\n",
      "loss: 1.249429  [ 1600/ 3200]\n",
      "loss: 1.264714  [ 1616/ 3200]\n",
      "loss: 1.259778  [ 1632/ 3200]\n",
      "loss: 1.237593  [ 1648/ 3200]\n",
      "loss: 1.089185  [ 1664/ 3200]\n",
      "loss: 1.186635  [ 1680/ 3200]\n",
      "loss: 1.248971  [ 1696/ 3200]\n",
      "loss: 1.047516  [ 1712/ 3200]\n",
      "loss: 1.220758  [ 1728/ 3200]\n",
      "loss: 1.220070  [ 1744/ 3200]\n",
      "loss: 1.120192  [ 1760/ 3200]\n",
      "loss: 1.217781  [ 1776/ 3200]\n",
      "loss: 1.213577  [ 1792/ 3200]\n",
      "loss: 1.238576  [ 1808/ 3200]\n",
      "loss: 1.152543  [ 1824/ 3200]\n",
      "loss: 1.150905  [ 1840/ 3200]\n",
      "loss: 1.255809  [ 1856/ 3200]\n",
      "loss: 1.196906  [ 1872/ 3200]\n",
      "loss: 1.208563  [ 1888/ 3200]\n",
      "loss: 1.120174  [ 1904/ 3200]\n",
      "loss: 1.244583  [ 1920/ 3200]\n",
      "loss: 1.240330  [ 1936/ 3200]\n",
      "loss: 1.199264  [ 1952/ 3200]\n",
      "loss: 1.104581  [ 1968/ 3200]\n",
      "loss: 1.215045  [ 1984/ 3200]\n",
      "loss: 1.203564  [ 2000/ 3200]\n",
      "loss: 1.192728  [ 2016/ 3200]\n",
      "loss: 1.123888  [ 2032/ 3200]\n",
      "loss: 1.161512  [ 2048/ 3200]\n",
      "loss: 1.256403  [ 2064/ 3200]\n",
      "loss: 1.269165  [ 2080/ 3200]\n",
      "loss: 1.131327  [ 2096/ 3200]\n",
      "loss: 1.069536  [ 2112/ 3200]\n",
      "loss: 1.312584  [ 2128/ 3200]\n",
      "loss: 1.235200  [ 2144/ 3200]\n",
      "loss: 1.150340  [ 2160/ 3200]\n",
      "loss: 1.216419  [ 2176/ 3200]\n",
      "loss: 1.063757  [ 2192/ 3200]\n",
      "loss: 1.137861  [ 2208/ 3200]\n",
      "loss: 1.262740  [ 2224/ 3200]\n",
      "loss: 1.132727  [ 2240/ 3200]\n",
      "loss: 1.292872  [ 2256/ 3200]\n",
      "loss: 1.181021  [ 2272/ 3200]\n",
      "loss: 1.244957  [ 2288/ 3200]\n",
      "loss: 1.122800  [ 2304/ 3200]\n",
      "loss: 1.469879  [ 2320/ 3200]\n",
      "loss: 1.174750  [ 2336/ 3200]\n",
      "loss: 1.319990  [ 2352/ 3200]\n",
      "loss: 1.149347  [ 2368/ 3200]\n",
      "loss: 1.226896  [ 2384/ 3200]\n",
      "loss: 1.244440  [ 2400/ 3200]\n",
      "loss: 1.219355  [ 2416/ 3200]\n",
      "loss: 1.156914  [ 2432/ 3200]\n",
      "loss: 1.185023  [ 2448/ 3200]\n",
      "loss: 1.129469  [ 2464/ 3200]\n",
      "loss: 1.247196  [ 2480/ 3200]\n",
      "loss: 1.226975  [ 2496/ 3200]\n",
      "loss: 1.257622  [ 2512/ 3200]\n",
      "loss: 1.197747  [ 2528/ 3200]\n",
      "loss: 1.230007  [ 2544/ 3200]\n",
      "loss: 1.237754  [ 2560/ 3200]\n",
      "loss: 1.205605  [ 2576/ 3200]\n",
      "loss: 1.145788  [ 2592/ 3200]\n",
      "loss: 1.322742  [ 2608/ 3200]\n",
      "loss: 1.219477  [ 2624/ 3200]\n",
      "loss: 1.186451  [ 2640/ 3200]\n",
      "loss: 1.158797  [ 2656/ 3200]\n",
      "loss: 1.285284  [ 2672/ 3200]\n",
      "loss: 1.246438  [ 2688/ 3200]\n",
      "loss: 1.217734  [ 2704/ 3200]\n",
      "loss: 1.199404  [ 2720/ 3200]\n",
      "loss: 1.194482  [ 2736/ 3200]\n",
      "loss: 1.239554  [ 2752/ 3200]\n",
      "loss: 1.321281  [ 2768/ 3200]\n",
      "loss: 1.184625  [ 2784/ 3200]\n",
      "loss: 1.204031  [ 2800/ 3200]\n",
      "loss: 1.104279  [ 2816/ 3200]\n",
      "loss: 1.166049  [ 2832/ 3200]\n",
      "loss: 1.200937  [ 2848/ 3200]\n",
      "loss: 1.102903  [ 2864/ 3200]\n",
      "loss: 1.156037  [ 2880/ 3200]\n",
      "loss: 1.287869  [ 2896/ 3200]\n",
      "loss: 1.323849  [ 2912/ 3200]\n",
      "loss: 1.169074  [ 2928/ 3200]\n",
      "loss: 1.334559  [ 2944/ 3200]\n",
      "loss: 1.107740  [ 2960/ 3200]\n",
      "loss: 1.195718  [ 2976/ 3200]\n",
      "loss: 1.154414  [ 2992/ 3200]\n",
      "loss: 1.125202  [ 3008/ 3200]\n",
      "loss: 1.227832  [ 3024/ 3200]\n",
      "loss: 1.242904  [ 3040/ 3200]\n",
      "loss: 1.263706  [ 3056/ 3200]\n",
      "loss: 1.288480  [ 3072/ 3200]\n",
      "loss: 1.126602  [ 3088/ 3200]\n",
      "loss: 1.203673  [ 3104/ 3200]\n",
      "loss: 1.207323  [ 3120/ 3200]\n",
      "loss: 1.151475  [ 3136/ 3200]\n",
      "loss: 1.188962  [ 3152/ 3200]\n",
      "loss: 1.080176  [ 3168/ 3200]\n",
      "loss: 1.205914  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 1.305923  [    0/ 3200]\n",
      "loss: 1.323387  [   16/ 3200]\n",
      "loss: 1.179438  [   32/ 3200]\n",
      "loss: 1.140158  [   48/ 3200]\n",
      "loss: 1.035371  [   64/ 3200]\n",
      "loss: 1.109664  [   80/ 3200]\n",
      "loss: 1.252801  [   96/ 3200]\n",
      "loss: 1.245110  [  112/ 3200]\n",
      "loss: 1.121257  [  128/ 3200]\n",
      "loss: 1.221623  [  144/ 3200]\n",
      "loss: 1.209634  [  160/ 3200]\n",
      "loss: 1.162798  [  176/ 3200]\n",
      "loss: 1.237535  [  192/ 3200]\n",
      "loss: 1.228732  [  208/ 3200]\n",
      "loss: 1.130586  [  224/ 3200]\n",
      "loss: 1.202666  [  240/ 3200]\n",
      "loss: 1.268348  [  256/ 3200]\n",
      "loss: 1.151061  [  272/ 3200]\n",
      "loss: 1.143878  [  288/ 3200]\n",
      "loss: 1.168591  [  304/ 3200]\n",
      "loss: 1.296089  [  320/ 3200]\n",
      "loss: 1.176863  [  336/ 3200]\n",
      "loss: 1.178980  [  352/ 3200]\n",
      "loss: 1.124880  [  368/ 3200]\n",
      "loss: 1.114317  [  384/ 3200]\n",
      "loss: 1.174607  [  400/ 3200]\n",
      "loss: 1.152459  [  416/ 3200]\n",
      "loss: 1.203355  [  432/ 3200]\n",
      "loss: 1.187177  [  448/ 3200]\n",
      "loss: 1.133061  [  464/ 3200]\n",
      "loss: 1.176912  [  480/ 3200]\n",
      "loss: 1.129605  [  496/ 3200]\n",
      "loss: 1.198542  [  512/ 3200]\n",
      "loss: 1.343519  [  528/ 3200]\n",
      "loss: 1.209791  [  544/ 3200]\n",
      "loss: 1.177382  [  560/ 3200]\n",
      "loss: 1.196481  [  576/ 3200]\n",
      "loss: 1.209716  [  592/ 3200]\n",
      "loss: 1.216670  [  608/ 3200]\n",
      "loss: 1.186175  [  624/ 3200]\n",
      "loss: 1.117212  [  640/ 3200]\n",
      "loss: 1.215813  [  656/ 3200]\n",
      "loss: 1.150873  [  672/ 3200]\n",
      "loss: 1.147822  [  688/ 3200]\n",
      "loss: 1.141944  [  704/ 3200]\n",
      "loss: 1.187641  [  720/ 3200]\n",
      "loss: 0.993966  [  736/ 3200]\n",
      "loss: 1.150899  [  752/ 3200]\n",
      "loss: 1.135363  [  768/ 3200]\n",
      "loss: 1.152726  [  784/ 3200]\n",
      "loss: 1.225125  [  800/ 3200]\n",
      "loss: 1.125571  [  816/ 3200]\n",
      "loss: 1.142990  [  832/ 3200]\n",
      "loss: 1.226512  [  848/ 3200]\n",
      "loss: 1.169688  [  864/ 3200]\n",
      "loss: 1.159986  [  880/ 3200]\n",
      "loss: 1.224662  [  896/ 3200]\n",
      "loss: 1.263865  [  912/ 3200]\n",
      "loss: 1.151553  [  928/ 3200]\n",
      "loss: 1.445724  [  944/ 3200]\n",
      "loss: 1.244913  [  960/ 3200]\n",
      "loss: 1.210627  [  976/ 3200]\n",
      "loss: 1.157500  [  992/ 3200]\n",
      "loss: 1.404019  [ 1008/ 3200]\n",
      "loss: 1.053827  [ 1024/ 3200]\n",
      "loss: 1.310060  [ 1040/ 3200]\n",
      "loss: 1.153436  [ 1056/ 3200]\n",
      "loss: 1.231040  [ 1072/ 3200]\n",
      "loss: 1.133856  [ 1088/ 3200]\n",
      "loss: 1.069407  [ 1104/ 3200]\n",
      "loss: 1.146208  [ 1120/ 3200]\n",
      "loss: 1.173346  [ 1136/ 3200]\n",
      "loss: 1.126368  [ 1152/ 3200]\n",
      "loss: 1.119486  [ 1168/ 3200]\n",
      "loss: 1.108023  [ 1184/ 3200]\n",
      "loss: 1.073886  [ 1200/ 3200]\n",
      "loss: 1.281259  [ 1216/ 3200]\n",
      "loss: 1.327269  [ 1232/ 3200]\n",
      "loss: 1.295290  [ 1248/ 3200]\n",
      "loss: 1.186722  [ 1264/ 3200]\n",
      "loss: 1.245927  [ 1280/ 3200]\n",
      "loss: 1.098912  [ 1296/ 3200]\n",
      "loss: 1.187819  [ 1312/ 3200]\n",
      "loss: 1.165617  [ 1328/ 3200]\n",
      "loss: 1.397893  [ 1344/ 3200]\n",
      "loss: 1.155403  [ 1360/ 3200]\n",
      "loss: 1.109509  [ 1376/ 3200]\n",
      "loss: 1.117457  [ 1392/ 3200]\n",
      "loss: 1.120226  [ 1408/ 3200]\n",
      "loss: 1.241319  [ 1424/ 3200]\n",
      "loss: 1.159599  [ 1440/ 3200]\n",
      "loss: 1.273833  [ 1456/ 3200]\n",
      "loss: 1.105233  [ 1472/ 3200]\n",
      "loss: 1.258030  [ 1488/ 3200]\n",
      "loss: 1.369317  [ 1504/ 3200]\n",
      "loss: 1.156606  [ 1520/ 3200]\n",
      "loss: 1.237894  [ 1536/ 3200]\n",
      "loss: 1.136086  [ 1552/ 3200]\n",
      "loss: 1.138023  [ 1568/ 3200]\n",
      "loss: 1.266028  [ 1584/ 3200]\n",
      "loss: 1.217376  [ 1600/ 3200]\n",
      "loss: 1.165920  [ 1616/ 3200]\n",
      "loss: 1.065931  [ 1632/ 3200]\n",
      "loss: 1.124532  [ 1648/ 3200]\n",
      "loss: 1.152323  [ 1664/ 3200]\n",
      "loss: 1.154768  [ 1680/ 3200]\n",
      "loss: 1.186649  [ 1696/ 3200]\n",
      "loss: 1.141813  [ 1712/ 3200]\n",
      "loss: 1.085175  [ 1728/ 3200]\n",
      "loss: 1.223066  [ 1744/ 3200]\n",
      "loss: 1.091027  [ 1760/ 3200]\n",
      "loss: 1.118243  [ 1776/ 3200]\n",
      "loss: 1.193487  [ 1792/ 3200]\n",
      "loss: 1.067523  [ 1808/ 3200]\n",
      "loss: 1.158165  [ 1824/ 3200]\n",
      "loss: 1.232524  [ 1840/ 3200]\n",
      "loss: 1.276982  [ 1856/ 3200]\n",
      "loss: 1.353846  [ 1872/ 3200]\n",
      "loss: 1.084445  [ 1888/ 3200]\n",
      "loss: 1.202759  [ 1904/ 3200]\n",
      "loss: 1.226772  [ 1920/ 3200]\n",
      "loss: 1.212536  [ 1936/ 3200]\n",
      "loss: 1.159545  [ 1952/ 3200]\n",
      "loss: 1.237092  [ 1968/ 3200]\n",
      "loss: 1.148927  [ 1984/ 3200]\n",
      "loss: 1.113677  [ 2000/ 3200]\n",
      "loss: 1.159283  [ 2016/ 3200]\n",
      "loss: 1.198470  [ 2032/ 3200]\n",
      "loss: 1.200757  [ 2048/ 3200]\n",
      "loss: 1.055138  [ 2064/ 3200]\n",
      "loss: 1.072304  [ 2080/ 3200]\n",
      "loss: 1.174359  [ 2096/ 3200]\n",
      "loss: 1.265861  [ 2112/ 3200]\n",
      "loss: 1.132042  [ 2128/ 3200]\n",
      "loss: 1.124739  [ 2144/ 3200]\n",
      "loss: 1.238368  [ 2160/ 3200]\n",
      "loss: 1.197890  [ 2176/ 3200]\n",
      "loss: 1.145441  [ 2192/ 3200]\n",
      "loss: 1.181059  [ 2208/ 3200]\n",
      "loss: 1.118165  [ 2224/ 3200]\n",
      "loss: 1.397804  [ 2240/ 3200]\n",
      "loss: 1.236972  [ 2256/ 3200]\n",
      "loss: 1.199880  [ 2272/ 3200]\n",
      "loss: 1.107213  [ 2288/ 3200]\n",
      "loss: 1.198134  [ 2304/ 3200]\n",
      "loss: 1.117133  [ 2320/ 3200]\n",
      "loss: 1.157922  [ 2336/ 3200]\n",
      "loss: 1.066644  [ 2352/ 3200]\n",
      "loss: 1.366956  [ 2368/ 3200]\n",
      "loss: 1.208021  [ 2384/ 3200]\n",
      "loss: 1.152611  [ 2400/ 3200]\n",
      "loss: 1.154086  [ 2416/ 3200]\n",
      "loss: 1.128533  [ 2432/ 3200]\n",
      "loss: 1.197129  [ 2448/ 3200]\n",
      "loss: 1.045903  [ 2464/ 3200]\n",
      "loss: 1.210000  [ 2480/ 3200]\n",
      "loss: 1.142150  [ 2496/ 3200]\n",
      "loss: 1.144968  [ 2512/ 3200]\n",
      "loss: 1.180365  [ 2528/ 3200]\n",
      "loss: 1.151709  [ 2544/ 3200]\n",
      "loss: 1.199986  [ 2560/ 3200]\n",
      "loss: 1.214629  [ 2576/ 3200]\n",
      "loss: 1.154146  [ 2592/ 3200]\n",
      "loss: 1.212896  [ 2608/ 3200]\n",
      "loss: 1.102934  [ 2624/ 3200]\n",
      "loss: 1.065356  [ 2640/ 3200]\n",
      "loss: 1.125694  [ 2656/ 3200]\n",
      "loss: 1.179515  [ 2672/ 3200]\n",
      "loss: 1.215310  [ 2688/ 3200]\n",
      "loss: 1.033523  [ 2704/ 3200]\n",
      "loss: 1.280837  [ 2720/ 3200]\n",
      "loss: 1.148054  [ 2736/ 3200]\n",
      "loss: 1.069946  [ 2752/ 3200]\n",
      "loss: 1.005271  [ 2768/ 3200]\n",
      "loss: 1.206113  [ 2784/ 3200]\n",
      "loss: 1.260272  [ 2800/ 3200]\n",
      "loss: 1.218855  [ 2816/ 3200]\n",
      "loss: 1.244227  [ 2832/ 3200]\n",
      "loss: 1.028296  [ 2848/ 3200]\n",
      "loss: 1.072798  [ 2864/ 3200]\n",
      "loss: 1.108776  [ 2880/ 3200]\n",
      "loss: 1.031634  [ 2896/ 3200]\n",
      "loss: 1.179846  [ 2912/ 3200]\n",
      "loss: 1.180488  [ 2928/ 3200]\n",
      "loss: 1.197400  [ 2944/ 3200]\n",
      "loss: 1.193316  [ 2960/ 3200]\n",
      "loss: 1.185450  [ 2976/ 3200]\n",
      "loss: 1.147947  [ 2992/ 3200]\n",
      "loss: 1.130676  [ 3008/ 3200]\n",
      "loss: 1.132675  [ 3024/ 3200]\n",
      "loss: 1.030382  [ 3040/ 3200]\n",
      "loss: 1.140149  [ 3056/ 3200]\n",
      "loss: 1.243586  [ 3072/ 3200]\n",
      "loss: 1.237471  [ 3088/ 3200]\n",
      "loss: 0.956542  [ 3104/ 3200]\n",
      "loss: 1.068579  [ 3120/ 3200]\n",
      "loss: 1.220646  [ 3136/ 3200]\n",
      "loss: 1.289823  [ 3152/ 3200]\n",
      "loss: 1.278285  [ 3168/ 3200]\n",
      "loss: 1.156398  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 1.186722  [    0/ 3200]\n",
      "loss: 1.288388  [   16/ 3200]\n",
      "loss: 1.155179  [   32/ 3200]\n",
      "loss: 1.176943  [   48/ 3200]\n",
      "loss: 1.209867  [   64/ 3200]\n",
      "loss: 1.180138  [   80/ 3200]\n",
      "loss: 1.301737  [   96/ 3200]\n",
      "loss: 1.187665  [  112/ 3200]\n",
      "loss: 1.241914  [  128/ 3200]\n",
      "loss: 1.126573  [  144/ 3200]\n",
      "loss: 1.232062  [  160/ 3200]\n",
      "loss: 1.118519  [  176/ 3200]\n",
      "loss: 0.951263  [  192/ 3200]\n",
      "loss: 1.209956  [  208/ 3200]\n",
      "loss: 1.152369  [  224/ 3200]\n",
      "loss: 1.124378  [  240/ 3200]\n",
      "loss: 1.152394  [  256/ 3200]\n",
      "loss: 1.141980  [  272/ 3200]\n",
      "loss: 1.203323  [  288/ 3200]\n",
      "loss: 1.261654  [  304/ 3200]\n",
      "loss: 1.077991  [  320/ 3200]\n",
      "loss: 1.288998  [  336/ 3200]\n",
      "loss: 1.225411  [  352/ 3200]\n",
      "loss: 1.137176  [  368/ 3200]\n",
      "loss: 1.036745  [  384/ 3200]\n",
      "loss: 1.119229  [  400/ 3200]\n",
      "loss: 1.116714  [  416/ 3200]\n",
      "loss: 1.232171  [  432/ 3200]\n",
      "loss: 1.133366  [  448/ 3200]\n",
      "loss: 1.120620  [  464/ 3200]\n",
      "loss: 1.225071  [  480/ 3200]\n",
      "loss: 1.080782  [  496/ 3200]\n",
      "loss: 1.282813  [  512/ 3200]\n",
      "loss: 1.134552  [  528/ 3200]\n",
      "loss: 1.119996  [  544/ 3200]\n",
      "loss: 1.177231  [  560/ 3200]\n",
      "loss: 1.254769  [  576/ 3200]\n",
      "loss: 1.052093  [  592/ 3200]\n",
      "loss: 1.131795  [  608/ 3200]\n",
      "loss: 1.137464  [  624/ 3200]\n",
      "loss: 1.109401  [  640/ 3200]\n",
      "loss: 1.208622  [  656/ 3200]\n",
      "loss: 1.210497  [  672/ 3200]\n",
      "loss: 1.287728  [  688/ 3200]\n",
      "loss: 1.200292  [  704/ 3200]\n",
      "loss: 1.190433  [  720/ 3200]\n",
      "loss: 1.205027  [  736/ 3200]\n",
      "loss: 1.208894  [  752/ 3200]\n",
      "loss: 1.287922  [  768/ 3200]\n",
      "loss: 1.130633  [  784/ 3200]\n",
      "loss: 1.028076  [  800/ 3200]\n",
      "loss: 1.194420  [  816/ 3200]\n",
      "loss: 1.198909  [  832/ 3200]\n",
      "loss: 1.126704  [  848/ 3200]\n",
      "loss: 1.113857  [  864/ 3200]\n",
      "loss: 1.149885  [  880/ 3200]\n",
      "loss: 1.039718  [  896/ 3200]\n",
      "loss: 1.103400  [  912/ 3200]\n",
      "loss: 1.183560  [  928/ 3200]\n",
      "loss: 1.222607  [  944/ 3200]\n",
      "loss: 1.256369  [  960/ 3200]\n",
      "loss: 1.003033  [  976/ 3200]\n",
      "loss: 1.115183  [  992/ 3200]\n",
      "loss: 1.133595  [ 1008/ 3200]\n",
      "loss: 1.275108  [ 1024/ 3200]\n",
      "loss: 1.099852  [ 1040/ 3200]\n",
      "loss: 1.111889  [ 1056/ 3200]\n",
      "loss: 1.098435  [ 1072/ 3200]\n",
      "loss: 1.208921  [ 1088/ 3200]\n",
      "loss: 1.168789  [ 1104/ 3200]\n",
      "loss: 1.120159  [ 1120/ 3200]\n",
      "loss: 0.920091  [ 1136/ 3200]\n",
      "loss: 1.019826  [ 1152/ 3200]\n",
      "loss: 1.042356  [ 1168/ 3200]\n",
      "loss: 1.089725  [ 1184/ 3200]\n",
      "loss: 1.391022  [ 1200/ 3200]\n",
      "loss: 1.147688  [ 1216/ 3200]\n",
      "loss: 1.252490  [ 1232/ 3200]\n",
      "loss: 1.158427  [ 1248/ 3200]\n",
      "loss: 1.345663  [ 1264/ 3200]\n",
      "loss: 1.148377  [ 1280/ 3200]\n",
      "loss: 1.115977  [ 1296/ 3200]\n",
      "loss: 1.172986  [ 1312/ 3200]\n",
      "loss: 1.150288  [ 1328/ 3200]\n",
      "loss: 0.994586  [ 1344/ 3200]\n",
      "loss: 0.984579  [ 1360/ 3200]\n",
      "loss: 1.237202  [ 1376/ 3200]\n",
      "loss: 1.231075  [ 1392/ 3200]\n",
      "loss: 1.165271  [ 1408/ 3200]\n",
      "loss: 1.141417  [ 1424/ 3200]\n",
      "loss: 1.162803  [ 1440/ 3200]\n",
      "loss: 1.070969  [ 1456/ 3200]\n",
      "loss: 1.159941  [ 1472/ 3200]\n",
      "loss: 1.106512  [ 1488/ 3200]\n",
      "loss: 1.166495  [ 1504/ 3200]\n",
      "loss: 1.071934  [ 1520/ 3200]\n",
      "loss: 1.247315  [ 1536/ 3200]\n",
      "loss: 1.087606  [ 1552/ 3200]\n",
      "loss: 1.239447  [ 1568/ 3200]\n",
      "loss: 1.256459  [ 1584/ 3200]\n",
      "loss: 1.284341  [ 1600/ 3200]\n",
      "loss: 1.198908  [ 1616/ 3200]\n",
      "loss: 1.105590  [ 1632/ 3200]\n",
      "loss: 1.160887  [ 1648/ 3200]\n",
      "loss: 1.203184  [ 1664/ 3200]\n",
      "loss: 1.182962  [ 1680/ 3200]\n",
      "loss: 1.210894  [ 1696/ 3200]\n",
      "loss: 1.191018  [ 1712/ 3200]\n",
      "loss: 1.146244  [ 1728/ 3200]\n",
      "loss: 1.217459  [ 1744/ 3200]\n",
      "loss: 1.236091  [ 1760/ 3200]\n",
      "loss: 1.075378  [ 1776/ 3200]\n",
      "loss: 1.098613  [ 1792/ 3200]\n",
      "loss: 1.110755  [ 1808/ 3200]\n",
      "loss: 0.985241  [ 1824/ 3200]\n",
      "loss: 1.218803  [ 1840/ 3200]\n",
      "loss: 1.190838  [ 1856/ 3200]\n",
      "loss: 1.126255  [ 1872/ 3200]\n",
      "loss: 1.173910  [ 1888/ 3200]\n",
      "loss: 1.229677  [ 1904/ 3200]\n",
      "loss: 1.100143  [ 1920/ 3200]\n",
      "loss: 1.210725  [ 1936/ 3200]\n",
      "loss: 1.158900  [ 1952/ 3200]\n",
      "loss: 1.177504  [ 1968/ 3200]\n",
      "loss: 1.009567  [ 1984/ 3200]\n",
      "loss: 1.239147  [ 2000/ 3200]\n",
      "loss: 1.237529  [ 2016/ 3200]\n",
      "loss: 1.190861  [ 2032/ 3200]\n",
      "loss: 1.141823  [ 2048/ 3200]\n",
      "loss: 1.055556  [ 2064/ 3200]\n",
      "loss: 1.113047  [ 2080/ 3200]\n",
      "loss: 1.059136  [ 2096/ 3200]\n",
      "loss: 1.108050  [ 2112/ 3200]\n",
      "loss: 1.179447  [ 2128/ 3200]\n",
      "loss: 1.194376  [ 2144/ 3200]\n",
      "loss: 1.192153  [ 2160/ 3200]\n",
      "loss: 1.282824  [ 2176/ 3200]\n",
      "loss: 0.976055  [ 2192/ 3200]\n",
      "loss: 1.199018  [ 2208/ 3200]\n",
      "loss: 1.181038  [ 2224/ 3200]\n",
      "loss: 1.071206  [ 2240/ 3200]\n",
      "loss: 1.155984  [ 2256/ 3200]\n",
      "loss: 1.148898  [ 2272/ 3200]\n",
      "loss: 1.190134  [ 2288/ 3200]\n",
      "loss: 1.164129  [ 2304/ 3200]\n",
      "loss: 1.155402  [ 2320/ 3200]\n",
      "loss: 1.038752  [ 2336/ 3200]\n",
      "loss: 1.268340  [ 2352/ 3200]\n",
      "loss: 1.236812  [ 2368/ 3200]\n",
      "loss: 1.163938  [ 2384/ 3200]\n",
      "loss: 1.247219  [ 2400/ 3200]\n",
      "loss: 1.118587  [ 2416/ 3200]\n",
      "loss: 1.324100  [ 2432/ 3200]\n",
      "loss: 1.094335  [ 2448/ 3200]\n",
      "loss: 1.117347  [ 2464/ 3200]\n",
      "loss: 0.944201  [ 2480/ 3200]\n",
      "loss: 1.199432  [ 2496/ 3200]\n",
      "loss: 1.184489  [ 2512/ 3200]\n",
      "loss: 1.194549  [ 2528/ 3200]\n",
      "loss: 1.100045  [ 2544/ 3200]\n",
      "loss: 1.082286  [ 2560/ 3200]\n",
      "loss: 1.025692  [ 2576/ 3200]\n",
      "loss: 1.263691  [ 2592/ 3200]\n",
      "loss: 1.154192  [ 2608/ 3200]\n",
      "loss: 1.166873  [ 2624/ 3200]\n",
      "loss: 1.272496  [ 2640/ 3200]\n",
      "loss: 1.160080  [ 2656/ 3200]\n",
      "loss: 1.121850  [ 2672/ 3200]\n",
      "loss: 1.182114  [ 2688/ 3200]\n",
      "loss: 1.029040  [ 2704/ 3200]\n",
      "loss: 1.077072  [ 2720/ 3200]\n",
      "loss: 0.913335  [ 2736/ 3200]\n",
      "loss: 1.203201  [ 2752/ 3200]\n",
      "loss: 1.136537  [ 2768/ 3200]\n",
      "loss: 1.164511  [ 2784/ 3200]\n",
      "loss: 1.113409  [ 2800/ 3200]\n",
      "loss: 1.191135  [ 2816/ 3200]\n",
      "loss: 1.300548  [ 2832/ 3200]\n",
      "loss: 1.201486  [ 2848/ 3200]\n",
      "loss: 1.079124  [ 2864/ 3200]\n",
      "loss: 1.148031  [ 2880/ 3200]\n",
      "loss: 1.062366  [ 2896/ 3200]\n",
      "loss: 1.031467  [ 2912/ 3200]\n",
      "loss: 1.193660  [ 2928/ 3200]\n",
      "loss: 1.192987  [ 2944/ 3200]\n",
      "loss: 1.133600  [ 2960/ 3200]\n",
      "loss: 1.248409  [ 2976/ 3200]\n",
      "loss: 1.107678  [ 2992/ 3200]\n",
      "loss: 1.273954  [ 3008/ 3200]\n",
      "loss: 1.140198  [ 3024/ 3200]\n",
      "loss: 1.017410  [ 3040/ 3200]\n",
      "loss: 1.078225  [ 3056/ 3200]\n",
      "loss: 1.164234  [ 3072/ 3200]\n",
      "loss: 0.954548  [ 3088/ 3200]\n",
      "loss: 0.966623  [ 3104/ 3200]\n",
      "loss: 1.222309  [ 3120/ 3200]\n",
      "loss: 1.190818  [ 3136/ 3200]\n",
      "loss: 1.246543  [ 3152/ 3200]\n",
      "loss: 1.135341  [ 3168/ 3200]\n",
      "loss: 1.088400  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 1.269665  [    0/ 3200]\n",
      "loss: 1.157825  [   16/ 3200]\n",
      "loss: 1.110665  [   32/ 3200]\n",
      "loss: 1.274759  [   48/ 3200]\n",
      "loss: 1.145063  [   64/ 3200]\n",
      "loss: 1.284095  [   80/ 3200]\n",
      "loss: 1.092124  [   96/ 3200]\n",
      "loss: 1.193578  [  112/ 3200]\n",
      "loss: 1.186532  [  128/ 3200]\n",
      "loss: 1.324676  [  144/ 3200]\n",
      "loss: 1.065534  [  160/ 3200]\n",
      "loss: 1.092757  [  176/ 3200]\n",
      "loss: 1.013260  [  192/ 3200]\n",
      "loss: 1.042621  [  208/ 3200]\n",
      "loss: 1.180447  [  224/ 3200]\n",
      "loss: 1.085469  [  240/ 3200]\n",
      "loss: 1.223237  [  256/ 3200]\n",
      "loss: 1.143573  [  272/ 3200]\n",
      "loss: 1.206956  [  288/ 3200]\n",
      "loss: 1.099489  [  304/ 3200]\n",
      "loss: 1.137298  [  320/ 3200]\n",
      "loss: 1.202667  [  336/ 3200]\n",
      "loss: 1.072975  [  352/ 3200]\n",
      "loss: 1.256023  [  368/ 3200]\n",
      "loss: 1.120298  [  384/ 3200]\n",
      "loss: 1.053225  [  400/ 3200]\n",
      "loss: 1.116637  [  416/ 3200]\n",
      "loss: 1.182911  [  432/ 3200]\n",
      "loss: 1.272619  [  448/ 3200]\n",
      "loss: 1.107079  [  464/ 3200]\n",
      "loss: 1.080228  [  480/ 3200]\n",
      "loss: 1.076943  [  496/ 3200]\n",
      "loss: 1.023145  [  512/ 3200]\n",
      "loss: 1.097747  [  528/ 3200]\n",
      "loss: 1.232170  [  544/ 3200]\n",
      "loss: 1.156084  [  560/ 3200]\n",
      "loss: 1.118195  [  576/ 3200]\n",
      "loss: 1.141228  [  592/ 3200]\n",
      "loss: 1.265474  [  608/ 3200]\n",
      "loss: 1.266245  [  624/ 3200]\n",
      "loss: 1.219413  [  640/ 3200]\n",
      "loss: 1.141424  [  656/ 3200]\n",
      "loss: 0.999986  [  672/ 3200]\n",
      "loss: 1.126414  [  688/ 3200]\n",
      "loss: 1.196907  [  704/ 3200]\n",
      "loss: 1.115021  [  720/ 3200]\n",
      "loss: 1.099771  [  736/ 3200]\n",
      "loss: 1.114739  [  752/ 3200]\n",
      "loss: 0.983080  [  768/ 3200]\n",
      "loss: 1.247393  [  784/ 3200]\n",
      "loss: 1.084596  [  800/ 3200]\n",
      "loss: 1.144058  [  816/ 3200]\n",
      "loss: 1.077263  [  832/ 3200]\n",
      "loss: 1.373530  [  848/ 3200]\n",
      "loss: 1.123122  [  864/ 3200]\n",
      "loss: 0.988986  [  880/ 3200]\n",
      "loss: 0.995965  [  896/ 3200]\n",
      "loss: 1.217155  [  912/ 3200]\n",
      "loss: 1.083304  [  928/ 3200]\n",
      "loss: 1.201991  [  944/ 3200]\n",
      "loss: 1.069100  [  960/ 3200]\n",
      "loss: 1.022074  [  976/ 3200]\n",
      "loss: 1.129675  [  992/ 3200]\n",
      "loss: 0.982250  [ 1008/ 3200]\n",
      "loss: 1.210546  [ 1024/ 3200]\n",
      "loss: 1.201944  [ 1040/ 3200]\n",
      "loss: 1.274508  [ 1056/ 3200]\n",
      "loss: 1.066922  [ 1072/ 3200]\n",
      "loss: 1.098119  [ 1088/ 3200]\n",
      "loss: 1.039960  [ 1104/ 3200]\n",
      "loss: 1.160363  [ 1120/ 3200]\n",
      "loss: 1.021168  [ 1136/ 3200]\n",
      "loss: 1.115207  [ 1152/ 3200]\n",
      "loss: 1.315205  [ 1168/ 3200]\n",
      "loss: 1.004109  [ 1184/ 3200]\n",
      "loss: 1.022443  [ 1200/ 3200]\n",
      "loss: 1.056885  [ 1216/ 3200]\n",
      "loss: 1.097563  [ 1232/ 3200]\n",
      "loss: 1.050154  [ 1248/ 3200]\n",
      "loss: 1.036384  [ 1264/ 3200]\n",
      "loss: 1.043518  [ 1280/ 3200]\n",
      "loss: 1.245757  [ 1296/ 3200]\n",
      "loss: 1.187597  [ 1312/ 3200]\n",
      "loss: 1.070660  [ 1328/ 3200]\n",
      "loss: 1.257583  [ 1344/ 3200]\n",
      "loss: 0.996843  [ 1360/ 3200]\n",
      "loss: 1.254423  [ 1376/ 3200]\n",
      "loss: 1.071932  [ 1392/ 3200]\n",
      "loss: 0.962139  [ 1408/ 3200]\n",
      "loss: 1.114927  [ 1424/ 3200]\n",
      "loss: 1.276517  [ 1440/ 3200]\n",
      "loss: 1.297370  [ 1456/ 3200]\n",
      "loss: 1.056557  [ 1472/ 3200]\n",
      "loss: 1.235904  [ 1488/ 3200]\n",
      "loss: 0.993493  [ 1504/ 3200]\n",
      "loss: 1.190183  [ 1520/ 3200]\n",
      "loss: 1.183614  [ 1536/ 3200]\n",
      "loss: 0.979327  [ 1552/ 3200]\n",
      "loss: 1.174329  [ 1568/ 3200]\n",
      "loss: 1.100080  [ 1584/ 3200]\n",
      "loss: 1.141215  [ 1600/ 3200]\n",
      "loss: 1.092828  [ 1616/ 3200]\n",
      "loss: 1.221433  [ 1632/ 3200]\n",
      "loss: 1.207955  [ 1648/ 3200]\n",
      "loss: 1.218774  [ 1664/ 3200]\n",
      "loss: 1.024414  [ 1680/ 3200]\n",
      "loss: 1.248607  [ 1696/ 3200]\n",
      "loss: 1.104956  [ 1712/ 3200]\n",
      "loss: 1.138098  [ 1728/ 3200]\n",
      "loss: 0.977503  [ 1744/ 3200]\n",
      "loss: 1.033609  [ 1760/ 3200]\n",
      "loss: 1.182094  [ 1776/ 3200]\n",
      "loss: 1.095491  [ 1792/ 3200]\n",
      "loss: 0.950427  [ 1808/ 3200]\n",
      "loss: 1.036511  [ 1824/ 3200]\n",
      "loss: 1.110908  [ 1840/ 3200]\n",
      "loss: 1.162051  [ 1856/ 3200]\n",
      "loss: 1.070479  [ 1872/ 3200]\n",
      "loss: 1.207134  [ 1888/ 3200]\n",
      "loss: 1.121273  [ 1904/ 3200]\n",
      "loss: 1.183160  [ 1920/ 3200]\n",
      "loss: 0.931905  [ 1936/ 3200]\n",
      "loss: 1.284997  [ 1952/ 3200]\n",
      "loss: 1.154463  [ 1968/ 3200]\n",
      "loss: 1.105442  [ 1984/ 3200]\n",
      "loss: 1.109342  [ 2000/ 3200]\n",
      "loss: 1.284539  [ 2016/ 3200]\n",
      "loss: 1.236333  [ 2032/ 3200]\n",
      "loss: 1.106542  [ 2048/ 3200]\n",
      "loss: 1.173292  [ 2064/ 3200]\n",
      "loss: 1.164300  [ 2080/ 3200]\n",
      "loss: 1.293440  [ 2096/ 3200]\n",
      "loss: 1.190548  [ 2112/ 3200]\n",
      "loss: 1.238451  [ 2128/ 3200]\n",
      "loss: 1.315227  [ 2144/ 3200]\n",
      "loss: 1.112265  [ 2160/ 3200]\n",
      "loss: 1.150491  [ 2176/ 3200]\n",
      "loss: 1.152761  [ 2192/ 3200]\n",
      "loss: 1.140193  [ 2208/ 3200]\n",
      "loss: 1.174370  [ 2224/ 3200]\n",
      "loss: 1.034150  [ 2240/ 3200]\n",
      "loss: 1.010776  [ 2256/ 3200]\n",
      "loss: 1.072298  [ 2272/ 3200]\n",
      "loss: 1.397610  [ 2288/ 3200]\n",
      "loss: 1.064749  [ 2304/ 3200]\n",
      "loss: 1.108270  [ 2320/ 3200]\n",
      "loss: 1.153758  [ 2336/ 3200]\n",
      "loss: 1.136481  [ 2352/ 3200]\n",
      "loss: 1.113766  [ 2368/ 3200]\n",
      "loss: 1.235866  [ 2384/ 3200]\n",
      "loss: 1.078299  [ 2400/ 3200]\n",
      "loss: 0.946110  [ 2416/ 3200]\n",
      "loss: 1.357293  [ 2432/ 3200]\n",
      "loss: 1.123792  [ 2448/ 3200]\n",
      "loss: 1.030772  [ 2464/ 3200]\n",
      "loss: 1.030243  [ 2480/ 3200]\n",
      "loss: 1.260885  [ 2496/ 3200]\n",
      "loss: 0.990478  [ 2512/ 3200]\n",
      "loss: 1.282095  [ 2528/ 3200]\n",
      "loss: 1.107989  [ 2544/ 3200]\n",
      "loss: 0.990443  [ 2560/ 3200]\n",
      "loss: 1.159791  [ 2576/ 3200]\n",
      "loss: 1.035725  [ 2592/ 3200]\n",
      "loss: 1.173686  [ 2608/ 3200]\n",
      "loss: 1.266662  [ 2624/ 3200]\n",
      "loss: 0.999907  [ 2640/ 3200]\n",
      "loss: 1.128268  [ 2656/ 3200]\n",
      "loss: 1.272036  [ 2672/ 3200]\n",
      "loss: 1.269798  [ 2688/ 3200]\n",
      "loss: 1.025385  [ 2704/ 3200]\n",
      "loss: 1.140277  [ 2720/ 3200]\n",
      "loss: 1.151056  [ 2736/ 3200]\n",
      "loss: 1.205834  [ 2752/ 3200]\n",
      "loss: 1.204503  [ 2768/ 3200]\n",
      "loss: 1.092866  [ 2784/ 3200]\n",
      "loss: 1.142841  [ 2800/ 3200]\n",
      "loss: 1.207808  [ 2816/ 3200]\n",
      "loss: 1.160459  [ 2832/ 3200]\n",
      "loss: 1.276042  [ 2848/ 3200]\n",
      "loss: 0.899749  [ 2864/ 3200]\n",
      "loss: 1.213215  [ 2880/ 3200]\n",
      "loss: 1.171835  [ 2896/ 3200]\n",
      "loss: 1.118756  [ 2912/ 3200]\n",
      "loss: 1.170781  [ 2928/ 3200]\n",
      "loss: 1.108170  [ 2944/ 3200]\n",
      "loss: 1.391660  [ 2960/ 3200]\n",
      "loss: 1.105208  [ 2976/ 3200]\n",
      "loss: 1.244256  [ 2992/ 3200]\n",
      "loss: 0.974655  [ 3008/ 3200]\n",
      "loss: 0.939078  [ 3024/ 3200]\n",
      "loss: 1.273090  [ 3040/ 3200]\n",
      "loss: 1.208173  [ 3056/ 3200]\n",
      "loss: 1.230001  [ 3072/ 3200]\n",
      "loss: 1.040225  [ 3088/ 3200]\n",
      "loss: 1.148738  [ 3104/ 3200]\n",
      "loss: 1.204316  [ 3120/ 3200]\n",
      "loss: 1.153224  [ 3136/ 3200]\n",
      "loss: 1.157252  [ 3152/ 3200]\n",
      "loss: 1.202896  [ 3168/ 3200]\n",
      "loss: 1.160517  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 1.034133  [    0/ 3200]\n",
      "loss: 1.233960  [   16/ 3200]\n",
      "loss: 1.083513  [   32/ 3200]\n",
      "loss: 1.003022  [   48/ 3200]\n",
      "loss: 1.040386  [   64/ 3200]\n",
      "loss: 1.134939  [   80/ 3200]\n",
      "loss: 1.078149  [   96/ 3200]\n",
      "loss: 1.040696  [  112/ 3200]\n",
      "loss: 0.984702  [  128/ 3200]\n",
      "loss: 1.177092  [  144/ 3200]\n",
      "loss: 1.163267  [  160/ 3200]\n",
      "loss: 1.210740  [  176/ 3200]\n",
      "loss: 1.106183  [  192/ 3200]\n",
      "loss: 1.110987  [  208/ 3200]\n",
      "loss: 1.214664  [  224/ 3200]\n",
      "loss: 1.051546  [  240/ 3200]\n",
      "loss: 1.185522  [  256/ 3200]\n",
      "loss: 1.117069  [  272/ 3200]\n",
      "loss: 1.090879  [  288/ 3200]\n",
      "loss: 0.904509  [  304/ 3200]\n",
      "loss: 1.112630  [  320/ 3200]\n",
      "loss: 1.051781  [  336/ 3200]\n",
      "loss: 1.325406  [  352/ 3200]\n",
      "loss: 0.979854  [  368/ 3200]\n",
      "loss: 1.153660  [  384/ 3200]\n",
      "loss: 1.275169  [  400/ 3200]\n",
      "loss: 1.173459  [  416/ 3200]\n",
      "loss: 1.223960  [  432/ 3200]\n",
      "loss: 1.133823  [  448/ 3200]\n",
      "loss: 1.250953  [  464/ 3200]\n",
      "loss: 1.370682  [  480/ 3200]\n",
      "loss: 1.206875  [  496/ 3200]\n",
      "loss: 1.031795  [  512/ 3200]\n",
      "loss: 1.215760  [  528/ 3200]\n",
      "loss: 1.020673  [  544/ 3200]\n",
      "loss: 1.189710  [  560/ 3200]\n",
      "loss: 1.201787  [  576/ 3200]\n",
      "loss: 1.032831  [  592/ 3200]\n",
      "loss: 1.047293  [  608/ 3200]\n",
      "loss: 1.100674  [  624/ 3200]\n",
      "loss: 1.205113  [  640/ 3200]\n",
      "loss: 1.135012  [  656/ 3200]\n",
      "loss: 1.214382  [  672/ 3200]\n",
      "loss: 1.046878  [  688/ 3200]\n",
      "loss: 1.162108  [  704/ 3200]\n",
      "loss: 1.061910  [  720/ 3200]\n",
      "loss: 1.065917  [  736/ 3200]\n",
      "loss: 1.104133  [  752/ 3200]\n",
      "loss: 1.050563  [  768/ 3200]\n",
      "loss: 1.138180  [  784/ 3200]\n",
      "loss: 1.231751  [  800/ 3200]\n",
      "loss: 1.220975  [  816/ 3200]\n",
      "loss: 1.082096  [  832/ 3200]\n",
      "loss: 1.166618  [  848/ 3200]\n",
      "loss: 1.022651  [  864/ 3200]\n",
      "loss: 1.135959  [  880/ 3200]\n",
      "loss: 1.272187  [  896/ 3200]\n",
      "loss: 1.113571  [  912/ 3200]\n",
      "loss: 1.105782  [  928/ 3200]\n",
      "loss: 1.212533  [  944/ 3200]\n",
      "loss: 1.001097  [  960/ 3200]\n",
      "loss: 1.122436  [  976/ 3200]\n",
      "loss: 1.203048  [  992/ 3200]\n",
      "loss: 1.158308  [ 1008/ 3200]\n",
      "loss: 0.919226  [ 1024/ 3200]\n",
      "loss: 1.115648  [ 1040/ 3200]\n",
      "loss: 1.231731  [ 1056/ 3200]\n",
      "loss: 1.174531  [ 1072/ 3200]\n",
      "loss: 1.041533  [ 1088/ 3200]\n",
      "loss: 1.205148  [ 1104/ 3200]\n",
      "loss: 1.335574  [ 1120/ 3200]\n",
      "loss: 1.058354  [ 1136/ 3200]\n",
      "loss: 1.093189  [ 1152/ 3200]\n",
      "loss: 1.082477  [ 1168/ 3200]\n",
      "loss: 1.086839  [ 1184/ 3200]\n",
      "loss: 1.081226  [ 1200/ 3200]\n",
      "loss: 1.320244  [ 1216/ 3200]\n",
      "loss: 1.119268  [ 1232/ 3200]\n",
      "loss: 1.134233  [ 1248/ 3200]\n",
      "loss: 1.212261  [ 1264/ 3200]\n",
      "loss: 1.021711  [ 1280/ 3200]\n",
      "loss: 1.006516  [ 1296/ 3200]\n",
      "loss: 1.110101  [ 1312/ 3200]\n",
      "loss: 1.328161  [ 1328/ 3200]\n",
      "loss: 1.236677  [ 1344/ 3200]\n",
      "loss: 1.146116  [ 1360/ 3200]\n",
      "loss: 1.182685  [ 1376/ 3200]\n",
      "loss: 1.017629  [ 1392/ 3200]\n",
      "loss: 1.240877  [ 1408/ 3200]\n",
      "loss: 1.152266  [ 1424/ 3200]\n",
      "loss: 1.005445  [ 1440/ 3200]\n",
      "loss: 1.097153  [ 1456/ 3200]\n",
      "loss: 0.947325  [ 1472/ 3200]\n",
      "loss: 1.070214  [ 1488/ 3200]\n",
      "loss: 1.144411  [ 1504/ 3200]\n",
      "loss: 1.123849  [ 1520/ 3200]\n",
      "loss: 1.041109  [ 1536/ 3200]\n",
      "loss: 1.072078  [ 1552/ 3200]\n",
      "loss: 1.063131  [ 1568/ 3200]\n",
      "loss: 1.076613  [ 1584/ 3200]\n",
      "loss: 0.988596  [ 1600/ 3200]\n",
      "loss: 1.035890  [ 1616/ 3200]\n",
      "loss: 1.053154  [ 1632/ 3200]\n",
      "loss: 1.216380  [ 1648/ 3200]\n",
      "loss: 1.150343  [ 1664/ 3200]\n",
      "loss: 1.200346  [ 1680/ 3200]\n",
      "loss: 1.221955  [ 1696/ 3200]\n",
      "loss: 1.101966  [ 1712/ 3200]\n",
      "loss: 1.194503  [ 1728/ 3200]\n",
      "loss: 1.064927  [ 1744/ 3200]\n",
      "loss: 1.061464  [ 1760/ 3200]\n",
      "loss: 1.085804  [ 1776/ 3200]\n",
      "loss: 1.164955  [ 1792/ 3200]\n",
      "loss: 1.004359  [ 1808/ 3200]\n",
      "loss: 1.120528  [ 1824/ 3200]\n",
      "loss: 1.115211  [ 1840/ 3200]\n",
      "loss: 1.068929  [ 1856/ 3200]\n",
      "loss: 1.097013  [ 1872/ 3200]\n",
      "loss: 1.207252  [ 1888/ 3200]\n",
      "loss: 1.117137  [ 1904/ 3200]\n",
      "loss: 1.076409  [ 1920/ 3200]\n",
      "loss: 1.130408  [ 1936/ 3200]\n",
      "loss: 1.099450  [ 1952/ 3200]\n",
      "loss: 1.020808  [ 1968/ 3200]\n",
      "loss: 0.953473  [ 1984/ 3200]\n",
      "loss: 1.215347  [ 2000/ 3200]\n",
      "loss: 1.103409  [ 2016/ 3200]\n",
      "loss: 1.110614  [ 2032/ 3200]\n",
      "loss: 1.252707  [ 2048/ 3200]\n",
      "loss: 1.052010  [ 2064/ 3200]\n",
      "loss: 1.079892  [ 2080/ 3200]\n",
      "loss: 1.086796  [ 2096/ 3200]\n",
      "loss: 1.093293  [ 2112/ 3200]\n",
      "loss: 1.153252  [ 2128/ 3200]\n",
      "loss: 0.889341  [ 2144/ 3200]\n",
      "loss: 0.966833  [ 2160/ 3200]\n",
      "loss: 1.199830  [ 2176/ 3200]\n",
      "loss: 1.105154  [ 2192/ 3200]\n",
      "loss: 1.090526  [ 2208/ 3200]\n",
      "loss: 1.109410  [ 2224/ 3200]\n",
      "loss: 1.076352  [ 2240/ 3200]\n",
      "loss: 1.036705  [ 2256/ 3200]\n",
      "loss: 1.043342  [ 2272/ 3200]\n",
      "loss: 1.197095  [ 2288/ 3200]\n",
      "loss: 1.170913  [ 2304/ 3200]\n",
      "loss: 1.234403  [ 2320/ 3200]\n",
      "loss: 1.041042  [ 2336/ 3200]\n",
      "loss: 1.040505  [ 2352/ 3200]\n",
      "loss: 1.153062  [ 2368/ 3200]\n",
      "loss: 1.206286  [ 2384/ 3200]\n",
      "loss: 0.977835  [ 2400/ 3200]\n",
      "loss: 1.172163  [ 2416/ 3200]\n",
      "loss: 1.220218  [ 2432/ 3200]\n",
      "loss: 1.125984  [ 2448/ 3200]\n",
      "loss: 1.088809  [ 2464/ 3200]\n",
      "loss: 1.006618  [ 2480/ 3200]\n",
      "loss: 1.076147  [ 2496/ 3200]\n",
      "loss: 1.135190  [ 2512/ 3200]\n",
      "loss: 1.178352  [ 2528/ 3200]\n",
      "loss: 1.022147  [ 2544/ 3200]\n",
      "loss: 1.159069  [ 2560/ 3200]\n",
      "loss: 1.130779  [ 2576/ 3200]\n",
      "loss: 1.171101  [ 2592/ 3200]\n",
      "loss: 1.036353  [ 2608/ 3200]\n",
      "loss: 1.065896  [ 2624/ 3200]\n",
      "loss: 0.990139  [ 2640/ 3200]\n",
      "loss: 1.065683  [ 2656/ 3200]\n",
      "loss: 1.076922  [ 2672/ 3200]\n",
      "loss: 1.220338  [ 2688/ 3200]\n",
      "loss: 1.112771  [ 2704/ 3200]\n",
      "loss: 1.345658  [ 2720/ 3200]\n",
      "loss: 1.355348  [ 2736/ 3200]\n",
      "loss: 1.102599  [ 2752/ 3200]\n",
      "loss: 1.153514  [ 2768/ 3200]\n",
      "loss: 0.952912  [ 2784/ 3200]\n",
      "loss: 1.255345  [ 2800/ 3200]\n",
      "loss: 1.006211  [ 2816/ 3200]\n",
      "loss: 1.211077  [ 2832/ 3200]\n",
      "loss: 0.950878  [ 2848/ 3200]\n",
      "loss: 1.120352  [ 2864/ 3200]\n",
      "loss: 1.083844  [ 2880/ 3200]\n",
      "loss: 1.050626  [ 2896/ 3200]\n",
      "loss: 1.206507  [ 2912/ 3200]\n",
      "loss: 1.039503  [ 2928/ 3200]\n",
      "loss: 1.039066  [ 2944/ 3200]\n",
      "loss: 0.977287  [ 2960/ 3200]\n",
      "loss: 1.183736  [ 2976/ 3200]\n",
      "loss: 1.235602  [ 2992/ 3200]\n",
      "loss: 0.997228  [ 3008/ 3200]\n",
      "loss: 1.130956  [ 3024/ 3200]\n",
      "loss: 1.030772  [ 3040/ 3200]\n",
      "loss: 1.208492  [ 3056/ 3200]\n",
      "loss: 0.962367  [ 3072/ 3200]\n",
      "loss: 0.943185  [ 3088/ 3200]\n",
      "loss: 1.099135  [ 3104/ 3200]\n",
      "loss: 1.278233  [ 3120/ 3200]\n",
      "loss: 1.330307  [ 3136/ 3200]\n",
      "loss: 1.155817  [ 3152/ 3200]\n",
      "loss: 0.977890  [ 3168/ 3200]\n",
      "loss: 1.156312  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 1.133996  [    0/ 3200]\n",
      "loss: 1.113551  [   16/ 3200]\n",
      "loss: 0.986300  [   32/ 3200]\n",
      "loss: 1.051597  [   48/ 3200]\n",
      "loss: 1.063705  [   64/ 3200]\n",
      "loss: 1.423679  [   80/ 3200]\n",
      "loss: 1.067392  [   96/ 3200]\n",
      "loss: 1.265554  [  112/ 3200]\n",
      "loss: 1.210092  [  128/ 3200]\n",
      "loss: 1.143309  [  144/ 3200]\n",
      "loss: 1.144641  [  160/ 3200]\n",
      "loss: 1.065684  [  176/ 3200]\n",
      "loss: 1.169483  [  192/ 3200]\n",
      "loss: 0.970483  [  208/ 3200]\n",
      "loss: 1.092659  [  224/ 3200]\n",
      "loss: 1.094189  [  240/ 3200]\n",
      "loss: 0.970487  [  256/ 3200]\n",
      "loss: 1.188105  [  272/ 3200]\n",
      "loss: 0.890301  [  288/ 3200]\n",
      "loss: 1.262061  [  304/ 3200]\n",
      "loss: 1.155639  [  320/ 3200]\n",
      "loss: 1.112282  [  336/ 3200]\n",
      "loss: 1.148243  [  352/ 3200]\n",
      "loss: 1.141889  [  368/ 3200]\n",
      "loss: 0.959036  [  384/ 3200]\n",
      "loss: 1.223554  [  400/ 3200]\n",
      "loss: 1.085510  [  416/ 3200]\n",
      "loss: 1.013430  [  432/ 3200]\n",
      "loss: 1.055273  [  448/ 3200]\n",
      "loss: 1.058595  [  464/ 3200]\n",
      "loss: 1.105448  [  480/ 3200]\n",
      "loss: 1.251631  [  496/ 3200]\n",
      "loss: 1.126151  [  512/ 3200]\n",
      "loss: 1.112041  [  528/ 3200]\n",
      "loss: 1.054085  [  544/ 3200]\n",
      "loss: 1.021233  [  560/ 3200]\n",
      "loss: 1.154297  [  576/ 3200]\n",
      "loss: 1.130783  [  592/ 3200]\n",
      "loss: 0.956345  [  608/ 3200]\n",
      "loss: 1.135697  [  624/ 3200]\n",
      "loss: 1.173205  [  640/ 3200]\n",
      "loss: 1.114094  [  656/ 3200]\n",
      "loss: 1.033488  [  672/ 3200]\n",
      "loss: 1.194364  [  688/ 3200]\n",
      "loss: 1.099246  [  704/ 3200]\n",
      "loss: 1.175110  [  720/ 3200]\n",
      "loss: 1.200926  [  736/ 3200]\n",
      "loss: 1.090628  [  752/ 3200]\n",
      "loss: 1.028397  [  768/ 3200]\n",
      "loss: 1.274728  [  784/ 3200]\n",
      "loss: 1.246077  [  800/ 3200]\n",
      "loss: 1.233393  [  816/ 3200]\n",
      "loss: 1.151671  [  832/ 3200]\n",
      "loss: 0.988600  [  848/ 3200]\n",
      "loss: 0.989457  [  864/ 3200]\n",
      "loss: 1.100109  [  880/ 3200]\n",
      "loss: 1.258370  [  896/ 3200]\n",
      "loss: 1.212566  [  912/ 3200]\n",
      "loss: 1.144238  [  928/ 3200]\n",
      "loss: 1.223478  [  944/ 3200]\n",
      "loss: 1.064512  [  960/ 3200]\n",
      "loss: 1.152189  [  976/ 3200]\n",
      "loss: 1.174379  [  992/ 3200]\n",
      "loss: 1.003791  [ 1008/ 3200]\n",
      "loss: 1.084027  [ 1024/ 3200]\n",
      "loss: 0.993189  [ 1040/ 3200]\n",
      "loss: 1.028554  [ 1056/ 3200]\n",
      "loss: 1.046228  [ 1072/ 3200]\n",
      "loss: 1.114059  [ 1088/ 3200]\n",
      "loss: 1.254341  [ 1104/ 3200]\n",
      "loss: 1.122750  [ 1120/ 3200]\n",
      "loss: 0.989520  [ 1136/ 3200]\n",
      "loss: 1.124840  [ 1152/ 3200]\n",
      "loss: 1.132868  [ 1168/ 3200]\n",
      "loss: 1.042206  [ 1184/ 3200]\n",
      "loss: 1.152371  [ 1200/ 3200]\n",
      "loss: 1.050185  [ 1216/ 3200]\n",
      "loss: 1.106764  [ 1232/ 3200]\n",
      "loss: 1.103759  [ 1248/ 3200]\n",
      "loss: 1.137826  [ 1264/ 3200]\n",
      "loss: 1.126771  [ 1280/ 3200]\n",
      "loss: 1.228709  [ 1296/ 3200]\n",
      "loss: 1.237764  [ 1312/ 3200]\n",
      "loss: 1.168615  [ 1328/ 3200]\n",
      "loss: 1.105373  [ 1344/ 3200]\n",
      "loss: 1.292556  [ 1360/ 3200]\n",
      "loss: 0.964468  [ 1376/ 3200]\n",
      "loss: 1.092790  [ 1392/ 3200]\n",
      "loss: 1.254889  [ 1408/ 3200]\n",
      "loss: 1.140188  [ 1424/ 3200]\n",
      "loss: 1.268811  [ 1440/ 3200]\n",
      "loss: 1.122977  [ 1456/ 3200]\n",
      "loss: 1.084886  [ 1472/ 3200]\n",
      "loss: 1.123062  [ 1488/ 3200]\n",
      "loss: 1.250427  [ 1504/ 3200]\n",
      "loss: 1.163573  [ 1520/ 3200]\n",
      "loss: 1.232445  [ 1536/ 3200]\n",
      "loss: 1.096198  [ 1552/ 3200]\n",
      "loss: 1.133726  [ 1568/ 3200]\n",
      "loss: 1.027141  [ 1584/ 3200]\n",
      "loss: 1.022928  [ 1600/ 3200]\n",
      "loss: 0.981764  [ 1616/ 3200]\n",
      "loss: 1.118008  [ 1632/ 3200]\n",
      "loss: 1.018506  [ 1648/ 3200]\n",
      "loss: 1.111197  [ 1664/ 3200]\n",
      "loss: 1.161668  [ 1680/ 3200]\n",
      "loss: 1.026803  [ 1696/ 3200]\n",
      "loss: 0.974877  [ 1712/ 3200]\n",
      "loss: 0.979111  [ 1728/ 3200]\n",
      "loss: 1.236714  [ 1744/ 3200]\n",
      "loss: 0.834647  [ 1760/ 3200]\n",
      "loss: 0.894012  [ 1776/ 3200]\n",
      "loss: 0.925228  [ 1792/ 3200]\n",
      "loss: 1.023934  [ 1808/ 3200]\n",
      "loss: 1.341053  [ 1824/ 3200]\n",
      "loss: 1.220206  [ 1840/ 3200]\n",
      "loss: 1.036999  [ 1856/ 3200]\n",
      "loss: 1.203073  [ 1872/ 3200]\n",
      "loss: 0.967542  [ 1888/ 3200]\n",
      "loss: 1.079642  [ 1904/ 3200]\n",
      "loss: 1.007772  [ 1920/ 3200]\n",
      "loss: 1.037866  [ 1936/ 3200]\n",
      "loss: 0.926503  [ 1952/ 3200]\n",
      "loss: 1.170323  [ 1968/ 3200]\n",
      "loss: 1.197559  [ 1984/ 3200]\n",
      "loss: 1.162994  [ 2000/ 3200]\n",
      "loss: 1.112841  [ 2016/ 3200]\n",
      "loss: 1.108667  [ 2032/ 3200]\n",
      "loss: 1.316445  [ 2048/ 3200]\n",
      "loss: 0.808590  [ 2064/ 3200]\n",
      "loss: 1.196547  [ 2080/ 3200]\n",
      "loss: 0.994421  [ 2096/ 3200]\n",
      "loss: 1.102446  [ 2112/ 3200]\n",
      "loss: 1.048768  [ 2128/ 3200]\n",
      "loss: 1.237309  [ 2144/ 3200]\n",
      "loss: 1.179067  [ 2160/ 3200]\n",
      "loss: 0.989045  [ 2176/ 3200]\n",
      "loss: 0.950429  [ 2192/ 3200]\n",
      "loss: 1.084150  [ 2208/ 3200]\n",
      "loss: 0.986509  [ 2224/ 3200]\n",
      "loss: 0.988465  [ 2240/ 3200]\n",
      "loss: 1.218800  [ 2256/ 3200]\n",
      "loss: 1.103475  [ 2272/ 3200]\n",
      "loss: 1.086672  [ 2288/ 3200]\n",
      "loss: 1.113443  [ 2304/ 3200]\n",
      "loss: 1.003739  [ 2320/ 3200]\n",
      "loss: 1.033561  [ 2336/ 3200]\n",
      "loss: 1.117173  [ 2352/ 3200]\n",
      "loss: 1.116102  [ 2368/ 3200]\n",
      "loss: 1.086944  [ 2384/ 3200]\n",
      "loss: 0.888457  [ 2400/ 3200]\n",
      "loss: 1.086172  [ 2416/ 3200]\n",
      "loss: 1.002293  [ 2432/ 3200]\n",
      "loss: 0.982850  [ 2448/ 3200]\n",
      "loss: 1.150133  [ 2464/ 3200]\n",
      "loss: 0.939730  [ 2480/ 3200]\n",
      "loss: 1.216178  [ 2496/ 3200]\n",
      "loss: 1.108757  [ 2512/ 3200]\n",
      "loss: 1.105398  [ 2528/ 3200]\n",
      "loss: 1.116364  [ 2544/ 3200]\n",
      "loss: 1.017876  [ 2560/ 3200]\n",
      "loss: 1.031953  [ 2576/ 3200]\n",
      "loss: 1.082386  [ 2592/ 3200]\n",
      "loss: 1.248390  [ 2608/ 3200]\n",
      "loss: 1.146514  [ 2624/ 3200]\n",
      "loss: 1.072929  [ 2640/ 3200]\n",
      "loss: 1.183593  [ 2656/ 3200]\n",
      "loss: 1.005948  [ 2672/ 3200]\n",
      "loss: 1.020674  [ 2688/ 3200]\n",
      "loss: 0.986257  [ 2704/ 3200]\n",
      "loss: 1.043648  [ 2720/ 3200]\n",
      "loss: 1.014563  [ 2736/ 3200]\n",
      "loss: 0.965555  [ 2752/ 3200]\n",
      "loss: 0.972219  [ 2768/ 3200]\n",
      "loss: 1.197125  [ 2784/ 3200]\n",
      "loss: 1.000247  [ 2800/ 3200]\n",
      "loss: 1.098391  [ 2816/ 3200]\n",
      "loss: 1.147783  [ 2832/ 3200]\n",
      "loss: 0.999772  [ 2848/ 3200]\n",
      "loss: 1.221213  [ 2864/ 3200]\n",
      "loss: 1.059747  [ 2880/ 3200]\n",
      "loss: 0.934179  [ 2896/ 3200]\n",
      "loss: 1.049934  [ 2912/ 3200]\n",
      "loss: 1.093293  [ 2928/ 3200]\n",
      "loss: 0.962367  [ 2944/ 3200]\n",
      "loss: 1.041967  [ 2960/ 3200]\n",
      "loss: 1.024438  [ 2976/ 3200]\n",
      "loss: 1.097065  [ 2992/ 3200]\n",
      "loss: 1.334762  [ 3008/ 3200]\n",
      "loss: 1.035504  [ 3024/ 3200]\n",
      "loss: 1.198680  [ 3040/ 3200]\n",
      "loss: 1.082423  [ 3056/ 3200]\n",
      "loss: 0.887598  [ 3072/ 3200]\n",
      "loss: 1.372363  [ 3088/ 3200]\n",
      "loss: 1.199941  [ 3104/ 3200]\n",
      "loss: 1.049884  [ 3120/ 3200]\n",
      "loss: 1.128219  [ 3136/ 3200]\n",
      "loss: 1.020222  [ 3152/ 3200]\n",
      "loss: 1.208743  [ 3168/ 3200]\n",
      "loss: 1.270792  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 1.062153  [    0/ 3200]\n",
      "loss: 0.921099  [   16/ 3200]\n",
      "loss: 1.165591  [   32/ 3200]\n",
      "loss: 1.268475  [   48/ 3200]\n",
      "loss: 1.162152  [   64/ 3200]\n",
      "loss: 1.195828  [   80/ 3200]\n",
      "loss: 1.129389  [   96/ 3200]\n",
      "loss: 1.189645  [  112/ 3200]\n",
      "loss: 1.125524  [  128/ 3200]\n",
      "loss: 1.060121  [  144/ 3200]\n",
      "loss: 0.962041  [  160/ 3200]\n",
      "loss: 1.293802  [  176/ 3200]\n",
      "loss: 1.011961  [  192/ 3200]\n",
      "loss: 1.023163  [  208/ 3200]\n",
      "loss: 1.040752  [  224/ 3200]\n",
      "loss: 0.941848  [  240/ 3200]\n",
      "loss: 1.266518  [  256/ 3200]\n",
      "loss: 1.205271  [  272/ 3200]\n",
      "loss: 1.204516  [  288/ 3200]\n",
      "loss: 1.159379  [  304/ 3200]\n",
      "loss: 1.008991  [  320/ 3200]\n",
      "loss: 1.095314  [  336/ 3200]\n",
      "loss: 1.186075  [  352/ 3200]\n",
      "loss: 1.016690  [  368/ 3200]\n",
      "loss: 0.984946  [  384/ 3200]\n",
      "loss: 1.138422  [  400/ 3200]\n",
      "loss: 1.148881  [  416/ 3200]\n",
      "loss: 0.995158  [  432/ 3200]\n",
      "loss: 0.944411  [  448/ 3200]\n",
      "loss: 0.924401  [  464/ 3200]\n",
      "loss: 0.897473  [  480/ 3200]\n",
      "loss: 0.757970  [  496/ 3200]\n",
      "loss: 0.887939  [  512/ 3200]\n",
      "loss: 1.128449  [  528/ 3200]\n",
      "loss: 1.042007  [  544/ 3200]\n",
      "loss: 0.975758  [  560/ 3200]\n",
      "loss: 0.895446  [  576/ 3200]\n",
      "loss: 1.092338  [  592/ 3200]\n",
      "loss: 1.120024  [  608/ 3200]\n",
      "loss: 0.945080  [  624/ 3200]\n",
      "loss: 1.057225  [  640/ 3200]\n",
      "loss: 1.007088  [  656/ 3200]\n",
      "loss: 1.077906  [  672/ 3200]\n",
      "loss: 1.050482  [  688/ 3200]\n",
      "loss: 1.075582  [  704/ 3200]\n",
      "loss: 1.205250  [  720/ 3200]\n",
      "loss: 1.067136  [  736/ 3200]\n",
      "loss: 1.093886  [  752/ 3200]\n",
      "loss: 0.937895  [  768/ 3200]\n",
      "loss: 1.070456  [  784/ 3200]\n",
      "loss: 1.073612  [  800/ 3200]\n",
      "loss: 1.099207  [  816/ 3200]\n",
      "loss: 1.257799  [  832/ 3200]\n",
      "loss: 1.024256  [  848/ 3200]\n",
      "loss: 0.993050  [  864/ 3200]\n",
      "loss: 0.969333  [  880/ 3200]\n",
      "loss: 1.001366  [  896/ 3200]\n",
      "loss: 1.079683  [  912/ 3200]\n",
      "loss: 1.119663  [  928/ 3200]\n",
      "loss: 1.166419  [  944/ 3200]\n",
      "loss: 1.265593  [  960/ 3200]\n",
      "loss: 0.884465  [  976/ 3200]\n",
      "loss: 1.169128  [  992/ 3200]\n",
      "loss: 1.143963  [ 1008/ 3200]\n",
      "loss: 1.247729  [ 1024/ 3200]\n",
      "loss: 1.154671  [ 1040/ 3200]\n",
      "loss: 0.965491  [ 1056/ 3200]\n",
      "loss: 1.106612  [ 1072/ 3200]\n",
      "loss: 1.077955  [ 1088/ 3200]\n",
      "loss: 1.087410  [ 1104/ 3200]\n",
      "loss: 1.179444  [ 1120/ 3200]\n",
      "loss: 1.058033  [ 1136/ 3200]\n",
      "loss: 1.274992  [ 1152/ 3200]\n",
      "loss: 1.232633  [ 1168/ 3200]\n",
      "loss: 1.129977  [ 1184/ 3200]\n",
      "loss: 1.145274  [ 1200/ 3200]\n",
      "loss: 1.281454  [ 1216/ 3200]\n",
      "loss: 0.933471  [ 1232/ 3200]\n",
      "loss: 1.050550  [ 1248/ 3200]\n",
      "loss: 1.053756  [ 1264/ 3200]\n",
      "loss: 1.135104  [ 1280/ 3200]\n",
      "loss: 1.018124  [ 1296/ 3200]\n",
      "loss: 0.909095  [ 1312/ 3200]\n",
      "loss: 1.349710  [ 1328/ 3200]\n",
      "loss: 0.933459  [ 1344/ 3200]\n",
      "loss: 0.928011  [ 1360/ 3200]\n",
      "loss: 1.008711  [ 1376/ 3200]\n",
      "loss: 1.167331  [ 1392/ 3200]\n",
      "loss: 0.994253  [ 1408/ 3200]\n",
      "loss: 1.260924  [ 1424/ 3200]\n",
      "loss: 0.993422  [ 1440/ 3200]\n",
      "loss: 1.149477  [ 1456/ 3200]\n",
      "loss: 1.236787  [ 1472/ 3200]\n",
      "loss: 1.035323  [ 1488/ 3200]\n",
      "loss: 1.227616  [ 1504/ 3200]\n",
      "loss: 1.167401  [ 1520/ 3200]\n",
      "loss: 0.992462  [ 1536/ 3200]\n",
      "loss: 1.287306  [ 1552/ 3200]\n",
      "loss: 0.924467  [ 1568/ 3200]\n",
      "loss: 1.092351  [ 1584/ 3200]\n",
      "loss: 1.022954  [ 1600/ 3200]\n",
      "loss: 1.073226  [ 1616/ 3200]\n",
      "loss: 1.062835  [ 1632/ 3200]\n",
      "loss: 1.046344  [ 1648/ 3200]\n",
      "loss: 1.036846  [ 1664/ 3200]\n",
      "loss: 1.046062  [ 1680/ 3200]\n",
      "loss: 1.163132  [ 1696/ 3200]\n",
      "loss: 0.922221  [ 1712/ 3200]\n",
      "loss: 1.230931  [ 1728/ 3200]\n",
      "loss: 1.144263  [ 1744/ 3200]\n",
      "loss: 1.050937  [ 1760/ 3200]\n",
      "loss: 1.436563  [ 1776/ 3200]\n",
      "loss: 1.098710  [ 1792/ 3200]\n",
      "loss: 0.988258  [ 1808/ 3200]\n",
      "loss: 1.035486  [ 1824/ 3200]\n",
      "loss: 0.983258  [ 1840/ 3200]\n",
      "loss: 0.936410  [ 1856/ 3200]\n",
      "loss: 1.196758  [ 1872/ 3200]\n",
      "loss: 0.963087  [ 1888/ 3200]\n",
      "loss: 1.311241  [ 1904/ 3200]\n",
      "loss: 1.026354  [ 1920/ 3200]\n",
      "loss: 1.146689  [ 1936/ 3200]\n",
      "loss: 1.051886  [ 1952/ 3200]\n",
      "loss: 1.057121  [ 1968/ 3200]\n",
      "loss: 1.050770  [ 1984/ 3200]\n",
      "loss: 1.158370  [ 2000/ 3200]\n",
      "loss: 1.173469  [ 2016/ 3200]\n",
      "loss: 1.092223  [ 2032/ 3200]\n",
      "loss: 1.064821  [ 2048/ 3200]\n",
      "loss: 1.078604  [ 2064/ 3200]\n",
      "loss: 1.075394  [ 2080/ 3200]\n",
      "loss: 1.157678  [ 2096/ 3200]\n",
      "loss: 1.192322  [ 2112/ 3200]\n",
      "loss: 0.960766  [ 2128/ 3200]\n",
      "loss: 0.966778  [ 2144/ 3200]\n",
      "loss: 0.994114  [ 2160/ 3200]\n",
      "loss: 1.264548  [ 2176/ 3200]\n",
      "loss: 0.918539  [ 2192/ 3200]\n",
      "loss: 1.134076  [ 2208/ 3200]\n",
      "loss: 1.170726  [ 2224/ 3200]\n",
      "loss: 0.922420  [ 2240/ 3200]\n",
      "loss: 0.898210  [ 2256/ 3200]\n",
      "loss: 0.987720  [ 2272/ 3200]\n",
      "loss: 1.012942  [ 2288/ 3200]\n",
      "loss: 1.160306  [ 2304/ 3200]\n",
      "loss: 0.969954  [ 2320/ 3200]\n",
      "loss: 1.008560  [ 2336/ 3200]\n",
      "loss: 1.095987  [ 2352/ 3200]\n",
      "loss: 0.938520  [ 2368/ 3200]\n",
      "loss: 1.179494  [ 2384/ 3200]\n",
      "loss: 1.124739  [ 2400/ 3200]\n",
      "loss: 1.109775  [ 2416/ 3200]\n",
      "loss: 0.915890  [ 2432/ 3200]\n",
      "loss: 1.121408  [ 2448/ 3200]\n",
      "loss: 0.998552  [ 2464/ 3200]\n",
      "loss: 1.139507  [ 2480/ 3200]\n",
      "loss: 1.003539  [ 2496/ 3200]\n",
      "loss: 1.108074  [ 2512/ 3200]\n",
      "loss: 1.094766  [ 2528/ 3200]\n",
      "loss: 1.246108  [ 2544/ 3200]\n",
      "loss: 0.944666  [ 2560/ 3200]\n",
      "loss: 0.955341  [ 2576/ 3200]\n",
      "loss: 1.054401  [ 2592/ 3200]\n",
      "loss: 1.004585  [ 2608/ 3200]\n",
      "loss: 0.902737  [ 2624/ 3200]\n",
      "loss: 1.127593  [ 2640/ 3200]\n",
      "loss: 1.091874  [ 2656/ 3200]\n",
      "loss: 1.053177  [ 2672/ 3200]\n",
      "loss: 1.055907  [ 2688/ 3200]\n",
      "loss: 1.020728  [ 2704/ 3200]\n",
      "loss: 1.241595  [ 2720/ 3200]\n",
      "loss: 0.984798  [ 2736/ 3200]\n",
      "loss: 1.295773  [ 2752/ 3200]\n",
      "loss: 1.094049  [ 2768/ 3200]\n",
      "loss: 1.019846  [ 2784/ 3200]\n",
      "loss: 1.282034  [ 2800/ 3200]\n",
      "loss: 1.024746  [ 2816/ 3200]\n",
      "loss: 1.195640  [ 2832/ 3200]\n",
      "loss: 1.218220  [ 2848/ 3200]\n",
      "loss: 1.248392  [ 2864/ 3200]\n",
      "loss: 1.048113  [ 2880/ 3200]\n",
      "loss: 1.159630  [ 2896/ 3200]\n",
      "loss: 0.911532  [ 2912/ 3200]\n",
      "loss: 1.049254  [ 2928/ 3200]\n",
      "loss: 1.043659  [ 2944/ 3200]\n",
      "loss: 1.157188  [ 2960/ 3200]\n",
      "loss: 0.916993  [ 2976/ 3200]\n",
      "loss: 1.189655  [ 2992/ 3200]\n",
      "loss: 0.974748  [ 3008/ 3200]\n",
      "loss: 1.285103  [ 3024/ 3200]\n",
      "loss: 0.957934  [ 3040/ 3200]\n",
      "loss: 1.100490  [ 3056/ 3200]\n",
      "loss: 1.135013  [ 3072/ 3200]\n",
      "loss: 0.964568  [ 3088/ 3200]\n",
      "loss: 1.078490  [ 3104/ 3200]\n",
      "loss: 0.969642  [ 3120/ 3200]\n",
      "loss: 1.151551  [ 3136/ 3200]\n",
      "loss: 1.234878  [ 3152/ 3200]\n",
      "loss: 1.186374  [ 3168/ 3200]\n",
      "loss: 1.138243  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 1.195417  [    0/ 3200]\n",
      "loss: 0.990550  [   16/ 3200]\n",
      "loss: 0.933278  [   32/ 3200]\n",
      "loss: 1.153004  [   48/ 3200]\n",
      "loss: 0.981966  [   64/ 3200]\n",
      "loss: 0.985561  [   80/ 3200]\n",
      "loss: 1.064958  [   96/ 3200]\n",
      "loss: 1.093097  [  112/ 3200]\n",
      "loss: 1.284359  [  128/ 3200]\n",
      "loss: 0.920259  [  144/ 3200]\n",
      "loss: 1.208456  [  160/ 3200]\n",
      "loss: 1.089412  [  176/ 3200]\n",
      "loss: 1.182703  [  192/ 3200]\n",
      "loss: 1.119105  [  208/ 3200]\n",
      "loss: 1.046292  [  224/ 3200]\n",
      "loss: 1.091625  [  240/ 3200]\n",
      "loss: 1.067248  [  256/ 3200]\n",
      "loss: 1.160004  [  272/ 3200]\n",
      "loss: 1.217821  [  288/ 3200]\n",
      "loss: 1.140771  [  304/ 3200]\n",
      "loss: 1.037144  [  320/ 3200]\n",
      "loss: 1.032119  [  336/ 3200]\n",
      "loss: 1.083137  [  352/ 3200]\n",
      "loss: 0.991718  [  368/ 3200]\n",
      "loss: 0.932395  [  384/ 3200]\n",
      "loss: 0.951335  [  400/ 3200]\n",
      "loss: 1.122133  [  416/ 3200]\n",
      "loss: 1.017825  [  432/ 3200]\n",
      "loss: 0.904033  [  448/ 3200]\n",
      "loss: 0.932971  [  464/ 3200]\n",
      "loss: 0.783993  [  480/ 3200]\n",
      "loss: 1.009643  [  496/ 3200]\n",
      "loss: 1.059700  [  512/ 3200]\n",
      "loss: 1.245981  [  528/ 3200]\n",
      "loss: 1.030132  [  544/ 3200]\n",
      "loss: 1.074488  [  560/ 3200]\n",
      "loss: 1.252764  [  576/ 3200]\n",
      "loss: 1.360782  [  592/ 3200]\n",
      "loss: 1.098027  [  608/ 3200]\n",
      "loss: 0.977370  [  624/ 3200]\n",
      "loss: 0.955934  [  640/ 3200]\n",
      "loss: 1.090422  [  656/ 3200]\n",
      "loss: 1.085062  [  672/ 3200]\n",
      "loss: 0.975323  [  688/ 3200]\n",
      "loss: 1.032151  [  704/ 3200]\n",
      "loss: 0.979231  [  720/ 3200]\n",
      "loss: 1.064750  [  736/ 3200]\n",
      "loss: 1.000389  [  752/ 3200]\n",
      "loss: 1.311455  [  768/ 3200]\n",
      "loss: 1.178214  [  784/ 3200]\n",
      "loss: 1.212084  [  800/ 3200]\n",
      "loss: 1.049718  [  816/ 3200]\n",
      "loss: 1.020021  [  832/ 3200]\n",
      "loss: 0.983554  [  848/ 3200]\n",
      "loss: 1.240639  [  864/ 3200]\n",
      "loss: 1.072261  [  880/ 3200]\n",
      "loss: 1.234422  [  896/ 3200]\n",
      "loss: 1.111484  [  912/ 3200]\n",
      "loss: 1.133586  [  928/ 3200]\n",
      "loss: 1.019145  [  944/ 3200]\n",
      "loss: 1.195248  [  960/ 3200]\n",
      "loss: 1.020265  [  976/ 3200]\n",
      "loss: 1.038275  [  992/ 3200]\n",
      "loss: 1.087168  [ 1008/ 3200]\n",
      "loss: 1.085869  [ 1024/ 3200]\n",
      "loss: 1.227890  [ 1040/ 3200]\n",
      "loss: 0.955908  [ 1056/ 3200]\n",
      "loss: 1.042980  [ 1072/ 3200]\n",
      "loss: 1.063611  [ 1088/ 3200]\n",
      "loss: 0.903438  [ 1104/ 3200]\n",
      "loss: 1.307988  [ 1120/ 3200]\n",
      "loss: 1.159590  [ 1136/ 3200]\n",
      "loss: 1.370600  [ 1152/ 3200]\n",
      "loss: 0.990121  [ 1168/ 3200]\n",
      "loss: 1.037007  [ 1184/ 3200]\n",
      "loss: 0.986855  [ 1200/ 3200]\n",
      "loss: 1.143331  [ 1216/ 3200]\n",
      "loss: 0.971371  [ 1232/ 3200]\n",
      "loss: 1.049261  [ 1248/ 3200]\n",
      "loss: 0.791169  [ 1264/ 3200]\n",
      "loss: 1.014606  [ 1280/ 3200]\n",
      "loss: 1.154379  [ 1296/ 3200]\n",
      "loss: 0.937167  [ 1312/ 3200]\n",
      "loss: 0.974248  [ 1328/ 3200]\n",
      "loss: 1.069104  [ 1344/ 3200]\n",
      "loss: 1.024256  [ 1360/ 3200]\n",
      "loss: 1.031199  [ 1376/ 3200]\n",
      "loss: 1.146666  [ 1392/ 3200]\n",
      "loss: 0.932830  [ 1408/ 3200]\n",
      "loss: 0.925546  [ 1424/ 3200]\n",
      "loss: 1.009721  [ 1440/ 3200]\n",
      "loss: 1.110772  [ 1456/ 3200]\n",
      "loss: 1.073563  [ 1472/ 3200]\n",
      "loss: 1.160013  [ 1488/ 3200]\n",
      "loss: 0.964945  [ 1504/ 3200]\n",
      "loss: 1.073090  [ 1520/ 3200]\n",
      "loss: 0.930736  [ 1536/ 3200]\n",
      "loss: 0.979689  [ 1552/ 3200]\n",
      "loss: 1.046757  [ 1568/ 3200]\n",
      "loss: 1.108580  [ 1584/ 3200]\n",
      "loss: 1.003658  [ 1600/ 3200]\n",
      "loss: 1.089390  [ 1616/ 3200]\n",
      "loss: 1.096391  [ 1632/ 3200]\n",
      "loss: 1.092097  [ 1648/ 3200]\n",
      "loss: 0.999159  [ 1664/ 3200]\n",
      "loss: 1.006430  [ 1680/ 3200]\n",
      "loss: 0.958555  [ 1696/ 3200]\n",
      "loss: 0.888169  [ 1712/ 3200]\n",
      "loss: 0.979799  [ 1728/ 3200]\n",
      "loss: 1.000129  [ 1744/ 3200]\n",
      "loss: 1.003814  [ 1760/ 3200]\n",
      "loss: 1.076872  [ 1776/ 3200]\n",
      "loss: 1.104420  [ 1792/ 3200]\n",
      "loss: 1.245597  [ 1808/ 3200]\n",
      "loss: 1.002180  [ 1824/ 3200]\n",
      "loss: 1.052292  [ 1840/ 3200]\n",
      "loss: 1.364951  [ 1856/ 3200]\n",
      "loss: 0.979944  [ 1872/ 3200]\n",
      "loss: 1.021543  [ 1888/ 3200]\n",
      "loss: 0.941133  [ 1904/ 3200]\n",
      "loss: 1.154611  [ 1920/ 3200]\n",
      "loss: 0.964154  [ 1936/ 3200]\n",
      "loss: 1.124749  [ 1952/ 3200]\n",
      "loss: 1.127787  [ 1968/ 3200]\n",
      "loss: 1.093620  [ 1984/ 3200]\n",
      "loss: 1.054989  [ 2000/ 3200]\n",
      "loss: 1.206815  [ 2016/ 3200]\n",
      "loss: 1.172859  [ 2032/ 3200]\n",
      "loss: 0.992769  [ 2048/ 3200]\n",
      "loss: 1.089523  [ 2064/ 3200]\n",
      "loss: 1.105923  [ 2080/ 3200]\n",
      "loss: 0.717190  [ 2096/ 3200]\n",
      "loss: 1.065590  [ 2112/ 3200]\n",
      "loss: 1.204081  [ 2128/ 3200]\n",
      "loss: 1.021609  [ 2144/ 3200]\n",
      "loss: 0.984513  [ 2160/ 3200]\n",
      "loss: 1.100837  [ 2176/ 3200]\n",
      "loss: 1.224069  [ 2192/ 3200]\n",
      "loss: 1.229113  [ 2208/ 3200]\n",
      "loss: 1.105211  [ 2224/ 3200]\n",
      "loss: 1.084140  [ 2240/ 3200]\n",
      "loss: 1.106348  [ 2256/ 3200]\n",
      "loss: 0.851378  [ 2272/ 3200]\n",
      "loss: 1.029053  [ 2288/ 3200]\n",
      "loss: 0.997009  [ 2304/ 3200]\n",
      "loss: 0.909455  [ 2320/ 3200]\n",
      "loss: 1.311324  [ 2336/ 3200]\n",
      "loss: 1.029280  [ 2352/ 3200]\n",
      "loss: 1.338139  [ 2368/ 3200]\n",
      "loss: 1.004146  [ 2384/ 3200]\n",
      "loss: 1.119643  [ 2400/ 3200]\n",
      "loss: 1.059166  [ 2416/ 3200]\n",
      "loss: 0.954457  [ 2432/ 3200]\n",
      "loss: 1.119094  [ 2448/ 3200]\n",
      "loss: 1.128968  [ 2464/ 3200]\n",
      "loss: 0.926455  [ 2480/ 3200]\n",
      "loss: 0.912564  [ 2496/ 3200]\n",
      "loss: 0.945388  [ 2512/ 3200]\n",
      "loss: 1.170443  [ 2528/ 3200]\n",
      "loss: 1.175014  [ 2544/ 3200]\n",
      "loss: 0.995294  [ 2560/ 3200]\n",
      "loss: 1.010374  [ 2576/ 3200]\n",
      "loss: 1.030523  [ 2592/ 3200]\n",
      "loss: 1.108719  [ 2608/ 3200]\n",
      "loss: 1.040210  [ 2624/ 3200]\n",
      "loss: 1.026824  [ 2640/ 3200]\n",
      "loss: 1.092976  [ 2656/ 3200]\n",
      "loss: 1.362383  [ 2672/ 3200]\n",
      "loss: 0.972573  [ 2688/ 3200]\n",
      "loss: 1.014192  [ 2704/ 3200]\n",
      "loss: 1.126548  [ 2720/ 3200]\n",
      "loss: 1.161078  [ 2736/ 3200]\n",
      "loss: 0.850296  [ 2752/ 3200]\n",
      "loss: 1.280603  [ 2768/ 3200]\n",
      "loss: 0.961956  [ 2784/ 3200]\n",
      "loss: 1.098094  [ 2800/ 3200]\n",
      "loss: 0.972823  [ 2816/ 3200]\n",
      "loss: 1.044973  [ 2832/ 3200]\n",
      "loss: 1.191970  [ 2848/ 3200]\n",
      "loss: 0.929144  [ 2864/ 3200]\n",
      "loss: 0.851049  [ 2880/ 3200]\n",
      "loss: 1.249949  [ 2896/ 3200]\n",
      "loss: 1.014911  [ 2912/ 3200]\n",
      "loss: 1.144381  [ 2928/ 3200]\n",
      "loss: 1.119979  [ 2944/ 3200]\n",
      "loss: 1.321497  [ 2960/ 3200]\n",
      "loss: 0.933671  [ 2976/ 3200]\n",
      "loss: 0.880834  [ 2992/ 3200]\n",
      "loss: 1.343627  [ 3008/ 3200]\n",
      "loss: 1.132522  [ 3024/ 3200]\n",
      "loss: 1.310432  [ 3040/ 3200]\n",
      "loss: 1.036879  [ 3056/ 3200]\n",
      "loss: 1.037188  [ 3072/ 3200]\n",
      "loss: 1.205436  [ 3088/ 3200]\n",
      "loss: 1.088120  [ 3104/ 3200]\n",
      "loss: 1.121885  [ 3120/ 3200]\n",
      "loss: 0.973865  [ 3136/ 3200]\n",
      "loss: 0.984721  [ 3152/ 3200]\n",
      "loss: 1.126649  [ 3168/ 3200]\n",
      "loss: 1.059879  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 1.134806  [    0/ 3200]\n",
      "loss: 1.100512  [   16/ 3200]\n",
      "loss: 1.103817  [   32/ 3200]\n",
      "loss: 1.135591  [   48/ 3200]\n",
      "loss: 0.961273  [   64/ 3200]\n",
      "loss: 0.963242  [   80/ 3200]\n",
      "loss: 1.045265  [   96/ 3200]\n",
      "loss: 1.029876  [  112/ 3200]\n",
      "loss: 0.991479  [  128/ 3200]\n",
      "loss: 1.017379  [  144/ 3200]\n",
      "loss: 1.207545  [  160/ 3200]\n",
      "loss: 0.858161  [  176/ 3200]\n",
      "loss: 1.006086  [  192/ 3200]\n",
      "loss: 1.227963  [  208/ 3200]\n",
      "loss: 1.064631  [  224/ 3200]\n",
      "loss: 0.973883  [  240/ 3200]\n",
      "loss: 1.133054  [  256/ 3200]\n",
      "loss: 0.992443  [  272/ 3200]\n",
      "loss: 0.994979  [  288/ 3200]\n",
      "loss: 1.002748  [  304/ 3200]\n",
      "loss: 1.049313  [  320/ 3200]\n",
      "loss: 0.795964  [  336/ 3200]\n",
      "loss: 0.979297  [  352/ 3200]\n",
      "loss: 0.904230  [  368/ 3200]\n",
      "loss: 1.321333  [  384/ 3200]\n",
      "loss: 1.208007  [  400/ 3200]\n",
      "loss: 1.019319  [  416/ 3200]\n",
      "loss: 1.060628  [  432/ 3200]\n",
      "loss: 1.046517  [  448/ 3200]\n",
      "loss: 1.061115  [  464/ 3200]\n",
      "loss: 1.120629  [  480/ 3200]\n",
      "loss: 1.045990  [  496/ 3200]\n",
      "loss: 1.041997  [  512/ 3200]\n",
      "loss: 1.173647  [  528/ 3200]\n",
      "loss: 1.122905  [  544/ 3200]\n",
      "loss: 1.188989  [  560/ 3200]\n",
      "loss: 0.900078  [  576/ 3200]\n",
      "loss: 1.064813  [  592/ 3200]\n",
      "loss: 1.138258  [  608/ 3200]\n",
      "loss: 1.015210  [  624/ 3200]\n",
      "loss: 1.138483  [  640/ 3200]\n",
      "loss: 1.340571  [  656/ 3200]\n",
      "loss: 0.817744  [  672/ 3200]\n",
      "loss: 1.018686  [  688/ 3200]\n",
      "loss: 0.979018  [  704/ 3200]\n",
      "loss: 0.977242  [  720/ 3200]\n",
      "loss: 1.279708  [  736/ 3200]\n",
      "loss: 0.835321  [  752/ 3200]\n",
      "loss: 1.096020  [  768/ 3200]\n",
      "loss: 0.961070  [  784/ 3200]\n",
      "loss: 0.994673  [  800/ 3200]\n",
      "loss: 0.976208  [  816/ 3200]\n",
      "loss: 1.209024  [  832/ 3200]\n",
      "loss: 1.066047  [  848/ 3200]\n",
      "loss: 1.182482  [  864/ 3200]\n",
      "loss: 1.033290  [  880/ 3200]\n",
      "loss: 0.968387  [  896/ 3200]\n",
      "loss: 1.205471  [  912/ 3200]\n",
      "loss: 0.972926  [  928/ 3200]\n",
      "loss: 1.129918  [  944/ 3200]\n",
      "loss: 1.119152  [  960/ 3200]\n",
      "loss: 0.921621  [  976/ 3200]\n",
      "loss: 1.227108  [  992/ 3200]\n",
      "loss: 0.878341  [ 1008/ 3200]\n",
      "loss: 0.892526  [ 1024/ 3200]\n",
      "loss: 1.109334  [ 1040/ 3200]\n",
      "loss: 1.084836  [ 1056/ 3200]\n",
      "loss: 1.036296  [ 1072/ 3200]\n",
      "loss: 0.996222  [ 1088/ 3200]\n",
      "loss: 1.175045  [ 1104/ 3200]\n",
      "loss: 1.067902  [ 1120/ 3200]\n",
      "loss: 1.253484  [ 1136/ 3200]\n",
      "loss: 1.092722  [ 1152/ 3200]\n",
      "loss: 1.021068  [ 1168/ 3200]\n",
      "loss: 0.946232  [ 1184/ 3200]\n",
      "loss: 1.096355  [ 1200/ 3200]\n",
      "loss: 1.004228  [ 1216/ 3200]\n",
      "loss: 0.992832  [ 1232/ 3200]\n",
      "loss: 1.063869  [ 1248/ 3200]\n",
      "loss: 1.120938  [ 1264/ 3200]\n",
      "loss: 1.225310  [ 1280/ 3200]\n",
      "loss: 1.171366  [ 1296/ 3200]\n",
      "loss: 1.184566  [ 1312/ 3200]\n",
      "loss: 1.062525  [ 1328/ 3200]\n",
      "loss: 1.071254  [ 1344/ 3200]\n",
      "loss: 1.074829  [ 1360/ 3200]\n",
      "loss: 1.056643  [ 1376/ 3200]\n",
      "loss: 1.149589  [ 1392/ 3200]\n",
      "loss: 0.996728  [ 1408/ 3200]\n",
      "loss: 1.128133  [ 1424/ 3200]\n",
      "loss: 1.035826  [ 1440/ 3200]\n",
      "loss: 0.975140  [ 1456/ 3200]\n",
      "loss: 0.865041  [ 1472/ 3200]\n",
      "loss: 1.221122  [ 1488/ 3200]\n",
      "loss: 1.115749  [ 1504/ 3200]\n",
      "loss: 1.150546  [ 1520/ 3200]\n",
      "loss: 1.104373  [ 1536/ 3200]\n",
      "loss: 1.186554  [ 1552/ 3200]\n",
      "loss: 1.066515  [ 1568/ 3200]\n",
      "loss: 1.164328  [ 1584/ 3200]\n",
      "loss: 1.209231  [ 1600/ 3200]\n",
      "loss: 1.008611  [ 1616/ 3200]\n",
      "loss: 1.183344  [ 1632/ 3200]\n",
      "loss: 1.001613  [ 1648/ 3200]\n",
      "loss: 1.286603  [ 1664/ 3200]\n",
      "loss: 1.245122  [ 1680/ 3200]\n",
      "loss: 0.900645  [ 1696/ 3200]\n",
      "loss: 1.094280  [ 1712/ 3200]\n",
      "loss: 0.983218  [ 1728/ 3200]\n",
      "loss: 1.161370  [ 1744/ 3200]\n",
      "loss: 1.079688  [ 1760/ 3200]\n",
      "loss: 0.922303  [ 1776/ 3200]\n",
      "loss: 1.057708  [ 1792/ 3200]\n",
      "loss: 1.081557  [ 1808/ 3200]\n",
      "loss: 0.961940  [ 1824/ 3200]\n",
      "loss: 0.922798  [ 1840/ 3200]\n",
      "loss: 1.224409  [ 1856/ 3200]\n",
      "loss: 1.161343  [ 1872/ 3200]\n",
      "loss: 0.966744  [ 1888/ 3200]\n",
      "loss: 1.098027  [ 1904/ 3200]\n",
      "loss: 0.935505  [ 1920/ 3200]\n",
      "loss: 1.092219  [ 1936/ 3200]\n",
      "loss: 1.080444  [ 1952/ 3200]\n",
      "loss: 1.071124  [ 1968/ 3200]\n",
      "loss: 0.991397  [ 1984/ 3200]\n",
      "loss: 1.098659  [ 2000/ 3200]\n",
      "loss: 1.054752  [ 2016/ 3200]\n",
      "loss: 1.208528  [ 2032/ 3200]\n",
      "loss: 1.216484  [ 2048/ 3200]\n",
      "loss: 0.933697  [ 2064/ 3200]\n",
      "loss: 1.191963  [ 2080/ 3200]\n",
      "loss: 0.777531  [ 2096/ 3200]\n",
      "loss: 1.154042  [ 2112/ 3200]\n",
      "loss: 1.166595  [ 2128/ 3200]\n",
      "loss: 1.237065  [ 2144/ 3200]\n",
      "loss: 1.034955  [ 2160/ 3200]\n",
      "loss: 1.100295  [ 2176/ 3200]\n",
      "loss: 1.241686  [ 2192/ 3200]\n",
      "loss: 1.268852  [ 2208/ 3200]\n",
      "loss: 0.990875  [ 2224/ 3200]\n",
      "loss: 0.937417  [ 2240/ 3200]\n",
      "loss: 1.103718  [ 2256/ 3200]\n",
      "loss: 1.178806  [ 2272/ 3200]\n",
      "loss: 0.911067  [ 2288/ 3200]\n",
      "loss: 0.795255  [ 2304/ 3200]\n",
      "loss: 0.887882  [ 2320/ 3200]\n",
      "loss: 1.106797  [ 2336/ 3200]\n",
      "loss: 0.924086  [ 2352/ 3200]\n",
      "loss: 1.142933  [ 2368/ 3200]\n",
      "loss: 1.188666  [ 2384/ 3200]\n",
      "loss: 1.156750  [ 2400/ 3200]\n",
      "loss: 0.932929  [ 2416/ 3200]\n",
      "loss: 0.801747  [ 2432/ 3200]\n",
      "loss: 0.956615  [ 2448/ 3200]\n",
      "loss: 0.940731  [ 2464/ 3200]\n",
      "loss: 0.982234  [ 2480/ 3200]\n",
      "loss: 0.981777  [ 2496/ 3200]\n",
      "loss: 1.155670  [ 2512/ 3200]\n",
      "loss: 1.129470  [ 2528/ 3200]\n",
      "loss: 0.911508  [ 2544/ 3200]\n",
      "loss: 1.186840  [ 2560/ 3200]\n",
      "loss: 1.039574  [ 2576/ 3200]\n",
      "loss: 0.989550  [ 2592/ 3200]\n",
      "loss: 1.112348  [ 2608/ 3200]\n",
      "loss: 1.121401  [ 2624/ 3200]\n",
      "loss: 0.956878  [ 2640/ 3200]\n",
      "loss: 1.010507  [ 2656/ 3200]\n",
      "loss: 1.092555  [ 2672/ 3200]\n",
      "loss: 0.873929  [ 2688/ 3200]\n",
      "loss: 1.208757  [ 2704/ 3200]\n",
      "loss: 1.112928  [ 2720/ 3200]\n",
      "loss: 1.157150  [ 2736/ 3200]\n",
      "loss: 0.929073  [ 2752/ 3200]\n",
      "loss: 1.314119  [ 2768/ 3200]\n",
      "loss: 0.997674  [ 2784/ 3200]\n",
      "loss: 0.992079  [ 2800/ 3200]\n",
      "loss: 1.179398  [ 2816/ 3200]\n",
      "loss: 0.941803  [ 2832/ 3200]\n",
      "loss: 0.835225  [ 2848/ 3200]\n",
      "loss: 1.057960  [ 2864/ 3200]\n",
      "loss: 1.132935  [ 2880/ 3200]\n",
      "loss: 0.788940  [ 2896/ 3200]\n",
      "loss: 1.197562  [ 2912/ 3200]\n",
      "loss: 1.048604  [ 2928/ 3200]\n",
      "loss: 0.745525  [ 2944/ 3200]\n",
      "loss: 1.044142  [ 2960/ 3200]\n",
      "loss: 0.886568  [ 2976/ 3200]\n",
      "loss: 1.066255  [ 2992/ 3200]\n",
      "loss: 1.018061  [ 3008/ 3200]\n",
      "loss: 1.058976  [ 3024/ 3200]\n",
      "loss: 0.961906  [ 3040/ 3200]\n",
      "loss: 0.920584  [ 3056/ 3200]\n",
      "loss: 1.035994  [ 3072/ 3200]\n",
      "loss: 0.901348  [ 3088/ 3200]\n",
      "loss: 0.890987  [ 3104/ 3200]\n",
      "loss: 1.110282  [ 3120/ 3200]\n",
      "loss: 1.056934  [ 3136/ 3200]\n",
      "loss: 0.909418  [ 3152/ 3200]\n",
      "loss: 1.104089  [ 3168/ 3200]\n",
      "loss: 0.946401  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.977205  [    0/ 3200]\n",
      "loss: 1.075845  [   16/ 3200]\n",
      "loss: 0.961472  [   32/ 3200]\n",
      "loss: 1.050431  [   48/ 3200]\n",
      "loss: 1.048904  [   64/ 3200]\n",
      "loss: 0.926111  [   80/ 3200]\n",
      "loss: 1.001335  [   96/ 3200]\n",
      "loss: 1.051328  [  112/ 3200]\n",
      "loss: 1.082538  [  128/ 3200]\n",
      "loss: 1.004052  [  144/ 3200]\n",
      "loss: 1.085417  [  160/ 3200]\n",
      "loss: 1.084514  [  176/ 3200]\n",
      "loss: 0.930907  [  192/ 3200]\n",
      "loss: 1.139553  [  208/ 3200]\n",
      "loss: 1.075301  [  224/ 3200]\n",
      "loss: 0.955503  [  240/ 3200]\n",
      "loss: 1.156765  [  256/ 3200]\n",
      "loss: 0.765722  [  272/ 3200]\n",
      "loss: 1.032536  [  288/ 3200]\n",
      "loss: 1.156104  [  304/ 3200]\n",
      "loss: 1.054910  [  320/ 3200]\n",
      "loss: 0.949317  [  336/ 3200]\n",
      "loss: 0.993026  [  352/ 3200]\n",
      "loss: 1.060458  [  368/ 3200]\n",
      "loss: 0.925736  [  384/ 3200]\n",
      "loss: 0.939127  [  400/ 3200]\n",
      "loss: 1.040818  [  416/ 3200]\n",
      "loss: 1.151414  [  432/ 3200]\n",
      "loss: 1.013449  [  448/ 3200]\n",
      "loss: 0.988500  [  464/ 3200]\n",
      "loss: 0.956097  [  480/ 3200]\n",
      "loss: 0.923682  [  496/ 3200]\n",
      "loss: 0.909526  [  512/ 3200]\n",
      "loss: 0.748808  [  528/ 3200]\n",
      "loss: 0.836424  [  544/ 3200]\n",
      "loss: 1.015669  [  560/ 3200]\n",
      "loss: 1.031219  [  576/ 3200]\n",
      "loss: 1.160185  [  592/ 3200]\n",
      "loss: 1.121073  [  608/ 3200]\n",
      "loss: 0.878562  [  624/ 3200]\n",
      "loss: 1.108952  [  640/ 3200]\n",
      "loss: 1.004650  [  656/ 3200]\n",
      "loss: 1.030551  [  672/ 3200]\n",
      "loss: 1.008937  [  688/ 3200]\n",
      "loss: 1.122082  [  704/ 3200]\n",
      "loss: 1.120102  [  720/ 3200]\n",
      "loss: 1.255051  [  736/ 3200]\n",
      "loss: 1.093839  [  752/ 3200]\n",
      "loss: 0.946486  [  768/ 3200]\n",
      "loss: 1.183445  [  784/ 3200]\n",
      "loss: 1.138973  [  800/ 3200]\n",
      "loss: 0.894041  [  816/ 3200]\n",
      "loss: 1.209544  [  832/ 3200]\n",
      "loss: 1.160155  [  848/ 3200]\n",
      "loss: 1.073920  [  864/ 3200]\n",
      "loss: 0.975187  [  880/ 3200]\n",
      "loss: 1.302660  [  896/ 3200]\n",
      "loss: 1.149115  [  912/ 3200]\n",
      "loss: 1.162200  [  928/ 3200]\n",
      "loss: 0.827642  [  944/ 3200]\n",
      "loss: 0.949150  [  960/ 3200]\n",
      "loss: 1.211558  [  976/ 3200]\n",
      "loss: 0.954076  [  992/ 3200]\n",
      "loss: 1.032617  [ 1008/ 3200]\n",
      "loss: 1.112405  [ 1024/ 3200]\n",
      "loss: 1.052301  [ 1040/ 3200]\n",
      "loss: 0.922473  [ 1056/ 3200]\n",
      "loss: 1.228735  [ 1072/ 3200]\n",
      "loss: 1.138043  [ 1088/ 3200]\n",
      "loss: 1.032218  [ 1104/ 3200]\n",
      "loss: 0.956748  [ 1120/ 3200]\n",
      "loss: 1.067068  [ 1136/ 3200]\n",
      "loss: 1.150453  [ 1152/ 3200]\n",
      "loss: 0.962901  [ 1168/ 3200]\n",
      "loss: 0.970954  [ 1184/ 3200]\n",
      "loss: 1.130029  [ 1200/ 3200]\n",
      "loss: 1.045270  [ 1216/ 3200]\n",
      "loss: 1.329669  [ 1232/ 3200]\n",
      "loss: 1.068717  [ 1248/ 3200]\n",
      "loss: 1.048935  [ 1264/ 3200]\n",
      "loss: 1.069938  [ 1280/ 3200]\n",
      "loss: 0.888787  [ 1296/ 3200]\n",
      "loss: 1.287517  [ 1312/ 3200]\n",
      "loss: 1.191188  [ 1328/ 3200]\n",
      "loss: 1.258591  [ 1344/ 3200]\n",
      "loss: 1.091734  [ 1360/ 3200]\n",
      "loss: 0.961424  [ 1376/ 3200]\n",
      "loss: 0.833238  [ 1392/ 3200]\n",
      "loss: 0.952553  [ 1408/ 3200]\n",
      "loss: 0.961029  [ 1424/ 3200]\n",
      "loss: 0.835734  [ 1440/ 3200]\n",
      "loss: 1.032864  [ 1456/ 3200]\n",
      "loss: 0.993812  [ 1472/ 3200]\n",
      "loss: 0.723566  [ 1488/ 3200]\n",
      "loss: 0.847439  [ 1504/ 3200]\n",
      "loss: 1.186224  [ 1520/ 3200]\n",
      "loss: 1.174160  [ 1536/ 3200]\n",
      "loss: 0.890414  [ 1552/ 3200]\n",
      "loss: 1.074524  [ 1568/ 3200]\n",
      "loss: 1.099808  [ 1584/ 3200]\n",
      "loss: 1.110397  [ 1600/ 3200]\n",
      "loss: 1.097740  [ 1616/ 3200]\n",
      "loss: 0.964343  [ 1632/ 3200]\n",
      "loss: 0.924178  [ 1648/ 3200]\n",
      "loss: 0.802878  [ 1664/ 3200]\n",
      "loss: 0.923409  [ 1680/ 3200]\n",
      "loss: 1.049343  [ 1696/ 3200]\n",
      "loss: 1.088082  [ 1712/ 3200]\n",
      "loss: 0.912236  [ 1728/ 3200]\n",
      "loss: 1.113027  [ 1744/ 3200]\n",
      "loss: 0.903590  [ 1760/ 3200]\n",
      "loss: 0.924481  [ 1776/ 3200]\n",
      "loss: 1.006813  [ 1792/ 3200]\n",
      "loss: 0.929414  [ 1808/ 3200]\n",
      "loss: 1.072036  [ 1824/ 3200]\n",
      "loss: 1.000298  [ 1840/ 3200]\n",
      "loss: 1.229707  [ 1856/ 3200]\n",
      "loss: 0.977008  [ 1872/ 3200]\n",
      "loss: 1.191896  [ 1888/ 3200]\n",
      "loss: 1.007208  [ 1904/ 3200]\n",
      "loss: 0.869218  [ 1920/ 3200]\n",
      "loss: 0.901860  [ 1936/ 3200]\n",
      "loss: 0.960621  [ 1952/ 3200]\n",
      "loss: 1.192157  [ 1968/ 3200]\n",
      "loss: 1.127458  [ 1984/ 3200]\n",
      "loss: 1.171866  [ 2000/ 3200]\n",
      "loss: 1.081487  [ 2016/ 3200]\n",
      "loss: 0.965794  [ 2032/ 3200]\n",
      "loss: 1.220502  [ 2048/ 3200]\n",
      "loss: 1.086135  [ 2064/ 3200]\n",
      "loss: 0.942062  [ 2080/ 3200]\n",
      "loss: 1.105515  [ 2096/ 3200]\n",
      "loss: 1.124185  [ 2112/ 3200]\n",
      "loss: 1.242197  [ 2128/ 3200]\n",
      "loss: 1.038760  [ 2144/ 3200]\n",
      "loss: 1.263700  [ 2160/ 3200]\n",
      "loss: 1.102868  [ 2176/ 3200]\n",
      "loss: 1.315416  [ 2192/ 3200]\n",
      "loss: 0.974413  [ 2208/ 3200]\n",
      "loss: 1.351915  [ 2224/ 3200]\n",
      "loss: 1.004875  [ 2240/ 3200]\n",
      "loss: 1.031338  [ 2256/ 3200]\n",
      "loss: 1.026670  [ 2272/ 3200]\n",
      "loss: 1.071456  [ 2288/ 3200]\n",
      "loss: 0.732263  [ 2304/ 3200]\n",
      "loss: 1.121534  [ 2320/ 3200]\n",
      "loss: 0.877469  [ 2336/ 3200]\n",
      "loss: 0.972281  [ 2352/ 3200]\n",
      "loss: 0.899736  [ 2368/ 3200]\n",
      "loss: 0.881078  [ 2384/ 3200]\n",
      "loss: 0.858168  [ 2400/ 3200]\n",
      "loss: 1.041829  [ 2416/ 3200]\n",
      "loss: 1.017658  [ 2432/ 3200]\n",
      "loss: 1.154528  [ 2448/ 3200]\n",
      "loss: 1.116297  [ 2464/ 3200]\n",
      "loss: 1.057540  [ 2480/ 3200]\n",
      "loss: 1.212719  [ 2496/ 3200]\n",
      "loss: 0.877761  [ 2512/ 3200]\n",
      "loss: 1.148510  [ 2528/ 3200]\n",
      "loss: 1.094357  [ 2544/ 3200]\n",
      "loss: 1.018366  [ 2560/ 3200]\n",
      "loss: 1.188090  [ 2576/ 3200]\n",
      "loss: 0.881295  [ 2592/ 3200]\n",
      "loss: 1.105925  [ 2608/ 3200]\n",
      "loss: 1.011580  [ 2624/ 3200]\n",
      "loss: 0.904135  [ 2640/ 3200]\n",
      "loss: 1.009784  [ 2656/ 3200]\n",
      "loss: 1.032530  [ 2672/ 3200]\n",
      "loss: 1.131111  [ 2688/ 3200]\n",
      "loss: 1.007180  [ 2704/ 3200]\n",
      "loss: 0.923850  [ 2720/ 3200]\n",
      "loss: 1.034546  [ 2736/ 3200]\n",
      "loss: 1.003889  [ 2752/ 3200]\n",
      "loss: 1.177776  [ 2768/ 3200]\n",
      "loss: 1.250317  [ 2784/ 3200]\n",
      "loss: 1.092637  [ 2800/ 3200]\n",
      "loss: 1.055007  [ 2816/ 3200]\n",
      "loss: 1.458761  [ 2832/ 3200]\n",
      "loss: 1.097773  [ 2848/ 3200]\n",
      "loss: 1.098788  [ 2864/ 3200]\n",
      "loss: 1.059422  [ 2880/ 3200]\n",
      "loss: 1.132866  [ 2896/ 3200]\n",
      "loss: 0.984936  [ 2912/ 3200]\n",
      "loss: 0.931437  [ 2928/ 3200]\n",
      "loss: 1.099265  [ 2944/ 3200]\n",
      "loss: 1.177339  [ 2960/ 3200]\n",
      "loss: 1.064912  [ 2976/ 3200]\n",
      "loss: 0.882857  [ 2992/ 3200]\n",
      "loss: 0.786304  [ 3008/ 3200]\n",
      "loss: 1.040903  [ 3024/ 3200]\n",
      "loss: 0.896912  [ 3040/ 3200]\n",
      "loss: 1.035537  [ 3056/ 3200]\n",
      "loss: 1.256580  [ 3072/ 3200]\n",
      "loss: 0.979122  [ 3088/ 3200]\n",
      "loss: 0.849912  [ 3104/ 3200]\n",
      "loss: 1.156102  [ 3120/ 3200]\n",
      "loss: 0.965372  [ 3136/ 3200]\n",
      "loss: 1.112167  [ 3152/ 3200]\n",
      "loss: 1.001358  [ 3168/ 3200]\n",
      "loss: 1.060508  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 1.018079  [    0/ 3200]\n",
      "loss: 0.732319  [   16/ 3200]\n",
      "loss: 0.850708  [   32/ 3200]\n",
      "loss: 0.913663  [   48/ 3200]\n",
      "loss: 1.024835  [   64/ 3200]\n",
      "loss: 1.108516  [   80/ 3200]\n",
      "loss: 1.140160  [   96/ 3200]\n",
      "loss: 1.145883  [  112/ 3200]\n",
      "loss: 1.046662  [  128/ 3200]\n",
      "loss: 1.257538  [  144/ 3200]\n",
      "loss: 1.010065  [  160/ 3200]\n",
      "loss: 0.955397  [  176/ 3200]\n",
      "loss: 1.012576  [  192/ 3200]\n",
      "loss: 1.405575  [  208/ 3200]\n",
      "loss: 1.093443  [  224/ 3200]\n",
      "loss: 0.818426  [  240/ 3200]\n",
      "loss: 0.978698  [  256/ 3200]\n",
      "loss: 0.839469  [  272/ 3200]\n",
      "loss: 0.918923  [  288/ 3200]\n",
      "loss: 1.331865  [  304/ 3200]\n",
      "loss: 0.965915  [  320/ 3200]\n",
      "loss: 1.278780  [  336/ 3200]\n",
      "loss: 1.007410  [  352/ 3200]\n",
      "loss: 1.121462  [  368/ 3200]\n",
      "loss: 1.117053  [  384/ 3200]\n",
      "loss: 1.417354  [  400/ 3200]\n",
      "loss: 0.864763  [  416/ 3200]\n",
      "loss: 1.232887  [  432/ 3200]\n",
      "loss: 0.829432  [  448/ 3200]\n",
      "loss: 1.138322  [  464/ 3200]\n",
      "loss: 0.976150  [  480/ 3200]\n",
      "loss: 1.074963  [  496/ 3200]\n",
      "loss: 1.079695  [  512/ 3200]\n",
      "loss: 1.106846  [  528/ 3200]\n",
      "loss: 1.179600  [  544/ 3200]\n",
      "loss: 1.196440  [  560/ 3200]\n",
      "loss: 0.923362  [  576/ 3200]\n",
      "loss: 1.134536  [  592/ 3200]\n",
      "loss: 1.126155  [  608/ 3200]\n",
      "loss: 1.061089  [  624/ 3200]\n",
      "loss: 1.127217  [  640/ 3200]\n",
      "loss: 1.179705  [  656/ 3200]\n",
      "loss: 1.053879  [  672/ 3200]\n",
      "loss: 0.950005  [  688/ 3200]\n",
      "loss: 0.963293  [  704/ 3200]\n",
      "loss: 0.783893  [  720/ 3200]\n",
      "loss: 1.151979  [  736/ 3200]\n",
      "loss: 1.092834  [  752/ 3200]\n",
      "loss: 0.984992  [  768/ 3200]\n",
      "loss: 1.120631  [  784/ 3200]\n",
      "loss: 1.202049  [  800/ 3200]\n",
      "loss: 1.225663  [  816/ 3200]\n",
      "loss: 0.839530  [  832/ 3200]\n",
      "loss: 0.941716  [  848/ 3200]\n",
      "loss: 1.090011  [  864/ 3200]\n",
      "loss: 0.912698  [  880/ 3200]\n",
      "loss: 0.819709  [  896/ 3200]\n",
      "loss: 0.890800  [  912/ 3200]\n",
      "loss: 1.220270  [  928/ 3200]\n",
      "loss: 1.003981  [  944/ 3200]\n",
      "loss: 0.846422  [  960/ 3200]\n",
      "loss: 0.787535  [  976/ 3200]\n",
      "loss: 1.115323  [  992/ 3200]\n",
      "loss: 0.998131  [ 1008/ 3200]\n",
      "loss: 1.106034  [ 1024/ 3200]\n",
      "loss: 0.861819  [ 1040/ 3200]\n",
      "loss: 1.012277  [ 1056/ 3200]\n",
      "loss: 1.006184  [ 1072/ 3200]\n",
      "loss: 0.947255  [ 1088/ 3200]\n",
      "loss: 1.078771  [ 1104/ 3200]\n",
      "loss: 0.785172  [ 1120/ 3200]\n",
      "loss: 1.000720  [ 1136/ 3200]\n",
      "loss: 0.729286  [ 1152/ 3200]\n",
      "loss: 1.061134  [ 1168/ 3200]\n",
      "loss: 0.990377  [ 1184/ 3200]\n",
      "loss: 0.925486  [ 1200/ 3200]\n",
      "loss: 0.996323  [ 1216/ 3200]\n",
      "loss: 0.993388  [ 1232/ 3200]\n",
      "loss: 1.235398  [ 1248/ 3200]\n",
      "loss: 0.962374  [ 1264/ 3200]\n",
      "loss: 0.819901  [ 1280/ 3200]\n",
      "loss: 1.063938  [ 1296/ 3200]\n",
      "loss: 0.873742  [ 1312/ 3200]\n",
      "loss: 0.786166  [ 1328/ 3200]\n",
      "loss: 1.011840  [ 1344/ 3200]\n",
      "loss: 0.974782  [ 1360/ 3200]\n",
      "loss: 1.161475  [ 1376/ 3200]\n",
      "loss: 0.949135  [ 1392/ 3200]\n",
      "loss: 1.172287  [ 1408/ 3200]\n",
      "loss: 1.043315  [ 1424/ 3200]\n",
      "loss: 0.999659  [ 1440/ 3200]\n",
      "loss: 0.672397  [ 1456/ 3200]\n",
      "loss: 0.934212  [ 1472/ 3200]\n",
      "loss: 1.146903  [ 1488/ 3200]\n",
      "loss: 0.863031  [ 1504/ 3200]\n",
      "loss: 1.132328  [ 1520/ 3200]\n",
      "loss: 0.890134  [ 1536/ 3200]\n",
      "loss: 0.890108  [ 1552/ 3200]\n",
      "loss: 1.083673  [ 1568/ 3200]\n",
      "loss: 1.179040  [ 1584/ 3200]\n",
      "loss: 1.045488  [ 1600/ 3200]\n",
      "loss: 0.840390  [ 1616/ 3200]\n",
      "loss: 1.123575  [ 1632/ 3200]\n",
      "loss: 1.370082  [ 1648/ 3200]\n",
      "loss: 1.202638  [ 1664/ 3200]\n",
      "loss: 1.017668  [ 1680/ 3200]\n",
      "loss: 1.206882  [ 1696/ 3200]\n",
      "loss: 1.006058  [ 1712/ 3200]\n",
      "loss: 0.817785  [ 1728/ 3200]\n",
      "loss: 1.410485  [ 1744/ 3200]\n",
      "loss: 1.142000  [ 1760/ 3200]\n",
      "loss: 0.786227  [ 1776/ 3200]\n",
      "loss: 1.053558  [ 1792/ 3200]\n",
      "loss: 1.210624  [ 1808/ 3200]\n",
      "loss: 0.852535  [ 1824/ 3200]\n",
      "loss: 0.827458  [ 1840/ 3200]\n",
      "loss: 0.991028  [ 1856/ 3200]\n",
      "loss: 1.117992  [ 1872/ 3200]\n",
      "loss: 0.930542  [ 1888/ 3200]\n",
      "loss: 0.972043  [ 1904/ 3200]\n",
      "loss: 1.217370  [ 1920/ 3200]\n",
      "loss: 1.033508  [ 1936/ 3200]\n",
      "loss: 0.964489  [ 1952/ 3200]\n",
      "loss: 1.066759  [ 1968/ 3200]\n",
      "loss: 1.187943  [ 1984/ 3200]\n",
      "loss: 1.023608  [ 2000/ 3200]\n",
      "loss: 1.421041  [ 2016/ 3200]\n",
      "loss: 1.063920  [ 2032/ 3200]\n",
      "loss: 0.882305  [ 2048/ 3200]\n",
      "loss: 0.999599  [ 2064/ 3200]\n",
      "loss: 0.932154  [ 2080/ 3200]\n",
      "loss: 0.847196  [ 2096/ 3200]\n",
      "loss: 1.117871  [ 2112/ 3200]\n",
      "loss: 1.179991  [ 2128/ 3200]\n",
      "loss: 0.850387  [ 2144/ 3200]\n",
      "loss: 0.881310  [ 2160/ 3200]\n",
      "loss: 0.905138  [ 2176/ 3200]\n",
      "loss: 1.243994  [ 2192/ 3200]\n",
      "loss: 1.225054  [ 2208/ 3200]\n",
      "loss: 0.826907  [ 2224/ 3200]\n",
      "loss: 1.159979  [ 2240/ 3200]\n",
      "loss: 1.004292  [ 2256/ 3200]\n",
      "loss: 1.047628  [ 2272/ 3200]\n",
      "loss: 1.145718  [ 2288/ 3200]\n",
      "loss: 0.941891  [ 2304/ 3200]\n",
      "loss: 1.126552  [ 2320/ 3200]\n",
      "loss: 1.018645  [ 2336/ 3200]\n",
      "loss: 1.121070  [ 2352/ 3200]\n",
      "loss: 0.879667  [ 2368/ 3200]\n",
      "loss: 1.091975  [ 2384/ 3200]\n",
      "loss: 0.986378  [ 2400/ 3200]\n",
      "loss: 1.076042  [ 2416/ 3200]\n",
      "loss: 0.956798  [ 2432/ 3200]\n",
      "loss: 0.844701  [ 2448/ 3200]\n",
      "loss: 0.794991  [ 2464/ 3200]\n",
      "loss: 1.053321  [ 2480/ 3200]\n",
      "loss: 1.269871  [ 2496/ 3200]\n",
      "loss: 1.218295  [ 2512/ 3200]\n",
      "loss: 0.922147  [ 2528/ 3200]\n",
      "loss: 0.978607  [ 2544/ 3200]\n",
      "loss: 1.150774  [ 2560/ 3200]\n",
      "loss: 0.971093  [ 2576/ 3200]\n",
      "loss: 1.055223  [ 2592/ 3200]\n",
      "loss: 1.098768  [ 2608/ 3200]\n",
      "loss: 1.015665  [ 2624/ 3200]\n",
      "loss: 0.891105  [ 2640/ 3200]\n",
      "loss: 0.873721  [ 2656/ 3200]\n",
      "loss: 0.932455  [ 2672/ 3200]\n",
      "loss: 1.149528  [ 2688/ 3200]\n",
      "loss: 0.916755  [ 2704/ 3200]\n",
      "loss: 0.910683  [ 2720/ 3200]\n",
      "loss: 1.215823  [ 2736/ 3200]\n",
      "loss: 0.974996  [ 2752/ 3200]\n",
      "loss: 1.144377  [ 2768/ 3200]\n",
      "loss: 0.912844  [ 2784/ 3200]\n",
      "loss: 0.888907  [ 2800/ 3200]\n",
      "loss: 0.990797  [ 2816/ 3200]\n",
      "loss: 0.949145  [ 2832/ 3200]\n",
      "loss: 1.180606  [ 2848/ 3200]\n",
      "loss: 0.999652  [ 2864/ 3200]\n",
      "loss: 0.859538  [ 2880/ 3200]\n",
      "loss: 1.045565  [ 2896/ 3200]\n",
      "loss: 0.970199  [ 2912/ 3200]\n",
      "loss: 1.029626  [ 2928/ 3200]\n",
      "loss: 0.894847  [ 2944/ 3200]\n",
      "loss: 1.128204  [ 2960/ 3200]\n",
      "loss: 0.746834  [ 2976/ 3200]\n",
      "loss: 1.014299  [ 2992/ 3200]\n",
      "loss: 0.942431  [ 3008/ 3200]\n",
      "loss: 1.287560  [ 3024/ 3200]\n",
      "loss: 0.959785  [ 3040/ 3200]\n",
      "loss: 1.267322  [ 3056/ 3200]\n",
      "loss: 1.006630  [ 3072/ 3200]\n",
      "loss: 1.040602  [ 3088/ 3200]\n",
      "loss: 1.033522  [ 3104/ 3200]\n",
      "loss: 1.039469  [ 3120/ 3200]\n",
      "loss: 0.977489  [ 3136/ 3200]\n",
      "loss: 0.763374  [ 3152/ 3200]\n",
      "loss: 1.201927  [ 3168/ 3200]\n",
      "loss: 1.039756  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 1.097344  [    0/ 3200]\n",
      "loss: 1.132700  [   16/ 3200]\n",
      "loss: 1.245655  [   32/ 3200]\n",
      "loss: 0.930468  [   48/ 3200]\n",
      "loss: 0.870391  [   64/ 3200]\n",
      "loss: 0.864658  [   80/ 3200]\n",
      "loss: 1.139355  [   96/ 3200]\n",
      "loss: 0.880355  [  112/ 3200]\n",
      "loss: 0.948022  [  128/ 3200]\n",
      "loss: 0.995672  [  144/ 3200]\n",
      "loss: 1.025229  [  160/ 3200]\n",
      "loss: 1.077670  [  176/ 3200]\n",
      "loss: 0.804131  [  192/ 3200]\n",
      "loss: 0.994913  [  208/ 3200]\n",
      "loss: 0.880693  [  224/ 3200]\n",
      "loss: 0.914361  [  240/ 3200]\n",
      "loss: 1.193170  [  256/ 3200]\n",
      "loss: 1.004472  [  272/ 3200]\n",
      "loss: 0.839495  [  288/ 3200]\n",
      "loss: 1.021152  [  304/ 3200]\n",
      "loss: 1.029257  [  320/ 3200]\n",
      "loss: 0.898895  [  336/ 3200]\n",
      "loss: 1.204842  [  352/ 3200]\n",
      "loss: 0.840089  [  368/ 3200]\n",
      "loss: 0.926979  [  384/ 3200]\n",
      "loss: 0.988639  [  400/ 3200]\n",
      "loss: 1.114435  [  416/ 3200]\n",
      "loss: 0.924387  [  432/ 3200]\n",
      "loss: 1.161490  [  448/ 3200]\n",
      "loss: 0.996336  [  464/ 3200]\n",
      "loss: 0.799152  [  480/ 3200]\n",
      "loss: 1.058295  [  496/ 3200]\n",
      "loss: 1.153298  [  512/ 3200]\n",
      "loss: 0.893647  [  528/ 3200]\n",
      "loss: 0.851855  [  544/ 3200]\n",
      "loss: 1.004775  [  560/ 3200]\n",
      "loss: 0.898514  [  576/ 3200]\n",
      "loss: 0.913356  [  592/ 3200]\n",
      "loss: 1.075823  [  608/ 3200]\n",
      "loss: 0.971034  [  624/ 3200]\n",
      "loss: 1.073171  [  640/ 3200]\n",
      "loss: 1.004233  [  656/ 3200]\n",
      "loss: 1.193766  [  672/ 3200]\n",
      "loss: 1.197527  [  688/ 3200]\n",
      "loss: 1.170789  [  704/ 3200]\n",
      "loss: 0.917806  [  720/ 3200]\n",
      "loss: 1.227951  [  736/ 3200]\n",
      "loss: 0.975499  [  752/ 3200]\n",
      "loss: 0.947452  [  768/ 3200]\n",
      "loss: 0.990206  [  784/ 3200]\n",
      "loss: 0.799215  [  800/ 3200]\n",
      "loss: 1.037315  [  816/ 3200]\n",
      "loss: 1.098114  [  832/ 3200]\n",
      "loss: 0.851700  [  848/ 3200]\n",
      "loss: 1.240719  [  864/ 3200]\n",
      "loss: 1.247811  [  880/ 3200]\n",
      "loss: 1.109114  [  896/ 3200]\n",
      "loss: 0.754167  [  912/ 3200]\n",
      "loss: 1.100541  [  928/ 3200]\n",
      "loss: 1.025198  [  944/ 3200]\n",
      "loss: 1.251708  [  960/ 3200]\n",
      "loss: 1.014955  [  976/ 3200]\n",
      "loss: 0.784544  [  992/ 3200]\n",
      "loss: 0.945980  [ 1008/ 3200]\n",
      "loss: 1.134606  [ 1024/ 3200]\n",
      "loss: 1.032567  [ 1040/ 3200]\n",
      "loss: 1.184100  [ 1056/ 3200]\n",
      "loss: 0.955059  [ 1072/ 3200]\n",
      "loss: 1.244970  [ 1088/ 3200]\n",
      "loss: 1.291124  [ 1104/ 3200]\n",
      "loss: 1.085626  [ 1120/ 3200]\n",
      "loss: 0.951948  [ 1136/ 3200]\n",
      "loss: 1.011199  [ 1152/ 3200]\n",
      "loss: 1.004863  [ 1168/ 3200]\n",
      "loss: 1.292705  [ 1184/ 3200]\n",
      "loss: 1.018494  [ 1200/ 3200]\n",
      "loss: 0.929677  [ 1216/ 3200]\n",
      "loss: 1.067090  [ 1232/ 3200]\n",
      "loss: 0.788587  [ 1248/ 3200]\n",
      "loss: 1.115994  [ 1264/ 3200]\n",
      "loss: 1.015115  [ 1280/ 3200]\n",
      "loss: 0.923187  [ 1296/ 3200]\n",
      "loss: 1.213044  [ 1312/ 3200]\n",
      "loss: 1.197190  [ 1328/ 3200]\n",
      "loss: 0.909539  [ 1344/ 3200]\n",
      "loss: 0.872360  [ 1360/ 3200]\n",
      "loss: 0.884972  [ 1376/ 3200]\n",
      "loss: 1.091370  [ 1392/ 3200]\n",
      "loss: 1.040384  [ 1408/ 3200]\n",
      "loss: 1.044132  [ 1424/ 3200]\n",
      "loss: 0.957061  [ 1440/ 3200]\n",
      "loss: 0.851583  [ 1456/ 3200]\n",
      "loss: 0.907087  [ 1472/ 3200]\n",
      "loss: 1.221753  [ 1488/ 3200]\n",
      "loss: 0.920732  [ 1504/ 3200]\n",
      "loss: 1.134654  [ 1520/ 3200]\n",
      "loss: 0.930747  [ 1536/ 3200]\n",
      "loss: 0.847274  [ 1552/ 3200]\n",
      "loss: 0.834907  [ 1568/ 3200]\n",
      "loss: 1.097123  [ 1584/ 3200]\n",
      "loss: 0.975962  [ 1600/ 3200]\n",
      "loss: 1.187933  [ 1616/ 3200]\n",
      "loss: 1.030854  [ 1632/ 3200]\n",
      "loss: 0.896490  [ 1648/ 3200]\n",
      "loss: 0.889825  [ 1664/ 3200]\n",
      "loss: 0.964151  [ 1680/ 3200]\n",
      "loss: 0.867904  [ 1696/ 3200]\n",
      "loss: 1.027886  [ 1712/ 3200]\n",
      "loss: 1.047803  [ 1728/ 3200]\n",
      "loss: 1.092135  [ 1744/ 3200]\n",
      "loss: 0.967026  [ 1760/ 3200]\n",
      "loss: 1.087844  [ 1776/ 3200]\n",
      "loss: 0.661496  [ 1792/ 3200]\n",
      "loss: 0.975769  [ 1808/ 3200]\n",
      "loss: 0.917235  [ 1824/ 3200]\n",
      "loss: 0.947467  [ 1840/ 3200]\n",
      "loss: 0.991951  [ 1856/ 3200]\n",
      "loss: 0.871247  [ 1872/ 3200]\n",
      "loss: 1.099817  [ 1888/ 3200]\n",
      "loss: 0.992644  [ 1904/ 3200]\n",
      "loss: 1.224949  [ 1920/ 3200]\n",
      "loss: 1.062105  [ 1936/ 3200]\n",
      "loss: 0.870981  [ 1952/ 3200]\n",
      "loss: 1.091185  [ 1968/ 3200]\n",
      "loss: 1.091046  [ 1984/ 3200]\n",
      "loss: 0.873025  [ 2000/ 3200]\n",
      "loss: 1.037260  [ 2016/ 3200]\n",
      "loss: 0.996126  [ 2032/ 3200]\n",
      "loss: 1.039832  [ 2048/ 3200]\n",
      "loss: 0.919608  [ 2064/ 3200]\n",
      "loss: 1.310689  [ 2080/ 3200]\n",
      "loss: 1.231513  [ 2096/ 3200]\n",
      "loss: 1.073186  [ 2112/ 3200]\n",
      "loss: 0.923941  [ 2128/ 3200]\n",
      "loss: 0.951043  [ 2144/ 3200]\n",
      "loss: 1.047267  [ 2160/ 3200]\n",
      "loss: 0.976732  [ 2176/ 3200]\n",
      "loss: 0.961234  [ 2192/ 3200]\n",
      "loss: 0.981659  [ 2208/ 3200]\n",
      "loss: 0.942370  [ 2224/ 3200]\n",
      "loss: 0.727162  [ 2240/ 3200]\n",
      "loss: 0.855556  [ 2256/ 3200]\n",
      "loss: 1.072292  [ 2272/ 3200]\n",
      "loss: 1.219266  [ 2288/ 3200]\n",
      "loss: 1.018336  [ 2304/ 3200]\n",
      "loss: 0.883673  [ 2320/ 3200]\n",
      "loss: 1.141598  [ 2336/ 3200]\n",
      "loss: 1.188818  [ 2352/ 3200]\n",
      "loss: 0.958433  [ 2368/ 3200]\n",
      "loss: 1.068107  [ 2384/ 3200]\n",
      "loss: 1.011629  [ 2400/ 3200]\n",
      "loss: 0.982875  [ 2416/ 3200]\n",
      "loss: 1.170932  [ 2432/ 3200]\n",
      "loss: 1.127007  [ 2448/ 3200]\n",
      "loss: 1.176179  [ 2464/ 3200]\n",
      "loss: 0.858158  [ 2480/ 3200]\n",
      "loss: 1.131888  [ 2496/ 3200]\n",
      "loss: 0.912874  [ 2512/ 3200]\n",
      "loss: 1.174946  [ 2528/ 3200]\n",
      "loss: 1.064954  [ 2544/ 3200]\n",
      "loss: 1.065279  [ 2560/ 3200]\n",
      "loss: 0.895586  [ 2576/ 3200]\n",
      "loss: 1.209617  [ 2592/ 3200]\n",
      "loss: 0.917024  [ 2608/ 3200]\n",
      "loss: 0.966421  [ 2624/ 3200]\n",
      "loss: 0.864118  [ 2640/ 3200]\n",
      "loss: 1.050561  [ 2656/ 3200]\n",
      "loss: 0.942686  [ 2672/ 3200]\n",
      "loss: 0.987794  [ 2688/ 3200]\n",
      "loss: 0.999005  [ 2704/ 3200]\n",
      "loss: 0.970481  [ 2720/ 3200]\n",
      "loss: 0.879032  [ 2736/ 3200]\n",
      "loss: 1.066315  [ 2752/ 3200]\n",
      "loss: 1.069089  [ 2768/ 3200]\n",
      "loss: 1.050183  [ 2784/ 3200]\n",
      "loss: 1.298029  [ 2800/ 3200]\n",
      "loss: 1.192423  [ 2816/ 3200]\n",
      "loss: 1.108624  [ 2832/ 3200]\n",
      "loss: 0.822035  [ 2848/ 3200]\n",
      "loss: 1.083539  [ 2864/ 3200]\n",
      "loss: 0.935697  [ 2880/ 3200]\n",
      "loss: 1.047077  [ 2896/ 3200]\n",
      "loss: 0.900504  [ 2912/ 3200]\n",
      "loss: 1.049516  [ 2928/ 3200]\n",
      "loss: 1.051419  [ 2944/ 3200]\n",
      "loss: 0.953978  [ 2960/ 3200]\n",
      "loss: 1.150196  [ 2976/ 3200]\n",
      "loss: 1.158434  [ 2992/ 3200]\n",
      "loss: 1.230658  [ 3008/ 3200]\n",
      "loss: 1.158578  [ 3024/ 3200]\n",
      "loss: 1.017511  [ 3040/ 3200]\n",
      "loss: 0.954049  [ 3056/ 3200]\n",
      "loss: 0.952575  [ 3072/ 3200]\n",
      "loss: 0.940287  [ 3088/ 3200]\n",
      "loss: 1.352779  [ 3104/ 3200]\n",
      "loss: 0.932147  [ 3120/ 3200]\n",
      "loss: 0.913582  [ 3136/ 3200]\n",
      "loss: 0.978709  [ 3152/ 3200]\n",
      "loss: 0.934739  [ 3168/ 3200]\n",
      "loss: 1.076926  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.930735  [    0/ 3200]\n",
      "loss: 0.789562  [   16/ 3200]\n",
      "loss: 1.056940  [   32/ 3200]\n",
      "loss: 0.860195  [   48/ 3200]\n",
      "loss: 0.971137  [   64/ 3200]\n",
      "loss: 1.177356  [   80/ 3200]\n",
      "loss: 1.036177  [   96/ 3200]\n",
      "loss: 0.855122  [  112/ 3200]\n",
      "loss: 0.969777  [  128/ 3200]\n",
      "loss: 1.332648  [  144/ 3200]\n",
      "loss: 1.301333  [  160/ 3200]\n",
      "loss: 0.862470  [  176/ 3200]\n",
      "loss: 0.987366  [  192/ 3200]\n",
      "loss: 1.149607  [  208/ 3200]\n",
      "loss: 0.936399  [  224/ 3200]\n",
      "loss: 0.904175  [  240/ 3200]\n",
      "loss: 1.037441  [  256/ 3200]\n",
      "loss: 0.890673  [  272/ 3200]\n",
      "loss: 0.950722  [  288/ 3200]\n",
      "loss: 1.088739  [  304/ 3200]\n",
      "loss: 0.703719  [  320/ 3200]\n",
      "loss: 0.942157  [  336/ 3200]\n",
      "loss: 1.011399  [  352/ 3200]\n",
      "loss: 0.945611  [  368/ 3200]\n",
      "loss: 1.344330  [  384/ 3200]\n",
      "loss: 1.199601  [  400/ 3200]\n",
      "loss: 0.832581  [  416/ 3200]\n",
      "loss: 0.957427  [  432/ 3200]\n",
      "loss: 1.003767  [  448/ 3200]\n",
      "loss: 1.076757  [  464/ 3200]\n",
      "loss: 1.135744  [  480/ 3200]\n",
      "loss: 1.219239  [  496/ 3200]\n",
      "loss: 1.177880  [  512/ 3200]\n",
      "loss: 0.875537  [  528/ 3200]\n",
      "loss: 1.108163  [  544/ 3200]\n",
      "loss: 1.169991  [  560/ 3200]\n",
      "loss: 1.123548  [  576/ 3200]\n",
      "loss: 1.130152  [  592/ 3200]\n",
      "loss: 0.941721  [  608/ 3200]\n",
      "loss: 1.020177  [  624/ 3200]\n",
      "loss: 1.033249  [  640/ 3200]\n",
      "loss: 0.898764  [  656/ 3200]\n",
      "loss: 0.808137  [  672/ 3200]\n",
      "loss: 0.866567  [  688/ 3200]\n",
      "loss: 0.994497  [  704/ 3200]\n",
      "loss: 1.024520  [  720/ 3200]\n",
      "loss: 0.813414  [  736/ 3200]\n",
      "loss: 0.961797  [  752/ 3200]\n",
      "loss: 1.142053  [  768/ 3200]\n",
      "loss: 0.878124  [  784/ 3200]\n",
      "loss: 1.186595  [  800/ 3200]\n",
      "loss: 1.202808  [  816/ 3200]\n",
      "loss: 1.025309  [  832/ 3200]\n",
      "loss: 1.245400  [  848/ 3200]\n",
      "loss: 0.975437  [  864/ 3200]\n",
      "loss: 0.902845  [  880/ 3200]\n",
      "loss: 0.992838  [  896/ 3200]\n",
      "loss: 0.921624  [  912/ 3200]\n",
      "loss: 1.106529  [  928/ 3200]\n",
      "loss: 1.018648  [  944/ 3200]\n",
      "loss: 0.974899  [  960/ 3200]\n",
      "loss: 1.196655  [  976/ 3200]\n",
      "loss: 1.059763  [  992/ 3200]\n",
      "loss: 0.896242  [ 1008/ 3200]\n",
      "loss: 0.874737  [ 1024/ 3200]\n",
      "loss: 1.085876  [ 1040/ 3200]\n",
      "loss: 0.776673  [ 1056/ 3200]\n",
      "loss: 0.782602  [ 1072/ 3200]\n",
      "loss: 0.958746  [ 1088/ 3200]\n",
      "loss: 0.815341  [ 1104/ 3200]\n",
      "loss: 1.025209  [ 1120/ 3200]\n",
      "loss: 0.856028  [ 1136/ 3200]\n",
      "loss: 0.997021  [ 1152/ 3200]\n",
      "loss: 0.854018  [ 1168/ 3200]\n",
      "loss: 0.923176  [ 1184/ 3200]\n",
      "loss: 1.268672  [ 1200/ 3200]\n",
      "loss: 0.850045  [ 1216/ 3200]\n",
      "loss: 1.017517  [ 1232/ 3200]\n",
      "loss: 0.976498  [ 1248/ 3200]\n",
      "loss: 1.114380  [ 1264/ 3200]\n",
      "loss: 0.995854  [ 1280/ 3200]\n",
      "loss: 0.919677  [ 1296/ 3200]\n",
      "loss: 1.092108  [ 1312/ 3200]\n",
      "loss: 0.990237  [ 1328/ 3200]\n",
      "loss: 1.007840  [ 1344/ 3200]\n",
      "loss: 1.294496  [ 1360/ 3200]\n",
      "loss: 1.134671  [ 1376/ 3200]\n",
      "loss: 0.984493  [ 1392/ 3200]\n",
      "loss: 0.865785  [ 1408/ 3200]\n",
      "loss: 1.153120  [ 1424/ 3200]\n",
      "loss: 0.856443  [ 1440/ 3200]\n",
      "loss: 1.285825  [ 1456/ 3200]\n",
      "loss: 1.144608  [ 1472/ 3200]\n",
      "loss: 0.969226  [ 1488/ 3200]\n",
      "loss: 0.956058  [ 1504/ 3200]\n",
      "loss: 0.775066  [ 1520/ 3200]\n",
      "loss: 1.112698  [ 1536/ 3200]\n",
      "loss: 1.111465  [ 1552/ 3200]\n",
      "loss: 0.994616  [ 1568/ 3200]\n",
      "loss: 1.056008  [ 1584/ 3200]\n",
      "loss: 0.910035  [ 1600/ 3200]\n",
      "loss: 1.128496  [ 1616/ 3200]\n",
      "loss: 1.104523  [ 1632/ 3200]\n",
      "loss: 0.985869  [ 1648/ 3200]\n",
      "loss: 0.893495  [ 1664/ 3200]\n",
      "loss: 1.050138  [ 1680/ 3200]\n",
      "loss: 1.057411  [ 1696/ 3200]\n",
      "loss: 0.934236  [ 1712/ 3200]\n",
      "loss: 0.765432  [ 1728/ 3200]\n",
      "loss: 0.947783  [ 1744/ 3200]\n",
      "loss: 0.691204  [ 1760/ 3200]\n",
      "loss: 1.180394  [ 1776/ 3200]\n",
      "loss: 0.998968  [ 1792/ 3200]\n",
      "loss: 1.417585  [ 1808/ 3200]\n",
      "loss: 1.034534  [ 1824/ 3200]\n",
      "loss: 1.092739  [ 1840/ 3200]\n",
      "loss: 1.179313  [ 1856/ 3200]\n",
      "loss: 1.045411  [ 1872/ 3200]\n",
      "loss: 1.216562  [ 1888/ 3200]\n",
      "loss: 1.019828  [ 1904/ 3200]\n",
      "loss: 1.127635  [ 1920/ 3200]\n",
      "loss: 0.960919  [ 1936/ 3200]\n",
      "loss: 1.226087  [ 1952/ 3200]\n",
      "loss: 0.938664  [ 1968/ 3200]\n",
      "loss: 0.749828  [ 1984/ 3200]\n",
      "loss: 1.154978  [ 2000/ 3200]\n",
      "loss: 0.964930  [ 2016/ 3200]\n",
      "loss: 0.888331  [ 2032/ 3200]\n",
      "loss: 0.803790  [ 2048/ 3200]\n",
      "loss: 1.010156  [ 2064/ 3200]\n",
      "loss: 0.971271  [ 2080/ 3200]\n",
      "loss: 1.071381  [ 2096/ 3200]\n",
      "loss: 0.817935  [ 2112/ 3200]\n",
      "loss: 1.072521  [ 2128/ 3200]\n",
      "loss: 1.183631  [ 2144/ 3200]\n",
      "loss: 1.104362  [ 2160/ 3200]\n",
      "loss: 0.780366  [ 2176/ 3200]\n",
      "loss: 1.079909  [ 2192/ 3200]\n",
      "loss: 0.890359  [ 2208/ 3200]\n",
      "loss: 1.084999  [ 2224/ 3200]\n",
      "loss: 1.100557  [ 2240/ 3200]\n",
      "loss: 0.856940  [ 2256/ 3200]\n",
      "loss: 1.122278  [ 2272/ 3200]\n",
      "loss: 1.147126  [ 2288/ 3200]\n",
      "loss: 1.273163  [ 2304/ 3200]\n",
      "loss: 0.947832  [ 2320/ 3200]\n",
      "loss: 0.971616  [ 2336/ 3200]\n",
      "loss: 1.127810  [ 2352/ 3200]\n",
      "loss: 0.844069  [ 2368/ 3200]\n",
      "loss: 0.803745  [ 2384/ 3200]\n",
      "loss: 1.004577  [ 2400/ 3200]\n",
      "loss: 0.910155  [ 2416/ 3200]\n",
      "loss: 1.055449  [ 2432/ 3200]\n",
      "loss: 1.011170  [ 2448/ 3200]\n",
      "loss: 0.815896  [ 2464/ 3200]\n",
      "loss: 1.260005  [ 2480/ 3200]\n",
      "loss: 1.124798  [ 2496/ 3200]\n",
      "loss: 1.052413  [ 2512/ 3200]\n",
      "loss: 1.217035  [ 2528/ 3200]\n",
      "loss: 0.796254  [ 2544/ 3200]\n",
      "loss: 0.752155  [ 2560/ 3200]\n",
      "loss: 0.824522  [ 2576/ 3200]\n",
      "loss: 0.917487  [ 2592/ 3200]\n",
      "loss: 1.162295  [ 2608/ 3200]\n",
      "loss: 1.384789  [ 2624/ 3200]\n",
      "loss: 0.923500  [ 2640/ 3200]\n",
      "loss: 0.924199  [ 2656/ 3200]\n",
      "loss: 0.959191  [ 2672/ 3200]\n",
      "loss: 0.881863  [ 2688/ 3200]\n",
      "loss: 1.167775  [ 2704/ 3200]\n",
      "loss: 0.828121  [ 2720/ 3200]\n",
      "loss: 1.072619  [ 2736/ 3200]\n",
      "loss: 0.970880  [ 2752/ 3200]\n",
      "loss: 0.644323  [ 2768/ 3200]\n",
      "loss: 0.907503  [ 2784/ 3200]\n",
      "loss: 0.945756  [ 2800/ 3200]\n",
      "loss: 1.017777  [ 2816/ 3200]\n",
      "loss: 0.964682  [ 2832/ 3200]\n",
      "loss: 0.938581  [ 2848/ 3200]\n",
      "loss: 1.277186  [ 2864/ 3200]\n",
      "loss: 0.905157  [ 2880/ 3200]\n",
      "loss: 0.763733  [ 2896/ 3200]\n",
      "loss: 1.029407  [ 2912/ 3200]\n",
      "loss: 1.014691  [ 2928/ 3200]\n",
      "loss: 0.849837  [ 2944/ 3200]\n",
      "loss: 1.121369  [ 2960/ 3200]\n",
      "loss: 1.096182  [ 2976/ 3200]\n",
      "loss: 0.914437  [ 2992/ 3200]\n",
      "loss: 0.992833  [ 3008/ 3200]\n",
      "loss: 1.079863  [ 3024/ 3200]\n",
      "loss: 1.059292  [ 3040/ 3200]\n",
      "loss: 0.965060  [ 3056/ 3200]\n",
      "loss: 1.073303  [ 3072/ 3200]\n",
      "loss: 1.004223  [ 3088/ 3200]\n",
      "loss: 0.930256  [ 3104/ 3200]\n",
      "loss: 0.723235  [ 3120/ 3200]\n",
      "loss: 1.032783  [ 3136/ 3200]\n",
      "loss: 1.002704  [ 3152/ 3200]\n",
      "loss: 0.744295  [ 3168/ 3200]\n",
      "loss: 0.984606  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 0.755474  [    0/ 3200]\n",
      "loss: 1.004892  [   16/ 3200]\n",
      "loss: 0.932918  [   32/ 3200]\n",
      "loss: 0.828794  [   48/ 3200]\n",
      "loss: 0.884149  [   64/ 3200]\n",
      "loss: 1.063609  [   80/ 3200]\n",
      "loss: 1.032473  [   96/ 3200]\n",
      "loss: 0.955369  [  112/ 3200]\n",
      "loss: 1.071419  [  128/ 3200]\n",
      "loss: 1.222562  [  144/ 3200]\n",
      "loss: 1.029220  [  160/ 3200]\n",
      "loss: 1.020077  [  176/ 3200]\n",
      "loss: 0.961118  [  192/ 3200]\n",
      "loss: 0.987725  [  208/ 3200]\n",
      "loss: 1.034263  [  224/ 3200]\n",
      "loss: 0.995784  [  240/ 3200]\n",
      "loss: 0.968819  [  256/ 3200]\n",
      "loss: 1.082845  [  272/ 3200]\n",
      "loss: 0.839186  [  288/ 3200]\n",
      "loss: 0.813676  [  304/ 3200]\n",
      "loss: 0.943206  [  320/ 3200]\n",
      "loss: 0.917461  [  336/ 3200]\n",
      "loss: 1.103743  [  352/ 3200]\n",
      "loss: 1.049897  [  368/ 3200]\n",
      "loss: 1.028530  [  384/ 3200]\n",
      "loss: 1.207743  [  400/ 3200]\n",
      "loss: 0.886494  [  416/ 3200]\n",
      "loss: 1.101345  [  432/ 3200]\n",
      "loss: 1.225953  [  448/ 3200]\n",
      "loss: 1.071611  [  464/ 3200]\n",
      "loss: 0.939900  [  480/ 3200]\n",
      "loss: 1.155336  [  496/ 3200]\n",
      "loss: 0.758112  [  512/ 3200]\n",
      "loss: 1.079982  [  528/ 3200]\n",
      "loss: 1.098333  [  544/ 3200]\n",
      "loss: 0.849497  [  560/ 3200]\n",
      "loss: 0.780325  [  576/ 3200]\n",
      "loss: 1.042622  [  592/ 3200]\n",
      "loss: 1.130341  [  608/ 3200]\n",
      "loss: 1.137945  [  624/ 3200]\n",
      "loss: 1.171221  [  640/ 3200]\n",
      "loss: 0.738677  [  656/ 3200]\n",
      "loss: 1.129958  [  672/ 3200]\n",
      "loss: 1.047167  [  688/ 3200]\n",
      "loss: 0.830080  [  704/ 3200]\n",
      "loss: 1.078238  [  720/ 3200]\n",
      "loss: 0.787866  [  736/ 3200]\n",
      "loss: 0.875165  [  752/ 3200]\n",
      "loss: 0.973644  [  768/ 3200]\n",
      "loss: 1.492402  [  784/ 3200]\n",
      "loss: 1.100112  [  800/ 3200]\n",
      "loss: 0.975749  [  816/ 3200]\n",
      "loss: 1.035544  [  832/ 3200]\n",
      "loss: 1.268119  [  848/ 3200]\n",
      "loss: 1.039559  [  864/ 3200]\n",
      "loss: 0.989451  [  880/ 3200]\n",
      "loss: 0.949283  [  896/ 3200]\n",
      "loss: 1.096330  [  912/ 3200]\n",
      "loss: 0.976882  [  928/ 3200]\n",
      "loss: 1.088823  [  944/ 3200]\n",
      "loss: 0.916281  [  960/ 3200]\n",
      "loss: 1.195346  [  976/ 3200]\n",
      "loss: 0.982417  [  992/ 3200]\n",
      "loss: 1.187651  [ 1008/ 3200]\n",
      "loss: 0.801825  [ 1024/ 3200]\n",
      "loss: 0.943612  [ 1040/ 3200]\n",
      "loss: 0.995125  [ 1056/ 3200]\n",
      "loss: 0.887289  [ 1072/ 3200]\n",
      "loss: 0.830101  [ 1088/ 3200]\n",
      "loss: 0.902020  [ 1104/ 3200]\n",
      "loss: 0.965356  [ 1120/ 3200]\n",
      "loss: 1.075462  [ 1136/ 3200]\n",
      "loss: 0.987135  [ 1152/ 3200]\n",
      "loss: 1.048387  [ 1168/ 3200]\n",
      "loss: 0.939255  [ 1184/ 3200]\n",
      "loss: 0.952007  [ 1200/ 3200]\n",
      "loss: 0.962369  [ 1216/ 3200]\n",
      "loss: 1.145214  [ 1232/ 3200]\n",
      "loss: 0.874669  [ 1248/ 3200]\n",
      "loss: 1.014871  [ 1264/ 3200]\n",
      "loss: 1.007485  [ 1280/ 3200]\n",
      "loss: 0.684966  [ 1296/ 3200]\n",
      "loss: 1.028861  [ 1312/ 3200]\n",
      "loss: 0.789047  [ 1328/ 3200]\n",
      "loss: 0.674425  [ 1344/ 3200]\n",
      "loss: 0.726940  [ 1360/ 3200]\n",
      "loss: 0.812950  [ 1376/ 3200]\n",
      "loss: 0.940579  [ 1392/ 3200]\n",
      "loss: 1.014582  [ 1408/ 3200]\n",
      "loss: 0.657129  [ 1424/ 3200]\n",
      "loss: 0.926351  [ 1440/ 3200]\n",
      "loss: 1.274828  [ 1456/ 3200]\n",
      "loss: 0.917679  [ 1472/ 3200]\n",
      "loss: 1.203474  [ 1488/ 3200]\n",
      "loss: 1.383433  [ 1504/ 3200]\n",
      "loss: 1.237538  [ 1520/ 3200]\n",
      "loss: 1.094104  [ 1536/ 3200]\n",
      "loss: 0.828971  [ 1552/ 3200]\n",
      "loss: 1.000628  [ 1568/ 3200]\n",
      "loss: 0.984712  [ 1584/ 3200]\n",
      "loss: 1.037051  [ 1600/ 3200]\n",
      "loss: 0.961154  [ 1616/ 3200]\n",
      "loss: 1.102755  [ 1632/ 3200]\n",
      "loss: 1.137954  [ 1648/ 3200]\n",
      "loss: 1.284087  [ 1664/ 3200]\n",
      "loss: 1.196222  [ 1680/ 3200]\n",
      "loss: 1.031433  [ 1696/ 3200]\n",
      "loss: 1.011627  [ 1712/ 3200]\n",
      "loss: 0.804629  [ 1728/ 3200]\n",
      "loss: 0.910877  [ 1744/ 3200]\n",
      "loss: 1.009210  [ 1760/ 3200]\n",
      "loss: 1.046755  [ 1776/ 3200]\n",
      "loss: 0.864463  [ 1792/ 3200]\n",
      "loss: 0.740879  [ 1808/ 3200]\n",
      "loss: 0.925447  [ 1824/ 3200]\n",
      "loss: 1.061526  [ 1840/ 3200]\n",
      "loss: 0.839586  [ 1856/ 3200]\n",
      "loss: 1.017615  [ 1872/ 3200]\n",
      "loss: 0.833338  [ 1888/ 3200]\n",
      "loss: 0.886161  [ 1904/ 3200]\n",
      "loss: 1.003785  [ 1920/ 3200]\n",
      "loss: 1.059879  [ 1936/ 3200]\n",
      "loss: 0.977358  [ 1952/ 3200]\n",
      "loss: 0.974979  [ 1968/ 3200]\n",
      "loss: 0.927248  [ 1984/ 3200]\n",
      "loss: 1.056676  [ 2000/ 3200]\n",
      "loss: 0.893740  [ 2016/ 3200]\n",
      "loss: 0.946889  [ 2032/ 3200]\n",
      "loss: 0.993591  [ 2048/ 3200]\n",
      "loss: 0.854009  [ 2064/ 3200]\n",
      "loss: 0.958106  [ 2080/ 3200]\n",
      "loss: 0.979576  [ 2096/ 3200]\n",
      "loss: 1.027086  [ 2112/ 3200]\n",
      "loss: 0.863638  [ 2128/ 3200]\n",
      "loss: 0.822640  [ 2144/ 3200]\n",
      "loss: 1.092783  [ 2160/ 3200]\n",
      "loss: 0.846992  [ 2176/ 3200]\n",
      "loss: 0.958717  [ 2192/ 3200]\n",
      "loss: 1.034552  [ 2208/ 3200]\n",
      "loss: 1.426490  [ 2224/ 3200]\n",
      "loss: 0.807991  [ 2240/ 3200]\n",
      "loss: 0.960296  [ 2256/ 3200]\n",
      "loss: 1.009167  [ 2272/ 3200]\n",
      "loss: 1.123653  [ 2288/ 3200]\n",
      "loss: 0.911806  [ 2304/ 3200]\n",
      "loss: 1.086362  [ 2320/ 3200]\n",
      "loss: 1.103077  [ 2336/ 3200]\n",
      "loss: 1.036282  [ 2352/ 3200]\n",
      "loss: 0.792863  [ 2368/ 3200]\n",
      "loss: 1.031389  [ 2384/ 3200]\n",
      "loss: 0.884203  [ 2400/ 3200]\n",
      "loss: 1.007292  [ 2416/ 3200]\n",
      "loss: 0.944761  [ 2432/ 3200]\n",
      "loss: 1.229752  [ 2448/ 3200]\n",
      "loss: 1.019042  [ 2464/ 3200]\n",
      "loss: 1.131515  [ 2480/ 3200]\n",
      "loss: 1.161064  [ 2496/ 3200]\n",
      "loss: 1.046283  [ 2512/ 3200]\n",
      "loss: 0.811629  [ 2528/ 3200]\n",
      "loss: 1.062617  [ 2544/ 3200]\n",
      "loss: 1.086546  [ 2560/ 3200]\n",
      "loss: 1.070413  [ 2576/ 3200]\n",
      "loss: 0.806628  [ 2592/ 3200]\n",
      "loss: 1.049287  [ 2608/ 3200]\n",
      "loss: 1.015373  [ 2624/ 3200]\n",
      "loss: 1.344890  [ 2640/ 3200]\n",
      "loss: 1.028585  [ 2656/ 3200]\n",
      "loss: 0.891260  [ 2672/ 3200]\n",
      "loss: 0.872277  [ 2688/ 3200]\n",
      "loss: 1.116965  [ 2704/ 3200]\n",
      "loss: 1.053476  [ 2720/ 3200]\n",
      "loss: 1.155710  [ 2736/ 3200]\n",
      "loss: 1.013244  [ 2752/ 3200]\n",
      "loss: 0.856200  [ 2768/ 3200]\n",
      "loss: 0.943115  [ 2784/ 3200]\n",
      "loss: 0.947439  [ 2800/ 3200]\n",
      "loss: 1.243448  [ 2816/ 3200]\n",
      "loss: 0.859048  [ 2832/ 3200]\n",
      "loss: 0.990627  [ 2848/ 3200]\n",
      "loss: 1.015148  [ 2864/ 3200]\n",
      "loss: 0.835912  [ 2880/ 3200]\n",
      "loss: 1.196972  [ 2896/ 3200]\n",
      "loss: 1.163995  [ 2912/ 3200]\n",
      "loss: 1.008801  [ 2928/ 3200]\n",
      "loss: 1.073640  [ 2944/ 3200]\n",
      "loss: 0.735363  [ 2960/ 3200]\n",
      "loss: 1.164467  [ 2976/ 3200]\n",
      "loss: 0.904326  [ 2992/ 3200]\n",
      "loss: 0.827471  [ 3008/ 3200]\n",
      "loss: 1.096264  [ 3024/ 3200]\n",
      "loss: 0.872970  [ 3040/ 3200]\n",
      "loss: 0.941857  [ 3056/ 3200]\n",
      "loss: 1.080180  [ 3072/ 3200]\n",
      "loss: 0.915660  [ 3088/ 3200]\n",
      "loss: 0.980173  [ 3104/ 3200]\n",
      "loss: 1.056494  [ 3120/ 3200]\n",
      "loss: 1.158275  [ 3136/ 3200]\n",
      "loss: 0.675543  [ 3152/ 3200]\n",
      "loss: 1.093034  [ 3168/ 3200]\n",
      "loss: 0.833840  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.762753  [    0/ 3200]\n",
      "loss: 1.052281  [   16/ 3200]\n",
      "loss: 0.862589  [   32/ 3200]\n",
      "loss: 0.956759  [   48/ 3200]\n",
      "loss: 0.872413  [   64/ 3200]\n",
      "loss: 1.145385  [   80/ 3200]\n",
      "loss: 0.845537  [   96/ 3200]\n",
      "loss: 1.003147  [  112/ 3200]\n",
      "loss: 0.970485  [  128/ 3200]\n",
      "loss: 0.966622  [  144/ 3200]\n",
      "loss: 0.833257  [  160/ 3200]\n",
      "loss: 0.827602  [  176/ 3200]\n",
      "loss: 1.078345  [  192/ 3200]\n",
      "loss: 1.017551  [  208/ 3200]\n",
      "loss: 0.893533  [  224/ 3200]\n",
      "loss: 1.051114  [  240/ 3200]\n",
      "loss: 0.862598  [  256/ 3200]\n",
      "loss: 1.043934  [  272/ 3200]\n",
      "loss: 1.018713  [  288/ 3200]\n",
      "loss: 1.032555  [  304/ 3200]\n",
      "loss: 1.093177  [  320/ 3200]\n",
      "loss: 1.041117  [  336/ 3200]\n",
      "loss: 1.037461  [  352/ 3200]\n",
      "loss: 1.477985  [  368/ 3200]\n",
      "loss: 0.980935  [  384/ 3200]\n",
      "loss: 0.969504  [  400/ 3200]\n",
      "loss: 1.049026  [  416/ 3200]\n",
      "loss: 0.893943  [  432/ 3200]\n",
      "loss: 1.024291  [  448/ 3200]\n",
      "loss: 1.054604  [  464/ 3200]\n",
      "loss: 1.259960  [  480/ 3200]\n",
      "loss: 1.249622  [  496/ 3200]\n",
      "loss: 0.982027  [  512/ 3200]\n",
      "loss: 0.940484  [  528/ 3200]\n",
      "loss: 0.889770  [  544/ 3200]\n",
      "loss: 0.963744  [  560/ 3200]\n",
      "loss: 0.955782  [  576/ 3200]\n",
      "loss: 0.870767  [  592/ 3200]\n",
      "loss: 1.220021  [  608/ 3200]\n",
      "loss: 0.924301  [  624/ 3200]\n",
      "loss: 0.752936  [  640/ 3200]\n",
      "loss: 0.863828  [  656/ 3200]\n",
      "loss: 0.833850  [  672/ 3200]\n",
      "loss: 1.125177  [  688/ 3200]\n",
      "loss: 0.844191  [  704/ 3200]\n",
      "loss: 0.929795  [  720/ 3200]\n",
      "loss: 1.297204  [  736/ 3200]\n",
      "loss: 1.094457  [  752/ 3200]\n",
      "loss: 1.027534  [  768/ 3200]\n",
      "loss: 0.944143  [  784/ 3200]\n",
      "loss: 1.065082  [  800/ 3200]\n",
      "loss: 1.067110  [  816/ 3200]\n",
      "loss: 1.029107  [  832/ 3200]\n",
      "loss: 1.056789  [  848/ 3200]\n",
      "loss: 1.056012  [  864/ 3200]\n",
      "loss: 1.151860  [  880/ 3200]\n",
      "loss: 0.884176  [  896/ 3200]\n",
      "loss: 0.961096  [  912/ 3200]\n",
      "loss: 0.829546  [  928/ 3200]\n",
      "loss: 0.978977  [  944/ 3200]\n",
      "loss: 1.078378  [  960/ 3200]\n",
      "loss: 1.306457  [  976/ 3200]\n",
      "loss: 0.936592  [  992/ 3200]\n",
      "loss: 1.092809  [ 1008/ 3200]\n",
      "loss: 1.189945  [ 1024/ 3200]\n",
      "loss: 0.821262  [ 1040/ 3200]\n",
      "loss: 1.373223  [ 1056/ 3200]\n",
      "loss: 1.284486  [ 1072/ 3200]\n",
      "loss: 1.295852  [ 1088/ 3200]\n",
      "loss: 0.917292  [ 1104/ 3200]\n",
      "loss: 0.847308  [ 1120/ 3200]\n",
      "loss: 0.817471  [ 1136/ 3200]\n",
      "loss: 0.989312  [ 1152/ 3200]\n",
      "loss: 1.136591  [ 1168/ 3200]\n",
      "loss: 0.965850  [ 1184/ 3200]\n",
      "loss: 0.936895  [ 1200/ 3200]\n",
      "loss: 1.153509  [ 1216/ 3200]\n",
      "loss: 0.932038  [ 1232/ 3200]\n",
      "loss: 0.971628  [ 1248/ 3200]\n",
      "loss: 0.846746  [ 1264/ 3200]\n",
      "loss: 0.938257  [ 1280/ 3200]\n",
      "loss: 0.943697  [ 1296/ 3200]\n",
      "loss: 0.839445  [ 1312/ 3200]\n",
      "loss: 0.962157  [ 1328/ 3200]\n",
      "loss: 0.995372  [ 1344/ 3200]\n",
      "loss: 0.869462  [ 1360/ 3200]\n",
      "loss: 1.186559  [ 1376/ 3200]\n",
      "loss: 0.935762  [ 1392/ 3200]\n",
      "loss: 1.197114  [ 1408/ 3200]\n",
      "loss: 1.079478  [ 1424/ 3200]\n",
      "loss: 1.070498  [ 1440/ 3200]\n",
      "loss: 1.296134  [ 1456/ 3200]\n",
      "loss: 0.952597  [ 1472/ 3200]\n",
      "loss: 1.057892  [ 1488/ 3200]\n",
      "loss: 0.831890  [ 1504/ 3200]\n",
      "loss: 0.885395  [ 1520/ 3200]\n",
      "loss: 0.937874  [ 1536/ 3200]\n",
      "loss: 0.974819  [ 1552/ 3200]\n",
      "loss: 0.894725  [ 1568/ 3200]\n",
      "loss: 1.301178  [ 1584/ 3200]\n",
      "loss: 1.084800  [ 1600/ 3200]\n",
      "loss: 0.798648  [ 1616/ 3200]\n",
      "loss: 0.829109  [ 1632/ 3200]\n",
      "loss: 0.909173  [ 1648/ 3200]\n",
      "loss: 1.062251  [ 1664/ 3200]\n",
      "loss: 1.016891  [ 1680/ 3200]\n",
      "loss: 0.799538  [ 1696/ 3200]\n",
      "loss: 0.658145  [ 1712/ 3200]\n",
      "loss: 1.135494  [ 1728/ 3200]\n",
      "loss: 0.772061  [ 1744/ 3200]\n",
      "loss: 0.957542  [ 1760/ 3200]\n",
      "loss: 0.843502  [ 1776/ 3200]\n",
      "loss: 0.977460  [ 1792/ 3200]\n",
      "loss: 0.775282  [ 1808/ 3200]\n",
      "loss: 1.018535  [ 1824/ 3200]\n",
      "loss: 0.846834  [ 1840/ 3200]\n",
      "loss: 1.066697  [ 1856/ 3200]\n",
      "loss: 0.949438  [ 1872/ 3200]\n",
      "loss: 0.826919  [ 1888/ 3200]\n",
      "loss: 0.948053  [ 1904/ 3200]\n",
      "loss: 0.841791  [ 1920/ 3200]\n",
      "loss: 0.901184  [ 1936/ 3200]\n",
      "loss: 0.852320  [ 1952/ 3200]\n",
      "loss: 0.904323  [ 1968/ 3200]\n",
      "loss: 1.014140  [ 1984/ 3200]\n",
      "loss: 0.710894  [ 2000/ 3200]\n",
      "loss: 0.995823  [ 2016/ 3200]\n",
      "loss: 1.171526  [ 2032/ 3200]\n",
      "loss: 0.851461  [ 2048/ 3200]\n",
      "loss: 1.011736  [ 2064/ 3200]\n",
      "loss: 1.203963  [ 2080/ 3200]\n",
      "loss: 0.847586  [ 2096/ 3200]\n",
      "loss: 0.898913  [ 2112/ 3200]\n",
      "loss: 0.813260  [ 2128/ 3200]\n",
      "loss: 1.246031  [ 2144/ 3200]\n",
      "loss: 1.023854  [ 2160/ 3200]\n",
      "loss: 1.136858  [ 2176/ 3200]\n",
      "loss: 1.180955  [ 2192/ 3200]\n",
      "loss: 1.039266  [ 2208/ 3200]\n",
      "loss: 0.934643  [ 2224/ 3200]\n",
      "loss: 0.962789  [ 2240/ 3200]\n",
      "loss: 0.748123  [ 2256/ 3200]\n",
      "loss: 1.171609  [ 2272/ 3200]\n",
      "loss: 0.892896  [ 2288/ 3200]\n",
      "loss: 1.047962  [ 2304/ 3200]\n",
      "loss: 1.031845  [ 2320/ 3200]\n",
      "loss: 0.920746  [ 2336/ 3200]\n",
      "loss: 0.873663  [ 2352/ 3200]\n",
      "loss: 1.014733  [ 2368/ 3200]\n",
      "loss: 1.103543  [ 2384/ 3200]\n",
      "loss: 1.050770  [ 2400/ 3200]\n",
      "loss: 0.711631  [ 2416/ 3200]\n",
      "loss: 1.052089  [ 2432/ 3200]\n",
      "loss: 0.821470  [ 2448/ 3200]\n",
      "loss: 0.956405  [ 2464/ 3200]\n",
      "loss: 1.261622  [ 2480/ 3200]\n",
      "loss: 1.204960  [ 2496/ 3200]\n",
      "loss: 0.989607  [ 2512/ 3200]\n",
      "loss: 0.975802  [ 2528/ 3200]\n",
      "loss: 0.895210  [ 2544/ 3200]\n",
      "loss: 1.201368  [ 2560/ 3200]\n",
      "loss: 1.070326  [ 2576/ 3200]\n",
      "loss: 1.000204  [ 2592/ 3200]\n",
      "loss: 1.109588  [ 2608/ 3200]\n",
      "loss: 1.084346  [ 2624/ 3200]\n",
      "loss: 0.868603  [ 2640/ 3200]\n",
      "loss: 1.107519  [ 2656/ 3200]\n",
      "loss: 0.981469  [ 2672/ 3200]\n",
      "loss: 0.838360  [ 2688/ 3200]\n",
      "loss: 0.756600  [ 2704/ 3200]\n",
      "loss: 1.149340  [ 2720/ 3200]\n",
      "loss: 1.062374  [ 2736/ 3200]\n",
      "loss: 1.014150  [ 2752/ 3200]\n",
      "loss: 0.797995  [ 2768/ 3200]\n",
      "loss: 0.972958  [ 2784/ 3200]\n",
      "loss: 0.992574  [ 2800/ 3200]\n",
      "loss: 0.866529  [ 2816/ 3200]\n",
      "loss: 0.927411  [ 2832/ 3200]\n",
      "loss: 0.972740  [ 2848/ 3200]\n",
      "loss: 1.182707  [ 2864/ 3200]\n",
      "loss: 0.787236  [ 2880/ 3200]\n",
      "loss: 0.855807  [ 2896/ 3200]\n",
      "loss: 0.903995  [ 2912/ 3200]\n",
      "loss: 0.779782  [ 2928/ 3200]\n",
      "loss: 0.969040  [ 2944/ 3200]\n",
      "loss: 0.836582  [ 2960/ 3200]\n",
      "loss: 0.791537  [ 2976/ 3200]\n",
      "loss: 1.127247  [ 2992/ 3200]\n",
      "loss: 1.105096  [ 3008/ 3200]\n",
      "loss: 1.060579  [ 3024/ 3200]\n",
      "loss: 0.974930  [ 3040/ 3200]\n",
      "loss: 1.362059  [ 3056/ 3200]\n",
      "loss: 1.000477  [ 3072/ 3200]\n",
      "loss: 0.933014  [ 3088/ 3200]\n",
      "loss: 1.388515  [ 3104/ 3200]\n",
      "loss: 1.147360  [ 3120/ 3200]\n",
      "loss: 1.272632  [ 3136/ 3200]\n",
      "loss: 1.126696  [ 3152/ 3200]\n",
      "loss: 0.904505  [ 3168/ 3200]\n",
      "loss: 0.766698  [ 3184/ 3200]\n",
      "\n",
      "CPU times: user 9.36 s, sys: 361 ms, total: 9.72 s\n",
      "Wall time: 9.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cpu_model = train_neural_network(epochs, optimizer, train_dataloader, loss_function, cpu_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gswfmqmeFNuw"
   },
   "source": [
    "Test our Neural Network on the test data using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDcZgOTDFOCx",
    "outputId": "658d3ca6-a40d-43b7-ba52-d792366c3a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:\n",
      "Avg loss               : 0.061879\n",
      "f1 macro averaged score: 0.609656\n",
      "Accuracy               : 62.1%\n",
      "Confusion matrix       :\n",
      "tensor([[242,  31,  14,  10],\n",
      "        [ 27, 103, 120,  74],\n",
      "        [ 14,  51, 285,   6],\n",
      "        [ 47,  61,  67, 224]])\n",
      "CPU times: user 39.4 ms, sys: 2.75 ms, total: 42.2 ms\n",
      "Wall time: 72.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = test_neural_network(test_dataloader, loss_function, cpu_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMCsNydCrfyb"
   },
   "source": [
    "### Step 6 - Training using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8NiJMLk2UHL"
   },
   "source": [
    "Make sure that the GPU is in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjtGBb2UoSOF",
    "outputId": "ed2575b7-6970-4a14-8046-0c36d8fef08d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXa17Mz6L6GS"
   },
   "source": [
    "Initialize model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5B-6jV7PL7Md",
    "outputId": "260e36d8-4271-4a77-81e9-03ae1816eb05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=26, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gpu_model = NeuralNetwork().to(device)\n",
    "print(gpu_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5N7Ct_FngI-"
   },
   "source": [
    "Train model using $learning$ $rate = 0.002$, Stochastic Gradient Descent optimizer and Cross Entropy Loss function for $30$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUXgNBrkniA2"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "optimizer = SGD(params=gpu_model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kykEtE6Y2XqM"
   },
   "source": [
    "Train our Neural Network using the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZ2w7NuYibst",
    "outputId": "417a4d10-f613-4ce4-846c-f246010fab64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 1.316662  [ 1200/ 3200]\n",
      "loss: 1.358754  [ 1216/ 3200]\n",
      "loss: 1.338683  [ 1232/ 3200]\n",
      "loss: 1.334791  [ 1248/ 3200]\n",
      "loss: 1.303640  [ 1264/ 3200]\n",
      "loss: 1.380085  [ 1280/ 3200]\n",
      "loss: 1.290490  [ 1296/ 3200]\n",
      "loss: 1.372623  [ 1312/ 3200]\n",
      "loss: 1.338779  [ 1328/ 3200]\n",
      "loss: 1.274673  [ 1344/ 3200]\n",
      "loss: 1.325277  [ 1360/ 3200]\n",
      "loss: 1.290899  [ 1376/ 3200]\n",
      "loss: 1.358966  [ 1392/ 3200]\n",
      "loss: 1.314071  [ 1408/ 3200]\n",
      "loss: 1.311050  [ 1424/ 3200]\n",
      "loss: 1.352547  [ 1440/ 3200]\n",
      "loss: 1.363265  [ 1456/ 3200]\n",
      "loss: 1.308970  [ 1472/ 3200]\n",
      "loss: 1.315032  [ 1488/ 3200]\n",
      "loss: 1.338723  [ 1504/ 3200]\n",
      "loss: 1.343131  [ 1520/ 3200]\n",
      "loss: 1.340748  [ 1536/ 3200]\n",
      "loss: 1.380608  [ 1552/ 3200]\n",
      "loss: 1.333950  [ 1568/ 3200]\n",
      "loss: 1.315450  [ 1584/ 3200]\n",
      "loss: 1.327723  [ 1600/ 3200]\n",
      "loss: 1.329021  [ 1616/ 3200]\n",
      "loss: 1.330154  [ 1632/ 3200]\n",
      "loss: 1.335737  [ 1648/ 3200]\n",
      "loss: 1.317521  [ 1664/ 3200]\n",
      "loss: 1.356256  [ 1680/ 3200]\n",
      "loss: 1.312389  [ 1696/ 3200]\n",
      "loss: 1.335533  [ 1712/ 3200]\n",
      "loss: 1.344584  [ 1728/ 3200]\n",
      "loss: 1.306438  [ 1744/ 3200]\n",
      "loss: 1.310580  [ 1760/ 3200]\n",
      "loss: 1.321425  [ 1776/ 3200]\n",
      "loss: 1.303898  [ 1792/ 3200]\n",
      "loss: 1.331511  [ 1808/ 3200]\n",
      "loss: 1.338095  [ 1824/ 3200]\n",
      "loss: 1.328736  [ 1840/ 3200]\n",
      "loss: 1.340114  [ 1856/ 3200]\n",
      "loss: 1.289600  [ 1872/ 3200]\n",
      "loss: 1.356107  [ 1888/ 3200]\n",
      "loss: 1.398810  [ 1904/ 3200]\n",
      "loss: 1.341286  [ 1920/ 3200]\n",
      "loss: 1.299695  [ 1936/ 3200]\n",
      "loss: 1.337025  [ 1952/ 3200]\n",
      "loss: 1.351846  [ 1968/ 3200]\n",
      "loss: 1.326343  [ 1984/ 3200]\n",
      "loss: 1.333400  [ 2000/ 3200]\n",
      "loss: 1.277457  [ 2016/ 3200]\n",
      "loss: 1.314609  [ 2032/ 3200]\n",
      "loss: 1.335302  [ 2048/ 3200]\n",
      "loss: 1.342529  [ 2064/ 3200]\n",
      "loss: 1.262529  [ 2080/ 3200]\n",
      "loss: 1.283719  [ 2096/ 3200]\n",
      "loss: 1.386901  [ 2112/ 3200]\n",
      "loss: 1.307891  [ 2128/ 3200]\n",
      "loss: 1.346403  [ 2144/ 3200]\n",
      "loss: 1.311544  [ 2160/ 3200]\n",
      "loss: 1.317634  [ 2176/ 3200]\n",
      "loss: 1.329715  [ 2192/ 3200]\n",
      "loss: 1.305962  [ 2208/ 3200]\n",
      "loss: 1.280897  [ 2224/ 3200]\n",
      "loss: 1.345275  [ 2240/ 3200]\n",
      "loss: 1.374445  [ 2256/ 3200]\n",
      "loss: 1.327260  [ 2272/ 3200]\n",
      "loss: 1.312356  [ 2288/ 3200]\n",
      "loss: 1.303780  [ 2304/ 3200]\n",
      "loss: 1.328501  [ 2320/ 3200]\n",
      "loss: 1.322778  [ 2336/ 3200]\n",
      "loss: 1.339977  [ 2352/ 3200]\n",
      "loss: 1.352141  [ 2368/ 3200]\n",
      "loss: 1.319854  [ 2384/ 3200]\n",
      "loss: 1.352183  [ 2400/ 3200]\n",
      "loss: 1.314386  [ 2416/ 3200]\n",
      "loss: 1.306593  [ 2432/ 3200]\n",
      "loss: 1.305877  [ 2448/ 3200]\n",
      "loss: 1.321874  [ 2464/ 3200]\n",
      "loss: 1.347313  [ 2480/ 3200]\n",
      "loss: 1.315167  [ 2496/ 3200]\n",
      "loss: 1.358973  [ 2512/ 3200]\n",
      "loss: 1.341433  [ 2528/ 3200]\n",
      "loss: 1.346354  [ 2544/ 3200]\n",
      "loss: 1.315702  [ 2560/ 3200]\n",
      "loss: 1.274575  [ 2576/ 3200]\n",
      "loss: 1.271281  [ 2592/ 3200]\n",
      "loss: 1.352297  [ 2608/ 3200]\n",
      "loss: 1.357770  [ 2624/ 3200]\n",
      "loss: 1.291362  [ 2640/ 3200]\n",
      "loss: 1.310717  [ 2656/ 3200]\n",
      "loss: 1.291710  [ 2672/ 3200]\n",
      "loss: 1.323957  [ 2688/ 3200]\n",
      "loss: 1.310081  [ 2704/ 3200]\n",
      "loss: 1.298488  [ 2720/ 3200]\n",
      "loss: 1.349937  [ 2736/ 3200]\n",
      "loss: 1.365721  [ 2752/ 3200]\n",
      "loss: 1.300095  [ 2768/ 3200]\n",
      "loss: 1.337552  [ 2784/ 3200]\n",
      "loss: 1.324265  [ 2800/ 3200]\n",
      "loss: 1.339149  [ 2816/ 3200]\n",
      "loss: 1.320698  [ 2832/ 3200]\n",
      "loss: 1.340887  [ 2848/ 3200]\n",
      "loss: 1.312169  [ 2864/ 3200]\n",
      "loss: 1.299395  [ 2880/ 3200]\n",
      "loss: 1.297930  [ 2896/ 3200]\n",
      "loss: 1.353005  [ 2912/ 3200]\n",
      "loss: 1.328880  [ 2928/ 3200]\n",
      "loss: 1.354623  [ 2944/ 3200]\n",
      "loss: 1.318457  [ 2960/ 3200]\n",
      "loss: 1.316012  [ 2976/ 3200]\n",
      "loss: 1.303026  [ 2992/ 3200]\n",
      "loss: 1.319576  [ 3008/ 3200]\n",
      "loss: 1.318135  [ 3024/ 3200]\n",
      "loss: 1.309611  [ 3040/ 3200]\n",
      "loss: 1.337185  [ 3056/ 3200]\n",
      "loss: 1.342196  [ 3072/ 3200]\n",
      "loss: 1.368438  [ 3088/ 3200]\n",
      "loss: 1.315550  [ 3104/ 3200]\n",
      "loss: 1.277646  [ 3120/ 3200]\n",
      "loss: 1.330290  [ 3136/ 3200]\n",
      "loss: 1.313888  [ 3152/ 3200]\n",
      "loss: 1.341691  [ 3168/ 3200]\n",
      "loss: 1.333714  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 7\n",
      "-----------------------------\n",
      "loss: 1.326988  [    0/ 3200]\n",
      "loss: 1.330851  [   16/ 3200]\n",
      "loss: 1.316434  [   32/ 3200]\n",
      "loss: 1.299192  [   48/ 3200]\n",
      "loss: 1.336401  [   64/ 3200]\n",
      "loss: 1.325992  [   80/ 3200]\n",
      "loss: 1.326742  [   96/ 3200]\n",
      "loss: 1.313077  [  112/ 3200]\n",
      "loss: 1.356978  [  128/ 3200]\n",
      "loss: 1.337023  [  144/ 3200]\n",
      "loss: 1.348682  [  160/ 3200]\n",
      "loss: 1.297634  [  176/ 3200]\n",
      "loss: 1.330202  [  192/ 3200]\n",
      "loss: 1.343239  [  208/ 3200]\n",
      "loss: 1.322369  [  224/ 3200]\n",
      "loss: 1.342633  [  240/ 3200]\n",
      "loss: 1.314177  [  256/ 3200]\n",
      "loss: 1.315330  [  272/ 3200]\n",
      "loss: 1.300157  [  288/ 3200]\n",
      "loss: 1.352441  [  304/ 3200]\n",
      "loss: 1.336754  [  320/ 3200]\n",
      "loss: 1.316781  [  336/ 3200]\n",
      "loss: 1.314035  [  352/ 3200]\n",
      "loss: 1.272771  [  368/ 3200]\n",
      "loss: 1.335981  [  384/ 3200]\n",
      "loss: 1.313940  [  400/ 3200]\n",
      "loss: 1.279842  [  416/ 3200]\n",
      "loss: 1.260715  [  432/ 3200]\n",
      "loss: 1.358117  [  448/ 3200]\n",
      "loss: 1.314704  [  464/ 3200]\n",
      "loss: 1.311819  [  480/ 3200]\n",
      "loss: 1.322212  [  496/ 3200]\n",
      "loss: 1.303046  [  512/ 3200]\n",
      "loss: 1.313326  [  528/ 3200]\n",
      "loss: 1.321781  [  544/ 3200]\n",
      "loss: 1.307077  [  560/ 3200]\n",
      "loss: 1.305575  [  576/ 3200]\n",
      "loss: 1.346881  [  592/ 3200]\n",
      "loss: 1.336964  [  608/ 3200]\n",
      "loss: 1.283177  [  624/ 3200]\n",
      "loss: 1.299080  [  640/ 3200]\n",
      "loss: 1.281858  [  656/ 3200]\n",
      "loss: 1.299634  [  672/ 3200]\n",
      "loss: 1.306346  [  688/ 3200]\n",
      "loss: 1.373345  [  704/ 3200]\n",
      "loss: 1.315934  [  720/ 3200]\n",
      "loss: 1.323890  [  736/ 3200]\n",
      "loss: 1.310014  [  752/ 3200]\n",
      "loss: 1.311716  [  768/ 3200]\n",
      "loss: 1.316742  [  784/ 3200]\n",
      "loss: 1.334212  [  800/ 3200]\n",
      "loss: 1.298273  [  816/ 3200]\n",
      "loss: 1.327561  [  832/ 3200]\n",
      "loss: 1.288204  [  848/ 3200]\n",
      "loss: 1.362415  [  864/ 3200]\n",
      "loss: 1.305547  [  880/ 3200]\n",
      "loss: 1.281787  [  896/ 3200]\n",
      "loss: 1.319744  [  912/ 3200]\n",
      "loss: 1.277913  [  928/ 3200]\n",
      "loss: 1.285850  [  944/ 3200]\n",
      "loss: 1.345847  [  960/ 3200]\n",
      "loss: 1.312146  [  976/ 3200]\n",
      "loss: 1.311048  [  992/ 3200]\n",
      "loss: 1.365443  [ 1008/ 3200]\n",
      "loss: 1.306070  [ 1024/ 3200]\n",
      "loss: 1.276242  [ 1040/ 3200]\n",
      "loss: 1.356299  [ 1056/ 3200]\n",
      "loss: 1.272577  [ 1072/ 3200]\n",
      "loss: 1.306325  [ 1088/ 3200]\n",
      "loss: 1.324367  [ 1104/ 3200]\n",
      "loss: 1.302219  [ 1120/ 3200]\n",
      "loss: 1.374869  [ 1136/ 3200]\n",
      "loss: 1.356492  [ 1152/ 3200]\n",
      "loss: 1.342253  [ 1168/ 3200]\n",
      "loss: 1.384459  [ 1184/ 3200]\n",
      "loss: 1.322571  [ 1200/ 3200]\n",
      "loss: 1.284130  [ 1216/ 3200]\n",
      "loss: 1.286056  [ 1232/ 3200]\n",
      "loss: 1.285142  [ 1248/ 3200]\n",
      "loss: 1.265967  [ 1264/ 3200]\n",
      "loss: 1.405461  [ 1280/ 3200]\n",
      "loss: 1.341509  [ 1296/ 3200]\n",
      "loss: 1.314151  [ 1312/ 3200]\n",
      "loss: 1.316224  [ 1328/ 3200]\n",
      "loss: 1.285617  [ 1344/ 3200]\n",
      "loss: 1.246018  [ 1360/ 3200]\n",
      "loss: 1.367423  [ 1376/ 3200]\n",
      "loss: 1.335873  [ 1392/ 3200]\n",
      "loss: 1.336610  [ 1408/ 3200]\n",
      "loss: 1.281140  [ 1424/ 3200]\n",
      "loss: 1.336868  [ 1440/ 3200]\n",
      "loss: 1.293334  [ 1456/ 3200]\n",
      "loss: 1.338958  [ 1472/ 3200]\n",
      "loss: 1.307555  [ 1488/ 3200]\n",
      "loss: 1.278663  [ 1504/ 3200]\n",
      "loss: 1.307222  [ 1520/ 3200]\n",
      "loss: 1.270631  [ 1536/ 3200]\n",
      "loss: 1.332931  [ 1552/ 3200]\n",
      "loss: 1.325615  [ 1568/ 3200]\n",
      "loss: 1.306405  [ 1584/ 3200]\n",
      "loss: 1.300905  [ 1600/ 3200]\n",
      "loss: 1.242138  [ 1616/ 3200]\n",
      "loss: 1.273385  [ 1632/ 3200]\n",
      "loss: 1.259974  [ 1648/ 3200]\n",
      "loss: 1.469225  [ 1664/ 3200]\n",
      "loss: 1.347850  [ 1680/ 3200]\n",
      "loss: 1.329696  [ 1696/ 3200]\n",
      "loss: 1.311494  [ 1712/ 3200]\n",
      "loss: 1.269860  [ 1728/ 3200]\n",
      "loss: 1.358742  [ 1744/ 3200]\n",
      "loss: 1.342168  [ 1760/ 3200]\n",
      "loss: 1.333276  [ 1776/ 3200]\n",
      "loss: 1.274713  [ 1792/ 3200]\n",
      "loss: 1.325141  [ 1808/ 3200]\n",
      "loss: 1.298571  [ 1824/ 3200]\n",
      "loss: 1.311625  [ 1840/ 3200]\n",
      "loss: 1.330745  [ 1856/ 3200]\n",
      "loss: 1.333159  [ 1872/ 3200]\n",
      "loss: 1.330052  [ 1888/ 3200]\n",
      "loss: 1.282306  [ 1904/ 3200]\n",
      "loss: 1.314125  [ 1920/ 3200]\n",
      "loss: 1.360099  [ 1936/ 3200]\n",
      "loss: 1.283513  [ 1952/ 3200]\n",
      "loss: 1.332186  [ 1968/ 3200]\n",
      "loss: 1.343655  [ 1984/ 3200]\n",
      "loss: 1.253447  [ 2000/ 3200]\n",
      "loss: 1.359885  [ 2016/ 3200]\n",
      "loss: 1.366357  [ 2032/ 3200]\n",
      "loss: 1.332346  [ 2048/ 3200]\n",
      "loss: 1.330385  [ 2064/ 3200]\n",
      "loss: 1.309415  [ 2080/ 3200]\n",
      "loss: 1.310437  [ 2096/ 3200]\n",
      "loss: 1.278370  [ 2112/ 3200]\n",
      "loss: 1.248773  [ 2128/ 3200]\n",
      "loss: 1.332193  [ 2144/ 3200]\n",
      "loss: 1.325859  [ 2160/ 3200]\n",
      "loss: 1.257229  [ 2176/ 3200]\n",
      "loss: 1.322409  [ 2192/ 3200]\n",
      "loss: 1.254531  [ 2208/ 3200]\n",
      "loss: 1.347305  [ 2224/ 3200]\n",
      "loss: 1.273928  [ 2240/ 3200]\n",
      "loss: 1.288950  [ 2256/ 3200]\n",
      "loss: 1.315122  [ 2272/ 3200]\n",
      "loss: 1.238656  [ 2288/ 3200]\n",
      "loss: 1.263938  [ 2304/ 3200]\n",
      "loss: 1.340393  [ 2320/ 3200]\n",
      "loss: 1.293684  [ 2336/ 3200]\n",
      "loss: 1.409026  [ 2352/ 3200]\n",
      "loss: 1.316865  [ 2368/ 3200]\n",
      "loss: 1.316022  [ 2384/ 3200]\n",
      "loss: 1.277719  [ 2400/ 3200]\n",
      "loss: 1.258646  [ 2416/ 3200]\n",
      "loss: 1.328208  [ 2432/ 3200]\n",
      "loss: 1.310911  [ 2448/ 3200]\n",
      "loss: 1.359235  [ 2464/ 3200]\n",
      "loss: 1.322666  [ 2480/ 3200]\n",
      "loss: 1.271954  [ 2496/ 3200]\n",
      "loss: 1.346615  [ 2512/ 3200]\n",
      "loss: 1.262985  [ 2528/ 3200]\n",
      "loss: 1.313733  [ 2544/ 3200]\n",
      "loss: 1.336814  [ 2560/ 3200]\n",
      "loss: 1.361183  [ 2576/ 3200]\n",
      "loss: 1.241268  [ 2592/ 3200]\n",
      "loss: 1.204046  [ 2608/ 3200]\n",
      "loss: 1.273258  [ 2624/ 3200]\n",
      "loss: 1.329653  [ 2640/ 3200]\n",
      "loss: 1.264621  [ 2656/ 3200]\n",
      "loss: 1.296953  [ 2672/ 3200]\n",
      "loss: 1.260436  [ 2688/ 3200]\n",
      "loss: 1.313889  [ 2704/ 3200]\n",
      "loss: 1.317547  [ 2720/ 3200]\n",
      "loss: 1.402532  [ 2736/ 3200]\n",
      "loss: 1.305665  [ 2752/ 3200]\n",
      "loss: 1.392682  [ 2768/ 3200]\n",
      "loss: 1.372020  [ 2784/ 3200]\n",
      "loss: 1.295688  [ 2800/ 3200]\n",
      "loss: 1.245842  [ 2816/ 3200]\n",
      "loss: 1.282548  [ 2832/ 3200]\n",
      "loss: 1.249875  [ 2848/ 3200]\n",
      "loss: 1.326003  [ 2864/ 3200]\n",
      "loss: 1.321854  [ 2880/ 3200]\n",
      "loss: 1.367492  [ 2896/ 3200]\n",
      "loss: 1.313169  [ 2912/ 3200]\n",
      "loss: 1.287158  [ 2928/ 3200]\n",
      "loss: 1.311978  [ 2944/ 3200]\n",
      "loss: 1.248268  [ 2960/ 3200]\n",
      "loss: 1.350621  [ 2976/ 3200]\n",
      "loss: 1.323730  [ 2992/ 3200]\n",
      "loss: 1.327419  [ 3008/ 3200]\n",
      "loss: 1.302145  [ 3024/ 3200]\n",
      "loss: 1.299610  [ 3040/ 3200]\n",
      "loss: 1.310514  [ 3056/ 3200]\n",
      "loss: 1.298690  [ 3072/ 3200]\n",
      "loss: 1.301246  [ 3088/ 3200]\n",
      "loss: 1.296301  [ 3104/ 3200]\n",
      "loss: 1.282637  [ 3120/ 3200]\n",
      "loss: 1.303275  [ 3136/ 3200]\n",
      "loss: 1.323668  [ 3152/ 3200]\n",
      "loss: 1.308438  [ 3168/ 3200]\n",
      "loss: 1.254529  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 1.339613  [    0/ 3200]\n",
      "loss: 1.291149  [   16/ 3200]\n",
      "loss: 1.380676  [   32/ 3200]\n",
      "loss: 1.294386  [   48/ 3200]\n",
      "loss: 1.347219  [   64/ 3200]\n",
      "loss: 1.330439  [   80/ 3200]\n",
      "loss: 1.304007  [   96/ 3200]\n",
      "loss: 1.303598  [  112/ 3200]\n",
      "loss: 1.304477  [  128/ 3200]\n",
      "loss: 1.294912  [  144/ 3200]\n",
      "loss: 1.261150  [  160/ 3200]\n",
      "loss: 1.248016  [  176/ 3200]\n",
      "loss: 1.359399  [  192/ 3200]\n",
      "loss: 1.296839  [  208/ 3200]\n",
      "loss: 1.347168  [  224/ 3200]\n",
      "loss: 1.278304  [  240/ 3200]\n",
      "loss: 1.294724  [  256/ 3200]\n",
      "loss: 1.376808  [  272/ 3200]\n",
      "loss: 1.335930  [  288/ 3200]\n",
      "loss: 1.298911  [  304/ 3200]\n",
      "loss: 1.334908  [  320/ 3200]\n",
      "loss: 1.286803  [  336/ 3200]\n",
      "loss: 1.277510  [  352/ 3200]\n",
      "loss: 1.315411  [  368/ 3200]\n",
      "loss: 1.285085  [  384/ 3200]\n",
      "loss: 1.361489  [  400/ 3200]\n",
      "loss: 1.348140  [  416/ 3200]\n",
      "loss: 1.292332  [  432/ 3200]\n",
      "loss: 1.320817  [  448/ 3200]\n",
      "loss: 1.316658  [  464/ 3200]\n",
      "loss: 1.328227  [  480/ 3200]\n",
      "loss: 1.284036  [  496/ 3200]\n",
      "loss: 1.351724  [  512/ 3200]\n",
      "loss: 1.300626  [  528/ 3200]\n",
      "loss: 1.314261  [  544/ 3200]\n",
      "loss: 1.333722  [  560/ 3200]\n",
      "loss: 1.263741  [  576/ 3200]\n",
      "loss: 1.299107  [  592/ 3200]\n",
      "loss: 1.267058  [  608/ 3200]\n",
      "loss: 1.281836  [  624/ 3200]\n",
      "loss: 1.248597  [  640/ 3200]\n",
      "loss: 1.331478  [  656/ 3200]\n",
      "loss: 1.317236  [  672/ 3200]\n",
      "loss: 1.352683  [  688/ 3200]\n",
      "loss: 1.327925  [  704/ 3200]\n",
      "loss: 1.306686  [  720/ 3200]\n",
      "loss: 1.298904  [  736/ 3200]\n",
      "loss: 1.318457  [  752/ 3200]\n",
      "loss: 1.333330  [  768/ 3200]\n",
      "loss: 1.311834  [  784/ 3200]\n",
      "loss: 1.326463  [  800/ 3200]\n",
      "loss: 1.267533  [  816/ 3200]\n",
      "loss: 1.371632  [  832/ 3200]\n",
      "loss: 1.267100  [  848/ 3200]\n",
      "loss: 1.302356  [  864/ 3200]\n",
      "loss: 1.266674  [  880/ 3200]\n",
      "loss: 1.350820  [  896/ 3200]\n",
      "loss: 1.320765  [  912/ 3200]\n",
      "loss: 1.329206  [  928/ 3200]\n",
      "loss: 1.269681  [  944/ 3200]\n",
      "loss: 1.250189  [  960/ 3200]\n",
      "loss: 1.340286  [  976/ 3200]\n",
      "loss: 1.249404  [  992/ 3200]\n",
      "loss: 1.284076  [ 1008/ 3200]\n",
      "loss: 1.306337  [ 1024/ 3200]\n",
      "loss: 1.292444  [ 1040/ 3200]\n",
      "loss: 1.283822  [ 1056/ 3200]\n",
      "loss: 1.278371  [ 1072/ 3200]\n",
      "loss: 1.354773  [ 1088/ 3200]\n",
      "loss: 1.249629  [ 1104/ 3200]\n",
      "loss: 1.371031  [ 1120/ 3200]\n",
      "loss: 1.319692  [ 1136/ 3200]\n",
      "loss: 1.269913  [ 1152/ 3200]\n",
      "loss: 1.314302  [ 1168/ 3200]\n",
      "loss: 1.281703  [ 1184/ 3200]\n",
      "loss: 1.296655  [ 1200/ 3200]\n",
      "loss: 1.311003  [ 1216/ 3200]\n",
      "loss: 1.322647  [ 1232/ 3200]\n",
      "loss: 1.358414  [ 1248/ 3200]\n",
      "loss: 1.301116  [ 1264/ 3200]\n",
      "loss: 1.300355  [ 1280/ 3200]\n",
      "loss: 1.310946  [ 1296/ 3200]\n",
      "loss: 1.371964  [ 1312/ 3200]\n",
      "loss: 1.309732  [ 1328/ 3200]\n",
      "loss: 1.298615  [ 1344/ 3200]\n",
      "loss: 1.290395  [ 1360/ 3200]\n",
      "loss: 1.272773  [ 1376/ 3200]\n",
      "loss: 1.323358  [ 1392/ 3200]\n",
      "loss: 1.277206  [ 1408/ 3200]\n",
      "loss: 1.261819  [ 1424/ 3200]\n",
      "loss: 1.295099  [ 1440/ 3200]\n",
      "loss: 1.285621  [ 1456/ 3200]\n",
      "loss: 1.341166  [ 1472/ 3200]\n",
      "loss: 1.301176  [ 1488/ 3200]\n",
      "loss: 1.299512  [ 1504/ 3200]\n",
      "loss: 1.300974  [ 1520/ 3200]\n",
      "loss: 1.256852  [ 1536/ 3200]\n",
      "loss: 1.296514  [ 1552/ 3200]\n",
      "loss: 1.213879  [ 1568/ 3200]\n",
      "loss: 1.311069  [ 1584/ 3200]\n",
      "loss: 1.356474  [ 1600/ 3200]\n",
      "loss: 1.317531  [ 1616/ 3200]\n",
      "loss: 1.258555  [ 1632/ 3200]\n",
      "loss: 1.294312  [ 1648/ 3200]\n",
      "loss: 1.288619  [ 1664/ 3200]\n",
      "loss: 1.309781  [ 1680/ 3200]\n",
      "loss: 1.250780  [ 1696/ 3200]\n",
      "loss: 1.274643  [ 1712/ 3200]\n",
      "loss: 1.289773  [ 1728/ 3200]\n",
      "loss: 1.238655  [ 1744/ 3200]\n",
      "loss: 1.273646  [ 1760/ 3200]\n",
      "loss: 1.367806  [ 1776/ 3200]\n",
      "loss: 1.304867  [ 1792/ 3200]\n",
      "loss: 1.371426  [ 1808/ 3200]\n",
      "loss: 1.254058  [ 1824/ 3200]\n",
      "loss: 1.276920  [ 1840/ 3200]\n",
      "loss: 1.192770  [ 1856/ 3200]\n",
      "loss: 1.315963  [ 1872/ 3200]\n",
      "loss: 1.281054  [ 1888/ 3200]\n",
      "loss: 1.304144  [ 1904/ 3200]\n",
      "loss: 1.369237  [ 1920/ 3200]\n",
      "loss: 1.265747  [ 1936/ 3200]\n",
      "loss: 1.296661  [ 1952/ 3200]\n",
      "loss: 1.275457  [ 1968/ 3200]\n",
      "loss: 1.308449  [ 1984/ 3200]\n",
      "loss: 1.346966  [ 2000/ 3200]\n",
      "loss: 1.301168  [ 2016/ 3200]\n",
      "loss: 1.306079  [ 2032/ 3200]\n",
      "loss: 1.247577  [ 2048/ 3200]\n",
      "loss: 1.321558  [ 2064/ 3200]\n",
      "loss: 1.298867  [ 2080/ 3200]\n",
      "loss: 1.255265  [ 2096/ 3200]\n",
      "loss: 1.341294  [ 2112/ 3200]\n",
      "loss: 1.278199  [ 2128/ 3200]\n",
      "loss: 1.284584  [ 2144/ 3200]\n",
      "loss: 1.281627  [ 2160/ 3200]\n",
      "loss: 1.380264  [ 2176/ 3200]\n",
      "loss: 1.290949  [ 2192/ 3200]\n",
      "loss: 1.299291  [ 2208/ 3200]\n",
      "loss: 1.314312  [ 2224/ 3200]\n",
      "loss: 1.297057  [ 2240/ 3200]\n",
      "loss: 1.251985  [ 2256/ 3200]\n",
      "loss: 1.259560  [ 2272/ 3200]\n",
      "loss: 1.230509  [ 2288/ 3200]\n",
      "loss: 1.314767  [ 2304/ 3200]\n",
      "loss: 1.191340  [ 2320/ 3200]\n",
      "loss: 1.384131  [ 2336/ 3200]\n",
      "loss: 1.313532  [ 2352/ 3200]\n",
      "loss: 1.318488  [ 2368/ 3200]\n",
      "loss: 1.289453  [ 2384/ 3200]\n",
      "loss: 1.257637  [ 2400/ 3200]\n",
      "loss: 1.281757  [ 2416/ 3200]\n",
      "loss: 1.235283  [ 2432/ 3200]\n",
      "loss: 1.240521  [ 2448/ 3200]\n",
      "loss: 1.313552  [ 2464/ 3200]\n",
      "loss: 1.290433  [ 2480/ 3200]\n",
      "loss: 1.272700  [ 2496/ 3200]\n",
      "loss: 1.270097  [ 2512/ 3200]\n",
      "loss: 1.278963  [ 2528/ 3200]\n",
      "loss: 1.264329  [ 2544/ 3200]\n",
      "loss: 1.306237  [ 2560/ 3200]\n",
      "loss: 1.265422  [ 2576/ 3200]\n",
      "loss: 1.321714  [ 2592/ 3200]\n",
      "loss: 1.311890  [ 2608/ 3200]\n",
      "loss: 1.233255  [ 2624/ 3200]\n",
      "loss: 1.255781  [ 2640/ 3200]\n",
      "loss: 1.325834  [ 2656/ 3200]\n",
      "loss: 1.264202  [ 2672/ 3200]\n",
      "loss: 1.238184  [ 2688/ 3200]\n",
      "loss: 1.271366  [ 2704/ 3200]\n",
      "loss: 1.343497  [ 2720/ 3200]\n",
      "loss: 1.316497  [ 2736/ 3200]\n",
      "loss: 1.289288  [ 2752/ 3200]\n",
      "loss: 1.295462  [ 2768/ 3200]\n",
      "loss: 1.372826  [ 2784/ 3200]\n",
      "loss: 1.245422  [ 2800/ 3200]\n",
      "loss: 1.286461  [ 2816/ 3200]\n",
      "loss: 1.339624  [ 2832/ 3200]\n",
      "loss: 1.326216  [ 2848/ 3200]\n",
      "loss: 1.344324  [ 2864/ 3200]\n",
      "loss: 1.337856  [ 2880/ 3200]\n",
      "loss: 1.314525  [ 2896/ 3200]\n",
      "loss: 1.234989  [ 2912/ 3200]\n",
      "loss: 1.295816  [ 2928/ 3200]\n",
      "loss: 1.267729  [ 2944/ 3200]\n",
      "loss: 1.258780  [ 2960/ 3200]\n",
      "loss: 1.304970  [ 2976/ 3200]\n",
      "loss: 1.262390  [ 2992/ 3200]\n",
      "loss: 1.282678  [ 3008/ 3200]\n",
      "loss: 1.304884  [ 3024/ 3200]\n",
      "loss: 1.299285  [ 3040/ 3200]\n",
      "loss: 1.289953  [ 3056/ 3200]\n",
      "loss: 1.335076  [ 3072/ 3200]\n",
      "loss: 1.336356  [ 3088/ 3200]\n",
      "loss: 1.308369  [ 3104/ 3200]\n",
      "loss: 1.304464  [ 3120/ 3200]\n",
      "loss: 1.292619  [ 3136/ 3200]\n",
      "loss: 1.300329  [ 3152/ 3200]\n",
      "loss: 1.263857  [ 3168/ 3200]\n",
      "loss: 1.230687  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 1.310238  [    0/ 3200]\n",
      "loss: 1.250379  [   16/ 3200]\n",
      "loss: 1.279530  [   32/ 3200]\n",
      "loss: 1.279877  [   48/ 3200]\n",
      "loss: 1.287308  [   64/ 3200]\n",
      "loss: 1.290394  [   80/ 3200]\n",
      "loss: 1.248816  [   96/ 3200]\n",
      "loss: 1.219190  [  112/ 3200]\n",
      "loss: 1.257474  [  128/ 3200]\n",
      "loss: 1.310157  [  144/ 3200]\n",
      "loss: 1.268470  [  160/ 3200]\n",
      "loss: 1.278430  [  176/ 3200]\n",
      "loss: 1.273597  [  192/ 3200]\n",
      "loss: 1.289531  [  208/ 3200]\n",
      "loss: 1.339263  [  224/ 3200]\n",
      "loss: 1.315387  [  240/ 3200]\n",
      "loss: 1.296284  [  256/ 3200]\n",
      "loss: 1.275593  [  272/ 3200]\n",
      "loss: 1.295872  [  288/ 3200]\n",
      "loss: 1.244664  [  304/ 3200]\n",
      "loss: 1.222173  [  320/ 3200]\n",
      "loss: 1.307678  [  336/ 3200]\n",
      "loss: 1.289228  [  352/ 3200]\n",
      "loss: 1.302790  [  368/ 3200]\n",
      "loss: 1.277153  [  384/ 3200]\n",
      "loss: 1.266233  [  400/ 3200]\n",
      "loss: 1.281761  [  416/ 3200]\n",
      "loss: 1.294756  [  432/ 3200]\n",
      "loss: 1.261091  [  448/ 3200]\n",
      "loss: 1.325360  [  464/ 3200]\n",
      "loss: 1.299680  [  480/ 3200]\n",
      "loss: 1.179140  [  496/ 3200]\n",
      "loss: 1.275394  [  512/ 3200]\n",
      "loss: 1.342718  [  528/ 3200]\n",
      "loss: 1.251541  [  544/ 3200]\n",
      "loss: 1.228096  [  560/ 3200]\n",
      "loss: 1.284767  [  576/ 3200]\n",
      "loss: 1.284838  [  592/ 3200]\n",
      "loss: 1.282636  [  608/ 3200]\n",
      "loss: 1.325437  [  624/ 3200]\n",
      "loss: 1.282599  [  640/ 3200]\n",
      "loss: 1.295789  [  656/ 3200]\n",
      "loss: 1.197480  [  672/ 3200]\n",
      "loss: 1.324309  [  688/ 3200]\n",
      "loss: 1.319388  [  704/ 3200]\n",
      "loss: 1.258234  [  720/ 3200]\n",
      "loss: 1.245206  [  736/ 3200]\n",
      "loss: 1.361030  [  752/ 3200]\n",
      "loss: 1.268388  [  768/ 3200]\n",
      "loss: 1.335717  [  784/ 3200]\n",
      "loss: 1.198432  [  800/ 3200]\n",
      "loss: 1.275228  [  816/ 3200]\n",
      "loss: 1.223948  [  832/ 3200]\n",
      "loss: 1.378252  [  848/ 3200]\n",
      "loss: 1.312671  [  864/ 3200]\n",
      "loss: 1.324185  [  880/ 3200]\n",
      "loss: 1.296036  [  896/ 3200]\n",
      "loss: 1.269701  [  912/ 3200]\n",
      "loss: 1.265355  [  928/ 3200]\n",
      "loss: 1.302210  [  944/ 3200]\n",
      "loss: 1.237181  [  960/ 3200]\n",
      "loss: 1.212689  [  976/ 3200]\n",
      "loss: 1.332469  [  992/ 3200]\n",
      "loss: 1.319029  [ 1008/ 3200]\n",
      "loss: 1.300302  [ 1024/ 3200]\n",
      "loss: 1.341806  [ 1040/ 3200]\n",
      "loss: 1.198190  [ 1056/ 3200]\n",
      "loss: 1.244642  [ 1072/ 3200]\n",
      "loss: 1.307065  [ 1088/ 3200]\n",
      "loss: 1.315065  [ 1104/ 3200]\n",
      "loss: 1.331631  [ 1120/ 3200]\n",
      "loss: 1.244212  [ 1136/ 3200]\n",
      "loss: 1.374524  [ 1152/ 3200]\n",
      "loss: 1.266695  [ 1168/ 3200]\n",
      "loss: 1.272486  [ 1184/ 3200]\n",
      "loss: 1.315040  [ 1200/ 3200]\n",
      "loss: 1.291711  [ 1216/ 3200]\n",
      "loss: 1.331411  [ 1232/ 3200]\n",
      "loss: 1.301908  [ 1248/ 3200]\n",
      "loss: 1.349854  [ 1264/ 3200]\n",
      "loss: 1.349737  [ 1280/ 3200]\n",
      "loss: 1.275728  [ 1296/ 3200]\n",
      "loss: 1.247450  [ 1312/ 3200]\n",
      "loss: 1.269547  [ 1328/ 3200]\n",
      "loss: 1.238090  [ 1344/ 3200]\n",
      "loss: 1.257824  [ 1360/ 3200]\n",
      "loss: 1.287168  [ 1376/ 3200]\n",
      "loss: 1.308183  [ 1392/ 3200]\n",
      "loss: 1.282694  [ 1408/ 3200]\n",
      "loss: 1.253032  [ 1424/ 3200]\n",
      "loss: 1.298527  [ 1440/ 3200]\n",
      "loss: 1.310028  [ 1456/ 3200]\n",
      "loss: 1.290814  [ 1472/ 3200]\n",
      "loss: 1.221859  [ 1488/ 3200]\n",
      "loss: 1.324908  [ 1504/ 3200]\n",
      "loss: 1.276145  [ 1520/ 3200]\n",
      "loss: 1.308743  [ 1536/ 3200]\n",
      "loss: 1.334810  [ 1552/ 3200]\n",
      "loss: 1.299879  [ 1568/ 3200]\n",
      "loss: 1.230606  [ 1584/ 3200]\n",
      "loss: 1.327241  [ 1600/ 3200]\n",
      "loss: 1.260594  [ 1616/ 3200]\n",
      "loss: 1.262921  [ 1632/ 3200]\n",
      "loss: 1.284015  [ 1648/ 3200]\n",
      "loss: 1.247667  [ 1664/ 3200]\n",
      "loss: 1.347636  [ 1680/ 3200]\n",
      "loss: 1.298360  [ 1696/ 3200]\n",
      "loss: 1.322840  [ 1712/ 3200]\n",
      "loss: 1.305618  [ 1728/ 3200]\n",
      "loss: 1.294819  [ 1744/ 3200]\n",
      "loss: 1.325795  [ 1760/ 3200]\n",
      "loss: 1.268815  [ 1776/ 3200]\n",
      "loss: 1.272722  [ 1792/ 3200]\n",
      "loss: 1.262314  [ 1808/ 3200]\n",
      "loss: 1.307806  [ 1824/ 3200]\n",
      "loss: 1.256299  [ 1840/ 3200]\n",
      "loss: 1.206838  [ 1856/ 3200]\n",
      "loss: 1.256161  [ 1872/ 3200]\n",
      "loss: 1.266147  [ 1888/ 3200]\n",
      "loss: 1.288359  [ 1904/ 3200]\n",
      "loss: 1.258201  [ 1920/ 3200]\n",
      "loss: 1.281752  [ 1936/ 3200]\n",
      "loss: 1.241407  [ 1952/ 3200]\n",
      "loss: 1.274654  [ 1968/ 3200]\n",
      "loss: 1.319680  [ 1984/ 3200]\n",
      "loss: 1.304606  [ 2000/ 3200]\n",
      "loss: 1.295761  [ 2016/ 3200]\n",
      "loss: 1.277864  [ 2032/ 3200]\n",
      "loss: 1.275628  [ 2048/ 3200]\n",
      "loss: 1.264685  [ 2064/ 3200]\n",
      "loss: 1.293515  [ 2080/ 3200]\n",
      "loss: 1.305867  [ 2096/ 3200]\n",
      "loss: 1.268751  [ 2112/ 3200]\n",
      "loss: 1.227439  [ 2128/ 3200]\n",
      "loss: 1.291250  [ 2144/ 3200]\n",
      "loss: 1.382964  [ 2160/ 3200]\n",
      "loss: 1.256342  [ 2176/ 3200]\n",
      "loss: 1.304145  [ 2192/ 3200]\n",
      "loss: 1.214092  [ 2208/ 3200]\n",
      "loss: 1.340101  [ 2224/ 3200]\n",
      "loss: 1.294865  [ 2240/ 3200]\n",
      "loss: 1.216467  [ 2256/ 3200]\n",
      "loss: 1.251424  [ 2272/ 3200]\n",
      "loss: 1.241090  [ 2288/ 3200]\n",
      "loss: 1.305978  [ 2304/ 3200]\n",
      "loss: 1.331347  [ 2320/ 3200]\n",
      "loss: 1.324678  [ 2336/ 3200]\n",
      "loss: 1.319859  [ 2352/ 3200]\n",
      "loss: 1.262877  [ 2368/ 3200]\n",
      "loss: 1.321602  [ 2384/ 3200]\n",
      "loss: 1.273102  [ 2400/ 3200]\n",
      "loss: 1.270176  [ 2416/ 3200]\n",
      "loss: 1.264634  [ 2432/ 3200]\n",
      "loss: 1.297013  [ 2448/ 3200]\n",
      "loss: 1.316648  [ 2464/ 3200]\n",
      "loss: 1.295591  [ 2480/ 3200]\n",
      "loss: 1.260540  [ 2496/ 3200]\n",
      "loss: 1.343332  [ 2512/ 3200]\n",
      "loss: 1.253469  [ 2528/ 3200]\n",
      "loss: 1.275568  [ 2544/ 3200]\n",
      "loss: 1.283283  [ 2560/ 3200]\n",
      "loss: 1.189447  [ 2576/ 3200]\n",
      "loss: 1.296299  [ 2592/ 3200]\n",
      "loss: 1.291786  [ 2608/ 3200]\n",
      "loss: 1.247863  [ 2624/ 3200]\n",
      "loss: 1.269883  [ 2640/ 3200]\n",
      "loss: 1.282364  [ 2656/ 3200]\n",
      "loss: 1.257894  [ 2672/ 3200]\n",
      "loss: 1.234280  [ 2688/ 3200]\n",
      "loss: 1.310630  [ 2704/ 3200]\n",
      "loss: 1.294459  [ 2720/ 3200]\n",
      "loss: 1.312866  [ 2736/ 3200]\n",
      "loss: 1.256689  [ 2752/ 3200]\n",
      "loss: 1.290124  [ 2768/ 3200]\n",
      "loss: 1.227542  [ 2784/ 3200]\n",
      "loss: 1.306552  [ 2800/ 3200]\n",
      "loss: 1.222798  [ 2816/ 3200]\n",
      "loss: 1.301956  [ 2832/ 3200]\n",
      "loss: 1.221077  [ 2848/ 3200]\n",
      "loss: 1.323400  [ 2864/ 3200]\n",
      "loss: 1.231780  [ 2880/ 3200]\n",
      "loss: 1.253651  [ 2896/ 3200]\n",
      "loss: 1.295183  [ 2912/ 3200]\n",
      "loss: 1.284138  [ 2928/ 3200]\n",
      "loss: 1.317895  [ 2944/ 3200]\n",
      "loss: 1.299617  [ 2960/ 3200]\n",
      "loss: 1.306039  [ 2976/ 3200]\n",
      "loss: 1.323965  [ 2992/ 3200]\n",
      "loss: 1.260213  [ 3008/ 3200]\n",
      "loss: 1.295501  [ 3024/ 3200]\n",
      "loss: 1.286328  [ 3040/ 3200]\n",
      "loss: 1.270804  [ 3056/ 3200]\n",
      "loss: 1.214754  [ 3072/ 3200]\n",
      "loss: 1.322498  [ 3088/ 3200]\n",
      "loss: 1.224229  [ 3104/ 3200]\n",
      "loss: 1.304197  [ 3120/ 3200]\n",
      "loss: 1.296345  [ 3136/ 3200]\n",
      "loss: 1.306973  [ 3152/ 3200]\n",
      "loss: 1.282542  [ 3168/ 3200]\n",
      "loss: 1.253219  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 1.242914  [    0/ 3200]\n",
      "loss: 1.331136  [   16/ 3200]\n",
      "loss: 1.309888  [   32/ 3200]\n",
      "loss: 1.245402  [   48/ 3200]\n",
      "loss: 1.265705  [   64/ 3200]\n",
      "loss: 1.315784  [   80/ 3200]\n",
      "loss: 1.304622  [   96/ 3200]\n",
      "loss: 1.243113  [  112/ 3200]\n",
      "loss: 1.261524  [  128/ 3200]\n",
      "loss: 1.288651  [  144/ 3200]\n",
      "loss: 1.323286  [  160/ 3200]\n",
      "loss: 1.302559  [  176/ 3200]\n",
      "loss: 1.316891  [  192/ 3200]\n",
      "loss: 1.212887  [  208/ 3200]\n",
      "loss: 1.300797  [  224/ 3200]\n",
      "loss: 1.276880  [  240/ 3200]\n",
      "loss: 1.279558  [  256/ 3200]\n",
      "loss: 1.296232  [  272/ 3200]\n",
      "loss: 1.359297  [  288/ 3200]\n",
      "loss: 1.281353  [  304/ 3200]\n",
      "loss: 1.212299  [  320/ 3200]\n",
      "loss: 1.287982  [  336/ 3200]\n",
      "loss: 1.226710  [  352/ 3200]\n",
      "loss: 1.309526  [  368/ 3200]\n",
      "loss: 1.219122  [  384/ 3200]\n",
      "loss: 1.189229  [  400/ 3200]\n",
      "loss: 1.271902  [  416/ 3200]\n",
      "loss: 1.366655  [  432/ 3200]\n",
      "loss: 1.272184  [  448/ 3200]\n",
      "loss: 1.301770  [  464/ 3200]\n",
      "loss: 1.261308  [  480/ 3200]\n",
      "loss: 1.288901  [  496/ 3200]\n",
      "loss: 1.343823  [  512/ 3200]\n",
      "loss: 1.225929  [  528/ 3200]\n",
      "loss: 1.232787  [  544/ 3200]\n",
      "loss: 1.277795  [  560/ 3200]\n",
      "loss: 1.255778  [  576/ 3200]\n",
      "loss: 1.268217  [  592/ 3200]\n",
      "loss: 1.289320  [  608/ 3200]\n",
      "loss: 1.256557  [  624/ 3200]\n",
      "loss: 1.256524  [  640/ 3200]\n",
      "loss: 1.308636  [  656/ 3200]\n",
      "loss: 1.308817  [  672/ 3200]\n",
      "loss: 1.247473  [  688/ 3200]\n",
      "loss: 1.226135  [  704/ 3200]\n",
      "loss: 1.284697  [  720/ 3200]\n",
      "loss: 1.318922  [  736/ 3200]\n",
      "loss: 1.172792  [  752/ 3200]\n",
      "loss: 1.288620  [  768/ 3200]\n",
      "loss: 1.238206  [  784/ 3200]\n",
      "loss: 1.316394  [  800/ 3200]\n",
      "loss: 1.204520  [  816/ 3200]\n",
      "loss: 1.246337  [  832/ 3200]\n",
      "loss: 1.239299  [  848/ 3200]\n",
      "loss: 1.310490  [  864/ 3200]\n",
      "loss: 1.256182  [  880/ 3200]\n",
      "loss: 1.226210  [  896/ 3200]\n",
      "loss: 1.316615  [  912/ 3200]\n",
      "loss: 1.325786  [  928/ 3200]\n",
      "loss: 1.229688  [  944/ 3200]\n",
      "loss: 1.276653  [  960/ 3200]\n",
      "loss: 1.337936  [  976/ 3200]\n",
      "loss: 1.287327  [  992/ 3200]\n",
      "loss: 1.297891  [ 1008/ 3200]\n",
      "loss: 1.284280  [ 1024/ 3200]\n",
      "loss: 1.237708  [ 1040/ 3200]\n",
      "loss: 1.285884  [ 1056/ 3200]\n",
      "loss: 1.243810  [ 1072/ 3200]\n",
      "loss: 1.247874  [ 1088/ 3200]\n",
      "loss: 1.287029  [ 1104/ 3200]\n",
      "loss: 1.289373  [ 1120/ 3200]\n",
      "loss: 1.321295  [ 1136/ 3200]\n",
      "loss: 1.287435  [ 1152/ 3200]\n",
      "loss: 1.207579  [ 1168/ 3200]\n",
      "loss: 1.267940  [ 1184/ 3200]\n",
      "loss: 1.315706  [ 1200/ 3200]\n",
      "loss: 1.263833  [ 1216/ 3200]\n",
      "loss: 1.311588  [ 1232/ 3200]\n",
      "loss: 1.285136  [ 1248/ 3200]\n",
      "loss: 1.232719  [ 1264/ 3200]\n",
      "loss: 1.259715  [ 1280/ 3200]\n",
      "loss: 1.261614  [ 1296/ 3200]\n",
      "loss: 1.219908  [ 1312/ 3200]\n",
      "loss: 1.282756  [ 1328/ 3200]\n",
      "loss: 1.338443  [ 1344/ 3200]\n",
      "loss: 1.187436  [ 1360/ 3200]\n",
      "loss: 1.271359  [ 1376/ 3200]\n",
      "loss: 1.191912  [ 1392/ 3200]\n",
      "loss: 1.162414  [ 1408/ 3200]\n",
      "loss: 1.278420  [ 1424/ 3200]\n",
      "loss: 1.282748  [ 1440/ 3200]\n",
      "loss: 1.286105  [ 1456/ 3200]\n",
      "loss: 1.341342  [ 1472/ 3200]\n",
      "loss: 1.165538  [ 1488/ 3200]\n",
      "loss: 1.268954  [ 1504/ 3200]\n",
      "loss: 1.190096  [ 1520/ 3200]\n",
      "loss: 1.275769  [ 1536/ 3200]\n",
      "loss: 1.214775  [ 1552/ 3200]\n",
      "loss: 1.293646  [ 1568/ 3200]\n",
      "loss: 1.305810  [ 1584/ 3200]\n",
      "loss: 1.224537  [ 1600/ 3200]\n",
      "loss: 1.295053  [ 1616/ 3200]\n",
      "loss: 1.203515  [ 1632/ 3200]\n",
      "loss: 1.302386  [ 1648/ 3200]\n",
      "loss: 1.259162  [ 1664/ 3200]\n",
      "loss: 1.224429  [ 1680/ 3200]\n",
      "loss: 1.255170  [ 1696/ 3200]\n",
      "loss: 1.264022  [ 1712/ 3200]\n",
      "loss: 1.173109  [ 1728/ 3200]\n",
      "loss: 1.174801  [ 1744/ 3200]\n",
      "loss: 1.258225  [ 1760/ 3200]\n",
      "loss: 1.245108  [ 1776/ 3200]\n",
      "loss: 1.220989  [ 1792/ 3200]\n",
      "loss: 1.241342  [ 1808/ 3200]\n",
      "loss: 1.349100  [ 1824/ 3200]\n",
      "loss: 1.232833  [ 1840/ 3200]\n",
      "loss: 1.228410  [ 1856/ 3200]\n",
      "loss: 1.301927  [ 1872/ 3200]\n",
      "loss: 1.298806  [ 1888/ 3200]\n",
      "loss: 1.261788  [ 1904/ 3200]\n",
      "loss: 1.188789  [ 1920/ 3200]\n",
      "loss: 1.240556  [ 1936/ 3200]\n",
      "loss: 1.171596  [ 1952/ 3200]\n",
      "loss: 1.375559  [ 1968/ 3200]\n",
      "loss: 1.199490  [ 1984/ 3200]\n",
      "loss: 1.244351  [ 2000/ 3200]\n",
      "loss: 1.268373  [ 2016/ 3200]\n",
      "loss: 1.232166  [ 2032/ 3200]\n",
      "loss: 1.340640  [ 2048/ 3200]\n",
      "loss: 1.336674  [ 2064/ 3200]\n",
      "loss: 1.301075  [ 2080/ 3200]\n",
      "loss: 1.288810  [ 2096/ 3200]\n",
      "loss: 1.273826  [ 2112/ 3200]\n",
      "loss: 1.229847  [ 2128/ 3200]\n",
      "loss: 1.285104  [ 2144/ 3200]\n",
      "loss: 1.236169  [ 2160/ 3200]\n",
      "loss: 1.207751  [ 2176/ 3200]\n",
      "loss: 1.258747  [ 2192/ 3200]\n",
      "loss: 1.185849  [ 2208/ 3200]\n",
      "loss: 1.153736  [ 2224/ 3200]\n",
      "loss: 1.284645  [ 2240/ 3200]\n",
      "loss: 1.247304  [ 2256/ 3200]\n",
      "loss: 1.268299  [ 2272/ 3200]\n",
      "loss: 1.200286  [ 2288/ 3200]\n",
      "loss: 1.215604  [ 2304/ 3200]\n",
      "loss: 1.207372  [ 2320/ 3200]\n",
      "loss: 1.307822  [ 2336/ 3200]\n",
      "loss: 1.340484  [ 2352/ 3200]\n",
      "loss: 1.196549  [ 2368/ 3200]\n",
      "loss: 1.171033  [ 2384/ 3200]\n",
      "loss: 1.260881  [ 2400/ 3200]\n",
      "loss: 1.268583  [ 2416/ 3200]\n",
      "loss: 1.328175  [ 2432/ 3200]\n",
      "loss: 1.253216  [ 2448/ 3200]\n",
      "loss: 1.234679  [ 2464/ 3200]\n",
      "loss: 1.265837  [ 2480/ 3200]\n",
      "loss: 1.246960  [ 2496/ 3200]\n",
      "loss: 1.298428  [ 2512/ 3200]\n",
      "loss: 1.241327  [ 2528/ 3200]\n",
      "loss: 1.287748  [ 2544/ 3200]\n",
      "loss: 1.240304  [ 2560/ 3200]\n",
      "loss: 1.242319  [ 2576/ 3200]\n",
      "loss: 1.175151  [ 2592/ 3200]\n",
      "loss: 1.273811  [ 2608/ 3200]\n",
      "loss: 1.239779  [ 2624/ 3200]\n",
      "loss: 1.252129  [ 2640/ 3200]\n",
      "loss: 1.304221  [ 2656/ 3200]\n",
      "loss: 1.259673  [ 2672/ 3200]\n",
      "loss: 1.213476  [ 2688/ 3200]\n",
      "loss: 1.285154  [ 2704/ 3200]\n",
      "loss: 1.227276  [ 2720/ 3200]\n",
      "loss: 1.200675  [ 2736/ 3200]\n",
      "loss: 1.277719  [ 2752/ 3200]\n",
      "loss: 1.265669  [ 2768/ 3200]\n",
      "loss: 1.232884  [ 2784/ 3200]\n",
      "loss: 1.246626  [ 2800/ 3200]\n",
      "loss: 1.298562  [ 2816/ 3200]\n",
      "loss: 1.220010  [ 2832/ 3200]\n",
      "loss: 1.347287  [ 2848/ 3200]\n",
      "loss: 1.302607  [ 2864/ 3200]\n",
      "loss: 1.259440  [ 2880/ 3200]\n",
      "loss: 1.279216  [ 2896/ 3200]\n",
      "loss: 1.214340  [ 2912/ 3200]\n",
      "loss: 1.283085  [ 2928/ 3200]\n",
      "loss: 1.286749  [ 2944/ 3200]\n",
      "loss: 1.237215  [ 2960/ 3200]\n",
      "loss: 1.218829  [ 2976/ 3200]\n",
      "loss: 1.358037  [ 2992/ 3200]\n",
      "loss: 1.303192  [ 3008/ 3200]\n",
      "loss: 1.250969  [ 3024/ 3200]\n",
      "loss: 1.237842  [ 3040/ 3200]\n",
      "loss: 1.244665  [ 3056/ 3200]\n",
      "loss: 1.268552  [ 3072/ 3200]\n",
      "loss: 1.263319  [ 3088/ 3200]\n",
      "loss: 1.266290  [ 3104/ 3200]\n",
      "loss: 1.275060  [ 3120/ 3200]\n",
      "loss: 1.343437  [ 3136/ 3200]\n",
      "loss: 1.320362  [ 3152/ 3200]\n",
      "loss: 1.253331  [ 3168/ 3200]\n",
      "loss: 1.295011  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 1.210641  [    0/ 3200]\n",
      "loss: 1.211898  [   16/ 3200]\n",
      "loss: 1.186587  [   32/ 3200]\n",
      "loss: 1.307784  [   48/ 3200]\n",
      "loss: 1.237897  [   64/ 3200]\n",
      "loss: 1.250329  [   80/ 3200]\n",
      "loss: 1.288882  [   96/ 3200]\n",
      "loss: 1.165128  [  112/ 3200]\n",
      "loss: 1.200881  [  128/ 3200]\n",
      "loss: 1.274462  [  144/ 3200]\n",
      "loss: 1.198580  [  160/ 3200]\n",
      "loss: 1.210099  [  176/ 3200]\n",
      "loss: 1.306306  [  192/ 3200]\n",
      "loss: 1.266130  [  208/ 3200]\n",
      "loss: 1.307775  [  224/ 3200]\n",
      "loss: 1.318172  [  240/ 3200]\n",
      "loss: 1.207191  [  256/ 3200]\n",
      "loss: 1.219848  [  272/ 3200]\n",
      "loss: 1.189197  [  288/ 3200]\n",
      "loss: 1.216920  [  304/ 3200]\n",
      "loss: 1.301028  [  320/ 3200]\n",
      "loss: 1.180651  [  336/ 3200]\n",
      "loss: 1.283392  [  352/ 3200]\n",
      "loss: 1.292278  [  368/ 3200]\n",
      "loss: 1.190814  [  384/ 3200]\n",
      "loss: 1.299072  [  400/ 3200]\n",
      "loss: 1.186346  [  416/ 3200]\n",
      "loss: 1.248808  [  432/ 3200]\n",
      "loss: 1.269002  [  448/ 3200]\n",
      "loss: 1.277550  [  464/ 3200]\n",
      "loss: 1.258972  [  480/ 3200]\n",
      "loss: 1.230176  [  496/ 3200]\n",
      "loss: 1.302688  [  512/ 3200]\n",
      "loss: 1.178655  [  528/ 3200]\n",
      "loss: 1.167684  [  544/ 3200]\n",
      "loss: 1.277609  [  560/ 3200]\n",
      "loss: 1.297804  [  576/ 3200]\n",
      "loss: 1.192624  [  592/ 3200]\n",
      "loss: 1.235553  [  608/ 3200]\n",
      "loss: 1.309311  [  624/ 3200]\n",
      "loss: 1.261128  [  640/ 3200]\n",
      "loss: 1.259224  [  656/ 3200]\n",
      "loss: 1.249923  [  672/ 3200]\n",
      "loss: 1.255333  [  688/ 3200]\n",
      "loss: 1.238388  [  704/ 3200]\n",
      "loss: 1.277455  [  720/ 3200]\n",
      "loss: 1.259018  [  736/ 3200]\n",
      "loss: 1.278238  [  752/ 3200]\n",
      "loss: 1.266337  [  768/ 3200]\n",
      "loss: 1.224693  [  784/ 3200]\n",
      "loss: 1.286980  [  800/ 3200]\n",
      "loss: 1.181316  [  816/ 3200]\n",
      "loss: 1.303006  [  832/ 3200]\n",
      "loss: 1.269309  [  848/ 3200]\n",
      "loss: 1.309609  [  864/ 3200]\n",
      "loss: 1.261652  [  880/ 3200]\n",
      "loss: 1.248924  [  896/ 3200]\n",
      "loss: 1.284616  [  912/ 3200]\n",
      "loss: 1.353632  [  928/ 3200]\n",
      "loss: 1.215768  [  944/ 3200]\n",
      "loss: 1.240658  [  960/ 3200]\n",
      "loss: 1.324850  [  976/ 3200]\n",
      "loss: 1.225769  [  992/ 3200]\n",
      "loss: 1.296725  [ 1008/ 3200]\n",
      "loss: 1.192632  [ 1024/ 3200]\n",
      "loss: 1.231652  [ 1040/ 3200]\n",
      "loss: 1.159560  [ 1056/ 3200]\n",
      "loss: 1.275892  [ 1072/ 3200]\n",
      "loss: 1.157895  [ 1088/ 3200]\n",
      "loss: 1.160169  [ 1104/ 3200]\n",
      "loss: 1.195428  [ 1120/ 3200]\n",
      "loss: 1.320833  [ 1136/ 3200]\n",
      "loss: 1.210085  [ 1152/ 3200]\n",
      "loss: 1.314353  [ 1168/ 3200]\n",
      "loss: 1.256498  [ 1184/ 3200]\n",
      "loss: 1.190520  [ 1200/ 3200]\n",
      "loss: 1.381467  [ 1216/ 3200]\n",
      "loss: 1.332162  [ 1232/ 3200]\n",
      "loss: 1.271101  [ 1248/ 3200]\n",
      "loss: 1.306337  [ 1264/ 3200]\n",
      "loss: 1.174864  [ 1280/ 3200]\n",
      "loss: 1.335999  [ 1296/ 3200]\n",
      "loss: 1.252509  [ 1312/ 3200]\n",
      "loss: 1.306939  [ 1328/ 3200]\n",
      "loss: 1.232410  [ 1344/ 3200]\n",
      "loss: 1.274575  [ 1360/ 3200]\n",
      "loss: 1.238344  [ 1376/ 3200]\n",
      "loss: 1.176507  [ 1392/ 3200]\n",
      "loss: 1.255125  [ 1408/ 3200]\n",
      "loss: 1.228144  [ 1424/ 3200]\n",
      "loss: 1.193819  [ 1440/ 3200]\n",
      "loss: 1.210428  [ 1456/ 3200]\n",
      "loss: 1.280422  [ 1472/ 3200]\n",
      "loss: 1.214831  [ 1488/ 3200]\n",
      "loss: 1.256673  [ 1504/ 3200]\n",
      "loss: 1.324831  [ 1520/ 3200]\n",
      "loss: 1.246796  [ 1536/ 3200]\n",
      "loss: 1.215126  [ 1552/ 3200]\n",
      "loss: 1.297154  [ 1568/ 3200]\n",
      "loss: 1.252431  [ 1584/ 3200]\n",
      "loss: 1.238410  [ 1600/ 3200]\n",
      "loss: 1.292454  [ 1616/ 3200]\n",
      "loss: 1.161321  [ 1632/ 3200]\n",
      "loss: 1.263668  [ 1648/ 3200]\n",
      "loss: 1.167820  [ 1664/ 3200]\n",
      "loss: 1.338324  [ 1680/ 3200]\n",
      "loss: 1.287036  [ 1696/ 3200]\n",
      "loss: 1.211095  [ 1712/ 3200]\n",
      "loss: 1.279415  [ 1728/ 3200]\n",
      "loss: 1.348929  [ 1744/ 3200]\n",
      "loss: 1.197901  [ 1760/ 3200]\n",
      "loss: 1.307495  [ 1776/ 3200]\n",
      "loss: 1.249471  [ 1792/ 3200]\n",
      "loss: 1.274364  [ 1808/ 3200]\n",
      "loss: 1.218942  [ 1824/ 3200]\n",
      "loss: 1.222841  [ 1840/ 3200]\n",
      "loss: 1.210946  [ 1856/ 3200]\n",
      "loss: 1.252777  [ 1872/ 3200]\n",
      "loss: 1.272488  [ 1888/ 3200]\n",
      "loss: 1.176596  [ 1904/ 3200]\n",
      "loss: 1.303200  [ 1920/ 3200]\n",
      "loss: 1.277063  [ 1936/ 3200]\n",
      "loss: 1.337550  [ 1952/ 3200]\n",
      "loss: 1.254967  [ 1968/ 3200]\n",
      "loss: 1.194726  [ 1984/ 3200]\n",
      "loss: 1.217293  [ 2000/ 3200]\n",
      "loss: 1.195635  [ 2016/ 3200]\n",
      "loss: 1.180069  [ 2032/ 3200]\n",
      "loss: 1.168261  [ 2048/ 3200]\n",
      "loss: 1.240451  [ 2064/ 3200]\n",
      "loss: 1.219405  [ 2080/ 3200]\n",
      "loss: 1.213227  [ 2096/ 3200]\n",
      "loss: 1.260767  [ 2112/ 3200]\n",
      "loss: 1.282051  [ 2128/ 3200]\n",
      "loss: 1.277897  [ 2144/ 3200]\n",
      "loss: 1.232028  [ 2160/ 3200]\n",
      "loss: 1.209306  [ 2176/ 3200]\n",
      "loss: 1.201279  [ 2192/ 3200]\n",
      "loss: 1.273615  [ 2208/ 3200]\n",
      "loss: 1.339310  [ 2224/ 3200]\n",
      "loss: 1.247230  [ 2240/ 3200]\n",
      "loss: 1.283742  [ 2256/ 3200]\n",
      "loss: 1.253362  [ 2272/ 3200]\n",
      "loss: 1.240519  [ 2288/ 3200]\n",
      "loss: 1.195415  [ 2304/ 3200]\n",
      "loss: 1.191158  [ 2320/ 3200]\n",
      "loss: 1.174629  [ 2336/ 3200]\n",
      "loss: 1.310306  [ 2352/ 3200]\n",
      "loss: 1.301779  [ 2368/ 3200]\n",
      "loss: 1.299243  [ 2384/ 3200]\n",
      "loss: 1.201979  [ 2400/ 3200]\n",
      "loss: 1.220136  [ 2416/ 3200]\n",
      "loss: 1.175305  [ 2432/ 3200]\n",
      "loss: 1.186143  [ 2448/ 3200]\n",
      "loss: 1.239921  [ 2464/ 3200]\n",
      "loss: 1.176355  [ 2480/ 3200]\n",
      "loss: 1.155792  [ 2496/ 3200]\n",
      "loss: 1.181840  [ 2512/ 3200]\n",
      "loss: 1.202643  [ 2528/ 3200]\n",
      "loss: 1.250652  [ 2544/ 3200]\n",
      "loss: 1.234202  [ 2560/ 3200]\n",
      "loss: 1.305630  [ 2576/ 3200]\n",
      "loss: 1.259607  [ 2592/ 3200]\n",
      "loss: 1.214115  [ 2608/ 3200]\n",
      "loss: 1.326052  [ 2624/ 3200]\n",
      "loss: 1.250537  [ 2640/ 3200]\n",
      "loss: 1.258823  [ 2656/ 3200]\n",
      "loss: 1.028394  [ 2672/ 3200]\n",
      "loss: 1.244293  [ 2688/ 3200]\n",
      "loss: 1.183583  [ 2704/ 3200]\n",
      "loss: 1.246276  [ 2720/ 3200]\n",
      "loss: 1.208389  [ 2736/ 3200]\n",
      "loss: 1.244324  [ 2752/ 3200]\n",
      "loss: 1.161661  [ 2768/ 3200]\n",
      "loss: 1.263846  [ 2784/ 3200]\n",
      "loss: 1.289631  [ 2800/ 3200]\n",
      "loss: 1.168413  [ 2816/ 3200]\n",
      "loss: 1.045982  [ 2832/ 3200]\n",
      "loss: 1.145451  [ 2848/ 3200]\n",
      "loss: 1.202808  [ 2864/ 3200]\n",
      "loss: 1.181584  [ 2880/ 3200]\n",
      "loss: 1.148907  [ 2896/ 3200]\n",
      "loss: 1.216119  [ 2912/ 3200]\n",
      "loss: 1.289401  [ 2928/ 3200]\n",
      "loss: 1.324850  [ 2944/ 3200]\n",
      "loss: 1.262969  [ 2960/ 3200]\n",
      "loss: 1.260878  [ 2976/ 3200]\n",
      "loss: 1.313079  [ 2992/ 3200]\n",
      "loss: 1.195048  [ 3008/ 3200]\n",
      "loss: 1.213727  [ 3024/ 3200]\n",
      "loss: 1.189855  [ 3040/ 3200]\n",
      "loss: 1.241984  [ 3056/ 3200]\n",
      "loss: 1.168313  [ 3072/ 3200]\n",
      "loss: 1.228473  [ 3088/ 3200]\n",
      "loss: 1.169657  [ 3104/ 3200]\n",
      "loss: 1.201744  [ 3120/ 3200]\n",
      "loss: 1.293677  [ 3136/ 3200]\n",
      "loss: 1.288152  [ 3152/ 3200]\n",
      "loss: 1.320045  [ 3168/ 3200]\n",
      "loss: 1.310461  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 1.243549  [    0/ 3200]\n",
      "loss: 1.194319  [   16/ 3200]\n",
      "loss: 1.225895  [   32/ 3200]\n",
      "loss: 1.166502  [   48/ 3200]\n",
      "loss: 1.347263  [   64/ 3200]\n",
      "loss: 1.123987  [   80/ 3200]\n",
      "loss: 1.208709  [   96/ 3200]\n",
      "loss: 1.401812  [  112/ 3200]\n",
      "loss: 1.203784  [  128/ 3200]\n",
      "loss: 1.189144  [  144/ 3200]\n",
      "loss: 1.211525  [  160/ 3200]\n",
      "loss: 1.215532  [  176/ 3200]\n",
      "loss: 1.251464  [  192/ 3200]\n",
      "loss: 1.276182  [  208/ 3200]\n",
      "loss: 1.181269  [  224/ 3200]\n",
      "loss: 1.205745  [  240/ 3200]\n",
      "loss: 1.299303  [  256/ 3200]\n",
      "loss: 1.382011  [  272/ 3200]\n",
      "loss: 1.245059  [  288/ 3200]\n",
      "loss: 1.285601  [  304/ 3200]\n",
      "loss: 1.256904  [  320/ 3200]\n",
      "loss: 1.177403  [  336/ 3200]\n",
      "loss: 1.238682  [  352/ 3200]\n",
      "loss: 1.262069  [  368/ 3200]\n",
      "loss: 1.333467  [  384/ 3200]\n",
      "loss: 1.191237  [  400/ 3200]\n",
      "loss: 1.257244  [  416/ 3200]\n",
      "loss: 1.199423  [  432/ 3200]\n",
      "loss: 1.180352  [  448/ 3200]\n",
      "loss: 1.209040  [  464/ 3200]\n",
      "loss: 1.174191  [  480/ 3200]\n",
      "loss: 1.270371  [  496/ 3200]\n",
      "loss: 1.245078  [  512/ 3200]\n",
      "loss: 1.245620  [  528/ 3200]\n",
      "loss: 1.153692  [  544/ 3200]\n",
      "loss: 1.175819  [  560/ 3200]\n",
      "loss: 1.218373  [  576/ 3200]\n",
      "loss: 1.317549  [  592/ 3200]\n",
      "loss: 1.201380  [  608/ 3200]\n",
      "loss: 1.203112  [  624/ 3200]\n",
      "loss: 1.146527  [  640/ 3200]\n",
      "loss: 1.270468  [  656/ 3200]\n",
      "loss: 1.243894  [  672/ 3200]\n",
      "loss: 1.178594  [  688/ 3200]\n",
      "loss: 1.232253  [  704/ 3200]\n",
      "loss: 1.287605  [  720/ 3200]\n",
      "loss: 1.237107  [  736/ 3200]\n",
      "loss: 1.196607  [  752/ 3200]\n",
      "loss: 1.250925  [  768/ 3200]\n",
      "loss: 1.169980  [  784/ 3200]\n",
      "loss: 1.312179  [  800/ 3200]\n",
      "loss: 1.256757  [  816/ 3200]\n",
      "loss: 1.240709  [  832/ 3200]\n",
      "loss: 1.223076  [  848/ 3200]\n",
      "loss: 1.269940  [  864/ 3200]\n",
      "loss: 1.255386  [  880/ 3200]\n",
      "loss: 1.306056  [  896/ 3200]\n",
      "loss: 1.187626  [  912/ 3200]\n",
      "loss: 1.195075  [  928/ 3200]\n",
      "loss: 1.246041  [  944/ 3200]\n",
      "loss: 1.206348  [  960/ 3200]\n",
      "loss: 1.177700  [  976/ 3200]\n",
      "loss: 1.195981  [  992/ 3200]\n",
      "loss: 1.198560  [ 1008/ 3200]\n",
      "loss: 1.211299  [ 1024/ 3200]\n",
      "loss: 1.172788  [ 1040/ 3200]\n",
      "loss: 1.157251  [ 1056/ 3200]\n",
      "loss: 1.229540  [ 1072/ 3200]\n",
      "loss: 1.284587  [ 1088/ 3200]\n",
      "loss: 1.223571  [ 1104/ 3200]\n",
      "loss: 1.281139  [ 1120/ 3200]\n",
      "loss: 1.278740  [ 1136/ 3200]\n",
      "loss: 1.235179  [ 1152/ 3200]\n",
      "loss: 1.188250  [ 1168/ 3200]\n",
      "loss: 1.212548  [ 1184/ 3200]\n",
      "loss: 1.254263  [ 1200/ 3200]\n",
      "loss: 1.233905  [ 1216/ 3200]\n",
      "loss: 1.249639  [ 1232/ 3200]\n",
      "loss: 1.237500  [ 1248/ 3200]\n",
      "loss: 1.246998  [ 1264/ 3200]\n",
      "loss: 1.249056  [ 1280/ 3200]\n",
      "loss: 1.262183  [ 1296/ 3200]\n",
      "loss: 1.214564  [ 1312/ 3200]\n",
      "loss: 1.088365  [ 1328/ 3200]\n",
      "loss: 1.228825  [ 1344/ 3200]\n",
      "loss: 1.159445  [ 1360/ 3200]\n",
      "loss: 1.184035  [ 1376/ 3200]\n",
      "loss: 1.224570  [ 1392/ 3200]\n",
      "loss: 1.197270  [ 1408/ 3200]\n",
      "loss: 1.227475  [ 1424/ 3200]\n",
      "loss: 1.309113  [ 1440/ 3200]\n",
      "loss: 1.277971  [ 1456/ 3200]\n",
      "loss: 1.258717  [ 1472/ 3200]\n",
      "loss: 1.298987  [ 1488/ 3200]\n",
      "loss: 1.269156  [ 1504/ 3200]\n",
      "loss: 1.210578  [ 1520/ 3200]\n",
      "loss: 1.210737  [ 1536/ 3200]\n",
      "loss: 1.213781  [ 1552/ 3200]\n",
      "loss: 1.242271  [ 1568/ 3200]\n",
      "loss: 1.136187  [ 1584/ 3200]\n",
      "loss: 1.194296  [ 1600/ 3200]\n",
      "loss: 1.267602  [ 1616/ 3200]\n",
      "loss: 1.221838  [ 1632/ 3200]\n",
      "loss: 1.255753  [ 1648/ 3200]\n",
      "loss: 1.222624  [ 1664/ 3200]\n",
      "loss: 1.231011  [ 1680/ 3200]\n",
      "loss: 1.248855  [ 1696/ 3200]\n",
      "loss: 1.189552  [ 1712/ 3200]\n",
      "loss: 1.231843  [ 1728/ 3200]\n",
      "loss: 1.129005  [ 1744/ 3200]\n",
      "loss: 1.293020  [ 1760/ 3200]\n",
      "loss: 1.223004  [ 1776/ 3200]\n",
      "loss: 1.268320  [ 1792/ 3200]\n",
      "loss: 1.180417  [ 1808/ 3200]\n",
      "loss: 1.171187  [ 1824/ 3200]\n",
      "loss: 1.152606  [ 1840/ 3200]\n",
      "loss: 1.196226  [ 1856/ 3200]\n",
      "loss: 1.219393  [ 1872/ 3200]\n",
      "loss: 1.221774  [ 1888/ 3200]\n",
      "loss: 1.238194  [ 1904/ 3200]\n",
      "loss: 1.166868  [ 1920/ 3200]\n",
      "loss: 1.180765  [ 1936/ 3200]\n",
      "loss: 1.268306  [ 1952/ 3200]\n",
      "loss: 1.201801  [ 1968/ 3200]\n",
      "loss: 1.258251  [ 1984/ 3200]\n",
      "loss: 1.231263  [ 2000/ 3200]\n",
      "loss: 1.311285  [ 2016/ 3200]\n",
      "loss: 1.194981  [ 2032/ 3200]\n",
      "loss: 1.163966  [ 2048/ 3200]\n",
      "loss: 1.214716  [ 2064/ 3200]\n",
      "loss: 1.279599  [ 2080/ 3200]\n",
      "loss: 1.190525  [ 2096/ 3200]\n",
      "loss: 1.240083  [ 2112/ 3200]\n",
      "loss: 1.152581  [ 2128/ 3200]\n",
      "loss: 1.229782  [ 2144/ 3200]\n",
      "loss: 1.262959  [ 2160/ 3200]\n",
      "loss: 1.192746  [ 2176/ 3200]\n",
      "loss: 1.220425  [ 2192/ 3200]\n",
      "loss: 1.181929  [ 2208/ 3200]\n",
      "loss: 1.149927  [ 2224/ 3200]\n",
      "loss: 1.178201  [ 2240/ 3200]\n",
      "loss: 1.067640  [ 2256/ 3200]\n",
      "loss: 1.172201  [ 2272/ 3200]\n",
      "loss: 1.278290  [ 2288/ 3200]\n",
      "loss: 1.147374  [ 2304/ 3200]\n",
      "loss: 1.232227  [ 2320/ 3200]\n",
      "loss: 1.226969  [ 2336/ 3200]\n",
      "loss: 1.313899  [ 2352/ 3200]\n",
      "loss: 1.198102  [ 2368/ 3200]\n",
      "loss: 1.297535  [ 2384/ 3200]\n",
      "loss: 1.273526  [ 2400/ 3200]\n",
      "loss: 1.179307  [ 2416/ 3200]\n",
      "loss: 1.201890  [ 2432/ 3200]\n",
      "loss: 1.182308  [ 2448/ 3200]\n",
      "loss: 1.149529  [ 2464/ 3200]\n",
      "loss: 1.263902  [ 2480/ 3200]\n",
      "loss: 1.183700  [ 2496/ 3200]\n",
      "loss: 1.241385  [ 2512/ 3200]\n",
      "loss: 1.164448  [ 2528/ 3200]\n",
      "loss: 1.266337  [ 2544/ 3200]\n",
      "loss: 1.247834  [ 2560/ 3200]\n",
      "loss: 1.170465  [ 2576/ 3200]\n",
      "loss: 1.249289  [ 2592/ 3200]\n",
      "loss: 1.230553  [ 2608/ 3200]\n",
      "loss: 1.228545  [ 2624/ 3200]\n",
      "loss: 1.224937  [ 2640/ 3200]\n",
      "loss: 1.246754  [ 2656/ 3200]\n",
      "loss: 1.160298  [ 2672/ 3200]\n",
      "loss: 1.254937  [ 2688/ 3200]\n",
      "loss: 1.242605  [ 2704/ 3200]\n",
      "loss: 1.150978  [ 2720/ 3200]\n",
      "loss: 1.238058  [ 2736/ 3200]\n",
      "loss: 1.151461  [ 2752/ 3200]\n",
      "loss: 1.211487  [ 2768/ 3200]\n",
      "loss: 1.360707  [ 2784/ 3200]\n",
      "loss: 1.092252  [ 2800/ 3200]\n",
      "loss: 1.125824  [ 2816/ 3200]\n",
      "loss: 1.171956  [ 2832/ 3200]\n",
      "loss: 1.301416  [ 2848/ 3200]\n",
      "loss: 1.212253  [ 2864/ 3200]\n",
      "loss: 1.140746  [ 2880/ 3200]\n",
      "loss: 1.232889  [ 2896/ 3200]\n",
      "loss: 1.176739  [ 2912/ 3200]\n",
      "loss: 1.228253  [ 2928/ 3200]\n",
      "loss: 1.125885  [ 2944/ 3200]\n",
      "loss: 1.277440  [ 2960/ 3200]\n",
      "loss: 1.300323  [ 2976/ 3200]\n",
      "loss: 1.242582  [ 2992/ 3200]\n",
      "loss: 1.087909  [ 3008/ 3200]\n",
      "loss: 1.163487  [ 3024/ 3200]\n",
      "loss: 1.241838  [ 3040/ 3200]\n",
      "loss: 1.214332  [ 3056/ 3200]\n",
      "loss: 1.204225  [ 3072/ 3200]\n",
      "loss: 1.194620  [ 3088/ 3200]\n",
      "loss: 1.231438  [ 3104/ 3200]\n",
      "loss: 1.182607  [ 3120/ 3200]\n",
      "loss: 1.213300  [ 3136/ 3200]\n",
      "loss: 1.226004  [ 3152/ 3200]\n",
      "loss: 1.287502  [ 3168/ 3200]\n",
      "loss: 1.141350  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 1.195971  [    0/ 3200]\n",
      "loss: 1.212670  [   16/ 3200]\n",
      "loss: 1.158277  [   32/ 3200]\n",
      "loss: 1.198401  [   48/ 3200]\n",
      "loss: 1.192689  [   64/ 3200]\n",
      "loss: 1.212689  [   80/ 3200]\n",
      "loss: 1.234428  [   96/ 3200]\n",
      "loss: 1.289264  [  112/ 3200]\n",
      "loss: 1.081492  [  128/ 3200]\n",
      "loss: 1.209129  [  144/ 3200]\n",
      "loss: 1.245714  [  160/ 3200]\n",
      "loss: 1.180577  [  176/ 3200]\n",
      "loss: 1.258206  [  192/ 3200]\n",
      "loss: 1.202350  [  208/ 3200]\n",
      "loss: 1.274116  [  224/ 3200]\n",
      "loss: 1.289062  [  240/ 3200]\n",
      "loss: 1.159897  [  256/ 3200]\n",
      "loss: 1.135792  [  272/ 3200]\n",
      "loss: 1.193988  [  288/ 3200]\n",
      "loss: 1.169047  [  304/ 3200]\n",
      "loss: 1.276197  [  320/ 3200]\n",
      "loss: 1.257825  [  336/ 3200]\n",
      "loss: 1.197515  [  352/ 3200]\n",
      "loss: 1.146662  [  368/ 3200]\n",
      "loss: 1.274756  [  384/ 3200]\n",
      "loss: 1.143227  [  400/ 3200]\n",
      "loss: 1.214777  [  416/ 3200]\n",
      "loss: 1.197056  [  432/ 3200]\n",
      "loss: 1.147925  [  448/ 3200]\n",
      "loss: 1.166524  [  464/ 3200]\n",
      "loss: 1.181164  [  480/ 3200]\n",
      "loss: 1.124938  [  496/ 3200]\n",
      "loss: 1.356129  [  512/ 3200]\n",
      "loss: 1.207168  [  528/ 3200]\n",
      "loss: 1.230257  [  544/ 3200]\n",
      "loss: 1.189591  [  560/ 3200]\n",
      "loss: 1.340824  [  576/ 3200]\n",
      "loss: 1.077022  [  592/ 3200]\n",
      "loss: 1.293585  [  608/ 3200]\n",
      "loss: 1.223384  [  624/ 3200]\n",
      "loss: 1.150298  [  640/ 3200]\n",
      "loss: 1.220596  [  656/ 3200]\n",
      "loss: 1.157280  [  672/ 3200]\n",
      "loss: 1.225776  [  688/ 3200]\n",
      "loss: 1.263421  [  704/ 3200]\n",
      "loss: 1.131991  [  720/ 3200]\n",
      "loss: 1.175173  [  736/ 3200]\n",
      "loss: 1.145795  [  752/ 3200]\n",
      "loss: 1.277750  [  768/ 3200]\n",
      "loss: 1.139478  [  784/ 3200]\n",
      "loss: 1.189668  [  800/ 3200]\n",
      "loss: 1.225518  [  816/ 3200]\n",
      "loss: 1.216046  [  832/ 3200]\n",
      "loss: 1.176829  [  848/ 3200]\n",
      "loss: 1.203046  [  864/ 3200]\n",
      "loss: 1.165774  [  880/ 3200]\n",
      "loss: 1.256279  [  896/ 3200]\n",
      "loss: 1.093567  [  912/ 3200]\n",
      "loss: 1.315667  [  928/ 3200]\n",
      "loss: 1.295576  [  944/ 3200]\n",
      "loss: 1.162559  [  960/ 3200]\n",
      "loss: 1.264494  [  976/ 3200]\n",
      "loss: 1.276770  [  992/ 3200]\n",
      "loss: 1.284528  [ 1008/ 3200]\n",
      "loss: 1.216711  [ 1024/ 3200]\n",
      "loss: 1.206432  [ 1040/ 3200]\n",
      "loss: 1.036242  [ 1056/ 3200]\n",
      "loss: 1.278989  [ 1072/ 3200]\n",
      "loss: 1.193218  [ 1088/ 3200]\n",
      "loss: 1.270974  [ 1104/ 3200]\n",
      "loss: 1.157337  [ 1120/ 3200]\n",
      "loss: 1.159554  [ 1136/ 3200]\n",
      "loss: 1.199889  [ 1152/ 3200]\n",
      "loss: 1.241128  [ 1168/ 3200]\n",
      "loss: 1.171607  [ 1184/ 3200]\n",
      "loss: 1.120837  [ 1200/ 3200]\n",
      "loss: 1.292864  [ 1216/ 3200]\n",
      "loss: 1.137905  [ 1232/ 3200]\n",
      "loss: 1.163224  [ 1248/ 3200]\n",
      "loss: 1.083568  [ 1264/ 3200]\n",
      "loss: 1.217652  [ 1280/ 3200]\n",
      "loss: 1.329463  [ 1296/ 3200]\n",
      "loss: 1.283081  [ 1312/ 3200]\n",
      "loss: 1.054502  [ 1328/ 3200]\n",
      "loss: 1.134020  [ 1344/ 3200]\n",
      "loss: 1.117267  [ 1360/ 3200]\n",
      "loss: 1.237499  [ 1376/ 3200]\n",
      "loss: 1.240017  [ 1392/ 3200]\n",
      "loss: 1.274940  [ 1408/ 3200]\n",
      "loss: 1.242649  [ 1424/ 3200]\n",
      "loss: 1.095073  [ 1440/ 3200]\n",
      "loss: 1.142770  [ 1456/ 3200]\n",
      "loss: 1.208293  [ 1472/ 3200]\n",
      "loss: 1.203694  [ 1488/ 3200]\n",
      "loss: 1.195812  [ 1504/ 3200]\n",
      "loss: 1.222499  [ 1520/ 3200]\n",
      "loss: 1.156776  [ 1536/ 3200]\n",
      "loss: 1.121479  [ 1552/ 3200]\n",
      "loss: 1.182178  [ 1568/ 3200]\n",
      "loss: 1.202427  [ 1584/ 3200]\n",
      "loss: 1.154796  [ 1600/ 3200]\n",
      "loss: 1.249789  [ 1616/ 3200]\n",
      "loss: 1.205728  [ 1632/ 3200]\n",
      "loss: 1.131926  [ 1648/ 3200]\n",
      "loss: 1.303135  [ 1664/ 3200]\n",
      "loss: 1.171318  [ 1680/ 3200]\n",
      "loss: 1.301491  [ 1696/ 3200]\n",
      "loss: 1.241759  [ 1712/ 3200]\n",
      "loss: 1.253317  [ 1728/ 3200]\n",
      "loss: 1.250382  [ 1744/ 3200]\n",
      "loss: 1.134551  [ 1760/ 3200]\n",
      "loss: 1.107263  [ 1776/ 3200]\n",
      "loss: 1.196398  [ 1792/ 3200]\n",
      "loss: 1.133390  [ 1808/ 3200]\n",
      "loss: 1.201980  [ 1824/ 3200]\n",
      "loss: 1.148208  [ 1840/ 3200]\n",
      "loss: 1.137591  [ 1856/ 3200]\n",
      "loss: 1.148794  [ 1872/ 3200]\n",
      "loss: 1.027055  [ 1888/ 3200]\n",
      "loss: 1.202202  [ 1904/ 3200]\n",
      "loss: 1.196657  [ 1920/ 3200]\n",
      "loss: 1.174625  [ 1936/ 3200]\n",
      "loss: 1.247431  [ 1952/ 3200]\n",
      "loss: 1.124107  [ 1968/ 3200]\n",
      "loss: 1.172232  [ 1984/ 3200]\n",
      "loss: 1.166825  [ 2000/ 3200]\n",
      "loss: 1.196985  [ 2016/ 3200]\n",
      "loss: 1.132036  [ 2032/ 3200]\n",
      "loss: 1.259484  [ 2048/ 3200]\n",
      "loss: 1.266249  [ 2064/ 3200]\n",
      "loss: 1.106870  [ 2080/ 3200]\n",
      "loss: 1.111259  [ 2096/ 3200]\n",
      "loss: 1.221268  [ 2112/ 3200]\n",
      "loss: 1.253488  [ 2128/ 3200]\n",
      "loss: 1.212919  [ 2144/ 3200]\n",
      "loss: 1.213686  [ 2160/ 3200]\n",
      "loss: 1.259074  [ 2176/ 3200]\n",
      "loss: 1.091935  [ 2192/ 3200]\n",
      "loss: 1.181671  [ 2208/ 3200]\n",
      "loss: 1.204149  [ 2224/ 3200]\n",
      "loss: 1.085676  [ 2240/ 3200]\n",
      "loss: 1.244003  [ 2256/ 3200]\n",
      "loss: 1.262338  [ 2272/ 3200]\n",
      "loss: 1.304445  [ 2288/ 3200]\n",
      "loss: 1.124148  [ 2304/ 3200]\n",
      "loss: 1.196310  [ 2320/ 3200]\n",
      "loss: 1.188367  [ 2336/ 3200]\n",
      "loss: 1.168171  [ 2352/ 3200]\n",
      "loss: 1.109192  [ 2368/ 3200]\n",
      "loss: 1.062562  [ 2384/ 3200]\n",
      "loss: 1.350965  [ 2400/ 3200]\n",
      "loss: 1.266292  [ 2416/ 3200]\n",
      "loss: 1.180160  [ 2432/ 3200]\n",
      "loss: 1.182106  [ 2448/ 3200]\n",
      "loss: 1.251523  [ 2464/ 3200]\n",
      "loss: 1.234613  [ 2480/ 3200]\n",
      "loss: 1.237839  [ 2496/ 3200]\n",
      "loss: 1.148081  [ 2512/ 3200]\n",
      "loss: 1.283825  [ 2528/ 3200]\n",
      "loss: 1.205107  [ 2544/ 3200]\n",
      "loss: 1.144307  [ 2560/ 3200]\n",
      "loss: 1.141127  [ 2576/ 3200]\n",
      "loss: 1.100268  [ 2592/ 3200]\n",
      "loss: 1.167651  [ 2608/ 3200]\n",
      "loss: 1.262063  [ 2624/ 3200]\n",
      "loss: 1.291709  [ 2640/ 3200]\n",
      "loss: 1.223273  [ 2656/ 3200]\n",
      "loss: 1.134475  [ 2672/ 3200]\n",
      "loss: 1.253262  [ 2688/ 3200]\n",
      "loss: 1.151510  [ 2704/ 3200]\n",
      "loss: 1.317504  [ 2720/ 3200]\n",
      "loss: 1.216272  [ 2736/ 3200]\n",
      "loss: 1.223545  [ 2752/ 3200]\n",
      "loss: 1.165066  [ 2768/ 3200]\n",
      "loss: 1.178275  [ 2784/ 3200]\n",
      "loss: 1.094242  [ 2800/ 3200]\n",
      "loss: 1.219764  [ 2816/ 3200]\n",
      "loss: 1.256040  [ 2832/ 3200]\n",
      "loss: 1.097338  [ 2848/ 3200]\n",
      "loss: 1.301310  [ 2864/ 3200]\n",
      "loss: 1.262260  [ 2880/ 3200]\n",
      "loss: 1.212529  [ 2896/ 3200]\n",
      "loss: 1.150931  [ 2912/ 3200]\n",
      "loss: 1.298057  [ 2928/ 3200]\n",
      "loss: 1.205015  [ 2944/ 3200]\n",
      "loss: 1.136701  [ 2960/ 3200]\n",
      "loss: 1.139461  [ 2976/ 3200]\n",
      "loss: 1.186616  [ 2992/ 3200]\n",
      "loss: 1.170391  [ 3008/ 3200]\n",
      "loss: 1.201995  [ 3024/ 3200]\n",
      "loss: 1.221898  [ 3040/ 3200]\n",
      "loss: 1.190022  [ 3056/ 3200]\n",
      "loss: 1.311713  [ 3072/ 3200]\n",
      "loss: 1.128298  [ 3088/ 3200]\n",
      "loss: 1.208584  [ 3104/ 3200]\n",
      "loss: 1.167854  [ 3120/ 3200]\n",
      "loss: 1.221187  [ 3136/ 3200]\n",
      "loss: 1.134886  [ 3152/ 3200]\n",
      "loss: 1.152172  [ 3168/ 3200]\n",
      "loss: 1.189301  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 1.260816  [    0/ 3200]\n",
      "loss: 1.019504  [   16/ 3200]\n",
      "loss: 1.115791  [   32/ 3200]\n",
      "loss: 1.155682  [   48/ 3200]\n",
      "loss: 1.116764  [   64/ 3200]\n",
      "loss: 1.269240  [   80/ 3200]\n",
      "loss: 1.198122  [   96/ 3200]\n",
      "loss: 1.225533  [  112/ 3200]\n",
      "loss: 1.078402  [  128/ 3200]\n",
      "loss: 1.157096  [  144/ 3200]\n",
      "loss: 1.167679  [  160/ 3200]\n",
      "loss: 1.242772  [  176/ 3200]\n",
      "loss: 1.324828  [  192/ 3200]\n",
      "loss: 1.178941  [  208/ 3200]\n",
      "loss: 1.189638  [  224/ 3200]\n",
      "loss: 1.163696  [  240/ 3200]\n",
      "loss: 1.096865  [  256/ 3200]\n",
      "loss: 1.096656  [  272/ 3200]\n",
      "loss: 1.098701  [  288/ 3200]\n",
      "loss: 1.293440  [  304/ 3200]\n",
      "loss: 1.076768  [  320/ 3200]\n",
      "loss: 1.088299  [  336/ 3200]\n",
      "loss: 1.243650  [  352/ 3200]\n",
      "loss: 1.269744  [  368/ 3200]\n",
      "loss: 1.137571  [  384/ 3200]\n",
      "loss: 1.150197  [  400/ 3200]\n",
      "loss: 1.139655  [  416/ 3200]\n",
      "loss: 1.236261  [  432/ 3200]\n",
      "loss: 1.138348  [  448/ 3200]\n",
      "loss: 1.232711  [  464/ 3200]\n",
      "loss: 1.142720  [  480/ 3200]\n",
      "loss: 1.170730  [  496/ 3200]\n",
      "loss: 1.129738  [  512/ 3200]\n",
      "loss: 1.152573  [  528/ 3200]\n",
      "loss: 1.167873  [  544/ 3200]\n",
      "loss: 1.255895  [  560/ 3200]\n",
      "loss: 1.059758  [  576/ 3200]\n",
      "loss: 1.163986  [  592/ 3200]\n",
      "loss: 0.947016  [  608/ 3200]\n",
      "loss: 1.262871  [  624/ 3200]\n",
      "loss: 1.102443  [  640/ 3200]\n",
      "loss: 1.178293  [  656/ 3200]\n",
      "loss: 1.152036  [  672/ 3200]\n",
      "loss: 1.266149  [  688/ 3200]\n",
      "loss: 1.273862  [  704/ 3200]\n",
      "loss: 1.213790  [  720/ 3200]\n",
      "loss: 1.043671  [  736/ 3200]\n",
      "loss: 1.082718  [  752/ 3200]\n",
      "loss: 1.258030  [  768/ 3200]\n",
      "loss: 1.100993  [  784/ 3200]\n",
      "loss: 1.035458  [  800/ 3200]\n",
      "loss: 1.051941  [  816/ 3200]\n",
      "loss: 1.118946  [  832/ 3200]\n",
      "loss: 1.118014  [  848/ 3200]\n",
      "loss: 1.246916  [  864/ 3200]\n",
      "loss: 1.257781  [  880/ 3200]\n",
      "loss: 1.196530  [  896/ 3200]\n",
      "loss: 1.184252  [  912/ 3200]\n",
      "loss: 1.153466  [  928/ 3200]\n",
      "loss: 1.218468  [  944/ 3200]\n",
      "loss: 1.123586  [  960/ 3200]\n",
      "loss: 1.180640  [  976/ 3200]\n",
      "loss: 1.236898  [  992/ 3200]\n",
      "loss: 1.276593  [ 1008/ 3200]\n",
      "loss: 1.146140  [ 1024/ 3200]\n",
      "loss: 1.136696  [ 1040/ 3200]\n",
      "loss: 1.240244  [ 1056/ 3200]\n",
      "loss: 1.128259  [ 1072/ 3200]\n",
      "loss: 1.196726  [ 1088/ 3200]\n",
      "loss: 1.089013  [ 1104/ 3200]\n",
      "loss: 1.120228  [ 1120/ 3200]\n",
      "loss: 1.124113  [ 1136/ 3200]\n",
      "loss: 0.998379  [ 1152/ 3200]\n",
      "loss: 1.151223  [ 1168/ 3200]\n",
      "loss: 1.324481  [ 1184/ 3200]\n",
      "loss: 1.209260  [ 1200/ 3200]\n",
      "loss: 1.247484  [ 1216/ 3200]\n",
      "loss: 1.224683  [ 1232/ 3200]\n",
      "loss: 1.093868  [ 1248/ 3200]\n",
      "loss: 1.218985  [ 1264/ 3200]\n",
      "loss: 1.220536  [ 1280/ 3200]\n",
      "loss: 1.229892  [ 1296/ 3200]\n",
      "loss: 1.200660  [ 1312/ 3200]\n",
      "loss: 1.127462  [ 1328/ 3200]\n",
      "loss: 1.083378  [ 1344/ 3200]\n",
      "loss: 1.164758  [ 1360/ 3200]\n",
      "loss: 1.163273  [ 1376/ 3200]\n",
      "loss: 1.144388  [ 1392/ 3200]\n",
      "loss: 1.291686  [ 1408/ 3200]\n",
      "loss: 1.120341  [ 1424/ 3200]\n",
      "loss: 1.156496  [ 1440/ 3200]\n",
      "loss: 1.018664  [ 1456/ 3200]\n",
      "loss: 1.086952  [ 1472/ 3200]\n",
      "loss: 1.253170  [ 1488/ 3200]\n",
      "loss: 1.105425  [ 1504/ 3200]\n",
      "loss: 1.186225  [ 1520/ 3200]\n",
      "loss: 1.303571  [ 1536/ 3200]\n",
      "loss: 1.059041  [ 1552/ 3200]\n",
      "loss: 1.163069  [ 1568/ 3200]\n",
      "loss: 1.236805  [ 1584/ 3200]\n",
      "loss: 1.176215  [ 1600/ 3200]\n",
      "loss: 1.086373  [ 1616/ 3200]\n",
      "loss: 1.172109  [ 1632/ 3200]\n",
      "loss: 1.334828  [ 1648/ 3200]\n",
      "loss: 1.210752  [ 1664/ 3200]\n",
      "loss: 1.211625  [ 1680/ 3200]\n",
      "loss: 1.254620  [ 1696/ 3200]\n",
      "loss: 1.186654  [ 1712/ 3200]\n",
      "loss: 1.111351  [ 1728/ 3200]\n",
      "loss: 1.234417  [ 1744/ 3200]\n",
      "loss: 1.262571  [ 1760/ 3200]\n",
      "loss: 1.103938  [ 1776/ 3200]\n",
      "loss: 1.268457  [ 1792/ 3200]\n",
      "loss: 1.171516  [ 1808/ 3200]\n",
      "loss: 1.192462  [ 1824/ 3200]\n",
      "loss: 1.285684  [ 1840/ 3200]\n",
      "loss: 1.115534  [ 1856/ 3200]\n",
      "loss: 1.168793  [ 1872/ 3200]\n",
      "loss: 1.175220  [ 1888/ 3200]\n",
      "loss: 1.157964  [ 1904/ 3200]\n",
      "loss: 1.289548  [ 1920/ 3200]\n",
      "loss: 1.125613  [ 1936/ 3200]\n",
      "loss: 1.169198  [ 1952/ 3200]\n",
      "loss: 1.065220  [ 1968/ 3200]\n",
      "loss: 1.193593  [ 1984/ 3200]\n",
      "loss: 1.171124  [ 2000/ 3200]\n",
      "loss: 1.113132  [ 2016/ 3200]\n",
      "loss: 1.183170  [ 2032/ 3200]\n",
      "loss: 1.200717  [ 2048/ 3200]\n",
      "loss: 1.291351  [ 2064/ 3200]\n",
      "loss: 1.148219  [ 2080/ 3200]\n",
      "loss: 1.181749  [ 2096/ 3200]\n",
      "loss: 1.271739  [ 2112/ 3200]\n",
      "loss: 0.969835  [ 2128/ 3200]\n",
      "loss: 1.102066  [ 2144/ 3200]\n",
      "loss: 1.141311  [ 2160/ 3200]\n",
      "loss: 1.218423  [ 2176/ 3200]\n",
      "loss: 1.179134  [ 2192/ 3200]\n",
      "loss: 1.018988  [ 2208/ 3200]\n",
      "loss: 1.172870  [ 2224/ 3200]\n",
      "loss: 1.189510  [ 2240/ 3200]\n",
      "loss: 1.061142  [ 2256/ 3200]\n",
      "loss: 1.140165  [ 2272/ 3200]\n",
      "loss: 1.078447  [ 2288/ 3200]\n",
      "loss: 1.324882  [ 2304/ 3200]\n",
      "loss: 1.189434  [ 2320/ 3200]\n",
      "loss: 1.212297  [ 2336/ 3200]\n",
      "loss: 1.165019  [ 2352/ 3200]\n",
      "loss: 1.176237  [ 2368/ 3200]\n",
      "loss: 1.156926  [ 2384/ 3200]\n",
      "loss: 1.175558  [ 2400/ 3200]\n",
      "loss: 1.267925  [ 2416/ 3200]\n",
      "loss: 1.237003  [ 2432/ 3200]\n",
      "loss: 1.287941  [ 2448/ 3200]\n",
      "loss: 1.319197  [ 2464/ 3200]\n",
      "loss: 1.251055  [ 2480/ 3200]\n",
      "loss: 1.176452  [ 2496/ 3200]\n",
      "loss: 1.182029  [ 2512/ 3200]\n",
      "loss: 1.200586  [ 2528/ 3200]\n",
      "loss: 1.152723  [ 2544/ 3200]\n",
      "loss: 1.239911  [ 2560/ 3200]\n",
      "loss: 1.178029  [ 2576/ 3200]\n",
      "loss: 1.162442  [ 2592/ 3200]\n",
      "loss: 1.274664  [ 2608/ 3200]\n",
      "loss: 1.265180  [ 2624/ 3200]\n",
      "loss: 1.319844  [ 2640/ 3200]\n",
      "loss: 1.234616  [ 2656/ 3200]\n",
      "loss: 1.122866  [ 2672/ 3200]\n",
      "loss: 1.015360  [ 2688/ 3200]\n",
      "loss: 1.138182  [ 2704/ 3200]\n",
      "loss: 1.102560  [ 2720/ 3200]\n",
      "loss: 1.132928  [ 2736/ 3200]\n",
      "loss: 1.176271  [ 2752/ 3200]\n",
      "loss: 1.172233  [ 2768/ 3200]\n",
      "loss: 1.286864  [ 2784/ 3200]\n",
      "loss: 1.176840  [ 2800/ 3200]\n",
      "loss: 1.088097  [ 2816/ 3200]\n",
      "loss: 1.089431  [ 2832/ 3200]\n",
      "loss: 0.882119  [ 2848/ 3200]\n",
      "loss: 1.167945  [ 2864/ 3200]\n",
      "loss: 1.250749  [ 2880/ 3200]\n",
      "loss: 1.315873  [ 2896/ 3200]\n",
      "loss: 1.158719  [ 2912/ 3200]\n",
      "loss: 1.074712  [ 2928/ 3200]\n",
      "loss: 1.278236  [ 2944/ 3200]\n",
      "loss: 1.240671  [ 2960/ 3200]\n",
      "loss: 1.152745  [ 2976/ 3200]\n",
      "loss: 1.068414  [ 2992/ 3200]\n",
      "loss: 1.240227  [ 3008/ 3200]\n",
      "loss: 1.209963  [ 3024/ 3200]\n",
      "loss: 1.097476  [ 3040/ 3200]\n",
      "loss: 1.134994  [ 3056/ 3200]\n",
      "loss: 1.132342  [ 3072/ 3200]\n",
      "loss: 1.232514  [ 3088/ 3200]\n",
      "loss: 1.030381  [ 3104/ 3200]\n",
      "loss: 1.121947  [ 3120/ 3200]\n",
      "loss: 0.966001  [ 3136/ 3200]\n",
      "loss: 1.114568  [ 3152/ 3200]\n",
      "loss: 1.173329  [ 3168/ 3200]\n",
      "loss: 1.289771  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 1.213681  [    0/ 3200]\n",
      "loss: 1.215668  [   16/ 3200]\n",
      "loss: 1.136191  [   32/ 3200]\n",
      "loss: 1.237292  [   48/ 3200]\n",
      "loss: 1.207456  [   64/ 3200]\n",
      "loss: 1.116961  [   80/ 3200]\n",
      "loss: 1.166368  [   96/ 3200]\n",
      "loss: 1.162314  [  112/ 3200]\n",
      "loss: 1.109397  [  128/ 3200]\n",
      "loss: 1.229003  [  144/ 3200]\n",
      "loss: 1.098865  [  160/ 3200]\n",
      "loss: 1.252530  [  176/ 3200]\n",
      "loss: 1.102746  [  192/ 3200]\n",
      "loss: 1.233637  [  208/ 3200]\n",
      "loss: 1.030299  [  224/ 3200]\n",
      "loss: 1.082962  [  240/ 3200]\n",
      "loss: 1.128934  [  256/ 3200]\n",
      "loss: 1.112416  [  272/ 3200]\n",
      "loss: 1.039705  [  288/ 3200]\n",
      "loss: 1.136372  [  304/ 3200]\n",
      "loss: 1.120957  [  320/ 3200]\n",
      "loss: 1.246425  [  336/ 3200]\n",
      "loss: 1.225589  [  352/ 3200]\n",
      "loss: 1.178212  [  368/ 3200]\n",
      "loss: 1.281879  [  384/ 3200]\n",
      "loss: 1.269817  [  400/ 3200]\n",
      "loss: 1.155205  [  416/ 3200]\n",
      "loss: 1.178635  [  432/ 3200]\n",
      "loss: 1.137534  [  448/ 3200]\n",
      "loss: 1.193440  [  464/ 3200]\n",
      "loss: 1.174412  [  480/ 3200]\n",
      "loss: 1.147184  [  496/ 3200]\n",
      "loss: 1.056978  [  512/ 3200]\n",
      "loss: 0.990140  [  528/ 3200]\n",
      "loss: 1.129969  [  544/ 3200]\n",
      "loss: 1.032253  [  560/ 3200]\n",
      "loss: 1.092236  [  576/ 3200]\n",
      "loss: 1.092323  [  592/ 3200]\n",
      "loss: 1.063227  [  608/ 3200]\n",
      "loss: 1.324200  [  624/ 3200]\n",
      "loss: 1.308533  [  640/ 3200]\n",
      "loss: 1.316124  [  656/ 3200]\n",
      "loss: 1.254856  [  672/ 3200]\n",
      "loss: 1.121993  [  688/ 3200]\n",
      "loss: 1.102643  [  704/ 3200]\n",
      "loss: 1.192468  [  720/ 3200]\n",
      "loss: 1.062811  [  736/ 3200]\n",
      "loss: 1.107985  [  752/ 3200]\n",
      "loss: 1.183261  [  768/ 3200]\n",
      "loss: 1.092611  [  784/ 3200]\n",
      "loss: 1.150152  [  800/ 3200]\n",
      "loss: 1.144468  [  816/ 3200]\n",
      "loss: 1.198001  [  832/ 3200]\n",
      "loss: 1.230937  [  848/ 3200]\n",
      "loss: 1.120783  [  864/ 3200]\n",
      "loss: 0.976597  [  880/ 3200]\n",
      "loss: 1.164083  [  896/ 3200]\n",
      "loss: 1.140486  [  912/ 3200]\n",
      "loss: 1.114875  [  928/ 3200]\n",
      "loss: 1.021214  [  944/ 3200]\n",
      "loss: 1.146999  [  960/ 3200]\n",
      "loss: 1.275655  [  976/ 3200]\n",
      "loss: 1.076871  [  992/ 3200]\n",
      "loss: 1.203948  [ 1008/ 3200]\n",
      "loss: 1.237140  [ 1024/ 3200]\n",
      "loss: 1.060716  [ 1040/ 3200]\n",
      "loss: 1.216868  [ 1056/ 3200]\n",
      "loss: 1.123660  [ 1072/ 3200]\n",
      "loss: 1.273082  [ 1088/ 3200]\n",
      "loss: 1.052791  [ 1104/ 3200]\n",
      "loss: 1.203356  [ 1120/ 3200]\n",
      "loss: 1.175768  [ 1136/ 3200]\n",
      "loss: 1.140420  [ 1152/ 3200]\n",
      "loss: 1.121089  [ 1168/ 3200]\n",
      "loss: 1.082523  [ 1184/ 3200]\n",
      "loss: 1.142642  [ 1200/ 3200]\n",
      "loss: 1.053793  [ 1216/ 3200]\n",
      "loss: 1.229531  [ 1232/ 3200]\n",
      "loss: 1.164296  [ 1248/ 3200]\n",
      "loss: 1.210105  [ 1264/ 3200]\n",
      "loss: 1.124832  [ 1280/ 3200]\n",
      "loss: 1.152842  [ 1296/ 3200]\n",
      "loss: 1.226462  [ 1312/ 3200]\n",
      "loss: 1.054333  [ 1328/ 3200]\n",
      "loss: 1.173740  [ 1344/ 3200]\n",
      "loss: 1.186090  [ 1360/ 3200]\n",
      "loss: 1.083416  [ 1376/ 3200]\n",
      "loss: 1.262376  [ 1392/ 3200]\n",
      "loss: 1.048234  [ 1408/ 3200]\n",
      "loss: 1.122985  [ 1424/ 3200]\n",
      "loss: 1.029677  [ 1440/ 3200]\n",
      "loss: 1.220110  [ 1456/ 3200]\n",
      "loss: 1.027365  [ 1472/ 3200]\n",
      "loss: 1.245641  [ 1488/ 3200]\n",
      "loss: 1.147797  [ 1504/ 3200]\n",
      "loss: 1.060835  [ 1520/ 3200]\n",
      "loss: 1.233143  [ 1536/ 3200]\n",
      "loss: 1.080833  [ 1552/ 3200]\n",
      "loss: 1.121129  [ 1568/ 3200]\n",
      "loss: 1.153260  [ 1584/ 3200]\n",
      "loss: 1.202688  [ 1600/ 3200]\n",
      "loss: 1.208729  [ 1616/ 3200]\n",
      "loss: 1.249357  [ 1632/ 3200]\n",
      "loss: 1.149386  [ 1648/ 3200]\n",
      "loss: 1.073648  [ 1664/ 3200]\n",
      "loss: 1.137678  [ 1680/ 3200]\n",
      "loss: 1.113401  [ 1696/ 3200]\n",
      "loss: 1.038863  [ 1712/ 3200]\n",
      "loss: 1.318447  [ 1728/ 3200]\n",
      "loss: 1.262271  [ 1744/ 3200]\n",
      "loss: 1.150977  [ 1760/ 3200]\n",
      "loss: 1.232560  [ 1776/ 3200]\n",
      "loss: 1.006848  [ 1792/ 3200]\n",
      "loss: 1.088155  [ 1808/ 3200]\n",
      "loss: 1.055259  [ 1824/ 3200]\n",
      "loss: 1.140785  [ 1840/ 3200]\n",
      "loss: 1.241729  [ 1856/ 3200]\n",
      "loss: 1.202448  [ 1872/ 3200]\n",
      "loss: 1.081130  [ 1888/ 3200]\n",
      "loss: 1.179460  [ 1904/ 3200]\n",
      "loss: 1.231169  [ 1920/ 3200]\n",
      "loss: 1.190805  [ 1936/ 3200]\n",
      "loss: 1.211219  [ 1952/ 3200]\n",
      "loss: 1.203069  [ 1968/ 3200]\n",
      "loss: 1.074402  [ 1984/ 3200]\n",
      "loss: 1.188276  [ 2000/ 3200]\n",
      "loss: 1.085592  [ 2016/ 3200]\n",
      "loss: 1.197566  [ 2032/ 3200]\n",
      "loss: 1.089354  [ 2048/ 3200]\n",
      "loss: 1.147028  [ 2064/ 3200]\n",
      "loss: 1.056690  [ 2080/ 3200]\n",
      "loss: 1.277831  [ 2096/ 3200]\n",
      "loss: 1.016638  [ 2112/ 3200]\n",
      "loss: 1.057811  [ 2128/ 3200]\n",
      "loss: 1.104621  [ 2144/ 3200]\n",
      "loss: 1.307718  [ 2160/ 3200]\n",
      "loss: 1.231364  [ 2176/ 3200]\n",
      "loss: 1.161257  [ 2192/ 3200]\n",
      "loss: 1.107173  [ 2208/ 3200]\n",
      "loss: 1.059839  [ 2224/ 3200]\n",
      "loss: 1.131411  [ 2240/ 3200]\n",
      "loss: 1.080538  [ 2256/ 3200]\n",
      "loss: 1.227879  [ 2272/ 3200]\n",
      "loss: 1.206779  [ 2288/ 3200]\n",
      "loss: 1.177596  [ 2304/ 3200]\n",
      "loss: 1.108439  [ 2320/ 3200]\n",
      "loss: 1.026417  [ 2336/ 3200]\n",
      "loss: 1.118216  [ 2352/ 3200]\n",
      "loss: 1.159125  [ 2368/ 3200]\n",
      "loss: 0.977567  [ 2384/ 3200]\n",
      "loss: 1.259523  [ 2400/ 3200]\n",
      "loss: 1.334149  [ 2416/ 3200]\n",
      "loss: 1.165516  [ 2432/ 3200]\n",
      "loss: 1.329141  [ 2448/ 3200]\n",
      "loss: 1.129672  [ 2464/ 3200]\n",
      "loss: 1.170548  [ 2480/ 3200]\n",
      "loss: 1.138010  [ 2496/ 3200]\n",
      "loss: 1.184448  [ 2512/ 3200]\n",
      "loss: 1.125800  [ 2528/ 3200]\n",
      "loss: 1.253424  [ 2544/ 3200]\n",
      "loss: 1.013904  [ 2560/ 3200]\n",
      "loss: 1.109960  [ 2576/ 3200]\n",
      "loss: 1.106875  [ 2592/ 3200]\n",
      "loss: 1.202083  [ 2608/ 3200]\n",
      "loss: 1.112388  [ 2624/ 3200]\n",
      "loss: 1.160938  [ 2640/ 3200]\n",
      "loss: 1.167861  [ 2656/ 3200]\n",
      "loss: 0.887586  [ 2672/ 3200]\n",
      "loss: 1.136296  [ 2688/ 3200]\n",
      "loss: 1.027769  [ 2704/ 3200]\n",
      "loss: 1.289053  [ 2720/ 3200]\n",
      "loss: 1.090376  [ 2736/ 3200]\n",
      "loss: 1.170407  [ 2752/ 3200]\n",
      "loss: 1.122646  [ 2768/ 3200]\n",
      "loss: 1.104346  [ 2784/ 3200]\n",
      "loss: 1.134490  [ 2800/ 3200]\n",
      "loss: 1.011231  [ 2816/ 3200]\n",
      "loss: 1.048422  [ 2832/ 3200]\n",
      "loss: 1.181892  [ 2848/ 3200]\n",
      "loss: 1.179049  [ 2864/ 3200]\n",
      "loss: 1.120616  [ 2880/ 3200]\n",
      "loss: 1.027632  [ 2896/ 3200]\n",
      "loss: 1.069649  [ 2912/ 3200]\n",
      "loss: 1.276989  [ 2928/ 3200]\n",
      "loss: 1.214356  [ 2944/ 3200]\n",
      "loss: 1.276610  [ 2960/ 3200]\n",
      "loss: 1.005038  [ 2976/ 3200]\n",
      "loss: 1.031149  [ 2992/ 3200]\n",
      "loss: 1.183409  [ 3008/ 3200]\n",
      "loss: 1.083610  [ 3024/ 3200]\n",
      "loss: 1.039929  [ 3040/ 3200]\n",
      "loss: 1.094146  [ 3056/ 3200]\n",
      "loss: 1.242549  [ 3072/ 3200]\n",
      "loss: 1.062266  [ 3088/ 3200]\n",
      "loss: 1.124361  [ 3104/ 3200]\n",
      "loss: 1.086405  [ 3120/ 3200]\n",
      "loss: 1.137732  [ 3136/ 3200]\n",
      "loss: 1.044696  [ 3152/ 3200]\n",
      "loss: 1.252259  [ 3168/ 3200]\n",
      "loss: 1.064482  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 1.078824  [    0/ 3200]\n",
      "loss: 1.325313  [   16/ 3200]\n",
      "loss: 1.043011  [   32/ 3200]\n",
      "loss: 1.084222  [   48/ 3200]\n",
      "loss: 1.068593  [   64/ 3200]\n",
      "loss: 1.108470  [   80/ 3200]\n",
      "loss: 1.068328  [   96/ 3200]\n",
      "loss: 1.203544  [  112/ 3200]\n",
      "loss: 1.188699  [  128/ 3200]\n",
      "loss: 1.046280  [  144/ 3200]\n",
      "loss: 1.200974  [  160/ 3200]\n",
      "loss: 1.127193  [  176/ 3200]\n",
      "loss: 1.070657  [  192/ 3200]\n",
      "loss: 1.182677  [  208/ 3200]\n",
      "loss: 1.255024  [  224/ 3200]\n",
      "loss: 1.143689  [  240/ 3200]\n",
      "loss: 1.152893  [  256/ 3200]\n",
      "loss: 1.234884  [  272/ 3200]\n",
      "loss: 1.013969  [  288/ 3200]\n",
      "loss: 1.169788  [  304/ 3200]\n",
      "loss: 1.130976  [  320/ 3200]\n",
      "loss: 1.056496  [  336/ 3200]\n",
      "loss: 1.207092  [  352/ 3200]\n",
      "loss: 1.264582  [  368/ 3200]\n",
      "loss: 1.049193  [  384/ 3200]\n",
      "loss: 1.199554  [  400/ 3200]\n",
      "loss: 1.218472  [  416/ 3200]\n",
      "loss: 1.079917  [  432/ 3200]\n",
      "loss: 1.056495  [  448/ 3200]\n",
      "loss: 0.992061  [  464/ 3200]\n",
      "loss: 1.112414  [  480/ 3200]\n",
      "loss: 1.250728  [  496/ 3200]\n",
      "loss: 1.013512  [  512/ 3200]\n",
      "loss: 1.029203  [  528/ 3200]\n",
      "loss: 1.278581  [  544/ 3200]\n",
      "loss: 1.121907  [  560/ 3200]\n",
      "loss: 1.203657  [  576/ 3200]\n",
      "loss: 1.040471  [  592/ 3200]\n",
      "loss: 1.113963  [  608/ 3200]\n",
      "loss: 1.220469  [  624/ 3200]\n",
      "loss: 1.098576  [  640/ 3200]\n",
      "loss: 1.084695  [  656/ 3200]\n",
      "loss: 1.101781  [  672/ 3200]\n",
      "loss: 1.065446  [  688/ 3200]\n",
      "loss: 1.077891  [  704/ 3200]\n",
      "loss: 1.168385  [  720/ 3200]\n",
      "loss: 1.013691  [  736/ 3200]\n",
      "loss: 1.308815  [  752/ 3200]\n",
      "loss: 1.196870  [  768/ 3200]\n",
      "loss: 1.223138  [  784/ 3200]\n",
      "loss: 1.037298  [  800/ 3200]\n",
      "loss: 1.137056  [  816/ 3200]\n",
      "loss: 1.057041  [  832/ 3200]\n",
      "loss: 1.174870  [  848/ 3200]\n",
      "loss: 1.209954  [  864/ 3200]\n",
      "loss: 1.046403  [  880/ 3200]\n",
      "loss: 1.128624  [  896/ 3200]\n",
      "loss: 0.925107  [  912/ 3200]\n",
      "loss: 1.092249  [  928/ 3200]\n",
      "loss: 1.062914  [  944/ 3200]\n",
      "loss: 1.091226  [  960/ 3200]\n",
      "loss: 1.246247  [  976/ 3200]\n",
      "loss: 1.286574  [  992/ 3200]\n",
      "loss: 0.929059  [ 1008/ 3200]\n",
      "loss: 1.104958  [ 1024/ 3200]\n",
      "loss: 1.056167  [ 1040/ 3200]\n",
      "loss: 1.087680  [ 1056/ 3200]\n",
      "loss: 1.140672  [ 1072/ 3200]\n",
      "loss: 1.079971  [ 1088/ 3200]\n",
      "loss: 1.162289  [ 1104/ 3200]\n",
      "loss: 1.154597  [ 1120/ 3200]\n",
      "loss: 1.206693  [ 1136/ 3200]\n",
      "loss: 1.058490  [ 1152/ 3200]\n",
      "loss: 1.099262  [ 1168/ 3200]\n",
      "loss: 1.112610  [ 1184/ 3200]\n",
      "loss: 1.108854  [ 1200/ 3200]\n",
      "loss: 1.169945  [ 1216/ 3200]\n",
      "loss: 1.192360  [ 1232/ 3200]\n",
      "loss: 1.053593  [ 1248/ 3200]\n",
      "loss: 1.137238  [ 1264/ 3200]\n",
      "loss: 1.074617  [ 1280/ 3200]\n",
      "loss: 0.987274  [ 1296/ 3200]\n",
      "loss: 0.979418  [ 1312/ 3200]\n",
      "loss: 1.169713  [ 1328/ 3200]\n",
      "loss: 1.243346  [ 1344/ 3200]\n",
      "loss: 0.922632  [ 1360/ 3200]\n",
      "loss: 1.063336  [ 1376/ 3200]\n",
      "loss: 1.063290  [ 1392/ 3200]\n",
      "loss: 1.013359  [ 1408/ 3200]\n",
      "loss: 1.116125  [ 1424/ 3200]\n",
      "loss: 1.111110  [ 1440/ 3200]\n",
      "loss: 1.209303  [ 1456/ 3200]\n",
      "loss: 1.176036  [ 1472/ 3200]\n",
      "loss: 0.933351  [ 1488/ 3200]\n",
      "loss: 1.111143  [ 1504/ 3200]\n",
      "loss: 0.954387  [ 1520/ 3200]\n",
      "loss: 1.144747  [ 1536/ 3200]\n",
      "loss: 1.128076  [ 1552/ 3200]\n",
      "loss: 1.041246  [ 1568/ 3200]\n",
      "loss: 1.034382  [ 1584/ 3200]\n",
      "loss: 1.168084  [ 1600/ 3200]\n",
      "loss: 1.182762  [ 1616/ 3200]\n",
      "loss: 1.150423  [ 1632/ 3200]\n",
      "loss: 1.233445  [ 1648/ 3200]\n",
      "loss: 1.235309  [ 1664/ 3200]\n",
      "loss: 1.074024  [ 1680/ 3200]\n",
      "loss: 1.073179  [ 1696/ 3200]\n",
      "loss: 1.175596  [ 1712/ 3200]\n",
      "loss: 1.099288  [ 1728/ 3200]\n",
      "loss: 1.153379  [ 1744/ 3200]\n",
      "loss: 1.045540  [ 1760/ 3200]\n",
      "loss: 1.184429  [ 1776/ 3200]\n",
      "loss: 1.048193  [ 1792/ 3200]\n",
      "loss: 1.204749  [ 1808/ 3200]\n",
      "loss: 1.126679  [ 1824/ 3200]\n",
      "loss: 1.260498  [ 1840/ 3200]\n",
      "loss: 0.946533  [ 1856/ 3200]\n",
      "loss: 1.107746  [ 1872/ 3200]\n",
      "loss: 1.225914  [ 1888/ 3200]\n",
      "loss: 1.131428  [ 1904/ 3200]\n",
      "loss: 1.034140  [ 1920/ 3200]\n",
      "loss: 1.148634  [ 1936/ 3200]\n",
      "loss: 1.062805  [ 1952/ 3200]\n",
      "loss: 1.033684  [ 1968/ 3200]\n",
      "loss: 1.167775  [ 1984/ 3200]\n",
      "loss: 1.090985  [ 2000/ 3200]\n",
      "loss: 1.197339  [ 2016/ 3200]\n",
      "loss: 1.179950  [ 2032/ 3200]\n",
      "loss: 1.086970  [ 2048/ 3200]\n",
      "loss: 0.986322  [ 2064/ 3200]\n",
      "loss: 1.206404  [ 2080/ 3200]\n",
      "loss: 1.193825  [ 2096/ 3200]\n",
      "loss: 1.153573  [ 2112/ 3200]\n",
      "loss: 1.112790  [ 2128/ 3200]\n",
      "loss: 1.177175  [ 2144/ 3200]\n",
      "loss: 1.105321  [ 2160/ 3200]\n",
      "loss: 1.172088  [ 2176/ 3200]\n",
      "loss: 1.008526  [ 2192/ 3200]\n",
      "loss: 1.315880  [ 2208/ 3200]\n",
      "loss: 1.006309  [ 2224/ 3200]\n",
      "loss: 0.983833  [ 2240/ 3200]\n",
      "loss: 1.225250  [ 2256/ 3200]\n",
      "loss: 1.220963  [ 2272/ 3200]\n",
      "loss: 1.289169  [ 2288/ 3200]\n",
      "loss: 1.199748  [ 2304/ 3200]\n",
      "loss: 0.971551  [ 2320/ 3200]\n",
      "loss: 1.170602  [ 2336/ 3200]\n",
      "loss: 1.184665  [ 2352/ 3200]\n",
      "loss: 1.171306  [ 2368/ 3200]\n",
      "loss: 1.252094  [ 2384/ 3200]\n",
      "loss: 1.280261  [ 2400/ 3200]\n",
      "loss: 1.199008  [ 2416/ 3200]\n",
      "loss: 1.180480  [ 2432/ 3200]\n",
      "loss: 0.945729  [ 2448/ 3200]\n",
      "loss: 1.081560  [ 2464/ 3200]\n",
      "loss: 1.209364  [ 2480/ 3200]\n",
      "loss: 1.020866  [ 2496/ 3200]\n",
      "loss: 0.998414  [ 2512/ 3200]\n",
      "loss: 1.155258  [ 2528/ 3200]\n",
      "loss: 1.229248  [ 2544/ 3200]\n",
      "loss: 1.073958  [ 2560/ 3200]\n",
      "loss: 1.056200  [ 2576/ 3200]\n",
      "loss: 1.198733  [ 2592/ 3200]\n",
      "loss: 1.005163  [ 2608/ 3200]\n",
      "loss: 1.196193  [ 2624/ 3200]\n",
      "loss: 1.074495  [ 2640/ 3200]\n",
      "loss: 1.046052  [ 2656/ 3200]\n",
      "loss: 1.051990  [ 2672/ 3200]\n",
      "loss: 1.178369  [ 2688/ 3200]\n",
      "loss: 1.019411  [ 2704/ 3200]\n",
      "loss: 1.117530  [ 2720/ 3200]\n",
      "loss: 1.279679  [ 2736/ 3200]\n",
      "loss: 1.073145  [ 2752/ 3200]\n",
      "loss: 1.077742  [ 2768/ 3200]\n",
      "loss: 1.135229  [ 2784/ 3200]\n",
      "loss: 1.129623  [ 2800/ 3200]\n",
      "loss: 1.053016  [ 2816/ 3200]\n",
      "loss: 1.124647  [ 2832/ 3200]\n",
      "loss: 1.089801  [ 2848/ 3200]\n",
      "loss: 1.118891  [ 2864/ 3200]\n",
      "loss: 0.998639  [ 2880/ 3200]\n",
      "loss: 1.107262  [ 2896/ 3200]\n",
      "loss: 1.146126  [ 2912/ 3200]\n",
      "loss: 1.058853  [ 2928/ 3200]\n",
      "loss: 0.997921  [ 2944/ 3200]\n",
      "loss: 0.996380  [ 2960/ 3200]\n",
      "loss: 1.083466  [ 2976/ 3200]\n",
      "loss: 1.089310  [ 2992/ 3200]\n",
      "loss: 1.107862  [ 3008/ 3200]\n",
      "loss: 1.139539  [ 3024/ 3200]\n",
      "loss: 1.097392  [ 3040/ 3200]\n",
      "loss: 1.184803  [ 3056/ 3200]\n",
      "loss: 1.120757  [ 3072/ 3200]\n",
      "loss: 1.139746  [ 3088/ 3200]\n",
      "loss: 1.197307  [ 3104/ 3200]\n",
      "loss: 1.129639  [ 3120/ 3200]\n",
      "loss: 1.329252  [ 3136/ 3200]\n",
      "loss: 1.145887  [ 3152/ 3200]\n",
      "loss: 1.150114  [ 3168/ 3200]\n",
      "loss: 1.017730  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 1.140970  [    0/ 3200]\n",
      "loss: 1.181130  [   16/ 3200]\n",
      "loss: 1.121704  [   32/ 3200]\n",
      "loss: 1.094281  [   48/ 3200]\n",
      "loss: 1.172561  [   64/ 3200]\n",
      "loss: 1.517496  [   80/ 3200]\n",
      "loss: 1.047271  [   96/ 3200]\n",
      "loss: 1.112978  [  112/ 3200]\n",
      "loss: 1.092964  [  128/ 3200]\n",
      "loss: 1.136375  [  144/ 3200]\n",
      "loss: 1.225030  [  160/ 3200]\n",
      "loss: 1.145431  [  176/ 3200]\n",
      "loss: 1.075280  [  192/ 3200]\n",
      "loss: 1.189642  [  208/ 3200]\n",
      "loss: 1.140197  [  224/ 3200]\n",
      "loss: 0.965711  [  240/ 3200]\n",
      "loss: 1.102892  [  256/ 3200]\n",
      "loss: 1.029150  [  272/ 3200]\n",
      "loss: 1.139083  [  288/ 3200]\n",
      "loss: 1.259603  [  304/ 3200]\n",
      "loss: 0.982477  [  320/ 3200]\n",
      "loss: 1.120750  [  336/ 3200]\n",
      "loss: 0.959528  [  352/ 3200]\n",
      "loss: 1.158552  [  368/ 3200]\n",
      "loss: 0.969408  [  384/ 3200]\n",
      "loss: 1.103960  [  400/ 3200]\n",
      "loss: 1.140179  [  416/ 3200]\n",
      "loss: 1.200515  [  432/ 3200]\n",
      "loss: 1.179914  [  448/ 3200]\n",
      "loss: 1.100148  [  464/ 3200]\n",
      "loss: 1.100911  [  480/ 3200]\n",
      "loss: 1.062679  [  496/ 3200]\n",
      "loss: 1.113258  [  512/ 3200]\n",
      "loss: 1.125820  [  528/ 3200]\n",
      "loss: 1.091196  [  544/ 3200]\n",
      "loss: 1.183967  [  560/ 3200]\n",
      "loss: 1.099848  [  576/ 3200]\n",
      "loss: 1.168099  [  592/ 3200]\n",
      "loss: 0.976925  [  608/ 3200]\n",
      "loss: 1.004550  [  624/ 3200]\n",
      "loss: 1.279195  [  640/ 3200]\n",
      "loss: 0.950116  [  656/ 3200]\n",
      "loss: 1.122267  [  672/ 3200]\n",
      "loss: 0.978979  [  688/ 3200]\n",
      "loss: 1.143019  [  704/ 3200]\n",
      "loss: 1.119411  [  720/ 3200]\n",
      "loss: 1.049808  [  736/ 3200]\n",
      "loss: 1.183245  [  752/ 3200]\n",
      "loss: 0.999360  [  768/ 3200]\n",
      "loss: 1.064677  [  784/ 3200]\n",
      "loss: 1.022188  [  800/ 3200]\n",
      "loss: 1.061748  [  816/ 3200]\n",
      "loss: 1.001729  [  832/ 3200]\n",
      "loss: 1.064331  [  848/ 3200]\n",
      "loss: 1.163462  [  864/ 3200]\n",
      "loss: 1.055601  [  880/ 3200]\n",
      "loss: 1.213546  [  896/ 3200]\n",
      "loss: 1.290971  [  912/ 3200]\n",
      "loss: 1.053187  [  928/ 3200]\n",
      "loss: 1.058825  [  944/ 3200]\n",
      "loss: 1.060871  [  960/ 3200]\n",
      "loss: 0.951854  [  976/ 3200]\n",
      "loss: 1.238649  [  992/ 3200]\n",
      "loss: 1.070458  [ 1008/ 3200]\n",
      "loss: 0.920812  [ 1024/ 3200]\n",
      "loss: 0.976941  [ 1040/ 3200]\n",
      "loss: 1.055031  [ 1056/ 3200]\n",
      "loss: 1.078065  [ 1072/ 3200]\n",
      "loss: 1.053672  [ 1088/ 3200]\n",
      "loss: 0.984998  [ 1104/ 3200]\n",
      "loss: 1.172358  [ 1120/ 3200]\n",
      "loss: 1.060003  [ 1136/ 3200]\n",
      "loss: 0.964645  [ 1152/ 3200]\n",
      "loss: 1.391379  [ 1168/ 3200]\n",
      "loss: 1.246777  [ 1184/ 3200]\n",
      "loss: 1.060998  [ 1200/ 3200]\n",
      "loss: 0.967393  [ 1216/ 3200]\n",
      "loss: 0.992193  [ 1232/ 3200]\n",
      "loss: 0.998748  [ 1248/ 3200]\n",
      "loss: 1.167241  [ 1264/ 3200]\n",
      "loss: 1.096319  [ 1280/ 3200]\n",
      "loss: 1.068914  [ 1296/ 3200]\n",
      "loss: 1.068999  [ 1312/ 3200]\n",
      "loss: 1.093049  [ 1328/ 3200]\n",
      "loss: 0.976209  [ 1344/ 3200]\n",
      "loss: 1.044330  [ 1360/ 3200]\n",
      "loss: 1.234317  [ 1376/ 3200]\n",
      "loss: 1.220904  [ 1392/ 3200]\n",
      "loss: 1.146524  [ 1408/ 3200]\n",
      "loss: 1.125982  [ 1424/ 3200]\n",
      "loss: 1.020464  [ 1440/ 3200]\n",
      "loss: 1.136011  [ 1456/ 3200]\n",
      "loss: 1.152111  [ 1472/ 3200]\n",
      "loss: 0.881097  [ 1488/ 3200]\n",
      "loss: 1.005058  [ 1504/ 3200]\n",
      "loss: 1.135151  [ 1520/ 3200]\n",
      "loss: 1.212631  [ 1536/ 3200]\n",
      "loss: 1.131192  [ 1552/ 3200]\n",
      "loss: 1.055662  [ 1568/ 3200]\n",
      "loss: 1.067238  [ 1584/ 3200]\n",
      "loss: 1.149002  [ 1600/ 3200]\n",
      "loss: 1.059409  [ 1616/ 3200]\n",
      "loss: 1.263985  [ 1632/ 3200]\n",
      "loss: 1.070259  [ 1648/ 3200]\n",
      "loss: 1.167073  [ 1664/ 3200]\n",
      "loss: 0.967062  [ 1680/ 3200]\n",
      "loss: 1.301053  [ 1696/ 3200]\n",
      "loss: 1.089660  [ 1712/ 3200]\n",
      "loss: 1.042262  [ 1728/ 3200]\n",
      "loss: 1.083164  [ 1744/ 3200]\n",
      "loss: 1.079843  [ 1760/ 3200]\n",
      "loss: 1.308278  [ 1776/ 3200]\n",
      "loss: 1.163196  [ 1792/ 3200]\n",
      "loss: 0.982273  [ 1808/ 3200]\n",
      "loss: 1.105015  [ 1824/ 3200]\n",
      "loss: 1.048874  [ 1840/ 3200]\n",
      "loss: 0.829937  [ 1856/ 3200]\n",
      "loss: 1.292609  [ 1872/ 3200]\n",
      "loss: 1.113652  [ 1888/ 3200]\n",
      "loss: 1.057327  [ 1904/ 3200]\n",
      "loss: 0.922657  [ 1920/ 3200]\n",
      "loss: 1.088395  [ 1936/ 3200]\n",
      "loss: 1.175401  [ 1952/ 3200]\n",
      "loss: 0.962091  [ 1968/ 3200]\n",
      "loss: 1.159489  [ 1984/ 3200]\n",
      "loss: 1.062557  [ 2000/ 3200]\n",
      "loss: 1.153261  [ 2016/ 3200]\n",
      "loss: 1.038326  [ 2032/ 3200]\n",
      "loss: 1.150031  [ 2048/ 3200]\n",
      "loss: 1.071785  [ 2064/ 3200]\n",
      "loss: 1.031459  [ 2080/ 3200]\n",
      "loss: 1.038132  [ 2096/ 3200]\n",
      "loss: 1.160507  [ 2112/ 3200]\n",
      "loss: 1.227751  [ 2128/ 3200]\n",
      "loss: 1.145049  [ 2144/ 3200]\n",
      "loss: 0.903759  [ 2160/ 3200]\n",
      "loss: 1.028642  [ 2176/ 3200]\n",
      "loss: 1.214369  [ 2192/ 3200]\n",
      "loss: 1.021842  [ 2208/ 3200]\n",
      "loss: 1.164268  [ 2224/ 3200]\n",
      "loss: 1.239434  [ 2240/ 3200]\n",
      "loss: 1.209091  [ 2256/ 3200]\n",
      "loss: 1.068710  [ 2272/ 3200]\n",
      "loss: 1.117897  [ 2288/ 3200]\n",
      "loss: 1.048327  [ 2304/ 3200]\n",
      "loss: 1.018052  [ 2320/ 3200]\n",
      "loss: 1.240418  [ 2336/ 3200]\n",
      "loss: 1.175012  [ 2352/ 3200]\n",
      "loss: 1.156512  [ 2368/ 3200]\n",
      "loss: 1.058870  [ 2384/ 3200]\n",
      "loss: 1.104955  [ 2400/ 3200]\n",
      "loss: 1.062127  [ 2416/ 3200]\n",
      "loss: 0.929837  [ 2432/ 3200]\n",
      "loss: 1.125639  [ 2448/ 3200]\n",
      "loss: 1.030568  [ 2464/ 3200]\n",
      "loss: 1.078130  [ 2480/ 3200]\n",
      "loss: 0.993540  [ 2496/ 3200]\n",
      "loss: 1.170004  [ 2512/ 3200]\n",
      "loss: 1.141656  [ 2528/ 3200]\n",
      "loss: 1.053725  [ 2544/ 3200]\n",
      "loss: 1.127773  [ 2560/ 3200]\n",
      "loss: 1.081276  [ 2576/ 3200]\n",
      "loss: 1.166329  [ 2592/ 3200]\n",
      "loss: 1.058314  [ 2608/ 3200]\n",
      "loss: 1.113724  [ 2624/ 3200]\n",
      "loss: 1.095158  [ 2640/ 3200]\n",
      "loss: 1.086371  [ 2656/ 3200]\n",
      "loss: 1.119260  [ 2672/ 3200]\n",
      "loss: 1.146300  [ 2688/ 3200]\n",
      "loss: 1.014206  [ 2704/ 3200]\n",
      "loss: 1.057492  [ 2720/ 3200]\n",
      "loss: 0.892132  [ 2736/ 3200]\n",
      "loss: 1.166316  [ 2752/ 3200]\n",
      "loss: 1.025068  [ 2768/ 3200]\n",
      "loss: 0.882077  [ 2784/ 3200]\n",
      "loss: 1.203541  [ 2800/ 3200]\n",
      "loss: 0.960427  [ 2816/ 3200]\n",
      "loss: 1.039446  [ 2832/ 3200]\n",
      "loss: 1.135651  [ 2848/ 3200]\n",
      "loss: 1.026511  [ 2864/ 3200]\n",
      "loss: 1.097352  [ 2880/ 3200]\n",
      "loss: 1.063363  [ 2896/ 3200]\n",
      "loss: 1.375687  [ 2912/ 3200]\n",
      "loss: 1.266315  [ 2928/ 3200]\n",
      "loss: 1.384044  [ 2944/ 3200]\n",
      "loss: 1.233298  [ 2960/ 3200]\n",
      "loss: 1.052992  [ 2976/ 3200]\n",
      "loss: 1.034895  [ 2992/ 3200]\n",
      "loss: 1.130708  [ 3008/ 3200]\n",
      "loss: 1.101365  [ 3024/ 3200]\n",
      "loss: 1.261189  [ 3040/ 3200]\n",
      "loss: 0.994291  [ 3056/ 3200]\n",
      "loss: 1.129363  [ 3072/ 3200]\n",
      "loss: 1.035051  [ 3088/ 3200]\n",
      "loss: 1.243750  [ 3104/ 3200]\n",
      "loss: 1.092256  [ 3120/ 3200]\n",
      "loss: 1.186624  [ 3136/ 3200]\n",
      "loss: 1.066251  [ 3152/ 3200]\n",
      "loss: 1.212657  [ 3168/ 3200]\n",
      "loss: 1.043934  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 1.107272  [    0/ 3200]\n",
      "loss: 1.126410  [   16/ 3200]\n",
      "loss: 1.021617  [   32/ 3200]\n",
      "loss: 1.122011  [   48/ 3200]\n",
      "loss: 0.962755  [   64/ 3200]\n",
      "loss: 1.165156  [   80/ 3200]\n",
      "loss: 1.112585  [   96/ 3200]\n",
      "loss: 1.136145  [  112/ 3200]\n",
      "loss: 1.035827  [  128/ 3200]\n",
      "loss: 0.985339  [  144/ 3200]\n",
      "loss: 1.095273  [  160/ 3200]\n",
      "loss: 1.180034  [  176/ 3200]\n",
      "loss: 1.120333  [  192/ 3200]\n",
      "loss: 1.033275  [  208/ 3200]\n",
      "loss: 1.150810  [  224/ 3200]\n",
      "loss: 1.093985  [  240/ 3200]\n",
      "loss: 1.044720  [  256/ 3200]\n",
      "loss: 1.174203  [  272/ 3200]\n",
      "loss: 1.148265  [  288/ 3200]\n",
      "loss: 1.053676  [  304/ 3200]\n",
      "loss: 1.036303  [  320/ 3200]\n",
      "loss: 1.091898  [  336/ 3200]\n",
      "loss: 1.043830  [  352/ 3200]\n",
      "loss: 1.199598  [  368/ 3200]\n",
      "loss: 1.080267  [  384/ 3200]\n",
      "loss: 1.036541  [  400/ 3200]\n",
      "loss: 1.167129  [  416/ 3200]\n",
      "loss: 1.084675  [  432/ 3200]\n",
      "loss: 1.085572  [  448/ 3200]\n",
      "loss: 1.043903  [  464/ 3200]\n",
      "loss: 0.953984  [  480/ 3200]\n",
      "loss: 1.120201  [  496/ 3200]\n",
      "loss: 1.098110  [  512/ 3200]\n",
      "loss: 1.099051  [  528/ 3200]\n",
      "loss: 1.059646  [  544/ 3200]\n",
      "loss: 1.086699  [  560/ 3200]\n",
      "loss: 1.097376  [  576/ 3200]\n",
      "loss: 1.154571  [  592/ 3200]\n",
      "loss: 1.223318  [  608/ 3200]\n",
      "loss: 0.961126  [  624/ 3200]\n",
      "loss: 1.447220  [  640/ 3200]\n",
      "loss: 1.246684  [  656/ 3200]\n",
      "loss: 1.023223  [  672/ 3200]\n",
      "loss: 1.050391  [  688/ 3200]\n",
      "loss: 1.087984  [  704/ 3200]\n",
      "loss: 1.036969  [  720/ 3200]\n",
      "loss: 1.031562  [  736/ 3200]\n",
      "loss: 1.286001  [  752/ 3200]\n",
      "loss: 1.004856  [  768/ 3200]\n",
      "loss: 1.065764  [  784/ 3200]\n",
      "loss: 0.992452  [  800/ 3200]\n",
      "loss: 0.908260  [  816/ 3200]\n",
      "loss: 1.177349  [  832/ 3200]\n",
      "loss: 1.072513  [  848/ 3200]\n",
      "loss: 0.985893  [  864/ 3200]\n",
      "loss: 1.098358  [  880/ 3200]\n",
      "loss: 1.146289  [  896/ 3200]\n",
      "loss: 1.126608  [  912/ 3200]\n",
      "loss: 1.038960  [  928/ 3200]\n",
      "loss: 1.257132  [  944/ 3200]\n",
      "loss: 1.100278  [  960/ 3200]\n",
      "loss: 0.869406  [  976/ 3200]\n",
      "loss: 1.197733  [  992/ 3200]\n",
      "loss: 1.095899  [ 1008/ 3200]\n",
      "loss: 1.135982  [ 1024/ 3200]\n",
      "loss: 1.060028  [ 1040/ 3200]\n",
      "loss: 0.958308  [ 1056/ 3200]\n",
      "loss: 0.884724  [ 1072/ 3200]\n",
      "loss: 0.927618  [ 1088/ 3200]\n",
      "loss: 1.082943  [ 1104/ 3200]\n",
      "loss: 1.208454  [ 1120/ 3200]\n",
      "loss: 1.173777  [ 1136/ 3200]\n",
      "loss: 1.096733  [ 1152/ 3200]\n",
      "loss: 0.984470  [ 1168/ 3200]\n",
      "loss: 1.021043  [ 1184/ 3200]\n",
      "loss: 1.177390  [ 1200/ 3200]\n",
      "loss: 1.087507  [ 1216/ 3200]\n",
      "loss: 1.072756  [ 1232/ 3200]\n",
      "loss: 1.140510  [ 1248/ 3200]\n",
      "loss: 1.233178  [ 1264/ 3200]\n",
      "loss: 0.946088  [ 1280/ 3200]\n",
      "loss: 1.045479  [ 1296/ 3200]\n",
      "loss: 1.190879  [ 1312/ 3200]\n",
      "loss: 1.066579  [ 1328/ 3200]\n",
      "loss: 0.907337  [ 1344/ 3200]\n",
      "loss: 1.006644  [ 1360/ 3200]\n",
      "loss: 1.101246  [ 1376/ 3200]\n",
      "loss: 0.984719  [ 1392/ 3200]\n",
      "loss: 1.168761  [ 1408/ 3200]\n",
      "loss: 1.030999  [ 1424/ 3200]\n",
      "loss: 1.179374  [ 1440/ 3200]\n",
      "loss: 1.275539  [ 1456/ 3200]\n",
      "loss: 0.982777  [ 1472/ 3200]\n",
      "loss: 1.058704  [ 1488/ 3200]\n",
      "loss: 1.071908  [ 1504/ 3200]\n",
      "loss: 0.817027  [ 1520/ 3200]\n",
      "loss: 1.131614  [ 1536/ 3200]\n",
      "loss: 0.955920  [ 1552/ 3200]\n",
      "loss: 0.983302  [ 1568/ 3200]\n",
      "loss: 1.121550  [ 1584/ 3200]\n",
      "loss: 1.318778  [ 1600/ 3200]\n",
      "loss: 1.282745  [ 1616/ 3200]\n",
      "loss: 0.999485  [ 1632/ 3200]\n",
      "loss: 1.060699  [ 1648/ 3200]\n",
      "loss: 0.990444  [ 1664/ 3200]\n",
      "loss: 1.035970  [ 1680/ 3200]\n",
      "loss: 1.150401  [ 1696/ 3200]\n",
      "loss: 0.886212  [ 1712/ 3200]\n",
      "loss: 1.162693  [ 1728/ 3200]\n",
      "loss: 1.121229  [ 1744/ 3200]\n",
      "loss: 1.019304  [ 1760/ 3200]\n",
      "loss: 1.084697  [ 1776/ 3200]\n",
      "loss: 1.117632  [ 1792/ 3200]\n",
      "loss: 0.904220  [ 1808/ 3200]\n",
      "loss: 0.957525  [ 1824/ 3200]\n",
      "loss: 1.017415  [ 1840/ 3200]\n",
      "loss: 1.215283  [ 1856/ 3200]\n",
      "loss: 0.956474  [ 1872/ 3200]\n",
      "loss: 1.134712  [ 1888/ 3200]\n",
      "loss: 0.966130  [ 1904/ 3200]\n",
      "loss: 0.969955  [ 1920/ 3200]\n",
      "loss: 1.097754  [ 1936/ 3200]\n",
      "loss: 1.176730  [ 1952/ 3200]\n",
      "loss: 1.037214  [ 1968/ 3200]\n",
      "loss: 1.056510  [ 1984/ 3200]\n",
      "loss: 1.110457  [ 2000/ 3200]\n",
      "loss: 0.982077  [ 2016/ 3200]\n",
      "loss: 0.948403  [ 2032/ 3200]\n",
      "loss: 1.120861  [ 2048/ 3200]\n",
      "loss: 1.047150  [ 2064/ 3200]\n",
      "loss: 1.229444  [ 2080/ 3200]\n",
      "loss: 1.044186  [ 2096/ 3200]\n",
      "loss: 1.242975  [ 2112/ 3200]\n",
      "loss: 0.961397  [ 2128/ 3200]\n",
      "loss: 0.981107  [ 2144/ 3200]\n",
      "loss: 1.008773  [ 2160/ 3200]\n",
      "loss: 1.091158  [ 2176/ 3200]\n",
      "loss: 1.142367  [ 2192/ 3200]\n",
      "loss: 1.080836  [ 2208/ 3200]\n",
      "loss: 1.307320  [ 2224/ 3200]\n",
      "loss: 1.151912  [ 2240/ 3200]\n",
      "loss: 0.817857  [ 2256/ 3200]\n",
      "loss: 0.955058  [ 2272/ 3200]\n",
      "loss: 1.055450  [ 2288/ 3200]\n",
      "loss: 1.001570  [ 2304/ 3200]\n",
      "loss: 0.964239  [ 2320/ 3200]\n",
      "loss: 1.092704  [ 2336/ 3200]\n",
      "loss: 1.240804  [ 2352/ 3200]\n",
      "loss: 0.992987  [ 2368/ 3200]\n",
      "loss: 1.063905  [ 2384/ 3200]\n",
      "loss: 1.009868  [ 2400/ 3200]\n",
      "loss: 1.223361  [ 2416/ 3200]\n",
      "loss: 1.061216  [ 2432/ 3200]\n",
      "loss: 1.138246  [ 2448/ 3200]\n",
      "loss: 0.896558  [ 2464/ 3200]\n",
      "loss: 1.027733  [ 2480/ 3200]\n",
      "loss: 1.191270  [ 2496/ 3200]\n",
      "loss: 0.814904  [ 2512/ 3200]\n",
      "loss: 0.895953  [ 2528/ 3200]\n",
      "loss: 0.842633  [ 2544/ 3200]\n",
      "loss: 1.221007  [ 2560/ 3200]\n",
      "loss: 1.387383  [ 2576/ 3200]\n",
      "loss: 1.243283  [ 2592/ 3200]\n",
      "loss: 1.127730  [ 2608/ 3200]\n",
      "loss: 0.936574  [ 2624/ 3200]\n",
      "loss: 1.106106  [ 2640/ 3200]\n",
      "loss: 0.996228  [ 2656/ 3200]\n",
      "loss: 1.153483  [ 2672/ 3200]\n",
      "loss: 1.078623  [ 2688/ 3200]\n",
      "loss: 1.157738  [ 2704/ 3200]\n",
      "loss: 1.067646  [ 2720/ 3200]\n",
      "loss: 1.053560  [ 2736/ 3200]\n",
      "loss: 1.032054  [ 2752/ 3200]\n",
      "loss: 1.156413  [ 2768/ 3200]\n",
      "loss: 0.925356  [ 2784/ 3200]\n",
      "loss: 1.046070  [ 2800/ 3200]\n",
      "loss: 1.105260  [ 2816/ 3200]\n",
      "loss: 1.213405  [ 2832/ 3200]\n",
      "loss: 0.978879  [ 2848/ 3200]\n",
      "loss: 1.185178  [ 2864/ 3200]\n",
      "loss: 1.120073  [ 2880/ 3200]\n",
      "loss: 1.188438  [ 2896/ 3200]\n",
      "loss: 1.179053  [ 2912/ 3200]\n",
      "loss: 1.009224  [ 2928/ 3200]\n",
      "loss: 1.164689  [ 2944/ 3200]\n",
      "loss: 1.129196  [ 2960/ 3200]\n",
      "loss: 1.005801  [ 2976/ 3200]\n",
      "loss: 0.939706  [ 2992/ 3200]\n",
      "loss: 1.017437  [ 3008/ 3200]\n",
      "loss: 1.038043  [ 3024/ 3200]\n",
      "loss: 1.267828  [ 3040/ 3200]\n",
      "loss: 1.072685  [ 3056/ 3200]\n",
      "loss: 0.951646  [ 3072/ 3200]\n",
      "loss: 1.026210  [ 3088/ 3200]\n",
      "loss: 1.159959  [ 3104/ 3200]\n",
      "loss: 1.189486  [ 3120/ 3200]\n",
      "loss: 1.119229  [ 3136/ 3200]\n",
      "loss: 1.203952  [ 3152/ 3200]\n",
      "loss: 1.207686  [ 3168/ 3200]\n",
      "loss: 0.911239  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 0.965784  [    0/ 3200]\n",
      "loss: 0.912613  [   16/ 3200]\n",
      "loss: 1.205539  [   32/ 3200]\n",
      "loss: 1.014211  [   48/ 3200]\n",
      "loss: 1.103594  [   64/ 3200]\n",
      "loss: 0.986337  [   80/ 3200]\n",
      "loss: 1.062831  [   96/ 3200]\n",
      "loss: 1.129083  [  112/ 3200]\n",
      "loss: 1.029987  [  128/ 3200]\n",
      "loss: 1.049479  [  144/ 3200]\n",
      "loss: 1.109478  [  160/ 3200]\n",
      "loss: 1.163334  [  176/ 3200]\n",
      "loss: 0.937300  [  192/ 3200]\n",
      "loss: 1.170491  [  208/ 3200]\n",
      "loss: 0.981086  [  224/ 3200]\n",
      "loss: 1.065817  [  240/ 3200]\n",
      "loss: 1.254820  [  256/ 3200]\n",
      "loss: 0.787405  [  272/ 3200]\n",
      "loss: 1.098283  [  288/ 3200]\n",
      "loss: 0.918325  [  304/ 3200]\n",
      "loss: 1.072217  [  320/ 3200]\n",
      "loss: 1.005716  [  336/ 3200]\n",
      "loss: 1.000956  [  352/ 3200]\n",
      "loss: 1.134654  [  368/ 3200]\n",
      "loss: 0.982630  [  384/ 3200]\n",
      "loss: 1.010653  [  400/ 3200]\n",
      "loss: 1.243703  [  416/ 3200]\n",
      "loss: 1.163895  [  432/ 3200]\n",
      "loss: 1.030131  [  448/ 3200]\n",
      "loss: 1.075488  [  464/ 3200]\n",
      "loss: 1.001101  [  480/ 3200]\n",
      "loss: 1.051110  [  496/ 3200]\n",
      "loss: 1.117633  [  512/ 3200]\n",
      "loss: 0.969350  [  528/ 3200]\n",
      "loss: 0.930791  [  544/ 3200]\n",
      "loss: 0.964951  [  560/ 3200]\n",
      "loss: 1.039032  [  576/ 3200]\n",
      "loss: 1.050938  [  592/ 3200]\n",
      "loss: 0.881828  [  608/ 3200]\n",
      "loss: 1.076606  [  624/ 3200]\n",
      "loss: 1.028045  [  640/ 3200]\n",
      "loss: 1.051725  [  656/ 3200]\n",
      "loss: 1.185491  [  672/ 3200]\n",
      "loss: 1.042968  [  688/ 3200]\n",
      "loss: 1.098592  [  704/ 3200]\n",
      "loss: 0.873585  [  720/ 3200]\n",
      "loss: 1.226373  [  736/ 3200]\n",
      "loss: 0.995399  [  752/ 3200]\n",
      "loss: 1.257918  [  768/ 3200]\n",
      "loss: 1.000689  [  784/ 3200]\n",
      "loss: 0.991639  [  800/ 3200]\n",
      "loss: 1.147060  [  816/ 3200]\n",
      "loss: 0.871754  [  832/ 3200]\n",
      "loss: 1.092806  [  848/ 3200]\n",
      "loss: 0.963092  [  864/ 3200]\n",
      "loss: 1.138070  [  880/ 3200]\n",
      "loss: 1.040102  [  896/ 3200]\n",
      "loss: 0.932620  [  912/ 3200]\n",
      "loss: 1.355933  [  928/ 3200]\n",
      "loss: 0.909851  [  944/ 3200]\n",
      "loss: 1.124307  [  960/ 3200]\n",
      "loss: 1.023753  [  976/ 3200]\n",
      "loss: 0.955927  [  992/ 3200]\n",
      "loss: 1.059026  [ 1008/ 3200]\n",
      "loss: 1.055546  [ 1024/ 3200]\n",
      "loss: 1.261440  [ 1040/ 3200]\n",
      "loss: 1.025135  [ 1056/ 3200]\n",
      "loss: 0.909345  [ 1072/ 3200]\n",
      "loss: 1.256598  [ 1088/ 3200]\n",
      "loss: 1.181024  [ 1104/ 3200]\n",
      "loss: 1.098384  [ 1120/ 3200]\n",
      "loss: 1.129256  [ 1136/ 3200]\n",
      "loss: 1.219219  [ 1152/ 3200]\n",
      "loss: 1.003993  [ 1168/ 3200]\n",
      "loss: 0.950753  [ 1184/ 3200]\n",
      "loss: 1.109816  [ 1200/ 3200]\n",
      "loss: 1.109940  [ 1216/ 3200]\n",
      "loss: 0.919566  [ 1232/ 3200]\n",
      "loss: 1.135397  [ 1248/ 3200]\n",
      "loss: 1.005008  [ 1264/ 3200]\n",
      "loss: 1.064848  [ 1280/ 3200]\n",
      "loss: 1.133334  [ 1296/ 3200]\n",
      "loss: 0.876236  [ 1312/ 3200]\n",
      "loss: 1.077011  [ 1328/ 3200]\n",
      "loss: 1.053448  [ 1344/ 3200]\n",
      "loss: 0.958512  [ 1360/ 3200]\n",
      "loss: 0.858203  [ 1376/ 3200]\n",
      "loss: 1.228763  [ 1392/ 3200]\n",
      "loss: 1.120288  [ 1408/ 3200]\n",
      "loss: 1.137622  [ 1424/ 3200]\n",
      "loss: 0.871323  [ 1440/ 3200]\n",
      "loss: 1.119323  [ 1456/ 3200]\n",
      "loss: 1.017777  [ 1472/ 3200]\n",
      "loss: 1.076093  [ 1488/ 3200]\n",
      "loss: 1.166224  [ 1504/ 3200]\n",
      "loss: 1.036330  [ 1520/ 3200]\n",
      "loss: 1.097775  [ 1536/ 3200]\n",
      "loss: 1.106367  [ 1552/ 3200]\n",
      "loss: 1.207863  [ 1568/ 3200]\n",
      "loss: 1.172425  [ 1584/ 3200]\n",
      "loss: 1.093228  [ 1600/ 3200]\n",
      "loss: 0.944280  [ 1616/ 3200]\n",
      "loss: 1.305514  [ 1632/ 3200]\n",
      "loss: 0.935673  [ 1648/ 3200]\n",
      "loss: 1.202229  [ 1664/ 3200]\n",
      "loss: 0.974374  [ 1680/ 3200]\n",
      "loss: 0.999739  [ 1696/ 3200]\n",
      "loss: 1.136145  [ 1712/ 3200]\n",
      "loss: 1.085701  [ 1728/ 3200]\n",
      "loss: 1.190930  [ 1744/ 3200]\n",
      "loss: 1.212269  [ 1760/ 3200]\n",
      "loss: 0.779862  [ 1776/ 3200]\n",
      "loss: 1.145281  [ 1792/ 3200]\n",
      "loss: 1.029798  [ 1808/ 3200]\n",
      "loss: 0.882417  [ 1824/ 3200]\n",
      "loss: 1.113633  [ 1840/ 3200]\n",
      "loss: 1.021817  [ 1856/ 3200]\n",
      "loss: 0.922009  [ 1872/ 3200]\n",
      "loss: 1.059471  [ 1888/ 3200]\n",
      "loss: 1.073618  [ 1904/ 3200]\n",
      "loss: 0.903345  [ 1920/ 3200]\n",
      "loss: 1.129091  [ 1936/ 3200]\n",
      "loss: 1.011505  [ 1952/ 3200]\n",
      "loss: 1.127009  [ 1968/ 3200]\n",
      "loss: 1.109752  [ 1984/ 3200]\n",
      "loss: 0.994796  [ 2000/ 3200]\n",
      "loss: 1.081299  [ 2016/ 3200]\n",
      "loss: 1.166039  [ 2032/ 3200]\n",
      "loss: 0.907444  [ 2048/ 3200]\n",
      "loss: 1.135478  [ 2064/ 3200]\n",
      "loss: 1.178899  [ 2080/ 3200]\n",
      "loss: 1.176308  [ 2096/ 3200]\n",
      "loss: 0.975013  [ 2112/ 3200]\n",
      "loss: 0.803432  [ 2128/ 3200]\n",
      "loss: 1.042036  [ 2144/ 3200]\n",
      "loss: 0.874944  [ 2160/ 3200]\n",
      "loss: 1.390590  [ 2176/ 3200]\n",
      "loss: 1.019791  [ 2192/ 3200]\n",
      "loss: 0.978260  [ 2208/ 3200]\n",
      "loss: 1.041837  [ 2224/ 3200]\n",
      "loss: 0.930096  [ 2240/ 3200]\n",
      "loss: 1.205153  [ 2256/ 3200]\n",
      "loss: 1.043237  [ 2272/ 3200]\n",
      "loss: 1.123302  [ 2288/ 3200]\n",
      "loss: 1.265690  [ 2304/ 3200]\n",
      "loss: 1.035368  [ 2320/ 3200]\n",
      "loss: 1.050526  [ 2336/ 3200]\n",
      "loss: 1.177634  [ 2352/ 3200]\n",
      "loss: 0.981521  [ 2368/ 3200]\n",
      "loss: 1.083200  [ 2384/ 3200]\n",
      "loss: 1.228770  [ 2400/ 3200]\n",
      "loss: 1.129494  [ 2416/ 3200]\n",
      "loss: 1.207393  [ 2432/ 3200]\n",
      "loss: 0.960363  [ 2448/ 3200]\n",
      "loss: 1.016127  [ 2464/ 3200]\n",
      "loss: 1.063242  [ 2480/ 3200]\n",
      "loss: 1.155337  [ 2496/ 3200]\n",
      "loss: 1.087698  [ 2512/ 3200]\n",
      "loss: 0.937528  [ 2528/ 3200]\n",
      "loss: 1.029234  [ 2544/ 3200]\n",
      "loss: 1.158534  [ 2560/ 3200]\n",
      "loss: 1.096657  [ 2576/ 3200]\n",
      "loss: 1.114874  [ 2592/ 3200]\n",
      "loss: 0.964068  [ 2608/ 3200]\n",
      "loss: 1.209156  [ 2624/ 3200]\n",
      "loss: 1.138141  [ 2640/ 3200]\n",
      "loss: 1.100337  [ 2656/ 3200]\n",
      "loss: 1.037464  [ 2672/ 3200]\n",
      "loss: 1.120785  [ 2688/ 3200]\n",
      "loss: 1.017462  [ 2704/ 3200]\n",
      "loss: 0.923210  [ 2720/ 3200]\n",
      "loss: 1.009803  [ 2736/ 3200]\n",
      "loss: 0.960330  [ 2752/ 3200]\n",
      "loss: 1.142480  [ 2768/ 3200]\n",
      "loss: 0.824066  [ 2784/ 3200]\n",
      "loss: 1.086762  [ 2800/ 3200]\n",
      "loss: 1.201037  [ 2816/ 3200]\n",
      "loss: 1.130046  [ 2832/ 3200]\n",
      "loss: 0.886952  [ 2848/ 3200]\n",
      "loss: 1.039030  [ 2864/ 3200]\n",
      "loss: 1.090509  [ 2880/ 3200]\n",
      "loss: 1.038522  [ 2896/ 3200]\n",
      "loss: 1.049759  [ 2912/ 3200]\n",
      "loss: 1.051536  [ 2928/ 3200]\n",
      "loss: 1.244555  [ 2944/ 3200]\n",
      "loss: 1.153801  [ 2960/ 3200]\n",
      "loss: 0.918081  [ 2976/ 3200]\n",
      "loss: 1.189577  [ 2992/ 3200]\n",
      "loss: 1.093919  [ 3008/ 3200]\n",
      "loss: 1.034716  [ 3024/ 3200]\n",
      "loss: 0.869564  [ 3040/ 3200]\n",
      "loss: 0.967327  [ 3056/ 3200]\n",
      "loss: 0.849423  [ 3072/ 3200]\n",
      "loss: 1.067281  [ 3088/ 3200]\n",
      "loss: 1.127802  [ 3104/ 3200]\n",
      "loss: 1.188508  [ 3120/ 3200]\n",
      "loss: 1.177422  [ 3136/ 3200]\n",
      "loss: 1.148618  [ 3152/ 3200]\n",
      "loss: 1.078889  [ 3168/ 3200]\n",
      "loss: 1.172891  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 1.018812  [    0/ 3200]\n",
      "loss: 0.985255  [   16/ 3200]\n",
      "loss: 1.190795  [   32/ 3200]\n",
      "loss: 0.949699  [   48/ 3200]\n",
      "loss: 1.036244  [   64/ 3200]\n",
      "loss: 1.055575  [   80/ 3200]\n",
      "loss: 1.086773  [   96/ 3200]\n",
      "loss: 1.062743  [  112/ 3200]\n",
      "loss: 1.059554  [  128/ 3200]\n",
      "loss: 1.042274  [  144/ 3200]\n",
      "loss: 1.365814  [  160/ 3200]\n",
      "loss: 1.127664  [  176/ 3200]\n",
      "loss: 0.819895  [  192/ 3200]\n",
      "loss: 1.214135  [  208/ 3200]\n",
      "loss: 1.057270  [  224/ 3200]\n",
      "loss: 1.042226  [  240/ 3200]\n",
      "loss: 1.215053  [  256/ 3200]\n",
      "loss: 1.115143  [  272/ 3200]\n",
      "loss: 0.928381  [  288/ 3200]\n",
      "loss: 1.061652  [  304/ 3200]\n",
      "loss: 1.008260  [  320/ 3200]\n",
      "loss: 1.002634  [  336/ 3200]\n",
      "loss: 1.130821  [  352/ 3200]\n",
      "loss: 0.921886  [  368/ 3200]\n",
      "loss: 1.179953  [  384/ 3200]\n",
      "loss: 0.987996  [  400/ 3200]\n",
      "loss: 1.006986  [  416/ 3200]\n",
      "loss: 1.061444  [  432/ 3200]\n",
      "loss: 0.991058  [  448/ 3200]\n",
      "loss: 1.094703  [  464/ 3200]\n",
      "loss: 0.961474  [  480/ 3200]\n",
      "loss: 0.904505  [  496/ 3200]\n",
      "loss: 1.058820  [  512/ 3200]\n",
      "loss: 1.058166  [  528/ 3200]\n",
      "loss: 1.025564  [  544/ 3200]\n",
      "loss: 1.010892  [  560/ 3200]\n",
      "loss: 1.257362  [  576/ 3200]\n",
      "loss: 0.886593  [  592/ 3200]\n",
      "loss: 0.867211  [  608/ 3200]\n",
      "loss: 1.115348  [  624/ 3200]\n",
      "loss: 0.890245  [  640/ 3200]\n",
      "loss: 1.140230  [  656/ 3200]\n",
      "loss: 1.370082  [  672/ 3200]\n",
      "loss: 0.822071  [  688/ 3200]\n",
      "loss: 0.971328  [  704/ 3200]\n",
      "loss: 0.920504  [  720/ 3200]\n",
      "loss: 0.979222  [  736/ 3200]\n",
      "loss: 1.045649  [  752/ 3200]\n",
      "loss: 0.972947  [  768/ 3200]\n",
      "loss: 1.177485  [  784/ 3200]\n",
      "loss: 1.004951  [  800/ 3200]\n",
      "loss: 1.028993  [  816/ 3200]\n",
      "loss: 1.015765  [  832/ 3200]\n",
      "loss: 1.081218  [  848/ 3200]\n",
      "loss: 1.018659  [  864/ 3200]\n",
      "loss: 1.093163  [  880/ 3200]\n",
      "loss: 1.003189  [  896/ 3200]\n",
      "loss: 0.901038  [  912/ 3200]\n",
      "loss: 1.021737  [  928/ 3200]\n",
      "loss: 1.008878  [  944/ 3200]\n",
      "loss: 0.942007  [  960/ 3200]\n",
      "loss: 0.975566  [  976/ 3200]\n",
      "loss: 1.144452  [  992/ 3200]\n",
      "loss: 1.326125  [ 1008/ 3200]\n",
      "loss: 0.813539  [ 1024/ 3200]\n",
      "loss: 0.945721  [ 1040/ 3200]\n",
      "loss: 1.155031  [ 1056/ 3200]\n",
      "loss: 1.138183  [ 1072/ 3200]\n",
      "loss: 1.055176  [ 1088/ 3200]\n",
      "loss: 1.016433  [ 1104/ 3200]\n",
      "loss: 1.057389  [ 1120/ 3200]\n",
      "loss: 1.088804  [ 1136/ 3200]\n",
      "loss: 1.229134  [ 1152/ 3200]\n",
      "loss: 1.164892  [ 1168/ 3200]\n",
      "loss: 0.960169  [ 1184/ 3200]\n",
      "loss: 1.280218  [ 1200/ 3200]\n",
      "loss: 1.146161  [ 1216/ 3200]\n",
      "loss: 1.171927  [ 1232/ 3200]\n",
      "loss: 1.087867  [ 1248/ 3200]\n",
      "loss: 1.033275  [ 1264/ 3200]\n",
      "loss: 0.972568  [ 1280/ 3200]\n",
      "loss: 0.966635  [ 1296/ 3200]\n",
      "loss: 1.226064  [ 1312/ 3200]\n",
      "loss: 0.917295  [ 1328/ 3200]\n",
      "loss: 0.916425  [ 1344/ 3200]\n",
      "loss: 1.116009  [ 1360/ 3200]\n",
      "loss: 1.151301  [ 1376/ 3200]\n",
      "loss: 1.236117  [ 1392/ 3200]\n",
      "loss: 1.015161  [ 1408/ 3200]\n",
      "loss: 0.866922  [ 1424/ 3200]\n",
      "loss: 1.035033  [ 1440/ 3200]\n",
      "loss: 1.191459  [ 1456/ 3200]\n",
      "loss: 1.090066  [ 1472/ 3200]\n",
      "loss: 1.051960  [ 1488/ 3200]\n",
      "loss: 0.850103  [ 1504/ 3200]\n",
      "loss: 1.133431  [ 1520/ 3200]\n",
      "loss: 1.025875  [ 1536/ 3200]\n",
      "loss: 1.138169  [ 1552/ 3200]\n",
      "loss: 1.251540  [ 1568/ 3200]\n",
      "loss: 1.026851  [ 1584/ 3200]\n",
      "loss: 0.973136  [ 1600/ 3200]\n",
      "loss: 1.118550  [ 1616/ 3200]\n",
      "loss: 0.988050  [ 1632/ 3200]\n",
      "loss: 0.932678  [ 1648/ 3200]\n",
      "loss: 0.990604  [ 1664/ 3200]\n",
      "loss: 1.095472  [ 1680/ 3200]\n",
      "loss: 0.911288  [ 1696/ 3200]\n",
      "loss: 1.037883  [ 1712/ 3200]\n",
      "loss: 1.106593  [ 1728/ 3200]\n",
      "loss: 1.048255  [ 1744/ 3200]\n",
      "loss: 1.095277  [ 1760/ 3200]\n",
      "loss: 0.955752  [ 1776/ 3200]\n",
      "loss: 1.168953  [ 1792/ 3200]\n",
      "loss: 1.016856  [ 1808/ 3200]\n",
      "loss: 0.816602  [ 1824/ 3200]\n",
      "loss: 1.140700  [ 1840/ 3200]\n",
      "loss: 0.846825  [ 1856/ 3200]\n",
      "loss: 1.066365  [ 1872/ 3200]\n",
      "loss: 1.025039  [ 1888/ 3200]\n",
      "loss: 1.091222  [ 1904/ 3200]\n",
      "loss: 0.826955  [ 1920/ 3200]\n",
      "loss: 0.910668  [ 1936/ 3200]\n",
      "loss: 0.888881  [ 1952/ 3200]\n",
      "loss: 1.261096  [ 1968/ 3200]\n",
      "loss: 0.970551  [ 1984/ 3200]\n",
      "loss: 0.788437  [ 2000/ 3200]\n",
      "loss: 1.145850  [ 2016/ 3200]\n",
      "loss: 1.233126  [ 2032/ 3200]\n",
      "loss: 0.928444  [ 2048/ 3200]\n",
      "loss: 1.027268  [ 2064/ 3200]\n",
      "loss: 1.024890  [ 2080/ 3200]\n",
      "loss: 1.322532  [ 2096/ 3200]\n",
      "loss: 0.922729  [ 2112/ 3200]\n",
      "loss: 0.985570  [ 2128/ 3200]\n",
      "loss: 1.398141  [ 2144/ 3200]\n",
      "loss: 1.061321  [ 2160/ 3200]\n",
      "loss: 1.216263  [ 2176/ 3200]\n",
      "loss: 0.928768  [ 2192/ 3200]\n",
      "loss: 1.234277  [ 2208/ 3200]\n",
      "loss: 1.016104  [ 2224/ 3200]\n",
      "loss: 1.205631  [ 2240/ 3200]\n",
      "loss: 1.118760  [ 2256/ 3200]\n",
      "loss: 0.980291  [ 2272/ 3200]\n",
      "loss: 0.906775  [ 2288/ 3200]\n",
      "loss: 0.997398  [ 2304/ 3200]\n",
      "loss: 1.075317  [ 2320/ 3200]\n",
      "loss: 1.062702  [ 2336/ 3200]\n",
      "loss: 0.933257  [ 2352/ 3200]\n",
      "loss: 0.736920  [ 2368/ 3200]\n",
      "loss: 1.024503  [ 2384/ 3200]\n",
      "loss: 1.065872  [ 2400/ 3200]\n",
      "loss: 1.416141  [ 2416/ 3200]\n",
      "loss: 0.845257  [ 2432/ 3200]\n",
      "loss: 1.181305  [ 2448/ 3200]\n",
      "loss: 1.102186  [ 2464/ 3200]\n",
      "loss: 1.034470  [ 2480/ 3200]\n",
      "loss: 1.216581  [ 2496/ 3200]\n",
      "loss: 1.063804  [ 2512/ 3200]\n",
      "loss: 1.082599  [ 2528/ 3200]\n",
      "loss: 1.131651  [ 2544/ 3200]\n",
      "loss: 0.992985  [ 2560/ 3200]\n",
      "loss: 1.024932  [ 2576/ 3200]\n",
      "loss: 1.000981  [ 2592/ 3200]\n",
      "loss: 1.211542  [ 2608/ 3200]\n",
      "loss: 1.305561  [ 2624/ 3200]\n",
      "loss: 1.008080  [ 2640/ 3200]\n",
      "loss: 0.983857  [ 2656/ 3200]\n",
      "loss: 1.079767  [ 2672/ 3200]\n",
      "loss: 0.902926  [ 2688/ 3200]\n",
      "loss: 0.911427  [ 2704/ 3200]\n",
      "loss: 1.017585  [ 2720/ 3200]\n",
      "loss: 1.170034  [ 2736/ 3200]\n",
      "loss: 1.076172  [ 2752/ 3200]\n",
      "loss: 1.098729  [ 2768/ 3200]\n",
      "loss: 0.780392  [ 2784/ 3200]\n",
      "loss: 0.974069  [ 2800/ 3200]\n",
      "loss: 0.771923  [ 2816/ 3200]\n",
      "loss: 0.914029  [ 2832/ 3200]\n",
      "loss: 0.896720  [ 2848/ 3200]\n",
      "loss: 1.112294  [ 2864/ 3200]\n",
      "loss: 0.971705  [ 2880/ 3200]\n",
      "loss: 0.912696  [ 2896/ 3200]\n",
      "loss: 1.091199  [ 2912/ 3200]\n",
      "loss: 0.988429  [ 2928/ 3200]\n",
      "loss: 1.106699  [ 2944/ 3200]\n",
      "loss: 1.010365  [ 2960/ 3200]\n",
      "loss: 0.955632  [ 2976/ 3200]\n",
      "loss: 1.044608  [ 2992/ 3200]\n",
      "loss: 1.097457  [ 3008/ 3200]\n",
      "loss: 0.931632  [ 3024/ 3200]\n",
      "loss: 0.840903  [ 3040/ 3200]\n",
      "loss: 1.139283  [ 3056/ 3200]\n",
      "loss: 1.131466  [ 3072/ 3200]\n",
      "loss: 1.037475  [ 3088/ 3200]\n",
      "loss: 1.084692  [ 3104/ 3200]\n",
      "loss: 0.963261  [ 3120/ 3200]\n",
      "loss: 0.855839  [ 3136/ 3200]\n",
      "loss: 1.050194  [ 3152/ 3200]\n",
      "loss: 1.043708  [ 3168/ 3200]\n",
      "loss: 1.128130  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.794428  [    0/ 3200]\n",
      "loss: 0.946021  [   16/ 3200]\n",
      "loss: 1.171497  [   32/ 3200]\n",
      "loss: 1.039986  [   48/ 3200]\n",
      "loss: 1.005810  [   64/ 3200]\n",
      "loss: 1.059222  [   80/ 3200]\n",
      "loss: 1.057393  [   96/ 3200]\n",
      "loss: 1.114437  [  112/ 3200]\n",
      "loss: 1.056564  [  128/ 3200]\n",
      "loss: 0.935855  [  144/ 3200]\n",
      "loss: 1.127022  [  160/ 3200]\n",
      "loss: 0.923192  [  176/ 3200]\n",
      "loss: 0.948120  [  192/ 3200]\n",
      "loss: 1.094195  [  208/ 3200]\n",
      "loss: 0.991953  [  224/ 3200]\n",
      "loss: 1.079623  [  240/ 3200]\n",
      "loss: 0.947189  [  256/ 3200]\n",
      "loss: 0.941776  [  272/ 3200]\n",
      "loss: 0.965701  [  288/ 3200]\n",
      "loss: 0.997004  [  304/ 3200]\n",
      "loss: 1.262281  [  320/ 3200]\n",
      "loss: 1.088513  [  336/ 3200]\n",
      "loss: 1.003211  [  352/ 3200]\n",
      "loss: 1.086053  [  368/ 3200]\n",
      "loss: 1.281609  [  384/ 3200]\n",
      "loss: 1.027983  [  400/ 3200]\n",
      "loss: 0.959423  [  416/ 3200]\n",
      "loss: 1.027332  [  432/ 3200]\n",
      "loss: 0.982428  [  448/ 3200]\n",
      "loss: 1.183706  [  464/ 3200]\n",
      "loss: 0.925880  [  480/ 3200]\n",
      "loss: 0.973345  [  496/ 3200]\n",
      "loss: 0.906007  [  512/ 3200]\n",
      "loss: 0.853937  [  528/ 3200]\n",
      "loss: 1.179481  [  544/ 3200]\n",
      "loss: 1.122905  [  560/ 3200]\n",
      "loss: 1.112972  [  576/ 3200]\n",
      "loss: 1.086918  [  592/ 3200]\n",
      "loss: 1.049772  [  608/ 3200]\n",
      "loss: 0.973983  [  624/ 3200]\n",
      "loss: 1.066890  [  640/ 3200]\n",
      "loss: 0.895786  [  656/ 3200]\n",
      "loss: 0.987427  [  672/ 3200]\n",
      "loss: 0.979296  [  688/ 3200]\n",
      "loss: 0.912247  [  704/ 3200]\n",
      "loss: 1.040651  [  720/ 3200]\n",
      "loss: 0.874362  [  736/ 3200]\n",
      "loss: 0.894519  [  752/ 3200]\n",
      "loss: 0.872607  [  768/ 3200]\n",
      "loss: 0.943516  [  784/ 3200]\n",
      "loss: 1.279234  [  800/ 3200]\n",
      "loss: 0.945611  [  816/ 3200]\n",
      "loss: 0.982469  [  832/ 3200]\n",
      "loss: 0.991839  [  848/ 3200]\n",
      "loss: 1.303882  [  864/ 3200]\n",
      "loss: 1.085886  [  880/ 3200]\n",
      "loss: 0.890648  [  896/ 3200]\n",
      "loss: 1.069993  [  912/ 3200]\n",
      "loss: 1.117303  [  928/ 3200]\n",
      "loss: 1.156224  [  944/ 3200]\n",
      "loss: 1.177533  [  960/ 3200]\n",
      "loss: 0.943173  [  976/ 3200]\n",
      "loss: 1.020926  [  992/ 3200]\n",
      "loss: 0.928285  [ 1008/ 3200]\n",
      "loss: 1.069006  [ 1024/ 3200]\n",
      "loss: 1.163580  [ 1040/ 3200]\n",
      "loss: 1.155868  [ 1056/ 3200]\n",
      "loss: 1.178750  [ 1072/ 3200]\n",
      "loss: 1.027146  [ 1088/ 3200]\n",
      "loss: 0.821135  [ 1104/ 3200]\n",
      "loss: 1.196514  [ 1120/ 3200]\n",
      "loss: 0.788890  [ 1136/ 3200]\n",
      "loss: 1.085956  [ 1152/ 3200]\n",
      "loss: 0.942955  [ 1168/ 3200]\n",
      "loss: 0.992615  [ 1184/ 3200]\n",
      "loss: 1.040993  [ 1200/ 3200]\n",
      "loss: 0.928345  [ 1216/ 3200]\n",
      "loss: 1.008691  [ 1232/ 3200]\n",
      "loss: 0.923695  [ 1248/ 3200]\n",
      "loss: 0.937487  [ 1264/ 3200]\n",
      "loss: 1.101248  [ 1280/ 3200]\n",
      "loss: 1.050919  [ 1296/ 3200]\n",
      "loss: 1.320080  [ 1312/ 3200]\n",
      "loss: 1.093807  [ 1328/ 3200]\n",
      "loss: 0.969970  [ 1344/ 3200]\n",
      "loss: 1.242132  [ 1360/ 3200]\n",
      "loss: 1.078175  [ 1376/ 3200]\n",
      "loss: 1.065945  [ 1392/ 3200]\n",
      "loss: 0.991604  [ 1408/ 3200]\n",
      "loss: 1.068816  [ 1424/ 3200]\n",
      "loss: 0.867471  [ 1440/ 3200]\n",
      "loss: 1.033602  [ 1456/ 3200]\n",
      "loss: 0.954563  [ 1472/ 3200]\n",
      "loss: 1.147927  [ 1488/ 3200]\n",
      "loss: 1.096593  [ 1504/ 3200]\n",
      "loss: 0.975860  [ 1520/ 3200]\n",
      "loss: 0.935385  [ 1536/ 3200]\n",
      "loss: 0.848602  [ 1552/ 3200]\n",
      "loss: 1.150558  [ 1568/ 3200]\n",
      "loss: 1.219632  [ 1584/ 3200]\n",
      "loss: 0.839200  [ 1600/ 3200]\n",
      "loss: 1.112273  [ 1616/ 3200]\n",
      "loss: 0.752797  [ 1632/ 3200]\n",
      "loss: 0.908831  [ 1648/ 3200]\n",
      "loss: 0.931929  [ 1664/ 3200]\n",
      "loss: 0.972788  [ 1680/ 3200]\n",
      "loss: 0.921630  [ 1696/ 3200]\n",
      "loss: 0.936980  [ 1712/ 3200]\n",
      "loss: 0.888912  [ 1728/ 3200]\n",
      "loss: 0.983500  [ 1744/ 3200]\n",
      "loss: 1.040459  [ 1760/ 3200]\n",
      "loss: 0.966778  [ 1776/ 3200]\n",
      "loss: 1.008415  [ 1792/ 3200]\n",
      "loss: 1.113175  [ 1808/ 3200]\n",
      "loss: 0.980119  [ 1824/ 3200]\n",
      "loss: 0.994724  [ 1840/ 3200]\n",
      "loss: 1.084999  [ 1856/ 3200]\n",
      "loss: 1.250495  [ 1872/ 3200]\n",
      "loss: 1.308810  [ 1888/ 3200]\n",
      "loss: 0.955139  [ 1904/ 3200]\n",
      "loss: 1.073632  [ 1920/ 3200]\n",
      "loss: 1.062648  [ 1936/ 3200]\n",
      "loss: 1.045921  [ 1952/ 3200]\n",
      "loss: 0.758845  [ 1968/ 3200]\n",
      "loss: 1.209843  [ 1984/ 3200]\n",
      "loss: 0.891361  [ 2000/ 3200]\n",
      "loss: 1.113616  [ 2016/ 3200]\n",
      "loss: 0.977331  [ 2032/ 3200]\n",
      "loss: 1.108391  [ 2048/ 3200]\n",
      "loss: 0.836680  [ 2064/ 3200]\n",
      "loss: 1.080526  [ 2080/ 3200]\n",
      "loss: 0.973349  [ 2096/ 3200]\n",
      "loss: 1.040961  [ 2112/ 3200]\n",
      "loss: 1.014330  [ 2128/ 3200]\n",
      "loss: 1.104350  [ 2144/ 3200]\n",
      "loss: 1.038817  [ 2160/ 3200]\n",
      "loss: 1.149446  [ 2176/ 3200]\n",
      "loss: 1.067769  [ 2192/ 3200]\n",
      "loss: 1.053815  [ 2208/ 3200]\n",
      "loss: 1.089095  [ 2224/ 3200]\n",
      "loss: 0.997730  [ 2240/ 3200]\n",
      "loss: 0.889014  [ 2256/ 3200]\n",
      "loss: 0.843197  [ 2272/ 3200]\n",
      "loss: 1.021611  [ 2288/ 3200]\n",
      "loss: 1.169410  [ 2304/ 3200]\n",
      "loss: 0.959028  [ 2320/ 3200]\n",
      "loss: 1.059023  [ 2336/ 3200]\n",
      "loss: 1.066599  [ 2352/ 3200]\n",
      "loss: 0.905139  [ 2368/ 3200]\n",
      "loss: 1.053043  [ 2384/ 3200]\n",
      "loss: 1.098615  [ 2400/ 3200]\n",
      "loss: 1.209889  [ 2416/ 3200]\n",
      "loss: 0.983138  [ 2432/ 3200]\n",
      "loss: 0.847832  [ 2448/ 3200]\n",
      "loss: 0.897325  [ 2464/ 3200]\n",
      "loss: 0.958761  [ 2480/ 3200]\n",
      "loss: 1.143901  [ 2496/ 3200]\n",
      "loss: 1.093717  [ 2512/ 3200]\n",
      "loss: 1.043197  [ 2528/ 3200]\n",
      "loss: 1.012477  [ 2544/ 3200]\n",
      "loss: 1.176507  [ 2560/ 3200]\n",
      "loss: 1.252961  [ 2576/ 3200]\n",
      "loss: 0.895929  [ 2592/ 3200]\n",
      "loss: 1.103757  [ 2608/ 3200]\n",
      "loss: 1.074092  [ 2624/ 3200]\n",
      "loss: 0.979098  [ 2640/ 3200]\n",
      "loss: 0.859125  [ 2656/ 3200]\n",
      "loss: 1.260594  [ 2672/ 3200]\n",
      "loss: 0.885384  [ 2688/ 3200]\n",
      "loss: 1.113008  [ 2704/ 3200]\n",
      "loss: 1.041034  [ 2720/ 3200]\n",
      "loss: 0.856169  [ 2736/ 3200]\n",
      "loss: 1.202704  [ 2752/ 3200]\n",
      "loss: 1.187625  [ 2768/ 3200]\n",
      "loss: 0.762944  [ 2784/ 3200]\n",
      "loss: 1.008158  [ 2800/ 3200]\n",
      "loss: 0.904089  [ 2816/ 3200]\n",
      "loss: 0.945781  [ 2832/ 3200]\n",
      "loss: 0.874606  [ 2848/ 3200]\n",
      "loss: 1.122517  [ 2864/ 3200]\n",
      "loss: 0.829171  [ 2880/ 3200]\n",
      "loss: 1.119513  [ 2896/ 3200]\n",
      "loss: 0.839745  [ 2912/ 3200]\n",
      "loss: 1.262367  [ 2928/ 3200]\n",
      "loss: 0.737390  [ 2944/ 3200]\n",
      "loss: 1.027949  [ 2960/ 3200]\n",
      "loss: 1.177419  [ 2976/ 3200]\n",
      "loss: 1.206143  [ 2992/ 3200]\n",
      "loss: 1.141427  [ 3008/ 3200]\n",
      "loss: 1.179769  [ 3024/ 3200]\n",
      "loss: 0.913956  [ 3040/ 3200]\n",
      "loss: 0.917932  [ 3056/ 3200]\n",
      "loss: 0.888452  [ 3072/ 3200]\n",
      "loss: 1.181443  [ 3088/ 3200]\n",
      "loss: 0.983371  [ 3104/ 3200]\n",
      "loss: 0.851619  [ 3120/ 3200]\n",
      "loss: 0.907655  [ 3136/ 3200]\n",
      "loss: 1.227337  [ 3152/ 3200]\n",
      "loss: 1.408858  [ 3168/ 3200]\n",
      "loss: 0.932244  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 1.026732  [    0/ 3200]\n",
      "loss: 0.989334  [   16/ 3200]\n",
      "loss: 1.232792  [   32/ 3200]\n",
      "loss: 0.987727  [   48/ 3200]\n",
      "loss: 0.949971  [   64/ 3200]\n",
      "loss: 0.989759  [   80/ 3200]\n",
      "loss: 1.226456  [   96/ 3200]\n",
      "loss: 0.867444  [  112/ 3200]\n",
      "loss: 1.029098  [  128/ 3200]\n",
      "loss: 1.126145  [  144/ 3200]\n",
      "loss: 0.930094  [  160/ 3200]\n",
      "loss: 1.070837  [  176/ 3200]\n",
      "loss: 0.983609  [  192/ 3200]\n",
      "loss: 0.919649  [  208/ 3200]\n",
      "loss: 1.087453  [  224/ 3200]\n",
      "loss: 1.047051  [  240/ 3200]\n",
      "loss: 0.950775  [  256/ 3200]\n",
      "loss: 1.026897  [  272/ 3200]\n",
      "loss: 0.975373  [  288/ 3200]\n",
      "loss: 0.880608  [  304/ 3200]\n",
      "loss: 0.789723  [  320/ 3200]\n",
      "loss: 1.000604  [  336/ 3200]\n",
      "loss: 1.114722  [  352/ 3200]\n",
      "loss: 1.317281  [  368/ 3200]\n",
      "loss: 1.120445  [  384/ 3200]\n",
      "loss: 1.223769  [  400/ 3200]\n",
      "loss: 1.067719  [  416/ 3200]\n",
      "loss: 0.927278  [  432/ 3200]\n",
      "loss: 0.828816  [  448/ 3200]\n",
      "loss: 1.203147  [  464/ 3200]\n",
      "loss: 1.049986  [  480/ 3200]\n",
      "loss: 1.343320  [  496/ 3200]\n",
      "loss: 0.865451  [  512/ 3200]\n",
      "loss: 1.138466  [  528/ 3200]\n",
      "loss: 0.914554  [  544/ 3200]\n",
      "loss: 1.103259  [  560/ 3200]\n",
      "loss: 0.822406  [  576/ 3200]\n",
      "loss: 1.037822  [  592/ 3200]\n",
      "loss: 1.067254  [  608/ 3200]\n",
      "loss: 0.975542  [  624/ 3200]\n",
      "loss: 1.019314  [  640/ 3200]\n",
      "loss: 0.992497  [  656/ 3200]\n",
      "loss: 0.874279  [  672/ 3200]\n",
      "loss: 0.944958  [  688/ 3200]\n",
      "loss: 0.945872  [  704/ 3200]\n",
      "loss: 0.845453  [  720/ 3200]\n",
      "loss: 0.915800  [  736/ 3200]\n",
      "loss: 1.222549  [  752/ 3200]\n",
      "loss: 0.874147  [  768/ 3200]\n",
      "loss: 0.921646  [  784/ 3200]\n",
      "loss: 0.974914  [  800/ 3200]\n",
      "loss: 0.794508  [  816/ 3200]\n",
      "loss: 0.879095  [  832/ 3200]\n",
      "loss: 0.951850  [  848/ 3200]\n",
      "loss: 0.785459  [  864/ 3200]\n",
      "loss: 0.945940  [  880/ 3200]\n",
      "loss: 0.998112  [  896/ 3200]\n",
      "loss: 1.053941  [  912/ 3200]\n",
      "loss: 1.046638  [  928/ 3200]\n",
      "loss: 1.209252  [  944/ 3200]\n",
      "loss: 1.103802  [  960/ 3200]\n",
      "loss: 0.818023  [  976/ 3200]\n",
      "loss: 0.940832  [  992/ 3200]\n",
      "loss: 0.925357  [ 1008/ 3200]\n",
      "loss: 1.060642  [ 1024/ 3200]\n",
      "loss: 1.078158  [ 1040/ 3200]\n",
      "loss: 1.217108  [ 1056/ 3200]\n",
      "loss: 1.121375  [ 1072/ 3200]\n",
      "loss: 1.289077  [ 1088/ 3200]\n",
      "loss: 1.132448  [ 1104/ 3200]\n",
      "loss: 1.002340  [ 1120/ 3200]\n",
      "loss: 1.008373  [ 1136/ 3200]\n",
      "loss: 0.800893  [ 1152/ 3200]\n",
      "loss: 0.958294  [ 1168/ 3200]\n",
      "loss: 0.917156  [ 1184/ 3200]\n",
      "loss: 0.940555  [ 1200/ 3200]\n",
      "loss: 1.202256  [ 1216/ 3200]\n",
      "loss: 1.309556  [ 1232/ 3200]\n",
      "loss: 1.078448  [ 1248/ 3200]\n",
      "loss: 0.954539  [ 1264/ 3200]\n",
      "loss: 1.105247  [ 1280/ 3200]\n",
      "loss: 1.304500  [ 1296/ 3200]\n",
      "loss: 0.961552  [ 1312/ 3200]\n",
      "loss: 1.039802  [ 1328/ 3200]\n",
      "loss: 1.034683  [ 1344/ 3200]\n",
      "loss: 1.103861  [ 1360/ 3200]\n",
      "loss: 1.054040  [ 1376/ 3200]\n",
      "loss: 1.018273  [ 1392/ 3200]\n",
      "loss: 0.916643  [ 1408/ 3200]\n",
      "loss: 1.060762  [ 1424/ 3200]\n",
      "loss: 1.147062  [ 1440/ 3200]\n",
      "loss: 1.141890  [ 1456/ 3200]\n",
      "loss: 1.000249  [ 1472/ 3200]\n",
      "loss: 1.043314  [ 1488/ 3200]\n",
      "loss: 1.121445  [ 1504/ 3200]\n",
      "loss: 0.968378  [ 1520/ 3200]\n",
      "loss: 1.003877  [ 1536/ 3200]\n",
      "loss: 0.847456  [ 1552/ 3200]\n",
      "loss: 1.012840  [ 1568/ 3200]\n",
      "loss: 0.791971  [ 1584/ 3200]\n",
      "loss: 0.893006  [ 1600/ 3200]\n",
      "loss: 0.888691  [ 1616/ 3200]\n",
      "loss: 0.981081  [ 1632/ 3200]\n",
      "loss: 0.941583  [ 1648/ 3200]\n",
      "loss: 1.119777  [ 1664/ 3200]\n",
      "loss: 0.979187  [ 1680/ 3200]\n",
      "loss: 1.084840  [ 1696/ 3200]\n",
      "loss: 0.975314  [ 1712/ 3200]\n",
      "loss: 0.968828  [ 1728/ 3200]\n",
      "loss: 0.947063  [ 1744/ 3200]\n",
      "loss: 0.917296  [ 1760/ 3200]\n",
      "loss: 0.886995  [ 1776/ 3200]\n",
      "loss: 0.931048  [ 1792/ 3200]\n",
      "loss: 1.147945  [ 1808/ 3200]\n",
      "loss: 0.974831  [ 1824/ 3200]\n",
      "loss: 1.228536  [ 1840/ 3200]\n",
      "loss: 1.022020  [ 1856/ 3200]\n",
      "loss: 0.947084  [ 1872/ 3200]\n",
      "loss: 0.924384  [ 1888/ 3200]\n",
      "loss: 0.970108  [ 1904/ 3200]\n",
      "loss: 0.805060  [ 1920/ 3200]\n",
      "loss: 0.885393  [ 1936/ 3200]\n",
      "loss: 1.001740  [ 1952/ 3200]\n",
      "loss: 1.168548  [ 1968/ 3200]\n",
      "loss: 1.092378  [ 1984/ 3200]\n",
      "loss: 0.927845  [ 2000/ 3200]\n",
      "loss: 1.661689  [ 2016/ 3200]\n",
      "loss: 1.065690  [ 2032/ 3200]\n",
      "loss: 1.042042  [ 2048/ 3200]\n",
      "loss: 1.036857  [ 2064/ 3200]\n",
      "loss: 1.072304  [ 2080/ 3200]\n",
      "loss: 1.069338  [ 2096/ 3200]\n",
      "loss: 1.443932  [ 2112/ 3200]\n",
      "loss: 1.005166  [ 2128/ 3200]\n",
      "loss: 1.255267  [ 2144/ 3200]\n",
      "loss: 1.012975  [ 2160/ 3200]\n",
      "loss: 0.975611  [ 2176/ 3200]\n",
      "loss: 1.152228  [ 2192/ 3200]\n",
      "loss: 0.935857  [ 2208/ 3200]\n",
      "loss: 0.938479  [ 2224/ 3200]\n",
      "loss: 0.915951  [ 2240/ 3200]\n",
      "loss: 0.948769  [ 2256/ 3200]\n",
      "loss: 0.916877  [ 2272/ 3200]\n",
      "loss: 0.667726  [ 2288/ 3200]\n",
      "loss: 1.100602  [ 2304/ 3200]\n",
      "loss: 0.882617  [ 2320/ 3200]\n",
      "loss: 1.169805  [ 2336/ 3200]\n",
      "loss: 0.996303  [ 2352/ 3200]\n",
      "loss: 1.161853  [ 2368/ 3200]\n",
      "loss: 1.178610  [ 2384/ 3200]\n",
      "loss: 0.954852  [ 2400/ 3200]\n",
      "loss: 0.859816  [ 2416/ 3200]\n",
      "loss: 1.267613  [ 2432/ 3200]\n",
      "loss: 0.866124  [ 2448/ 3200]\n",
      "loss: 1.100333  [ 2464/ 3200]\n",
      "loss: 0.962295  [ 2480/ 3200]\n",
      "loss: 0.944590  [ 2496/ 3200]\n",
      "loss: 0.906884  [ 2512/ 3200]\n",
      "loss: 1.108552  [ 2528/ 3200]\n",
      "loss: 1.133528  [ 2544/ 3200]\n",
      "loss: 1.023614  [ 2560/ 3200]\n",
      "loss: 1.108565  [ 2576/ 3200]\n",
      "loss: 0.729721  [ 2592/ 3200]\n",
      "loss: 0.956513  [ 2608/ 3200]\n",
      "loss: 0.902280  [ 2624/ 3200]\n",
      "loss: 1.055651  [ 2640/ 3200]\n",
      "loss: 0.874031  [ 2656/ 3200]\n",
      "loss: 1.043628  [ 2672/ 3200]\n",
      "loss: 1.061319  [ 2688/ 3200]\n",
      "loss: 0.756476  [ 2704/ 3200]\n",
      "loss: 1.048456  [ 2720/ 3200]\n",
      "loss: 1.131087  [ 2736/ 3200]\n",
      "loss: 1.034419  [ 2752/ 3200]\n",
      "loss: 0.939725  [ 2768/ 3200]\n",
      "loss: 1.107494  [ 2784/ 3200]\n",
      "loss: 0.963111  [ 2800/ 3200]\n",
      "loss: 1.286177  [ 2816/ 3200]\n",
      "loss: 1.027808  [ 2832/ 3200]\n",
      "loss: 1.045345  [ 2848/ 3200]\n",
      "loss: 0.960512  [ 2864/ 3200]\n",
      "loss: 1.015392  [ 2880/ 3200]\n",
      "loss: 1.093046  [ 2896/ 3200]\n",
      "loss: 0.911379  [ 2912/ 3200]\n",
      "loss: 1.183794  [ 2928/ 3200]\n",
      "loss: 0.907803  [ 2944/ 3200]\n",
      "loss: 0.996176  [ 2960/ 3200]\n",
      "loss: 0.950808  [ 2976/ 3200]\n",
      "loss: 1.464849  [ 2992/ 3200]\n",
      "loss: 1.076930  [ 3008/ 3200]\n",
      "loss: 0.975903  [ 3024/ 3200]\n",
      "loss: 1.038924  [ 3040/ 3200]\n",
      "loss: 1.178602  [ 3056/ 3200]\n",
      "loss: 0.746901  [ 3072/ 3200]\n",
      "loss: 0.878096  [ 3088/ 3200]\n",
      "loss: 1.089453  [ 3104/ 3200]\n",
      "loss: 1.035966  [ 3120/ 3200]\n",
      "loss: 0.849028  [ 3136/ 3200]\n",
      "loss: 0.980156  [ 3152/ 3200]\n",
      "loss: 0.706832  [ 3168/ 3200]\n",
      "loss: 0.922285  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.761364  [    0/ 3200]\n",
      "loss: 1.091364  [   16/ 3200]\n",
      "loss: 0.999250  [   32/ 3200]\n",
      "loss: 1.106618  [   48/ 3200]\n",
      "loss: 0.909052  [   64/ 3200]\n",
      "loss: 0.940271  [   80/ 3200]\n",
      "loss: 1.089518  [   96/ 3200]\n",
      "loss: 1.069164  [  112/ 3200]\n",
      "loss: 0.915436  [  128/ 3200]\n",
      "loss: 1.003108  [  144/ 3200]\n",
      "loss: 0.930982  [  160/ 3200]\n",
      "loss: 0.837068  [  176/ 3200]\n",
      "loss: 1.160970  [  192/ 3200]\n",
      "loss: 0.748031  [  208/ 3200]\n",
      "loss: 1.217227  [  224/ 3200]\n",
      "loss: 1.062446  [  240/ 3200]\n",
      "loss: 1.196874  [  256/ 3200]\n",
      "loss: 1.142829  [  272/ 3200]\n",
      "loss: 1.090289  [  288/ 3200]\n",
      "loss: 0.997772  [  304/ 3200]\n",
      "loss: 1.087722  [  320/ 3200]\n",
      "loss: 0.862819  [  336/ 3200]\n",
      "loss: 0.915250  [  352/ 3200]\n",
      "loss: 0.906481  [  368/ 3200]\n",
      "loss: 0.854369  [  384/ 3200]\n",
      "loss: 0.950584  [  400/ 3200]\n",
      "loss: 0.885640  [  416/ 3200]\n",
      "loss: 0.744612  [  432/ 3200]\n",
      "loss: 1.043286  [  448/ 3200]\n",
      "loss: 0.922130  [  464/ 3200]\n",
      "loss: 0.931132  [  480/ 3200]\n",
      "loss: 1.136387  [  496/ 3200]\n",
      "loss: 1.002187  [  512/ 3200]\n",
      "loss: 1.061574  [  528/ 3200]\n",
      "loss: 1.001767  [  544/ 3200]\n",
      "loss: 1.085063  [  560/ 3200]\n",
      "loss: 0.818280  [  576/ 3200]\n",
      "loss: 1.190330  [  592/ 3200]\n",
      "loss: 0.797912  [  608/ 3200]\n",
      "loss: 1.116579  [  624/ 3200]\n",
      "loss: 1.010255  [  640/ 3200]\n",
      "loss: 0.943376  [  656/ 3200]\n",
      "loss: 0.955205  [  672/ 3200]\n",
      "loss: 0.811398  [  688/ 3200]\n",
      "loss: 0.967812  [  704/ 3200]\n",
      "loss: 1.144339  [  720/ 3200]\n",
      "loss: 1.203801  [  736/ 3200]\n",
      "loss: 1.090660  [  752/ 3200]\n",
      "loss: 0.883127  [  768/ 3200]\n",
      "loss: 1.090336  [  784/ 3200]\n",
      "loss: 1.255899  [  800/ 3200]\n",
      "loss: 1.245870  [  816/ 3200]\n",
      "loss: 0.965500  [  832/ 3200]\n",
      "loss: 1.123024  [  848/ 3200]\n",
      "loss: 0.956107  [  864/ 3200]\n",
      "loss: 0.839742  [  880/ 3200]\n",
      "loss: 1.022508  [  896/ 3200]\n",
      "loss: 0.956180  [  912/ 3200]\n",
      "loss: 1.036909  [  928/ 3200]\n",
      "loss: 1.126312  [  944/ 3200]\n",
      "loss: 0.948331  [  960/ 3200]\n",
      "loss: 1.001615  [  976/ 3200]\n",
      "loss: 1.160871  [  992/ 3200]\n",
      "loss: 1.043215  [ 1008/ 3200]\n",
      "loss: 1.041921  [ 1024/ 3200]\n",
      "loss: 0.787359  [ 1040/ 3200]\n",
      "loss: 0.935867  [ 1056/ 3200]\n",
      "loss: 1.025286  [ 1072/ 3200]\n",
      "loss: 0.984777  [ 1088/ 3200]\n",
      "loss: 1.200364  [ 1104/ 3200]\n",
      "loss: 1.305099  [ 1120/ 3200]\n",
      "loss: 1.110732  [ 1136/ 3200]\n",
      "loss: 1.084274  [ 1152/ 3200]\n",
      "loss: 0.928298  [ 1168/ 3200]\n",
      "loss: 0.993317  [ 1184/ 3200]\n",
      "loss: 0.995556  [ 1200/ 3200]\n",
      "loss: 0.880700  [ 1216/ 3200]\n",
      "loss: 1.086490  [ 1232/ 3200]\n",
      "loss: 1.153751  [ 1248/ 3200]\n",
      "loss: 1.003004  [ 1264/ 3200]\n",
      "loss: 0.972704  [ 1280/ 3200]\n",
      "loss: 1.342944  [ 1296/ 3200]\n",
      "loss: 1.174617  [ 1312/ 3200]\n",
      "loss: 0.929425  [ 1328/ 3200]\n",
      "loss: 1.128821  [ 1344/ 3200]\n",
      "loss: 0.998218  [ 1360/ 3200]\n",
      "loss: 0.974773  [ 1376/ 3200]\n",
      "loss: 0.752353  [ 1392/ 3200]\n",
      "loss: 1.161945  [ 1408/ 3200]\n",
      "loss: 0.909228  [ 1424/ 3200]\n",
      "loss: 0.797716  [ 1440/ 3200]\n",
      "loss: 1.117511  [ 1456/ 3200]\n",
      "loss: 1.197584  [ 1472/ 3200]\n",
      "loss: 1.042973  [ 1488/ 3200]\n",
      "loss: 0.978107  [ 1504/ 3200]\n",
      "loss: 1.138379  [ 1520/ 3200]\n",
      "loss: 0.985857  [ 1536/ 3200]\n",
      "loss: 1.119785  [ 1552/ 3200]\n",
      "loss: 1.079610  [ 1568/ 3200]\n",
      "loss: 0.947232  [ 1584/ 3200]\n",
      "loss: 0.836408  [ 1600/ 3200]\n",
      "loss: 0.925645  [ 1616/ 3200]\n",
      "loss: 0.851377  [ 1632/ 3200]\n",
      "loss: 1.152472  [ 1648/ 3200]\n",
      "loss: 0.940667  [ 1664/ 3200]\n",
      "loss: 0.970564  [ 1680/ 3200]\n",
      "loss: 1.094525  [ 1696/ 3200]\n",
      "loss: 0.861237  [ 1712/ 3200]\n",
      "loss: 0.882318  [ 1728/ 3200]\n",
      "loss: 0.900113  [ 1744/ 3200]\n",
      "loss: 0.896210  [ 1760/ 3200]\n",
      "loss: 1.254603  [ 1776/ 3200]\n",
      "loss: 1.028553  [ 1792/ 3200]\n",
      "loss: 0.878950  [ 1808/ 3200]\n",
      "loss: 1.125967  [ 1824/ 3200]\n",
      "loss: 0.898657  [ 1840/ 3200]\n",
      "loss: 1.141955  [ 1856/ 3200]\n",
      "loss: 1.177202  [ 1872/ 3200]\n",
      "loss: 0.860227  [ 1888/ 3200]\n",
      "loss: 0.917713  [ 1904/ 3200]\n",
      "loss: 1.040718  [ 1920/ 3200]\n",
      "loss: 1.017162  [ 1936/ 3200]\n",
      "loss: 0.860304  [ 1952/ 3200]\n",
      "loss: 0.911681  [ 1968/ 3200]\n",
      "loss: 1.023324  [ 1984/ 3200]\n",
      "loss: 0.923914  [ 2000/ 3200]\n",
      "loss: 0.831195  [ 2016/ 3200]\n",
      "loss: 1.405617  [ 2032/ 3200]\n",
      "loss: 1.035128  [ 2048/ 3200]\n",
      "loss: 0.883375  [ 2064/ 3200]\n",
      "loss: 1.087158  [ 2080/ 3200]\n",
      "loss: 1.089916  [ 2096/ 3200]\n",
      "loss: 1.104163  [ 2112/ 3200]\n",
      "loss: 0.846337  [ 2128/ 3200]\n",
      "loss: 0.908264  [ 2144/ 3200]\n",
      "loss: 0.902213  [ 2160/ 3200]\n",
      "loss: 0.919495  [ 2176/ 3200]\n",
      "loss: 0.966920  [ 2192/ 3200]\n",
      "loss: 0.996433  [ 2208/ 3200]\n",
      "loss: 0.758786  [ 2224/ 3200]\n",
      "loss: 0.998302  [ 2240/ 3200]\n",
      "loss: 0.793244  [ 2256/ 3200]\n",
      "loss: 1.200362  [ 2272/ 3200]\n",
      "loss: 1.141426  [ 2288/ 3200]\n",
      "loss: 0.915824  [ 2304/ 3200]\n",
      "loss: 0.946769  [ 2320/ 3200]\n",
      "loss: 1.027540  [ 2336/ 3200]\n",
      "loss: 1.119211  [ 2352/ 3200]\n",
      "loss: 0.953845  [ 2368/ 3200]\n",
      "loss: 1.241305  [ 2384/ 3200]\n",
      "loss: 1.081427  [ 2400/ 3200]\n",
      "loss: 1.055384  [ 2416/ 3200]\n",
      "loss: 0.946211  [ 2432/ 3200]\n",
      "loss: 1.260170  [ 2448/ 3200]\n",
      "loss: 1.160590  [ 2464/ 3200]\n",
      "loss: 0.925765  [ 2480/ 3200]\n",
      "loss: 0.913489  [ 2496/ 3200]\n",
      "loss: 0.901191  [ 2512/ 3200]\n",
      "loss: 0.808349  [ 2528/ 3200]\n",
      "loss: 0.966990  [ 2544/ 3200]\n",
      "loss: 0.953314  [ 2560/ 3200]\n",
      "loss: 0.960380  [ 2576/ 3200]\n",
      "loss: 1.127383  [ 2592/ 3200]\n",
      "loss: 0.951788  [ 2608/ 3200]\n",
      "loss: 0.938817  [ 2624/ 3200]\n",
      "loss: 0.995536  [ 2640/ 3200]\n",
      "loss: 0.754165  [ 2656/ 3200]\n",
      "loss: 0.979030  [ 2672/ 3200]\n",
      "loss: 1.152921  [ 2688/ 3200]\n",
      "loss: 1.104797  [ 2704/ 3200]\n",
      "loss: 0.747586  [ 2720/ 3200]\n",
      "loss: 1.066039  [ 2736/ 3200]\n",
      "loss: 0.795755  [ 2752/ 3200]\n",
      "loss: 0.881898  [ 2768/ 3200]\n",
      "loss: 0.993102  [ 2784/ 3200]\n",
      "loss: 0.986588  [ 2800/ 3200]\n",
      "loss: 1.058868  [ 2816/ 3200]\n",
      "loss: 1.269735  [ 2832/ 3200]\n",
      "loss: 0.991992  [ 2848/ 3200]\n",
      "loss: 0.974225  [ 2864/ 3200]\n",
      "loss: 1.102930  [ 2880/ 3200]\n",
      "loss: 0.852794  [ 2896/ 3200]\n",
      "loss: 1.081478  [ 2912/ 3200]\n",
      "loss: 0.918625  [ 2928/ 3200]\n",
      "loss: 0.875744  [ 2944/ 3200]\n",
      "loss: 0.990564  [ 2960/ 3200]\n",
      "loss: 0.873822  [ 2976/ 3200]\n",
      "loss: 0.943774  [ 2992/ 3200]\n",
      "loss: 1.068246  [ 3008/ 3200]\n",
      "loss: 1.022676  [ 3024/ 3200]\n",
      "loss: 0.778674  [ 3040/ 3200]\n",
      "loss: 1.003072  [ 3056/ 3200]\n",
      "loss: 1.090897  [ 3072/ 3200]\n",
      "loss: 1.041793  [ 3088/ 3200]\n",
      "loss: 0.941698  [ 3104/ 3200]\n",
      "loss: 1.000538  [ 3120/ 3200]\n",
      "loss: 1.005843  [ 3136/ 3200]\n",
      "loss: 0.886251  [ 3152/ 3200]\n",
      "loss: 0.724350  [ 3168/ 3200]\n",
      "loss: 1.175777  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.756654  [    0/ 3200]\n",
      "loss: 0.913983  [   16/ 3200]\n",
      "loss: 0.811719  [   32/ 3200]\n",
      "loss: 0.958929  [   48/ 3200]\n",
      "loss: 1.253775  [   64/ 3200]\n",
      "loss: 0.983721  [   80/ 3200]\n",
      "loss: 1.292787  [   96/ 3200]\n",
      "loss: 1.067006  [  112/ 3200]\n",
      "loss: 1.093500  [  128/ 3200]\n",
      "loss: 0.885041  [  144/ 3200]\n",
      "loss: 0.917987  [  160/ 3200]\n",
      "loss: 0.973617  [  176/ 3200]\n",
      "loss: 0.887606  [  192/ 3200]\n",
      "loss: 0.900300  [  208/ 3200]\n",
      "loss: 1.078255  [  224/ 3200]\n",
      "loss: 1.122700  [  240/ 3200]\n",
      "loss: 0.908590  [  256/ 3200]\n",
      "loss: 0.867199  [  272/ 3200]\n",
      "loss: 0.794387  [  288/ 3200]\n",
      "loss: 0.930533  [  304/ 3200]\n",
      "loss: 0.728424  [  320/ 3200]\n",
      "loss: 0.794643  [  336/ 3200]\n",
      "loss: 1.029067  [  352/ 3200]\n",
      "loss: 1.106993  [  368/ 3200]\n",
      "loss: 1.145895  [  384/ 3200]\n",
      "loss: 0.714938  [  400/ 3200]\n",
      "loss: 1.103828  [  416/ 3200]\n",
      "loss: 0.735645  [  432/ 3200]\n",
      "loss: 0.886303  [  448/ 3200]\n",
      "loss: 1.000790  [  464/ 3200]\n",
      "loss: 0.922518  [  480/ 3200]\n",
      "loss: 1.071153  [  496/ 3200]\n",
      "loss: 1.190819  [  512/ 3200]\n",
      "loss: 1.064050  [  528/ 3200]\n",
      "loss: 1.075357  [  544/ 3200]\n",
      "loss: 1.183360  [  560/ 3200]\n",
      "loss: 0.985281  [  576/ 3200]\n",
      "loss: 1.040156  [  592/ 3200]\n",
      "loss: 0.943124  [  608/ 3200]\n",
      "loss: 0.945925  [  624/ 3200]\n",
      "loss: 0.847004  [  640/ 3200]\n",
      "loss: 0.788772  [  656/ 3200]\n",
      "loss: 0.828773  [  672/ 3200]\n",
      "loss: 1.133417  [  688/ 3200]\n",
      "loss: 1.037922  [  704/ 3200]\n",
      "loss: 0.985819  [  720/ 3200]\n",
      "loss: 0.915411  [  736/ 3200]\n",
      "loss: 0.822360  [  752/ 3200]\n",
      "loss: 1.284062  [  768/ 3200]\n",
      "loss: 1.038771  [  784/ 3200]\n",
      "loss: 0.940942  [  800/ 3200]\n",
      "loss: 1.216590  [  816/ 3200]\n",
      "loss: 0.890405  [  832/ 3200]\n",
      "loss: 1.298283  [  848/ 3200]\n",
      "loss: 1.080018  [  864/ 3200]\n",
      "loss: 1.081280  [  880/ 3200]\n",
      "loss: 0.895777  [  896/ 3200]\n",
      "loss: 0.912143  [  912/ 3200]\n",
      "loss: 0.962405  [  928/ 3200]\n",
      "loss: 0.716974  [  944/ 3200]\n",
      "loss: 1.077847  [  960/ 3200]\n",
      "loss: 0.926888  [  976/ 3200]\n",
      "loss: 0.978910  [  992/ 3200]\n",
      "loss: 0.826402  [ 1008/ 3200]\n",
      "loss: 0.992979  [ 1024/ 3200]\n",
      "loss: 0.928968  [ 1040/ 3200]\n",
      "loss: 0.861871  [ 1056/ 3200]\n",
      "loss: 0.880497  [ 1072/ 3200]\n",
      "loss: 1.024572  [ 1088/ 3200]\n",
      "loss: 1.420141  [ 1104/ 3200]\n",
      "loss: 0.943004  [ 1120/ 3200]\n",
      "loss: 0.947657  [ 1136/ 3200]\n",
      "loss: 0.876278  [ 1152/ 3200]\n",
      "loss: 0.906733  [ 1168/ 3200]\n",
      "loss: 0.880320  [ 1184/ 3200]\n",
      "loss: 0.642992  [ 1200/ 3200]\n",
      "loss: 0.992820  [ 1216/ 3200]\n",
      "loss: 1.020729  [ 1232/ 3200]\n",
      "loss: 1.020323  [ 1248/ 3200]\n",
      "loss: 0.979444  [ 1264/ 3200]\n",
      "loss: 1.010320  [ 1280/ 3200]\n",
      "loss: 0.978136  [ 1296/ 3200]\n",
      "loss: 0.982078  [ 1312/ 3200]\n",
      "loss: 0.909286  [ 1328/ 3200]\n",
      "loss: 1.305151  [ 1344/ 3200]\n",
      "loss: 0.836602  [ 1360/ 3200]\n",
      "loss: 1.208600  [ 1376/ 3200]\n",
      "loss: 1.041061  [ 1392/ 3200]\n",
      "loss: 1.108071  [ 1408/ 3200]\n",
      "loss: 1.126754  [ 1424/ 3200]\n",
      "loss: 0.814743  [ 1440/ 3200]\n",
      "loss: 0.856754  [ 1456/ 3200]\n",
      "loss: 0.902612  [ 1472/ 3200]\n",
      "loss: 1.048669  [ 1488/ 3200]\n",
      "loss: 0.988405  [ 1504/ 3200]\n",
      "loss: 1.004099  [ 1520/ 3200]\n",
      "loss: 1.356751  [ 1536/ 3200]\n",
      "loss: 0.972037  [ 1552/ 3200]\n",
      "loss: 1.084860  [ 1568/ 3200]\n",
      "loss: 0.851403  [ 1584/ 3200]\n",
      "loss: 0.894958  [ 1600/ 3200]\n",
      "loss: 0.982298  [ 1616/ 3200]\n",
      "loss: 0.780556  [ 1632/ 3200]\n",
      "loss: 1.107661  [ 1648/ 3200]\n",
      "loss: 1.282330  [ 1664/ 3200]\n",
      "loss: 0.868605  [ 1680/ 3200]\n",
      "loss: 1.038153  [ 1696/ 3200]\n",
      "loss: 1.021633  [ 1712/ 3200]\n",
      "loss: 1.080471  [ 1728/ 3200]\n",
      "loss: 1.128822  [ 1744/ 3200]\n",
      "loss: 1.186793  [ 1760/ 3200]\n",
      "loss: 0.868164  [ 1776/ 3200]\n",
      "loss: 0.857780  [ 1792/ 3200]\n",
      "loss: 1.180918  [ 1808/ 3200]\n",
      "loss: 1.249133  [ 1824/ 3200]\n",
      "loss: 0.806558  [ 1840/ 3200]\n",
      "loss: 0.831688  [ 1856/ 3200]\n",
      "loss: 0.897504  [ 1872/ 3200]\n",
      "loss: 0.815803  [ 1888/ 3200]\n",
      "loss: 1.085241  [ 1904/ 3200]\n",
      "loss: 1.062621  [ 1920/ 3200]\n",
      "loss: 1.005665  [ 1936/ 3200]\n",
      "loss: 1.118496  [ 1952/ 3200]\n",
      "loss: 0.926401  [ 1968/ 3200]\n",
      "loss: 0.895696  [ 1984/ 3200]\n",
      "loss: 1.009102  [ 2000/ 3200]\n",
      "loss: 0.979482  [ 2016/ 3200]\n",
      "loss: 0.696842  [ 2032/ 3200]\n",
      "loss: 0.873213  [ 2048/ 3200]\n",
      "loss: 0.912121  [ 2064/ 3200]\n",
      "loss: 1.096990  [ 2080/ 3200]\n",
      "loss: 0.773398  [ 2096/ 3200]\n",
      "loss: 1.373951  [ 2112/ 3200]\n",
      "loss: 0.899336  [ 2128/ 3200]\n",
      "loss: 1.071408  [ 2144/ 3200]\n",
      "loss: 1.205176  [ 2160/ 3200]\n",
      "loss: 1.189139  [ 2176/ 3200]\n",
      "loss: 1.025970  [ 2192/ 3200]\n",
      "loss: 1.081077  [ 2208/ 3200]\n",
      "loss: 0.905153  [ 2224/ 3200]\n",
      "loss: 1.083598  [ 2240/ 3200]\n",
      "loss: 1.129324  [ 2256/ 3200]\n",
      "loss: 1.035381  [ 2272/ 3200]\n",
      "loss: 0.902429  [ 2288/ 3200]\n",
      "loss: 1.118034  [ 2304/ 3200]\n",
      "loss: 0.954580  [ 2320/ 3200]\n",
      "loss: 1.022189  [ 2336/ 3200]\n",
      "loss: 0.905582  [ 2352/ 3200]\n",
      "loss: 1.001760  [ 2368/ 3200]\n",
      "loss: 0.839737  [ 2384/ 3200]\n",
      "loss: 0.962215  [ 2400/ 3200]\n",
      "loss: 1.335489  [ 2416/ 3200]\n",
      "loss: 1.015200  [ 2432/ 3200]\n",
      "loss: 1.003709  [ 2448/ 3200]\n",
      "loss: 0.904346  [ 2464/ 3200]\n",
      "loss: 0.971848  [ 2480/ 3200]\n",
      "loss: 0.945764  [ 2496/ 3200]\n",
      "loss: 0.966029  [ 2512/ 3200]\n",
      "loss: 0.972595  [ 2528/ 3200]\n",
      "loss: 1.100605  [ 2544/ 3200]\n",
      "loss: 0.983438  [ 2560/ 3200]\n",
      "loss: 1.096860  [ 2576/ 3200]\n",
      "loss: 0.911472  [ 2592/ 3200]\n",
      "loss: 0.818312  [ 2608/ 3200]\n",
      "loss: 0.810267  [ 2624/ 3200]\n",
      "loss: 0.934234  [ 2640/ 3200]\n",
      "loss: 0.910148  [ 2656/ 3200]\n",
      "loss: 0.862932  [ 2672/ 3200]\n",
      "loss: 1.050958  [ 2688/ 3200]\n",
      "loss: 0.979496  [ 2704/ 3200]\n",
      "loss: 1.089162  [ 2720/ 3200]\n",
      "loss: 0.784012  [ 2736/ 3200]\n",
      "loss: 0.921727  [ 2752/ 3200]\n",
      "loss: 1.119610  [ 2768/ 3200]\n",
      "loss: 0.979599  [ 2784/ 3200]\n",
      "loss: 0.765379  [ 2800/ 3200]\n",
      "loss: 1.117210  [ 2816/ 3200]\n",
      "loss: 1.095005  [ 2832/ 3200]\n",
      "loss: 0.910614  [ 2848/ 3200]\n",
      "loss: 1.072406  [ 2864/ 3200]\n",
      "loss: 0.764160  [ 2880/ 3200]\n",
      "loss: 0.967289  [ 2896/ 3200]\n",
      "loss: 0.729021  [ 2912/ 3200]\n",
      "loss: 1.081294  [ 2928/ 3200]\n",
      "loss: 0.921220  [ 2944/ 3200]\n",
      "loss: 0.965380  [ 2960/ 3200]\n",
      "loss: 1.041309  [ 2976/ 3200]\n",
      "loss: 0.987607  [ 2992/ 3200]\n",
      "loss: 0.994399  [ 3008/ 3200]\n",
      "loss: 1.001005  [ 3024/ 3200]\n",
      "loss: 0.978775  [ 3040/ 3200]\n",
      "loss: 0.946971  [ 3056/ 3200]\n",
      "loss: 1.175887  [ 3072/ 3200]\n",
      "loss: 1.025323  [ 3088/ 3200]\n",
      "loss: 0.835868  [ 3104/ 3200]\n",
      "loss: 1.008236  [ 3120/ 3200]\n",
      "loss: 1.119638  [ 3136/ 3200]\n",
      "loss: 1.138163  [ 3152/ 3200]\n",
      "loss: 0.684039  [ 3168/ 3200]\n",
      "loss: 1.113876  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.966252  [    0/ 3200]\n",
      "loss: 0.918754  [   16/ 3200]\n",
      "loss: 0.746162  [   32/ 3200]\n",
      "loss: 0.856430  [   48/ 3200]\n",
      "loss: 1.073080  [   64/ 3200]\n",
      "loss: 0.783196  [   80/ 3200]\n",
      "loss: 0.948834  [   96/ 3200]\n",
      "loss: 0.980831  [  112/ 3200]\n",
      "loss: 0.811990  [  128/ 3200]\n",
      "loss: 0.935467  [  144/ 3200]\n",
      "loss: 0.764790  [  160/ 3200]\n",
      "loss: 0.694366  [  176/ 3200]\n",
      "loss: 0.900642  [  192/ 3200]\n",
      "loss: 0.988512  [  208/ 3200]\n",
      "loss: 0.945770  [  224/ 3200]\n",
      "loss: 1.074292  [  240/ 3200]\n",
      "loss: 0.949410  [  256/ 3200]\n",
      "loss: 0.738630  [  272/ 3200]\n",
      "loss: 0.662735  [  288/ 3200]\n",
      "loss: 1.032365  [  304/ 3200]\n",
      "loss: 0.891261  [  320/ 3200]\n",
      "loss: 1.069988  [  336/ 3200]\n",
      "loss: 1.142545  [  352/ 3200]\n",
      "loss: 0.908302  [  368/ 3200]\n",
      "loss: 0.978856  [  384/ 3200]\n",
      "loss: 0.879331  [  400/ 3200]\n",
      "loss: 1.147930  [  416/ 3200]\n",
      "loss: 0.756721  [  432/ 3200]\n",
      "loss: 0.948755  [  448/ 3200]\n",
      "loss: 0.893519  [  464/ 3200]\n",
      "loss: 1.135746  [  480/ 3200]\n",
      "loss: 0.800055  [  496/ 3200]\n",
      "loss: 0.742849  [  512/ 3200]\n",
      "loss: 0.823870  [  528/ 3200]\n",
      "loss: 0.906155  [  544/ 3200]\n",
      "loss: 1.110931  [  560/ 3200]\n",
      "loss: 0.798449  [  576/ 3200]\n",
      "loss: 0.700574  [  592/ 3200]\n",
      "loss: 0.777948  [  608/ 3200]\n",
      "loss: 0.883983  [  624/ 3200]\n",
      "loss: 1.344645  [  640/ 3200]\n",
      "loss: 1.014480  [  656/ 3200]\n",
      "loss: 0.836071  [  672/ 3200]\n",
      "loss: 1.001408  [  688/ 3200]\n",
      "loss: 1.013028  [  704/ 3200]\n",
      "loss: 0.952563  [  720/ 3200]\n",
      "loss: 1.026592  [  736/ 3200]\n",
      "loss: 1.066065  [  752/ 3200]\n",
      "loss: 1.180134  [  768/ 3200]\n",
      "loss: 1.102189  [  784/ 3200]\n",
      "loss: 1.153730  [  800/ 3200]\n",
      "loss: 0.862692  [  816/ 3200]\n",
      "loss: 0.921738  [  832/ 3200]\n",
      "loss: 1.036553  [  848/ 3200]\n",
      "loss: 0.759369  [  864/ 3200]\n",
      "loss: 0.909321  [  880/ 3200]\n",
      "loss: 1.019158  [  896/ 3200]\n",
      "loss: 0.950798  [  912/ 3200]\n",
      "loss: 1.095169  [  928/ 3200]\n",
      "loss: 0.944032  [  944/ 3200]\n",
      "loss: 0.878836  [  960/ 3200]\n",
      "loss: 1.223679  [  976/ 3200]\n",
      "loss: 0.962241  [  992/ 3200]\n",
      "loss: 0.788227  [ 1008/ 3200]\n",
      "loss: 0.779458  [ 1024/ 3200]\n",
      "loss: 1.114516  [ 1040/ 3200]\n",
      "loss: 0.919992  [ 1056/ 3200]\n",
      "loss: 1.054316  [ 1072/ 3200]\n",
      "loss: 0.805938  [ 1088/ 3200]\n",
      "loss: 0.951773  [ 1104/ 3200]\n",
      "loss: 1.074646  [ 1120/ 3200]\n",
      "loss: 0.903394  [ 1136/ 3200]\n",
      "loss: 0.867800  [ 1152/ 3200]\n",
      "loss: 0.817606  [ 1168/ 3200]\n",
      "loss: 1.089885  [ 1184/ 3200]\n",
      "loss: 1.077239  [ 1200/ 3200]\n",
      "loss: 1.051987  [ 1216/ 3200]\n",
      "loss: 1.117505  [ 1232/ 3200]\n",
      "loss: 0.902612  [ 1248/ 3200]\n",
      "loss: 1.464789  [ 1264/ 3200]\n",
      "loss: 1.135545  [ 1280/ 3200]\n",
      "loss: 1.045537  [ 1296/ 3200]\n",
      "loss: 0.908338  [ 1312/ 3200]\n",
      "loss: 1.024156  [ 1328/ 3200]\n",
      "loss: 1.045054  [ 1344/ 3200]\n",
      "loss: 1.352020  [ 1360/ 3200]\n",
      "loss: 0.989815  [ 1376/ 3200]\n",
      "loss: 1.254618  [ 1392/ 3200]\n",
      "loss: 0.970866  [ 1408/ 3200]\n",
      "loss: 0.924805  [ 1424/ 3200]\n",
      "loss: 1.062941  [ 1440/ 3200]\n",
      "loss: 0.709984  [ 1456/ 3200]\n",
      "loss: 0.920657  [ 1472/ 3200]\n",
      "loss: 0.956742  [ 1488/ 3200]\n",
      "loss: 1.031972  [ 1504/ 3200]\n",
      "loss: 0.737303  [ 1520/ 3200]\n",
      "loss: 0.995870  [ 1536/ 3200]\n",
      "loss: 0.909141  [ 1552/ 3200]\n",
      "loss: 0.815034  [ 1568/ 3200]\n",
      "loss: 1.071080  [ 1584/ 3200]\n",
      "loss: 0.998328  [ 1600/ 3200]\n",
      "loss: 0.983182  [ 1616/ 3200]\n",
      "loss: 1.263258  [ 1632/ 3200]\n",
      "loss: 0.838380  [ 1648/ 3200]\n",
      "loss: 1.250626  [ 1664/ 3200]\n",
      "loss: 1.128305  [ 1680/ 3200]\n",
      "loss: 1.081582  [ 1696/ 3200]\n",
      "loss: 1.133654  [ 1712/ 3200]\n",
      "loss: 0.976649  [ 1728/ 3200]\n",
      "loss: 0.884090  [ 1744/ 3200]\n",
      "loss: 0.896625  [ 1760/ 3200]\n",
      "loss: 0.886078  [ 1776/ 3200]\n",
      "loss: 0.931966  [ 1792/ 3200]\n",
      "loss: 1.044479  [ 1808/ 3200]\n",
      "loss: 1.126465  [ 1824/ 3200]\n",
      "loss: 0.969095  [ 1840/ 3200]\n",
      "loss: 0.944221  [ 1856/ 3200]\n",
      "loss: 1.064338  [ 1872/ 3200]\n",
      "loss: 1.204377  [ 1888/ 3200]\n",
      "loss: 0.905048  [ 1904/ 3200]\n",
      "loss: 0.860995  [ 1920/ 3200]\n",
      "loss: 1.392845  [ 1936/ 3200]\n",
      "loss: 1.170262  [ 1952/ 3200]\n",
      "loss: 0.828101  [ 1968/ 3200]\n",
      "loss: 0.969703  [ 1984/ 3200]\n",
      "loss: 1.244070  [ 2000/ 3200]\n",
      "loss: 1.057102  [ 2016/ 3200]\n",
      "loss: 0.959568  [ 2032/ 3200]\n",
      "loss: 1.013667  [ 2048/ 3200]\n",
      "loss: 1.186203  [ 2064/ 3200]\n",
      "loss: 0.887053  [ 2080/ 3200]\n",
      "loss: 1.121777  [ 2096/ 3200]\n",
      "loss: 1.319018  [ 2112/ 3200]\n",
      "loss: 0.863704  [ 2128/ 3200]\n",
      "loss: 0.932820  [ 2144/ 3200]\n",
      "loss: 0.946229  [ 2160/ 3200]\n",
      "loss: 1.176116  [ 2176/ 3200]\n",
      "loss: 1.146806  [ 2192/ 3200]\n",
      "loss: 0.818305  [ 2208/ 3200]\n",
      "loss: 0.941284  [ 2224/ 3200]\n",
      "loss: 1.051694  [ 2240/ 3200]\n",
      "loss: 0.781852  [ 2256/ 3200]\n",
      "loss: 1.216822  [ 2272/ 3200]\n",
      "loss: 0.888806  [ 2288/ 3200]\n",
      "loss: 0.864253  [ 2304/ 3200]\n",
      "loss: 1.065527  [ 2320/ 3200]\n",
      "loss: 1.282896  [ 2336/ 3200]\n",
      "loss: 0.996448  [ 2352/ 3200]\n",
      "loss: 0.949285  [ 2368/ 3200]\n",
      "loss: 0.796294  [ 2384/ 3200]\n",
      "loss: 1.076081  [ 2400/ 3200]\n",
      "loss: 0.869028  [ 2416/ 3200]\n",
      "loss: 0.879263  [ 2432/ 3200]\n",
      "loss: 1.042667  [ 2448/ 3200]\n",
      "loss: 0.995454  [ 2464/ 3200]\n",
      "loss: 1.211680  [ 2480/ 3200]\n",
      "loss: 0.812533  [ 2496/ 3200]\n",
      "loss: 0.984474  [ 2512/ 3200]\n",
      "loss: 0.685757  [ 2528/ 3200]\n",
      "loss: 0.851786  [ 2544/ 3200]\n",
      "loss: 0.962406  [ 2560/ 3200]\n",
      "loss: 0.843832  [ 2576/ 3200]\n",
      "loss: 1.066038  [ 2592/ 3200]\n",
      "loss: 0.845807  [ 2608/ 3200]\n",
      "loss: 0.833274  [ 2624/ 3200]\n",
      "loss: 1.069713  [ 2640/ 3200]\n",
      "loss: 1.156673  [ 2656/ 3200]\n",
      "loss: 1.140847  [ 2672/ 3200]\n",
      "loss: 1.049847  [ 2688/ 3200]\n",
      "loss: 0.897972  [ 2704/ 3200]\n",
      "loss: 1.120036  [ 2720/ 3200]\n",
      "loss: 0.847854  [ 2736/ 3200]\n",
      "loss: 0.912738  [ 2752/ 3200]\n",
      "loss: 1.061291  [ 2768/ 3200]\n",
      "loss: 1.233871  [ 2784/ 3200]\n",
      "loss: 0.904600  [ 2800/ 3200]\n",
      "loss: 1.082787  [ 2816/ 3200]\n",
      "loss: 1.134778  [ 2832/ 3200]\n",
      "loss: 1.100763  [ 2848/ 3200]\n",
      "loss: 1.387693  [ 2864/ 3200]\n",
      "loss: 0.914951  [ 2880/ 3200]\n",
      "loss: 1.167099  [ 2896/ 3200]\n",
      "loss: 1.059688  [ 2912/ 3200]\n",
      "loss: 1.075717  [ 2928/ 3200]\n",
      "loss: 0.954344  [ 2944/ 3200]\n",
      "loss: 1.078771  [ 2960/ 3200]\n",
      "loss: 1.158325  [ 2976/ 3200]\n",
      "loss: 0.880923  [ 2992/ 3200]\n",
      "loss: 0.851583  [ 3008/ 3200]\n",
      "loss: 0.858030  [ 3024/ 3200]\n",
      "loss: 0.848161  [ 3040/ 3200]\n",
      "loss: 0.940775  [ 3056/ 3200]\n",
      "loss: 1.143496  [ 3072/ 3200]\n",
      "loss: 1.104524  [ 3088/ 3200]\n",
      "loss: 1.139525  [ 3104/ 3200]\n",
      "loss: 1.005286  [ 3120/ 3200]\n",
      "loss: 0.702661  [ 3136/ 3200]\n",
      "loss: 0.976384  [ 3152/ 3200]\n",
      "loss: 0.778239  [ 3168/ 3200]\n",
      "loss: 1.070434  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 1.076113  [    0/ 3200]\n",
      "loss: 1.117061  [   16/ 3200]\n",
      "loss: 0.960299  [   32/ 3200]\n",
      "loss: 1.214957  [   48/ 3200]\n",
      "loss: 0.882444  [   64/ 3200]\n",
      "loss: 1.196502  [   80/ 3200]\n",
      "loss: 0.952977  [   96/ 3200]\n",
      "loss: 0.845270  [  112/ 3200]\n",
      "loss: 0.859008  [  128/ 3200]\n",
      "loss: 1.121908  [  144/ 3200]\n",
      "loss: 0.842187  [  160/ 3200]\n",
      "loss: 1.063826  [  176/ 3200]\n",
      "loss: 1.047479  [  192/ 3200]\n",
      "loss: 0.924810  [  208/ 3200]\n",
      "loss: 1.109555  [  224/ 3200]\n",
      "loss: 1.078671  [  240/ 3200]\n",
      "loss: 1.010116  [  256/ 3200]\n",
      "loss: 1.126142  [  272/ 3200]\n",
      "loss: 0.649636  [  288/ 3200]\n",
      "loss: 0.840107  [  304/ 3200]\n",
      "loss: 0.746222  [  320/ 3200]\n",
      "loss: 1.105279  [  336/ 3200]\n",
      "loss: 0.777371  [  352/ 3200]\n",
      "loss: 0.851344  [  368/ 3200]\n",
      "loss: 1.211428  [  384/ 3200]\n",
      "loss: 0.982341  [  400/ 3200]\n",
      "loss: 0.780116  [  416/ 3200]\n",
      "loss: 0.999638  [  432/ 3200]\n",
      "loss: 0.964782  [  448/ 3200]\n",
      "loss: 0.875891  [  464/ 3200]\n",
      "loss: 1.098935  [  480/ 3200]\n",
      "loss: 0.869209  [  496/ 3200]\n",
      "loss: 0.952344  [  512/ 3200]\n",
      "loss: 1.125533  [  528/ 3200]\n",
      "loss: 1.103281  [  544/ 3200]\n",
      "loss: 0.965198  [  560/ 3200]\n",
      "loss: 1.212993  [  576/ 3200]\n",
      "loss: 0.909448  [  592/ 3200]\n",
      "loss: 0.922432  [  608/ 3200]\n",
      "loss: 1.027383  [  624/ 3200]\n",
      "loss: 0.819428  [  640/ 3200]\n",
      "loss: 0.758705  [  656/ 3200]\n",
      "loss: 1.051451  [  672/ 3200]\n",
      "loss: 0.998230  [  688/ 3200]\n",
      "loss: 0.787949  [  704/ 3200]\n",
      "loss: 0.897599  [  720/ 3200]\n",
      "loss: 0.946496  [  736/ 3200]\n",
      "loss: 1.020854  [  752/ 3200]\n",
      "loss: 1.157772  [  768/ 3200]\n",
      "loss: 0.833785  [  784/ 3200]\n",
      "loss: 1.250325  [  800/ 3200]\n",
      "loss: 0.793500  [  816/ 3200]\n",
      "loss: 1.019590  [  832/ 3200]\n",
      "loss: 1.055081  [  848/ 3200]\n",
      "loss: 1.032337  [  864/ 3200]\n",
      "loss: 1.217309  [  880/ 3200]\n",
      "loss: 1.236156  [  896/ 3200]\n",
      "loss: 0.666078  [  912/ 3200]\n",
      "loss: 0.936356  [  928/ 3200]\n",
      "loss: 0.816066  [  944/ 3200]\n",
      "loss: 1.050899  [  960/ 3200]\n",
      "loss: 1.024100  [  976/ 3200]\n",
      "loss: 0.976664  [  992/ 3200]\n",
      "loss: 1.062218  [ 1008/ 3200]\n",
      "loss: 0.799017  [ 1024/ 3200]\n",
      "loss: 0.911210  [ 1040/ 3200]\n",
      "loss: 1.115822  [ 1056/ 3200]\n",
      "loss: 1.426678  [ 1072/ 3200]\n",
      "loss: 1.060207  [ 1088/ 3200]\n",
      "loss: 1.040869  [ 1104/ 3200]\n",
      "loss: 1.357625  [ 1120/ 3200]\n",
      "loss: 0.819749  [ 1136/ 3200]\n",
      "loss: 0.887974  [ 1152/ 3200]\n",
      "loss: 1.030553  [ 1168/ 3200]\n",
      "loss: 0.868101  [ 1184/ 3200]\n",
      "loss: 0.971275  [ 1200/ 3200]\n",
      "loss: 1.075778  [ 1216/ 3200]\n",
      "loss: 0.934382  [ 1232/ 3200]\n",
      "loss: 1.087088  [ 1248/ 3200]\n",
      "loss: 0.770522  [ 1264/ 3200]\n",
      "loss: 0.912096  [ 1280/ 3200]\n",
      "loss: 0.926343  [ 1296/ 3200]\n",
      "loss: 0.984743  [ 1312/ 3200]\n",
      "loss: 0.941392  [ 1328/ 3200]\n",
      "loss: 1.037207  [ 1344/ 3200]\n",
      "loss: 1.174955  [ 1360/ 3200]\n",
      "loss: 0.745113  [ 1376/ 3200]\n",
      "loss: 1.282523  [ 1392/ 3200]\n",
      "loss: 1.279259  [ 1408/ 3200]\n",
      "loss: 1.033968  [ 1424/ 3200]\n",
      "loss: 1.047253  [ 1440/ 3200]\n",
      "loss: 0.917349  [ 1456/ 3200]\n",
      "loss: 0.977654  [ 1472/ 3200]\n",
      "loss: 1.152190  [ 1488/ 3200]\n",
      "loss: 1.025362  [ 1504/ 3200]\n",
      "loss: 1.128253  [ 1520/ 3200]\n",
      "loss: 0.974682  [ 1536/ 3200]\n",
      "loss: 1.140028  [ 1552/ 3200]\n",
      "loss: 1.056049  [ 1568/ 3200]\n",
      "loss: 1.070804  [ 1584/ 3200]\n",
      "loss: 1.011191  [ 1600/ 3200]\n",
      "loss: 0.877949  [ 1616/ 3200]\n",
      "loss: 1.009769  [ 1632/ 3200]\n",
      "loss: 0.845356  [ 1648/ 3200]\n",
      "loss: 1.101140  [ 1664/ 3200]\n",
      "loss: 0.931795  [ 1680/ 3200]\n",
      "loss: 0.862290  [ 1696/ 3200]\n",
      "loss: 0.884409  [ 1712/ 3200]\n",
      "loss: 1.369613  [ 1728/ 3200]\n",
      "loss: 1.056902  [ 1744/ 3200]\n",
      "loss: 1.029482  [ 1760/ 3200]\n",
      "loss: 0.878202  [ 1776/ 3200]\n",
      "loss: 0.977642  [ 1792/ 3200]\n",
      "loss: 1.255318  [ 1808/ 3200]\n",
      "loss: 0.829727  [ 1824/ 3200]\n",
      "loss: 0.876021  [ 1840/ 3200]\n",
      "loss: 0.879532  [ 1856/ 3200]\n",
      "loss: 0.904882  [ 1872/ 3200]\n",
      "loss: 0.936368  [ 1888/ 3200]\n",
      "loss: 0.935401  [ 1904/ 3200]\n",
      "loss: 1.121200  [ 1920/ 3200]\n",
      "loss: 0.916341  [ 1936/ 3200]\n",
      "loss: 0.948831  [ 1952/ 3200]\n",
      "loss: 0.740014  [ 1968/ 3200]\n",
      "loss: 0.879451  [ 1984/ 3200]\n",
      "loss: 1.075985  [ 2000/ 3200]\n",
      "loss: 1.246634  [ 2016/ 3200]\n",
      "loss: 1.028920  [ 2032/ 3200]\n",
      "loss: 1.087737  [ 2048/ 3200]\n",
      "loss: 1.079434  [ 2064/ 3200]\n",
      "loss: 0.649915  [ 2080/ 3200]\n",
      "loss: 0.925743  [ 2096/ 3200]\n",
      "loss: 1.060078  [ 2112/ 3200]\n",
      "loss: 0.767851  [ 2128/ 3200]\n",
      "loss: 0.985233  [ 2144/ 3200]\n",
      "loss: 1.086921  [ 2160/ 3200]\n",
      "loss: 1.066026  [ 2176/ 3200]\n",
      "loss: 0.894424  [ 2192/ 3200]\n",
      "loss: 0.994756  [ 2208/ 3200]\n",
      "loss: 0.792294  [ 2224/ 3200]\n",
      "loss: 0.788710  [ 2240/ 3200]\n",
      "loss: 0.773959  [ 2256/ 3200]\n",
      "loss: 1.030902  [ 2272/ 3200]\n",
      "loss: 0.851786  [ 2288/ 3200]\n",
      "loss: 1.081442  [ 2304/ 3200]\n",
      "loss: 0.883386  [ 2320/ 3200]\n",
      "loss: 0.967890  [ 2336/ 3200]\n",
      "loss: 1.162789  [ 2352/ 3200]\n",
      "loss: 0.950744  [ 2368/ 3200]\n",
      "loss: 0.933711  [ 2384/ 3200]\n",
      "loss: 0.862963  [ 2400/ 3200]\n",
      "loss: 0.869263  [ 2416/ 3200]\n",
      "loss: 0.890630  [ 2432/ 3200]\n",
      "loss: 1.040130  [ 2448/ 3200]\n",
      "loss: 0.933512  [ 2464/ 3200]\n",
      "loss: 0.934481  [ 2480/ 3200]\n",
      "loss: 1.119432  [ 2496/ 3200]\n",
      "loss: 1.379572  [ 2512/ 3200]\n",
      "loss: 0.873990  [ 2528/ 3200]\n",
      "loss: 1.020841  [ 2544/ 3200]\n",
      "loss: 1.321669  [ 2560/ 3200]\n",
      "loss: 1.003419  [ 2576/ 3200]\n",
      "loss: 0.944023  [ 2592/ 3200]\n",
      "loss: 0.804939  [ 2608/ 3200]\n",
      "loss: 1.284364  [ 2624/ 3200]\n",
      "loss: 0.926947  [ 2640/ 3200]\n",
      "loss: 0.923177  [ 2656/ 3200]\n",
      "loss: 0.768709  [ 2672/ 3200]\n",
      "loss: 0.998778  [ 2688/ 3200]\n",
      "loss: 1.072322  [ 2704/ 3200]\n",
      "loss: 0.804758  [ 2720/ 3200]\n",
      "loss: 0.939324  [ 2736/ 3200]\n",
      "loss: 0.989094  [ 2752/ 3200]\n",
      "loss: 0.824290  [ 2768/ 3200]\n",
      "loss: 0.951001  [ 2784/ 3200]\n",
      "loss: 1.156946  [ 2800/ 3200]\n",
      "loss: 0.993287  [ 2816/ 3200]\n",
      "loss: 0.991389  [ 2832/ 3200]\n",
      "loss: 0.899224  [ 2848/ 3200]\n",
      "loss: 1.161356  [ 2864/ 3200]\n",
      "loss: 0.866730  [ 2880/ 3200]\n",
      "loss: 0.763890  [ 2896/ 3200]\n",
      "loss: 0.911615  [ 2912/ 3200]\n",
      "loss: 0.716106  [ 2928/ 3200]\n",
      "loss: 0.972629  [ 2944/ 3200]\n",
      "loss: 1.021475  [ 2960/ 3200]\n",
      "loss: 0.967373  [ 2976/ 3200]\n",
      "loss: 1.012376  [ 2992/ 3200]\n",
      "loss: 0.693013  [ 3008/ 3200]\n",
      "loss: 0.740389  [ 3024/ 3200]\n",
      "loss: 0.927470  [ 3040/ 3200]\n",
      "loss: 0.922556  [ 3056/ 3200]\n",
      "loss: 0.919824  [ 3072/ 3200]\n",
      "loss: 0.982213  [ 3088/ 3200]\n",
      "loss: 0.836265  [ 3104/ 3200]\n",
      "loss: 1.009235  [ 3120/ 3200]\n",
      "loss: 1.115131  [ 3136/ 3200]\n",
      "loss: 1.114794  [ 3152/ 3200]\n",
      "loss: 0.881017  [ 3168/ 3200]\n",
      "loss: 0.697340  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 1.066331  [    0/ 3200]\n",
      "loss: 0.849925  [   16/ 3200]\n",
      "loss: 1.106594  [   32/ 3200]\n",
      "loss: 0.828939  [   48/ 3200]\n",
      "loss: 1.179586  [   64/ 3200]\n",
      "loss: 0.845708  [   80/ 3200]\n",
      "loss: 0.973250  [   96/ 3200]\n",
      "loss: 1.201949  [  112/ 3200]\n",
      "loss: 0.911884  [  128/ 3200]\n",
      "loss: 0.971712  [  144/ 3200]\n",
      "loss: 0.980398  [  160/ 3200]\n",
      "loss: 0.875423  [  176/ 3200]\n",
      "loss: 0.921260  [  192/ 3200]\n",
      "loss: 1.067697  [  208/ 3200]\n",
      "loss: 0.863433  [  224/ 3200]\n",
      "loss: 1.079567  [  240/ 3200]\n",
      "loss: 1.011709  [  256/ 3200]\n",
      "loss: 1.025522  [  272/ 3200]\n",
      "loss: 1.024630  [  288/ 3200]\n",
      "loss: 1.028990  [  304/ 3200]\n",
      "loss: 0.800419  [  320/ 3200]\n",
      "loss: 0.949550  [  336/ 3200]\n",
      "loss: 0.859577  [  352/ 3200]\n",
      "loss: 1.031179  [  368/ 3200]\n",
      "loss: 0.870593  [  384/ 3200]\n",
      "loss: 0.988901  [  400/ 3200]\n",
      "loss: 0.741978  [  416/ 3200]\n",
      "loss: 0.925126  [  432/ 3200]\n",
      "loss: 0.902111  [  448/ 3200]\n",
      "loss: 0.806471  [  464/ 3200]\n",
      "loss: 1.139367  [  480/ 3200]\n",
      "loss: 0.836841  [  496/ 3200]\n",
      "loss: 0.925184  [  512/ 3200]\n",
      "loss: 0.928567  [  528/ 3200]\n",
      "loss: 1.045623  [  544/ 3200]\n",
      "loss: 0.792350  [  560/ 3200]\n",
      "loss: 1.073609  [  576/ 3200]\n",
      "loss: 1.034430  [  592/ 3200]\n",
      "loss: 0.795162  [  608/ 3200]\n",
      "loss: 1.155258  [  624/ 3200]\n",
      "loss: 0.857174  [  640/ 3200]\n",
      "loss: 1.220573  [  656/ 3200]\n",
      "loss: 0.771618  [  672/ 3200]\n",
      "loss: 0.928706  [  688/ 3200]\n",
      "loss: 0.884090  [  704/ 3200]\n",
      "loss: 1.055414  [  720/ 3200]\n",
      "loss: 0.834379  [  736/ 3200]\n",
      "loss: 1.008637  [  752/ 3200]\n",
      "loss: 0.920365  [  768/ 3200]\n",
      "loss: 1.175308  [  784/ 3200]\n",
      "loss: 1.086566  [  800/ 3200]\n",
      "loss: 0.702874  [  816/ 3200]\n",
      "loss: 0.871671  [  832/ 3200]\n",
      "loss: 1.074452  [  848/ 3200]\n",
      "loss: 1.275747  [  864/ 3200]\n",
      "loss: 0.842889  [  880/ 3200]\n",
      "loss: 0.838264  [  896/ 3200]\n",
      "loss: 0.665126  [  912/ 3200]\n",
      "loss: 0.743419  [  928/ 3200]\n",
      "loss: 0.847511  [  944/ 3200]\n",
      "loss: 1.030587  [  960/ 3200]\n",
      "loss: 1.123014  [  976/ 3200]\n",
      "loss: 0.972276  [  992/ 3200]\n",
      "loss: 1.005636  [ 1008/ 3200]\n",
      "loss: 0.865800  [ 1024/ 3200]\n",
      "loss: 1.033978  [ 1040/ 3200]\n",
      "loss: 0.672425  [ 1056/ 3200]\n",
      "loss: 0.986146  [ 1072/ 3200]\n",
      "loss: 0.847100  [ 1088/ 3200]\n",
      "loss: 0.959085  [ 1104/ 3200]\n",
      "loss: 0.741046  [ 1120/ 3200]\n",
      "loss: 0.988084  [ 1136/ 3200]\n",
      "loss: 0.745233  [ 1152/ 3200]\n",
      "loss: 0.867497  [ 1168/ 3200]\n",
      "loss: 0.980010  [ 1184/ 3200]\n",
      "loss: 0.781633  [ 1200/ 3200]\n",
      "loss: 1.316399  [ 1216/ 3200]\n",
      "loss: 1.004345  [ 1232/ 3200]\n",
      "loss: 0.728408  [ 1248/ 3200]\n",
      "loss: 1.053099  [ 1264/ 3200]\n",
      "loss: 0.801118  [ 1280/ 3200]\n",
      "loss: 1.229326  [ 1296/ 3200]\n",
      "loss: 0.818214  [ 1312/ 3200]\n",
      "loss: 0.964081  [ 1328/ 3200]\n",
      "loss: 0.772784  [ 1344/ 3200]\n",
      "loss: 0.916251  [ 1360/ 3200]\n",
      "loss: 0.749285  [ 1376/ 3200]\n",
      "loss: 1.138200  [ 1392/ 3200]\n",
      "loss: 1.019382  [ 1408/ 3200]\n",
      "loss: 1.079749  [ 1424/ 3200]\n",
      "loss: 1.175741  [ 1440/ 3200]\n",
      "loss: 0.960614  [ 1456/ 3200]\n",
      "loss: 1.064458  [ 1472/ 3200]\n",
      "loss: 0.704452  [ 1488/ 3200]\n",
      "loss: 0.906340  [ 1504/ 3200]\n",
      "loss: 0.934987  [ 1520/ 3200]\n",
      "loss: 0.995500  [ 1536/ 3200]\n",
      "loss: 1.245523  [ 1552/ 3200]\n",
      "loss: 0.925581  [ 1568/ 3200]\n",
      "loss: 1.203979  [ 1584/ 3200]\n",
      "loss: 0.842645  [ 1600/ 3200]\n",
      "loss: 0.793294  [ 1616/ 3200]\n",
      "loss: 0.838265  [ 1632/ 3200]\n",
      "loss: 1.039970  [ 1648/ 3200]\n",
      "loss: 1.176669  [ 1664/ 3200]\n",
      "loss: 0.937193  [ 1680/ 3200]\n",
      "loss: 0.832525  [ 1696/ 3200]\n",
      "loss: 1.137230  [ 1712/ 3200]\n",
      "loss: 1.125430  [ 1728/ 3200]\n",
      "loss: 1.232307  [ 1744/ 3200]\n",
      "loss: 0.829544  [ 1760/ 3200]\n",
      "loss: 0.861220  [ 1776/ 3200]\n",
      "loss: 0.847666  [ 1792/ 3200]\n",
      "loss: 0.958173  [ 1808/ 3200]\n",
      "loss: 0.983911  [ 1824/ 3200]\n",
      "loss: 1.073606  [ 1840/ 3200]\n",
      "loss: 1.025812  [ 1856/ 3200]\n",
      "loss: 0.835773  [ 1872/ 3200]\n",
      "loss: 0.965648  [ 1888/ 3200]\n",
      "loss: 1.167290  [ 1904/ 3200]\n",
      "loss: 1.104493  [ 1920/ 3200]\n",
      "loss: 0.937279  [ 1936/ 3200]\n",
      "loss: 0.846436  [ 1952/ 3200]\n",
      "loss: 0.938518  [ 1968/ 3200]\n",
      "loss: 0.871466  [ 1984/ 3200]\n",
      "loss: 1.230804  [ 2000/ 3200]\n",
      "loss: 1.043856  [ 2016/ 3200]\n",
      "loss: 0.940087  [ 2032/ 3200]\n",
      "loss: 0.940921  [ 2048/ 3200]\n",
      "loss: 0.879367  [ 2064/ 3200]\n",
      "loss: 0.862331  [ 2080/ 3200]\n",
      "loss: 0.889723  [ 2096/ 3200]\n",
      "loss: 1.017958  [ 2112/ 3200]\n",
      "loss: 1.086119  [ 2128/ 3200]\n",
      "loss: 0.817492  [ 2144/ 3200]\n",
      "loss: 1.011170  [ 2160/ 3200]\n",
      "loss: 0.902577  [ 2176/ 3200]\n",
      "loss: 0.727287  [ 2192/ 3200]\n",
      "loss: 0.871370  [ 2208/ 3200]\n",
      "loss: 0.828207  [ 2224/ 3200]\n",
      "loss: 1.023460  [ 2240/ 3200]\n",
      "loss: 1.209205  [ 2256/ 3200]\n",
      "loss: 0.924168  [ 2272/ 3200]\n",
      "loss: 0.967460  [ 2288/ 3200]\n",
      "loss: 1.049044  [ 2304/ 3200]\n",
      "loss: 0.842041  [ 2320/ 3200]\n",
      "loss: 0.864403  [ 2336/ 3200]\n",
      "loss: 0.818949  [ 2352/ 3200]\n",
      "loss: 1.412118  [ 2368/ 3200]\n",
      "loss: 0.930895  [ 2384/ 3200]\n",
      "loss: 1.050504  [ 2400/ 3200]\n",
      "loss: 0.980155  [ 2416/ 3200]\n",
      "loss: 0.986425  [ 2432/ 3200]\n",
      "loss: 0.803143  [ 2448/ 3200]\n",
      "loss: 0.856921  [ 2464/ 3200]\n",
      "loss: 1.012474  [ 2480/ 3200]\n",
      "loss: 0.932417  [ 2496/ 3200]\n",
      "loss: 0.943864  [ 2512/ 3200]\n",
      "loss: 1.014384  [ 2528/ 3200]\n",
      "loss: 1.044883  [ 2544/ 3200]\n",
      "loss: 0.867581  [ 2560/ 3200]\n",
      "loss: 0.890947  [ 2576/ 3200]\n",
      "loss: 0.922599  [ 2592/ 3200]\n",
      "loss: 0.776686  [ 2608/ 3200]\n",
      "loss: 0.861631  [ 2624/ 3200]\n",
      "loss: 0.915759  [ 2640/ 3200]\n",
      "loss: 0.818652  [ 2656/ 3200]\n",
      "loss: 0.806935  [ 2672/ 3200]\n",
      "loss: 1.032668  [ 2688/ 3200]\n",
      "loss: 0.961206  [ 2704/ 3200]\n",
      "loss: 1.021474  [ 2720/ 3200]\n",
      "loss: 1.049846  [ 2736/ 3200]\n",
      "loss: 1.106885  [ 2752/ 3200]\n",
      "loss: 0.931325  [ 2768/ 3200]\n",
      "loss: 1.062941  [ 2784/ 3200]\n",
      "loss: 0.957713  [ 2800/ 3200]\n",
      "loss: 1.072555  [ 2816/ 3200]\n",
      "loss: 1.146712  [ 2832/ 3200]\n",
      "loss: 1.300106  [ 2848/ 3200]\n",
      "loss: 1.133428  [ 2864/ 3200]\n",
      "loss: 1.202052  [ 2880/ 3200]\n",
      "loss: 0.872501  [ 2896/ 3200]\n",
      "loss: 0.976833  [ 2912/ 3200]\n",
      "loss: 0.715474  [ 2928/ 3200]\n",
      "loss: 1.237735  [ 2944/ 3200]\n",
      "loss: 0.883725  [ 2960/ 3200]\n",
      "loss: 1.039087  [ 2976/ 3200]\n",
      "loss: 1.330667  [ 2992/ 3200]\n",
      "loss: 0.890212  [ 3008/ 3200]\n",
      "loss: 0.896146  [ 3024/ 3200]\n",
      "loss: 0.828915  [ 3040/ 3200]\n",
      "loss: 0.870996  [ 3056/ 3200]\n",
      "loss: 1.235867  [ 3072/ 3200]\n",
      "loss: 0.882958  [ 3088/ 3200]\n",
      "loss: 1.036184  [ 3104/ 3200]\n",
      "loss: 0.866027  [ 3120/ 3200]\n",
      "loss: 0.921940  [ 3136/ 3200]\n",
      "loss: 1.461635  [ 3152/ 3200]\n",
      "loss: 1.030803  [ 3168/ 3200]\n",
      "loss: 0.963767  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.901166  [    0/ 3200]\n",
      "loss: 0.951502  [   16/ 3200]\n",
      "loss: 0.858324  [   32/ 3200]\n",
      "loss: 0.685358  [   48/ 3200]\n",
      "loss: 0.896581  [   64/ 3200]\n",
      "loss: 0.899822  [   80/ 3200]\n",
      "loss: 0.736401  [   96/ 3200]\n",
      "loss: 0.997818  [  112/ 3200]\n",
      "loss: 0.993008  [  128/ 3200]\n",
      "loss: 1.240991  [  144/ 3200]\n",
      "loss: 1.129844  [  160/ 3200]\n",
      "loss: 1.020101  [  176/ 3200]\n",
      "loss: 0.940213  [  192/ 3200]\n",
      "loss: 0.730808  [  208/ 3200]\n",
      "loss: 0.941893  [  224/ 3200]\n",
      "loss: 0.851539  [  240/ 3200]\n",
      "loss: 0.933533  [  256/ 3200]\n",
      "loss: 1.053274  [  272/ 3200]\n",
      "loss: 0.890409  [  288/ 3200]\n",
      "loss: 0.878452  [  304/ 3200]\n",
      "loss: 0.881552  [  320/ 3200]\n",
      "loss: 0.889203  [  336/ 3200]\n",
      "loss: 1.018719  [  352/ 3200]\n",
      "loss: 0.998997  [  368/ 3200]\n",
      "loss: 1.028098  [  384/ 3200]\n",
      "loss: 0.890828  [  400/ 3200]\n",
      "loss: 1.315669  [  416/ 3200]\n",
      "loss: 1.017932  [  432/ 3200]\n",
      "loss: 1.102318  [  448/ 3200]\n",
      "loss: 0.863462  [  464/ 3200]\n",
      "loss: 0.973421  [  480/ 3200]\n",
      "loss: 0.868865  [  496/ 3200]\n",
      "loss: 1.083926  [  512/ 3200]\n",
      "loss: 0.807742  [  528/ 3200]\n",
      "loss: 0.796119  [  544/ 3200]\n",
      "loss: 0.897265  [  560/ 3200]\n",
      "loss: 0.824343  [  576/ 3200]\n",
      "loss: 0.984540  [  592/ 3200]\n",
      "loss: 0.803625  [  608/ 3200]\n",
      "loss: 0.976779  [  624/ 3200]\n",
      "loss: 0.790133  [  640/ 3200]\n",
      "loss: 1.186745  [  656/ 3200]\n",
      "loss: 0.900159  [  672/ 3200]\n",
      "loss: 1.110490  [  688/ 3200]\n",
      "loss: 1.162277  [  704/ 3200]\n",
      "loss: 0.860147  [  720/ 3200]\n",
      "loss: 1.040508  [  736/ 3200]\n",
      "loss: 1.114803  [  752/ 3200]\n",
      "loss: 1.255525  [  768/ 3200]\n",
      "loss: 1.198378  [  784/ 3200]\n",
      "loss: 1.474651  [  800/ 3200]\n",
      "loss: 0.759390  [  816/ 3200]\n",
      "loss: 0.958506  [  832/ 3200]\n",
      "loss: 1.152600  [  848/ 3200]\n",
      "loss: 0.758812  [  864/ 3200]\n",
      "loss: 1.231569  [  880/ 3200]\n",
      "loss: 0.977711  [  896/ 3200]\n",
      "loss: 0.986670  [  912/ 3200]\n",
      "loss: 0.919506  [  928/ 3200]\n",
      "loss: 1.088643  [  944/ 3200]\n",
      "loss: 0.786289  [  960/ 3200]\n",
      "loss: 1.084666  [  976/ 3200]\n",
      "loss: 1.049104  [  992/ 3200]\n",
      "loss: 0.969476  [ 1008/ 3200]\n",
      "loss: 0.638654  [ 1024/ 3200]\n",
      "loss: 0.910975  [ 1040/ 3200]\n",
      "loss: 0.795434  [ 1056/ 3200]\n",
      "loss: 1.327440  [ 1072/ 3200]\n",
      "loss: 0.864846  [ 1088/ 3200]\n",
      "loss: 0.893903  [ 1104/ 3200]\n",
      "loss: 1.105443  [ 1120/ 3200]\n",
      "loss: 0.841632  [ 1136/ 3200]\n",
      "loss: 1.068520  [ 1152/ 3200]\n",
      "loss: 1.065223  [ 1168/ 3200]\n",
      "loss: 0.893565  [ 1184/ 3200]\n",
      "loss: 1.170277  [ 1200/ 3200]\n",
      "loss: 1.009305  [ 1216/ 3200]\n",
      "loss: 1.115111  [ 1232/ 3200]\n",
      "loss: 0.780172  [ 1248/ 3200]\n",
      "loss: 0.738169  [ 1264/ 3200]\n",
      "loss: 1.018108  [ 1280/ 3200]\n",
      "loss: 0.742767  [ 1296/ 3200]\n",
      "loss: 0.894437  [ 1312/ 3200]\n",
      "loss: 0.831473  [ 1328/ 3200]\n",
      "loss: 0.940176  [ 1344/ 3200]\n",
      "loss: 0.707273  [ 1360/ 3200]\n",
      "loss: 0.925537  [ 1376/ 3200]\n",
      "loss: 1.207492  [ 1392/ 3200]\n",
      "loss: 1.099773  [ 1408/ 3200]\n",
      "loss: 0.955952  [ 1424/ 3200]\n",
      "loss: 0.927961  [ 1440/ 3200]\n",
      "loss: 0.921878  [ 1456/ 3200]\n",
      "loss: 0.907612  [ 1472/ 3200]\n",
      "loss: 0.971528  [ 1488/ 3200]\n",
      "loss: 0.798205  [ 1504/ 3200]\n",
      "loss: 1.213637  [ 1520/ 3200]\n",
      "loss: 0.862603  [ 1536/ 3200]\n",
      "loss: 0.993291  [ 1552/ 3200]\n",
      "loss: 0.941845  [ 1568/ 3200]\n",
      "loss: 0.980904  [ 1584/ 3200]\n",
      "loss: 0.748487  [ 1600/ 3200]\n",
      "loss: 0.963877  [ 1616/ 3200]\n",
      "loss: 0.785175  [ 1632/ 3200]\n",
      "loss: 0.887949  [ 1648/ 3200]\n",
      "loss: 0.986947  [ 1664/ 3200]\n",
      "loss: 0.865261  [ 1680/ 3200]\n",
      "loss: 1.079529  [ 1696/ 3200]\n",
      "loss: 0.850277  [ 1712/ 3200]\n",
      "loss: 0.740393  [ 1728/ 3200]\n",
      "loss: 0.811605  [ 1744/ 3200]\n",
      "loss: 1.186226  [ 1760/ 3200]\n",
      "loss: 1.012897  [ 1776/ 3200]\n",
      "loss: 0.782762  [ 1792/ 3200]\n",
      "loss: 0.809032  [ 1808/ 3200]\n",
      "loss: 0.781361  [ 1824/ 3200]\n",
      "loss: 0.980689  [ 1840/ 3200]\n",
      "loss: 0.930688  [ 1856/ 3200]\n",
      "loss: 1.052405  [ 1872/ 3200]\n",
      "loss: 1.064374  [ 1888/ 3200]\n",
      "loss: 1.054909  [ 1904/ 3200]\n",
      "loss: 1.000626  [ 1920/ 3200]\n",
      "loss: 1.149524  [ 1936/ 3200]\n",
      "loss: 1.140957  [ 1952/ 3200]\n",
      "loss: 0.867109  [ 1968/ 3200]\n",
      "loss: 1.126444  [ 1984/ 3200]\n",
      "loss: 0.856807  [ 2000/ 3200]\n",
      "loss: 0.771203  [ 2016/ 3200]\n",
      "loss: 1.005127  [ 2032/ 3200]\n",
      "loss: 0.956433  [ 2048/ 3200]\n",
      "loss: 0.512179  [ 2064/ 3200]\n",
      "loss: 1.149357  [ 2080/ 3200]\n",
      "loss: 1.070933  [ 2096/ 3200]\n",
      "loss: 1.023846  [ 2112/ 3200]\n",
      "loss: 1.214362  [ 2128/ 3200]\n",
      "loss: 0.910758  [ 2144/ 3200]\n",
      "loss: 0.967363  [ 2160/ 3200]\n",
      "loss: 1.125651  [ 2176/ 3200]\n",
      "loss: 0.966752  [ 2192/ 3200]\n",
      "loss: 1.145697  [ 2208/ 3200]\n",
      "loss: 1.458675  [ 2224/ 3200]\n",
      "loss: 0.734248  [ 2240/ 3200]\n",
      "loss: 1.002703  [ 2256/ 3200]\n",
      "loss: 1.000012  [ 2272/ 3200]\n",
      "loss: 0.964957  [ 2288/ 3200]\n",
      "loss: 0.882447  [ 2304/ 3200]\n",
      "loss: 0.975337  [ 2320/ 3200]\n",
      "loss: 0.750594  [ 2336/ 3200]\n",
      "loss: 0.848634  [ 2352/ 3200]\n",
      "loss: 0.649754  [ 2368/ 3200]\n",
      "loss: 1.397636  [ 2384/ 3200]\n",
      "loss: 1.099198  [ 2400/ 3200]\n",
      "loss: 1.181303  [ 2416/ 3200]\n",
      "loss: 1.251391  [ 2432/ 3200]\n",
      "loss: 0.976226  [ 2448/ 3200]\n",
      "loss: 0.664213  [ 2464/ 3200]\n",
      "loss: 0.903555  [ 2480/ 3200]\n",
      "loss: 1.072188  [ 2496/ 3200]\n",
      "loss: 0.977467  [ 2512/ 3200]\n",
      "loss: 0.769490  [ 2528/ 3200]\n",
      "loss: 0.961733  [ 2544/ 3200]\n",
      "loss: 0.989769  [ 2560/ 3200]\n",
      "loss: 1.154716  [ 2576/ 3200]\n",
      "loss: 0.990788  [ 2592/ 3200]\n",
      "loss: 1.152733  [ 2608/ 3200]\n",
      "loss: 0.813695  [ 2624/ 3200]\n",
      "loss: 0.932559  [ 2640/ 3200]\n",
      "loss: 0.916147  [ 2656/ 3200]\n",
      "loss: 0.882755  [ 2672/ 3200]\n",
      "loss: 0.747896  [ 2688/ 3200]\n",
      "loss: 0.774667  [ 2704/ 3200]\n",
      "loss: 1.175438  [ 2720/ 3200]\n",
      "loss: 0.669657  [ 2736/ 3200]\n",
      "loss: 1.037987  [ 2752/ 3200]\n",
      "loss: 1.037818  [ 2768/ 3200]\n",
      "loss: 1.228318  [ 2784/ 3200]\n",
      "loss: 0.989731  [ 2800/ 3200]\n",
      "loss: 1.009592  [ 2816/ 3200]\n",
      "loss: 0.829483  [ 2832/ 3200]\n",
      "loss: 0.992024  [ 2848/ 3200]\n",
      "loss: 1.062796  [ 2864/ 3200]\n",
      "loss: 0.740370  [ 2880/ 3200]\n",
      "loss: 0.982139  [ 2896/ 3200]\n",
      "loss: 0.907555  [ 2912/ 3200]\n",
      "loss: 0.981908  [ 2928/ 3200]\n",
      "loss: 0.739043  [ 2944/ 3200]\n",
      "loss: 0.821529  [ 2960/ 3200]\n",
      "loss: 0.938184  [ 2976/ 3200]\n",
      "loss: 1.024573  [ 2992/ 3200]\n",
      "loss: 1.109112  [ 3008/ 3200]\n",
      "loss: 0.815290  [ 3024/ 3200]\n",
      "loss: 0.834132  [ 3040/ 3200]\n",
      "loss: 0.931844  [ 3056/ 3200]\n",
      "loss: 0.653971  [ 3072/ 3200]\n",
      "loss: 1.071594  [ 3088/ 3200]\n",
      "loss: 1.142192  [ 3104/ 3200]\n",
      "loss: 0.965167  [ 3120/ 3200]\n",
      "loss: 0.730593  [ 3136/ 3200]\n",
      "loss: 1.101049  [ 3152/ 3200]\n",
      "loss: 0.754795  [ 3168/ 3200]\n",
      "loss: 1.069019  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 1.052999  [    0/ 3200]\n",
      "loss: 1.102759  [   16/ 3200]\n",
      "loss: 1.121650  [   32/ 3200]\n",
      "loss: 0.924881  [   48/ 3200]\n",
      "loss: 0.993784  [   64/ 3200]\n",
      "loss: 0.866673  [   80/ 3200]\n",
      "loss: 0.793047  [   96/ 3200]\n",
      "loss: 0.862644  [  112/ 3200]\n",
      "loss: 1.072574  [  128/ 3200]\n",
      "loss: 1.048064  [  144/ 3200]\n",
      "loss: 0.829994  [  160/ 3200]\n",
      "loss: 1.545396  [  176/ 3200]\n",
      "loss: 0.979671  [  192/ 3200]\n",
      "loss: 0.847208  [  208/ 3200]\n",
      "loss: 1.226485  [  224/ 3200]\n",
      "loss: 0.965683  [  240/ 3200]\n",
      "loss: 0.757747  [  256/ 3200]\n",
      "loss: 0.859261  [  272/ 3200]\n",
      "loss: 0.871547  [  288/ 3200]\n",
      "loss: 1.257220  [  304/ 3200]\n",
      "loss: 1.059819  [  320/ 3200]\n",
      "loss: 1.125469  [  336/ 3200]\n",
      "loss: 0.796238  [  352/ 3200]\n",
      "loss: 0.949286  [  368/ 3200]\n",
      "loss: 1.059389  [  384/ 3200]\n",
      "loss: 1.027163  [  400/ 3200]\n",
      "loss: 0.865532  [  416/ 3200]\n",
      "loss: 0.975293  [  432/ 3200]\n",
      "loss: 0.987309  [  448/ 3200]\n",
      "loss: 1.045345  [  464/ 3200]\n",
      "loss: 0.916944  [  480/ 3200]\n",
      "loss: 0.920635  [  496/ 3200]\n",
      "loss: 0.934290  [  512/ 3200]\n",
      "loss: 1.149781  [  528/ 3200]\n",
      "loss: 1.074135  [  544/ 3200]\n",
      "loss: 1.106320  [  560/ 3200]\n",
      "loss: 0.916660  [  576/ 3200]\n",
      "loss: 0.887908  [  592/ 3200]\n",
      "loss: 0.788154  [  608/ 3200]\n",
      "loss: 1.314864  [  624/ 3200]\n",
      "loss: 1.106351  [  640/ 3200]\n",
      "loss: 1.140787  [  656/ 3200]\n",
      "loss: 1.255770  [  672/ 3200]\n",
      "loss: 0.842904  [  688/ 3200]\n",
      "loss: 0.791641  [  704/ 3200]\n",
      "loss: 0.924006  [  720/ 3200]\n",
      "loss: 1.283758  [  736/ 3200]\n",
      "loss: 0.786088  [  752/ 3200]\n",
      "loss: 0.716436  [  768/ 3200]\n",
      "loss: 0.764397  [  784/ 3200]\n",
      "loss: 0.633106  [  800/ 3200]\n",
      "loss: 1.049613  [  816/ 3200]\n",
      "loss: 0.789235  [  832/ 3200]\n",
      "loss: 1.162682  [  848/ 3200]\n",
      "loss: 0.992359  [  864/ 3200]\n",
      "loss: 0.747442  [  880/ 3200]\n",
      "loss: 0.996257  [  896/ 3200]\n",
      "loss: 0.956610  [  912/ 3200]\n",
      "loss: 1.003739  [  928/ 3200]\n",
      "loss: 0.909144  [  944/ 3200]\n",
      "loss: 0.881610  [  960/ 3200]\n",
      "loss: 1.159093  [  976/ 3200]\n",
      "loss: 1.144382  [  992/ 3200]\n",
      "loss: 1.018777  [ 1008/ 3200]\n",
      "loss: 0.855393  [ 1024/ 3200]\n",
      "loss: 0.711116  [ 1040/ 3200]\n",
      "loss: 0.927281  [ 1056/ 3200]\n",
      "loss: 0.982997  [ 1072/ 3200]\n",
      "loss: 1.268862  [ 1088/ 3200]\n",
      "loss: 0.750366  [ 1104/ 3200]\n",
      "loss: 1.006666  [ 1120/ 3200]\n",
      "loss: 0.973950  [ 1136/ 3200]\n",
      "loss: 0.819425  [ 1152/ 3200]\n",
      "loss: 1.142160  [ 1168/ 3200]\n",
      "loss: 0.725675  [ 1184/ 3200]\n",
      "loss: 0.976494  [ 1200/ 3200]\n",
      "loss: 0.908645  [ 1216/ 3200]\n",
      "loss: 1.249224  [ 1232/ 3200]\n",
      "loss: 1.013788  [ 1248/ 3200]\n",
      "loss: 0.798777  [ 1264/ 3200]\n",
      "loss: 0.754480  [ 1280/ 3200]\n",
      "loss: 0.925997  [ 1296/ 3200]\n",
      "loss: 0.880038  [ 1312/ 3200]\n",
      "loss: 0.936176  [ 1328/ 3200]\n",
      "loss: 0.744479  [ 1344/ 3200]\n",
      "loss: 0.829505  [ 1360/ 3200]\n",
      "loss: 0.606828  [ 1376/ 3200]\n",
      "loss: 0.845318  [ 1392/ 3200]\n",
      "loss: 0.825656  [ 1408/ 3200]\n",
      "loss: 1.009011  [ 1424/ 3200]\n",
      "loss: 1.092688  [ 1440/ 3200]\n",
      "loss: 0.840658  [ 1456/ 3200]\n",
      "loss: 1.256627  [ 1472/ 3200]\n",
      "loss: 1.045523  [ 1488/ 3200]\n",
      "loss: 0.921746  [ 1504/ 3200]\n",
      "loss: 1.012028  [ 1520/ 3200]\n",
      "loss: 0.902158  [ 1536/ 3200]\n",
      "loss: 1.121087  [ 1552/ 3200]\n",
      "loss: 0.903394  [ 1568/ 3200]\n",
      "loss: 0.768956  [ 1584/ 3200]\n",
      "loss: 0.990801  [ 1600/ 3200]\n",
      "loss: 0.877276  [ 1616/ 3200]\n",
      "loss: 1.161356  [ 1632/ 3200]\n",
      "loss: 0.980132  [ 1648/ 3200]\n",
      "loss: 1.057311  [ 1664/ 3200]\n",
      "loss: 1.018544  [ 1680/ 3200]\n",
      "loss: 0.653027  [ 1696/ 3200]\n",
      "loss: 0.897754  [ 1712/ 3200]\n",
      "loss: 0.726864  [ 1728/ 3200]\n",
      "loss: 0.878590  [ 1744/ 3200]\n",
      "loss: 0.951218  [ 1760/ 3200]\n",
      "loss: 1.106764  [ 1776/ 3200]\n",
      "loss: 1.161134  [ 1792/ 3200]\n",
      "loss: 0.860121  [ 1808/ 3200]\n",
      "loss: 0.991406  [ 1824/ 3200]\n",
      "loss: 1.054094  [ 1840/ 3200]\n",
      "loss: 1.004129  [ 1856/ 3200]\n",
      "loss: 0.791700  [ 1872/ 3200]\n",
      "loss: 1.322343  [ 1888/ 3200]\n",
      "loss: 0.853273  [ 1904/ 3200]\n",
      "loss: 0.935521  [ 1920/ 3200]\n",
      "loss: 1.000702  [ 1936/ 3200]\n",
      "loss: 0.907396  [ 1952/ 3200]\n",
      "loss: 0.942731  [ 1968/ 3200]\n",
      "loss: 0.804963  [ 1984/ 3200]\n",
      "loss: 0.810303  [ 2000/ 3200]\n",
      "loss: 0.752015  [ 2016/ 3200]\n",
      "loss: 0.714236  [ 2032/ 3200]\n",
      "loss: 0.782898  [ 2048/ 3200]\n",
      "loss: 0.916468  [ 2064/ 3200]\n",
      "loss: 0.786127  [ 2080/ 3200]\n",
      "loss: 1.151918  [ 2096/ 3200]\n",
      "loss: 1.012510  [ 2112/ 3200]\n",
      "loss: 0.778523  [ 2128/ 3200]\n",
      "loss: 1.070604  [ 2144/ 3200]\n",
      "loss: 0.816792  [ 2160/ 3200]\n",
      "loss: 0.833004  [ 2176/ 3200]\n",
      "loss: 1.286611  [ 2192/ 3200]\n",
      "loss: 0.913735  [ 2208/ 3200]\n",
      "loss: 0.809238  [ 2224/ 3200]\n",
      "loss: 0.739249  [ 2240/ 3200]\n",
      "loss: 0.885478  [ 2256/ 3200]\n",
      "loss: 1.282902  [ 2272/ 3200]\n",
      "loss: 1.076846  [ 2288/ 3200]\n",
      "loss: 0.973268  [ 2304/ 3200]\n",
      "loss: 1.172822  [ 2320/ 3200]\n",
      "loss: 1.040687  [ 2336/ 3200]\n",
      "loss: 0.761454  [ 2352/ 3200]\n",
      "loss: 0.867448  [ 2368/ 3200]\n",
      "loss: 0.922590  [ 2384/ 3200]\n",
      "loss: 1.260412  [ 2400/ 3200]\n",
      "loss: 0.926483  [ 2416/ 3200]\n",
      "loss: 0.789220  [ 2432/ 3200]\n",
      "loss: 0.980043  [ 2448/ 3200]\n",
      "loss: 0.719750  [ 2464/ 3200]\n",
      "loss: 0.893316  [ 2480/ 3200]\n",
      "loss: 1.078790  [ 2496/ 3200]\n",
      "loss: 0.745447  [ 2512/ 3200]\n",
      "loss: 0.839417  [ 2528/ 3200]\n",
      "loss: 0.905828  [ 2544/ 3200]\n",
      "loss: 0.650791  [ 2560/ 3200]\n",
      "loss: 0.625039  [ 2576/ 3200]\n",
      "loss: 1.054975  [ 2592/ 3200]\n",
      "loss: 0.706062  [ 2608/ 3200]\n",
      "loss: 1.021976  [ 2624/ 3200]\n",
      "loss: 0.736546  [ 2640/ 3200]\n",
      "loss: 1.151150  [ 2656/ 3200]\n",
      "loss: 1.007302  [ 2672/ 3200]\n",
      "loss: 1.250527  [ 2688/ 3200]\n",
      "loss: 0.748130  [ 2704/ 3200]\n",
      "loss: 1.004963  [ 2720/ 3200]\n",
      "loss: 0.846169  [ 2736/ 3200]\n",
      "loss: 0.706891  [ 2752/ 3200]\n",
      "loss: 0.905582  [ 2768/ 3200]\n",
      "loss: 0.747715  [ 2784/ 3200]\n",
      "loss: 0.939549  [ 2800/ 3200]\n",
      "loss: 0.987939  [ 2816/ 3200]\n",
      "loss: 1.081699  [ 2832/ 3200]\n",
      "loss: 1.116695  [ 2848/ 3200]\n",
      "loss: 0.770764  [ 2864/ 3200]\n",
      "loss: 1.053216  [ 2880/ 3200]\n",
      "loss: 1.180407  [ 2896/ 3200]\n",
      "loss: 1.170105  [ 2912/ 3200]\n",
      "loss: 1.210072  [ 2928/ 3200]\n",
      "loss: 1.075831  [ 2944/ 3200]\n",
      "loss: 0.888911  [ 2960/ 3200]\n",
      "loss: 0.978019  [ 2976/ 3200]\n",
      "loss: 1.167687  [ 2992/ 3200]\n",
      "loss: 1.074577  [ 3008/ 3200]\n",
      "loss: 0.910440  [ 3024/ 3200]\n",
      "loss: 0.925188  [ 3040/ 3200]\n",
      "loss: 0.952071  [ 3056/ 3200]\n",
      "loss: 1.190181  [ 3072/ 3200]\n",
      "loss: 0.739562  [ 3088/ 3200]\n",
      "loss: 1.174755  [ 3104/ 3200]\n",
      "loss: 1.128481  [ 3120/ 3200]\n",
      "loss: 0.993790  [ 3136/ 3200]\n",
      "loss: 1.134135  [ 3152/ 3200]\n",
      "loss: 0.712351  [ 3168/ 3200]\n",
      "loss: 0.811688  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 1.212462  [    0/ 3200]\n",
      "loss: 1.043204  [   16/ 3200]\n",
      "loss: 1.114832  [   32/ 3200]\n",
      "loss: 0.893735  [   48/ 3200]\n",
      "loss: 0.995837  [   64/ 3200]\n",
      "loss: 0.943381  [   80/ 3200]\n",
      "loss: 1.175536  [   96/ 3200]\n",
      "loss: 0.767915  [  112/ 3200]\n",
      "loss: 0.948294  [  128/ 3200]\n",
      "loss: 0.795393  [  144/ 3200]\n",
      "loss: 1.109428  [  160/ 3200]\n",
      "loss: 0.940973  [  176/ 3200]\n",
      "loss: 0.960648  [  192/ 3200]\n",
      "loss: 1.117802  [  208/ 3200]\n",
      "loss: 1.264483  [  224/ 3200]\n",
      "loss: 0.837210  [  240/ 3200]\n",
      "loss: 0.744108  [  256/ 3200]\n",
      "loss: 0.957651  [  272/ 3200]\n",
      "loss: 0.850669  [  288/ 3200]\n",
      "loss: 1.254748  [  304/ 3200]\n",
      "loss: 1.160914  [  320/ 3200]\n",
      "loss: 0.817511  [  336/ 3200]\n",
      "loss: 0.846179  [  352/ 3200]\n",
      "loss: 0.863260  [  368/ 3200]\n",
      "loss: 0.946492  [  384/ 3200]\n",
      "loss: 0.792396  [  400/ 3200]\n",
      "loss: 0.811294  [  416/ 3200]\n",
      "loss: 0.940824  [  432/ 3200]\n",
      "loss: 0.866954  [  448/ 3200]\n",
      "loss: 0.920058  [  464/ 3200]\n",
      "loss: 0.960013  [  480/ 3200]\n",
      "loss: 0.849388  [  496/ 3200]\n",
      "loss: 1.072893  [  512/ 3200]\n",
      "loss: 0.963384  [  528/ 3200]\n",
      "loss: 0.805698  [  544/ 3200]\n",
      "loss: 0.811030  [  560/ 3200]\n",
      "loss: 0.885801  [  576/ 3200]\n",
      "loss: 1.184236  [  592/ 3200]\n",
      "loss: 0.778066  [  608/ 3200]\n",
      "loss: 0.895783  [  624/ 3200]\n",
      "loss: 0.963427  [  640/ 3200]\n",
      "loss: 1.011450  [  656/ 3200]\n",
      "loss: 0.918825  [  672/ 3200]\n",
      "loss: 0.875795  [  688/ 3200]\n",
      "loss: 0.855175  [  704/ 3200]\n",
      "loss: 0.973679  [  720/ 3200]\n",
      "loss: 0.693740  [  736/ 3200]\n",
      "loss: 0.939367  [  752/ 3200]\n",
      "loss: 1.004804  [  768/ 3200]\n",
      "loss: 1.056607  [  784/ 3200]\n",
      "loss: 1.384549  [  800/ 3200]\n",
      "loss: 1.007480  [  816/ 3200]\n",
      "loss: 1.214171  [  832/ 3200]\n",
      "loss: 1.210067  [  848/ 3200]\n",
      "loss: 0.803943  [  864/ 3200]\n",
      "loss: 1.021432  [  880/ 3200]\n",
      "loss: 0.826220  [  896/ 3200]\n",
      "loss: 1.026975  [  912/ 3200]\n",
      "loss: 1.296733  [  928/ 3200]\n",
      "loss: 1.057369  [  944/ 3200]\n",
      "loss: 1.348774  [  960/ 3200]\n",
      "loss: 1.007702  [  976/ 3200]\n",
      "loss: 0.854206  [  992/ 3200]\n",
      "loss: 1.201836  [ 1008/ 3200]\n",
      "loss: 1.108365  [ 1024/ 3200]\n",
      "loss: 1.030658  [ 1040/ 3200]\n",
      "loss: 0.851183  [ 1056/ 3200]\n",
      "loss: 0.897641  [ 1072/ 3200]\n",
      "loss: 0.950499  [ 1088/ 3200]\n",
      "loss: 0.900894  [ 1104/ 3200]\n",
      "loss: 1.137858  [ 1120/ 3200]\n",
      "loss: 1.042787  [ 1136/ 3200]\n",
      "loss: 0.924290  [ 1152/ 3200]\n",
      "loss: 0.636401  [ 1168/ 3200]\n",
      "loss: 0.961981  [ 1184/ 3200]\n",
      "loss: 1.183645  [ 1200/ 3200]\n",
      "loss: 1.414050  [ 1216/ 3200]\n",
      "loss: 1.103790  [ 1232/ 3200]\n",
      "loss: 0.621209  [ 1248/ 3200]\n",
      "loss: 0.807286  [ 1264/ 3200]\n",
      "loss: 0.773714  [ 1280/ 3200]\n",
      "loss: 0.792803  [ 1296/ 3200]\n",
      "loss: 1.206869  [ 1312/ 3200]\n",
      "loss: 0.816689  [ 1328/ 3200]\n",
      "loss: 1.036997  [ 1344/ 3200]\n",
      "loss: 0.917398  [ 1360/ 3200]\n",
      "loss: 0.753678  [ 1376/ 3200]\n",
      "loss: 1.033083  [ 1392/ 3200]\n",
      "loss: 0.983728  [ 1408/ 3200]\n",
      "loss: 1.019887  [ 1424/ 3200]\n",
      "loss: 0.764587  [ 1440/ 3200]\n",
      "loss: 0.975406  [ 1456/ 3200]\n",
      "loss: 1.067380  [ 1472/ 3200]\n",
      "loss: 0.912937  [ 1488/ 3200]\n",
      "loss: 1.046761  [ 1504/ 3200]\n",
      "loss: 0.824370  [ 1520/ 3200]\n",
      "loss: 0.932186  [ 1536/ 3200]\n",
      "loss: 1.147363  [ 1552/ 3200]\n",
      "loss: 1.140675  [ 1568/ 3200]\n",
      "loss: 0.790306  [ 1584/ 3200]\n",
      "loss: 0.957351  [ 1600/ 3200]\n",
      "loss: 0.868250  [ 1616/ 3200]\n",
      "loss: 1.234563  [ 1632/ 3200]\n",
      "loss: 0.999948  [ 1648/ 3200]\n",
      "loss: 1.029067  [ 1664/ 3200]\n",
      "loss: 0.752773  [ 1680/ 3200]\n",
      "loss: 1.010504  [ 1696/ 3200]\n",
      "loss: 1.249075  [ 1712/ 3200]\n",
      "loss: 1.102635  [ 1728/ 3200]\n",
      "loss: 0.748072  [ 1744/ 3200]\n",
      "loss: 1.006742  [ 1760/ 3200]\n",
      "loss: 0.931504  [ 1776/ 3200]\n",
      "loss: 0.834704  [ 1792/ 3200]\n",
      "loss: 0.971776  [ 1808/ 3200]\n",
      "loss: 0.907694  [ 1824/ 3200]\n",
      "loss: 1.139995  [ 1840/ 3200]\n",
      "loss: 1.030253  [ 1856/ 3200]\n",
      "loss: 1.037174  [ 1872/ 3200]\n",
      "loss: 0.806574  [ 1888/ 3200]\n",
      "loss: 1.030608  [ 1904/ 3200]\n",
      "loss: 0.926631  [ 1920/ 3200]\n",
      "loss: 1.084837  [ 1936/ 3200]\n",
      "loss: 1.003106  [ 1952/ 3200]\n",
      "loss: 0.864547  [ 1968/ 3200]\n",
      "loss: 0.742779  [ 1984/ 3200]\n",
      "loss: 0.834385  [ 2000/ 3200]\n",
      "loss: 1.072171  [ 2016/ 3200]\n",
      "loss: 0.931265  [ 2032/ 3200]\n",
      "loss: 1.109618  [ 2048/ 3200]\n",
      "loss: 1.299664  [ 2064/ 3200]\n",
      "loss: 0.824612  [ 2080/ 3200]\n",
      "loss: 0.946641  [ 2096/ 3200]\n",
      "loss: 0.720622  [ 2112/ 3200]\n",
      "loss: 1.127210  [ 2128/ 3200]\n",
      "loss: 1.134510  [ 2144/ 3200]\n",
      "loss: 0.922736  [ 2160/ 3200]\n",
      "loss: 0.784419  [ 2176/ 3200]\n",
      "loss: 0.732924  [ 2192/ 3200]\n",
      "loss: 0.811871  [ 2208/ 3200]\n",
      "loss: 0.826646  [ 2224/ 3200]\n",
      "loss: 0.948590  [ 2240/ 3200]\n",
      "loss: 0.603352  [ 2256/ 3200]\n",
      "loss: 1.300370  [ 2272/ 3200]\n",
      "loss: 0.864486  [ 2288/ 3200]\n",
      "loss: 0.881851  [ 2304/ 3200]\n",
      "loss: 1.010027  [ 2320/ 3200]\n",
      "loss: 0.912882  [ 2336/ 3200]\n",
      "loss: 0.911284  [ 2352/ 3200]\n",
      "loss: 0.802249  [ 2368/ 3200]\n",
      "loss: 0.723783  [ 2384/ 3200]\n",
      "loss: 0.611749  [ 2400/ 3200]\n",
      "loss: 0.987952  [ 2416/ 3200]\n",
      "loss: 1.082057  [ 2432/ 3200]\n",
      "loss: 1.163960  [ 2448/ 3200]\n",
      "loss: 0.942085  [ 2464/ 3200]\n",
      "loss: 1.071425  [ 2480/ 3200]\n",
      "loss: 0.766447  [ 2496/ 3200]\n",
      "loss: 0.688772  [ 2512/ 3200]\n",
      "loss: 0.956492  [ 2528/ 3200]\n",
      "loss: 0.891021  [ 2544/ 3200]\n",
      "loss: 0.894197  [ 2560/ 3200]\n",
      "loss: 1.012423  [ 2576/ 3200]\n",
      "loss: 0.995727  [ 2592/ 3200]\n",
      "loss: 1.175053  [ 2608/ 3200]\n",
      "loss: 0.643579  [ 2624/ 3200]\n",
      "loss: 0.917714  [ 2640/ 3200]\n",
      "loss: 0.843210  [ 2656/ 3200]\n",
      "loss: 1.079437  [ 2672/ 3200]\n",
      "loss: 0.856380  [ 2688/ 3200]\n",
      "loss: 0.849579  [ 2704/ 3200]\n",
      "loss: 0.742083  [ 2720/ 3200]\n",
      "loss: 0.854060  [ 2736/ 3200]\n",
      "loss: 0.947816  [ 2752/ 3200]\n",
      "loss: 0.851213  [ 2768/ 3200]\n",
      "loss: 1.016403  [ 2784/ 3200]\n",
      "loss: 0.851805  [ 2800/ 3200]\n",
      "loss: 0.869776  [ 2816/ 3200]\n",
      "loss: 0.821446  [ 2832/ 3200]\n",
      "loss: 0.855245  [ 2848/ 3200]\n",
      "loss: 0.872201  [ 2864/ 3200]\n",
      "loss: 0.900404  [ 2880/ 3200]\n",
      "loss: 0.956110  [ 2896/ 3200]\n",
      "loss: 0.968354  [ 2912/ 3200]\n",
      "loss: 0.908603  [ 2928/ 3200]\n",
      "loss: 0.696536  [ 2944/ 3200]\n",
      "loss: 1.021225  [ 2960/ 3200]\n",
      "loss: 1.184254  [ 2976/ 3200]\n",
      "loss: 0.771174  [ 2992/ 3200]\n",
      "loss: 0.761934  [ 3008/ 3200]\n",
      "loss: 1.196368  [ 3024/ 3200]\n",
      "loss: 0.722808  [ 3040/ 3200]\n",
      "loss: 0.999331  [ 3056/ 3200]\n",
      "loss: 0.929352  [ 3072/ 3200]\n",
      "loss: 0.809009  [ 3088/ 3200]\n",
      "loss: 0.728274  [ 3104/ 3200]\n",
      "loss: 0.779021  [ 3120/ 3200]\n",
      "loss: 0.931889  [ 3136/ 3200]\n",
      "loss: 0.911483  [ 3152/ 3200]\n",
      "loss: 1.230919  [ 3168/ 3200]\n",
      "loss: 1.063750  [ 3184/ 3200]\n",
      "\n",
      "CPU times: user 10.6 s, sys: 679 ms, total: 11.3 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gpu_model = train_neural_network(epochs, optimizer, train_dataloader, loss_function, gpu_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60c0NH3U2pat"
   },
   "source": [
    "Test our Neural Network on the test data using the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "he6zHx4HifaI",
    "outputId": "2bd01b35-6d0b-439f-9b3a-c2db489ba569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:\n",
      "Avg loss               : 0.060366\n",
      "f1 macro averaged score: 0.583550\n",
      "Accuracy               : 61.3%\n",
      "Confusion matrix       :\n",
      "tensor([[244,  23,  11,  19],\n",
      "        [ 39,  61,  77, 147],\n",
      "        [ 21,  47, 241,  47],\n",
      "        [ 43,  21,  38, 297]], device='cuda:0')\n",
      "CPU times: user 75.3 ms, sys: 2.62 ms, total: 77.9 ms\n",
      "Wall time: 86.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = test_neural_network(test_dataloader, loss_function, gpu_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbGvYQGOpfta"
   },
   "source": [
    "The GPU was a little slower than the CPU.\n",
    "\n",
    "This could have been caused due to the overhead of the GPU accessing the computer's memory. [[1]](#reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpDhbHBArnwY"
   },
   "source": [
    "### Step 7 - Validation\n",
    "\n",
    "This function trains the Neural Network and tests it after each epoch, in order to find the epoch that has the best f1 macro averaged score.\n",
    "\n",
    "The best Neural Network is returned.\n",
    "<br></br>\n",
    "\n",
    "The code shown in compimentary courses has been modified in order to implement the validation procedure was described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfFcGiWcCMGq"
   },
   "outputs": [],
   "source": [
    "def validate_neural_network(epochs, optimizer, train_dataloader, val_dataloader, loss_function, model):\n",
    "  size = len(train_dataloader.dataset)\n",
    "\n",
    "  best = (0, 0, 0) # best model, best epoch, best f1 score\n",
    "  f1_list = []\n",
    "\n",
    "  for epoch in range(0, epochs):\n",
    "    # train model with train data\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"-----------------------------\")\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(x.float())\n",
    "      loss = loss_function(prediction, y)\n",
    "\n",
    "      # backpropagation\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss, current = loss.item(), batch * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # test model on validation set\n",
    "    results = test_neural_network(val_dataloader, loss_function, model)\n",
    "    f1_macro_avg = results[1]\n",
    "    f1_list.append(f1_macro_avg)\n",
    "    if f1_macro_avg > best[2]:\n",
    "      best = (model, epoch, f1_macro_avg)\n",
    "    print()\n",
    "\n",
    "  print(f\"Best epoch: {(best[1] + 1)} with f1 macro averaged score: {best[2]}\")\n",
    "  return best[0], f1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g5YcMKnL-IM"
   },
   "source": [
    "Initialize model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJIy_gCJL_XH",
    "outputId": "8cb267a3-2c52-4116-977b-b902f76acbdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=26, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA5L78ednpPf"
   },
   "source": [
    "Train model using $learning$ $rate = 0.002$, Stochastic Gradient Descent optimizer and Cross Entropy Loss function for $30$ epochs, while keeping the best current f1 macro averaged score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9Rprvp3nrMa"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "optimizer = SGD(params=model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_a_Vu6XL__Z"
   },
   "source": [
    "Find the model with the highest f1 macro averaged score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVrDnZbmLuJN",
    "outputId": "46ba459e-d35a-411e-ed0c-f796ce671b1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 1.310264  [ 1424/ 3200]\n",
      "loss: 1.340309  [ 1440/ 3200]\n",
      "loss: 1.335567  [ 1456/ 3200]\n",
      "loss: 1.321783  [ 1472/ 3200]\n",
      "loss: 1.318297  [ 1488/ 3200]\n",
      "loss: 1.276262  [ 1504/ 3200]\n",
      "loss: 1.348097  [ 1520/ 3200]\n",
      "loss: 1.282816  [ 1536/ 3200]\n",
      "loss: 1.340632  [ 1552/ 3200]\n",
      "loss: 1.240703  [ 1568/ 3200]\n",
      "loss: 1.307691  [ 1584/ 3200]\n",
      "loss: 1.330389  [ 1600/ 3200]\n",
      "loss: 1.277831  [ 1616/ 3200]\n",
      "loss: 1.321805  [ 1632/ 3200]\n",
      "loss: 1.343093  [ 1648/ 3200]\n",
      "loss: 1.284737  [ 1664/ 3200]\n",
      "loss: 1.264671  [ 1680/ 3200]\n",
      "loss: 1.281749  [ 1696/ 3200]\n",
      "loss: 1.299870  [ 1712/ 3200]\n",
      "loss: 1.284218  [ 1728/ 3200]\n",
      "loss: 1.226463  [ 1744/ 3200]\n",
      "loss: 1.365636  [ 1760/ 3200]\n",
      "loss: 1.297057  [ 1776/ 3200]\n",
      "loss: 1.304991  [ 1792/ 3200]\n",
      "loss: 1.286790  [ 1808/ 3200]\n",
      "loss: 1.285700  [ 1824/ 3200]\n",
      "loss: 1.326450  [ 1840/ 3200]\n",
      "loss: 1.358700  [ 1856/ 3200]\n",
      "loss: 1.339088  [ 1872/ 3200]\n",
      "loss: 1.301049  [ 1888/ 3200]\n",
      "loss: 1.312231  [ 1904/ 3200]\n",
      "loss: 1.255486  [ 1920/ 3200]\n",
      "loss: 1.299119  [ 1936/ 3200]\n",
      "loss: 1.296322  [ 1952/ 3200]\n",
      "loss: 1.324092  [ 1968/ 3200]\n",
      "loss: 1.244992  [ 1984/ 3200]\n",
      "loss: 1.266998  [ 2000/ 3200]\n",
      "loss: 1.287859  [ 2016/ 3200]\n",
      "loss: 1.343172  [ 2032/ 3200]\n",
      "loss: 1.331759  [ 2048/ 3200]\n",
      "loss: 1.345066  [ 2064/ 3200]\n",
      "loss: 1.260639  [ 2080/ 3200]\n",
      "loss: 1.298850  [ 2096/ 3200]\n",
      "loss: 1.258370  [ 2112/ 3200]\n",
      "loss: 1.389205  [ 2128/ 3200]\n",
      "loss: 1.316590  [ 2144/ 3200]\n",
      "loss: 1.396694  [ 2160/ 3200]\n",
      "loss: 1.339209  [ 2176/ 3200]\n",
      "loss: 1.347946  [ 2192/ 3200]\n",
      "loss: 1.306994  [ 2208/ 3200]\n",
      "loss: 1.366118  [ 2224/ 3200]\n",
      "loss: 1.290113  [ 2240/ 3200]\n",
      "loss: 1.341231  [ 2256/ 3200]\n",
      "loss: 1.333186  [ 2272/ 3200]\n",
      "loss: 1.315736  [ 2288/ 3200]\n",
      "loss: 1.331948  [ 2304/ 3200]\n",
      "loss: 1.347780  [ 2320/ 3200]\n",
      "loss: 1.313243  [ 2336/ 3200]\n",
      "loss: 1.322405  [ 2352/ 3200]\n",
      "loss: 1.301617  [ 2368/ 3200]\n",
      "loss: 1.307781  [ 2384/ 3200]\n",
      "loss: 1.294200  [ 2400/ 3200]\n",
      "loss: 1.337999  [ 2416/ 3200]\n",
      "loss: 1.277905  [ 2432/ 3200]\n",
      "loss: 1.310243  [ 2448/ 3200]\n",
      "loss: 1.333870  [ 2464/ 3200]\n",
      "loss: 1.326588  [ 2480/ 3200]\n",
      "loss: 1.295844  [ 2496/ 3200]\n",
      "loss: 1.325762  [ 2512/ 3200]\n",
      "loss: 1.327722  [ 2528/ 3200]\n",
      "loss: 1.336802  [ 2544/ 3200]\n",
      "loss: 1.304801  [ 2560/ 3200]\n",
      "loss: 1.295077  [ 2576/ 3200]\n",
      "loss: 1.304611  [ 2592/ 3200]\n",
      "loss: 1.320620  [ 2608/ 3200]\n",
      "loss: 1.318168  [ 2624/ 3200]\n",
      "loss: 1.312780  [ 2640/ 3200]\n",
      "loss: 1.310925  [ 2656/ 3200]\n",
      "loss: 1.316174  [ 2672/ 3200]\n",
      "loss: 1.293772  [ 2688/ 3200]\n",
      "loss: 1.288834  [ 2704/ 3200]\n",
      "loss: 1.260946  [ 2720/ 3200]\n",
      "loss: 1.370824  [ 2736/ 3200]\n",
      "loss: 1.351306  [ 2752/ 3200]\n",
      "loss: 1.311819  [ 2768/ 3200]\n",
      "loss: 1.317596  [ 2784/ 3200]\n",
      "loss: 1.336733  [ 2800/ 3200]\n",
      "loss: 1.291793  [ 2816/ 3200]\n",
      "loss: 1.362521  [ 2832/ 3200]\n",
      "loss: 1.316625  [ 2848/ 3200]\n",
      "loss: 1.293576  [ 2864/ 3200]\n",
      "loss: 1.218523  [ 2880/ 3200]\n",
      "loss: 1.262424  [ 2896/ 3200]\n",
      "loss: 1.279965  [ 2912/ 3200]\n",
      "loss: 1.274479  [ 2928/ 3200]\n",
      "loss: 1.354346  [ 2944/ 3200]\n",
      "loss: 1.275762  [ 2960/ 3200]\n",
      "loss: 1.310166  [ 2976/ 3200]\n",
      "loss: 1.386626  [ 2992/ 3200]\n",
      "loss: 1.330973  [ 3008/ 3200]\n",
      "loss: 1.324119  [ 3024/ 3200]\n",
      "loss: 1.334130  [ 3040/ 3200]\n",
      "loss: 1.354063  [ 3056/ 3200]\n",
      "loss: 1.353297  [ 3072/ 3200]\n",
      "loss: 1.262693  [ 3088/ 3200]\n",
      "loss: 1.330337  [ 3104/ 3200]\n",
      "loss: 1.321076  [ 3120/ 3200]\n",
      "loss: 1.313576  [ 3136/ 3200]\n",
      "loss: 1.289919  [ 3152/ 3200]\n",
      "loss: 1.291247  [ 3168/ 3200]\n",
      "loss: 1.309246  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.081490\n",
      "f1 macro averaged score: 0.603716\n",
      "Accuracy               : 58.9%\n",
      "Confusion matrix       :\n",
      "tensor([[150,  46,   3,   1],\n",
      "        [ 24, 130,  37,   9],\n",
      "        [  1,  89, 107,   3],\n",
      "        [  1, 100,  15,  84]])\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 1.329632  [    0/ 3200]\n",
      "loss: 1.297023  [   16/ 3200]\n",
      "loss: 1.324763  [   32/ 3200]\n",
      "loss: 1.274002  [   48/ 3200]\n",
      "loss: 1.389210  [   64/ 3200]\n",
      "loss: 1.327501  [   80/ 3200]\n",
      "loss: 1.281524  [   96/ 3200]\n",
      "loss: 1.303274  [  112/ 3200]\n",
      "loss: 1.329262  [  128/ 3200]\n",
      "loss: 1.299840  [  144/ 3200]\n",
      "loss: 1.285788  [  160/ 3200]\n",
      "loss: 1.301047  [  176/ 3200]\n",
      "loss: 1.301654  [  192/ 3200]\n",
      "loss: 1.326302  [  208/ 3200]\n",
      "loss: 1.332284  [  224/ 3200]\n",
      "loss: 1.312893  [  240/ 3200]\n",
      "loss: 1.265873  [  256/ 3200]\n",
      "loss: 1.311190  [  272/ 3200]\n",
      "loss: 1.276496  [  288/ 3200]\n",
      "loss: 1.278370  [  304/ 3200]\n",
      "loss: 1.356995  [  320/ 3200]\n",
      "loss: 1.336722  [  336/ 3200]\n",
      "loss: 1.318877  [  352/ 3200]\n",
      "loss: 1.331501  [  368/ 3200]\n",
      "loss: 1.315689  [  384/ 3200]\n",
      "loss: 1.294272  [  400/ 3200]\n",
      "loss: 1.356022  [  416/ 3200]\n",
      "loss: 1.287845  [  432/ 3200]\n",
      "loss: 1.245529  [  448/ 3200]\n",
      "loss: 1.268568  [  464/ 3200]\n",
      "loss: 1.236310  [  480/ 3200]\n",
      "loss: 1.217298  [  496/ 3200]\n",
      "loss: 1.235613  [  512/ 3200]\n",
      "loss: 1.329738  [  528/ 3200]\n",
      "loss: 1.334298  [  544/ 3200]\n",
      "loss: 1.369113  [  560/ 3200]\n",
      "loss: 1.318488  [  576/ 3200]\n",
      "loss: 1.286663  [  592/ 3200]\n",
      "loss: 1.338740  [  608/ 3200]\n",
      "loss: 1.289108  [  624/ 3200]\n",
      "loss: 1.350705  [  640/ 3200]\n",
      "loss: 1.311453  [  656/ 3200]\n",
      "loss: 1.322813  [  672/ 3200]\n",
      "loss: 1.338288  [  688/ 3200]\n",
      "loss: 1.353035  [  704/ 3200]\n",
      "loss: 1.329447  [  720/ 3200]\n",
      "loss: 1.317604  [  736/ 3200]\n",
      "loss: 1.281625  [  752/ 3200]\n",
      "loss: 1.324336  [  768/ 3200]\n",
      "loss: 1.290296  [  784/ 3200]\n",
      "loss: 1.292733  [  800/ 3200]\n",
      "loss: 1.286496  [  816/ 3200]\n",
      "loss: 1.300700  [  832/ 3200]\n",
      "loss: 1.262559  [  848/ 3200]\n",
      "loss: 1.261754  [  864/ 3200]\n",
      "loss: 1.295442  [  880/ 3200]\n",
      "loss: 1.355482  [  896/ 3200]\n",
      "loss: 1.280613  [  912/ 3200]\n",
      "loss: 1.275943  [  928/ 3200]\n",
      "loss: 1.349716  [  944/ 3200]\n",
      "loss: 1.284676  [  960/ 3200]\n",
      "loss: 1.344029  [  976/ 3200]\n",
      "loss: 1.325250  [  992/ 3200]\n",
      "loss: 1.313298  [ 1008/ 3200]\n",
      "loss: 1.347881  [ 1024/ 3200]\n",
      "loss: 1.273179  [ 1040/ 3200]\n",
      "loss: 1.275106  [ 1056/ 3200]\n",
      "loss: 1.282223  [ 1072/ 3200]\n",
      "loss: 1.298177  [ 1088/ 3200]\n",
      "loss: 1.307977  [ 1104/ 3200]\n",
      "loss: 1.351477  [ 1120/ 3200]\n",
      "loss: 1.208424  [ 1136/ 3200]\n",
      "loss: 1.327509  [ 1152/ 3200]\n",
      "loss: 1.304940  [ 1168/ 3200]\n",
      "loss: 1.337779  [ 1184/ 3200]\n",
      "loss: 1.351391  [ 1200/ 3200]\n",
      "loss: 1.230179  [ 1216/ 3200]\n",
      "loss: 1.319763  [ 1232/ 3200]\n",
      "loss: 1.295850  [ 1248/ 3200]\n",
      "loss: 1.265201  [ 1264/ 3200]\n",
      "loss: 1.320618  [ 1280/ 3200]\n",
      "loss: 1.332242  [ 1296/ 3200]\n",
      "loss: 1.296188  [ 1312/ 3200]\n",
      "loss: 1.273610  [ 1328/ 3200]\n",
      "loss: 1.292157  [ 1344/ 3200]\n",
      "loss: 1.311655  [ 1360/ 3200]\n",
      "loss: 1.363100  [ 1376/ 3200]\n",
      "loss: 1.319775  [ 1392/ 3200]\n",
      "loss: 1.319259  [ 1408/ 3200]\n",
      "loss: 1.300033  [ 1424/ 3200]\n",
      "loss: 1.342231  [ 1440/ 3200]\n",
      "loss: 1.305343  [ 1456/ 3200]\n",
      "loss: 1.270353  [ 1472/ 3200]\n",
      "loss: 1.303682  [ 1488/ 3200]\n",
      "loss: 1.309579  [ 1504/ 3200]\n",
      "loss: 1.247185  [ 1520/ 3200]\n",
      "loss: 1.358968  [ 1536/ 3200]\n",
      "loss: 1.311724  [ 1552/ 3200]\n",
      "loss: 1.294309  [ 1568/ 3200]\n",
      "loss: 1.333795  [ 1584/ 3200]\n",
      "loss: 1.357398  [ 1600/ 3200]\n",
      "loss: 1.253781  [ 1616/ 3200]\n",
      "loss: 1.326805  [ 1632/ 3200]\n",
      "loss: 1.286259  [ 1648/ 3200]\n",
      "loss: 1.323996  [ 1664/ 3200]\n",
      "loss: 1.200677  [ 1680/ 3200]\n",
      "loss: 1.312707  [ 1696/ 3200]\n",
      "loss: 1.351588  [ 1712/ 3200]\n",
      "loss: 1.296467  [ 1728/ 3200]\n",
      "loss: 1.322705  [ 1744/ 3200]\n",
      "loss: 1.351639  [ 1760/ 3200]\n",
      "loss: 1.309518  [ 1776/ 3200]\n",
      "loss: 1.307005  [ 1792/ 3200]\n",
      "loss: 1.289691  [ 1808/ 3200]\n",
      "loss: 1.278401  [ 1824/ 3200]\n",
      "loss: 1.308081  [ 1840/ 3200]\n",
      "loss: 1.285624  [ 1856/ 3200]\n",
      "loss: 1.322009  [ 1872/ 3200]\n",
      "loss: 1.296268  [ 1888/ 3200]\n",
      "loss: 1.338461  [ 1904/ 3200]\n",
      "loss: 1.299693  [ 1920/ 3200]\n",
      "loss: 1.222698  [ 1936/ 3200]\n",
      "loss: 1.308972  [ 1952/ 3200]\n",
      "loss: 1.266521  [ 1968/ 3200]\n",
      "loss: 1.282260  [ 1984/ 3200]\n",
      "loss: 1.281793  [ 2000/ 3200]\n",
      "loss: 1.288149  [ 2016/ 3200]\n",
      "loss: 1.288710  [ 2032/ 3200]\n",
      "loss: 1.291237  [ 2048/ 3200]\n",
      "loss: 1.242302  [ 2064/ 3200]\n",
      "loss: 1.300866  [ 2080/ 3200]\n",
      "loss: 1.401087  [ 2096/ 3200]\n",
      "loss: 1.348755  [ 2112/ 3200]\n",
      "loss: 1.320094  [ 2128/ 3200]\n",
      "loss: 1.273258  [ 2144/ 3200]\n",
      "loss: 1.306414  [ 2160/ 3200]\n",
      "loss: 1.314887  [ 2176/ 3200]\n",
      "loss: 1.306546  [ 2192/ 3200]\n",
      "loss: 1.321002  [ 2208/ 3200]\n",
      "loss: 1.309960  [ 2224/ 3200]\n",
      "loss: 1.312647  [ 2240/ 3200]\n",
      "loss: 1.305123  [ 2256/ 3200]\n",
      "loss: 1.330886  [ 2272/ 3200]\n",
      "loss: 1.278986  [ 2288/ 3200]\n",
      "loss: 1.270353  [ 2304/ 3200]\n",
      "loss: 1.311740  [ 2320/ 3200]\n",
      "loss: 1.285084  [ 2336/ 3200]\n",
      "loss: 1.287858  [ 2352/ 3200]\n",
      "loss: 1.263452  [ 2368/ 3200]\n",
      "loss: 1.324271  [ 2384/ 3200]\n",
      "loss: 1.255388  [ 2400/ 3200]\n",
      "loss: 1.262851  [ 2416/ 3200]\n",
      "loss: 1.248954  [ 2432/ 3200]\n",
      "loss: 1.282508  [ 2448/ 3200]\n",
      "loss: 1.340994  [ 2464/ 3200]\n",
      "loss: 1.281100  [ 2480/ 3200]\n",
      "loss: 1.371369  [ 2496/ 3200]\n",
      "loss: 1.347717  [ 2512/ 3200]\n",
      "loss: 1.333433  [ 2528/ 3200]\n",
      "loss: 1.327516  [ 2544/ 3200]\n",
      "loss: 1.346164  [ 2560/ 3200]\n",
      "loss: 1.323837  [ 2576/ 3200]\n",
      "loss: 1.310918  [ 2592/ 3200]\n",
      "loss: 1.320669  [ 2608/ 3200]\n",
      "loss: 1.306754  [ 2624/ 3200]\n",
      "loss: 1.317353  [ 2640/ 3200]\n",
      "loss: 1.257275  [ 2656/ 3200]\n",
      "loss: 1.305870  [ 2672/ 3200]\n",
      "loss: 1.316026  [ 2688/ 3200]\n",
      "loss: 1.276508  [ 2704/ 3200]\n",
      "loss: 1.287641  [ 2720/ 3200]\n",
      "loss: 1.318788  [ 2736/ 3200]\n",
      "loss: 1.328663  [ 2752/ 3200]\n",
      "loss: 1.226903  [ 2768/ 3200]\n",
      "loss: 1.304062  [ 2784/ 3200]\n",
      "loss: 1.303311  [ 2800/ 3200]\n",
      "loss: 1.379727  [ 2816/ 3200]\n",
      "loss: 1.269980  [ 2832/ 3200]\n",
      "loss: 1.245784  [ 2848/ 3200]\n",
      "loss: 1.316589  [ 2864/ 3200]\n",
      "loss: 1.303041  [ 2880/ 3200]\n",
      "loss: 1.274518  [ 2896/ 3200]\n",
      "loss: 1.308798  [ 2912/ 3200]\n",
      "loss: 1.261831  [ 2928/ 3200]\n",
      "loss: 1.291458  [ 2944/ 3200]\n",
      "loss: 1.318127  [ 2960/ 3200]\n",
      "loss: 1.281045  [ 2976/ 3200]\n",
      "loss: 1.268101  [ 2992/ 3200]\n",
      "loss: 1.278182  [ 3008/ 3200]\n",
      "loss: 1.345831  [ 3024/ 3200]\n",
      "loss: 1.214143  [ 3040/ 3200]\n",
      "loss: 1.221461  [ 3056/ 3200]\n",
      "loss: 1.317583  [ 3072/ 3200]\n",
      "loss: 1.300324  [ 3088/ 3200]\n",
      "loss: 1.248617  [ 3104/ 3200]\n",
      "loss: 1.261532  [ 3120/ 3200]\n",
      "loss: 1.323779  [ 3136/ 3200]\n",
      "loss: 1.273594  [ 3152/ 3200]\n",
      "loss: 1.318052  [ 3168/ 3200]\n",
      "loss: 1.222794  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.080854\n",
      "f1 macro averaged score: 0.505377\n",
      "Accuracy               : 57.9%\n",
      "Confusion matrix       :\n",
      "tensor([[121,   0,  46,  33],\n",
      "        [ 21,   0, 147,  32],\n",
      "        [  1,   0, 184,  15],\n",
      "        [  0,   0,  42, 158]])\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 1.233503  [    0/ 3200]\n",
      "loss: 1.281395  [   16/ 3200]\n",
      "loss: 1.248176  [   32/ 3200]\n",
      "loss: 1.200578  [   48/ 3200]\n",
      "loss: 1.337923  [   64/ 3200]\n",
      "loss: 1.261313  [   80/ 3200]\n",
      "loss: 1.354629  [   96/ 3200]\n",
      "loss: 1.337261  [  112/ 3200]\n",
      "loss: 1.278352  [  128/ 3200]\n",
      "loss: 1.292471  [  144/ 3200]\n",
      "loss: 1.247498  [  160/ 3200]\n",
      "loss: 1.261275  [  176/ 3200]\n",
      "loss: 1.290489  [  192/ 3200]\n",
      "loss: 1.344441  [  208/ 3200]\n",
      "loss: 1.269464  [  224/ 3200]\n",
      "loss: 1.283375  [  240/ 3200]\n",
      "loss: 1.271083  [  256/ 3200]\n",
      "loss: 1.309421  [  272/ 3200]\n",
      "loss: 1.264098  [  288/ 3200]\n",
      "loss: 1.259326  [  304/ 3200]\n",
      "loss: 1.310809  [  320/ 3200]\n",
      "loss: 1.244877  [  336/ 3200]\n",
      "loss: 1.308282  [  352/ 3200]\n",
      "loss: 1.300458  [  368/ 3200]\n",
      "loss: 1.267452  [  384/ 3200]\n",
      "loss: 1.321576  [  400/ 3200]\n",
      "loss: 1.320283  [  416/ 3200]\n",
      "loss: 1.342221  [  432/ 3200]\n",
      "loss: 1.280287  [  448/ 3200]\n",
      "loss: 1.316672  [  464/ 3200]\n",
      "loss: 1.280709  [  480/ 3200]\n",
      "loss: 1.321934  [  496/ 3200]\n",
      "loss: 1.288190  [  512/ 3200]\n",
      "loss: 1.332915  [  528/ 3200]\n",
      "loss: 1.256126  [  544/ 3200]\n",
      "loss: 1.369018  [  560/ 3200]\n",
      "loss: 1.308930  [  576/ 3200]\n",
      "loss: 1.267241  [  592/ 3200]\n",
      "loss: 1.336428  [  608/ 3200]\n",
      "loss: 1.262355  [  624/ 3200]\n",
      "loss: 1.289881  [  640/ 3200]\n",
      "loss: 1.245015  [  656/ 3200]\n",
      "loss: 1.338734  [  672/ 3200]\n",
      "loss: 1.352012  [  688/ 3200]\n",
      "loss: 1.235780  [  704/ 3200]\n",
      "loss: 1.230456  [  720/ 3200]\n",
      "loss: 1.288975  [  736/ 3200]\n",
      "loss: 1.299026  [  752/ 3200]\n",
      "loss: 1.353725  [  768/ 3200]\n",
      "loss: 1.345527  [  784/ 3200]\n",
      "loss: 1.312416  [  800/ 3200]\n",
      "loss: 1.252468  [  816/ 3200]\n",
      "loss: 1.276072  [  832/ 3200]\n",
      "loss: 1.301828  [  848/ 3200]\n",
      "loss: 1.277505  [  864/ 3200]\n",
      "loss: 1.289516  [  880/ 3200]\n",
      "loss: 1.323934  [  896/ 3200]\n",
      "loss: 1.330666  [  912/ 3200]\n",
      "loss: 1.295450  [  928/ 3200]\n",
      "loss: 1.257879  [  944/ 3200]\n",
      "loss: 1.345848  [  960/ 3200]\n",
      "loss: 1.254731  [  976/ 3200]\n",
      "loss: 1.316187  [  992/ 3200]\n",
      "loss: 1.270166  [ 1008/ 3200]\n",
      "loss: 1.319135  [ 1024/ 3200]\n",
      "loss: 1.320863  [ 1040/ 3200]\n",
      "loss: 1.295355  [ 1056/ 3200]\n",
      "loss: 1.289372  [ 1072/ 3200]\n",
      "loss: 1.244799  [ 1088/ 3200]\n",
      "loss: 1.288396  [ 1104/ 3200]\n",
      "loss: 1.274177  [ 1120/ 3200]\n",
      "loss: 1.285887  [ 1136/ 3200]\n",
      "loss: 1.274276  [ 1152/ 3200]\n",
      "loss: 1.325326  [ 1168/ 3200]\n",
      "loss: 1.257597  [ 1184/ 3200]\n",
      "loss: 1.268019  [ 1200/ 3200]\n",
      "loss: 1.274898  [ 1216/ 3200]\n",
      "loss: 1.270820  [ 1232/ 3200]\n",
      "loss: 1.312672  [ 1248/ 3200]\n",
      "loss: 1.305291  [ 1264/ 3200]\n",
      "loss: 1.268998  [ 1280/ 3200]\n",
      "loss: 1.296458  [ 1296/ 3200]\n",
      "loss: 1.304037  [ 1312/ 3200]\n",
      "loss: 1.233144  [ 1328/ 3200]\n",
      "loss: 1.263299  [ 1344/ 3200]\n",
      "loss: 1.246953  [ 1360/ 3200]\n",
      "loss: 1.240899  [ 1376/ 3200]\n",
      "loss: 1.265051  [ 1392/ 3200]\n",
      "loss: 1.324039  [ 1408/ 3200]\n",
      "loss: 1.300212  [ 1424/ 3200]\n",
      "loss: 1.309626  [ 1440/ 3200]\n",
      "loss: 1.310399  [ 1456/ 3200]\n",
      "loss: 1.313976  [ 1472/ 3200]\n",
      "loss: 1.295648  [ 1488/ 3200]\n",
      "loss: 1.281984  [ 1504/ 3200]\n",
      "loss: 1.320386  [ 1520/ 3200]\n",
      "loss: 1.285631  [ 1536/ 3200]\n",
      "loss: 1.327464  [ 1552/ 3200]\n",
      "loss: 1.312590  [ 1568/ 3200]\n",
      "loss: 1.266207  [ 1584/ 3200]\n",
      "loss: 1.302758  [ 1600/ 3200]\n",
      "loss: 1.272677  [ 1616/ 3200]\n",
      "loss: 1.243816  [ 1632/ 3200]\n",
      "loss: 1.259447  [ 1648/ 3200]\n",
      "loss: 1.249824  [ 1664/ 3200]\n",
      "loss: 1.255113  [ 1680/ 3200]\n",
      "loss: 1.320174  [ 1696/ 3200]\n",
      "loss: 1.271892  [ 1712/ 3200]\n",
      "loss: 1.284200  [ 1728/ 3200]\n",
      "loss: 1.331617  [ 1744/ 3200]\n",
      "loss: 1.302329  [ 1760/ 3200]\n",
      "loss: 1.293672  [ 1776/ 3200]\n",
      "loss: 1.352533  [ 1792/ 3200]\n",
      "loss: 1.305837  [ 1808/ 3200]\n",
      "loss: 1.301287  [ 1824/ 3200]\n",
      "loss: 1.240686  [ 1840/ 3200]\n",
      "loss: 1.311867  [ 1856/ 3200]\n",
      "loss: 1.301641  [ 1872/ 3200]\n",
      "loss: 1.323520  [ 1888/ 3200]\n",
      "loss: 1.209900  [ 1904/ 3200]\n",
      "loss: 1.232441  [ 1920/ 3200]\n",
      "loss: 1.261740  [ 1936/ 3200]\n",
      "loss: 1.254159  [ 1952/ 3200]\n",
      "loss: 1.256699  [ 1968/ 3200]\n",
      "loss: 1.220111  [ 1984/ 3200]\n",
      "loss: 1.259903  [ 2000/ 3200]\n",
      "loss: 1.273832  [ 2016/ 3200]\n",
      "loss: 1.271495  [ 2032/ 3200]\n",
      "loss: 1.261797  [ 2048/ 3200]\n",
      "loss: 1.306033  [ 2064/ 3200]\n",
      "loss: 1.253871  [ 2080/ 3200]\n",
      "loss: 1.263148  [ 2096/ 3200]\n",
      "loss: 1.280781  [ 2112/ 3200]\n",
      "loss: 1.303050  [ 2128/ 3200]\n",
      "loss: 1.247915  [ 2144/ 3200]\n",
      "loss: 1.238882  [ 2160/ 3200]\n",
      "loss: 1.272684  [ 2176/ 3200]\n",
      "loss: 1.319115  [ 2192/ 3200]\n",
      "loss: 1.287282  [ 2208/ 3200]\n",
      "loss: 1.265226  [ 2224/ 3200]\n",
      "loss: 1.327522  [ 2240/ 3200]\n",
      "loss: 1.250649  [ 2256/ 3200]\n",
      "loss: 1.196487  [ 2272/ 3200]\n",
      "loss: 1.285166  [ 2288/ 3200]\n",
      "loss: 1.319724  [ 2304/ 3200]\n",
      "loss: 1.289412  [ 2320/ 3200]\n",
      "loss: 1.285195  [ 2336/ 3200]\n",
      "loss: 1.250237  [ 2352/ 3200]\n",
      "loss: 1.247865  [ 2368/ 3200]\n",
      "loss: 1.283448  [ 2384/ 3200]\n",
      "loss: 1.200392  [ 2400/ 3200]\n",
      "loss: 1.272107  [ 2416/ 3200]\n",
      "loss: 1.297824  [ 2432/ 3200]\n",
      "loss: 1.258233  [ 2448/ 3200]\n",
      "loss: 1.250827  [ 2464/ 3200]\n",
      "loss: 1.299453  [ 2480/ 3200]\n",
      "loss: 1.322601  [ 2496/ 3200]\n",
      "loss: 1.245601  [ 2512/ 3200]\n",
      "loss: 1.321594  [ 2528/ 3200]\n",
      "loss: 1.312096  [ 2544/ 3200]\n",
      "loss: 1.353844  [ 2560/ 3200]\n",
      "loss: 1.361424  [ 2576/ 3200]\n",
      "loss: 1.247141  [ 2592/ 3200]\n",
      "loss: 1.333950  [ 2608/ 3200]\n",
      "loss: 1.257830  [ 2624/ 3200]\n",
      "loss: 1.304584  [ 2640/ 3200]\n",
      "loss: 1.290798  [ 2656/ 3200]\n",
      "loss: 1.317832  [ 2672/ 3200]\n",
      "loss: 1.297739  [ 2688/ 3200]\n",
      "loss: 1.269804  [ 2704/ 3200]\n",
      "loss: 1.275564  [ 2720/ 3200]\n",
      "loss: 1.219487  [ 2736/ 3200]\n",
      "loss: 1.260832  [ 2752/ 3200]\n",
      "loss: 1.236050  [ 2768/ 3200]\n",
      "loss: 1.328942  [ 2784/ 3200]\n",
      "loss: 1.310468  [ 2800/ 3200]\n",
      "loss: 1.316435  [ 2816/ 3200]\n",
      "loss: 1.245901  [ 2832/ 3200]\n",
      "loss: 1.282252  [ 2848/ 3200]\n",
      "loss: 1.292509  [ 2864/ 3200]\n",
      "loss: 1.192553  [ 2880/ 3200]\n",
      "loss: 1.192647  [ 2896/ 3200]\n",
      "loss: 1.262545  [ 2912/ 3200]\n",
      "loss: 1.310315  [ 2928/ 3200]\n",
      "loss: 1.308319  [ 2944/ 3200]\n",
      "loss: 1.283493  [ 2960/ 3200]\n",
      "loss: 1.293149  [ 2976/ 3200]\n",
      "loss: 1.340131  [ 2992/ 3200]\n",
      "loss: 1.230149  [ 3008/ 3200]\n",
      "loss: 1.310637  [ 3024/ 3200]\n",
      "loss: 1.286254  [ 3040/ 3200]\n",
      "loss: 1.279550  [ 3056/ 3200]\n",
      "loss: 1.212264  [ 3072/ 3200]\n",
      "loss: 1.300763  [ 3088/ 3200]\n",
      "loss: 1.270041  [ 3104/ 3200]\n",
      "loss: 1.327500  [ 3120/ 3200]\n",
      "loss: 1.223821  [ 3136/ 3200]\n",
      "loss: 1.329309  [ 3152/ 3200]\n",
      "loss: 1.325079  [ 3168/ 3200]\n",
      "loss: 1.286845  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.079418\n",
      "f1 macro averaged score: 0.515837\n",
      "Accuracy               : 59.8%\n",
      "Confusion matrix       :\n",
      "tensor([[192,   0,   5,   3],\n",
      "        [ 95,   1,  83,  21],\n",
      "        [ 37,   0, 150,  13],\n",
      "        [ 32,   0,  33, 135]])\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 1.219882  [    0/ 3200]\n",
      "loss: 1.369116  [   16/ 3200]\n",
      "loss: 1.356891  [   32/ 3200]\n",
      "loss: 1.315589  [   48/ 3200]\n",
      "loss: 1.290214  [   64/ 3200]\n",
      "loss: 1.309109  [   80/ 3200]\n",
      "loss: 1.291731  [   96/ 3200]\n",
      "loss: 1.233955  [  112/ 3200]\n",
      "loss: 1.284814  [  128/ 3200]\n",
      "loss: 1.293576  [  144/ 3200]\n",
      "loss: 1.268050  [  160/ 3200]\n",
      "loss: 1.237420  [  176/ 3200]\n",
      "loss: 1.260081  [  192/ 3200]\n",
      "loss: 1.266689  [  208/ 3200]\n",
      "loss: 1.323254  [  224/ 3200]\n",
      "loss: 1.274833  [  240/ 3200]\n",
      "loss: 1.266390  [  256/ 3200]\n",
      "loss: 1.259089  [  272/ 3200]\n",
      "loss: 1.220900  [  288/ 3200]\n",
      "loss: 1.337480  [  304/ 3200]\n",
      "loss: 1.270882  [  320/ 3200]\n",
      "loss: 1.292306  [  336/ 3200]\n",
      "loss: 1.274338  [  352/ 3200]\n",
      "loss: 1.269597  [  368/ 3200]\n",
      "loss: 1.306221  [  384/ 3200]\n",
      "loss: 1.301506  [  400/ 3200]\n",
      "loss: 1.283301  [  416/ 3200]\n",
      "loss: 1.286311  [  432/ 3200]\n",
      "loss: 1.280586  [  448/ 3200]\n",
      "loss: 1.218840  [  464/ 3200]\n",
      "loss: 1.171365  [  480/ 3200]\n",
      "loss: 1.281098  [  496/ 3200]\n",
      "loss: 1.287604  [  512/ 3200]\n",
      "loss: 1.257871  [  528/ 3200]\n",
      "loss: 1.333377  [  544/ 3200]\n",
      "loss: 1.244673  [  560/ 3200]\n",
      "loss: 1.286297  [  576/ 3200]\n",
      "loss: 1.276427  [  592/ 3200]\n",
      "loss: 1.317797  [  608/ 3200]\n",
      "loss: 1.317166  [  624/ 3200]\n",
      "loss: 1.270649  [  640/ 3200]\n",
      "loss: 1.256511  [  656/ 3200]\n",
      "loss: 1.260735  [  672/ 3200]\n",
      "loss: 1.285469  [  688/ 3200]\n",
      "loss: 1.284884  [  704/ 3200]\n",
      "loss: 1.295678  [  720/ 3200]\n",
      "loss: 1.247498  [  736/ 3200]\n",
      "loss: 1.289586  [  752/ 3200]\n",
      "loss: 1.304801  [  768/ 3200]\n",
      "loss: 1.285459  [  784/ 3200]\n",
      "loss: 1.267689  [  800/ 3200]\n",
      "loss: 1.293586  [  816/ 3200]\n",
      "loss: 1.213128  [  832/ 3200]\n",
      "loss: 1.248721  [  848/ 3200]\n",
      "loss: 1.309562  [  864/ 3200]\n",
      "loss: 1.284948  [  880/ 3200]\n",
      "loss: 1.260750  [  896/ 3200]\n",
      "loss: 1.221090  [  912/ 3200]\n",
      "loss: 1.252794  [  928/ 3200]\n",
      "loss: 1.248312  [  944/ 3200]\n",
      "loss: 1.367653  [  960/ 3200]\n",
      "loss: 1.274749  [  976/ 3200]\n",
      "loss: 1.205482  [  992/ 3200]\n",
      "loss: 1.238083  [ 1008/ 3200]\n",
      "loss: 1.196903  [ 1024/ 3200]\n",
      "loss: 1.264034  [ 1040/ 3200]\n",
      "loss: 1.277136  [ 1056/ 3200]\n",
      "loss: 1.307038  [ 1072/ 3200]\n",
      "loss: 1.271968  [ 1088/ 3200]\n",
      "loss: 1.235426  [ 1104/ 3200]\n",
      "loss: 1.280027  [ 1120/ 3200]\n",
      "loss: 1.258665  [ 1136/ 3200]\n",
      "loss: 1.283086  [ 1152/ 3200]\n",
      "loss: 1.388486  [ 1168/ 3200]\n",
      "loss: 1.272465  [ 1184/ 3200]\n",
      "loss: 1.230299  [ 1200/ 3200]\n",
      "loss: 1.298936  [ 1216/ 3200]\n",
      "loss: 1.329723  [ 1232/ 3200]\n",
      "loss: 1.303105  [ 1248/ 3200]\n",
      "loss: 1.215151  [ 1264/ 3200]\n",
      "loss: 1.218842  [ 1280/ 3200]\n",
      "loss: 1.253933  [ 1296/ 3200]\n",
      "loss: 1.161857  [ 1312/ 3200]\n",
      "loss: 1.290770  [ 1328/ 3200]\n",
      "loss: 1.293864  [ 1344/ 3200]\n",
      "loss: 1.258591  [ 1360/ 3200]\n",
      "loss: 1.291577  [ 1376/ 3200]\n",
      "loss: 1.329327  [ 1392/ 3200]\n",
      "loss: 1.261854  [ 1408/ 3200]\n",
      "loss: 1.308461  [ 1424/ 3200]\n",
      "loss: 1.235004  [ 1440/ 3200]\n",
      "loss: 1.217973  [ 1456/ 3200]\n",
      "loss: 1.233102  [ 1472/ 3200]\n",
      "loss: 1.201112  [ 1488/ 3200]\n",
      "loss: 1.230627  [ 1504/ 3200]\n",
      "loss: 1.348170  [ 1520/ 3200]\n",
      "loss: 1.314173  [ 1536/ 3200]\n",
      "loss: 1.287872  [ 1552/ 3200]\n",
      "loss: 1.372436  [ 1568/ 3200]\n",
      "loss: 1.290183  [ 1584/ 3200]\n",
      "loss: 1.181937  [ 1600/ 3200]\n",
      "loss: 1.202865  [ 1616/ 3200]\n",
      "loss: 1.278891  [ 1632/ 3200]\n",
      "loss: 1.251431  [ 1648/ 3200]\n",
      "loss: 1.248174  [ 1664/ 3200]\n",
      "loss: 1.182486  [ 1680/ 3200]\n",
      "loss: 1.281542  [ 1696/ 3200]\n",
      "loss: 1.220376  [ 1712/ 3200]\n",
      "loss: 1.262821  [ 1728/ 3200]\n",
      "loss: 1.206819  [ 1744/ 3200]\n",
      "loss: 1.279430  [ 1760/ 3200]\n",
      "loss: 1.236506  [ 1776/ 3200]\n",
      "loss: 1.301605  [ 1792/ 3200]\n",
      "loss: 1.271765  [ 1808/ 3200]\n",
      "loss: 1.245098  [ 1824/ 3200]\n",
      "loss: 1.274846  [ 1840/ 3200]\n",
      "loss: 1.310255  [ 1856/ 3200]\n",
      "loss: 1.267159  [ 1872/ 3200]\n",
      "loss: 1.276487  [ 1888/ 3200]\n",
      "loss: 1.223815  [ 1904/ 3200]\n",
      "loss: 1.256195  [ 1920/ 3200]\n",
      "loss: 1.316206  [ 1936/ 3200]\n",
      "loss: 1.230879  [ 1952/ 3200]\n",
      "loss: 1.279420  [ 1968/ 3200]\n",
      "loss: 1.284184  [ 1984/ 3200]\n",
      "loss: 1.297420  [ 2000/ 3200]\n",
      "loss: 1.316543  [ 2016/ 3200]\n",
      "loss: 1.256233  [ 2032/ 3200]\n",
      "loss: 1.289999  [ 2048/ 3200]\n",
      "loss: 1.281959  [ 2064/ 3200]\n",
      "loss: 1.294639  [ 2080/ 3200]\n",
      "loss: 1.282241  [ 2096/ 3200]\n",
      "loss: 1.231636  [ 2112/ 3200]\n",
      "loss: 1.276374  [ 2128/ 3200]\n",
      "loss: 1.264370  [ 2144/ 3200]\n",
      "loss: 1.256973  [ 2160/ 3200]\n",
      "loss: 1.226127  [ 2176/ 3200]\n",
      "loss: 1.260441  [ 2192/ 3200]\n",
      "loss: 1.193872  [ 2208/ 3200]\n",
      "loss: 1.205496  [ 2224/ 3200]\n",
      "loss: 1.327005  [ 2240/ 3200]\n",
      "loss: 1.238474  [ 2256/ 3200]\n",
      "loss: 1.288165  [ 2272/ 3200]\n",
      "loss: 1.305589  [ 2288/ 3200]\n",
      "loss: 1.353021  [ 2304/ 3200]\n",
      "loss: 1.254562  [ 2320/ 3200]\n",
      "loss: 1.279301  [ 2336/ 3200]\n",
      "loss: 1.356968  [ 2352/ 3200]\n",
      "loss: 1.247980  [ 2368/ 3200]\n",
      "loss: 1.265053  [ 2384/ 3200]\n",
      "loss: 1.265957  [ 2400/ 3200]\n",
      "loss: 1.266284  [ 2416/ 3200]\n",
      "loss: 1.255579  [ 2432/ 3200]\n",
      "loss: 1.272135  [ 2448/ 3200]\n",
      "loss: 1.274136  [ 2464/ 3200]\n",
      "loss: 1.276471  [ 2480/ 3200]\n",
      "loss: 1.262667  [ 2496/ 3200]\n",
      "loss: 1.291259  [ 2512/ 3200]\n",
      "loss: 1.285550  [ 2528/ 3200]\n",
      "loss: 1.175551  [ 2544/ 3200]\n",
      "loss: 1.325698  [ 2560/ 3200]\n",
      "loss: 1.274445  [ 2576/ 3200]\n",
      "loss: 1.255473  [ 2592/ 3200]\n",
      "loss: 1.254371  [ 2608/ 3200]\n",
      "loss: 1.298836  [ 2624/ 3200]\n",
      "loss: 1.214252  [ 2640/ 3200]\n",
      "loss: 1.198369  [ 2656/ 3200]\n",
      "loss: 1.293709  [ 2672/ 3200]\n",
      "loss: 1.287033  [ 2688/ 3200]\n",
      "loss: 1.188158  [ 2704/ 3200]\n",
      "loss: 1.270161  [ 2720/ 3200]\n",
      "loss: 1.393584  [ 2736/ 3200]\n",
      "loss: 1.200822  [ 2752/ 3200]\n",
      "loss: 1.269162  [ 2768/ 3200]\n",
      "loss: 1.183472  [ 2784/ 3200]\n",
      "loss: 1.126314  [ 2800/ 3200]\n",
      "loss: 1.222656  [ 2816/ 3200]\n",
      "loss: 1.323391  [ 2832/ 3200]\n",
      "loss: 1.216508  [ 2848/ 3200]\n",
      "loss: 1.264240  [ 2864/ 3200]\n",
      "loss: 1.326431  [ 2880/ 3200]\n",
      "loss: 1.243707  [ 2896/ 3200]\n",
      "loss: 1.232110  [ 2912/ 3200]\n",
      "loss: 1.248764  [ 2928/ 3200]\n",
      "loss: 1.309252  [ 2944/ 3200]\n",
      "loss: 1.207290  [ 2960/ 3200]\n",
      "loss: 1.216253  [ 2976/ 3200]\n",
      "loss: 1.212188  [ 2992/ 3200]\n",
      "loss: 1.232264  [ 3008/ 3200]\n",
      "loss: 1.269790  [ 3024/ 3200]\n",
      "loss: 1.371495  [ 3040/ 3200]\n",
      "loss: 1.251555  [ 3056/ 3200]\n",
      "loss: 1.260361  [ 3072/ 3200]\n",
      "loss: 1.178703  [ 3088/ 3200]\n",
      "loss: 1.313263  [ 3104/ 3200]\n",
      "loss: 1.272454  [ 3120/ 3200]\n",
      "loss: 1.252999  [ 3136/ 3200]\n",
      "loss: 1.313651  [ 3152/ 3200]\n",
      "loss: 1.298727  [ 3168/ 3200]\n",
      "loss: 1.223861  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.078614\n",
      "f1 macro averaged score: 0.467304\n",
      "Accuracy               : 51.9%\n",
      "Confusion matrix       :\n",
      "tensor([[110,   0,  88,   2],\n",
      "        [ 19,   0, 176,   5],\n",
      "        [  1,   0, 198,   1],\n",
      "        [  1,   0,  92, 107]])\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 1.306991  [    0/ 3200]\n",
      "loss: 1.274737  [   16/ 3200]\n",
      "loss: 1.287741  [   32/ 3200]\n",
      "loss: 1.229309  [   48/ 3200]\n",
      "loss: 1.243426  [   64/ 3200]\n",
      "loss: 1.209306  [   80/ 3200]\n",
      "loss: 1.269942  [   96/ 3200]\n",
      "loss: 1.183207  [  112/ 3200]\n",
      "loss: 1.280638  [  128/ 3200]\n",
      "loss: 1.217818  [  144/ 3200]\n",
      "loss: 1.264939  [  160/ 3200]\n",
      "loss: 1.307134  [  176/ 3200]\n",
      "loss: 1.229667  [  192/ 3200]\n",
      "loss: 1.199952  [  208/ 3200]\n",
      "loss: 1.145885  [  224/ 3200]\n",
      "loss: 1.201185  [  240/ 3200]\n",
      "loss: 1.265665  [  256/ 3200]\n",
      "loss: 1.302869  [  272/ 3200]\n",
      "loss: 1.277634  [  288/ 3200]\n",
      "loss: 1.205621  [  304/ 3200]\n",
      "loss: 1.271322  [  320/ 3200]\n",
      "loss: 1.154620  [  336/ 3200]\n",
      "loss: 1.274039  [  352/ 3200]\n",
      "loss: 1.140086  [  368/ 3200]\n",
      "loss: 1.284026  [  384/ 3200]\n",
      "loss: 1.157877  [  400/ 3200]\n",
      "loss: 1.213191  [  416/ 3200]\n",
      "loss: 1.171106  [  432/ 3200]\n",
      "loss: 1.202990  [  448/ 3200]\n",
      "loss: 1.270029  [  464/ 3200]\n",
      "loss: 1.295360  [  480/ 3200]\n",
      "loss: 1.192162  [  496/ 3200]\n",
      "loss: 1.335763  [  512/ 3200]\n",
      "loss: 1.265242  [  528/ 3200]\n",
      "loss: 1.360890  [  544/ 3200]\n",
      "loss: 1.292334  [  560/ 3200]\n",
      "loss: 1.236484  [  576/ 3200]\n",
      "loss: 1.336857  [  592/ 3200]\n",
      "loss: 1.267916  [  608/ 3200]\n",
      "loss: 1.234652  [  624/ 3200]\n",
      "loss: 1.275864  [  640/ 3200]\n",
      "loss: 1.222249  [  656/ 3200]\n",
      "loss: 1.218950  [  672/ 3200]\n",
      "loss: 1.329505  [  688/ 3200]\n",
      "loss: 1.232754  [  704/ 3200]\n",
      "loss: 1.192725  [  720/ 3200]\n",
      "loss: 1.260570  [  736/ 3200]\n",
      "loss: 1.252611  [  752/ 3200]\n",
      "loss: 1.274446  [  768/ 3200]\n",
      "loss: 1.182922  [  784/ 3200]\n",
      "loss: 1.190375  [  800/ 3200]\n",
      "loss: 1.262266  [  816/ 3200]\n",
      "loss: 1.260666  [  832/ 3200]\n",
      "loss: 1.160908  [  848/ 3200]\n",
      "loss: 1.284423  [  864/ 3200]\n",
      "loss: 1.243699  [  880/ 3200]\n",
      "loss: 1.286692  [  896/ 3200]\n",
      "loss: 1.307103  [  912/ 3200]\n",
      "loss: 1.265249  [  928/ 3200]\n",
      "loss: 1.278533  [  944/ 3200]\n",
      "loss: 1.289935  [  960/ 3200]\n",
      "loss: 1.315050  [  976/ 3200]\n",
      "loss: 1.271567  [  992/ 3200]\n",
      "loss: 1.274252  [ 1008/ 3200]\n",
      "loss: 1.190549  [ 1024/ 3200]\n",
      "loss: 1.201125  [ 1040/ 3200]\n",
      "loss: 1.284534  [ 1056/ 3200]\n",
      "loss: 1.211911  [ 1072/ 3200]\n",
      "loss: 1.227425  [ 1088/ 3200]\n",
      "loss: 1.170893  [ 1104/ 3200]\n",
      "loss: 1.263152  [ 1120/ 3200]\n",
      "loss: 1.378434  [ 1136/ 3200]\n",
      "loss: 1.276163  [ 1152/ 3200]\n",
      "loss: 1.235194  [ 1168/ 3200]\n",
      "loss: 1.297081  [ 1184/ 3200]\n",
      "loss: 1.311342  [ 1200/ 3200]\n",
      "loss: 1.234503  [ 1216/ 3200]\n",
      "loss: 1.281379  [ 1232/ 3200]\n",
      "loss: 1.232307  [ 1248/ 3200]\n",
      "loss: 1.243108  [ 1264/ 3200]\n",
      "loss: 1.157512  [ 1280/ 3200]\n",
      "loss: 1.182471  [ 1296/ 3200]\n",
      "loss: 1.254007  [ 1312/ 3200]\n",
      "loss: 1.262353  [ 1328/ 3200]\n",
      "loss: 1.368365  [ 1344/ 3200]\n",
      "loss: 1.235440  [ 1360/ 3200]\n",
      "loss: 1.263575  [ 1376/ 3200]\n",
      "loss: 1.279613  [ 1392/ 3200]\n",
      "loss: 1.267920  [ 1408/ 3200]\n",
      "loss: 1.165948  [ 1424/ 3200]\n",
      "loss: 1.281731  [ 1440/ 3200]\n",
      "loss: 1.279119  [ 1456/ 3200]\n",
      "loss: 1.279208  [ 1472/ 3200]\n",
      "loss: 1.172288  [ 1488/ 3200]\n",
      "loss: 1.246159  [ 1504/ 3200]\n",
      "loss: 1.292588  [ 1520/ 3200]\n",
      "loss: 1.282478  [ 1536/ 3200]\n",
      "loss: 1.312809  [ 1552/ 3200]\n",
      "loss: 1.183168  [ 1568/ 3200]\n",
      "loss: 1.219232  [ 1584/ 3200]\n",
      "loss: 1.216892  [ 1600/ 3200]\n",
      "loss: 1.306839  [ 1616/ 3200]\n",
      "loss: 1.219325  [ 1632/ 3200]\n",
      "loss: 1.268841  [ 1648/ 3200]\n",
      "loss: 1.265572  [ 1664/ 3200]\n",
      "loss: 1.149269  [ 1680/ 3200]\n",
      "loss: 1.279132  [ 1696/ 3200]\n",
      "loss: 1.277280  [ 1712/ 3200]\n",
      "loss: 1.269681  [ 1728/ 3200]\n",
      "loss: 1.302167  [ 1744/ 3200]\n",
      "loss: 1.210731  [ 1760/ 3200]\n",
      "loss: 1.185407  [ 1776/ 3200]\n",
      "loss: 1.229234  [ 1792/ 3200]\n",
      "loss: 1.301296  [ 1808/ 3200]\n",
      "loss: 1.205940  [ 1824/ 3200]\n",
      "loss: 1.279706  [ 1840/ 3200]\n",
      "loss: 1.287962  [ 1856/ 3200]\n",
      "loss: 1.155511  [ 1872/ 3200]\n",
      "loss: 1.218961  [ 1888/ 3200]\n",
      "loss: 1.252529  [ 1904/ 3200]\n",
      "loss: 1.235522  [ 1920/ 3200]\n",
      "loss: 1.287420  [ 1936/ 3200]\n",
      "loss: 1.170698  [ 1952/ 3200]\n",
      "loss: 1.265105  [ 1968/ 3200]\n",
      "loss: 1.263508  [ 1984/ 3200]\n",
      "loss: 1.172791  [ 2000/ 3200]\n",
      "loss: 1.223645  [ 2016/ 3200]\n",
      "loss: 1.252530  [ 2032/ 3200]\n",
      "loss: 1.241148  [ 2048/ 3200]\n",
      "loss: 1.160719  [ 2064/ 3200]\n",
      "loss: 1.267935  [ 2080/ 3200]\n",
      "loss: 1.221323  [ 2096/ 3200]\n",
      "loss: 1.296921  [ 2112/ 3200]\n",
      "loss: 1.181546  [ 2128/ 3200]\n",
      "loss: 1.215301  [ 2144/ 3200]\n",
      "loss: 1.217761  [ 2160/ 3200]\n",
      "loss: 1.324617  [ 2176/ 3200]\n",
      "loss: 1.333236  [ 2192/ 3200]\n",
      "loss: 1.226653  [ 2208/ 3200]\n",
      "loss: 1.280913  [ 2224/ 3200]\n",
      "loss: 1.292365  [ 2240/ 3200]\n",
      "loss: 1.273482  [ 2256/ 3200]\n",
      "loss: 1.135157  [ 2272/ 3200]\n",
      "loss: 1.261142  [ 2288/ 3200]\n",
      "loss: 1.321088  [ 2304/ 3200]\n",
      "loss: 1.228333  [ 2320/ 3200]\n",
      "loss: 1.288427  [ 2336/ 3200]\n",
      "loss: 1.208596  [ 2352/ 3200]\n",
      "loss: 1.307003  [ 2368/ 3200]\n",
      "loss: 1.275645  [ 2384/ 3200]\n",
      "loss: 1.210197  [ 2400/ 3200]\n",
      "loss: 1.227182  [ 2416/ 3200]\n",
      "loss: 1.318463  [ 2432/ 3200]\n",
      "loss: 1.231690  [ 2448/ 3200]\n",
      "loss: 1.273267  [ 2464/ 3200]\n",
      "loss: 1.290892  [ 2480/ 3200]\n",
      "loss: 1.233390  [ 2496/ 3200]\n",
      "loss: 1.237911  [ 2512/ 3200]\n",
      "loss: 1.317555  [ 2528/ 3200]\n",
      "loss: 1.217640  [ 2544/ 3200]\n",
      "loss: 1.217648  [ 2560/ 3200]\n",
      "loss: 1.173964  [ 2576/ 3200]\n",
      "loss: 1.252587  [ 2592/ 3200]\n",
      "loss: 1.226195  [ 2608/ 3200]\n",
      "loss: 1.204203  [ 2624/ 3200]\n",
      "loss: 1.246335  [ 2640/ 3200]\n",
      "loss: 1.194702  [ 2656/ 3200]\n",
      "loss: 1.231695  [ 2672/ 3200]\n",
      "loss: 1.147369  [ 2688/ 3200]\n",
      "loss: 1.233159  [ 2704/ 3200]\n",
      "loss: 1.235399  [ 2720/ 3200]\n",
      "loss: 1.286528  [ 2736/ 3200]\n",
      "loss: 1.188669  [ 2752/ 3200]\n",
      "loss: 1.245151  [ 2768/ 3200]\n",
      "loss: 1.331110  [ 2784/ 3200]\n",
      "loss: 1.225694  [ 2800/ 3200]\n",
      "loss: 1.229619  [ 2816/ 3200]\n",
      "loss: 1.248111  [ 2832/ 3200]\n",
      "loss: 1.216057  [ 2848/ 3200]\n",
      "loss: 1.166035  [ 2864/ 3200]\n",
      "loss: 1.142770  [ 2880/ 3200]\n",
      "loss: 1.281711  [ 2896/ 3200]\n",
      "loss: 1.204781  [ 2912/ 3200]\n",
      "loss: 1.174602  [ 2928/ 3200]\n",
      "loss: 1.246668  [ 2944/ 3200]\n",
      "loss: 1.312671  [ 2960/ 3200]\n",
      "loss: 1.165928  [ 2976/ 3200]\n",
      "loss: 1.211408  [ 2992/ 3200]\n",
      "loss: 1.328385  [ 3008/ 3200]\n",
      "loss: 1.240156  [ 3024/ 3200]\n",
      "loss: 1.249821  [ 3040/ 3200]\n",
      "loss: 1.256337  [ 3056/ 3200]\n",
      "loss: 1.269876  [ 3072/ 3200]\n",
      "loss: 1.214385  [ 3088/ 3200]\n",
      "loss: 1.260676  [ 3104/ 3200]\n",
      "loss: 1.177191  [ 3120/ 3200]\n",
      "loss: 1.256025  [ 3136/ 3200]\n",
      "loss: 1.228064  [ 3152/ 3200]\n",
      "loss: 1.189757  [ 3168/ 3200]\n",
      "loss: 1.257130  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.076879\n",
      "f1 macro averaged score: 0.606625\n",
      "Accuracy               : 62.5%\n",
      "Confusion matrix       :\n",
      "tensor([[183,  10,   3,   4],\n",
      "        [ 64,  63,  31,  42],\n",
      "        [ 13,  47, 102,  38],\n",
      "        [ 13,  26,   9, 152]])\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 1.156974  [    0/ 3200]\n",
      "loss: 1.102103  [   16/ 3200]\n",
      "loss: 1.152610  [   32/ 3200]\n",
      "loss: 1.236776  [   48/ 3200]\n",
      "loss: 1.278240  [   64/ 3200]\n",
      "loss: 1.176262  [   80/ 3200]\n",
      "loss: 1.323116  [   96/ 3200]\n",
      "loss: 1.291743  [  112/ 3200]\n",
      "loss: 1.244465  [  128/ 3200]\n",
      "loss: 1.218348  [  144/ 3200]\n",
      "loss: 1.268237  [  160/ 3200]\n",
      "loss: 1.172217  [  176/ 3200]\n",
      "loss: 1.240418  [  192/ 3200]\n",
      "loss: 1.147305  [  208/ 3200]\n",
      "loss: 1.240832  [  224/ 3200]\n",
      "loss: 1.274196  [  240/ 3200]\n",
      "loss: 1.329411  [  256/ 3200]\n",
      "loss: 1.200385  [  272/ 3200]\n",
      "loss: 1.272539  [  288/ 3200]\n",
      "loss: 1.308326  [  304/ 3200]\n",
      "loss: 1.100088  [  320/ 3200]\n",
      "loss: 1.133860  [  336/ 3200]\n",
      "loss: 1.160596  [  352/ 3200]\n",
      "loss: 1.266127  [  368/ 3200]\n",
      "loss: 1.211890  [  384/ 3200]\n",
      "loss: 1.343858  [  400/ 3200]\n",
      "loss: 1.174053  [  416/ 3200]\n",
      "loss: 1.338925  [  432/ 3200]\n",
      "loss: 1.213804  [  448/ 3200]\n",
      "loss: 1.294541  [  464/ 3200]\n",
      "loss: 1.161687  [  480/ 3200]\n",
      "loss: 1.255557  [  496/ 3200]\n",
      "loss: 1.115262  [  512/ 3200]\n",
      "loss: 1.223989  [  528/ 3200]\n",
      "loss: 1.163527  [  544/ 3200]\n",
      "loss: 1.292463  [  560/ 3200]\n",
      "loss: 1.202046  [  576/ 3200]\n",
      "loss: 1.319137  [  592/ 3200]\n",
      "loss: 1.112153  [  608/ 3200]\n",
      "loss: 1.252722  [  624/ 3200]\n",
      "loss: 1.260319  [  640/ 3200]\n",
      "loss: 1.202138  [  656/ 3200]\n",
      "loss: 1.196961  [  672/ 3200]\n",
      "loss: 1.250365  [  688/ 3200]\n",
      "loss: 1.222328  [  704/ 3200]\n",
      "loss: 1.283914  [  720/ 3200]\n",
      "loss: 1.219759  [  736/ 3200]\n",
      "loss: 1.270342  [  752/ 3200]\n",
      "loss: 1.272202  [  768/ 3200]\n",
      "loss: 1.237135  [  784/ 3200]\n",
      "loss: 1.285416  [  800/ 3200]\n",
      "loss: 1.186107  [  816/ 3200]\n",
      "loss: 1.235699  [  832/ 3200]\n",
      "loss: 1.296429  [  848/ 3200]\n",
      "loss: 1.244782  [  864/ 3200]\n",
      "loss: 1.214776  [  880/ 3200]\n",
      "loss: 1.315394  [  896/ 3200]\n",
      "loss: 1.293202  [  912/ 3200]\n",
      "loss: 1.192961  [  928/ 3200]\n",
      "loss: 1.109905  [  944/ 3200]\n",
      "loss: 1.297716  [  960/ 3200]\n",
      "loss: 1.236325  [  976/ 3200]\n",
      "loss: 1.250708  [  992/ 3200]\n",
      "loss: 1.254550  [ 1008/ 3200]\n",
      "loss: 1.212427  [ 1024/ 3200]\n",
      "loss: 1.329141  [ 1040/ 3200]\n",
      "loss: 1.220885  [ 1056/ 3200]\n",
      "loss: 1.259821  [ 1072/ 3200]\n",
      "loss: 1.253996  [ 1088/ 3200]\n",
      "loss: 1.235026  [ 1104/ 3200]\n",
      "loss: 1.164997  [ 1120/ 3200]\n",
      "loss: 1.179220  [ 1136/ 3200]\n",
      "loss: 1.262756  [ 1152/ 3200]\n",
      "loss: 1.234078  [ 1168/ 3200]\n",
      "loss: 1.240307  [ 1184/ 3200]\n",
      "loss: 1.208794  [ 1200/ 3200]\n",
      "loss: 1.197363  [ 1216/ 3200]\n",
      "loss: 1.234427  [ 1232/ 3200]\n",
      "loss: 1.195672  [ 1248/ 3200]\n",
      "loss: 1.208918  [ 1264/ 3200]\n",
      "loss: 1.229774  [ 1280/ 3200]\n",
      "loss: 1.179492  [ 1296/ 3200]\n",
      "loss: 1.303335  [ 1312/ 3200]\n",
      "loss: 1.263259  [ 1328/ 3200]\n",
      "loss: 1.283385  [ 1344/ 3200]\n",
      "loss: 1.211720  [ 1360/ 3200]\n",
      "loss: 1.210045  [ 1376/ 3200]\n",
      "loss: 1.338139  [ 1392/ 3200]\n",
      "loss: 1.240899  [ 1408/ 3200]\n",
      "loss: 1.278180  [ 1424/ 3200]\n",
      "loss: 1.248168  [ 1440/ 3200]\n",
      "loss: 1.243614  [ 1456/ 3200]\n",
      "loss: 1.234447  [ 1472/ 3200]\n",
      "loss: 1.181234  [ 1488/ 3200]\n",
      "loss: 1.230373  [ 1504/ 3200]\n",
      "loss: 1.281082  [ 1520/ 3200]\n",
      "loss: 1.197099  [ 1536/ 3200]\n",
      "loss: 1.136055  [ 1552/ 3200]\n",
      "loss: 1.065655  [ 1568/ 3200]\n",
      "loss: 1.295667  [ 1584/ 3200]\n",
      "loss: 1.169470  [ 1600/ 3200]\n",
      "loss: 1.159963  [ 1616/ 3200]\n",
      "loss: 1.240526  [ 1632/ 3200]\n",
      "loss: 1.296551  [ 1648/ 3200]\n",
      "loss: 1.187469  [ 1664/ 3200]\n",
      "loss: 1.282457  [ 1680/ 3200]\n",
      "loss: 1.289447  [ 1696/ 3200]\n",
      "loss: 1.198761  [ 1712/ 3200]\n",
      "loss: 1.261366  [ 1728/ 3200]\n",
      "loss: 1.189158  [ 1744/ 3200]\n",
      "loss: 1.176294  [ 1760/ 3200]\n",
      "loss: 1.253826  [ 1776/ 3200]\n",
      "loss: 1.218378  [ 1792/ 3200]\n",
      "loss: 1.232579  [ 1808/ 3200]\n",
      "loss: 1.204734  [ 1824/ 3200]\n",
      "loss: 1.204250  [ 1840/ 3200]\n",
      "loss: 1.194792  [ 1856/ 3200]\n",
      "loss: 1.232847  [ 1872/ 3200]\n",
      "loss: 1.132032  [ 1888/ 3200]\n",
      "loss: 1.208419  [ 1904/ 3200]\n",
      "loss: 1.128703  [ 1920/ 3200]\n",
      "loss: 1.269243  [ 1936/ 3200]\n",
      "loss: 1.173015  [ 1952/ 3200]\n",
      "loss: 1.327557  [ 1968/ 3200]\n",
      "loss: 1.292575  [ 1984/ 3200]\n",
      "loss: 1.236007  [ 2000/ 3200]\n",
      "loss: 1.246682  [ 2016/ 3200]\n",
      "loss: 1.207134  [ 2032/ 3200]\n",
      "loss: 1.226902  [ 2048/ 3200]\n",
      "loss: 1.248451  [ 2064/ 3200]\n",
      "loss: 1.166115  [ 2080/ 3200]\n",
      "loss: 1.273656  [ 2096/ 3200]\n",
      "loss: 1.201195  [ 2112/ 3200]\n",
      "loss: 1.199850  [ 2128/ 3200]\n",
      "loss: 1.287238  [ 2144/ 3200]\n",
      "loss: 1.249327  [ 2160/ 3200]\n",
      "loss: 1.131589  [ 2176/ 3200]\n",
      "loss: 1.157311  [ 2192/ 3200]\n",
      "loss: 1.234086  [ 2208/ 3200]\n",
      "loss: 1.281430  [ 2224/ 3200]\n",
      "loss: 1.200683  [ 2240/ 3200]\n",
      "loss: 1.246046  [ 2256/ 3200]\n",
      "loss: 1.217267  [ 2272/ 3200]\n",
      "loss: 1.173190  [ 2288/ 3200]\n",
      "loss: 1.270223  [ 2304/ 3200]\n",
      "loss: 1.201369  [ 2320/ 3200]\n",
      "loss: 1.263647  [ 2336/ 3200]\n",
      "loss: 1.177764  [ 2352/ 3200]\n",
      "loss: 1.280478  [ 2368/ 3200]\n",
      "loss: 1.275350  [ 2384/ 3200]\n",
      "loss: 1.170838  [ 2400/ 3200]\n",
      "loss: 1.216980  [ 2416/ 3200]\n",
      "loss: 1.241110  [ 2432/ 3200]\n",
      "loss: 1.227173  [ 2448/ 3200]\n",
      "loss: 1.242801  [ 2464/ 3200]\n",
      "loss: 1.194048  [ 2480/ 3200]\n",
      "loss: 1.150322  [ 2496/ 3200]\n",
      "loss: 1.133068  [ 2512/ 3200]\n",
      "loss: 1.197065  [ 2528/ 3200]\n",
      "loss: 1.289354  [ 2544/ 3200]\n",
      "loss: 1.268439  [ 2560/ 3200]\n",
      "loss: 1.248827  [ 2576/ 3200]\n",
      "loss: 1.246279  [ 2592/ 3200]\n",
      "loss: 1.314427  [ 2608/ 3200]\n",
      "loss: 1.195583  [ 2624/ 3200]\n",
      "loss: 1.177011  [ 2640/ 3200]\n",
      "loss: 1.215064  [ 2656/ 3200]\n",
      "loss: 1.222891  [ 2672/ 3200]\n",
      "loss: 1.236798  [ 2688/ 3200]\n",
      "loss: 1.192169  [ 2704/ 3200]\n",
      "loss: 1.194727  [ 2720/ 3200]\n",
      "loss: 1.141637  [ 2736/ 3200]\n",
      "loss: 1.149719  [ 2752/ 3200]\n",
      "loss: 1.106069  [ 2768/ 3200]\n",
      "loss: 1.236398  [ 2784/ 3200]\n",
      "loss: 1.167228  [ 2800/ 3200]\n",
      "loss: 1.270860  [ 2816/ 3200]\n",
      "loss: 1.244565  [ 2832/ 3200]\n",
      "loss: 1.259243  [ 2848/ 3200]\n",
      "loss: 1.117030  [ 2864/ 3200]\n",
      "loss: 1.343246  [ 2880/ 3200]\n",
      "loss: 1.169122  [ 2896/ 3200]\n",
      "loss: 1.127212  [ 2912/ 3200]\n",
      "loss: 1.255375  [ 2928/ 3200]\n",
      "loss: 1.165620  [ 2944/ 3200]\n",
      "loss: 1.179328  [ 2960/ 3200]\n",
      "loss: 1.141913  [ 2976/ 3200]\n",
      "loss: 1.351612  [ 2992/ 3200]\n",
      "loss: 1.223306  [ 3008/ 3200]\n",
      "loss: 1.260827  [ 3024/ 3200]\n",
      "loss: 1.137578  [ 3040/ 3200]\n",
      "loss: 1.240713  [ 3056/ 3200]\n",
      "loss: 1.230938  [ 3072/ 3200]\n",
      "loss: 1.269706  [ 3088/ 3200]\n",
      "loss: 1.187319  [ 3104/ 3200]\n",
      "loss: 1.202505  [ 3120/ 3200]\n",
      "loss: 1.148615  [ 3136/ 3200]\n",
      "loss: 1.210626  [ 3152/ 3200]\n",
      "loss: 1.303239  [ 3168/ 3200]\n",
      "loss: 1.182143  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.075621\n",
      "f1 macro averaged score: 0.459513\n",
      "Accuracy               : 54.6%\n",
      "Confusion matrix       :\n",
      "tensor([[176,   0,   4,  20],\n",
      "        [ 59,   0,  32, 109],\n",
      "        [ 13,   0,  75, 112],\n",
      "        [ 10,   0,   4, 186]])\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 1.219313  [    0/ 3200]\n",
      "loss: 1.213223  [   16/ 3200]\n",
      "loss: 1.135173  [   32/ 3200]\n",
      "loss: 1.223592  [   48/ 3200]\n",
      "loss: 1.196292  [   64/ 3200]\n",
      "loss: 1.194391  [   80/ 3200]\n",
      "loss: 1.214133  [   96/ 3200]\n",
      "loss: 1.280540  [  112/ 3200]\n",
      "loss: 1.256270  [  128/ 3200]\n",
      "loss: 1.269111  [  144/ 3200]\n",
      "loss: 1.156862  [  160/ 3200]\n",
      "loss: 1.236413  [  176/ 3200]\n",
      "loss: 1.132429  [  192/ 3200]\n",
      "loss: 1.154857  [  208/ 3200]\n",
      "loss: 1.233717  [  224/ 3200]\n",
      "loss: 1.217323  [  240/ 3200]\n",
      "loss: 1.172599  [  256/ 3200]\n",
      "loss: 1.179377  [  272/ 3200]\n",
      "loss: 1.211270  [  288/ 3200]\n",
      "loss: 1.209550  [  304/ 3200]\n",
      "loss: 1.220796  [  320/ 3200]\n",
      "loss: 1.316965  [  336/ 3200]\n",
      "loss: 1.330793  [  352/ 3200]\n",
      "loss: 1.147745  [  368/ 3200]\n",
      "loss: 1.129657  [  384/ 3200]\n",
      "loss: 1.184121  [  400/ 3200]\n",
      "loss: 1.353312  [  416/ 3200]\n",
      "loss: 1.239546  [  432/ 3200]\n",
      "loss: 1.283572  [  448/ 3200]\n",
      "loss: 1.120258  [  464/ 3200]\n",
      "loss: 1.219932  [  480/ 3200]\n",
      "loss: 1.084065  [  496/ 3200]\n",
      "loss: 1.199364  [  512/ 3200]\n",
      "loss: 1.225929  [  528/ 3200]\n",
      "loss: 1.215922  [  544/ 3200]\n",
      "loss: 1.230677  [  560/ 3200]\n",
      "loss: 1.227082  [  576/ 3200]\n",
      "loss: 1.349064  [  592/ 3200]\n",
      "loss: 1.257900  [  608/ 3200]\n",
      "loss: 1.221630  [  624/ 3200]\n",
      "loss: 1.177352  [  640/ 3200]\n",
      "loss: 1.267373  [  656/ 3200]\n",
      "loss: 1.234627  [  672/ 3200]\n",
      "loss: 1.167073  [  688/ 3200]\n",
      "loss: 1.118420  [  704/ 3200]\n",
      "loss: 1.325563  [  720/ 3200]\n",
      "loss: 1.227925  [  736/ 3200]\n",
      "loss: 1.227122  [  752/ 3200]\n",
      "loss: 1.298806  [  768/ 3200]\n",
      "loss: 1.160850  [  784/ 3200]\n",
      "loss: 1.201214  [  800/ 3200]\n",
      "loss: 1.213915  [  816/ 3200]\n",
      "loss: 1.170464  [  832/ 3200]\n",
      "loss: 1.249793  [  848/ 3200]\n",
      "loss: 1.098090  [  864/ 3200]\n",
      "loss: 1.179224  [  880/ 3200]\n",
      "loss: 1.240903  [  896/ 3200]\n",
      "loss: 1.241918  [  912/ 3200]\n",
      "loss: 1.185145  [  928/ 3200]\n",
      "loss: 1.246497  [  944/ 3200]\n",
      "loss: 1.215130  [  960/ 3200]\n",
      "loss: 1.225003  [  976/ 3200]\n",
      "loss: 1.209443  [  992/ 3200]\n",
      "loss: 1.169729  [ 1008/ 3200]\n",
      "loss: 1.209494  [ 1024/ 3200]\n",
      "loss: 1.169446  [ 1040/ 3200]\n",
      "loss: 1.105672  [ 1056/ 3200]\n",
      "loss: 1.218851  [ 1072/ 3200]\n",
      "loss: 1.300521  [ 1088/ 3200]\n",
      "loss: 1.132693  [ 1104/ 3200]\n",
      "loss: 1.224192  [ 1120/ 3200]\n",
      "loss: 1.367305  [ 1136/ 3200]\n",
      "loss: 1.302043  [ 1152/ 3200]\n",
      "loss: 1.173931  [ 1168/ 3200]\n",
      "loss: 1.197905  [ 1184/ 3200]\n",
      "loss: 1.195772  [ 1200/ 3200]\n",
      "loss: 1.254338  [ 1216/ 3200]\n",
      "loss: 1.243277  [ 1232/ 3200]\n",
      "loss: 1.236455  [ 1248/ 3200]\n",
      "loss: 1.242992  [ 1264/ 3200]\n",
      "loss: 1.259407  [ 1280/ 3200]\n",
      "loss: 1.150266  [ 1296/ 3200]\n",
      "loss: 1.075259  [ 1312/ 3200]\n",
      "loss: 1.372188  [ 1328/ 3200]\n",
      "loss: 1.276128  [ 1344/ 3200]\n",
      "loss: 1.151757  [ 1360/ 3200]\n",
      "loss: 1.310474  [ 1376/ 3200]\n",
      "loss: 1.122674  [ 1392/ 3200]\n",
      "loss: 1.099565  [ 1408/ 3200]\n",
      "loss: 1.113071  [ 1424/ 3200]\n",
      "loss: 1.270557  [ 1440/ 3200]\n",
      "loss: 1.122257  [ 1456/ 3200]\n",
      "loss: 1.160329  [ 1472/ 3200]\n",
      "loss: 1.099695  [ 1488/ 3200]\n",
      "loss: 1.207555  [ 1504/ 3200]\n",
      "loss: 1.288069  [ 1520/ 3200]\n",
      "loss: 1.167071  [ 1536/ 3200]\n",
      "loss: 1.256256  [ 1552/ 3200]\n",
      "loss: 1.119919  [ 1568/ 3200]\n",
      "loss: 1.140948  [ 1584/ 3200]\n",
      "loss: 1.154561  [ 1600/ 3200]\n",
      "loss: 1.133806  [ 1616/ 3200]\n",
      "loss: 1.224162  [ 1632/ 3200]\n",
      "loss: 1.263209  [ 1648/ 3200]\n",
      "loss: 1.130257  [ 1664/ 3200]\n",
      "loss: 1.232773  [ 1680/ 3200]\n",
      "loss: 1.282359  [ 1696/ 3200]\n",
      "loss: 1.095060  [ 1712/ 3200]\n",
      "loss: 1.272815  [ 1728/ 3200]\n",
      "loss: 1.186970  [ 1744/ 3200]\n",
      "loss: 1.199429  [ 1760/ 3200]\n",
      "loss: 1.164335  [ 1776/ 3200]\n",
      "loss: 1.304309  [ 1792/ 3200]\n",
      "loss: 1.133896  [ 1808/ 3200]\n",
      "loss: 1.219917  [ 1824/ 3200]\n",
      "loss: 1.165331  [ 1840/ 3200]\n",
      "loss: 1.236950  [ 1856/ 3200]\n",
      "loss: 1.165430  [ 1872/ 3200]\n",
      "loss: 1.227543  [ 1888/ 3200]\n",
      "loss: 1.190302  [ 1904/ 3200]\n",
      "loss: 1.309592  [ 1920/ 3200]\n",
      "loss: 1.065889  [ 1936/ 3200]\n",
      "loss: 1.106984  [ 1952/ 3200]\n",
      "loss: 1.155652  [ 1968/ 3200]\n",
      "loss: 1.203235  [ 1984/ 3200]\n",
      "loss: 1.145530  [ 2000/ 3200]\n",
      "loss: 1.249593  [ 2016/ 3200]\n",
      "loss: 1.174613  [ 2032/ 3200]\n",
      "loss: 1.092071  [ 2048/ 3200]\n",
      "loss: 1.232823  [ 2064/ 3200]\n",
      "loss: 1.177388  [ 2080/ 3200]\n",
      "loss: 1.156577  [ 2096/ 3200]\n",
      "loss: 1.258039  [ 2112/ 3200]\n",
      "loss: 1.168083  [ 2128/ 3200]\n",
      "loss: 1.264824  [ 2144/ 3200]\n",
      "loss: 1.193019  [ 2160/ 3200]\n",
      "loss: 1.208416  [ 2176/ 3200]\n",
      "loss: 1.190143  [ 2192/ 3200]\n",
      "loss: 1.275309  [ 2208/ 3200]\n",
      "loss: 1.224510  [ 2224/ 3200]\n",
      "loss: 1.103814  [ 2240/ 3200]\n",
      "loss: 1.254506  [ 2256/ 3200]\n",
      "loss: 1.131763  [ 2272/ 3200]\n",
      "loss: 1.221124  [ 2288/ 3200]\n",
      "loss: 1.085119  [ 2304/ 3200]\n",
      "loss: 1.205067  [ 2320/ 3200]\n",
      "loss: 1.203131  [ 2336/ 3200]\n",
      "loss: 1.180459  [ 2352/ 3200]\n",
      "loss: 1.184227  [ 2368/ 3200]\n",
      "loss: 1.207480  [ 2384/ 3200]\n",
      "loss: 1.109705  [ 2400/ 3200]\n",
      "loss: 1.097557  [ 2416/ 3200]\n",
      "loss: 1.139478  [ 2432/ 3200]\n",
      "loss: 1.212867  [ 2448/ 3200]\n",
      "loss: 1.221473  [ 2464/ 3200]\n",
      "loss: 1.205852  [ 2480/ 3200]\n",
      "loss: 1.234426  [ 2496/ 3200]\n",
      "loss: 1.179874  [ 2512/ 3200]\n",
      "loss: 1.298870  [ 2528/ 3200]\n",
      "loss: 1.132219  [ 2544/ 3200]\n",
      "loss: 1.149200  [ 2560/ 3200]\n",
      "loss: 1.241621  [ 2576/ 3200]\n",
      "loss: 1.336234  [ 2592/ 3200]\n",
      "loss: 1.317653  [ 2608/ 3200]\n",
      "loss: 1.225720  [ 2624/ 3200]\n",
      "loss: 1.122335  [ 2640/ 3200]\n",
      "loss: 1.152373  [ 2656/ 3200]\n",
      "loss: 1.131723  [ 2672/ 3200]\n",
      "loss: 1.219791  [ 2688/ 3200]\n",
      "loss: 1.137784  [ 2704/ 3200]\n",
      "loss: 1.251813  [ 2720/ 3200]\n",
      "loss: 1.313659  [ 2736/ 3200]\n",
      "loss: 1.240538  [ 2752/ 3200]\n",
      "loss: 1.232966  [ 2768/ 3200]\n",
      "loss: 1.210056  [ 2784/ 3200]\n",
      "loss: 1.289270  [ 2800/ 3200]\n",
      "loss: 1.201823  [ 2816/ 3200]\n",
      "loss: 1.237812  [ 2832/ 3200]\n",
      "loss: 1.087531  [ 2848/ 3200]\n",
      "loss: 1.190380  [ 2864/ 3200]\n",
      "loss: 1.192286  [ 2880/ 3200]\n",
      "loss: 1.147370  [ 2896/ 3200]\n",
      "loss: 1.128586  [ 2912/ 3200]\n",
      "loss: 1.250898  [ 2928/ 3200]\n",
      "loss: 1.214429  [ 2944/ 3200]\n",
      "loss: 1.173021  [ 2960/ 3200]\n",
      "loss: 1.206077  [ 2976/ 3200]\n",
      "loss: 1.224906  [ 2992/ 3200]\n",
      "loss: 0.991031  [ 3008/ 3200]\n",
      "loss: 1.189593  [ 3024/ 3200]\n",
      "loss: 1.137572  [ 3040/ 3200]\n",
      "loss: 1.077490  [ 3056/ 3200]\n",
      "loss: 1.242117  [ 3072/ 3200]\n",
      "loss: 1.265069  [ 3088/ 3200]\n",
      "loss: 1.325248  [ 3104/ 3200]\n",
      "loss: 1.185976  [ 3120/ 3200]\n",
      "loss: 1.232344  [ 3136/ 3200]\n",
      "loss: 1.162932  [ 3152/ 3200]\n",
      "loss: 1.177492  [ 3168/ 3200]\n",
      "loss: 1.240495  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.074467\n",
      "f1 macro averaged score: 0.606037\n",
      "Accuracy               : 59.6%\n",
      "Confusion matrix       :\n",
      "tensor([[108,  83,   6,   3],\n",
      "        [ 18,  85,  80,  17],\n",
      "        [  1,  37, 154,   8],\n",
      "        [  0,  40,  30, 130]])\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 1.161397  [    0/ 3200]\n",
      "loss: 1.166771  [   16/ 3200]\n",
      "loss: 1.279668  [   32/ 3200]\n",
      "loss: 1.138510  [   48/ 3200]\n",
      "loss: 1.221299  [   64/ 3200]\n",
      "loss: 1.070273  [   80/ 3200]\n",
      "loss: 1.268811  [   96/ 3200]\n",
      "loss: 1.251554  [  112/ 3200]\n",
      "loss: 1.181878  [  128/ 3200]\n",
      "loss: 1.191811  [  144/ 3200]\n",
      "loss: 1.106690  [  160/ 3200]\n",
      "loss: 1.237278  [  176/ 3200]\n",
      "loss: 1.052643  [  192/ 3200]\n",
      "loss: 1.174420  [  208/ 3200]\n",
      "loss: 1.279220  [  224/ 3200]\n",
      "loss: 1.218262  [  240/ 3200]\n",
      "loss: 1.157392  [  256/ 3200]\n",
      "loss: 1.180601  [  272/ 3200]\n",
      "loss: 1.182516  [  288/ 3200]\n",
      "loss: 1.136955  [  304/ 3200]\n",
      "loss: 1.217031  [  320/ 3200]\n",
      "loss: 1.167540  [  336/ 3200]\n",
      "loss: 1.124795  [  352/ 3200]\n",
      "loss: 1.081820  [  368/ 3200]\n",
      "loss: 1.172552  [  384/ 3200]\n",
      "loss: 1.226641  [  400/ 3200]\n",
      "loss: 1.141251  [  416/ 3200]\n",
      "loss: 1.117306  [  432/ 3200]\n",
      "loss: 1.225517  [  448/ 3200]\n",
      "loss: 1.109077  [  464/ 3200]\n",
      "loss: 1.101937  [  480/ 3200]\n",
      "loss: 1.201156  [  496/ 3200]\n",
      "loss: 1.284582  [  512/ 3200]\n",
      "loss: 1.187862  [  528/ 3200]\n",
      "loss: 1.141856  [  544/ 3200]\n",
      "loss: 1.193093  [  560/ 3200]\n",
      "loss: 1.160506  [  576/ 3200]\n",
      "loss: 1.259558  [  592/ 3200]\n",
      "loss: 1.226259  [  608/ 3200]\n",
      "loss: 1.221264  [  624/ 3200]\n",
      "loss: 1.140480  [  640/ 3200]\n",
      "loss: 1.137068  [  656/ 3200]\n",
      "loss: 1.147993  [  672/ 3200]\n",
      "loss: 1.208463  [  688/ 3200]\n",
      "loss: 1.184310  [  704/ 3200]\n",
      "loss: 1.289255  [  720/ 3200]\n",
      "loss: 1.108037  [  736/ 3200]\n",
      "loss: 1.276936  [  752/ 3200]\n",
      "loss: 1.004675  [  768/ 3200]\n",
      "loss: 1.166390  [  784/ 3200]\n",
      "loss: 1.124503  [  800/ 3200]\n",
      "loss: 1.102052  [  816/ 3200]\n",
      "loss: 1.113153  [  832/ 3200]\n",
      "loss: 1.188907  [  848/ 3200]\n",
      "loss: 1.182490  [  864/ 3200]\n",
      "loss: 1.279402  [  880/ 3200]\n",
      "loss: 1.096709  [  896/ 3200]\n",
      "loss: 1.171829  [  912/ 3200]\n",
      "loss: 1.196002  [  928/ 3200]\n",
      "loss: 1.291276  [  944/ 3200]\n",
      "loss: 1.241464  [  960/ 3200]\n",
      "loss: 1.262364  [  976/ 3200]\n",
      "loss: 1.208832  [  992/ 3200]\n",
      "loss: 1.132762  [ 1008/ 3200]\n",
      "loss: 1.153684  [ 1024/ 3200]\n",
      "loss: 1.290392  [ 1040/ 3200]\n",
      "loss: 1.079173  [ 1056/ 3200]\n",
      "loss: 1.218513  [ 1072/ 3200]\n",
      "loss: 1.174489  [ 1088/ 3200]\n",
      "loss: 1.138786  [ 1104/ 3200]\n",
      "loss: 1.097773  [ 1120/ 3200]\n",
      "loss: 1.142405  [ 1136/ 3200]\n",
      "loss: 1.278636  [ 1152/ 3200]\n",
      "loss: 1.270396  [ 1168/ 3200]\n",
      "loss: 1.308421  [ 1184/ 3200]\n",
      "loss: 1.215949  [ 1200/ 3200]\n",
      "loss: 1.196553  [ 1216/ 3200]\n",
      "loss: 1.142707  [ 1232/ 3200]\n",
      "loss: 1.147167  [ 1248/ 3200]\n",
      "loss: 1.311419  [ 1264/ 3200]\n",
      "loss: 1.265705  [ 1280/ 3200]\n",
      "loss: 0.966817  [ 1296/ 3200]\n",
      "loss: 1.084678  [ 1312/ 3200]\n",
      "loss: 1.276762  [ 1328/ 3200]\n",
      "loss: 1.127431  [ 1344/ 3200]\n",
      "loss: 1.137313  [ 1360/ 3200]\n",
      "loss: 1.063785  [ 1376/ 3200]\n",
      "loss: 1.130700  [ 1392/ 3200]\n",
      "loss: 1.289143  [ 1408/ 3200]\n",
      "loss: 1.163494  [ 1424/ 3200]\n",
      "loss: 1.223837  [ 1440/ 3200]\n",
      "loss: 1.251416  [ 1456/ 3200]\n",
      "loss: 1.132815  [ 1472/ 3200]\n",
      "loss: 1.226095  [ 1488/ 3200]\n",
      "loss: 1.224383  [ 1504/ 3200]\n",
      "loss: 1.106607  [ 1520/ 3200]\n",
      "loss: 1.228670  [ 1536/ 3200]\n",
      "loss: 1.258844  [ 1552/ 3200]\n",
      "loss: 1.189785  [ 1568/ 3200]\n",
      "loss: 1.135362  [ 1584/ 3200]\n",
      "loss: 1.180950  [ 1600/ 3200]\n",
      "loss: 1.171797  [ 1616/ 3200]\n",
      "loss: 1.163296  [ 1632/ 3200]\n",
      "loss: 1.154636  [ 1648/ 3200]\n",
      "loss: 1.186201  [ 1664/ 3200]\n",
      "loss: 1.286233  [ 1680/ 3200]\n",
      "loss: 1.218592  [ 1696/ 3200]\n",
      "loss: 1.016983  [ 1712/ 3200]\n",
      "loss: 1.195492  [ 1728/ 3200]\n",
      "loss: 1.166820  [ 1744/ 3200]\n",
      "loss: 1.238773  [ 1760/ 3200]\n",
      "loss: 1.074298  [ 1776/ 3200]\n",
      "loss: 1.130703  [ 1792/ 3200]\n",
      "loss: 1.211276  [ 1808/ 3200]\n",
      "loss: 1.180075  [ 1824/ 3200]\n",
      "loss: 1.197933  [ 1840/ 3200]\n",
      "loss: 1.244672  [ 1856/ 3200]\n",
      "loss: 1.223011  [ 1872/ 3200]\n",
      "loss: 1.239083  [ 1888/ 3200]\n",
      "loss: 1.202218  [ 1904/ 3200]\n",
      "loss: 1.233812  [ 1920/ 3200]\n",
      "loss: 1.302281  [ 1936/ 3200]\n",
      "loss: 1.179892  [ 1952/ 3200]\n",
      "loss: 1.137476  [ 1968/ 3200]\n",
      "loss: 1.207393  [ 1984/ 3200]\n",
      "loss: 1.194068  [ 2000/ 3200]\n",
      "loss: 1.106260  [ 2016/ 3200]\n",
      "loss: 1.254049  [ 2032/ 3200]\n",
      "loss: 1.193885  [ 2048/ 3200]\n",
      "loss: 1.298743  [ 2064/ 3200]\n",
      "loss: 1.214535  [ 2080/ 3200]\n",
      "loss: 1.225342  [ 2096/ 3200]\n",
      "loss: 1.142622  [ 2112/ 3200]\n",
      "loss: 1.213269  [ 2128/ 3200]\n",
      "loss: 1.307305  [ 2144/ 3200]\n",
      "loss: 1.105211  [ 2160/ 3200]\n",
      "loss: 1.155149  [ 2176/ 3200]\n",
      "loss: 1.093553  [ 2192/ 3200]\n",
      "loss: 1.170235  [ 2208/ 3200]\n",
      "loss: 1.145183  [ 2224/ 3200]\n",
      "loss: 1.136098  [ 2240/ 3200]\n",
      "loss: 1.168427  [ 2256/ 3200]\n",
      "loss: 1.161710  [ 2272/ 3200]\n",
      "loss: 1.241587  [ 2288/ 3200]\n",
      "loss: 1.227153  [ 2304/ 3200]\n",
      "loss: 1.129628  [ 2320/ 3200]\n",
      "loss: 1.113920  [ 2336/ 3200]\n",
      "loss: 1.151834  [ 2352/ 3200]\n",
      "loss: 1.187825  [ 2368/ 3200]\n",
      "loss: 1.245021  [ 2384/ 3200]\n",
      "loss: 1.152954  [ 2400/ 3200]\n",
      "loss: 1.123972  [ 2416/ 3200]\n",
      "loss: 1.193659  [ 2432/ 3200]\n",
      "loss: 1.261299  [ 2448/ 3200]\n",
      "loss: 1.207114  [ 2464/ 3200]\n",
      "loss: 1.231321  [ 2480/ 3200]\n",
      "loss: 1.095137  [ 2496/ 3200]\n",
      "loss: 1.143928  [ 2512/ 3200]\n",
      "loss: 0.997580  [ 2528/ 3200]\n",
      "loss: 1.312022  [ 2544/ 3200]\n",
      "loss: 1.249757  [ 2560/ 3200]\n",
      "loss: 1.140885  [ 2576/ 3200]\n",
      "loss: 1.261192  [ 2592/ 3200]\n",
      "loss: 1.182160  [ 2608/ 3200]\n",
      "loss: 1.131265  [ 2624/ 3200]\n",
      "loss: 1.099755  [ 2640/ 3200]\n",
      "loss: 1.161849  [ 2656/ 3200]\n",
      "loss: 1.169660  [ 2672/ 3200]\n",
      "loss: 1.240277  [ 2688/ 3200]\n",
      "loss: 1.150479  [ 2704/ 3200]\n",
      "loss: 1.215045  [ 2720/ 3200]\n",
      "loss: 1.086637  [ 2736/ 3200]\n",
      "loss: 1.069407  [ 2752/ 3200]\n",
      "loss: 1.180043  [ 2768/ 3200]\n",
      "loss: 1.151753  [ 2784/ 3200]\n",
      "loss: 1.093398  [ 2800/ 3200]\n",
      "loss: 1.320591  [ 2816/ 3200]\n",
      "loss: 1.147899  [ 2832/ 3200]\n",
      "loss: 1.172254  [ 2848/ 3200]\n",
      "loss: 1.069643  [ 2864/ 3200]\n",
      "loss: 1.186171  [ 2880/ 3200]\n",
      "loss: 1.142007  [ 2896/ 3200]\n",
      "loss: 1.279860  [ 2912/ 3200]\n",
      "loss: 1.197847  [ 2928/ 3200]\n",
      "loss: 1.207347  [ 2944/ 3200]\n",
      "loss: 1.271994  [ 2960/ 3200]\n",
      "loss: 1.203636  [ 2976/ 3200]\n",
      "loss: 1.124744  [ 2992/ 3200]\n",
      "loss: 1.274621  [ 3008/ 3200]\n",
      "loss: 1.185346  [ 3024/ 3200]\n",
      "loss: 1.071061  [ 3040/ 3200]\n",
      "loss: 1.189618  [ 3056/ 3200]\n",
      "loss: 1.264796  [ 3072/ 3200]\n",
      "loss: 1.191010  [ 3088/ 3200]\n",
      "loss: 1.192073  [ 3104/ 3200]\n",
      "loss: 1.196689  [ 3120/ 3200]\n",
      "loss: 1.203227  [ 3136/ 3200]\n",
      "loss: 1.082763  [ 3152/ 3200]\n",
      "loss: 1.240627  [ 3168/ 3200]\n",
      "loss: 1.138034  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.072523\n",
      "f1 macro averaged score: 0.502247\n",
      "Accuracy               : 57.5%\n",
      "Confusion matrix       :\n",
      "tensor([[184,   0,   4,  12],\n",
      "        [ 73,  11,  34,  82],\n",
      "        [ 23,   8,  86,  83],\n",
      "        [ 15,   0,   6, 179]])\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 1.132464  [    0/ 3200]\n",
      "loss: 1.159077  [   16/ 3200]\n",
      "loss: 1.226227  [   32/ 3200]\n",
      "loss: 1.166470  [   48/ 3200]\n",
      "loss: 1.225476  [   64/ 3200]\n",
      "loss: 1.139840  [   80/ 3200]\n",
      "loss: 1.139235  [   96/ 3200]\n",
      "loss: 1.229142  [  112/ 3200]\n",
      "loss: 1.225219  [  128/ 3200]\n",
      "loss: 1.184154  [  144/ 3200]\n",
      "loss: 1.192481  [  160/ 3200]\n",
      "loss: 1.166053  [  176/ 3200]\n",
      "loss: 1.285100  [  192/ 3200]\n",
      "loss: 1.118575  [  208/ 3200]\n",
      "loss: 1.138340  [  224/ 3200]\n",
      "loss: 1.156994  [  240/ 3200]\n",
      "loss: 1.275422  [  256/ 3200]\n",
      "loss: 1.149816  [  272/ 3200]\n",
      "loss: 1.086291  [  288/ 3200]\n",
      "loss: 1.152046  [  304/ 3200]\n",
      "loss: 1.177774  [  320/ 3200]\n",
      "loss: 1.483626  [  336/ 3200]\n",
      "loss: 1.040587  [  352/ 3200]\n",
      "loss: 0.932232  [  368/ 3200]\n",
      "loss: 1.220307  [  384/ 3200]\n",
      "loss: 0.932825  [  400/ 3200]\n",
      "loss: 1.173889  [  416/ 3200]\n",
      "loss: 1.206580  [  432/ 3200]\n",
      "loss: 1.158766  [  448/ 3200]\n",
      "loss: 1.196886  [  464/ 3200]\n",
      "loss: 1.175984  [  480/ 3200]\n",
      "loss: 1.162894  [  496/ 3200]\n",
      "loss: 1.049283  [  512/ 3200]\n",
      "loss: 1.237623  [  528/ 3200]\n",
      "loss: 1.186529  [  544/ 3200]\n",
      "loss: 1.229265  [  560/ 3200]\n",
      "loss: 1.171376  [  576/ 3200]\n",
      "loss: 1.108777  [  592/ 3200]\n",
      "loss: 1.308781  [  608/ 3200]\n",
      "loss: 1.115865  [  624/ 3200]\n",
      "loss: 1.230887  [  640/ 3200]\n",
      "loss: 1.192836  [  656/ 3200]\n",
      "loss: 1.156533  [  672/ 3200]\n",
      "loss: 1.164819  [  688/ 3200]\n",
      "loss: 1.221649  [  704/ 3200]\n",
      "loss: 1.360606  [  720/ 3200]\n",
      "loss: 1.228247  [  736/ 3200]\n",
      "loss: 1.218763  [  752/ 3200]\n",
      "loss: 1.281738  [  768/ 3200]\n",
      "loss: 1.207420  [  784/ 3200]\n",
      "loss: 1.066590  [  800/ 3200]\n",
      "loss: 1.194872  [  816/ 3200]\n",
      "loss: 1.172957  [  832/ 3200]\n",
      "loss: 1.180850  [  848/ 3200]\n",
      "loss: 1.163422  [  864/ 3200]\n",
      "loss: 1.090860  [  880/ 3200]\n",
      "loss: 1.075335  [  896/ 3200]\n",
      "loss: 1.100238  [  912/ 3200]\n",
      "loss: 1.117460  [  928/ 3200]\n",
      "loss: 1.159541  [  944/ 3200]\n",
      "loss: 1.273997  [  960/ 3200]\n",
      "loss: 1.197407  [  976/ 3200]\n",
      "loss: 1.032891  [  992/ 3200]\n",
      "loss: 1.121546  [ 1008/ 3200]\n",
      "loss: 1.206099  [ 1024/ 3200]\n",
      "loss: 1.162733  [ 1040/ 3200]\n",
      "loss: 1.286253  [ 1056/ 3200]\n",
      "loss: 1.002734  [ 1072/ 3200]\n",
      "loss: 1.204369  [ 1088/ 3200]\n",
      "loss: 1.272734  [ 1104/ 3200]\n",
      "loss: 1.153766  [ 1120/ 3200]\n",
      "loss: 1.056937  [ 1136/ 3200]\n",
      "loss: 1.175842  [ 1152/ 3200]\n",
      "loss: 1.209117  [ 1168/ 3200]\n",
      "loss: 1.179551  [ 1184/ 3200]\n",
      "loss: 1.199990  [ 1200/ 3200]\n",
      "loss: 1.262277  [ 1216/ 3200]\n",
      "loss: 1.182895  [ 1232/ 3200]\n",
      "loss: 1.192313  [ 1248/ 3200]\n",
      "loss: 1.153429  [ 1264/ 3200]\n",
      "loss: 1.360783  [ 1280/ 3200]\n",
      "loss: 1.217591  [ 1296/ 3200]\n",
      "loss: 1.170677  [ 1312/ 3200]\n",
      "loss: 1.074211  [ 1328/ 3200]\n",
      "loss: 0.919555  [ 1344/ 3200]\n",
      "loss: 1.189152  [ 1360/ 3200]\n",
      "loss: 1.006930  [ 1376/ 3200]\n",
      "loss: 1.264268  [ 1392/ 3200]\n",
      "loss: 1.230762  [ 1408/ 3200]\n",
      "loss: 0.926575  [ 1424/ 3200]\n",
      "loss: 1.266300  [ 1440/ 3200]\n",
      "loss: 1.085850  [ 1456/ 3200]\n",
      "loss: 1.206808  [ 1472/ 3200]\n",
      "loss: 1.220582  [ 1488/ 3200]\n",
      "loss: 1.165165  [ 1504/ 3200]\n",
      "loss: 1.249853  [ 1520/ 3200]\n",
      "loss: 1.189411  [ 1536/ 3200]\n",
      "loss: 1.223082  [ 1552/ 3200]\n",
      "loss: 1.163815  [ 1568/ 3200]\n",
      "loss: 1.074536  [ 1584/ 3200]\n",
      "loss: 1.097734  [ 1600/ 3200]\n",
      "loss: 1.112611  [ 1616/ 3200]\n",
      "loss: 1.167844  [ 1632/ 3200]\n",
      "loss: 1.172915  [ 1648/ 3200]\n",
      "loss: 1.080171  [ 1664/ 3200]\n",
      "loss: 1.057716  [ 1680/ 3200]\n",
      "loss: 1.166307  [ 1696/ 3200]\n",
      "loss: 1.154900  [ 1712/ 3200]\n",
      "loss: 1.088887  [ 1728/ 3200]\n",
      "loss: 1.159693  [ 1744/ 3200]\n",
      "loss: 1.109065  [ 1760/ 3200]\n",
      "loss: 1.105738  [ 1776/ 3200]\n",
      "loss: 1.157104  [ 1792/ 3200]\n",
      "loss: 1.228666  [ 1808/ 3200]\n",
      "loss: 0.973799  [ 1824/ 3200]\n",
      "loss: 1.244595  [ 1840/ 3200]\n",
      "loss: 1.119850  [ 1856/ 3200]\n",
      "loss: 1.234479  [ 1872/ 3200]\n",
      "loss: 1.186050  [ 1888/ 3200]\n",
      "loss: 0.934825  [ 1904/ 3200]\n",
      "loss: 1.205876  [ 1920/ 3200]\n",
      "loss: 1.111696  [ 1936/ 3200]\n",
      "loss: 1.199555  [ 1952/ 3200]\n",
      "loss: 1.119844  [ 1968/ 3200]\n",
      "loss: 1.209774  [ 1984/ 3200]\n",
      "loss: 1.205874  [ 2000/ 3200]\n",
      "loss: 0.934415  [ 2016/ 3200]\n",
      "loss: 1.052923  [ 2032/ 3200]\n",
      "loss: 1.143170  [ 2048/ 3200]\n",
      "loss: 1.162418  [ 2064/ 3200]\n",
      "loss: 1.195989  [ 2080/ 3200]\n",
      "loss: 1.144474  [ 2096/ 3200]\n",
      "loss: 1.266660  [ 2112/ 3200]\n",
      "loss: 1.181549  [ 2128/ 3200]\n",
      "loss: 1.003062  [ 2144/ 3200]\n",
      "loss: 1.031773  [ 2160/ 3200]\n",
      "loss: 1.178867  [ 2176/ 3200]\n",
      "loss: 1.127552  [ 2192/ 3200]\n",
      "loss: 1.159120  [ 2208/ 3200]\n",
      "loss: 1.110880  [ 2224/ 3200]\n",
      "loss: 1.224374  [ 2240/ 3200]\n",
      "loss: 1.078655  [ 2256/ 3200]\n",
      "loss: 1.276212  [ 2272/ 3200]\n",
      "loss: 1.172025  [ 2288/ 3200]\n",
      "loss: 1.245206  [ 2304/ 3200]\n",
      "loss: 1.058309  [ 2320/ 3200]\n",
      "loss: 1.048548  [ 2336/ 3200]\n",
      "loss: 1.108541  [ 2352/ 3200]\n",
      "loss: 1.209231  [ 2368/ 3200]\n",
      "loss: 1.024219  [ 2384/ 3200]\n",
      "loss: 1.154923  [ 2400/ 3200]\n",
      "loss: 1.306473  [ 2416/ 3200]\n",
      "loss: 1.029117  [ 2432/ 3200]\n",
      "loss: 1.220550  [ 2448/ 3200]\n",
      "loss: 1.181037  [ 2464/ 3200]\n",
      "loss: 1.252336  [ 2480/ 3200]\n",
      "loss: 1.162542  [ 2496/ 3200]\n",
      "loss: 1.208329  [ 2512/ 3200]\n",
      "loss: 1.228512  [ 2528/ 3200]\n",
      "loss: 1.200134  [ 2544/ 3200]\n",
      "loss: 1.164770  [ 2560/ 3200]\n",
      "loss: 1.251868  [ 2576/ 3200]\n",
      "loss: 1.241332  [ 2592/ 3200]\n",
      "loss: 1.257464  [ 2608/ 3200]\n",
      "loss: 1.131290  [ 2624/ 3200]\n",
      "loss: 1.186227  [ 2640/ 3200]\n",
      "loss: 1.278053  [ 2656/ 3200]\n",
      "loss: 1.020249  [ 2672/ 3200]\n",
      "loss: 1.181675  [ 2688/ 3200]\n",
      "loss: 1.181814  [ 2704/ 3200]\n",
      "loss: 1.270913  [ 2720/ 3200]\n",
      "loss: 1.011462  [ 2736/ 3200]\n",
      "loss: 1.155395  [ 2752/ 3200]\n",
      "loss: 1.246628  [ 2768/ 3200]\n",
      "loss: 1.162783  [ 2784/ 3200]\n",
      "loss: 1.199851  [ 2800/ 3200]\n",
      "loss: 1.025978  [ 2816/ 3200]\n",
      "loss: 1.113730  [ 2832/ 3200]\n",
      "loss: 1.252098  [ 2848/ 3200]\n",
      "loss: 1.202852  [ 2864/ 3200]\n",
      "loss: 1.133731  [ 2880/ 3200]\n",
      "loss: 1.153738  [ 2896/ 3200]\n",
      "loss: 1.160827  [ 2912/ 3200]\n",
      "loss: 1.200927  [ 2928/ 3200]\n",
      "loss: 1.053818  [ 2944/ 3200]\n",
      "loss: 1.031756  [ 2960/ 3200]\n",
      "loss: 1.151301  [ 2976/ 3200]\n",
      "loss: 1.104207  [ 2992/ 3200]\n",
      "loss: 1.075370  [ 3008/ 3200]\n",
      "loss: 1.264258  [ 3024/ 3200]\n",
      "loss: 1.096373  [ 3040/ 3200]\n",
      "loss: 1.120512  [ 3056/ 3200]\n",
      "loss: 1.052875  [ 3072/ 3200]\n",
      "loss: 1.032932  [ 3088/ 3200]\n",
      "loss: 1.348493  [ 3104/ 3200]\n",
      "loss: 1.207125  [ 3120/ 3200]\n",
      "loss: 1.001347  [ 3136/ 3200]\n",
      "loss: 1.223812  [ 3152/ 3200]\n",
      "loss: 1.187672  [ 3168/ 3200]\n",
      "loss: 1.158725  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.071107\n",
      "f1 macro averaged score: 0.574435\n",
      "Accuracy               : 58.8%\n",
      "Confusion matrix       :\n",
      "tensor([[181,  15,   4,   0],\n",
      "        [ 59,  60,  76,   5],\n",
      "        [ 12,  39, 147,   2],\n",
      "        [ 11,  74,  33,  82]])\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 0.969398  [    0/ 3200]\n",
      "loss: 1.216475  [   16/ 3200]\n",
      "loss: 1.190687  [   32/ 3200]\n",
      "loss: 1.075728  [   48/ 3200]\n",
      "loss: 1.172548  [   64/ 3200]\n",
      "loss: 1.175955  [   80/ 3200]\n",
      "loss: 1.064169  [   96/ 3200]\n",
      "loss: 1.179595  [  112/ 3200]\n",
      "loss: 1.185784  [  128/ 3200]\n",
      "loss: 1.204887  [  144/ 3200]\n",
      "loss: 1.101107  [  160/ 3200]\n",
      "loss: 1.101751  [  176/ 3200]\n",
      "loss: 1.089007  [  192/ 3200]\n",
      "loss: 1.105972  [  208/ 3200]\n",
      "loss: 1.049489  [  224/ 3200]\n",
      "loss: 1.289007  [  240/ 3200]\n",
      "loss: 1.165060  [  256/ 3200]\n",
      "loss: 1.091878  [  272/ 3200]\n",
      "loss: 1.134205  [  288/ 3200]\n",
      "loss: 0.928252  [  304/ 3200]\n",
      "loss: 1.210860  [  320/ 3200]\n",
      "loss: 1.065528  [  336/ 3200]\n",
      "loss: 1.264563  [  352/ 3200]\n",
      "loss: 1.145461  [  368/ 3200]\n",
      "loss: 1.172887  [  384/ 3200]\n",
      "loss: 1.289387  [  400/ 3200]\n",
      "loss: 1.181980  [  416/ 3200]\n",
      "loss: 1.088716  [  432/ 3200]\n",
      "loss: 1.222571  [  448/ 3200]\n",
      "loss: 1.070813  [  464/ 3200]\n",
      "loss: 1.108559  [  480/ 3200]\n",
      "loss: 1.200494  [  496/ 3200]\n",
      "loss: 1.238487  [  512/ 3200]\n",
      "loss: 1.108918  [  528/ 3200]\n",
      "loss: 1.149411  [  544/ 3200]\n",
      "loss: 1.266647  [  560/ 3200]\n",
      "loss: 1.179524  [  576/ 3200]\n",
      "loss: 1.156404  [  592/ 3200]\n",
      "loss: 1.194134  [  608/ 3200]\n",
      "loss: 1.098328  [  624/ 3200]\n",
      "loss: 1.031860  [  640/ 3200]\n",
      "loss: 0.904382  [  656/ 3200]\n",
      "loss: 1.234551  [  672/ 3200]\n",
      "loss: 1.003443  [  688/ 3200]\n",
      "loss: 1.208539  [  704/ 3200]\n",
      "loss: 1.097150  [  720/ 3200]\n",
      "loss: 1.071809  [  736/ 3200]\n",
      "loss: 1.206921  [  752/ 3200]\n",
      "loss: 1.099342  [  768/ 3200]\n",
      "loss: 1.137688  [  784/ 3200]\n",
      "loss: 1.139707  [  800/ 3200]\n",
      "loss: 1.179994  [  816/ 3200]\n",
      "loss: 1.082315  [  832/ 3200]\n",
      "loss: 1.110433  [  848/ 3200]\n",
      "loss: 1.158235  [  864/ 3200]\n",
      "loss: 1.158105  [  880/ 3200]\n",
      "loss: 1.238246  [  896/ 3200]\n",
      "loss: 1.063534  [  912/ 3200]\n",
      "loss: 1.239060  [  928/ 3200]\n",
      "loss: 1.184050  [  944/ 3200]\n",
      "loss: 0.946272  [  960/ 3200]\n",
      "loss: 1.015378  [  976/ 3200]\n",
      "loss: 1.062390  [  992/ 3200]\n",
      "loss: 1.153612  [ 1008/ 3200]\n",
      "loss: 1.236948  [ 1024/ 3200]\n",
      "loss: 1.137331  [ 1040/ 3200]\n",
      "loss: 1.254345  [ 1056/ 3200]\n",
      "loss: 1.231622  [ 1072/ 3200]\n",
      "loss: 1.213097  [ 1088/ 3200]\n",
      "loss: 1.181228  [ 1104/ 3200]\n",
      "loss: 1.130253  [ 1120/ 3200]\n",
      "loss: 1.224565  [ 1136/ 3200]\n",
      "loss: 1.134664  [ 1152/ 3200]\n",
      "loss: 1.177981  [ 1168/ 3200]\n",
      "loss: 1.115690  [ 1184/ 3200]\n",
      "loss: 1.043076  [ 1200/ 3200]\n",
      "loss: 1.090009  [ 1216/ 3200]\n",
      "loss: 1.212732  [ 1232/ 3200]\n",
      "loss: 1.237463  [ 1248/ 3200]\n",
      "loss: 1.099704  [ 1264/ 3200]\n",
      "loss: 1.100715  [ 1280/ 3200]\n",
      "loss: 1.080155  [ 1296/ 3200]\n",
      "loss: 1.137630  [ 1312/ 3200]\n",
      "loss: 1.182893  [ 1328/ 3200]\n",
      "loss: 1.113302  [ 1344/ 3200]\n",
      "loss: 1.073594  [ 1360/ 3200]\n",
      "loss: 1.070035  [ 1376/ 3200]\n",
      "loss: 1.205015  [ 1392/ 3200]\n",
      "loss: 1.208757  [ 1408/ 3200]\n",
      "loss: 1.144610  [ 1424/ 3200]\n",
      "loss: 1.044858  [ 1440/ 3200]\n",
      "loss: 0.987213  [ 1456/ 3200]\n",
      "loss: 1.215763  [ 1472/ 3200]\n",
      "loss: 0.918080  [ 1488/ 3200]\n",
      "loss: 1.136944  [ 1504/ 3200]\n",
      "loss: 1.228804  [ 1520/ 3200]\n",
      "loss: 0.931319  [ 1536/ 3200]\n",
      "loss: 1.110915  [ 1552/ 3200]\n",
      "loss: 1.113570  [ 1568/ 3200]\n",
      "loss: 1.102023  [ 1584/ 3200]\n",
      "loss: 1.156525  [ 1600/ 3200]\n",
      "loss: 1.129766  [ 1616/ 3200]\n",
      "loss: 1.121589  [ 1632/ 3200]\n",
      "loss: 1.101104  [ 1648/ 3200]\n",
      "loss: 1.080637  [ 1664/ 3200]\n",
      "loss: 0.966112  [ 1680/ 3200]\n",
      "loss: 1.216108  [ 1696/ 3200]\n",
      "loss: 1.118451  [ 1712/ 3200]\n",
      "loss: 1.065243  [ 1728/ 3200]\n",
      "loss: 1.034830  [ 1744/ 3200]\n",
      "loss: 1.190648  [ 1760/ 3200]\n",
      "loss: 1.162426  [ 1776/ 3200]\n",
      "loss: 1.034392  [ 1792/ 3200]\n",
      "loss: 1.034519  [ 1808/ 3200]\n",
      "loss: 1.156281  [ 1824/ 3200]\n",
      "loss: 1.217359  [ 1840/ 3200]\n",
      "loss: 0.977131  [ 1856/ 3200]\n",
      "loss: 1.227002  [ 1872/ 3200]\n",
      "loss: 1.281528  [ 1888/ 3200]\n",
      "loss: 1.097960  [ 1904/ 3200]\n",
      "loss: 1.194460  [ 1920/ 3200]\n",
      "loss: 1.150823  [ 1936/ 3200]\n",
      "loss: 1.154927  [ 1952/ 3200]\n",
      "loss: 1.182068  [ 1968/ 3200]\n",
      "loss: 1.140846  [ 1984/ 3200]\n",
      "loss: 1.133341  [ 2000/ 3200]\n",
      "loss: 1.052095  [ 2016/ 3200]\n",
      "loss: 1.220665  [ 2032/ 3200]\n",
      "loss: 1.173566  [ 2048/ 3200]\n",
      "loss: 1.192266  [ 2064/ 3200]\n",
      "loss: 1.217779  [ 2080/ 3200]\n",
      "loss: 1.134274  [ 2096/ 3200]\n",
      "loss: 1.013267  [ 2112/ 3200]\n",
      "loss: 1.039576  [ 2128/ 3200]\n",
      "loss: 1.164386  [ 2144/ 3200]\n",
      "loss: 1.179333  [ 2160/ 3200]\n",
      "loss: 1.333110  [ 2176/ 3200]\n",
      "loss: 1.105139  [ 2192/ 3200]\n",
      "loss: 1.229014  [ 2208/ 3200]\n",
      "loss: 1.101671  [ 2224/ 3200]\n",
      "loss: 1.262516  [ 2240/ 3200]\n",
      "loss: 1.147168  [ 2256/ 3200]\n",
      "loss: 1.331764  [ 2272/ 3200]\n",
      "loss: 0.950696  [ 2288/ 3200]\n",
      "loss: 1.182187  [ 2304/ 3200]\n",
      "loss: 1.193846  [ 2320/ 3200]\n",
      "loss: 1.169680  [ 2336/ 3200]\n",
      "loss: 1.230682  [ 2352/ 3200]\n",
      "loss: 1.077314  [ 2368/ 3200]\n",
      "loss: 1.030304  [ 2384/ 3200]\n",
      "loss: 1.152130  [ 2400/ 3200]\n",
      "loss: 1.287283  [ 2416/ 3200]\n",
      "loss: 1.107830  [ 2432/ 3200]\n",
      "loss: 1.090275  [ 2448/ 3200]\n",
      "loss: 1.160562  [ 2464/ 3200]\n",
      "loss: 1.116624  [ 2480/ 3200]\n",
      "loss: 1.010968  [ 2496/ 3200]\n",
      "loss: 1.086558  [ 2512/ 3200]\n",
      "loss: 1.020607  [ 2528/ 3200]\n",
      "loss: 1.103716  [ 2544/ 3200]\n",
      "loss: 1.213563  [ 2560/ 3200]\n",
      "loss: 1.140497  [ 2576/ 3200]\n",
      "loss: 1.280707  [ 2592/ 3200]\n",
      "loss: 1.213687  [ 2608/ 3200]\n",
      "loss: 1.234123  [ 2624/ 3200]\n",
      "loss: 1.089506  [ 2640/ 3200]\n",
      "loss: 1.105403  [ 2656/ 3200]\n",
      "loss: 1.239698  [ 2672/ 3200]\n",
      "loss: 1.274991  [ 2688/ 3200]\n",
      "loss: 1.217352  [ 2704/ 3200]\n",
      "loss: 1.263385  [ 2720/ 3200]\n",
      "loss: 1.272104  [ 2736/ 3200]\n",
      "loss: 1.236420  [ 2752/ 3200]\n",
      "loss: 1.010549  [ 2768/ 3200]\n",
      "loss: 1.212069  [ 2784/ 3200]\n",
      "loss: 1.222720  [ 2800/ 3200]\n",
      "loss: 1.101403  [ 2816/ 3200]\n",
      "loss: 1.077314  [ 2832/ 3200]\n",
      "loss: 1.101120  [ 2848/ 3200]\n",
      "loss: 1.218431  [ 2864/ 3200]\n",
      "loss: 1.161938  [ 2880/ 3200]\n",
      "loss: 1.224937  [ 2896/ 3200]\n",
      "loss: 1.131984  [ 2912/ 3200]\n",
      "loss: 1.031366  [ 2928/ 3200]\n",
      "loss: 1.152092  [ 2944/ 3200]\n",
      "loss: 1.124881  [ 2960/ 3200]\n",
      "loss: 1.113686  [ 2976/ 3200]\n",
      "loss: 1.166506  [ 2992/ 3200]\n",
      "loss: 1.200188  [ 3008/ 3200]\n",
      "loss: 1.177369  [ 3024/ 3200]\n",
      "loss: 1.051040  [ 3040/ 3200]\n",
      "loss: 1.115647  [ 3056/ 3200]\n",
      "loss: 1.252504  [ 3072/ 3200]\n",
      "loss: 1.005990  [ 3088/ 3200]\n",
      "loss: 1.187762  [ 3104/ 3200]\n",
      "loss: 0.978992  [ 3120/ 3200]\n",
      "loss: 1.156034  [ 3136/ 3200]\n",
      "loss: 1.140775  [ 3152/ 3200]\n",
      "loss: 1.127598  [ 3168/ 3200]\n",
      "loss: 1.225057  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.069546\n",
      "f1 macro averaged score: 0.616911\n",
      "Accuracy               : 64.0%\n",
      "Confusion matrix       :\n",
      "tensor([[184,  10,   3,   3],\n",
      "        [ 62,  50,  61,  27],\n",
      "        [ 15,  31, 135,  19],\n",
      "        [ 15,  24,  18, 143]])\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 1.076028  [    0/ 3200]\n",
      "loss: 1.153165  [   16/ 3200]\n",
      "loss: 0.994676  [   32/ 3200]\n",
      "loss: 1.212419  [   48/ 3200]\n",
      "loss: 1.361407  [   64/ 3200]\n",
      "loss: 1.238167  [   80/ 3200]\n",
      "loss: 1.081340  [   96/ 3200]\n",
      "loss: 1.079688  [  112/ 3200]\n",
      "loss: 1.034918  [  128/ 3200]\n",
      "loss: 1.090448  [  144/ 3200]\n",
      "loss: 1.123381  [  160/ 3200]\n",
      "loss: 1.078696  [  176/ 3200]\n",
      "loss: 1.175870  [  192/ 3200]\n",
      "loss: 1.247991  [  208/ 3200]\n",
      "loss: 1.207749  [  224/ 3200]\n",
      "loss: 1.077095  [  240/ 3200]\n",
      "loss: 0.959761  [  256/ 3200]\n",
      "loss: 1.001568  [  272/ 3200]\n",
      "loss: 1.009182  [  288/ 3200]\n",
      "loss: 1.219962  [  304/ 3200]\n",
      "loss: 1.142002  [  320/ 3200]\n",
      "loss: 1.048495  [  336/ 3200]\n",
      "loss: 1.197432  [  352/ 3200]\n",
      "loss: 1.101771  [  368/ 3200]\n",
      "loss: 1.001935  [  384/ 3200]\n",
      "loss: 1.084749  [  400/ 3200]\n",
      "loss: 1.186235  [  416/ 3200]\n",
      "loss: 1.053621  [  432/ 3200]\n",
      "loss: 1.179469  [  448/ 3200]\n",
      "loss: 1.170043  [  464/ 3200]\n",
      "loss: 1.163035  [  480/ 3200]\n",
      "loss: 1.023196  [  496/ 3200]\n",
      "loss: 1.135876  [  512/ 3200]\n",
      "loss: 1.115674  [  528/ 3200]\n",
      "loss: 1.193560  [  544/ 3200]\n",
      "loss: 1.275296  [  560/ 3200]\n",
      "loss: 1.183602  [  576/ 3200]\n",
      "loss: 1.075469  [  592/ 3200]\n",
      "loss: 1.253205  [  608/ 3200]\n",
      "loss: 1.036345  [  624/ 3200]\n",
      "loss: 1.082988  [  640/ 3200]\n",
      "loss: 1.313575  [  656/ 3200]\n",
      "loss: 1.357985  [  672/ 3200]\n",
      "loss: 1.191642  [  688/ 3200]\n",
      "loss: 1.102902  [  704/ 3200]\n",
      "loss: 1.067108  [  720/ 3200]\n",
      "loss: 1.174525  [  736/ 3200]\n",
      "loss: 1.038748  [  752/ 3200]\n",
      "loss: 1.185484  [  768/ 3200]\n",
      "loss: 1.089629  [  784/ 3200]\n",
      "loss: 1.078518  [  800/ 3200]\n",
      "loss: 1.237343  [  816/ 3200]\n",
      "loss: 1.021841  [  832/ 3200]\n",
      "loss: 1.233456  [  848/ 3200]\n",
      "loss: 1.098217  [  864/ 3200]\n",
      "loss: 1.021372  [  880/ 3200]\n",
      "loss: 0.959591  [  896/ 3200]\n",
      "loss: 1.282913  [  912/ 3200]\n",
      "loss: 1.205998  [  928/ 3200]\n",
      "loss: 1.030969  [  944/ 3200]\n",
      "loss: 1.116913  [  960/ 3200]\n",
      "loss: 1.257593  [  976/ 3200]\n",
      "loss: 1.040866  [  992/ 3200]\n",
      "loss: 1.221661  [ 1008/ 3200]\n",
      "loss: 1.273204  [ 1024/ 3200]\n",
      "loss: 1.093801  [ 1040/ 3200]\n",
      "loss: 1.191818  [ 1056/ 3200]\n",
      "loss: 1.204826  [ 1072/ 3200]\n",
      "loss: 0.974398  [ 1088/ 3200]\n",
      "loss: 1.240463  [ 1104/ 3200]\n",
      "loss: 0.993742  [ 1120/ 3200]\n",
      "loss: 1.162485  [ 1136/ 3200]\n",
      "loss: 1.054902  [ 1152/ 3200]\n",
      "loss: 1.123406  [ 1168/ 3200]\n",
      "loss: 1.058891  [ 1184/ 3200]\n",
      "loss: 1.021413  [ 1200/ 3200]\n",
      "loss: 1.189996  [ 1216/ 3200]\n",
      "loss: 1.107357  [ 1232/ 3200]\n",
      "loss: 1.032063  [ 1248/ 3200]\n",
      "loss: 1.158237  [ 1264/ 3200]\n",
      "loss: 1.200486  [ 1280/ 3200]\n",
      "loss: 1.144532  [ 1296/ 3200]\n",
      "loss: 1.254167  [ 1312/ 3200]\n",
      "loss: 1.094976  [ 1328/ 3200]\n",
      "loss: 1.055936  [ 1344/ 3200]\n",
      "loss: 1.142067  [ 1360/ 3200]\n",
      "loss: 1.189135  [ 1376/ 3200]\n",
      "loss: 1.134653  [ 1392/ 3200]\n",
      "loss: 1.040207  [ 1408/ 3200]\n",
      "loss: 1.199550  [ 1424/ 3200]\n",
      "loss: 1.120749  [ 1440/ 3200]\n",
      "loss: 1.071735  [ 1456/ 3200]\n",
      "loss: 1.173079  [ 1472/ 3200]\n",
      "loss: 1.056149  [ 1488/ 3200]\n",
      "loss: 0.906088  [ 1504/ 3200]\n",
      "loss: 1.142458  [ 1520/ 3200]\n",
      "loss: 1.094435  [ 1536/ 3200]\n",
      "loss: 1.018110  [ 1552/ 3200]\n",
      "loss: 1.046468  [ 1568/ 3200]\n",
      "loss: 1.088038  [ 1584/ 3200]\n",
      "loss: 1.089635  [ 1600/ 3200]\n",
      "loss: 1.077594  [ 1616/ 3200]\n",
      "loss: 1.034703  [ 1632/ 3200]\n",
      "loss: 1.024144  [ 1648/ 3200]\n",
      "loss: 1.311816  [ 1664/ 3200]\n",
      "loss: 1.107626  [ 1680/ 3200]\n",
      "loss: 1.087028  [ 1696/ 3200]\n",
      "loss: 1.187624  [ 1712/ 3200]\n",
      "loss: 1.136429  [ 1728/ 3200]\n",
      "loss: 1.250515  [ 1744/ 3200]\n",
      "loss: 1.115016  [ 1760/ 3200]\n",
      "loss: 1.099780  [ 1776/ 3200]\n",
      "loss: 1.148734  [ 1792/ 3200]\n",
      "loss: 1.027458  [ 1808/ 3200]\n",
      "loss: 1.244636  [ 1824/ 3200]\n",
      "loss: 1.109382  [ 1840/ 3200]\n",
      "loss: 1.061128  [ 1856/ 3200]\n",
      "loss: 1.188284  [ 1872/ 3200]\n",
      "loss: 1.034779  [ 1888/ 3200]\n",
      "loss: 1.264825  [ 1904/ 3200]\n",
      "loss: 1.025180  [ 1920/ 3200]\n",
      "loss: 1.077080  [ 1936/ 3200]\n",
      "loss: 1.319412  [ 1952/ 3200]\n",
      "loss: 1.085559  [ 1968/ 3200]\n",
      "loss: 1.179567  [ 1984/ 3200]\n",
      "loss: 1.174098  [ 2000/ 3200]\n",
      "loss: 0.962276  [ 2016/ 3200]\n",
      "loss: 1.190928  [ 2032/ 3200]\n",
      "loss: 1.115376  [ 2048/ 3200]\n",
      "loss: 1.215393  [ 2064/ 3200]\n",
      "loss: 0.900281  [ 2080/ 3200]\n",
      "loss: 1.163164  [ 2096/ 3200]\n",
      "loss: 1.130421  [ 2112/ 3200]\n",
      "loss: 1.057615  [ 2128/ 3200]\n",
      "loss: 0.852892  [ 2144/ 3200]\n",
      "loss: 1.041574  [ 2160/ 3200]\n",
      "loss: 1.267830  [ 2176/ 3200]\n",
      "loss: 1.166536  [ 2192/ 3200]\n",
      "loss: 1.235965  [ 2208/ 3200]\n",
      "loss: 0.965653  [ 2224/ 3200]\n",
      "loss: 1.192497  [ 2240/ 3200]\n",
      "loss: 1.152620  [ 2256/ 3200]\n",
      "loss: 1.207520  [ 2272/ 3200]\n",
      "loss: 1.168469  [ 2288/ 3200]\n",
      "loss: 1.095979  [ 2304/ 3200]\n",
      "loss: 1.102425  [ 2320/ 3200]\n",
      "loss: 1.109529  [ 2336/ 3200]\n",
      "loss: 1.085426  [ 2352/ 3200]\n",
      "loss: 1.215662  [ 2368/ 3200]\n",
      "loss: 0.989394  [ 2384/ 3200]\n",
      "loss: 1.016121  [ 2400/ 3200]\n",
      "loss: 1.142790  [ 2416/ 3200]\n",
      "loss: 1.234478  [ 2432/ 3200]\n",
      "loss: 1.258168  [ 2448/ 3200]\n",
      "loss: 1.132292  [ 2464/ 3200]\n",
      "loss: 0.975595  [ 2480/ 3200]\n",
      "loss: 1.362949  [ 2496/ 3200]\n",
      "loss: 1.103323  [ 2512/ 3200]\n",
      "loss: 1.200256  [ 2528/ 3200]\n",
      "loss: 0.964050  [ 2544/ 3200]\n",
      "loss: 1.133254  [ 2560/ 3200]\n",
      "loss: 1.101475  [ 2576/ 3200]\n",
      "loss: 1.039747  [ 2592/ 3200]\n",
      "loss: 1.115458  [ 2608/ 3200]\n",
      "loss: 1.201859  [ 2624/ 3200]\n",
      "loss: 1.074126  [ 2640/ 3200]\n",
      "loss: 1.098940  [ 2656/ 3200]\n",
      "loss: 1.318591  [ 2672/ 3200]\n",
      "loss: 1.049617  [ 2688/ 3200]\n",
      "loss: 1.012165  [ 2704/ 3200]\n",
      "loss: 1.054500  [ 2720/ 3200]\n",
      "loss: 1.161631  [ 2736/ 3200]\n",
      "loss: 1.140427  [ 2752/ 3200]\n",
      "loss: 1.138965  [ 2768/ 3200]\n",
      "loss: 1.136772  [ 2784/ 3200]\n",
      "loss: 1.141114  [ 2800/ 3200]\n",
      "loss: 1.169990  [ 2816/ 3200]\n",
      "loss: 1.074445  [ 2832/ 3200]\n",
      "loss: 1.178685  [ 2848/ 3200]\n",
      "loss: 1.116045  [ 2864/ 3200]\n",
      "loss: 1.063688  [ 2880/ 3200]\n",
      "loss: 1.108348  [ 2896/ 3200]\n",
      "loss: 1.187649  [ 2912/ 3200]\n",
      "loss: 1.201213  [ 2928/ 3200]\n",
      "loss: 1.176431  [ 2944/ 3200]\n",
      "loss: 1.162325  [ 2960/ 3200]\n",
      "loss: 1.090183  [ 2976/ 3200]\n",
      "loss: 1.113243  [ 2992/ 3200]\n",
      "loss: 1.230606  [ 3008/ 3200]\n",
      "loss: 1.095646  [ 3024/ 3200]\n",
      "loss: 1.049213  [ 3040/ 3200]\n",
      "loss: 1.043642  [ 3056/ 3200]\n",
      "loss: 1.145114  [ 3072/ 3200]\n",
      "loss: 1.021195  [ 3088/ 3200]\n",
      "loss: 1.213938  [ 3104/ 3200]\n",
      "loss: 0.985405  [ 3120/ 3200]\n",
      "loss: 0.984874  [ 3136/ 3200]\n",
      "loss: 1.067981  [ 3152/ 3200]\n",
      "loss: 1.027286  [ 3168/ 3200]\n",
      "loss: 1.037504  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.068444\n",
      "f1 macro averaged score: 0.547978\n",
      "Accuracy               : 59.2%\n",
      "Confusion matrix       :\n",
      "tensor([[181,   3,   3,  13],\n",
      "        [ 59,  35,  19,  87],\n",
      "        [ 13,  28,  78,  81],\n",
      "        [ 11,   3,   6, 180]])\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 1.183746  [    0/ 3200]\n",
      "loss: 1.056272  [   16/ 3200]\n",
      "loss: 1.235455  [   32/ 3200]\n",
      "loss: 1.134366  [   48/ 3200]\n",
      "loss: 1.131806  [   64/ 3200]\n",
      "loss: 1.439036  [   80/ 3200]\n",
      "loss: 1.149602  [   96/ 3200]\n",
      "loss: 1.074230  [  112/ 3200]\n",
      "loss: 1.124250  [  128/ 3200]\n",
      "loss: 1.233057  [  144/ 3200]\n",
      "loss: 1.121372  [  160/ 3200]\n",
      "loss: 1.089700  [  176/ 3200]\n",
      "loss: 1.037094  [  192/ 3200]\n",
      "loss: 1.116667  [  208/ 3200]\n",
      "loss: 1.285630  [  224/ 3200]\n",
      "loss: 1.083080  [  240/ 3200]\n",
      "loss: 1.214633  [  256/ 3200]\n",
      "loss: 1.003415  [  272/ 3200]\n",
      "loss: 1.076326  [  288/ 3200]\n",
      "loss: 1.285608  [  304/ 3200]\n",
      "loss: 1.257926  [  320/ 3200]\n",
      "loss: 1.162569  [  336/ 3200]\n",
      "loss: 1.025970  [  352/ 3200]\n",
      "loss: 1.164054  [  368/ 3200]\n",
      "loss: 1.251208  [  384/ 3200]\n",
      "loss: 1.128014  [  400/ 3200]\n",
      "loss: 1.309739  [  416/ 3200]\n",
      "loss: 1.050285  [  432/ 3200]\n",
      "loss: 1.081605  [  448/ 3200]\n",
      "loss: 1.067075  [  464/ 3200]\n",
      "loss: 1.242267  [  480/ 3200]\n",
      "loss: 0.998849  [  496/ 3200]\n",
      "loss: 1.251371  [  512/ 3200]\n",
      "loss: 0.975984  [  528/ 3200]\n",
      "loss: 1.180048  [  544/ 3200]\n",
      "loss: 1.160992  [  560/ 3200]\n",
      "loss: 0.992104  [  576/ 3200]\n",
      "loss: 0.989454  [  592/ 3200]\n",
      "loss: 1.143313  [  608/ 3200]\n",
      "loss: 1.021754  [  624/ 3200]\n",
      "loss: 1.071323  [  640/ 3200]\n",
      "loss: 1.098598  [  656/ 3200]\n",
      "loss: 1.035363  [  672/ 3200]\n",
      "loss: 1.127197  [  688/ 3200]\n",
      "loss: 1.155438  [  704/ 3200]\n",
      "loss: 1.010267  [  720/ 3200]\n",
      "loss: 1.122003  [  736/ 3200]\n",
      "loss: 1.059523  [  752/ 3200]\n",
      "loss: 1.043328  [  768/ 3200]\n",
      "loss: 1.018049  [  784/ 3200]\n",
      "loss: 1.080157  [  800/ 3200]\n",
      "loss: 1.122997  [  816/ 3200]\n",
      "loss: 1.260032  [  832/ 3200]\n",
      "loss: 1.131265  [  848/ 3200]\n",
      "loss: 1.175662  [  864/ 3200]\n",
      "loss: 1.112884  [  880/ 3200]\n",
      "loss: 1.074457  [  896/ 3200]\n",
      "loss: 1.081757  [  912/ 3200]\n",
      "loss: 1.130946  [  928/ 3200]\n",
      "loss: 0.965368  [  944/ 3200]\n",
      "loss: 1.132222  [  960/ 3200]\n",
      "loss: 1.098142  [  976/ 3200]\n",
      "loss: 1.007657  [  992/ 3200]\n",
      "loss: 0.972927  [ 1008/ 3200]\n",
      "loss: 1.140580  [ 1024/ 3200]\n",
      "loss: 1.097099  [ 1040/ 3200]\n",
      "loss: 1.136053  [ 1056/ 3200]\n",
      "loss: 1.052903  [ 1072/ 3200]\n",
      "loss: 1.088153  [ 1088/ 3200]\n",
      "loss: 1.063344  [ 1104/ 3200]\n",
      "loss: 0.857502  [ 1120/ 3200]\n",
      "loss: 1.219608  [ 1136/ 3200]\n",
      "loss: 1.140452  [ 1152/ 3200]\n",
      "loss: 1.096769  [ 1168/ 3200]\n",
      "loss: 0.961935  [ 1184/ 3200]\n",
      "loss: 1.201479  [ 1200/ 3200]\n",
      "loss: 1.022590  [ 1216/ 3200]\n",
      "loss: 1.157625  [ 1232/ 3200]\n",
      "loss: 1.131440  [ 1248/ 3200]\n",
      "loss: 0.966157  [ 1264/ 3200]\n",
      "loss: 1.115138  [ 1280/ 3200]\n",
      "loss: 0.996718  [ 1296/ 3200]\n",
      "loss: 1.056464  [ 1312/ 3200]\n",
      "loss: 1.150589  [ 1328/ 3200]\n",
      "loss: 1.087183  [ 1344/ 3200]\n",
      "loss: 1.292429  [ 1360/ 3200]\n",
      "loss: 1.284165  [ 1376/ 3200]\n",
      "loss: 1.096861  [ 1392/ 3200]\n",
      "loss: 1.309456  [ 1408/ 3200]\n",
      "loss: 1.095321  [ 1424/ 3200]\n",
      "loss: 1.093860  [ 1440/ 3200]\n",
      "loss: 1.211961  [ 1456/ 3200]\n",
      "loss: 1.238219  [ 1472/ 3200]\n",
      "loss: 1.241304  [ 1488/ 3200]\n",
      "loss: 1.115332  [ 1504/ 3200]\n",
      "loss: 1.178178  [ 1520/ 3200]\n",
      "loss: 1.068373  [ 1536/ 3200]\n",
      "loss: 1.015508  [ 1552/ 3200]\n",
      "loss: 1.032483  [ 1568/ 3200]\n",
      "loss: 0.827557  [ 1584/ 3200]\n",
      "loss: 0.958200  [ 1600/ 3200]\n",
      "loss: 0.946642  [ 1616/ 3200]\n",
      "loss: 1.007208  [ 1632/ 3200]\n",
      "loss: 1.070650  [ 1648/ 3200]\n",
      "loss: 1.072035  [ 1664/ 3200]\n",
      "loss: 0.845768  [ 1680/ 3200]\n",
      "loss: 1.110744  [ 1696/ 3200]\n",
      "loss: 1.216886  [ 1712/ 3200]\n",
      "loss: 1.007586  [ 1728/ 3200]\n",
      "loss: 1.196628  [ 1744/ 3200]\n",
      "loss: 1.088065  [ 1760/ 3200]\n",
      "loss: 1.085075  [ 1776/ 3200]\n",
      "loss: 1.007905  [ 1792/ 3200]\n",
      "loss: 1.094119  [ 1808/ 3200]\n",
      "loss: 1.064659  [ 1824/ 3200]\n",
      "loss: 1.138108  [ 1840/ 3200]\n",
      "loss: 1.228304  [ 1856/ 3200]\n",
      "loss: 1.092506  [ 1872/ 3200]\n",
      "loss: 0.909659  [ 1888/ 3200]\n",
      "loss: 1.303587  [ 1904/ 3200]\n",
      "loss: 1.094615  [ 1920/ 3200]\n",
      "loss: 1.031088  [ 1936/ 3200]\n",
      "loss: 1.043604  [ 1952/ 3200]\n",
      "loss: 1.179153  [ 1968/ 3200]\n",
      "loss: 1.049087  [ 1984/ 3200]\n",
      "loss: 1.135966  [ 2000/ 3200]\n",
      "loss: 1.245024  [ 2016/ 3200]\n",
      "loss: 1.004610  [ 2032/ 3200]\n",
      "loss: 1.032821  [ 2048/ 3200]\n",
      "loss: 1.043099  [ 2064/ 3200]\n",
      "loss: 1.076989  [ 2080/ 3200]\n",
      "loss: 1.105738  [ 2096/ 3200]\n",
      "loss: 1.140230  [ 2112/ 3200]\n",
      "loss: 1.209383  [ 2128/ 3200]\n",
      "loss: 0.894410  [ 2144/ 3200]\n",
      "loss: 1.154864  [ 2160/ 3200]\n",
      "loss: 1.037101  [ 2176/ 3200]\n",
      "loss: 1.054012  [ 2192/ 3200]\n",
      "loss: 1.114749  [ 2208/ 3200]\n",
      "loss: 1.066497  [ 2224/ 3200]\n",
      "loss: 0.864984  [ 2240/ 3200]\n",
      "loss: 1.058192  [ 2256/ 3200]\n",
      "loss: 1.061425  [ 2272/ 3200]\n",
      "loss: 1.178464  [ 2288/ 3200]\n",
      "loss: 1.026251  [ 2304/ 3200]\n",
      "loss: 1.171790  [ 2320/ 3200]\n",
      "loss: 1.080453  [ 2336/ 3200]\n",
      "loss: 1.209697  [ 2352/ 3200]\n",
      "loss: 1.215327  [ 2368/ 3200]\n",
      "loss: 1.048848  [ 2384/ 3200]\n",
      "loss: 1.254288  [ 2400/ 3200]\n",
      "loss: 1.065445  [ 2416/ 3200]\n",
      "loss: 1.108610  [ 2432/ 3200]\n",
      "loss: 1.092613  [ 2448/ 3200]\n",
      "loss: 1.073709  [ 2464/ 3200]\n",
      "loss: 0.968017  [ 2480/ 3200]\n",
      "loss: 1.054218  [ 2496/ 3200]\n",
      "loss: 1.089339  [ 2512/ 3200]\n",
      "loss: 1.271548  [ 2528/ 3200]\n",
      "loss: 1.068680  [ 2544/ 3200]\n",
      "loss: 1.199328  [ 2560/ 3200]\n",
      "loss: 1.019147  [ 2576/ 3200]\n",
      "loss: 1.049145  [ 2592/ 3200]\n",
      "loss: 1.153469  [ 2608/ 3200]\n",
      "loss: 1.037661  [ 2624/ 3200]\n",
      "loss: 1.232749  [ 2640/ 3200]\n",
      "loss: 1.135692  [ 2656/ 3200]\n",
      "loss: 1.057504  [ 2672/ 3200]\n",
      "loss: 1.195383  [ 2688/ 3200]\n",
      "loss: 0.961219  [ 2704/ 3200]\n",
      "loss: 1.004785  [ 2720/ 3200]\n",
      "loss: 1.087612  [ 2736/ 3200]\n",
      "loss: 1.178272  [ 2752/ 3200]\n",
      "loss: 1.073103  [ 2768/ 3200]\n",
      "loss: 1.104650  [ 2784/ 3200]\n",
      "loss: 0.994299  [ 2800/ 3200]\n",
      "loss: 1.124608  [ 2816/ 3200]\n",
      "loss: 1.176563  [ 2832/ 3200]\n",
      "loss: 1.052784  [ 2848/ 3200]\n",
      "loss: 1.161557  [ 2864/ 3200]\n",
      "loss: 1.299001  [ 2880/ 3200]\n",
      "loss: 1.179721  [ 2896/ 3200]\n",
      "loss: 1.130988  [ 2912/ 3200]\n",
      "loss: 1.172885  [ 2928/ 3200]\n",
      "loss: 1.142453  [ 2944/ 3200]\n",
      "loss: 1.214335  [ 2960/ 3200]\n",
      "loss: 1.144329  [ 2976/ 3200]\n",
      "loss: 1.148333  [ 2992/ 3200]\n",
      "loss: 1.094793  [ 3008/ 3200]\n",
      "loss: 1.050219  [ 3024/ 3200]\n",
      "loss: 1.139843  [ 3040/ 3200]\n",
      "loss: 1.089004  [ 3056/ 3200]\n",
      "loss: 1.297911  [ 3072/ 3200]\n",
      "loss: 1.030324  [ 3088/ 3200]\n",
      "loss: 1.123509  [ 3104/ 3200]\n",
      "loss: 1.103675  [ 3120/ 3200]\n",
      "loss: 1.229824  [ 3136/ 3200]\n",
      "loss: 0.936096  [ 3152/ 3200]\n",
      "loss: 1.119594  [ 3168/ 3200]\n",
      "loss: 1.210178  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.067217\n",
      "f1 macro averaged score: 0.600937\n",
      "Accuracy               : 61.0%\n",
      "Confusion matrix       :\n",
      "tensor([[181,  15,   3,   1],\n",
      "        [ 57,  65,  69,   9],\n",
      "        [ 10,  43, 144,   3],\n",
      "        [ 11,  62,  29,  98]])\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 1.025307  [    0/ 3200]\n",
      "loss: 1.258529  [   16/ 3200]\n",
      "loss: 1.130051  [   32/ 3200]\n",
      "loss: 1.200102  [   48/ 3200]\n",
      "loss: 1.028548  [   64/ 3200]\n",
      "loss: 1.126469  [   80/ 3200]\n",
      "loss: 0.985503  [   96/ 3200]\n",
      "loss: 1.176269  [  112/ 3200]\n",
      "loss: 1.244949  [  128/ 3200]\n",
      "loss: 1.222755  [  144/ 3200]\n",
      "loss: 1.195252  [  160/ 3200]\n",
      "loss: 1.014542  [  176/ 3200]\n",
      "loss: 1.084388  [  192/ 3200]\n",
      "loss: 0.992890  [  208/ 3200]\n",
      "loss: 1.133980  [  224/ 3200]\n",
      "loss: 1.171821  [  240/ 3200]\n",
      "loss: 1.102872  [  256/ 3200]\n",
      "loss: 1.075314  [  272/ 3200]\n",
      "loss: 1.215407  [  288/ 3200]\n",
      "loss: 0.952201  [  304/ 3200]\n",
      "loss: 1.093397  [  320/ 3200]\n",
      "loss: 1.199700  [  336/ 3200]\n",
      "loss: 1.075185  [  352/ 3200]\n",
      "loss: 1.130849  [  368/ 3200]\n",
      "loss: 1.148717  [  384/ 3200]\n",
      "loss: 1.097682  [  400/ 3200]\n",
      "loss: 1.213449  [  416/ 3200]\n",
      "loss: 1.116038  [  432/ 3200]\n",
      "loss: 1.171467  [  448/ 3200]\n",
      "loss: 1.042676  [  464/ 3200]\n",
      "loss: 1.108878  [  480/ 3200]\n",
      "loss: 1.168836  [  496/ 3200]\n",
      "loss: 1.084653  [  512/ 3200]\n",
      "loss: 1.070431  [  528/ 3200]\n",
      "loss: 1.036605  [  544/ 3200]\n",
      "loss: 1.176171  [  560/ 3200]\n",
      "loss: 0.884266  [  576/ 3200]\n",
      "loss: 1.210704  [  592/ 3200]\n",
      "loss: 1.170424  [  608/ 3200]\n",
      "loss: 1.039972  [  624/ 3200]\n",
      "loss: 0.965046  [  640/ 3200]\n",
      "loss: 1.294340  [  656/ 3200]\n",
      "loss: 1.000106  [  672/ 3200]\n",
      "loss: 1.095600  [  688/ 3200]\n",
      "loss: 0.902241  [  704/ 3200]\n",
      "loss: 1.115035  [  720/ 3200]\n",
      "loss: 1.090594  [  736/ 3200]\n",
      "loss: 1.370551  [  752/ 3200]\n",
      "loss: 1.050804  [  768/ 3200]\n",
      "loss: 1.152197  [  784/ 3200]\n",
      "loss: 1.019920  [  800/ 3200]\n",
      "loss: 1.191442  [  816/ 3200]\n",
      "loss: 1.301175  [  832/ 3200]\n",
      "loss: 1.113958  [  848/ 3200]\n",
      "loss: 0.949715  [  864/ 3200]\n",
      "loss: 1.138686  [  880/ 3200]\n",
      "loss: 1.102233  [  896/ 3200]\n",
      "loss: 0.879491  [  912/ 3200]\n",
      "loss: 0.955801  [  928/ 3200]\n",
      "loss: 1.210454  [  944/ 3200]\n",
      "loss: 1.149668  [  960/ 3200]\n",
      "loss: 1.320665  [  976/ 3200]\n",
      "loss: 0.998836  [  992/ 3200]\n",
      "loss: 1.091978  [ 1008/ 3200]\n",
      "loss: 1.096711  [ 1024/ 3200]\n",
      "loss: 1.104377  [ 1040/ 3200]\n",
      "loss: 0.987634  [ 1056/ 3200]\n",
      "loss: 1.001508  [ 1072/ 3200]\n",
      "loss: 1.093351  [ 1088/ 3200]\n",
      "loss: 0.966879  [ 1104/ 3200]\n",
      "loss: 1.009287  [ 1120/ 3200]\n",
      "loss: 1.185261  [ 1136/ 3200]\n",
      "loss: 0.989090  [ 1152/ 3200]\n",
      "loss: 1.208339  [ 1168/ 3200]\n",
      "loss: 0.933310  [ 1184/ 3200]\n",
      "loss: 1.232619  [ 1200/ 3200]\n",
      "loss: 1.047991  [ 1216/ 3200]\n",
      "loss: 1.076988  [ 1232/ 3200]\n",
      "loss: 0.996030  [ 1248/ 3200]\n",
      "loss: 0.913484  [ 1264/ 3200]\n",
      "loss: 1.013344  [ 1280/ 3200]\n",
      "loss: 1.135806  [ 1296/ 3200]\n",
      "loss: 0.953771  [ 1312/ 3200]\n",
      "loss: 0.974174  [ 1328/ 3200]\n",
      "loss: 1.241440  [ 1344/ 3200]\n",
      "loss: 1.439325  [ 1360/ 3200]\n",
      "loss: 0.902724  [ 1376/ 3200]\n",
      "loss: 0.954436  [ 1392/ 3200]\n",
      "loss: 1.012270  [ 1408/ 3200]\n",
      "loss: 0.998007  [ 1424/ 3200]\n",
      "loss: 1.090373  [ 1440/ 3200]\n",
      "loss: 1.273941  [ 1456/ 3200]\n",
      "loss: 1.125928  [ 1472/ 3200]\n",
      "loss: 1.268777  [ 1488/ 3200]\n",
      "loss: 1.135699  [ 1504/ 3200]\n",
      "loss: 0.953339  [ 1520/ 3200]\n",
      "loss: 1.134133  [ 1536/ 3200]\n",
      "loss: 1.148337  [ 1552/ 3200]\n",
      "loss: 0.975610  [ 1568/ 3200]\n",
      "loss: 1.206249  [ 1584/ 3200]\n",
      "loss: 1.041824  [ 1600/ 3200]\n",
      "loss: 1.071316  [ 1616/ 3200]\n",
      "loss: 1.087196  [ 1632/ 3200]\n",
      "loss: 1.247333  [ 1648/ 3200]\n",
      "loss: 0.951250  [ 1664/ 3200]\n",
      "loss: 1.253244  [ 1680/ 3200]\n",
      "loss: 1.067941  [ 1696/ 3200]\n",
      "loss: 1.231475  [ 1712/ 3200]\n",
      "loss: 1.165657  [ 1728/ 3200]\n",
      "loss: 1.082743  [ 1744/ 3200]\n",
      "loss: 0.986183  [ 1760/ 3200]\n",
      "loss: 0.929301  [ 1776/ 3200]\n",
      "loss: 1.038081  [ 1792/ 3200]\n",
      "loss: 1.022173  [ 1808/ 3200]\n",
      "loss: 1.016837  [ 1824/ 3200]\n",
      "loss: 1.086178  [ 1840/ 3200]\n",
      "loss: 1.269561  [ 1856/ 3200]\n",
      "loss: 1.182675  [ 1872/ 3200]\n",
      "loss: 1.014021  [ 1888/ 3200]\n",
      "loss: 1.111797  [ 1904/ 3200]\n",
      "loss: 1.074069  [ 1920/ 3200]\n",
      "loss: 1.060821  [ 1936/ 3200]\n",
      "loss: 0.997665  [ 1952/ 3200]\n",
      "loss: 1.201512  [ 1968/ 3200]\n",
      "loss: 1.196585  [ 1984/ 3200]\n",
      "loss: 1.230742  [ 2000/ 3200]\n",
      "loss: 0.979452  [ 2016/ 3200]\n",
      "loss: 1.004074  [ 2032/ 3200]\n",
      "loss: 1.127124  [ 2048/ 3200]\n",
      "loss: 1.033962  [ 2064/ 3200]\n",
      "loss: 1.125394  [ 2080/ 3200]\n",
      "loss: 1.105770  [ 2096/ 3200]\n",
      "loss: 0.950634  [ 2112/ 3200]\n",
      "loss: 1.201807  [ 2128/ 3200]\n",
      "loss: 1.021451  [ 2144/ 3200]\n",
      "loss: 0.985722  [ 2160/ 3200]\n",
      "loss: 1.005054  [ 2176/ 3200]\n",
      "loss: 0.982282  [ 2192/ 3200]\n",
      "loss: 0.913665  [ 2208/ 3200]\n",
      "loss: 1.348806  [ 2224/ 3200]\n",
      "loss: 1.132524  [ 2240/ 3200]\n",
      "loss: 1.076451  [ 2256/ 3200]\n",
      "loss: 1.137072  [ 2272/ 3200]\n",
      "loss: 1.137389  [ 2288/ 3200]\n",
      "loss: 1.075737  [ 2304/ 3200]\n",
      "loss: 1.108745  [ 2320/ 3200]\n",
      "loss: 1.052877  [ 2336/ 3200]\n",
      "loss: 1.078743  [ 2352/ 3200]\n",
      "loss: 0.952301  [ 2368/ 3200]\n",
      "loss: 1.012896  [ 2384/ 3200]\n",
      "loss: 0.952710  [ 2400/ 3200]\n",
      "loss: 1.250139  [ 2416/ 3200]\n",
      "loss: 1.222400  [ 2432/ 3200]\n",
      "loss: 1.116312  [ 2448/ 3200]\n",
      "loss: 0.943606  [ 2464/ 3200]\n",
      "loss: 0.999526  [ 2480/ 3200]\n",
      "loss: 1.067761  [ 2496/ 3200]\n",
      "loss: 1.062287  [ 2512/ 3200]\n",
      "loss: 0.903828  [ 2528/ 3200]\n",
      "loss: 0.977937  [ 2544/ 3200]\n",
      "loss: 1.055573  [ 2560/ 3200]\n",
      "loss: 1.178829  [ 2576/ 3200]\n",
      "loss: 1.126038  [ 2592/ 3200]\n",
      "loss: 1.147608  [ 2608/ 3200]\n",
      "loss: 1.170145  [ 2624/ 3200]\n",
      "loss: 0.869177  [ 2640/ 3200]\n",
      "loss: 1.096181  [ 2656/ 3200]\n",
      "loss: 1.341327  [ 2672/ 3200]\n",
      "loss: 1.156529  [ 2688/ 3200]\n",
      "loss: 1.163152  [ 2704/ 3200]\n",
      "loss: 1.169419  [ 2720/ 3200]\n",
      "loss: 0.948784  [ 2736/ 3200]\n",
      "loss: 1.106396  [ 2752/ 3200]\n",
      "loss: 1.180668  [ 2768/ 3200]\n",
      "loss: 1.022331  [ 2784/ 3200]\n",
      "loss: 1.127532  [ 2800/ 3200]\n",
      "loss: 1.180062  [ 2816/ 3200]\n",
      "loss: 0.859169  [ 2832/ 3200]\n",
      "loss: 1.197017  [ 2848/ 3200]\n",
      "loss: 1.077600  [ 2864/ 3200]\n",
      "loss: 1.155518  [ 2880/ 3200]\n",
      "loss: 1.173718  [ 2896/ 3200]\n",
      "loss: 1.028321  [ 2912/ 3200]\n",
      "loss: 1.011454  [ 2928/ 3200]\n",
      "loss: 1.135749  [ 2944/ 3200]\n",
      "loss: 1.145167  [ 2960/ 3200]\n",
      "loss: 0.963219  [ 2976/ 3200]\n",
      "loss: 0.969600  [ 2992/ 3200]\n",
      "loss: 0.955788  [ 3008/ 3200]\n",
      "loss: 0.893714  [ 3024/ 3200]\n",
      "loss: 1.142961  [ 3040/ 3200]\n",
      "loss: 1.381795  [ 3056/ 3200]\n",
      "loss: 0.947107  [ 3072/ 3200]\n",
      "loss: 0.900821  [ 3088/ 3200]\n",
      "loss: 0.972124  [ 3104/ 3200]\n",
      "loss: 0.955142  [ 3120/ 3200]\n",
      "loss: 1.222504  [ 3136/ 3200]\n",
      "loss: 1.101578  [ 3152/ 3200]\n",
      "loss: 1.027291  [ 3168/ 3200]\n",
      "loss: 1.058802  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.066064\n",
      "f1 macro averaged score: 0.651701\n",
      "Accuracy               : 65.4%\n",
      "Confusion matrix       :\n",
      "tensor([[161,  32,   3,   4],\n",
      "        [ 33,  89,  44,  34],\n",
      "        [  2,  48, 119,  31],\n",
      "        [  7,  28,  11, 154]])\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 1.065644  [    0/ 3200]\n",
      "loss: 0.942238  [   16/ 3200]\n",
      "loss: 1.179616  [   32/ 3200]\n",
      "loss: 1.122257  [   48/ 3200]\n",
      "loss: 0.964722  [   64/ 3200]\n",
      "loss: 0.986230  [   80/ 3200]\n",
      "loss: 0.959881  [   96/ 3200]\n",
      "loss: 0.903798  [  112/ 3200]\n",
      "loss: 0.926167  [  128/ 3200]\n",
      "loss: 1.156642  [  144/ 3200]\n",
      "loss: 1.132031  [  160/ 3200]\n",
      "loss: 1.101682  [  176/ 3200]\n",
      "loss: 1.140434  [  192/ 3200]\n",
      "loss: 1.257274  [  208/ 3200]\n",
      "loss: 1.135915  [  224/ 3200]\n",
      "loss: 1.151377  [  240/ 3200]\n",
      "loss: 1.021106  [  256/ 3200]\n",
      "loss: 1.173727  [  272/ 3200]\n",
      "loss: 0.992734  [  288/ 3200]\n",
      "loss: 1.290881  [  304/ 3200]\n",
      "loss: 1.113816  [  320/ 3200]\n",
      "loss: 0.986497  [  336/ 3200]\n",
      "loss: 0.903796  [  352/ 3200]\n",
      "loss: 1.078125  [  368/ 3200]\n",
      "loss: 1.127127  [  384/ 3200]\n",
      "loss: 0.984958  [  400/ 3200]\n",
      "loss: 1.033409  [  416/ 3200]\n",
      "loss: 1.386211  [  432/ 3200]\n",
      "loss: 0.958938  [  448/ 3200]\n",
      "loss: 0.989113  [  464/ 3200]\n",
      "loss: 0.935392  [  480/ 3200]\n",
      "loss: 1.154974  [  496/ 3200]\n",
      "loss: 1.096758  [  512/ 3200]\n",
      "loss: 1.253697  [  528/ 3200]\n",
      "loss: 0.969194  [  544/ 3200]\n",
      "loss: 1.111562  [  560/ 3200]\n",
      "loss: 1.057176  [  576/ 3200]\n",
      "loss: 0.993495  [  592/ 3200]\n",
      "loss: 1.267246  [  608/ 3200]\n",
      "loss: 1.112113  [  624/ 3200]\n",
      "loss: 1.011087  [  640/ 3200]\n",
      "loss: 1.172575  [  656/ 3200]\n",
      "loss: 1.125984  [  672/ 3200]\n",
      "loss: 1.018308  [  688/ 3200]\n",
      "loss: 1.145717  [  704/ 3200]\n",
      "loss: 1.064316  [  720/ 3200]\n",
      "loss: 1.098296  [  736/ 3200]\n",
      "loss: 0.921058  [  752/ 3200]\n",
      "loss: 0.885607  [  768/ 3200]\n",
      "loss: 1.072411  [  784/ 3200]\n",
      "loss: 1.045609  [  800/ 3200]\n",
      "loss: 1.063167  [  816/ 3200]\n",
      "loss: 1.115215  [  832/ 3200]\n",
      "loss: 0.971144  [  848/ 3200]\n",
      "loss: 1.125030  [  864/ 3200]\n",
      "loss: 0.904255  [  880/ 3200]\n",
      "loss: 1.014378  [  896/ 3200]\n",
      "loss: 0.972721  [  912/ 3200]\n",
      "loss: 1.201720  [  928/ 3200]\n",
      "loss: 1.074915  [  944/ 3200]\n",
      "loss: 1.159376  [  960/ 3200]\n",
      "loss: 1.088959  [  976/ 3200]\n",
      "loss: 1.173338  [  992/ 3200]\n",
      "loss: 1.101464  [ 1008/ 3200]\n",
      "loss: 1.086300  [ 1024/ 3200]\n",
      "loss: 1.207093  [ 1040/ 3200]\n",
      "loss: 1.073583  [ 1056/ 3200]\n",
      "loss: 1.086405  [ 1072/ 3200]\n",
      "loss: 1.018489  [ 1088/ 3200]\n",
      "loss: 1.106386  [ 1104/ 3200]\n",
      "loss: 1.110629  [ 1120/ 3200]\n",
      "loss: 1.104028  [ 1136/ 3200]\n",
      "loss: 1.061691  [ 1152/ 3200]\n",
      "loss: 1.212763  [ 1168/ 3200]\n",
      "loss: 1.020541  [ 1184/ 3200]\n",
      "loss: 0.973890  [ 1200/ 3200]\n",
      "loss: 0.919342  [ 1216/ 3200]\n",
      "loss: 0.922095  [ 1232/ 3200]\n",
      "loss: 1.091672  [ 1248/ 3200]\n",
      "loss: 1.112040  [ 1264/ 3200]\n",
      "loss: 0.917282  [ 1280/ 3200]\n",
      "loss: 1.157418  [ 1296/ 3200]\n",
      "loss: 0.999588  [ 1312/ 3200]\n",
      "loss: 1.118798  [ 1328/ 3200]\n",
      "loss: 1.086619  [ 1344/ 3200]\n",
      "loss: 1.061929  [ 1360/ 3200]\n",
      "loss: 1.001612  [ 1376/ 3200]\n",
      "loss: 1.063288  [ 1392/ 3200]\n",
      "loss: 1.059632  [ 1408/ 3200]\n",
      "loss: 1.083724  [ 1424/ 3200]\n",
      "loss: 1.142460  [ 1440/ 3200]\n",
      "loss: 1.301471  [ 1456/ 3200]\n",
      "loss: 1.000776  [ 1472/ 3200]\n",
      "loss: 1.183869  [ 1488/ 3200]\n",
      "loss: 0.887105  [ 1504/ 3200]\n",
      "loss: 1.046729  [ 1520/ 3200]\n",
      "loss: 1.111536  [ 1536/ 3200]\n",
      "loss: 1.081804  [ 1552/ 3200]\n",
      "loss: 1.085799  [ 1568/ 3200]\n",
      "loss: 0.979230  [ 1584/ 3200]\n",
      "loss: 1.133468  [ 1600/ 3200]\n",
      "loss: 1.301417  [ 1616/ 3200]\n",
      "loss: 1.111709  [ 1632/ 3200]\n",
      "loss: 1.208241  [ 1648/ 3200]\n",
      "loss: 1.237422  [ 1664/ 3200]\n",
      "loss: 1.101982  [ 1680/ 3200]\n",
      "loss: 1.258416  [ 1696/ 3200]\n",
      "loss: 1.055099  [ 1712/ 3200]\n",
      "loss: 0.997727  [ 1728/ 3200]\n",
      "loss: 1.152046  [ 1744/ 3200]\n",
      "loss: 0.939500  [ 1760/ 3200]\n",
      "loss: 0.932046  [ 1776/ 3200]\n",
      "loss: 0.962987  [ 1792/ 3200]\n",
      "loss: 1.175573  [ 1808/ 3200]\n",
      "loss: 1.127649  [ 1824/ 3200]\n",
      "loss: 1.088772  [ 1840/ 3200]\n",
      "loss: 1.079115  [ 1856/ 3200]\n",
      "loss: 1.056524  [ 1872/ 3200]\n",
      "loss: 1.033988  [ 1888/ 3200]\n",
      "loss: 0.987744  [ 1904/ 3200]\n",
      "loss: 1.019078  [ 1920/ 3200]\n",
      "loss: 1.079909  [ 1936/ 3200]\n",
      "loss: 0.997060  [ 1952/ 3200]\n",
      "loss: 1.019081  [ 1968/ 3200]\n",
      "loss: 0.961240  [ 1984/ 3200]\n",
      "loss: 1.256978  [ 2000/ 3200]\n",
      "loss: 0.953297  [ 2016/ 3200]\n",
      "loss: 0.912346  [ 2032/ 3200]\n",
      "loss: 1.216807  [ 2048/ 3200]\n",
      "loss: 0.967185  [ 2064/ 3200]\n",
      "loss: 1.258383  [ 2080/ 3200]\n",
      "loss: 0.913493  [ 2096/ 3200]\n",
      "loss: 1.168108  [ 2112/ 3200]\n",
      "loss: 1.048534  [ 2128/ 3200]\n",
      "loss: 1.038129  [ 2144/ 3200]\n",
      "loss: 1.140986  [ 2160/ 3200]\n",
      "loss: 1.184999  [ 2176/ 3200]\n",
      "loss: 0.978184  [ 2192/ 3200]\n",
      "loss: 1.066939  [ 2208/ 3200]\n",
      "loss: 0.916030  [ 2224/ 3200]\n",
      "loss: 1.174563  [ 2240/ 3200]\n",
      "loss: 1.098232  [ 2256/ 3200]\n",
      "loss: 1.230635  [ 2272/ 3200]\n",
      "loss: 1.073944  [ 2288/ 3200]\n",
      "loss: 1.203143  [ 2304/ 3200]\n",
      "loss: 1.125134  [ 2320/ 3200]\n",
      "loss: 1.078106  [ 2336/ 3200]\n",
      "loss: 0.968696  [ 2352/ 3200]\n",
      "loss: 0.979575  [ 2368/ 3200]\n",
      "loss: 1.009995  [ 2384/ 3200]\n",
      "loss: 1.003968  [ 2400/ 3200]\n",
      "loss: 1.109314  [ 2416/ 3200]\n",
      "loss: 1.034738  [ 2432/ 3200]\n",
      "loss: 1.083019  [ 2448/ 3200]\n",
      "loss: 1.291942  [ 2464/ 3200]\n",
      "loss: 1.177264  [ 2480/ 3200]\n",
      "loss: 1.058471  [ 2496/ 3200]\n",
      "loss: 0.940406  [ 2512/ 3200]\n",
      "loss: 1.002854  [ 2528/ 3200]\n",
      "loss: 1.107435  [ 2544/ 3200]\n",
      "loss: 0.894039  [ 2560/ 3200]\n",
      "loss: 1.148522  [ 2576/ 3200]\n",
      "loss: 1.110393  [ 2592/ 3200]\n",
      "loss: 1.230701  [ 2608/ 3200]\n",
      "loss: 1.000031  [ 2624/ 3200]\n",
      "loss: 0.992131  [ 2640/ 3200]\n",
      "loss: 1.001428  [ 2656/ 3200]\n",
      "loss: 1.030526  [ 2672/ 3200]\n",
      "loss: 1.116948  [ 2688/ 3200]\n",
      "loss: 1.088803  [ 2704/ 3200]\n",
      "loss: 1.398792  [ 2720/ 3200]\n",
      "loss: 0.996370  [ 2736/ 3200]\n",
      "loss: 1.059939  [ 2752/ 3200]\n",
      "loss: 0.873789  [ 2768/ 3200]\n",
      "loss: 0.941674  [ 2784/ 3200]\n",
      "loss: 1.043852  [ 2800/ 3200]\n",
      "loss: 1.017716  [ 2816/ 3200]\n",
      "loss: 1.121167  [ 2832/ 3200]\n",
      "loss: 1.066946  [ 2848/ 3200]\n",
      "loss: 1.146046  [ 2864/ 3200]\n",
      "loss: 1.231304  [ 2880/ 3200]\n",
      "loss: 0.984841  [ 2896/ 3200]\n",
      "loss: 1.209429  [ 2912/ 3200]\n",
      "loss: 1.106916  [ 2928/ 3200]\n",
      "loss: 1.095463  [ 2944/ 3200]\n",
      "loss: 0.946759  [ 2960/ 3200]\n",
      "loss: 0.998847  [ 2976/ 3200]\n",
      "loss: 0.938836  [ 2992/ 3200]\n",
      "loss: 1.011595  [ 3008/ 3200]\n",
      "loss: 0.975573  [ 3024/ 3200]\n",
      "loss: 1.123428  [ 3040/ 3200]\n",
      "loss: 1.199645  [ 3056/ 3200]\n",
      "loss: 1.075518  [ 3072/ 3200]\n",
      "loss: 1.055404  [ 3088/ 3200]\n",
      "loss: 1.182298  [ 3104/ 3200]\n",
      "loss: 0.987612  [ 3120/ 3200]\n",
      "loss: 1.249279  [ 3136/ 3200]\n",
      "loss: 0.856476  [ 3152/ 3200]\n",
      "loss: 1.055580  [ 3168/ 3200]\n",
      "loss: 0.939555  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.064986\n",
      "f1 macro averaged score: 0.577832\n",
      "Accuracy               : 61.6%\n",
      "Confusion matrix       :\n",
      "tensor([[180,   4,   3,  13],\n",
      "        [ 53,  39,  30,  78],\n",
      "        [  8,  29,  94,  69],\n",
      "        [ 11,   3,   6, 180]])\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 1.068657  [    0/ 3200]\n",
      "loss: 0.850719  [   16/ 3200]\n",
      "loss: 1.309345  [   32/ 3200]\n",
      "loss: 1.019665  [   48/ 3200]\n",
      "loss: 0.933075  [   64/ 3200]\n",
      "loss: 1.176084  [   80/ 3200]\n",
      "loss: 1.033601  [   96/ 3200]\n",
      "loss: 1.241957  [  112/ 3200]\n",
      "loss: 0.988964  [  128/ 3200]\n",
      "loss: 0.989422  [  144/ 3200]\n",
      "loss: 1.065002  [  160/ 3200]\n",
      "loss: 1.005239  [  176/ 3200]\n",
      "loss: 1.131725  [  192/ 3200]\n",
      "loss: 1.074739  [  208/ 3200]\n",
      "loss: 0.832189  [  224/ 3200]\n",
      "loss: 1.040674  [  240/ 3200]\n",
      "loss: 0.903467  [  256/ 3200]\n",
      "loss: 0.887927  [  272/ 3200]\n",
      "loss: 1.102992  [  288/ 3200]\n",
      "loss: 1.099980  [  304/ 3200]\n",
      "loss: 0.950640  [  320/ 3200]\n",
      "loss: 1.209197  [  336/ 3200]\n",
      "loss: 1.159579  [  352/ 3200]\n",
      "loss: 0.972358  [  368/ 3200]\n",
      "loss: 0.994461  [  384/ 3200]\n",
      "loss: 1.053042  [  400/ 3200]\n",
      "loss: 1.193779  [  416/ 3200]\n",
      "loss: 1.342828  [  432/ 3200]\n",
      "loss: 1.091265  [  448/ 3200]\n",
      "loss: 1.001653  [  464/ 3200]\n",
      "loss: 0.964319  [  480/ 3200]\n",
      "loss: 1.009593  [  496/ 3200]\n",
      "loss: 0.971683  [  512/ 3200]\n",
      "loss: 1.244536  [  528/ 3200]\n",
      "loss: 1.065112  [  544/ 3200]\n",
      "loss: 0.994898  [  560/ 3200]\n",
      "loss: 1.011705  [  576/ 3200]\n",
      "loss: 0.990298  [  592/ 3200]\n",
      "loss: 0.949521  [  608/ 3200]\n",
      "loss: 1.036972  [  624/ 3200]\n",
      "loss: 0.983559  [  640/ 3200]\n",
      "loss: 1.111605  [  656/ 3200]\n",
      "loss: 1.018800  [  672/ 3200]\n",
      "loss: 1.080629  [  688/ 3200]\n",
      "loss: 1.160198  [  704/ 3200]\n",
      "loss: 1.054218  [  720/ 3200]\n",
      "loss: 1.042076  [  736/ 3200]\n",
      "loss: 1.191540  [  752/ 3200]\n",
      "loss: 1.061814  [  768/ 3200]\n",
      "loss: 1.203402  [  784/ 3200]\n",
      "loss: 0.952269  [  800/ 3200]\n",
      "loss: 1.194055  [  816/ 3200]\n",
      "loss: 1.191336  [  832/ 3200]\n",
      "loss: 1.076700  [  848/ 3200]\n",
      "loss: 1.122442  [  864/ 3200]\n",
      "loss: 1.004152  [  880/ 3200]\n",
      "loss: 0.905498  [  896/ 3200]\n",
      "loss: 0.626111  [  912/ 3200]\n",
      "loss: 1.019765  [  928/ 3200]\n",
      "loss: 1.116430  [  944/ 3200]\n",
      "loss: 1.166929  [  960/ 3200]\n",
      "loss: 1.095833  [  976/ 3200]\n",
      "loss: 1.159783  [  992/ 3200]\n",
      "loss: 1.175647  [ 1008/ 3200]\n",
      "loss: 1.123969  [ 1024/ 3200]\n",
      "loss: 1.136960  [ 1040/ 3200]\n",
      "loss: 0.941943  [ 1056/ 3200]\n",
      "loss: 1.255744  [ 1072/ 3200]\n",
      "loss: 1.009361  [ 1088/ 3200]\n",
      "loss: 1.057276  [ 1104/ 3200]\n",
      "loss: 0.896317  [ 1120/ 3200]\n",
      "loss: 0.892717  [ 1136/ 3200]\n",
      "loss: 1.243808  [ 1152/ 3200]\n",
      "loss: 1.013193  [ 1168/ 3200]\n",
      "loss: 0.901259  [ 1184/ 3200]\n",
      "loss: 0.862377  [ 1200/ 3200]\n",
      "loss: 0.911937  [ 1216/ 3200]\n",
      "loss: 1.222787  [ 1232/ 3200]\n",
      "loss: 1.158792  [ 1248/ 3200]\n",
      "loss: 1.188757  [ 1264/ 3200]\n",
      "loss: 1.137156  [ 1280/ 3200]\n",
      "loss: 0.998539  [ 1296/ 3200]\n",
      "loss: 1.005783  [ 1312/ 3200]\n",
      "loss: 1.081850  [ 1328/ 3200]\n",
      "loss: 1.116835  [ 1344/ 3200]\n",
      "loss: 1.160432  [ 1360/ 3200]\n",
      "loss: 1.160161  [ 1376/ 3200]\n",
      "loss: 1.119237  [ 1392/ 3200]\n",
      "loss: 1.055128  [ 1408/ 3200]\n",
      "loss: 1.095221  [ 1424/ 3200]\n",
      "loss: 0.822699  [ 1440/ 3200]\n",
      "loss: 0.998369  [ 1456/ 3200]\n",
      "loss: 1.084366  [ 1472/ 3200]\n",
      "loss: 1.152908  [ 1488/ 3200]\n",
      "loss: 0.840182  [ 1504/ 3200]\n",
      "loss: 0.990402  [ 1520/ 3200]\n",
      "loss: 1.060641  [ 1536/ 3200]\n",
      "loss: 1.152951  [ 1552/ 3200]\n",
      "loss: 0.972827  [ 1568/ 3200]\n",
      "loss: 1.171562  [ 1584/ 3200]\n",
      "loss: 1.231123  [ 1600/ 3200]\n",
      "loss: 0.979262  [ 1616/ 3200]\n",
      "loss: 1.050264  [ 1632/ 3200]\n",
      "loss: 1.052527  [ 1648/ 3200]\n",
      "loss: 0.947734  [ 1664/ 3200]\n",
      "loss: 1.268886  [ 1680/ 3200]\n",
      "loss: 1.304832  [ 1696/ 3200]\n",
      "loss: 1.088000  [ 1712/ 3200]\n",
      "loss: 1.126721  [ 1728/ 3200]\n",
      "loss: 0.872715  [ 1744/ 3200]\n",
      "loss: 1.013152  [ 1760/ 3200]\n",
      "loss: 1.046070  [ 1776/ 3200]\n",
      "loss: 1.150802  [ 1792/ 3200]\n",
      "loss: 1.163733  [ 1808/ 3200]\n",
      "loss: 1.083397  [ 1824/ 3200]\n",
      "loss: 0.967304  [ 1840/ 3200]\n",
      "loss: 1.096757  [ 1856/ 3200]\n",
      "loss: 0.937373  [ 1872/ 3200]\n",
      "loss: 1.032919  [ 1888/ 3200]\n",
      "loss: 1.015120  [ 1904/ 3200]\n",
      "loss: 1.090774  [ 1920/ 3200]\n",
      "loss: 1.007903  [ 1936/ 3200]\n",
      "loss: 0.980829  [ 1952/ 3200]\n",
      "loss: 1.170366  [ 1968/ 3200]\n",
      "loss: 1.119085  [ 1984/ 3200]\n",
      "loss: 1.048705  [ 2000/ 3200]\n",
      "loss: 1.113237  [ 2016/ 3200]\n",
      "loss: 1.123339  [ 2032/ 3200]\n",
      "loss: 1.063581  [ 2048/ 3200]\n",
      "loss: 0.918825  [ 2064/ 3200]\n",
      "loss: 1.257302  [ 2080/ 3200]\n",
      "loss: 0.994308  [ 2096/ 3200]\n",
      "loss: 1.180575  [ 2112/ 3200]\n",
      "loss: 1.311861  [ 2128/ 3200]\n",
      "loss: 0.941062  [ 2144/ 3200]\n",
      "loss: 1.068080  [ 2160/ 3200]\n",
      "loss: 1.023986  [ 2176/ 3200]\n",
      "loss: 1.000731  [ 2192/ 3200]\n",
      "loss: 1.210742  [ 2208/ 3200]\n",
      "loss: 1.032292  [ 2224/ 3200]\n",
      "loss: 1.362067  [ 2240/ 3200]\n",
      "loss: 0.885557  [ 2256/ 3200]\n",
      "loss: 0.989838  [ 2272/ 3200]\n",
      "loss: 1.120338  [ 2288/ 3200]\n",
      "loss: 0.967932  [ 2304/ 3200]\n",
      "loss: 1.010417  [ 2320/ 3200]\n",
      "loss: 1.069867  [ 2336/ 3200]\n",
      "loss: 1.201235  [ 2352/ 3200]\n",
      "loss: 1.023376  [ 2368/ 3200]\n",
      "loss: 1.114534  [ 2384/ 3200]\n",
      "loss: 0.924257  [ 2400/ 3200]\n",
      "loss: 1.123281  [ 2416/ 3200]\n",
      "loss: 0.972668  [ 2432/ 3200]\n",
      "loss: 0.942149  [ 2448/ 3200]\n",
      "loss: 1.195300  [ 2464/ 3200]\n",
      "loss: 1.214660  [ 2480/ 3200]\n",
      "loss: 0.956527  [ 2496/ 3200]\n",
      "loss: 1.014694  [ 2512/ 3200]\n",
      "loss: 1.123523  [ 2528/ 3200]\n",
      "loss: 0.900809  [ 2544/ 3200]\n",
      "loss: 0.999932  [ 2560/ 3200]\n",
      "loss: 0.902762  [ 2576/ 3200]\n",
      "loss: 1.065889  [ 2592/ 3200]\n",
      "loss: 0.788507  [ 2608/ 3200]\n",
      "loss: 1.082695  [ 2624/ 3200]\n",
      "loss: 1.253363  [ 2640/ 3200]\n",
      "loss: 1.077666  [ 2656/ 3200]\n",
      "loss: 0.920461  [ 2672/ 3200]\n",
      "loss: 1.280247  [ 2688/ 3200]\n",
      "loss: 1.037789  [ 2704/ 3200]\n",
      "loss: 1.152691  [ 2720/ 3200]\n",
      "loss: 0.919842  [ 2736/ 3200]\n",
      "loss: 1.253403  [ 2752/ 3200]\n",
      "loss: 0.947236  [ 2768/ 3200]\n",
      "loss: 0.842721  [ 2784/ 3200]\n",
      "loss: 1.091967  [ 2800/ 3200]\n",
      "loss: 1.058576  [ 2816/ 3200]\n",
      "loss: 1.155459  [ 2832/ 3200]\n",
      "loss: 1.159632  [ 2848/ 3200]\n",
      "loss: 0.999219  [ 2864/ 3200]\n",
      "loss: 1.232562  [ 2880/ 3200]\n",
      "loss: 1.049515  [ 2896/ 3200]\n",
      "loss: 0.899358  [ 2912/ 3200]\n",
      "loss: 0.929307  [ 2928/ 3200]\n",
      "loss: 1.207947  [ 2944/ 3200]\n",
      "loss: 0.924725  [ 2960/ 3200]\n",
      "loss: 1.258803  [ 2976/ 3200]\n",
      "loss: 0.886732  [ 2992/ 3200]\n",
      "loss: 0.965509  [ 3008/ 3200]\n",
      "loss: 1.066188  [ 3024/ 3200]\n",
      "loss: 0.965538  [ 3040/ 3200]\n",
      "loss: 0.969144  [ 3056/ 3200]\n",
      "loss: 0.990003  [ 3072/ 3200]\n",
      "loss: 1.158500  [ 3088/ 3200]\n",
      "loss: 1.160209  [ 3104/ 3200]\n",
      "loss: 0.935908  [ 3120/ 3200]\n",
      "loss: 0.961397  [ 3136/ 3200]\n",
      "loss: 1.056340  [ 3152/ 3200]\n",
      "loss: 1.089838  [ 3168/ 3200]\n",
      "loss: 0.917589  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.065483\n",
      "f1 macro averaged score: 0.553696\n",
      "Accuracy               : 56.8%\n",
      "Confusion matrix       :\n",
      "tensor([[126,  59,  14,   1],\n",
      "        [ 22,  31, 144,   3],\n",
      "        [  1,   7, 191,   1],\n",
      "        [  3,  33,  58, 106]])\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 1.199638  [    0/ 3200]\n",
      "loss: 1.056360  [   16/ 3200]\n",
      "loss: 1.120401  [   32/ 3200]\n",
      "loss: 0.940504  [   48/ 3200]\n",
      "loss: 0.913886  [   64/ 3200]\n",
      "loss: 1.151742  [   80/ 3200]\n",
      "loss: 1.097903  [   96/ 3200]\n",
      "loss: 1.023401  [  112/ 3200]\n",
      "loss: 1.081494  [  128/ 3200]\n",
      "loss: 0.837031  [  144/ 3200]\n",
      "loss: 1.154871  [  160/ 3200]\n",
      "loss: 1.128752  [  176/ 3200]\n",
      "loss: 0.829754  [  192/ 3200]\n",
      "loss: 0.893830  [  208/ 3200]\n",
      "loss: 0.990858  [  224/ 3200]\n",
      "loss: 0.983797  [  240/ 3200]\n",
      "loss: 1.224743  [  256/ 3200]\n",
      "loss: 1.298509  [  272/ 3200]\n",
      "loss: 1.152689  [  288/ 3200]\n",
      "loss: 1.186538  [  304/ 3200]\n",
      "loss: 0.932636  [  320/ 3200]\n",
      "loss: 1.052333  [  336/ 3200]\n",
      "loss: 1.046837  [  352/ 3200]\n",
      "loss: 1.026688  [  368/ 3200]\n",
      "loss: 1.020136  [  384/ 3200]\n",
      "loss: 1.191271  [  400/ 3200]\n",
      "loss: 1.067400  [  416/ 3200]\n",
      "loss: 1.108894  [  432/ 3200]\n",
      "loss: 1.168276  [  448/ 3200]\n",
      "loss: 1.025086  [  464/ 3200]\n",
      "loss: 1.115780  [  480/ 3200]\n",
      "loss: 1.147705  [  496/ 3200]\n",
      "loss: 1.090457  [  512/ 3200]\n",
      "loss: 1.031646  [  528/ 3200]\n",
      "loss: 0.769904  [  544/ 3200]\n",
      "loss: 1.008746  [  560/ 3200]\n",
      "loss: 1.048797  [  576/ 3200]\n",
      "loss: 0.872770  [  592/ 3200]\n",
      "loss: 0.933619  [  608/ 3200]\n",
      "loss: 1.062608  [  624/ 3200]\n",
      "loss: 1.153490  [  640/ 3200]\n",
      "loss: 1.079280  [  656/ 3200]\n",
      "loss: 1.171431  [  672/ 3200]\n",
      "loss: 0.948374  [  688/ 3200]\n",
      "loss: 1.129395  [  704/ 3200]\n",
      "loss: 1.066590  [  720/ 3200]\n",
      "loss: 0.949217  [  736/ 3200]\n",
      "loss: 1.214893  [  752/ 3200]\n",
      "loss: 1.016035  [  768/ 3200]\n",
      "loss: 1.015687  [  784/ 3200]\n",
      "loss: 0.957762  [  800/ 3200]\n",
      "loss: 1.110150  [  816/ 3200]\n",
      "loss: 1.032640  [  832/ 3200]\n",
      "loss: 1.000213  [  848/ 3200]\n",
      "loss: 0.973014  [  864/ 3200]\n",
      "loss: 1.131112  [  880/ 3200]\n",
      "loss: 0.929050  [  896/ 3200]\n",
      "loss: 1.205336  [  912/ 3200]\n",
      "loss: 0.953891  [  928/ 3200]\n",
      "loss: 1.077134  [  944/ 3200]\n",
      "loss: 1.420884  [  960/ 3200]\n",
      "loss: 0.976936  [  976/ 3200]\n",
      "loss: 0.867349  [  992/ 3200]\n",
      "loss: 0.895972  [ 1008/ 3200]\n",
      "loss: 1.006819  [ 1024/ 3200]\n",
      "loss: 0.827459  [ 1040/ 3200]\n",
      "loss: 1.114439  [ 1056/ 3200]\n",
      "loss: 0.948227  [ 1072/ 3200]\n",
      "loss: 1.108833  [ 1088/ 3200]\n",
      "loss: 1.212320  [ 1104/ 3200]\n",
      "loss: 1.047098  [ 1120/ 3200]\n",
      "loss: 1.039134  [ 1136/ 3200]\n",
      "loss: 1.206196  [ 1152/ 3200]\n",
      "loss: 0.958826  [ 1168/ 3200]\n",
      "loss: 1.004391  [ 1184/ 3200]\n",
      "loss: 1.068770  [ 1200/ 3200]\n",
      "loss: 0.928824  [ 1216/ 3200]\n",
      "loss: 0.892237  [ 1232/ 3200]\n",
      "loss: 0.860402  [ 1248/ 3200]\n",
      "loss: 1.189583  [ 1264/ 3200]\n",
      "loss: 1.046753  [ 1280/ 3200]\n",
      "loss: 1.061345  [ 1296/ 3200]\n",
      "loss: 1.084852  [ 1312/ 3200]\n",
      "loss: 0.904788  [ 1328/ 3200]\n",
      "loss: 1.272270  [ 1344/ 3200]\n",
      "loss: 1.220616  [ 1360/ 3200]\n",
      "loss: 1.284916  [ 1376/ 3200]\n",
      "loss: 1.095199  [ 1392/ 3200]\n",
      "loss: 1.030874  [ 1408/ 3200]\n",
      "loss: 1.297752  [ 1424/ 3200]\n",
      "loss: 1.194879  [ 1440/ 3200]\n",
      "loss: 1.055754  [ 1456/ 3200]\n",
      "loss: 0.887385  [ 1472/ 3200]\n",
      "loss: 1.102758  [ 1488/ 3200]\n",
      "loss: 1.124244  [ 1504/ 3200]\n",
      "loss: 1.009602  [ 1520/ 3200]\n",
      "loss: 1.261325  [ 1536/ 3200]\n",
      "loss: 1.029626  [ 1552/ 3200]\n",
      "loss: 0.939960  [ 1568/ 3200]\n",
      "loss: 0.983365  [ 1584/ 3200]\n",
      "loss: 0.968269  [ 1600/ 3200]\n",
      "loss: 0.991180  [ 1616/ 3200]\n",
      "loss: 1.019740  [ 1632/ 3200]\n",
      "loss: 0.995717  [ 1648/ 3200]\n",
      "loss: 1.005901  [ 1664/ 3200]\n",
      "loss: 1.129068  [ 1680/ 3200]\n",
      "loss: 0.915582  [ 1696/ 3200]\n",
      "loss: 0.998962  [ 1712/ 3200]\n",
      "loss: 1.108330  [ 1728/ 3200]\n",
      "loss: 1.073998  [ 1744/ 3200]\n",
      "loss: 1.198022  [ 1760/ 3200]\n",
      "loss: 0.935331  [ 1776/ 3200]\n",
      "loss: 1.031083  [ 1792/ 3200]\n",
      "loss: 0.935010  [ 1808/ 3200]\n",
      "loss: 0.917744  [ 1824/ 3200]\n",
      "loss: 1.052001  [ 1840/ 3200]\n",
      "loss: 0.908896  [ 1856/ 3200]\n",
      "loss: 1.004751  [ 1872/ 3200]\n",
      "loss: 0.826188  [ 1888/ 3200]\n",
      "loss: 1.035856  [ 1904/ 3200]\n",
      "loss: 1.033237  [ 1920/ 3200]\n",
      "loss: 1.040743  [ 1936/ 3200]\n",
      "loss: 1.374361  [ 1952/ 3200]\n",
      "loss: 1.053651  [ 1968/ 3200]\n",
      "loss: 1.098468  [ 1984/ 3200]\n",
      "loss: 1.161853  [ 2000/ 3200]\n",
      "loss: 0.987850  [ 2016/ 3200]\n",
      "loss: 0.856810  [ 2032/ 3200]\n",
      "loss: 1.075388  [ 2048/ 3200]\n",
      "loss: 1.003119  [ 2064/ 3200]\n",
      "loss: 0.980575  [ 2080/ 3200]\n",
      "loss: 0.983140  [ 2096/ 3200]\n",
      "loss: 1.062655  [ 2112/ 3200]\n",
      "loss: 1.207657  [ 2128/ 3200]\n",
      "loss: 0.899788  [ 2144/ 3200]\n",
      "loss: 1.267787  [ 2160/ 3200]\n",
      "loss: 0.986974  [ 2176/ 3200]\n",
      "loss: 1.288273  [ 2192/ 3200]\n",
      "loss: 0.972158  [ 2208/ 3200]\n",
      "loss: 1.116669  [ 2224/ 3200]\n",
      "loss: 0.787835  [ 2240/ 3200]\n",
      "loss: 0.945571  [ 2256/ 3200]\n",
      "loss: 1.024685  [ 2272/ 3200]\n",
      "loss: 1.012876  [ 2288/ 3200]\n",
      "loss: 0.870590  [ 2304/ 3200]\n",
      "loss: 1.025794  [ 2320/ 3200]\n",
      "loss: 0.836251  [ 2336/ 3200]\n",
      "loss: 1.076410  [ 2352/ 3200]\n",
      "loss: 1.244131  [ 2368/ 3200]\n",
      "loss: 1.034370  [ 2384/ 3200]\n",
      "loss: 1.178760  [ 2400/ 3200]\n",
      "loss: 1.159006  [ 2416/ 3200]\n",
      "loss: 1.025410  [ 2432/ 3200]\n",
      "loss: 1.051559  [ 2448/ 3200]\n",
      "loss: 1.042162  [ 2464/ 3200]\n",
      "loss: 1.233552  [ 2480/ 3200]\n",
      "loss: 0.919134  [ 2496/ 3200]\n",
      "loss: 1.315936  [ 2512/ 3200]\n",
      "loss: 0.962340  [ 2528/ 3200]\n",
      "loss: 1.008918  [ 2544/ 3200]\n",
      "loss: 1.084505  [ 2560/ 3200]\n",
      "loss: 0.830218  [ 2576/ 3200]\n",
      "loss: 0.780542  [ 2592/ 3200]\n",
      "loss: 0.861294  [ 2608/ 3200]\n",
      "loss: 1.081007  [ 2624/ 3200]\n",
      "loss: 1.439706  [ 2640/ 3200]\n",
      "loss: 0.964676  [ 2656/ 3200]\n",
      "loss: 0.877654  [ 2672/ 3200]\n",
      "loss: 0.990221  [ 2688/ 3200]\n",
      "loss: 1.053606  [ 2704/ 3200]\n",
      "loss: 1.119886  [ 2720/ 3200]\n",
      "loss: 1.059874  [ 2736/ 3200]\n",
      "loss: 1.056375  [ 2752/ 3200]\n",
      "loss: 1.084026  [ 2768/ 3200]\n",
      "loss: 1.105518  [ 2784/ 3200]\n",
      "loss: 1.221705  [ 2800/ 3200]\n",
      "loss: 1.082072  [ 2816/ 3200]\n",
      "loss: 1.040415  [ 2832/ 3200]\n",
      "loss: 1.116109  [ 2848/ 3200]\n",
      "loss: 1.012207  [ 2864/ 3200]\n",
      "loss: 0.949339  [ 2880/ 3200]\n",
      "loss: 0.967784  [ 2896/ 3200]\n",
      "loss: 1.141508  [ 2912/ 3200]\n",
      "loss: 0.956311  [ 2928/ 3200]\n",
      "loss: 1.073362  [ 2944/ 3200]\n",
      "loss: 1.021453  [ 2960/ 3200]\n",
      "loss: 1.172337  [ 2976/ 3200]\n",
      "loss: 1.120741  [ 2992/ 3200]\n",
      "loss: 1.203644  [ 3008/ 3200]\n",
      "loss: 0.981002  [ 3024/ 3200]\n",
      "loss: 1.115357  [ 3040/ 3200]\n",
      "loss: 1.259721  [ 3056/ 3200]\n",
      "loss: 0.954825  [ 3072/ 3200]\n",
      "loss: 1.011814  [ 3088/ 3200]\n",
      "loss: 0.979116  [ 3104/ 3200]\n",
      "loss: 0.883494  [ 3120/ 3200]\n",
      "loss: 1.198389  [ 3136/ 3200]\n",
      "loss: 1.181258  [ 3152/ 3200]\n",
      "loss: 0.911965  [ 3168/ 3200]\n",
      "loss: 1.042993  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.063887\n",
      "f1 macro averaged score: 0.579321\n",
      "Accuracy               : 60.6%\n",
      "Confusion matrix       :\n",
      "tensor([[151,  12,   5,  32],\n",
      "        [ 27,  40,  41,  92],\n",
      "        [  2,  20, 110,  68],\n",
      "        [  7,   2,   7, 184]])\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.860545  [    0/ 3200]\n",
      "loss: 0.988807  [   16/ 3200]\n",
      "loss: 0.923660  [   32/ 3200]\n",
      "loss: 0.968234  [   48/ 3200]\n",
      "loss: 0.977442  [   64/ 3200]\n",
      "loss: 1.106348  [   80/ 3200]\n",
      "loss: 1.072090  [   96/ 3200]\n",
      "loss: 0.979868  [  112/ 3200]\n",
      "loss: 0.864722  [  128/ 3200]\n",
      "loss: 1.152561  [  144/ 3200]\n",
      "loss: 0.973233  [  160/ 3200]\n",
      "loss: 0.921716  [  176/ 3200]\n",
      "loss: 1.143002  [  192/ 3200]\n",
      "loss: 1.081692  [  208/ 3200]\n",
      "loss: 1.056512  [  224/ 3200]\n",
      "loss: 1.131909  [  240/ 3200]\n",
      "loss: 1.214722  [  256/ 3200]\n",
      "loss: 1.090941  [  272/ 3200]\n",
      "loss: 0.965479  [  288/ 3200]\n",
      "loss: 0.971217  [  304/ 3200]\n",
      "loss: 1.029821  [  320/ 3200]\n",
      "loss: 0.749758  [  336/ 3200]\n",
      "loss: 1.202357  [  352/ 3200]\n",
      "loss: 1.251872  [  368/ 3200]\n",
      "loss: 0.900748  [  384/ 3200]\n",
      "loss: 1.048296  [  400/ 3200]\n",
      "loss: 1.084820  [  416/ 3200]\n",
      "loss: 0.969704  [  432/ 3200]\n",
      "loss: 0.898207  [  448/ 3200]\n",
      "loss: 1.031676  [  464/ 3200]\n",
      "loss: 0.878860  [  480/ 3200]\n",
      "loss: 1.169790  [  496/ 3200]\n",
      "loss: 1.251925  [  512/ 3200]\n",
      "loss: 0.718334  [  528/ 3200]\n",
      "loss: 1.326885  [  544/ 3200]\n",
      "loss: 0.927942  [  560/ 3200]\n",
      "loss: 0.979178  [  576/ 3200]\n",
      "loss: 0.939326  [  592/ 3200]\n",
      "loss: 0.982065  [  608/ 3200]\n",
      "loss: 0.892496  [  624/ 3200]\n",
      "loss: 0.690126  [  640/ 3200]\n",
      "loss: 0.944433  [  656/ 3200]\n",
      "loss: 1.177311  [  672/ 3200]\n",
      "loss: 1.157476  [  688/ 3200]\n",
      "loss: 1.066559  [  704/ 3200]\n",
      "loss: 1.174812  [  720/ 3200]\n",
      "loss: 0.954089  [  736/ 3200]\n",
      "loss: 1.209197  [  752/ 3200]\n",
      "loss: 0.989953  [  768/ 3200]\n",
      "loss: 0.727412  [  784/ 3200]\n",
      "loss: 0.904799  [  800/ 3200]\n",
      "loss: 1.138069  [  816/ 3200]\n",
      "loss: 0.911735  [  832/ 3200]\n",
      "loss: 1.232991  [  848/ 3200]\n",
      "loss: 1.178101  [  864/ 3200]\n",
      "loss: 1.204491  [  880/ 3200]\n",
      "loss: 1.014706  [  896/ 3200]\n",
      "loss: 1.080259  [  912/ 3200]\n",
      "loss: 1.119306  [  928/ 3200]\n",
      "loss: 1.055180  [  944/ 3200]\n",
      "loss: 1.068007  [  960/ 3200]\n",
      "loss: 0.982673  [  976/ 3200]\n",
      "loss: 1.305384  [  992/ 3200]\n",
      "loss: 1.063391  [ 1008/ 3200]\n",
      "loss: 0.868206  [ 1024/ 3200]\n",
      "loss: 1.179380  [ 1040/ 3200]\n",
      "loss: 0.978140  [ 1056/ 3200]\n",
      "loss: 0.834439  [ 1072/ 3200]\n",
      "loss: 0.937382  [ 1088/ 3200]\n",
      "loss: 1.085626  [ 1104/ 3200]\n",
      "loss: 1.007935  [ 1120/ 3200]\n",
      "loss: 1.161458  [ 1136/ 3200]\n",
      "loss: 1.373570  [ 1152/ 3200]\n",
      "loss: 1.276080  [ 1168/ 3200]\n",
      "loss: 0.801623  [ 1184/ 3200]\n",
      "loss: 0.845815  [ 1200/ 3200]\n",
      "loss: 1.005580  [ 1216/ 3200]\n",
      "loss: 0.921108  [ 1232/ 3200]\n",
      "loss: 0.860837  [ 1248/ 3200]\n",
      "loss: 1.200541  [ 1264/ 3200]\n",
      "loss: 1.097373  [ 1280/ 3200]\n",
      "loss: 1.053138  [ 1296/ 3200]\n",
      "loss: 1.093925  [ 1312/ 3200]\n",
      "loss: 1.002812  [ 1328/ 3200]\n",
      "loss: 0.916969  [ 1344/ 3200]\n",
      "loss: 0.897800  [ 1360/ 3200]\n",
      "loss: 1.138566  [ 1376/ 3200]\n",
      "loss: 0.861874  [ 1392/ 3200]\n",
      "loss: 1.020006  [ 1408/ 3200]\n",
      "loss: 1.023783  [ 1424/ 3200]\n",
      "loss: 1.042250  [ 1440/ 3200]\n",
      "loss: 0.772942  [ 1456/ 3200]\n",
      "loss: 0.781075  [ 1472/ 3200]\n",
      "loss: 1.173481  [ 1488/ 3200]\n",
      "loss: 1.051626  [ 1504/ 3200]\n",
      "loss: 1.136802  [ 1520/ 3200]\n",
      "loss: 1.097072  [ 1536/ 3200]\n",
      "loss: 0.971926  [ 1552/ 3200]\n",
      "loss: 1.062139  [ 1568/ 3200]\n",
      "loss: 0.877589  [ 1584/ 3200]\n",
      "loss: 1.224416  [ 1600/ 3200]\n",
      "loss: 0.886943  [ 1616/ 3200]\n",
      "loss: 1.045172  [ 1632/ 3200]\n",
      "loss: 0.999684  [ 1648/ 3200]\n",
      "loss: 1.005383  [ 1664/ 3200]\n",
      "loss: 0.882301  [ 1680/ 3200]\n",
      "loss: 0.968723  [ 1696/ 3200]\n",
      "loss: 0.944935  [ 1712/ 3200]\n",
      "loss: 0.858341  [ 1728/ 3200]\n",
      "loss: 0.861638  [ 1744/ 3200]\n",
      "loss: 1.115123  [ 1760/ 3200]\n",
      "loss: 1.027216  [ 1776/ 3200]\n",
      "loss: 1.129380  [ 1792/ 3200]\n",
      "loss: 0.914450  [ 1808/ 3200]\n",
      "loss: 1.201897  [ 1824/ 3200]\n",
      "loss: 0.982638  [ 1840/ 3200]\n",
      "loss: 0.881776  [ 1856/ 3200]\n",
      "loss: 1.086733  [ 1872/ 3200]\n",
      "loss: 1.235871  [ 1888/ 3200]\n",
      "loss: 1.152616  [ 1904/ 3200]\n",
      "loss: 1.077220  [ 1920/ 3200]\n",
      "loss: 1.306145  [ 1936/ 3200]\n",
      "loss: 1.052057  [ 1952/ 3200]\n",
      "loss: 1.076887  [ 1968/ 3200]\n",
      "loss: 1.117329  [ 1984/ 3200]\n",
      "loss: 1.013421  [ 2000/ 3200]\n",
      "loss: 1.073469  [ 2016/ 3200]\n",
      "loss: 0.957835  [ 2032/ 3200]\n",
      "loss: 0.840761  [ 2048/ 3200]\n",
      "loss: 0.760823  [ 2064/ 3200]\n",
      "loss: 0.939461  [ 2080/ 3200]\n",
      "loss: 1.244852  [ 2096/ 3200]\n",
      "loss: 1.032903  [ 2112/ 3200]\n",
      "loss: 1.227651  [ 2128/ 3200]\n",
      "loss: 1.271131  [ 2144/ 3200]\n",
      "loss: 1.113511  [ 2160/ 3200]\n",
      "loss: 1.115031  [ 2176/ 3200]\n",
      "loss: 1.069527  [ 2192/ 3200]\n",
      "loss: 1.184853  [ 2208/ 3200]\n",
      "loss: 1.026884  [ 2224/ 3200]\n",
      "loss: 1.144166  [ 2240/ 3200]\n",
      "loss: 1.068460  [ 2256/ 3200]\n",
      "loss: 1.196875  [ 2272/ 3200]\n",
      "loss: 1.317653  [ 2288/ 3200]\n",
      "loss: 1.127432  [ 2304/ 3200]\n",
      "loss: 1.104498  [ 2320/ 3200]\n",
      "loss: 0.922046  [ 2336/ 3200]\n",
      "loss: 1.056336  [ 2352/ 3200]\n",
      "loss: 1.234016  [ 2368/ 3200]\n",
      "loss: 1.251786  [ 2384/ 3200]\n",
      "loss: 1.160260  [ 2400/ 3200]\n",
      "loss: 0.855618  [ 2416/ 3200]\n",
      "loss: 0.988889  [ 2432/ 3200]\n",
      "loss: 0.883717  [ 2448/ 3200]\n",
      "loss: 0.849111  [ 2464/ 3200]\n",
      "loss: 0.941804  [ 2480/ 3200]\n",
      "loss: 1.115031  [ 2496/ 3200]\n",
      "loss: 0.907316  [ 2512/ 3200]\n",
      "loss: 0.994009  [ 2528/ 3200]\n",
      "loss: 1.047194  [ 2544/ 3200]\n",
      "loss: 1.133319  [ 2560/ 3200]\n",
      "loss: 0.923718  [ 2576/ 3200]\n",
      "loss: 0.997877  [ 2592/ 3200]\n",
      "loss: 1.109210  [ 2608/ 3200]\n",
      "loss: 1.165715  [ 2624/ 3200]\n",
      "loss: 1.055499  [ 2640/ 3200]\n",
      "loss: 0.863742  [ 2656/ 3200]\n",
      "loss: 0.888844  [ 2672/ 3200]\n",
      "loss: 1.306654  [ 2688/ 3200]\n",
      "loss: 0.897054  [ 2704/ 3200]\n",
      "loss: 0.913131  [ 2720/ 3200]\n",
      "loss: 1.005601  [ 2736/ 3200]\n",
      "loss: 1.156376  [ 2752/ 3200]\n",
      "loss: 0.974567  [ 2768/ 3200]\n",
      "loss: 1.173743  [ 2784/ 3200]\n",
      "loss: 0.914102  [ 2800/ 3200]\n",
      "loss: 1.107546  [ 2816/ 3200]\n",
      "loss: 1.013638  [ 2832/ 3200]\n",
      "loss: 0.982005  [ 2848/ 3200]\n",
      "loss: 1.164373  [ 2864/ 3200]\n",
      "loss: 1.055434  [ 2880/ 3200]\n",
      "loss: 1.028212  [ 2896/ 3200]\n",
      "loss: 1.116460  [ 2912/ 3200]\n",
      "loss: 1.122938  [ 2928/ 3200]\n",
      "loss: 1.159640  [ 2944/ 3200]\n",
      "loss: 1.008308  [ 2960/ 3200]\n",
      "loss: 1.139249  [ 2976/ 3200]\n",
      "loss: 1.008053  [ 2992/ 3200]\n",
      "loss: 0.971428  [ 3008/ 3200]\n",
      "loss: 1.053757  [ 3024/ 3200]\n",
      "loss: 1.025900  [ 3040/ 3200]\n",
      "loss: 1.203260  [ 3056/ 3200]\n",
      "loss: 1.056833  [ 3072/ 3200]\n",
      "loss: 1.052536  [ 3088/ 3200]\n",
      "loss: 1.172919  [ 3104/ 3200]\n",
      "loss: 0.983833  [ 3120/ 3200]\n",
      "loss: 1.137589  [ 3136/ 3200]\n",
      "loss: 1.070134  [ 3152/ 3200]\n",
      "loss: 0.984160  [ 3168/ 3200]\n",
      "loss: 0.983850  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.063227\n",
      "f1 macro averaged score: 0.607582\n",
      "Accuracy               : 62.3%\n",
      "Confusion matrix       :\n",
      "tensor([[128,  57,   6,   9],\n",
      "        [ 22,  47, 102,  29],\n",
      "        [  1,  15, 168,  16],\n",
      "        [  4,  16,  25, 155]])\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 1.044999  [    0/ 3200]\n",
      "loss: 1.211534  [   16/ 3200]\n",
      "loss: 1.005188  [   32/ 3200]\n",
      "loss: 0.752239  [   48/ 3200]\n",
      "loss: 1.018696  [   64/ 3200]\n",
      "loss: 1.015531  [   80/ 3200]\n",
      "loss: 1.021334  [   96/ 3200]\n",
      "loss: 1.141265  [  112/ 3200]\n",
      "loss: 0.982179  [  128/ 3200]\n",
      "loss: 0.921403  [  144/ 3200]\n",
      "loss: 0.906418  [  160/ 3200]\n",
      "loss: 1.145507  [  176/ 3200]\n",
      "loss: 0.960968  [  192/ 3200]\n",
      "loss: 1.301285  [  208/ 3200]\n",
      "loss: 0.942417  [  224/ 3200]\n",
      "loss: 1.010750  [  240/ 3200]\n",
      "loss: 0.959485  [  256/ 3200]\n",
      "loss: 1.140471  [  272/ 3200]\n",
      "loss: 0.677520  [  288/ 3200]\n",
      "loss: 1.013849  [  304/ 3200]\n",
      "loss: 1.238594  [  320/ 3200]\n",
      "loss: 0.757147  [  336/ 3200]\n",
      "loss: 1.006566  [  352/ 3200]\n",
      "loss: 1.153219  [  368/ 3200]\n",
      "loss: 0.824015  [  384/ 3200]\n",
      "loss: 1.066165  [  400/ 3200]\n",
      "loss: 1.065857  [  416/ 3200]\n",
      "loss: 0.769463  [  432/ 3200]\n",
      "loss: 1.029110  [  448/ 3200]\n",
      "loss: 1.139017  [  464/ 3200]\n",
      "loss: 1.225484  [  480/ 3200]\n",
      "loss: 1.206727  [  496/ 3200]\n",
      "loss: 1.160335  [  512/ 3200]\n",
      "loss: 1.166181  [  528/ 3200]\n",
      "loss: 1.057798  [  544/ 3200]\n",
      "loss: 1.028309  [  560/ 3200]\n",
      "loss: 1.244707  [  576/ 3200]\n",
      "loss: 0.901193  [  592/ 3200]\n",
      "loss: 0.825419  [  608/ 3200]\n",
      "loss: 1.034704  [  624/ 3200]\n",
      "loss: 0.895038  [  640/ 3200]\n",
      "loss: 1.222113  [  656/ 3200]\n",
      "loss: 0.967461  [  672/ 3200]\n",
      "loss: 1.022260  [  688/ 3200]\n",
      "loss: 1.083982  [  704/ 3200]\n",
      "loss: 1.150833  [  720/ 3200]\n",
      "loss: 1.184446  [  736/ 3200]\n",
      "loss: 1.054690  [  752/ 3200]\n",
      "loss: 0.821464  [  768/ 3200]\n",
      "loss: 1.009264  [  784/ 3200]\n",
      "loss: 0.824636  [  800/ 3200]\n",
      "loss: 1.048952  [  816/ 3200]\n",
      "loss: 1.375683  [  832/ 3200]\n",
      "loss: 1.165554  [  848/ 3200]\n",
      "loss: 1.001368  [  864/ 3200]\n",
      "loss: 0.836885  [  880/ 3200]\n",
      "loss: 0.988868  [  896/ 3200]\n",
      "loss: 1.066591  [  912/ 3200]\n",
      "loss: 1.181964  [  928/ 3200]\n",
      "loss: 1.126912  [  944/ 3200]\n",
      "loss: 1.065147  [  960/ 3200]\n",
      "loss: 0.959848  [  976/ 3200]\n",
      "loss: 1.304815  [  992/ 3200]\n",
      "loss: 1.137699  [ 1008/ 3200]\n",
      "loss: 1.010126  [ 1024/ 3200]\n",
      "loss: 1.078719  [ 1040/ 3200]\n",
      "loss: 0.941036  [ 1056/ 3200]\n",
      "loss: 1.121404  [ 1072/ 3200]\n",
      "loss: 0.995843  [ 1088/ 3200]\n",
      "loss: 1.004969  [ 1104/ 3200]\n",
      "loss: 0.971634  [ 1120/ 3200]\n",
      "loss: 1.150120  [ 1136/ 3200]\n",
      "loss: 1.078717  [ 1152/ 3200]\n",
      "loss: 0.980742  [ 1168/ 3200]\n",
      "loss: 0.984689  [ 1184/ 3200]\n",
      "loss: 1.112845  [ 1200/ 3200]\n",
      "loss: 1.161920  [ 1216/ 3200]\n",
      "loss: 1.021356  [ 1232/ 3200]\n",
      "loss: 0.953563  [ 1248/ 3200]\n",
      "loss: 1.019333  [ 1264/ 3200]\n",
      "loss: 1.068456  [ 1280/ 3200]\n",
      "loss: 1.212865  [ 1296/ 3200]\n",
      "loss: 1.126683  [ 1312/ 3200]\n",
      "loss: 1.193568  [ 1328/ 3200]\n",
      "loss: 0.992568  [ 1344/ 3200]\n",
      "loss: 1.179992  [ 1360/ 3200]\n",
      "loss: 0.962718  [ 1376/ 3200]\n",
      "loss: 1.462341  [ 1392/ 3200]\n",
      "loss: 0.886161  [ 1408/ 3200]\n",
      "loss: 0.948665  [ 1424/ 3200]\n",
      "loss: 1.049557  [ 1440/ 3200]\n",
      "loss: 0.997164  [ 1456/ 3200]\n",
      "loss: 0.939301  [ 1472/ 3200]\n",
      "loss: 0.885142  [ 1488/ 3200]\n",
      "loss: 1.027683  [ 1504/ 3200]\n",
      "loss: 1.022444  [ 1520/ 3200]\n",
      "loss: 1.122279  [ 1536/ 3200]\n",
      "loss: 1.175208  [ 1552/ 3200]\n",
      "loss: 1.009384  [ 1568/ 3200]\n",
      "loss: 1.054503  [ 1584/ 3200]\n",
      "loss: 0.968906  [ 1600/ 3200]\n",
      "loss: 1.157515  [ 1616/ 3200]\n",
      "loss: 0.853123  [ 1632/ 3200]\n",
      "loss: 1.187601  [ 1648/ 3200]\n",
      "loss: 0.844284  [ 1664/ 3200]\n",
      "loss: 0.974071  [ 1680/ 3200]\n",
      "loss: 1.011733  [ 1696/ 3200]\n",
      "loss: 1.131584  [ 1712/ 3200]\n",
      "loss: 0.909518  [ 1728/ 3200]\n",
      "loss: 0.898401  [ 1744/ 3200]\n",
      "loss: 0.909161  [ 1760/ 3200]\n",
      "loss: 1.195950  [ 1776/ 3200]\n",
      "loss: 0.874663  [ 1792/ 3200]\n",
      "loss: 1.034093  [ 1808/ 3200]\n",
      "loss: 1.014564  [ 1824/ 3200]\n",
      "loss: 0.956802  [ 1840/ 3200]\n",
      "loss: 1.160194  [ 1856/ 3200]\n",
      "loss: 1.092793  [ 1872/ 3200]\n",
      "loss: 1.020823  [ 1888/ 3200]\n",
      "loss: 1.209790  [ 1904/ 3200]\n",
      "loss: 1.082700  [ 1920/ 3200]\n",
      "loss: 1.393410  [ 1936/ 3200]\n",
      "loss: 1.019350  [ 1952/ 3200]\n",
      "loss: 1.184856  [ 1968/ 3200]\n",
      "loss: 1.042902  [ 1984/ 3200]\n",
      "loss: 1.073715  [ 2000/ 3200]\n",
      "loss: 1.155896  [ 2016/ 3200]\n",
      "loss: 0.895679  [ 2032/ 3200]\n",
      "loss: 1.006799  [ 2048/ 3200]\n",
      "loss: 1.067674  [ 2064/ 3200]\n",
      "loss: 0.826016  [ 2080/ 3200]\n",
      "loss: 1.085412  [ 2096/ 3200]\n",
      "loss: 1.100934  [ 2112/ 3200]\n",
      "loss: 0.975506  [ 2128/ 3200]\n",
      "loss: 1.429533  [ 2144/ 3200]\n",
      "loss: 0.987931  [ 2160/ 3200]\n",
      "loss: 1.040779  [ 2176/ 3200]\n",
      "loss: 0.750436  [ 2192/ 3200]\n",
      "loss: 0.997611  [ 2208/ 3200]\n",
      "loss: 1.007956  [ 2224/ 3200]\n",
      "loss: 0.958536  [ 2240/ 3200]\n",
      "loss: 0.991067  [ 2256/ 3200]\n",
      "loss: 1.149028  [ 2272/ 3200]\n",
      "loss: 0.885400  [ 2288/ 3200]\n",
      "loss: 1.032940  [ 2304/ 3200]\n",
      "loss: 1.114414  [ 2320/ 3200]\n",
      "loss: 0.913841  [ 2336/ 3200]\n",
      "loss: 0.925424  [ 2352/ 3200]\n",
      "loss: 0.885467  [ 2368/ 3200]\n",
      "loss: 1.007965  [ 2384/ 3200]\n",
      "loss: 0.913171  [ 2400/ 3200]\n",
      "loss: 1.021248  [ 2416/ 3200]\n",
      "loss: 0.686146  [ 2432/ 3200]\n",
      "loss: 1.084923  [ 2448/ 3200]\n",
      "loss: 1.058628  [ 2464/ 3200]\n",
      "loss: 1.026798  [ 2480/ 3200]\n",
      "loss: 1.150876  [ 2496/ 3200]\n",
      "loss: 0.962960  [ 2512/ 3200]\n",
      "loss: 0.970088  [ 2528/ 3200]\n",
      "loss: 0.932091  [ 2544/ 3200]\n",
      "loss: 0.908056  [ 2560/ 3200]\n",
      "loss: 0.941172  [ 2576/ 3200]\n",
      "loss: 1.042497  [ 2592/ 3200]\n",
      "loss: 1.007812  [ 2608/ 3200]\n",
      "loss: 0.998068  [ 2624/ 3200]\n",
      "loss: 1.164715  [ 2640/ 3200]\n",
      "loss: 0.984832  [ 2656/ 3200]\n",
      "loss: 0.986945  [ 2672/ 3200]\n",
      "loss: 0.901115  [ 2688/ 3200]\n",
      "loss: 1.026497  [ 2704/ 3200]\n",
      "loss: 0.976358  [ 2720/ 3200]\n",
      "loss: 0.993293  [ 2736/ 3200]\n",
      "loss: 1.161389  [ 2752/ 3200]\n",
      "loss: 1.074557  [ 2768/ 3200]\n",
      "loss: 0.773707  [ 2784/ 3200]\n",
      "loss: 0.997679  [ 2800/ 3200]\n",
      "loss: 1.222372  [ 2816/ 3200]\n",
      "loss: 1.069534  [ 2832/ 3200]\n",
      "loss: 0.900362  [ 2848/ 3200]\n",
      "loss: 0.958560  [ 2864/ 3200]\n",
      "loss: 0.884219  [ 2880/ 3200]\n",
      "loss: 0.926639  [ 2896/ 3200]\n",
      "loss: 0.834676  [ 2912/ 3200]\n",
      "loss: 1.073933  [ 2928/ 3200]\n",
      "loss: 0.846630  [ 2944/ 3200]\n",
      "loss: 0.998661  [ 2960/ 3200]\n",
      "loss: 0.909908  [ 2976/ 3200]\n",
      "loss: 1.102724  [ 2992/ 3200]\n",
      "loss: 0.774849  [ 3008/ 3200]\n",
      "loss: 1.010241  [ 3024/ 3200]\n",
      "loss: 1.206098  [ 3040/ 3200]\n",
      "loss: 0.874080  [ 3056/ 3200]\n",
      "loss: 1.071101  [ 3072/ 3200]\n",
      "loss: 0.959403  [ 3088/ 3200]\n",
      "loss: 1.086762  [ 3104/ 3200]\n",
      "loss: 0.907721  [ 3120/ 3200]\n",
      "loss: 0.842880  [ 3136/ 3200]\n",
      "loss: 1.162545  [ 3152/ 3200]\n",
      "loss: 1.083878  [ 3168/ 3200]\n",
      "loss: 0.963673  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.062063\n",
      "f1 macro averaged score: 0.571377\n",
      "Accuracy               : 62.0%\n",
      "Confusion matrix       :\n",
      "tensor([[154,   9,   6,  31],\n",
      "        [ 29,  21,  79,  71],\n",
      "        [  2,  10, 142,  46],\n",
      "        [  7,   2,  12, 179]])\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 1.110648  [    0/ 3200]\n",
      "loss: 1.213169  [   16/ 3200]\n",
      "loss: 1.014314  [   32/ 3200]\n",
      "loss: 0.949540  [   48/ 3200]\n",
      "loss: 0.970524  [   64/ 3200]\n",
      "loss: 0.977813  [   80/ 3200]\n",
      "loss: 0.942519  [   96/ 3200]\n",
      "loss: 1.262349  [  112/ 3200]\n",
      "loss: 1.100543  [  128/ 3200]\n",
      "loss: 0.831805  [  144/ 3200]\n",
      "loss: 0.990713  [  160/ 3200]\n",
      "loss: 0.873193  [  176/ 3200]\n",
      "loss: 0.938523  [  192/ 3200]\n",
      "loss: 1.196832  [  208/ 3200]\n",
      "loss: 1.088895  [  224/ 3200]\n",
      "loss: 0.920966  [  240/ 3200]\n",
      "loss: 1.078061  [  256/ 3200]\n",
      "loss: 1.158427  [  272/ 3200]\n",
      "loss: 1.029370  [  288/ 3200]\n",
      "loss: 1.005526  [  304/ 3200]\n",
      "loss: 1.022199  [  320/ 3200]\n",
      "loss: 1.040103  [  336/ 3200]\n",
      "loss: 0.905552  [  352/ 3200]\n",
      "loss: 0.649422  [  368/ 3200]\n",
      "loss: 1.152670  [  384/ 3200]\n",
      "loss: 1.213317  [  400/ 3200]\n",
      "loss: 1.163784  [  416/ 3200]\n",
      "loss: 0.921935  [  432/ 3200]\n",
      "loss: 1.059814  [  448/ 3200]\n",
      "loss: 1.109021  [  464/ 3200]\n",
      "loss: 0.907840  [  480/ 3200]\n",
      "loss: 0.954232  [  496/ 3200]\n",
      "loss: 1.125707  [  512/ 3200]\n",
      "loss: 1.050344  [  528/ 3200]\n",
      "loss: 0.881360  [  544/ 3200]\n",
      "loss: 1.213970  [  560/ 3200]\n",
      "loss: 1.027825  [  576/ 3200]\n",
      "loss: 0.948896  [  592/ 3200]\n",
      "loss: 0.870226  [  608/ 3200]\n",
      "loss: 0.988511  [  624/ 3200]\n",
      "loss: 1.123701  [  640/ 3200]\n",
      "loss: 1.235453  [  656/ 3200]\n",
      "loss: 1.251487  [  672/ 3200]\n",
      "loss: 1.021722  [  688/ 3200]\n",
      "loss: 0.953295  [  704/ 3200]\n",
      "loss: 1.022118  [  720/ 3200]\n",
      "loss: 1.070043  [  736/ 3200]\n",
      "loss: 1.008713  [  752/ 3200]\n",
      "loss: 1.227663  [  768/ 3200]\n",
      "loss: 1.003993  [  784/ 3200]\n",
      "loss: 1.083899  [  800/ 3200]\n",
      "loss: 0.900209  [  816/ 3200]\n",
      "loss: 1.099467  [  832/ 3200]\n",
      "loss: 0.997615  [  848/ 3200]\n",
      "loss: 1.140505  [  864/ 3200]\n",
      "loss: 0.983643  [  880/ 3200]\n",
      "loss: 0.735234  [  896/ 3200]\n",
      "loss: 1.114725  [  912/ 3200]\n",
      "loss: 0.960802  [  928/ 3200]\n",
      "loss: 1.250354  [  944/ 3200]\n",
      "loss: 1.078701  [  960/ 3200]\n",
      "loss: 1.177490  [  976/ 3200]\n",
      "loss: 0.892346  [  992/ 3200]\n",
      "loss: 0.967306  [ 1008/ 3200]\n",
      "loss: 0.836083  [ 1024/ 3200]\n",
      "loss: 1.098019  [ 1040/ 3200]\n",
      "loss: 0.968713  [ 1056/ 3200]\n",
      "loss: 1.032980  [ 1072/ 3200]\n",
      "loss: 0.930815  [ 1088/ 3200]\n",
      "loss: 1.258398  [ 1104/ 3200]\n",
      "loss: 1.035722  [ 1120/ 3200]\n",
      "loss: 1.082759  [ 1136/ 3200]\n",
      "loss: 0.955948  [ 1152/ 3200]\n",
      "loss: 1.051593  [ 1168/ 3200]\n",
      "loss: 1.217227  [ 1184/ 3200]\n",
      "loss: 0.901967  [ 1200/ 3200]\n",
      "loss: 0.955387  [ 1216/ 3200]\n",
      "loss: 0.970029  [ 1232/ 3200]\n",
      "loss: 1.012123  [ 1248/ 3200]\n",
      "loss: 0.954173  [ 1264/ 3200]\n",
      "loss: 1.321425  [ 1280/ 3200]\n",
      "loss: 1.052297  [ 1296/ 3200]\n",
      "loss: 1.144534  [ 1312/ 3200]\n",
      "loss: 0.979517  [ 1328/ 3200]\n",
      "loss: 1.093355  [ 1344/ 3200]\n",
      "loss: 0.939880  [ 1360/ 3200]\n",
      "loss: 1.089206  [ 1376/ 3200]\n",
      "loss: 1.296301  [ 1392/ 3200]\n",
      "loss: 0.923802  [ 1408/ 3200]\n",
      "loss: 0.769217  [ 1424/ 3200]\n",
      "loss: 0.873613  [ 1440/ 3200]\n",
      "loss: 0.930876  [ 1456/ 3200]\n",
      "loss: 1.257648  [ 1472/ 3200]\n",
      "loss: 0.944675  [ 1488/ 3200]\n",
      "loss: 1.490437  [ 1504/ 3200]\n",
      "loss: 0.861430  [ 1520/ 3200]\n",
      "loss: 1.057998  [ 1536/ 3200]\n",
      "loss: 0.808157  [ 1552/ 3200]\n",
      "loss: 0.975699  [ 1568/ 3200]\n",
      "loss: 0.793408  [ 1584/ 3200]\n",
      "loss: 1.335935  [ 1600/ 3200]\n",
      "loss: 1.197002  [ 1616/ 3200]\n",
      "loss: 0.927188  [ 1632/ 3200]\n",
      "loss: 1.029918  [ 1648/ 3200]\n",
      "loss: 0.898440  [ 1664/ 3200]\n",
      "loss: 0.935886  [ 1680/ 3200]\n",
      "loss: 0.956232  [ 1696/ 3200]\n",
      "loss: 0.851239  [ 1712/ 3200]\n",
      "loss: 0.897502  [ 1728/ 3200]\n",
      "loss: 0.916179  [ 1744/ 3200]\n",
      "loss: 0.813431  [ 1760/ 3200]\n",
      "loss: 0.827127  [ 1776/ 3200]\n",
      "loss: 1.128449  [ 1792/ 3200]\n",
      "loss: 0.973692  [ 1808/ 3200]\n",
      "loss: 1.110042  [ 1824/ 3200]\n",
      "loss: 0.971800  [ 1840/ 3200]\n",
      "loss: 0.822549  [ 1856/ 3200]\n",
      "loss: 1.091634  [ 1872/ 3200]\n",
      "loss: 1.135003  [ 1888/ 3200]\n",
      "loss: 1.007418  [ 1904/ 3200]\n",
      "loss: 1.002602  [ 1920/ 3200]\n",
      "loss: 0.941341  [ 1936/ 3200]\n",
      "loss: 1.083711  [ 1952/ 3200]\n",
      "loss: 0.958810  [ 1968/ 3200]\n",
      "loss: 0.892863  [ 1984/ 3200]\n",
      "loss: 1.132749  [ 2000/ 3200]\n",
      "loss: 0.775455  [ 2016/ 3200]\n",
      "loss: 0.901064  [ 2032/ 3200]\n",
      "loss: 0.946479  [ 2048/ 3200]\n",
      "loss: 1.462762  [ 2064/ 3200]\n",
      "loss: 1.081569  [ 2080/ 3200]\n",
      "loss: 1.043655  [ 2096/ 3200]\n",
      "loss: 0.856876  [ 2112/ 3200]\n",
      "loss: 0.836952  [ 2128/ 3200]\n",
      "loss: 0.976150  [ 2144/ 3200]\n",
      "loss: 0.724758  [ 2160/ 3200]\n",
      "loss: 0.974999  [ 2176/ 3200]\n",
      "loss: 1.001801  [ 2192/ 3200]\n",
      "loss: 0.932356  [ 2208/ 3200]\n",
      "loss: 1.003895  [ 2224/ 3200]\n",
      "loss: 0.681635  [ 2240/ 3200]\n",
      "loss: 1.015545  [ 2256/ 3200]\n",
      "loss: 0.888515  [ 2272/ 3200]\n",
      "loss: 0.956367  [ 2288/ 3200]\n",
      "loss: 1.145676  [ 2304/ 3200]\n",
      "loss: 0.866908  [ 2320/ 3200]\n",
      "loss: 0.819745  [ 2336/ 3200]\n",
      "loss: 0.801503  [ 2352/ 3200]\n",
      "loss: 0.994161  [ 2368/ 3200]\n",
      "loss: 1.043134  [ 2384/ 3200]\n",
      "loss: 1.111173  [ 2400/ 3200]\n",
      "loss: 1.036773  [ 2416/ 3200]\n",
      "loss: 1.062748  [ 2432/ 3200]\n",
      "loss: 1.178515  [ 2448/ 3200]\n",
      "loss: 1.235463  [ 2464/ 3200]\n",
      "loss: 0.934526  [ 2480/ 3200]\n",
      "loss: 1.028126  [ 2496/ 3200]\n",
      "loss: 1.036342  [ 2512/ 3200]\n",
      "loss: 1.069892  [ 2528/ 3200]\n",
      "loss: 1.007171  [ 2544/ 3200]\n",
      "loss: 0.959225  [ 2560/ 3200]\n",
      "loss: 1.121462  [ 2576/ 3200]\n",
      "loss: 1.003230  [ 2592/ 3200]\n",
      "loss: 1.043486  [ 2608/ 3200]\n",
      "loss: 0.949481  [ 2624/ 3200]\n",
      "loss: 0.795143  [ 2640/ 3200]\n",
      "loss: 0.894580  [ 2656/ 3200]\n",
      "loss: 0.764309  [ 2672/ 3200]\n",
      "loss: 1.156047  [ 2688/ 3200]\n",
      "loss: 0.922530  [ 2704/ 3200]\n",
      "loss: 1.033313  [ 2720/ 3200]\n",
      "loss: 0.958776  [ 2736/ 3200]\n",
      "loss: 0.991804  [ 2752/ 3200]\n",
      "loss: 0.952892  [ 2768/ 3200]\n",
      "loss: 0.952451  [ 2784/ 3200]\n",
      "loss: 1.137843  [ 2800/ 3200]\n",
      "loss: 1.087261  [ 2816/ 3200]\n",
      "loss: 0.949062  [ 2832/ 3200]\n",
      "loss: 0.954847  [ 2848/ 3200]\n",
      "loss: 1.019110  [ 2864/ 3200]\n",
      "loss: 1.086587  [ 2880/ 3200]\n",
      "loss: 1.043885  [ 2896/ 3200]\n",
      "loss: 1.094240  [ 2912/ 3200]\n",
      "loss: 0.957533  [ 2928/ 3200]\n",
      "loss: 1.177385  [ 2944/ 3200]\n",
      "loss: 0.779103  [ 2960/ 3200]\n",
      "loss: 1.033455  [ 2976/ 3200]\n",
      "loss: 1.169549  [ 2992/ 3200]\n",
      "loss: 0.949290  [ 3008/ 3200]\n",
      "loss: 0.891896  [ 3024/ 3200]\n",
      "loss: 0.972681  [ 3040/ 3200]\n",
      "loss: 0.929300  [ 3056/ 3200]\n",
      "loss: 0.963898  [ 3072/ 3200]\n",
      "loss: 0.965832  [ 3088/ 3200]\n",
      "loss: 1.063562  [ 3104/ 3200]\n",
      "loss: 0.967147  [ 3120/ 3200]\n",
      "loss: 1.454913  [ 3136/ 3200]\n",
      "loss: 0.919883  [ 3152/ 3200]\n",
      "loss: 1.246592  [ 3168/ 3200]\n",
      "loss: 1.289945  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.060945\n",
      "f1 macro averaged score: 0.599685\n",
      "Accuracy               : 64.0%\n",
      "Confusion matrix       :\n",
      "tensor([[160,  10,   5,  25],\n",
      "        [ 30,  31,  75,  64],\n",
      "        [  2,  12, 142,  44],\n",
      "        [  7,   2,  12, 179]])\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.849909  [    0/ 3200]\n",
      "loss: 1.003107  [   16/ 3200]\n",
      "loss: 0.951100  [   32/ 3200]\n",
      "loss: 1.298954  [   48/ 3200]\n",
      "loss: 1.038182  [   64/ 3200]\n",
      "loss: 1.171435  [   80/ 3200]\n",
      "loss: 1.047869  [   96/ 3200]\n",
      "loss: 1.029326  [  112/ 3200]\n",
      "loss: 0.703859  [  128/ 3200]\n",
      "loss: 1.011308  [  144/ 3200]\n",
      "loss: 0.982307  [  160/ 3200]\n",
      "loss: 0.771296  [  176/ 3200]\n",
      "loss: 0.908054  [  192/ 3200]\n",
      "loss: 0.847939  [  208/ 3200]\n",
      "loss: 0.839083  [  224/ 3200]\n",
      "loss: 0.891693  [  240/ 3200]\n",
      "loss: 1.184141  [  256/ 3200]\n",
      "loss: 1.104158  [  272/ 3200]\n",
      "loss: 0.795272  [  288/ 3200]\n",
      "loss: 0.830453  [  304/ 3200]\n",
      "loss: 0.725994  [  320/ 3200]\n",
      "loss: 0.902992  [  336/ 3200]\n",
      "loss: 1.158733  [  352/ 3200]\n",
      "loss: 1.039586  [  368/ 3200]\n",
      "loss: 1.055976  [  384/ 3200]\n",
      "loss: 0.788057  [  400/ 3200]\n",
      "loss: 1.117797  [  416/ 3200]\n",
      "loss: 1.061768  [  432/ 3200]\n",
      "loss: 0.919959  [  448/ 3200]\n",
      "loss: 1.213893  [  464/ 3200]\n",
      "loss: 1.211207  [  480/ 3200]\n",
      "loss: 0.891287  [  496/ 3200]\n",
      "loss: 1.086172  [  512/ 3200]\n",
      "loss: 0.948251  [  528/ 3200]\n",
      "loss: 0.890106  [  544/ 3200]\n",
      "loss: 0.859765  [  560/ 3200]\n",
      "loss: 1.095119  [  576/ 3200]\n",
      "loss: 1.030694  [  592/ 3200]\n",
      "loss: 0.959293  [  608/ 3200]\n",
      "loss: 1.099186  [  624/ 3200]\n",
      "loss: 0.907867  [  640/ 3200]\n",
      "loss: 0.968772  [  656/ 3200]\n",
      "loss: 1.092240  [  672/ 3200]\n",
      "loss: 1.010888  [  688/ 3200]\n",
      "loss: 0.979972  [  704/ 3200]\n",
      "loss: 0.891103  [  720/ 3200]\n",
      "loss: 0.700042  [  736/ 3200]\n",
      "loss: 0.923734  [  752/ 3200]\n",
      "loss: 0.772956  [  768/ 3200]\n",
      "loss: 0.924478  [  784/ 3200]\n",
      "loss: 1.202591  [  800/ 3200]\n",
      "loss: 0.958884  [  816/ 3200]\n",
      "loss: 1.103329  [  832/ 3200]\n",
      "loss: 1.057253  [  848/ 3200]\n",
      "loss: 1.034095  [  864/ 3200]\n",
      "loss: 1.305357  [  880/ 3200]\n",
      "loss: 1.038826  [  896/ 3200]\n",
      "loss: 0.899185  [  912/ 3200]\n",
      "loss: 1.036328  [  928/ 3200]\n",
      "loss: 0.891196  [  944/ 3200]\n",
      "loss: 0.893620  [  960/ 3200]\n",
      "loss: 1.016497  [  976/ 3200]\n",
      "loss: 0.911190  [  992/ 3200]\n",
      "loss: 0.933262  [ 1008/ 3200]\n",
      "loss: 0.875927  [ 1024/ 3200]\n",
      "loss: 1.044401  [ 1040/ 3200]\n",
      "loss: 0.891133  [ 1056/ 3200]\n",
      "loss: 0.874351  [ 1072/ 3200]\n",
      "loss: 1.001260  [ 1088/ 3200]\n",
      "loss: 1.100322  [ 1104/ 3200]\n",
      "loss: 0.924265  [ 1120/ 3200]\n",
      "loss: 1.037678  [ 1136/ 3200]\n",
      "loss: 1.132483  [ 1152/ 3200]\n",
      "loss: 1.311667  [ 1168/ 3200]\n",
      "loss: 1.172084  [ 1184/ 3200]\n",
      "loss: 0.999302  [ 1200/ 3200]\n",
      "loss: 0.813873  [ 1216/ 3200]\n",
      "loss: 0.791882  [ 1232/ 3200]\n",
      "loss: 1.107280  [ 1248/ 3200]\n",
      "loss: 1.051398  [ 1264/ 3200]\n",
      "loss: 1.090274  [ 1280/ 3200]\n",
      "loss: 1.218874  [ 1296/ 3200]\n",
      "loss: 1.116203  [ 1312/ 3200]\n",
      "loss: 1.163509  [ 1328/ 3200]\n",
      "loss: 0.944986  [ 1344/ 3200]\n",
      "loss: 1.071523  [ 1360/ 3200]\n",
      "loss: 1.027476  [ 1376/ 3200]\n",
      "loss: 0.923170  [ 1392/ 3200]\n",
      "loss: 0.934786  [ 1408/ 3200]\n",
      "loss: 0.861815  [ 1424/ 3200]\n",
      "loss: 1.063236  [ 1440/ 3200]\n",
      "loss: 1.114481  [ 1456/ 3200]\n",
      "loss: 0.946565  [ 1472/ 3200]\n",
      "loss: 1.148869  [ 1488/ 3200]\n",
      "loss: 1.150334  [ 1504/ 3200]\n",
      "loss: 1.056087  [ 1520/ 3200]\n",
      "loss: 0.898091  [ 1536/ 3200]\n",
      "loss: 0.923440  [ 1552/ 3200]\n",
      "loss: 0.820932  [ 1568/ 3200]\n",
      "loss: 0.996131  [ 1584/ 3200]\n",
      "loss: 1.166396  [ 1600/ 3200]\n",
      "loss: 1.098915  [ 1616/ 3200]\n",
      "loss: 0.775631  [ 1632/ 3200]\n",
      "loss: 1.087361  [ 1648/ 3200]\n",
      "loss: 1.210828  [ 1664/ 3200]\n",
      "loss: 0.956129  [ 1680/ 3200]\n",
      "loss: 0.945767  [ 1696/ 3200]\n",
      "loss: 1.363257  [ 1712/ 3200]\n",
      "loss: 0.843900  [ 1728/ 3200]\n",
      "loss: 0.947364  [ 1744/ 3200]\n",
      "loss: 0.754222  [ 1760/ 3200]\n",
      "loss: 1.041970  [ 1776/ 3200]\n",
      "loss: 1.183410  [ 1792/ 3200]\n",
      "loss: 1.029453  [ 1808/ 3200]\n",
      "loss: 0.972824  [ 1824/ 3200]\n",
      "loss: 1.183829  [ 1840/ 3200]\n",
      "loss: 1.175421  [ 1856/ 3200]\n",
      "loss: 0.964571  [ 1872/ 3200]\n",
      "loss: 1.002697  [ 1888/ 3200]\n",
      "loss: 1.011990  [ 1904/ 3200]\n",
      "loss: 0.871625  [ 1920/ 3200]\n",
      "loss: 0.789888  [ 1936/ 3200]\n",
      "loss: 1.024298  [ 1952/ 3200]\n",
      "loss: 0.778907  [ 1968/ 3200]\n",
      "loss: 1.005966  [ 1984/ 3200]\n",
      "loss: 0.856428  [ 2000/ 3200]\n",
      "loss: 1.122828  [ 2016/ 3200]\n",
      "loss: 1.115508  [ 2032/ 3200]\n",
      "loss: 1.104192  [ 2048/ 3200]\n",
      "loss: 1.160305  [ 2064/ 3200]\n",
      "loss: 0.860706  [ 2080/ 3200]\n",
      "loss: 0.714177  [ 2096/ 3200]\n",
      "loss: 0.950116  [ 2112/ 3200]\n",
      "loss: 1.021823  [ 2128/ 3200]\n",
      "loss: 0.876479  [ 2144/ 3200]\n",
      "loss: 1.017858  [ 2160/ 3200]\n",
      "loss: 1.203705  [ 2176/ 3200]\n",
      "loss: 1.006137  [ 2192/ 3200]\n",
      "loss: 1.284177  [ 2208/ 3200]\n",
      "loss: 1.039065  [ 2224/ 3200]\n",
      "loss: 1.014311  [ 2240/ 3200]\n",
      "loss: 0.919561  [ 2256/ 3200]\n",
      "loss: 1.082525  [ 2272/ 3200]\n",
      "loss: 0.850753  [ 2288/ 3200]\n",
      "loss: 0.876138  [ 2304/ 3200]\n",
      "loss: 1.071592  [ 2320/ 3200]\n",
      "loss: 1.117123  [ 2336/ 3200]\n",
      "loss: 1.313967  [ 2352/ 3200]\n",
      "loss: 1.052919  [ 2368/ 3200]\n",
      "loss: 1.080200  [ 2384/ 3200]\n",
      "loss: 1.032180  [ 2400/ 3200]\n",
      "loss: 1.254194  [ 2416/ 3200]\n",
      "loss: 1.150455  [ 2432/ 3200]\n",
      "loss: 1.118784  [ 2448/ 3200]\n",
      "loss: 0.909154  [ 2464/ 3200]\n",
      "loss: 0.913915  [ 2480/ 3200]\n",
      "loss: 1.046443  [ 2496/ 3200]\n",
      "loss: 0.805108  [ 2512/ 3200]\n",
      "loss: 0.978224  [ 2528/ 3200]\n",
      "loss: 0.845053  [ 2544/ 3200]\n",
      "loss: 0.993423  [ 2560/ 3200]\n",
      "loss: 1.070972  [ 2576/ 3200]\n",
      "loss: 1.209874  [ 2592/ 3200]\n",
      "loss: 0.885976  [ 2608/ 3200]\n",
      "loss: 1.019565  [ 2624/ 3200]\n",
      "loss: 0.880343  [ 2640/ 3200]\n",
      "loss: 0.944849  [ 2656/ 3200]\n",
      "loss: 1.071895  [ 2672/ 3200]\n",
      "loss: 0.732433  [ 2688/ 3200]\n",
      "loss: 1.075298  [ 2704/ 3200]\n",
      "loss: 1.109530  [ 2720/ 3200]\n",
      "loss: 0.942543  [ 2736/ 3200]\n",
      "loss: 0.752436  [ 2752/ 3200]\n",
      "loss: 1.057493  [ 2768/ 3200]\n",
      "loss: 0.938675  [ 2784/ 3200]\n",
      "loss: 1.106303  [ 2800/ 3200]\n",
      "loss: 0.917809  [ 2816/ 3200]\n",
      "loss: 1.187341  [ 2832/ 3200]\n",
      "loss: 1.102984  [ 2848/ 3200]\n",
      "loss: 1.149477  [ 2864/ 3200]\n",
      "loss: 1.059843  [ 2880/ 3200]\n",
      "loss: 1.060603  [ 2896/ 3200]\n",
      "loss: 1.058613  [ 2912/ 3200]\n",
      "loss: 0.967196  [ 2928/ 3200]\n",
      "loss: 0.855965  [ 2944/ 3200]\n",
      "loss: 1.047005  [ 2960/ 3200]\n",
      "loss: 1.010028  [ 2976/ 3200]\n",
      "loss: 0.971526  [ 2992/ 3200]\n",
      "loss: 0.821091  [ 3008/ 3200]\n",
      "loss: 1.024326  [ 3024/ 3200]\n",
      "loss: 1.076452  [ 3040/ 3200]\n",
      "loss: 0.941610  [ 3056/ 3200]\n",
      "loss: 1.308664  [ 3072/ 3200]\n",
      "loss: 1.051919  [ 3088/ 3200]\n",
      "loss: 0.838764  [ 3104/ 3200]\n",
      "loss: 0.977421  [ 3120/ 3200]\n",
      "loss: 1.130571  [ 3136/ 3200]\n",
      "loss: 1.052414  [ 3152/ 3200]\n",
      "loss: 0.994937  [ 3168/ 3200]\n",
      "loss: 1.161800  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.059928\n",
      "f1 macro averaged score: 0.655442\n",
      "Accuracy               : 66.1%\n",
      "Confusion matrix       :\n",
      "tensor([[155,  37,   5,   3],\n",
      "        [ 29,  69,  86,  16],\n",
      "        [  2,  26, 162,  10],\n",
      "        [  8,  26,  23, 143]])\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 1.028304  [    0/ 3200]\n",
      "loss: 1.149075  [   16/ 3200]\n",
      "loss: 0.848697  [   32/ 3200]\n",
      "loss: 1.107150  [   48/ 3200]\n",
      "loss: 1.182797  [   64/ 3200]\n",
      "loss: 0.897933  [   80/ 3200]\n",
      "loss: 0.917315  [   96/ 3200]\n",
      "loss: 0.872691  [  112/ 3200]\n",
      "loss: 1.080769  [  128/ 3200]\n",
      "loss: 1.249787  [  144/ 3200]\n",
      "loss: 0.886264  [  160/ 3200]\n",
      "loss: 0.789739  [  176/ 3200]\n",
      "loss: 0.857498  [  192/ 3200]\n",
      "loss: 0.978795  [  208/ 3200]\n",
      "loss: 0.746086  [  224/ 3200]\n",
      "loss: 0.940814  [  240/ 3200]\n",
      "loss: 1.052806  [  256/ 3200]\n",
      "loss: 1.205664  [  272/ 3200]\n",
      "loss: 1.130703  [  288/ 3200]\n",
      "loss: 1.171822  [  304/ 3200]\n",
      "loss: 0.949500  [  320/ 3200]\n",
      "loss: 1.185843  [  336/ 3200]\n",
      "loss: 1.180003  [  352/ 3200]\n",
      "loss: 0.993845  [  368/ 3200]\n",
      "loss: 1.309084  [  384/ 3200]\n",
      "loss: 0.821487  [  400/ 3200]\n",
      "loss: 0.947661  [  416/ 3200]\n",
      "loss: 0.878179  [  432/ 3200]\n",
      "loss: 1.011086  [  448/ 3200]\n",
      "loss: 1.250186  [  464/ 3200]\n",
      "loss: 0.896362  [  480/ 3200]\n",
      "loss: 1.020643  [  496/ 3200]\n",
      "loss: 1.099905  [  512/ 3200]\n",
      "loss: 0.812519  [  528/ 3200]\n",
      "loss: 1.317135  [  544/ 3200]\n",
      "loss: 0.935304  [  560/ 3200]\n",
      "loss: 0.963580  [  576/ 3200]\n",
      "loss: 0.772382  [  592/ 3200]\n",
      "loss: 1.040340  [  608/ 3200]\n",
      "loss: 1.085317  [  624/ 3200]\n",
      "loss: 1.012173  [  640/ 3200]\n",
      "loss: 1.108312  [  656/ 3200]\n",
      "loss: 0.939718  [  672/ 3200]\n",
      "loss: 1.250636  [  688/ 3200]\n",
      "loss: 0.977128  [  704/ 3200]\n",
      "loss: 1.003331  [  720/ 3200]\n",
      "loss: 1.153801  [  736/ 3200]\n",
      "loss: 1.025107  [  752/ 3200]\n",
      "loss: 0.908645  [  768/ 3200]\n",
      "loss: 1.081248  [  784/ 3200]\n",
      "loss: 0.856218  [  800/ 3200]\n",
      "loss: 0.875019  [  816/ 3200]\n",
      "loss: 0.919429  [  832/ 3200]\n",
      "loss: 0.961027  [  848/ 3200]\n",
      "loss: 1.029457  [  864/ 3200]\n",
      "loss: 1.022356  [  880/ 3200]\n",
      "loss: 1.064696  [  896/ 3200]\n",
      "loss: 0.866586  [  912/ 3200]\n",
      "loss: 0.976055  [  928/ 3200]\n",
      "loss: 1.007450  [  944/ 3200]\n",
      "loss: 1.070187  [  960/ 3200]\n",
      "loss: 1.135733  [  976/ 3200]\n",
      "loss: 0.995175  [  992/ 3200]\n",
      "loss: 0.694547  [ 1008/ 3200]\n",
      "loss: 1.155123  [ 1024/ 3200]\n",
      "loss: 1.000012  [ 1040/ 3200]\n",
      "loss: 1.198576  [ 1056/ 3200]\n",
      "loss: 1.050283  [ 1072/ 3200]\n",
      "loss: 0.914751  [ 1088/ 3200]\n",
      "loss: 1.018778  [ 1104/ 3200]\n",
      "loss: 0.867019  [ 1120/ 3200]\n",
      "loss: 1.036518  [ 1136/ 3200]\n",
      "loss: 0.961806  [ 1152/ 3200]\n",
      "loss: 0.902051  [ 1168/ 3200]\n",
      "loss: 1.098326  [ 1184/ 3200]\n",
      "loss: 0.934693  [ 1200/ 3200]\n",
      "loss: 1.079786  [ 1216/ 3200]\n",
      "loss: 1.041337  [ 1232/ 3200]\n",
      "loss: 0.904649  [ 1248/ 3200]\n",
      "loss: 1.020505  [ 1264/ 3200]\n",
      "loss: 1.121023  [ 1280/ 3200]\n",
      "loss: 1.044576  [ 1296/ 3200]\n",
      "loss: 0.904536  [ 1312/ 3200]\n",
      "loss: 1.278561  [ 1328/ 3200]\n",
      "loss: 0.904737  [ 1344/ 3200]\n",
      "loss: 0.662699  [ 1360/ 3200]\n",
      "loss: 0.825125  [ 1376/ 3200]\n",
      "loss: 1.084972  [ 1392/ 3200]\n",
      "loss: 0.881537  [ 1408/ 3200]\n",
      "loss: 0.725210  [ 1424/ 3200]\n",
      "loss: 1.141597  [ 1440/ 3200]\n",
      "loss: 1.172989  [ 1456/ 3200]\n",
      "loss: 0.923317  [ 1472/ 3200]\n",
      "loss: 0.869031  [ 1488/ 3200]\n",
      "loss: 0.807664  [ 1504/ 3200]\n",
      "loss: 1.065293  [ 1520/ 3200]\n",
      "loss: 0.887048  [ 1536/ 3200]\n",
      "loss: 0.946095  [ 1552/ 3200]\n",
      "loss: 1.126798  [ 1568/ 3200]\n",
      "loss: 0.847877  [ 1584/ 3200]\n",
      "loss: 0.889995  [ 1600/ 3200]\n",
      "loss: 1.217513  [ 1616/ 3200]\n",
      "loss: 0.867071  [ 1632/ 3200]\n",
      "loss: 0.888396  [ 1648/ 3200]\n",
      "loss: 1.111093  [ 1664/ 3200]\n",
      "loss: 1.056872  [ 1680/ 3200]\n",
      "loss: 1.022457  [ 1696/ 3200]\n",
      "loss: 0.877981  [ 1712/ 3200]\n",
      "loss: 0.824135  [ 1728/ 3200]\n",
      "loss: 0.977145  [ 1744/ 3200]\n",
      "loss: 1.257880  [ 1760/ 3200]\n",
      "loss: 0.916057  [ 1776/ 3200]\n",
      "loss: 1.074892  [ 1792/ 3200]\n",
      "loss: 1.113218  [ 1808/ 3200]\n",
      "loss: 0.950368  [ 1824/ 3200]\n",
      "loss: 0.945761  [ 1840/ 3200]\n",
      "loss: 1.216307  [ 1856/ 3200]\n",
      "loss: 0.979022  [ 1872/ 3200]\n",
      "loss: 0.785942  [ 1888/ 3200]\n",
      "loss: 0.902919  [ 1904/ 3200]\n",
      "loss: 0.877477  [ 1920/ 3200]\n",
      "loss: 0.952398  [ 1936/ 3200]\n",
      "loss: 1.045784  [ 1952/ 3200]\n",
      "loss: 0.958538  [ 1968/ 3200]\n",
      "loss: 0.788186  [ 1984/ 3200]\n",
      "loss: 1.047592  [ 2000/ 3200]\n",
      "loss: 1.070947  [ 2016/ 3200]\n",
      "loss: 0.916863  [ 2032/ 3200]\n",
      "loss: 0.859568  [ 2048/ 3200]\n",
      "loss: 0.885155  [ 2064/ 3200]\n",
      "loss: 1.052571  [ 2080/ 3200]\n",
      "loss: 1.153887  [ 2096/ 3200]\n",
      "loss: 1.154927  [ 2112/ 3200]\n",
      "loss: 0.827817  [ 2128/ 3200]\n",
      "loss: 1.183720  [ 2144/ 3200]\n",
      "loss: 0.998905  [ 2160/ 3200]\n",
      "loss: 1.083033  [ 2176/ 3200]\n",
      "loss: 0.805064  [ 2192/ 3200]\n",
      "loss: 0.969410  [ 2208/ 3200]\n",
      "loss: 0.943281  [ 2224/ 3200]\n",
      "loss: 1.087327  [ 2240/ 3200]\n",
      "loss: 0.985519  [ 2256/ 3200]\n",
      "loss: 1.045619  [ 2272/ 3200]\n",
      "loss: 1.188480  [ 2288/ 3200]\n",
      "loss: 0.904420  [ 2304/ 3200]\n",
      "loss: 1.100701  [ 2320/ 3200]\n",
      "loss: 0.986924  [ 2336/ 3200]\n",
      "loss: 0.956793  [ 2352/ 3200]\n",
      "loss: 1.181012  [ 2368/ 3200]\n",
      "loss: 1.046256  [ 2384/ 3200]\n",
      "loss: 1.059093  [ 2400/ 3200]\n",
      "loss: 1.149774  [ 2416/ 3200]\n",
      "loss: 1.157670  [ 2432/ 3200]\n",
      "loss: 1.274584  [ 2448/ 3200]\n",
      "loss: 1.099918  [ 2464/ 3200]\n",
      "loss: 1.256005  [ 2480/ 3200]\n",
      "loss: 0.780047  [ 2496/ 3200]\n",
      "loss: 0.757931  [ 2512/ 3200]\n",
      "loss: 0.850408  [ 2528/ 3200]\n",
      "loss: 1.212163  [ 2544/ 3200]\n",
      "loss: 0.961672  [ 2560/ 3200]\n",
      "loss: 0.890492  [ 2576/ 3200]\n",
      "loss: 0.929483  [ 2592/ 3200]\n",
      "loss: 1.345032  [ 2608/ 3200]\n",
      "loss: 1.160827  [ 2624/ 3200]\n",
      "loss: 0.830283  [ 2640/ 3200]\n",
      "loss: 1.514152  [ 2656/ 3200]\n",
      "loss: 1.140836  [ 2672/ 3200]\n",
      "loss: 1.025039  [ 2688/ 3200]\n",
      "loss: 0.954767  [ 2704/ 3200]\n",
      "loss: 0.986145  [ 2720/ 3200]\n",
      "loss: 0.911531  [ 2736/ 3200]\n",
      "loss: 0.974929  [ 2752/ 3200]\n",
      "loss: 0.841779  [ 2768/ 3200]\n",
      "loss: 0.898244  [ 2784/ 3200]\n",
      "loss: 0.932180  [ 2800/ 3200]\n",
      "loss: 0.995302  [ 2816/ 3200]\n",
      "loss: 0.920093  [ 2832/ 3200]\n",
      "loss: 0.967429  [ 2848/ 3200]\n",
      "loss: 1.043925  [ 2864/ 3200]\n",
      "loss: 1.213326  [ 2880/ 3200]\n",
      "loss: 0.652327  [ 2896/ 3200]\n",
      "loss: 1.157142  [ 2912/ 3200]\n",
      "loss: 0.720294  [ 2928/ 3200]\n",
      "loss: 1.105980  [ 2944/ 3200]\n",
      "loss: 1.194426  [ 2960/ 3200]\n",
      "loss: 0.895042  [ 2976/ 3200]\n",
      "loss: 1.060395  [ 2992/ 3200]\n",
      "loss: 1.313276  [ 3008/ 3200]\n",
      "loss: 0.973007  [ 3024/ 3200]\n",
      "loss: 0.947640  [ 3040/ 3200]\n",
      "loss: 1.124132  [ 3056/ 3200]\n",
      "loss: 1.291895  [ 3072/ 3200]\n",
      "loss: 0.981024  [ 3088/ 3200]\n",
      "loss: 0.911308  [ 3104/ 3200]\n",
      "loss: 0.858996  [ 3120/ 3200]\n",
      "loss: 0.969762  [ 3136/ 3200]\n",
      "loss: 0.976391  [ 3152/ 3200]\n",
      "loss: 0.937167  [ 3168/ 3200]\n",
      "loss: 1.142300  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.059393\n",
      "f1 macro averaged score: 0.644437\n",
      "Accuracy               : 65.1%\n",
      "Confusion matrix       :\n",
      "tensor([[148,  41,   5,   6],\n",
      "        [ 28,  64,  91,  17],\n",
      "        [  1,  24, 165,  10],\n",
      "        [  6,  26,  24, 144]])\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 1.025849  [    0/ 3200]\n",
      "loss: 0.868267  [   16/ 3200]\n",
      "loss: 1.218081  [   32/ 3200]\n",
      "loss: 1.106942  [   48/ 3200]\n",
      "loss: 1.051883  [   64/ 3200]\n",
      "loss: 1.217350  [   80/ 3200]\n",
      "loss: 1.011559  [   96/ 3200]\n",
      "loss: 0.952907  [  112/ 3200]\n",
      "loss: 1.030302  [  128/ 3200]\n",
      "loss: 1.053854  [  144/ 3200]\n",
      "loss: 0.969989  [  160/ 3200]\n",
      "loss: 1.324807  [  176/ 3200]\n",
      "loss: 0.969883  [  192/ 3200]\n",
      "loss: 0.999416  [  208/ 3200]\n",
      "loss: 1.011699  [  224/ 3200]\n",
      "loss: 0.928489  [  240/ 3200]\n",
      "loss: 0.840505  [  256/ 3200]\n",
      "loss: 0.759797  [  272/ 3200]\n",
      "loss: 0.667060  [  288/ 3200]\n",
      "loss: 0.835402  [  304/ 3200]\n",
      "loss: 1.013720  [  320/ 3200]\n",
      "loss: 1.287674  [  336/ 3200]\n",
      "loss: 1.213869  [  352/ 3200]\n",
      "loss: 0.913613  [  368/ 3200]\n",
      "loss: 0.913708  [  384/ 3200]\n",
      "loss: 0.991691  [  400/ 3200]\n",
      "loss: 1.019378  [  416/ 3200]\n",
      "loss: 1.108077  [  432/ 3200]\n",
      "loss: 0.862225  [  448/ 3200]\n",
      "loss: 0.944325  [  464/ 3200]\n",
      "loss: 0.957708  [  480/ 3200]\n",
      "loss: 0.775089  [  496/ 3200]\n",
      "loss: 0.973313  [  512/ 3200]\n",
      "loss: 1.138785  [  528/ 3200]\n",
      "loss: 0.824564  [  544/ 3200]\n",
      "loss: 0.762482  [  560/ 3200]\n",
      "loss: 1.107198  [  576/ 3200]\n",
      "loss: 0.755030  [  592/ 3200]\n",
      "loss: 1.127868  [  608/ 3200]\n",
      "loss: 1.279141  [  624/ 3200]\n",
      "loss: 0.774628  [  640/ 3200]\n",
      "loss: 0.951110  [  656/ 3200]\n",
      "loss: 0.937962  [  672/ 3200]\n",
      "loss: 1.040020  [  688/ 3200]\n",
      "loss: 0.847685  [  704/ 3200]\n",
      "loss: 1.249222  [  720/ 3200]\n",
      "loss: 0.827082  [  736/ 3200]\n",
      "loss: 1.025408  [  752/ 3200]\n",
      "loss: 1.032097  [  768/ 3200]\n",
      "loss: 0.998797  [  784/ 3200]\n",
      "loss: 1.244117  [  800/ 3200]\n",
      "loss: 0.979222  [  816/ 3200]\n",
      "loss: 1.059287  [  832/ 3200]\n",
      "loss: 1.060421  [  848/ 3200]\n",
      "loss: 1.103510  [  864/ 3200]\n",
      "loss: 1.164160  [  880/ 3200]\n",
      "loss: 0.800593  [  896/ 3200]\n",
      "loss: 1.042094  [  912/ 3200]\n",
      "loss: 0.809329  [  928/ 3200]\n",
      "loss: 0.997189  [  944/ 3200]\n",
      "loss: 0.775219  [  960/ 3200]\n",
      "loss: 0.939628  [  976/ 3200]\n",
      "loss: 0.995777  [  992/ 3200]\n",
      "loss: 0.857264  [ 1008/ 3200]\n",
      "loss: 0.935244  [ 1024/ 3200]\n",
      "loss: 1.003726  [ 1040/ 3200]\n",
      "loss: 0.697895  [ 1056/ 3200]\n",
      "loss: 0.982783  [ 1072/ 3200]\n",
      "loss: 0.806827  [ 1088/ 3200]\n",
      "loss: 1.013007  [ 1104/ 3200]\n",
      "loss: 0.827389  [ 1120/ 3200]\n",
      "loss: 1.120521  [ 1136/ 3200]\n",
      "loss: 0.784945  [ 1152/ 3200]\n",
      "loss: 0.887747  [ 1168/ 3200]\n",
      "loss: 1.032672  [ 1184/ 3200]\n",
      "loss: 0.812632  [ 1200/ 3200]\n",
      "loss: 0.876378  [ 1216/ 3200]\n",
      "loss: 1.119423  [ 1232/ 3200]\n",
      "loss: 1.083415  [ 1248/ 3200]\n",
      "loss: 0.951531  [ 1264/ 3200]\n",
      "loss: 1.058533  [ 1280/ 3200]\n",
      "loss: 1.192613  [ 1296/ 3200]\n",
      "loss: 1.094781  [ 1312/ 3200]\n",
      "loss: 0.814442  [ 1328/ 3200]\n",
      "loss: 0.837213  [ 1344/ 3200]\n",
      "loss: 0.810899  [ 1360/ 3200]\n",
      "loss: 0.997073  [ 1376/ 3200]\n",
      "loss: 0.950891  [ 1392/ 3200]\n",
      "loss: 1.141509  [ 1408/ 3200]\n",
      "loss: 0.885671  [ 1424/ 3200]\n",
      "loss: 1.272538  [ 1440/ 3200]\n",
      "loss: 1.085048  [ 1456/ 3200]\n",
      "loss: 1.068551  [ 1472/ 3200]\n",
      "loss: 1.313527  [ 1488/ 3200]\n",
      "loss: 1.019021  [ 1504/ 3200]\n",
      "loss: 0.923711  [ 1520/ 3200]\n",
      "loss: 1.061718  [ 1536/ 3200]\n",
      "loss: 1.005234  [ 1552/ 3200]\n",
      "loss: 0.996928  [ 1568/ 3200]\n",
      "loss: 1.089529  [ 1584/ 3200]\n",
      "loss: 0.950882  [ 1600/ 3200]\n",
      "loss: 0.797342  [ 1616/ 3200]\n",
      "loss: 1.042295  [ 1632/ 3200]\n",
      "loss: 1.206143  [ 1648/ 3200]\n",
      "loss: 0.855489  [ 1664/ 3200]\n",
      "loss: 0.848529  [ 1680/ 3200]\n",
      "loss: 0.968762  [ 1696/ 3200]\n",
      "loss: 0.883196  [ 1712/ 3200]\n",
      "loss: 1.113505  [ 1728/ 3200]\n",
      "loss: 1.234892  [ 1744/ 3200]\n",
      "loss: 0.835642  [ 1760/ 3200]\n",
      "loss: 0.995419  [ 1776/ 3200]\n",
      "loss: 1.016755  [ 1792/ 3200]\n",
      "loss: 1.013683  [ 1808/ 3200]\n",
      "loss: 0.982242  [ 1824/ 3200]\n",
      "loss: 1.061834  [ 1840/ 3200]\n",
      "loss: 0.836841  [ 1856/ 3200]\n",
      "loss: 0.970668  [ 1872/ 3200]\n",
      "loss: 1.255774  [ 1888/ 3200]\n",
      "loss: 1.029379  [ 1904/ 3200]\n",
      "loss: 0.786304  [ 1920/ 3200]\n",
      "loss: 0.924265  [ 1936/ 3200]\n",
      "loss: 0.775374  [ 1952/ 3200]\n",
      "loss: 1.103501  [ 1968/ 3200]\n",
      "loss: 0.903893  [ 1984/ 3200]\n",
      "loss: 1.207901  [ 2000/ 3200]\n",
      "loss: 1.098139  [ 2016/ 3200]\n",
      "loss: 0.785978  [ 2032/ 3200]\n",
      "loss: 1.344942  [ 2048/ 3200]\n",
      "loss: 1.331173  [ 2064/ 3200]\n",
      "loss: 1.090015  [ 2080/ 3200]\n",
      "loss: 0.951593  [ 2096/ 3200]\n",
      "loss: 1.003222  [ 2112/ 3200]\n",
      "loss: 0.911109  [ 2128/ 3200]\n",
      "loss: 0.574912  [ 2144/ 3200]\n",
      "loss: 0.989001  [ 2160/ 3200]\n",
      "loss: 0.902346  [ 2176/ 3200]\n",
      "loss: 1.327732  [ 2192/ 3200]\n",
      "loss: 0.837267  [ 2208/ 3200]\n",
      "loss: 0.836074  [ 2224/ 3200]\n",
      "loss: 0.889180  [ 2240/ 3200]\n",
      "loss: 1.095834  [ 2256/ 3200]\n",
      "loss: 0.972239  [ 2272/ 3200]\n",
      "loss: 1.140063  [ 2288/ 3200]\n",
      "loss: 0.749189  [ 2304/ 3200]\n",
      "loss: 1.182132  [ 2320/ 3200]\n",
      "loss: 0.890249  [ 2336/ 3200]\n",
      "loss: 1.141395  [ 2352/ 3200]\n",
      "loss: 1.004247  [ 2368/ 3200]\n",
      "loss: 0.807696  [ 2384/ 3200]\n",
      "loss: 0.890767  [ 2400/ 3200]\n",
      "loss: 0.792403  [ 2416/ 3200]\n",
      "loss: 1.383365  [ 2432/ 3200]\n",
      "loss: 0.784760  [ 2448/ 3200]\n",
      "loss: 0.979789  [ 2464/ 3200]\n",
      "loss: 0.922885  [ 2480/ 3200]\n",
      "loss: 0.923653  [ 2496/ 3200]\n",
      "loss: 0.898208  [ 2512/ 3200]\n",
      "loss: 0.996130  [ 2528/ 3200]\n",
      "loss: 0.880653  [ 2544/ 3200]\n",
      "loss: 0.853282  [ 2560/ 3200]\n",
      "loss: 0.965705  [ 2576/ 3200]\n",
      "loss: 1.011771  [ 2592/ 3200]\n",
      "loss: 0.856939  [ 2608/ 3200]\n",
      "loss: 0.846981  [ 2624/ 3200]\n",
      "loss: 0.898393  [ 2640/ 3200]\n",
      "loss: 1.164084  [ 2656/ 3200]\n",
      "loss: 1.136335  [ 2672/ 3200]\n",
      "loss: 1.078810  [ 2688/ 3200]\n",
      "loss: 0.991762  [ 2704/ 3200]\n",
      "loss: 1.027637  [ 2720/ 3200]\n",
      "loss: 0.969197  [ 2736/ 3200]\n",
      "loss: 1.140824  [ 2752/ 3200]\n",
      "loss: 1.023348  [ 2768/ 3200]\n",
      "loss: 0.775288  [ 2784/ 3200]\n",
      "loss: 1.098416  [ 2800/ 3200]\n",
      "loss: 1.021748  [ 2816/ 3200]\n",
      "loss: 0.988997  [ 2832/ 3200]\n",
      "loss: 0.894427  [ 2848/ 3200]\n",
      "loss: 0.913935  [ 2864/ 3200]\n",
      "loss: 0.978325  [ 2880/ 3200]\n",
      "loss: 0.973528  [ 2896/ 3200]\n",
      "loss: 1.068453  [ 2912/ 3200]\n",
      "loss: 0.920123  [ 2928/ 3200]\n",
      "loss: 0.658515  [ 2944/ 3200]\n",
      "loss: 1.209473  [ 2960/ 3200]\n",
      "loss: 0.890084  [ 2976/ 3200]\n",
      "loss: 0.962912  [ 2992/ 3200]\n",
      "loss: 1.351026  [ 3008/ 3200]\n",
      "loss: 1.266683  [ 3024/ 3200]\n",
      "loss: 1.189664  [ 3040/ 3200]\n",
      "loss: 0.947741  [ 3056/ 3200]\n",
      "loss: 0.874121  [ 3072/ 3200]\n",
      "loss: 0.937067  [ 3088/ 3200]\n",
      "loss: 0.997264  [ 3104/ 3200]\n",
      "loss: 1.091728  [ 3120/ 3200]\n",
      "loss: 0.761479  [ 3136/ 3200]\n",
      "loss: 1.031002  [ 3152/ 3200]\n",
      "loss: 0.985438  [ 3168/ 3200]\n",
      "loss: 1.107078  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.060171\n",
      "f1 macro averaged score: 0.613829\n",
      "Accuracy               : 62.6%\n",
      "Confusion matrix       :\n",
      "tensor([[193,   3,   3,   1],\n",
      "        [ 76,  73,  34,  17],\n",
      "        [ 20,  54, 111,  15],\n",
      "        [ 42,  26,   8, 124]])\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 1.128800  [    0/ 3200]\n",
      "loss: 0.976205  [   16/ 3200]\n",
      "loss: 1.158735  [   32/ 3200]\n",
      "loss: 1.170703  [   48/ 3200]\n",
      "loss: 1.203006  [   64/ 3200]\n",
      "loss: 1.028514  [   80/ 3200]\n",
      "loss: 1.046804  [   96/ 3200]\n",
      "loss: 0.992727  [  112/ 3200]\n",
      "loss: 1.097417  [  128/ 3200]\n",
      "loss: 1.265012  [  144/ 3200]\n",
      "loss: 0.835446  [  160/ 3200]\n",
      "loss: 1.057213  [  176/ 3200]\n",
      "loss: 1.152097  [  192/ 3200]\n",
      "loss: 1.448250  [  208/ 3200]\n",
      "loss: 0.821387  [  224/ 3200]\n",
      "loss: 0.655432  [  240/ 3200]\n",
      "loss: 1.093696  [  256/ 3200]\n",
      "loss: 0.917099  [  272/ 3200]\n",
      "loss: 0.920889  [  288/ 3200]\n",
      "loss: 1.016306  [  304/ 3200]\n",
      "loss: 0.954923  [  320/ 3200]\n",
      "loss: 0.903196  [  336/ 3200]\n",
      "loss: 1.157582  [  352/ 3200]\n",
      "loss: 0.914871  [  368/ 3200]\n",
      "loss: 0.677291  [  384/ 3200]\n",
      "loss: 0.877707  [  400/ 3200]\n",
      "loss: 0.717873  [  416/ 3200]\n",
      "loss: 0.876292  [  432/ 3200]\n",
      "loss: 1.059156  [  448/ 3200]\n",
      "loss: 0.930965  [  464/ 3200]\n",
      "loss: 1.122585  [  480/ 3200]\n",
      "loss: 0.777280  [  496/ 3200]\n",
      "loss: 1.190275  [  512/ 3200]\n",
      "loss: 0.839465  [  528/ 3200]\n",
      "loss: 1.063289  [  544/ 3200]\n",
      "loss: 0.708970  [  560/ 3200]\n",
      "loss: 0.897939  [  576/ 3200]\n",
      "loss: 0.873726  [  592/ 3200]\n",
      "loss: 1.007787  [  608/ 3200]\n",
      "loss: 0.950051  [  624/ 3200]\n",
      "loss: 0.956711  [  640/ 3200]\n",
      "loss: 1.177703  [  656/ 3200]\n",
      "loss: 1.014947  [  672/ 3200]\n",
      "loss: 1.105349  [  688/ 3200]\n",
      "loss: 0.956792  [  704/ 3200]\n",
      "loss: 1.112085  [  720/ 3200]\n",
      "loss: 0.979695  [  736/ 3200]\n",
      "loss: 0.885217  [  752/ 3200]\n",
      "loss: 1.116183  [  768/ 3200]\n",
      "loss: 0.903898  [  784/ 3200]\n",
      "loss: 0.956392  [  800/ 3200]\n",
      "loss: 0.831469  [  816/ 3200]\n",
      "loss: 0.868379  [  832/ 3200]\n",
      "loss: 0.997628  [  848/ 3200]\n",
      "loss: 0.938044  [  864/ 3200]\n",
      "loss: 0.904460  [  880/ 3200]\n",
      "loss: 1.229074  [  896/ 3200]\n",
      "loss: 0.928384  [  912/ 3200]\n",
      "loss: 0.941543  [  928/ 3200]\n",
      "loss: 0.916787  [  944/ 3200]\n",
      "loss: 0.951276  [  960/ 3200]\n",
      "loss: 1.010240  [  976/ 3200]\n",
      "loss: 0.991224  [  992/ 3200]\n",
      "loss: 1.049948  [ 1008/ 3200]\n",
      "loss: 0.787235  [ 1024/ 3200]\n",
      "loss: 0.962575  [ 1040/ 3200]\n",
      "loss: 0.664966  [ 1056/ 3200]\n",
      "loss: 0.767918  [ 1072/ 3200]\n",
      "loss: 0.774080  [ 1088/ 3200]\n",
      "loss: 0.833821  [ 1104/ 3200]\n",
      "loss: 1.221148  [ 1120/ 3200]\n",
      "loss: 1.119359  [ 1136/ 3200]\n",
      "loss: 0.905347  [ 1152/ 3200]\n",
      "loss: 0.997111  [ 1168/ 3200]\n",
      "loss: 1.021117  [ 1184/ 3200]\n",
      "loss: 0.978748  [ 1200/ 3200]\n",
      "loss: 0.709389  [ 1216/ 3200]\n",
      "loss: 1.077623  [ 1232/ 3200]\n",
      "loss: 1.005931  [ 1248/ 3200]\n",
      "loss: 0.942495  [ 1264/ 3200]\n",
      "loss: 0.971421  [ 1280/ 3200]\n",
      "loss: 0.992533  [ 1296/ 3200]\n",
      "loss: 0.956800  [ 1312/ 3200]\n",
      "loss: 0.943638  [ 1328/ 3200]\n",
      "loss: 1.149935  [ 1344/ 3200]\n",
      "loss: 1.120811  [ 1360/ 3200]\n",
      "loss: 1.014987  [ 1376/ 3200]\n",
      "loss: 1.125912  [ 1392/ 3200]\n",
      "loss: 0.913484  [ 1408/ 3200]\n",
      "loss: 0.995626  [ 1424/ 3200]\n",
      "loss: 1.049791  [ 1440/ 3200]\n",
      "loss: 1.049191  [ 1456/ 3200]\n",
      "loss: 0.865182  [ 1472/ 3200]\n",
      "loss: 0.946651  [ 1488/ 3200]\n",
      "loss: 0.966277  [ 1504/ 3200]\n",
      "loss: 0.627407  [ 1520/ 3200]\n",
      "loss: 0.819071  [ 1536/ 3200]\n",
      "loss: 1.130330  [ 1552/ 3200]\n",
      "loss: 1.048925  [ 1568/ 3200]\n",
      "loss: 0.931796  [ 1584/ 3200]\n",
      "loss: 0.758463  [ 1600/ 3200]\n",
      "loss: 1.001876  [ 1616/ 3200]\n",
      "loss: 1.048289  [ 1632/ 3200]\n",
      "loss: 1.156923  [ 1648/ 3200]\n",
      "loss: 0.922406  [ 1664/ 3200]\n",
      "loss: 0.852378  [ 1680/ 3200]\n",
      "loss: 0.851560  [ 1696/ 3200]\n",
      "loss: 1.124505  [ 1712/ 3200]\n",
      "loss: 1.500534  [ 1728/ 3200]\n",
      "loss: 1.086333  [ 1744/ 3200]\n",
      "loss: 0.981209  [ 1760/ 3200]\n",
      "loss: 1.170525  [ 1776/ 3200]\n",
      "loss: 0.913580  [ 1792/ 3200]\n",
      "loss: 1.006546  [ 1808/ 3200]\n",
      "loss: 1.038593  [ 1824/ 3200]\n",
      "loss: 0.890090  [ 1840/ 3200]\n",
      "loss: 0.952782  [ 1856/ 3200]\n",
      "loss: 0.755650  [ 1872/ 3200]\n",
      "loss: 0.902582  [ 1888/ 3200]\n",
      "loss: 0.828589  [ 1904/ 3200]\n",
      "loss: 1.025530  [ 1920/ 3200]\n",
      "loss: 1.154220  [ 1936/ 3200]\n",
      "loss: 0.922238  [ 1952/ 3200]\n",
      "loss: 0.856791  [ 1968/ 3200]\n",
      "loss: 0.592747  [ 1984/ 3200]\n",
      "loss: 0.968931  [ 2000/ 3200]\n",
      "loss: 0.941195  [ 2016/ 3200]\n",
      "loss: 1.028811  [ 2032/ 3200]\n",
      "loss: 0.893372  [ 2048/ 3200]\n",
      "loss: 0.990170  [ 2064/ 3200]\n",
      "loss: 1.017388  [ 2080/ 3200]\n",
      "loss: 1.309782  [ 2096/ 3200]\n",
      "loss: 1.097523  [ 2112/ 3200]\n",
      "loss: 1.236732  [ 2128/ 3200]\n",
      "loss: 0.977807  [ 2144/ 3200]\n",
      "loss: 0.875352  [ 2160/ 3200]\n",
      "loss: 0.977259  [ 2176/ 3200]\n",
      "loss: 1.129501  [ 2192/ 3200]\n",
      "loss: 0.957115  [ 2208/ 3200]\n",
      "loss: 1.045957  [ 2224/ 3200]\n",
      "loss: 0.741322  [ 2240/ 3200]\n",
      "loss: 0.967099  [ 2256/ 3200]\n",
      "loss: 0.911633  [ 2272/ 3200]\n",
      "loss: 1.034459  [ 2288/ 3200]\n",
      "loss: 1.033227  [ 2304/ 3200]\n",
      "loss: 0.910639  [ 2320/ 3200]\n",
      "loss: 0.901594  [ 2336/ 3200]\n",
      "loss: 0.812877  [ 2352/ 3200]\n",
      "loss: 0.944614  [ 2368/ 3200]\n",
      "loss: 0.930912  [ 2384/ 3200]\n",
      "loss: 0.923146  [ 2400/ 3200]\n",
      "loss: 0.818924  [ 2416/ 3200]\n",
      "loss: 0.884084  [ 2432/ 3200]\n",
      "loss: 0.745699  [ 2448/ 3200]\n",
      "loss: 0.909304  [ 2464/ 3200]\n",
      "loss: 1.166740  [ 2480/ 3200]\n",
      "loss: 1.051901  [ 2496/ 3200]\n",
      "loss: 0.656219  [ 2512/ 3200]\n",
      "loss: 0.969616  [ 2528/ 3200]\n",
      "loss: 0.835200  [ 2544/ 3200]\n",
      "loss: 1.083643  [ 2560/ 3200]\n",
      "loss: 1.035085  [ 2576/ 3200]\n",
      "loss: 1.128123  [ 2592/ 3200]\n",
      "loss: 0.889349  [ 2608/ 3200]\n",
      "loss: 1.134359  [ 2624/ 3200]\n",
      "loss: 1.129456  [ 2640/ 3200]\n",
      "loss: 0.813604  [ 2656/ 3200]\n",
      "loss: 1.051605  [ 2672/ 3200]\n",
      "loss: 1.075438  [ 2688/ 3200]\n",
      "loss: 0.983959  [ 2704/ 3200]\n",
      "loss: 1.104574  [ 2720/ 3200]\n",
      "loss: 1.006415  [ 2736/ 3200]\n",
      "loss: 1.012453  [ 2752/ 3200]\n",
      "loss: 0.798289  [ 2768/ 3200]\n",
      "loss: 1.094098  [ 2784/ 3200]\n",
      "loss: 0.944292  [ 2800/ 3200]\n",
      "loss: 1.126610  [ 2816/ 3200]\n",
      "loss: 0.989479  [ 2832/ 3200]\n",
      "loss: 0.862612  [ 2848/ 3200]\n",
      "loss: 1.077224  [ 2864/ 3200]\n",
      "loss: 0.999849  [ 2880/ 3200]\n",
      "loss: 0.984482  [ 2896/ 3200]\n",
      "loss: 1.086094  [ 2912/ 3200]\n",
      "loss: 0.944312  [ 2928/ 3200]\n",
      "loss: 0.791021  [ 2944/ 3200]\n",
      "loss: 1.487216  [ 2960/ 3200]\n",
      "loss: 1.008515  [ 2976/ 3200]\n",
      "loss: 1.050534  [ 2992/ 3200]\n",
      "loss: 0.958172  [ 3008/ 3200]\n",
      "loss: 0.950641  [ 3024/ 3200]\n",
      "loss: 1.023807  [ 3040/ 3200]\n",
      "loss: 1.106692  [ 3056/ 3200]\n",
      "loss: 0.873582  [ 3072/ 3200]\n",
      "loss: 0.814940  [ 3088/ 3200]\n",
      "loss: 1.184570  [ 3104/ 3200]\n",
      "loss: 1.028091  [ 3120/ 3200]\n",
      "loss: 1.302938  [ 3136/ 3200]\n",
      "loss: 0.990763  [ 3152/ 3200]\n",
      "loss: 0.717716  [ 3168/ 3200]\n",
      "loss: 0.932708  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.058171\n",
      "f1 macro averaged score: 0.623238\n",
      "Accuracy               : 64.4%\n",
      "Confusion matrix       :\n",
      "tensor([[185,   5,   3,   7],\n",
      "        [ 63,  64,  25,  48],\n",
      "        [  9,  49, 105,  37],\n",
      "        [ 18,  14,   7, 161]])\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 1.006506  [    0/ 3200]\n",
      "loss: 0.877895  [   16/ 3200]\n",
      "loss: 1.089232  [   32/ 3200]\n",
      "loss: 1.004200  [   48/ 3200]\n",
      "loss: 0.888645  [   64/ 3200]\n",
      "loss: 0.863593  [   80/ 3200]\n",
      "loss: 1.030323  [   96/ 3200]\n",
      "loss: 0.894218  [  112/ 3200]\n",
      "loss: 0.909651  [  128/ 3200]\n",
      "loss: 1.176637  [  144/ 3200]\n",
      "loss: 0.887212  [  160/ 3200]\n",
      "loss: 1.097092  [  176/ 3200]\n",
      "loss: 0.661783  [  192/ 3200]\n",
      "loss: 0.890786  [  208/ 3200]\n",
      "loss: 1.025289  [  224/ 3200]\n",
      "loss: 0.985936  [  240/ 3200]\n",
      "loss: 0.932972  [  256/ 3200]\n",
      "loss: 1.079790  [  272/ 3200]\n",
      "loss: 1.018072  [  288/ 3200]\n",
      "loss: 0.919947  [  304/ 3200]\n",
      "loss: 1.330079  [  320/ 3200]\n",
      "loss: 0.835357  [  336/ 3200]\n",
      "loss: 0.994127  [  352/ 3200]\n",
      "loss: 1.125231  [  368/ 3200]\n",
      "loss: 0.979646  [  384/ 3200]\n",
      "loss: 1.144490  [  400/ 3200]\n",
      "loss: 1.110582  [  416/ 3200]\n",
      "loss: 1.170988  [  432/ 3200]\n",
      "loss: 0.890789  [  448/ 3200]\n",
      "loss: 0.871547  [  464/ 3200]\n",
      "loss: 1.007417  [  480/ 3200]\n",
      "loss: 1.014480  [  496/ 3200]\n",
      "loss: 0.882667  [  512/ 3200]\n",
      "loss: 0.982590  [  528/ 3200]\n",
      "loss: 1.094533  [  544/ 3200]\n",
      "loss: 0.715658  [  560/ 3200]\n",
      "loss: 1.193709  [  576/ 3200]\n",
      "loss: 1.200414  [  592/ 3200]\n",
      "loss: 0.804006  [  608/ 3200]\n",
      "loss: 0.848118  [  624/ 3200]\n",
      "loss: 0.813199  [  640/ 3200]\n",
      "loss: 1.310055  [  656/ 3200]\n",
      "loss: 0.975505  [  672/ 3200]\n",
      "loss: 0.842397  [  688/ 3200]\n",
      "loss: 1.399107  [  704/ 3200]\n",
      "loss: 0.865778  [  720/ 3200]\n",
      "loss: 1.114900  [  736/ 3200]\n",
      "loss: 1.149424  [  752/ 3200]\n",
      "loss: 0.879542  [  768/ 3200]\n",
      "loss: 0.957704  [  784/ 3200]\n",
      "loss: 0.775248  [  800/ 3200]\n",
      "loss: 0.934601  [  816/ 3200]\n",
      "loss: 1.116208  [  832/ 3200]\n",
      "loss: 1.299361  [  848/ 3200]\n",
      "loss: 0.937736  [  864/ 3200]\n",
      "loss: 1.084596  [  880/ 3200]\n",
      "loss: 1.083675  [  896/ 3200]\n",
      "loss: 1.015098  [  912/ 3200]\n",
      "loss: 0.871301  [  928/ 3200]\n",
      "loss: 0.771134  [  944/ 3200]\n",
      "loss: 1.074816  [  960/ 3200]\n",
      "loss: 0.836416  [  976/ 3200]\n",
      "loss: 0.997874  [  992/ 3200]\n",
      "loss: 0.850648  [ 1008/ 3200]\n",
      "loss: 1.102032  [ 1024/ 3200]\n",
      "loss: 0.939923  [ 1040/ 3200]\n",
      "loss: 0.828398  [ 1056/ 3200]\n",
      "loss: 1.035439  [ 1072/ 3200]\n",
      "loss: 0.743976  [ 1088/ 3200]\n",
      "loss: 1.096205  [ 1104/ 3200]\n",
      "loss: 0.951735  [ 1120/ 3200]\n",
      "loss: 0.840026  [ 1136/ 3200]\n",
      "loss: 1.094491  [ 1152/ 3200]\n",
      "loss: 0.818814  [ 1168/ 3200]\n",
      "loss: 0.998965  [ 1184/ 3200]\n",
      "loss: 1.041484  [ 1200/ 3200]\n",
      "loss: 1.015516  [ 1216/ 3200]\n",
      "loss: 0.942126  [ 1232/ 3200]\n",
      "loss: 0.797676  [ 1248/ 3200]\n",
      "loss: 1.212986  [ 1264/ 3200]\n",
      "loss: 1.052306  [ 1280/ 3200]\n",
      "loss: 0.702398  [ 1296/ 3200]\n",
      "loss: 1.076820  [ 1312/ 3200]\n",
      "loss: 0.982545  [ 1328/ 3200]\n",
      "loss: 1.091593  [ 1344/ 3200]\n",
      "loss: 1.230263  [ 1360/ 3200]\n",
      "loss: 0.885224  [ 1376/ 3200]\n",
      "loss: 0.904261  [ 1392/ 3200]\n",
      "loss: 0.822279  [ 1408/ 3200]\n",
      "loss: 1.208796  [ 1424/ 3200]\n",
      "loss: 1.067982  [ 1440/ 3200]\n",
      "loss: 0.828832  [ 1456/ 3200]\n",
      "loss: 0.954681  [ 1472/ 3200]\n",
      "loss: 0.797284  [ 1488/ 3200]\n",
      "loss: 1.018087  [ 1504/ 3200]\n",
      "loss: 1.058177  [ 1520/ 3200]\n",
      "loss: 0.874748  [ 1536/ 3200]\n",
      "loss: 1.124178  [ 1552/ 3200]\n",
      "loss: 0.816292  [ 1568/ 3200]\n",
      "loss: 0.820334  [ 1584/ 3200]\n",
      "loss: 1.124344  [ 1600/ 3200]\n",
      "loss: 1.052129  [ 1616/ 3200]\n",
      "loss: 0.888113  [ 1632/ 3200]\n",
      "loss: 0.968914  [ 1648/ 3200]\n",
      "loss: 1.039335  [ 1664/ 3200]\n",
      "loss: 0.941196  [ 1680/ 3200]\n",
      "loss: 0.841472  [ 1696/ 3200]\n",
      "loss: 0.862112  [ 1712/ 3200]\n",
      "loss: 0.898327  [ 1728/ 3200]\n",
      "loss: 0.990363  [ 1744/ 3200]\n",
      "loss: 0.678175  [ 1760/ 3200]\n",
      "loss: 0.766074  [ 1776/ 3200]\n",
      "loss: 1.242482  [ 1792/ 3200]\n",
      "loss: 1.188370  [ 1808/ 3200]\n",
      "loss: 0.862669  [ 1824/ 3200]\n",
      "loss: 1.475657  [ 1840/ 3200]\n",
      "loss: 0.908453  [ 1856/ 3200]\n",
      "loss: 1.046791  [ 1872/ 3200]\n",
      "loss: 0.987373  [ 1888/ 3200]\n",
      "loss: 0.964478  [ 1904/ 3200]\n",
      "loss: 0.876080  [ 1920/ 3200]\n",
      "loss: 0.841583  [ 1936/ 3200]\n",
      "loss: 0.880465  [ 1952/ 3200]\n",
      "loss: 0.909368  [ 1968/ 3200]\n",
      "loss: 1.169196  [ 1984/ 3200]\n",
      "loss: 1.060597  [ 2000/ 3200]\n",
      "loss: 0.959843  [ 2016/ 3200]\n",
      "loss: 1.561267  [ 2032/ 3200]\n",
      "loss: 0.881990  [ 2048/ 3200]\n",
      "loss: 1.021973  [ 2064/ 3200]\n",
      "loss: 0.930205  [ 2080/ 3200]\n",
      "loss: 1.132628  [ 2096/ 3200]\n",
      "loss: 0.709290  [ 2112/ 3200]\n",
      "loss: 0.949888  [ 2128/ 3200]\n",
      "loss: 0.715336  [ 2144/ 3200]\n",
      "loss: 0.936233  [ 2160/ 3200]\n",
      "loss: 0.870961  [ 2176/ 3200]\n",
      "loss: 1.018465  [ 2192/ 3200]\n",
      "loss: 1.248581  [ 2208/ 3200]\n",
      "loss: 0.842449  [ 2224/ 3200]\n",
      "loss: 1.058637  [ 2240/ 3200]\n",
      "loss: 0.894650  [ 2256/ 3200]\n",
      "loss: 1.132357  [ 2272/ 3200]\n",
      "loss: 0.924558  [ 2288/ 3200]\n",
      "loss: 0.850087  [ 2304/ 3200]\n",
      "loss: 0.934512  [ 2320/ 3200]\n",
      "loss: 0.747043  [ 2336/ 3200]\n",
      "loss: 0.910268  [ 2352/ 3200]\n",
      "loss: 1.367303  [ 2368/ 3200]\n",
      "loss: 0.854392  [ 2384/ 3200]\n",
      "loss: 0.960960  [ 2400/ 3200]\n",
      "loss: 1.045453  [ 2416/ 3200]\n",
      "loss: 1.365480  [ 2432/ 3200]\n",
      "loss: 1.185352  [ 2448/ 3200]\n",
      "loss: 1.049664  [ 2464/ 3200]\n",
      "loss: 1.102400  [ 2480/ 3200]\n",
      "loss: 1.063130  [ 2496/ 3200]\n",
      "loss: 1.101870  [ 2512/ 3200]\n",
      "loss: 0.872378  [ 2528/ 3200]\n",
      "loss: 1.010982  [ 2544/ 3200]\n",
      "loss: 1.009256  [ 2560/ 3200]\n",
      "loss: 1.062437  [ 2576/ 3200]\n",
      "loss: 0.786404  [ 2592/ 3200]\n",
      "loss: 0.629073  [ 2608/ 3200]\n",
      "loss: 1.018712  [ 2624/ 3200]\n",
      "loss: 0.869009  [ 2640/ 3200]\n",
      "loss: 1.063912  [ 2656/ 3200]\n",
      "loss: 0.938509  [ 2672/ 3200]\n",
      "loss: 0.881830  [ 2688/ 3200]\n",
      "loss: 0.996105  [ 2704/ 3200]\n",
      "loss: 1.089413  [ 2720/ 3200]\n",
      "loss: 0.832766  [ 2736/ 3200]\n",
      "loss: 0.917268  [ 2752/ 3200]\n",
      "loss: 0.734854  [ 2768/ 3200]\n",
      "loss: 0.837793  [ 2784/ 3200]\n",
      "loss: 1.435879  [ 2800/ 3200]\n",
      "loss: 0.865469  [ 2816/ 3200]\n",
      "loss: 0.839847  [ 2832/ 3200]\n",
      "loss: 0.617180  [ 2848/ 3200]\n",
      "loss: 1.327347  [ 2864/ 3200]\n",
      "loss: 0.913823  [ 2880/ 3200]\n",
      "loss: 1.105276  [ 2896/ 3200]\n",
      "loss: 0.943363  [ 2912/ 3200]\n",
      "loss: 1.055735  [ 2928/ 3200]\n",
      "loss: 1.381329  [ 2944/ 3200]\n",
      "loss: 1.259798  [ 2960/ 3200]\n",
      "loss: 1.057990  [ 2976/ 3200]\n",
      "loss: 0.863228  [ 2992/ 3200]\n",
      "loss: 0.834196  [ 3008/ 3200]\n",
      "loss: 0.500293  [ 3024/ 3200]\n",
      "loss: 1.149833  [ 3040/ 3200]\n",
      "loss: 1.316749  [ 3056/ 3200]\n",
      "loss: 0.994572  [ 3072/ 3200]\n",
      "loss: 1.076541  [ 3088/ 3200]\n",
      "loss: 0.992577  [ 3104/ 3200]\n",
      "loss: 0.755421  [ 3120/ 3200]\n",
      "loss: 0.900595  [ 3136/ 3200]\n",
      "loss: 1.107294  [ 3152/ 3200]\n",
      "loss: 0.777373  [ 3168/ 3200]\n",
      "loss: 0.941507  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.057575\n",
      "f1 macro averaged score: 0.612269\n",
      "Accuracy               : 64.5%\n",
      "Confusion matrix       :\n",
      "tensor([[185,   8,   6,   1],\n",
      "        [ 56,  34,  98,  12],\n",
      "        [  6,  20, 169,   5],\n",
      "        [ 15,  29,  28, 128]])\n",
      "\n",
      "Best epoch: 26 with f1 macro averaged score: 0.6554416418075562\n",
      "CPU times: user 10.1 s, sys: 445 ms, total: 10.6 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_model, f1_per_epoch = validate_neural_network(epochs, optimizer, train_dataloader, val_dataloader, loss_function, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0eRDNsdh-BK"
   },
   "source": [
    "The best epoch for this execution is the $26$th.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IG3NNSuxqHrr"
   },
   "source": [
    "Plot f1 macro averaged score values for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "eKPiTAZEilCB",
    "outputId": "1c72dea7-461a-49df-e6cf-bcc3e0d894f5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, epochs + 1), f1_per_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('f1 macro averaged score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVFeQz1uo9w_"
   },
   "source": [
    "From the graph shown above, we can verify that the f1 macro averaged score reaches its maximum in the $26$th epoch out of the $30$ epochs tested.\n",
    "\n",
    "What we can observe is that there is a decrease after the $26$th epoch until the $30$th epoch, which is most likely caused because of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJ4xjGU1qGAH"
   },
   "source": [
    "Test the best Neural Network on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uk0DvVg7qGa7",
    "outputId": "ca1f489e-2896-4b63-c2ba-ba936daf371c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:\n",
      "Avg loss               : 0.062293\n",
      "f1 macro averaged score: 0.590420\n",
      "Accuracy               : 61.6%\n",
      "Confusion matrix       :\n",
      "tensor([[255,  22,  18,   2],\n",
      "        [ 39,  69, 145,  71],\n",
      "        [ 24,  29, 295,   8],\n",
      "        [ 46,  52,  72, 229]])\n",
      "CPU times: user 46.5 ms, sys: 0 ns, total: 46.5 ms\n",
      "Wall time: 47.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = test_neural_network(test_dataloader, loss_function, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgYSNYYNrH2_"
   },
   "source": [
    "The best model yields slightly better results on the test dataset than the models used in **Step 4** and **Step 5**.\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Yk4hf_VrtsO"
   },
   "source": [
    "# Question 2 - Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-StCxTcr1rY"
   },
   "source": [
    "### Step 1 - Load data (mfccs)\n",
    "\n",
    "Load train, test and validation data (spectrograms-melgrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWBFd_j4rsrN"
   },
   "outputs": [],
   "source": [
    "melgram_x_train = np.load('/content/music_genre_data_di/train/melgrams/X.npy')\n",
    "melgram_y_train = np.load('/content/music_genre_data_di/train/melgrams/labels.npy')\n",
    "\n",
    "melgram_x_test  = np.load('/content/music_genre_data_di/test/melgrams/X.npy')\n",
    "melgram_y_test  = np.load('/content/music_genre_data_di/test/melgrams/labels.npy')\n",
    "\n",
    "melgram_x_val   = np.load('/content/music_genre_data_di/val/melgrams/X.npy')\n",
    "melgram_y_val   = np.load('/content/music_genre_data_di/val/melgrams/labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "th75hT8irwND"
   },
   "source": [
    "Convert labels from strings <code>classical</code>, <code>blues</code>, <code>hiphop</code> and <code>rock_metal_hardrock</code> to integer values <code>0</code>, <code>1</code>, <code>2</code> and <code>3</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYYYiMr1sH7I"
   },
   "outputs": [],
   "source": [
    "melgram_y_train = convert_labels(melgram_y_train)\n",
    "melgram_y_test  = convert_labels(melgram_y_test)\n",
    "melgram_y_val   = convert_labels(melgram_y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4MPokuwsMzW"
   },
   "source": [
    "Use Pytorch Dataloaders to load data into the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLQJR_kGsOcS"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(list(zip(melgram_x_train, melgram_y_train)), batch_size=16, shuffle=True)\n",
    "test_dataloader  = DataLoader(list(zip(melgram_x_test, melgram_y_test)), batch_size=16, shuffle=True)\n",
    "val_dataloader   = DataLoader(list(zip(melgram_x_val, melgram_y_val)), batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4C_jF4MrmxB"
   },
   "source": [
    "Visualize a random melgram for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "dytOT0u-rrC1",
    "outputId": "83badda0-5fef-494f-f0fb-5ed2661fd02b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# invert label dict\n",
    "labels_int_to_str = dict(map(reversed, labels_str_to_int.items()))\n",
    "\n",
    "melgrams_for_vis = []\n",
    "while len(melgrams_for_vis) != 4:\n",
    "  index = random.randint(0, 3200)\n",
    "  # if haven't found a melgram with the same class as the random selected melgram\n",
    "  if melgram_y_train[index] not in [data[1] for data in melgrams_for_vis]:\n",
    "    # add it to list\n",
    "    melgrams_for_vis.append((index, melgram_y_train[index]))\n",
    "\n",
    "# sort list for better visualization\n",
    "melgrams_for_vis = sorted(melgrams_for_vis, key=lambda x: x[1])\n",
    "melgrams_indices = [data[0] for data in melgrams_for_vis]\n",
    "\n",
    "for i, index in enumerate(melgrams_indices):\n",
    "  melgram_x, melgram_y = melgram_x_train[index], melgram_y_train[index]\n",
    "  plt.figure(figsize=(7, 5))\n",
    "  plt.title(f'Melgram of a random song with label \\'{labels_int_to_str[melgram_y_train[index]]}\\'')\n",
    "  plt.axis('off')\n",
    "  plt.imshow(melgram_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caadKA2E-MP9"
   },
   "source": [
    "### Step 2 - Definition of the Convolutional Neural Network\n",
    "\n",
    "The Convolutional Neural Network that will be used consists of $4$ convolutional layers with kernel size $5$, so that the following channel (filter) sequence is taken: $1$, $16$, $32$, $64$ and $128$.\n",
    "\n",
    "The output of the Convolutional Neural Network defined above should be considered as the input of a fully connected Neural Network that consists of $5$ layers with (cnn_out_dimensions), $1024$, $256$, $32$ and (out_dim) number of perceptrons respectively.\n",
    "\n",
    "The first layer of the fully connected Neural Network is the input layer and it consists of $1024$ perceptrons, because the input size is $1024$.\n",
    "\n",
    "The next three layers that consist of $1024$, $256$ and $32$ perceptrons respectively are the hidden layers.\n",
    "\n",
    "The last layer is the output layer and it consists of $4$ perceptrons, because the number of different labels is $4$.\n",
    "<br></br>\n",
    "\n",
    "The code shown in compimentary courses has been modified in order to implement the Neural Network that was described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuIdBxTJ5iq4"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # convolutional layers\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5)   # 1 channel   -> 16 channels\n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)  # 16 channels -> 32 channels\n",
    "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)  # 32 channels -> 64 channels\n",
    "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5) # 64 channels -> 128 channels\n",
    "\n",
    "    # fully connected (dense) layers\n",
    "    self.dense1 = nn.Linear(1024, 1024) # input size = 1024 -> number of perceptrons in 1st hidden layer = 1024\n",
    "    self.dense2 = nn.Linear(1024, 256)  # number of perceptrons in 1st hidden layer = 1024 ->\n",
    "                                        # number of perceptrons in 2nd hidden layer = 256\n",
    "    self.dense3 = nn.Linear(256, 32)    # number of perceptrons in 2nd hidden layer = 256 ->\n",
    "                                        # number of perceptrons in 3rd hidden layer = 32\n",
    "    self.dense4 = nn.Linear(32, 4)      # number of perceptrons in 3rd hidden layer = 32 ->\n",
    "                                        # number of different labels = 4\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.conv4(x)\n",
    "\n",
    "    x = torch.flatten(x, 1)\n",
    "\n",
    "    x = self.dense1(x)\n",
    "    x = self.dense2(x)\n",
    "    x = self.dense3(x)\n",
    "    x = self.dense4(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUmUGhm7w-l8"
   },
   "source": [
    "### Step 3 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TME2XOVxp7A"
   },
   "source": [
    "Initialize model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7B1GFNTxiiC",
    "outputId": "3442702a-5ff0-49f6-c7a5-bf2b5d855776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (dense3): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (dense4): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Net().to(device)\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1aQc8AM2Nh-"
   },
   "source": [
    "Define the training procedure, similar to the training procedure that was used for the Neural Network in **Question 1**.\n",
    "\n",
    "The code shown in compimentary courses has been modified in order to implement the training procedure was described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyrHasRr2NEP"
   },
   "outputs": [],
   "source": [
    "def train_convolutional_neural_network(epochs, optimizer, dataloader, loss_function, model):\n",
    "  size = len(dataloader.dataset)\n",
    "\n",
    "  for epoch in range(0, epochs):\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"-----------------------------\")\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(torch.unsqueeze(x, 1).type(torch.float))\n",
    "      loss = loss_function(prediction, y)\n",
    "\n",
    "      # backpropagation\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss, current = loss.item(), batch * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    print()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IT_G7pEl4BWL"
   },
   "source": [
    "Define the testing procedure, similar to the testing procedure that was used for the Neural Network in **Question 1**.\n",
    "\n",
    "The code shown in compimentary courses has been modified in order to implement the testing procedure was described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXLL8SkE4FZn"
   },
   "outputs": [],
   "source": [
    "def test_convolutional_neural_network(dataloader, loss_function, model):\n",
    "  size = len(dataloader.dataset)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  predictions = []\n",
    "  labels = []\n",
    "  with torch.no_grad():\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(torch.unsqueeze(x, 1))\n",
    "      predictions.append(prediction.argmax(1))\n",
    "      test_loss += loss_function(prediction, y)\n",
    "      correct += (prediction.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "      # keep labels for later\n",
    "      labels.append(y)\n",
    "\n",
    "  test_loss /= size\n",
    "  accuracy = 100 * (correct / size)\n",
    "  predictions = torch.cat(predictions)\n",
    "  labels = torch.cat(labels)\n",
    "  f1_macro_avg = f1_score(predictions, labels, task='multiclass', num_classes=4, average='macro')\n",
    "\n",
    "  # move confusion matrix to GPU if GPU is in use\n",
    "  confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=4).to(device)\n",
    "\n",
    "  print(\"Test Error:\")\n",
    "  print(f\"Avg loss               : {test_loss:>8f}\")\n",
    "  print(f\"f1 macro averaged score: {f1_macro_avg:>8f}\")\n",
    "  print(f\"Accuracy               : {accuracy:>0.1f}%\")\n",
    "  print(f\"Confusion matrix       :\")\n",
    "  print(confusion_matrix(predictions, labels))\n",
    "\n",
    "  return test_loss, f1_macro_avg, accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhOfUiwJ7Ite"
   },
   "source": [
    "Define the validation procedure, similar to the validation procedure that was used for the Neural Network in **Question 1**.\n",
    "\n",
    "The code shown in compimentary courses has been modified in order to implement the validation procedure was described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYT56kzM7JCQ"
   },
   "outputs": [],
   "source": [
    "def validate_convolutional_neural_network(epochs, optimizer, train_dataloader, val_dataloader, loss_function, model):\n",
    "  size = len(train_dataloader.dataset)\n",
    "\n",
    "  best = (0, 0, 0) # best model, best epoch, best f1 score\n",
    "  f1_list = []\n",
    "\n",
    "  for epoch in range(0, epochs):\n",
    "    # train model with train data\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"-----------------------------\")\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(torch.unsqueeze(x, 1))\n",
    "      loss = loss_function(prediction, y)\n",
    "\n",
    "      # backpropagation\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss, current = loss.item(), batch * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # test model on validation set\n",
    "    results = test_convolutional_neural_network(val_dataloader, loss_function, model)\n",
    "    f1_macro_avg = results[1]\n",
    "    f1_list.append(f1_macro_avg)\n",
    "    if f1_macro_avg > best[2]:\n",
    "      best = (model, epoch, f1_macro_avg)\n",
    "    print()\n",
    "\n",
    "  print(f\"Best epoch: {(best[1] + 1)} with f1 macro averaged score: {best[2]}\")\n",
    "  return best[0], f1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hv8ErO48vKoZ"
   },
   "source": [
    "Train model using $learning$ $rate = 0.002$, Stochastic Gradient Descent optimizer and Cross Entropy Loss function for $30$ epochs, while keeping the best current f1 macro averaged score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ls6xT9DvKWP"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "optimizer = SGD(params=cnn_model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "fQ_rcN8DxBvl",
    "outputId": "d3c65aa5-c62b-4289-f5ed-45c7c8e50303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-a70e67e81f7a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_convolutional_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-892535ee53f2>\u001b[0m in \u001b[0;36mtrain_convolutional_neural_network\u001b[0;34m(epochs, optimizer, dataloader, loss_function, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;31m# calculate prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-e1f685cc0ee3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x71680 and 1024x1024)"
     ]
    }
   ],
   "source": [
    "cnn_model = train_convolutional_neural_network(epochs, optimizer, train_dataloader, loss_function, cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNDkMrq9xLAm"
   },
   "source": [
    "The model cannot be trained.\n",
    "<br></br>\n",
    "\n",
    "The reason is that the output of the last convolutional layer has size $16 \\times 71680$, where $16$ is the batch size and $71680$ is the length of the flattened data, that can be derived applying the following formula that has been shown in lectures for each layer:\n",
    "<br></br>\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "  & W_1 = \\frac{W_0 - F - 2P}{S} + 1\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "  & H_1 = \\frac{H_0 - F - 2P}{S} + 1\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "$D_1 = K$, where:\n",
    "<br></br>\n",
    "\n",
    "$W_0 \\times H_0 \\times D_0$: volume that is accepted by the first convolutional layer\n",
    "\n",
    "$K$: number of filters\n",
    "\n",
    "$F$: spacial extent of the filters (kernel size)\n",
    "\n",
    "$S$: stride\n",
    "\n",
    "$P$: zero padding\n",
    "<br></br>\n",
    "\n",
    "$W_0 \\times H_0 \\times D_0 = 21 \\times 128 \\times 1$\n",
    "\n",
    "layer $i$ | $K$   | $F$  | $S$  | $P$  | $W_i$ x $H_i$ x $D_i$ | flattened\n",
    ":--------:|:-----:|:----:|:----:|:----:|:---------------------:|:----:\n",
    "$1$       | $16$  | $5$  | $1$  | $0$  | $17$ x $124$ x $16$\n",
    "$2$       | $32$  | $5$  | $1$  | $0$  | $13$ x $120$ x $32$\n",
    "$3$       | $64$  | $5$  | $1$  | $0$  | $9$ x $116$ x $64$\n",
    "$4$       | $128$ | $5$  | $1$  | $0$  | $5$ x $112$ x $128$   | $71680$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFqKqrhjv57K"
   },
   "source": [
    "### Step 4 - Pooling and padding\n",
    "\n",
    "Redefine the Convolutional Network Model by adding $P = 2$ to the filters and pooling layers with kernel size $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n53Oo5XB7npB"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # convolutional layers\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, padding=2)   # 1 channel   -> 16 channels\n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2)  # 16 channels -> 32 channels\n",
    "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)  # 32 channels -> 64 channels\n",
    "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2) # 64 channels -> 128 channels\n",
    "\n",
    "    # fully connected (dense) layers\n",
    "    self.dense1 = nn.Linear(1024, 1024) # input size = 1024 -> number of perceptrons in 1st hidden layer = 1024\n",
    "    self.dense2 = nn.Linear(1024, 256)  # number of perceptrons in 1st hidden layer = 1024 ->\n",
    "                                        # number of perceptrons in 2nd hidden layer = 256\n",
    "    self.dense3 = nn.Linear(256, 32)    # number of perceptrons in 2nd hidden layer = 256 ->\n",
    "                                        # number of perceptrons in 3rd hidden layer = 32\n",
    "    self.dense4 = nn.Linear(32, 4)      # number of perceptrons in 3rd hidden layer = 32 ->\n",
    "                                        # number of different labels = 4\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv3(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv4(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    x = torch.flatten(x, 1)\n",
    "\n",
    "    x = self.dense1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dense2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dense3(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dense4(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy5Ni_U6002j"
   },
   "source": [
    "Initialize model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rlw5ZJ5W02n6",
    "outputId": "32c82a68-70ac-48b7-c7c0-081f1fb35443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (dense3): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (dense4): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Net().to(device)\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRd9KJux08dB"
   },
   "source": [
    "Train model using $learning$ $rate = 0.002$, Stochastic Gradient Descent optimizer and Cross Entropy Loss function for $30$ epochs, while keeping the best current f1 macro averaged score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGdXyY3l09QM"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "optimizer = SGD(params=cnn_model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8IvbMdWNybkT",
    "outputId": "253c1fb1-e6d6-46aa-a0a3-cfdc2d1ff5ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 1.260114  [ 1200/ 3200]\n",
      "loss: 1.040470  [ 1216/ 3200]\n",
      "loss: 1.191701  [ 1232/ 3200]\n",
      "loss: 1.208645  [ 1248/ 3200]\n",
      "loss: 1.005197  [ 1264/ 3200]\n",
      "loss: 1.390117  [ 1280/ 3200]\n",
      "loss: 1.085066  [ 1296/ 3200]\n",
      "loss: 1.120034  [ 1312/ 3200]\n",
      "loss: 1.363739  [ 1328/ 3200]\n",
      "loss: 1.098882  [ 1344/ 3200]\n",
      "loss: 1.166921  [ 1360/ 3200]\n",
      "loss: 1.022595  [ 1376/ 3200]\n",
      "loss: 1.388696  [ 1392/ 3200]\n",
      "loss: 1.320336  [ 1408/ 3200]\n",
      "loss: 1.316082  [ 1424/ 3200]\n",
      "loss: 1.271371  [ 1440/ 3200]\n",
      "loss: 0.996496  [ 1456/ 3200]\n",
      "loss: 1.155627  [ 1472/ 3200]\n",
      "loss: 1.166157  [ 1488/ 3200]\n",
      "loss: 1.195671  [ 1504/ 3200]\n",
      "loss: 1.066227  [ 1520/ 3200]\n",
      "loss: 1.203308  [ 1536/ 3200]\n",
      "loss: 1.307122  [ 1552/ 3200]\n",
      "loss: 1.265569  [ 1568/ 3200]\n",
      "loss: 1.346875  [ 1584/ 3200]\n",
      "loss: 1.187089  [ 1600/ 3200]\n",
      "loss: 1.083877  [ 1616/ 3200]\n",
      "loss: 1.263610  [ 1632/ 3200]\n",
      "loss: 1.157002  [ 1648/ 3200]\n",
      "loss: 1.439077  [ 1664/ 3200]\n",
      "loss: 1.067971  [ 1680/ 3200]\n",
      "loss: 1.033391  [ 1696/ 3200]\n",
      "loss: 0.967586  [ 1712/ 3200]\n",
      "loss: 0.987522  [ 1728/ 3200]\n",
      "loss: 1.104603  [ 1744/ 3200]\n",
      "loss: 1.125766  [ 1760/ 3200]\n",
      "loss: 1.160023  [ 1776/ 3200]\n",
      "loss: 1.245975  [ 1792/ 3200]\n",
      "loss: 1.277558  [ 1808/ 3200]\n",
      "loss: 1.050714  [ 1824/ 3200]\n",
      "loss: 1.455296  [ 1840/ 3200]\n",
      "loss: 1.531494  [ 1856/ 3200]\n",
      "loss: 1.257328  [ 1872/ 3200]\n",
      "loss: 1.153012  [ 1888/ 3200]\n",
      "loss: 1.202780  [ 1904/ 3200]\n",
      "loss: 1.027608  [ 1920/ 3200]\n",
      "loss: 1.226424  [ 1936/ 3200]\n",
      "loss: 1.245752  [ 1952/ 3200]\n",
      "loss: 1.317898  [ 1968/ 3200]\n",
      "loss: 1.158207  [ 1984/ 3200]\n",
      "loss: 1.173622  [ 2000/ 3200]\n",
      "loss: 1.146401  [ 2016/ 3200]\n",
      "loss: 1.201966  [ 2032/ 3200]\n",
      "loss: 1.262613  [ 2048/ 3200]\n",
      "loss: 1.152145  [ 2064/ 3200]\n",
      "loss: 1.243521  [ 2080/ 3200]\n",
      "loss: 1.062304  [ 2096/ 3200]\n",
      "loss: 1.119657  [ 2112/ 3200]\n",
      "loss: 1.353907  [ 2128/ 3200]\n",
      "loss: 1.598417  [ 2144/ 3200]\n",
      "loss: 1.080294  [ 2160/ 3200]\n",
      "loss: 0.883922  [ 2176/ 3200]\n",
      "loss: 1.485726  [ 2192/ 3200]\n",
      "loss: 1.182808  [ 2208/ 3200]\n",
      "loss: 1.053436  [ 2224/ 3200]\n",
      "loss: 1.236416  [ 2240/ 3200]\n",
      "loss: 1.078915  [ 2256/ 3200]\n",
      "loss: 1.281933  [ 2272/ 3200]\n",
      "loss: 1.114208  [ 2288/ 3200]\n",
      "loss: 1.149141  [ 2304/ 3200]\n",
      "loss: 1.113626  [ 2320/ 3200]\n",
      "loss: 1.249376  [ 2336/ 3200]\n",
      "loss: 1.086273  [ 2352/ 3200]\n",
      "loss: 1.324575  [ 2368/ 3200]\n",
      "loss: 1.199403  [ 2384/ 3200]\n",
      "loss: 1.093611  [ 2400/ 3200]\n",
      "loss: 0.966583  [ 2416/ 3200]\n",
      "loss: 1.173369  [ 2432/ 3200]\n",
      "loss: 1.305376  [ 2448/ 3200]\n",
      "loss: 1.258626  [ 2464/ 3200]\n",
      "loss: 1.313687  [ 2480/ 3200]\n",
      "loss: 1.076512  [ 2496/ 3200]\n",
      "loss: 1.185671  [ 2512/ 3200]\n",
      "loss: 1.282998  [ 2528/ 3200]\n",
      "loss: 1.288108  [ 2544/ 3200]\n",
      "loss: 1.022834  [ 2560/ 3200]\n",
      "loss: 1.407881  [ 2576/ 3200]\n",
      "loss: 1.093523  [ 2592/ 3200]\n",
      "loss: 1.094780  [ 2608/ 3200]\n",
      "loss: 1.226829  [ 2624/ 3200]\n",
      "loss: 1.164117  [ 2640/ 3200]\n",
      "loss: 1.144190  [ 2656/ 3200]\n",
      "loss: 1.118249  [ 2672/ 3200]\n",
      "loss: 1.263825  [ 2688/ 3200]\n",
      "loss: 1.242284  [ 2704/ 3200]\n",
      "loss: 1.358603  [ 2720/ 3200]\n",
      "loss: 1.385992  [ 2736/ 3200]\n",
      "loss: 1.445971  [ 2752/ 3200]\n",
      "loss: 1.035609  [ 2768/ 3200]\n",
      "loss: 1.247173  [ 2784/ 3200]\n",
      "loss: 1.106460  [ 2800/ 3200]\n",
      "loss: 1.580843  [ 2816/ 3200]\n",
      "loss: 1.079200  [ 2832/ 3200]\n",
      "loss: 0.988900  [ 2848/ 3200]\n",
      "loss: 1.285786  [ 2864/ 3200]\n",
      "loss: 1.220887  [ 2880/ 3200]\n",
      "loss: 1.063963  [ 2896/ 3200]\n",
      "loss: 1.171638  [ 2912/ 3200]\n",
      "loss: 1.242835  [ 2928/ 3200]\n",
      "loss: 1.040462  [ 2944/ 3200]\n",
      "loss: 1.231766  [ 2960/ 3200]\n",
      "loss: 1.141105  [ 2976/ 3200]\n",
      "loss: 1.290938  [ 2992/ 3200]\n",
      "loss: 1.097813  [ 3008/ 3200]\n",
      "loss: 1.193901  [ 3024/ 3200]\n",
      "loss: 1.431799  [ 3040/ 3200]\n",
      "loss: 1.139569  [ 3056/ 3200]\n",
      "loss: 0.981375  [ 3072/ 3200]\n",
      "loss: 1.170029  [ 3088/ 3200]\n",
      "loss: 1.406453  [ 3104/ 3200]\n",
      "loss: 1.064620  [ 3120/ 3200]\n",
      "loss: 1.587774  [ 3136/ 3200]\n",
      "loss: 1.246498  [ 3152/ 3200]\n",
      "loss: 1.150548  [ 3168/ 3200]\n",
      "loss: 1.065712  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 7\n",
      "-----------------------------\n",
      "loss: 1.057442  [    0/ 3200]\n",
      "loss: 1.202797  [   16/ 3200]\n",
      "loss: 1.274534  [   32/ 3200]\n",
      "loss: 1.127975  [   48/ 3200]\n",
      "loss: 1.071421  [   64/ 3200]\n",
      "loss: 0.960195  [   80/ 3200]\n",
      "loss: 1.364431  [   96/ 3200]\n",
      "loss: 1.097227  [  112/ 3200]\n",
      "loss: 1.204941  [  128/ 3200]\n",
      "loss: 1.235626  [  144/ 3200]\n",
      "loss: 1.233836  [  160/ 3200]\n",
      "loss: 0.973250  [  176/ 3200]\n",
      "loss: 1.420410  [  192/ 3200]\n",
      "loss: 1.134923  [  208/ 3200]\n",
      "loss: 1.272335  [  224/ 3200]\n",
      "loss: 0.989593  [  240/ 3200]\n",
      "loss: 0.978877  [  256/ 3200]\n",
      "loss: 1.148502  [  272/ 3200]\n",
      "loss: 1.364248  [  288/ 3200]\n",
      "loss: 1.220667  [  304/ 3200]\n",
      "loss: 1.190942  [  320/ 3200]\n",
      "loss: 1.054095  [  336/ 3200]\n",
      "loss: 1.212688  [  352/ 3200]\n",
      "loss: 1.257691  [  368/ 3200]\n",
      "loss: 0.870266  [  384/ 3200]\n",
      "loss: 1.239011  [  400/ 3200]\n",
      "loss: 1.464592  [  416/ 3200]\n",
      "loss: 1.379398  [  432/ 3200]\n",
      "loss: 1.220865  [  448/ 3200]\n",
      "loss: 1.078975  [  464/ 3200]\n",
      "loss: 1.213163  [  480/ 3200]\n",
      "loss: 0.981931  [  496/ 3200]\n",
      "loss: 1.071834  [  512/ 3200]\n",
      "loss: 1.032023  [  528/ 3200]\n",
      "loss: 1.376212  [  544/ 3200]\n",
      "loss: 1.194002  [  560/ 3200]\n",
      "loss: 1.140238  [  576/ 3200]\n",
      "loss: 1.213293  [  592/ 3200]\n",
      "loss: 1.288495  [  608/ 3200]\n",
      "loss: 1.487322  [  624/ 3200]\n",
      "loss: 1.372637  [  640/ 3200]\n",
      "loss: 1.344090  [  656/ 3200]\n",
      "loss: 1.126696  [  672/ 3200]\n",
      "loss: 1.032808  [  688/ 3200]\n",
      "loss: 1.151788  [  704/ 3200]\n",
      "loss: 1.373199  [  720/ 3200]\n",
      "loss: 1.145863  [  736/ 3200]\n",
      "loss: 1.243366  [  752/ 3200]\n",
      "loss: 1.073465  [  768/ 3200]\n",
      "loss: 1.182533  [  784/ 3200]\n",
      "loss: 1.316569  [  800/ 3200]\n",
      "loss: 1.348126  [  816/ 3200]\n",
      "loss: 1.221484  [  832/ 3200]\n",
      "loss: 1.055486  [  848/ 3200]\n",
      "loss: 1.106726  [  864/ 3200]\n",
      "loss: 1.068483  [  880/ 3200]\n",
      "loss: 1.312491  [  896/ 3200]\n",
      "loss: 0.967587  [  912/ 3200]\n",
      "loss: 1.154293  [  928/ 3200]\n",
      "loss: 1.180228  [  944/ 3200]\n",
      "loss: 0.969179  [  960/ 3200]\n",
      "loss: 0.851880  [  976/ 3200]\n",
      "loss: 1.289131  [  992/ 3200]\n",
      "loss: 1.350132  [ 1008/ 3200]\n",
      "loss: 1.010814  [ 1024/ 3200]\n",
      "loss: 1.015266  [ 1040/ 3200]\n",
      "loss: 1.206954  [ 1056/ 3200]\n",
      "loss: 1.018407  [ 1072/ 3200]\n",
      "loss: 1.304571  [ 1088/ 3200]\n",
      "loss: 1.284049  [ 1104/ 3200]\n",
      "loss: 1.176508  [ 1120/ 3200]\n",
      "loss: 1.344760  [ 1136/ 3200]\n",
      "loss: 1.258376  [ 1152/ 3200]\n",
      "loss: 1.199732  [ 1168/ 3200]\n",
      "loss: 1.360743  [ 1184/ 3200]\n",
      "loss: 1.282518  [ 1200/ 3200]\n",
      "loss: 1.172180  [ 1216/ 3200]\n",
      "loss: 1.219740  [ 1232/ 3200]\n",
      "loss: 0.964767  [ 1248/ 3200]\n",
      "loss: 1.000232  [ 1264/ 3200]\n",
      "loss: 1.004281  [ 1280/ 3200]\n",
      "loss: 1.175230  [ 1296/ 3200]\n",
      "loss: 0.884938  [ 1312/ 3200]\n",
      "loss: 1.220509  [ 1328/ 3200]\n",
      "loss: 1.116714  [ 1344/ 3200]\n",
      "loss: 1.218645  [ 1360/ 3200]\n",
      "loss: 1.106614  [ 1376/ 3200]\n",
      "loss: 1.239817  [ 1392/ 3200]\n",
      "loss: 1.146723  [ 1408/ 3200]\n",
      "loss: 1.131531  [ 1424/ 3200]\n",
      "loss: 1.289818  [ 1440/ 3200]\n",
      "loss: 1.178736  [ 1456/ 3200]\n",
      "loss: 1.383785  [ 1472/ 3200]\n",
      "loss: 1.024496  [ 1488/ 3200]\n",
      "loss: 1.064403  [ 1504/ 3200]\n",
      "loss: 1.180343  [ 1520/ 3200]\n",
      "loss: 1.224157  [ 1536/ 3200]\n",
      "loss: 0.860926  [ 1552/ 3200]\n",
      "loss: 1.195999  [ 1568/ 3200]\n",
      "loss: 1.313462  [ 1584/ 3200]\n",
      "loss: 1.081606  [ 1600/ 3200]\n",
      "loss: 1.232911  [ 1616/ 3200]\n",
      "loss: 1.178168  [ 1632/ 3200]\n",
      "loss: 1.055702  [ 1648/ 3200]\n",
      "loss: 1.066213  [ 1664/ 3200]\n",
      "loss: 1.101763  [ 1680/ 3200]\n",
      "loss: 1.241180  [ 1696/ 3200]\n",
      "loss: 1.063936  [ 1712/ 3200]\n",
      "loss: 1.465482  [ 1728/ 3200]\n",
      "loss: 1.125829  [ 1744/ 3200]\n",
      "loss: 1.276245  [ 1760/ 3200]\n",
      "loss: 1.291807  [ 1776/ 3200]\n",
      "loss: 1.791379  [ 1792/ 3200]\n",
      "loss: 1.231338  [ 1808/ 3200]\n",
      "loss: 1.170599  [ 1824/ 3200]\n",
      "loss: 1.254480  [ 1840/ 3200]\n",
      "loss: 1.110394  [ 1856/ 3200]\n",
      "loss: 1.226874  [ 1872/ 3200]\n",
      "loss: 1.253830  [ 1888/ 3200]\n",
      "loss: 1.233261  [ 1904/ 3200]\n",
      "loss: 1.300851  [ 1920/ 3200]\n",
      "loss: 1.254896  [ 1936/ 3200]\n",
      "loss: 1.110571  [ 1952/ 3200]\n",
      "loss: 0.863783  [ 1968/ 3200]\n",
      "loss: 1.146015  [ 1984/ 3200]\n",
      "loss: 1.420734  [ 2000/ 3200]\n",
      "loss: 1.092692  [ 2016/ 3200]\n",
      "loss: 1.256004  [ 2032/ 3200]\n",
      "loss: 1.126944  [ 2048/ 3200]\n",
      "loss: 1.185461  [ 2064/ 3200]\n",
      "loss: 1.334330  [ 2080/ 3200]\n",
      "loss: 1.050053  [ 2096/ 3200]\n",
      "loss: 1.153924  [ 2112/ 3200]\n",
      "loss: 1.367076  [ 2128/ 3200]\n",
      "loss: 1.110108  [ 2144/ 3200]\n",
      "loss: 1.198819  [ 2160/ 3200]\n",
      "loss: 1.173406  [ 2176/ 3200]\n",
      "loss: 1.030321  [ 2192/ 3200]\n",
      "loss: 1.094180  [ 2208/ 3200]\n",
      "loss: 1.293969  [ 2224/ 3200]\n",
      "loss: 1.305760  [ 2240/ 3200]\n",
      "loss: 1.067019  [ 2256/ 3200]\n",
      "loss: 1.188860  [ 2272/ 3200]\n",
      "loss: 1.311620  [ 2288/ 3200]\n",
      "loss: 0.842914  [ 2304/ 3200]\n",
      "loss: 1.140368  [ 2320/ 3200]\n",
      "loss: 1.051473  [ 2336/ 3200]\n",
      "loss: 1.121142  [ 2352/ 3200]\n",
      "loss: 1.048938  [ 2368/ 3200]\n",
      "loss: 1.098486  [ 2384/ 3200]\n",
      "loss: 1.327879  [ 2400/ 3200]\n",
      "loss: 0.929972  [ 2416/ 3200]\n",
      "loss: 1.113171  [ 2432/ 3200]\n",
      "loss: 0.927103  [ 2448/ 3200]\n",
      "loss: 1.043154  [ 2464/ 3200]\n",
      "loss: 1.445137  [ 2480/ 3200]\n",
      "loss: 1.240113  [ 2496/ 3200]\n",
      "loss: 1.105201  [ 2512/ 3200]\n",
      "loss: 1.052617  [ 2528/ 3200]\n",
      "loss: 1.001559  [ 2544/ 3200]\n",
      "loss: 1.309524  [ 2560/ 3200]\n",
      "loss: 1.527524  [ 2576/ 3200]\n",
      "loss: 1.339133  [ 2592/ 3200]\n",
      "loss: 1.192278  [ 2608/ 3200]\n",
      "loss: 1.252457  [ 2624/ 3200]\n",
      "loss: 1.073826  [ 2640/ 3200]\n",
      "loss: 1.130592  [ 2656/ 3200]\n",
      "loss: 1.225332  [ 2672/ 3200]\n",
      "loss: 1.354804  [ 2688/ 3200]\n",
      "loss: 1.143813  [ 2704/ 3200]\n",
      "loss: 1.116665  [ 2720/ 3200]\n",
      "loss: 1.252846  [ 2736/ 3200]\n",
      "loss: 1.078877  [ 2752/ 3200]\n",
      "loss: 1.089707  [ 2768/ 3200]\n",
      "loss: 1.308739  [ 2784/ 3200]\n",
      "loss: 0.996948  [ 2800/ 3200]\n",
      "loss: 1.117396  [ 2816/ 3200]\n",
      "loss: 1.282363  [ 2832/ 3200]\n",
      "loss: 1.150820  [ 2848/ 3200]\n",
      "loss: 1.100414  [ 2864/ 3200]\n",
      "loss: 1.047862  [ 2880/ 3200]\n",
      "loss: 1.090861  [ 2896/ 3200]\n",
      "loss: 1.236387  [ 2912/ 3200]\n",
      "loss: 1.187426  [ 2928/ 3200]\n",
      "loss: 1.021084  [ 2944/ 3200]\n",
      "loss: 1.023484  [ 2960/ 3200]\n",
      "loss: 1.051869  [ 2976/ 3200]\n",
      "loss: 1.085699  [ 2992/ 3200]\n",
      "loss: 1.220375  [ 3008/ 3200]\n",
      "loss: 1.179903  [ 3024/ 3200]\n",
      "loss: 1.112900  [ 3040/ 3200]\n",
      "loss: 1.207910  [ 3056/ 3200]\n",
      "loss: 1.269901  [ 3072/ 3200]\n",
      "loss: 1.111293  [ 3088/ 3200]\n",
      "loss: 1.106151  [ 3104/ 3200]\n",
      "loss: 1.228546  [ 3120/ 3200]\n",
      "loss: 1.195311  [ 3136/ 3200]\n",
      "loss: 0.950494  [ 3152/ 3200]\n",
      "loss: 1.145293  [ 3168/ 3200]\n",
      "loss: 1.297843  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 1.111202  [    0/ 3200]\n",
      "loss: 1.047913  [   16/ 3200]\n",
      "loss: 1.206990  [   32/ 3200]\n",
      "loss: 0.932940  [   48/ 3200]\n",
      "loss: 1.309643  [   64/ 3200]\n",
      "loss: 1.250422  [   80/ 3200]\n",
      "loss: 1.109924  [   96/ 3200]\n",
      "loss: 1.223568  [  112/ 3200]\n",
      "loss: 1.106371  [  128/ 3200]\n",
      "loss: 1.293326  [  144/ 3200]\n",
      "loss: 0.874748  [  160/ 3200]\n",
      "loss: 1.094708  [  176/ 3200]\n",
      "loss: 1.038784  [  192/ 3200]\n",
      "loss: 1.146900  [  208/ 3200]\n",
      "loss: 1.187561  [  224/ 3200]\n",
      "loss: 1.251945  [  240/ 3200]\n",
      "loss: 1.222754  [  256/ 3200]\n",
      "loss: 0.721909  [  272/ 3200]\n",
      "loss: 1.154053  [  288/ 3200]\n",
      "loss: 0.924568  [  304/ 3200]\n",
      "loss: 1.137859  [  320/ 3200]\n",
      "loss: 1.114743  [  336/ 3200]\n",
      "loss: 0.941326  [  352/ 3200]\n",
      "loss: 1.123477  [  368/ 3200]\n",
      "loss: 1.324786  [  384/ 3200]\n",
      "loss: 1.322687  [  400/ 3200]\n",
      "loss: 1.045798  [  416/ 3200]\n",
      "loss: 1.142326  [  432/ 3200]\n",
      "loss: 1.196500  [  448/ 3200]\n",
      "loss: 1.200170  [  464/ 3200]\n",
      "loss: 1.076651  [  480/ 3200]\n",
      "loss: 1.035696  [  496/ 3200]\n",
      "loss: 1.162243  [  512/ 3200]\n",
      "loss: 1.226634  [  528/ 3200]\n",
      "loss: 0.945343  [  544/ 3200]\n",
      "loss: 1.029309  [  560/ 3200]\n",
      "loss: 1.181109  [  576/ 3200]\n",
      "loss: 1.047346  [  592/ 3200]\n",
      "loss: 1.049828  [  608/ 3200]\n",
      "loss: 1.224461  [  624/ 3200]\n",
      "loss: 1.506366  [  640/ 3200]\n",
      "loss: 1.208044  [  656/ 3200]\n",
      "loss: 0.958522  [  672/ 3200]\n",
      "loss: 1.075306  [  688/ 3200]\n",
      "loss: 0.963916  [  704/ 3200]\n",
      "loss: 1.200882  [  720/ 3200]\n",
      "loss: 0.971926  [  736/ 3200]\n",
      "loss: 1.052604  [  752/ 3200]\n",
      "loss: 1.343199  [  768/ 3200]\n",
      "loss: 1.111065  [  784/ 3200]\n",
      "loss: 1.010174  [  800/ 3200]\n",
      "loss: 1.123474  [  816/ 3200]\n",
      "loss: 1.165515  [  832/ 3200]\n",
      "loss: 1.304073  [  848/ 3200]\n",
      "loss: 1.268141  [  864/ 3200]\n",
      "loss: 1.195115  [  880/ 3200]\n",
      "loss: 1.093543  [  896/ 3200]\n",
      "loss: 0.865016  [  912/ 3200]\n",
      "loss: 1.015246  [  928/ 3200]\n",
      "loss: 1.347412  [  944/ 3200]\n",
      "loss: 1.112761  [  960/ 3200]\n",
      "loss: 1.330825  [  976/ 3200]\n",
      "loss: 1.113415  [  992/ 3200]\n",
      "loss: 1.089523  [ 1008/ 3200]\n",
      "loss: 1.142493  [ 1024/ 3200]\n",
      "loss: 1.117688  [ 1040/ 3200]\n",
      "loss: 1.101249  [ 1056/ 3200]\n",
      "loss: 1.089779  [ 1072/ 3200]\n",
      "loss: 0.913594  [ 1088/ 3200]\n",
      "loss: 1.128355  [ 1104/ 3200]\n",
      "loss: 1.046496  [ 1120/ 3200]\n",
      "loss: 1.116621  [ 1136/ 3200]\n",
      "loss: 1.112863  [ 1152/ 3200]\n",
      "loss: 0.975181  [ 1168/ 3200]\n",
      "loss: 1.045385  [ 1184/ 3200]\n",
      "loss: 1.188260  [ 1200/ 3200]\n",
      "loss: 1.441144  [ 1216/ 3200]\n",
      "loss: 1.227062  [ 1232/ 3200]\n",
      "loss: 1.331581  [ 1248/ 3200]\n",
      "loss: 0.978768  [ 1264/ 3200]\n",
      "loss: 1.187656  [ 1280/ 3200]\n",
      "loss: 1.470115  [ 1296/ 3200]\n",
      "loss: 1.001975  [ 1312/ 3200]\n",
      "loss: 1.015091  [ 1328/ 3200]\n",
      "loss: 1.212585  [ 1344/ 3200]\n",
      "loss: 0.907178  [ 1360/ 3200]\n",
      "loss: 1.043184  [ 1376/ 3200]\n",
      "loss: 1.151044  [ 1392/ 3200]\n",
      "loss: 1.077380  [ 1408/ 3200]\n",
      "loss: 0.946917  [ 1424/ 3200]\n",
      "loss: 1.381236  [ 1440/ 3200]\n",
      "loss: 1.199717  [ 1456/ 3200]\n",
      "loss: 1.062844  [ 1472/ 3200]\n",
      "loss: 1.245954  [ 1488/ 3200]\n",
      "loss: 1.217162  [ 1504/ 3200]\n",
      "loss: 1.137789  [ 1520/ 3200]\n",
      "loss: 0.846024  [ 1536/ 3200]\n",
      "loss: 1.333469  [ 1552/ 3200]\n",
      "loss: 1.147621  [ 1568/ 3200]\n",
      "loss: 1.203315  [ 1584/ 3200]\n",
      "loss: 1.038208  [ 1600/ 3200]\n",
      "loss: 1.026588  [ 1616/ 3200]\n",
      "loss: 1.518275  [ 1632/ 3200]\n",
      "loss: 1.326922  [ 1648/ 3200]\n",
      "loss: 0.950005  [ 1664/ 3200]\n",
      "loss: 1.061758  [ 1680/ 3200]\n",
      "loss: 0.811339  [ 1696/ 3200]\n",
      "loss: 1.411972  [ 1712/ 3200]\n",
      "loss: 1.113346  [ 1728/ 3200]\n",
      "loss: 1.057355  [ 1744/ 3200]\n",
      "loss: 0.950040  [ 1760/ 3200]\n",
      "loss: 1.067397  [ 1776/ 3200]\n",
      "loss: 0.785078  [ 1792/ 3200]\n",
      "loss: 0.826612  [ 1808/ 3200]\n",
      "loss: 1.245272  [ 1824/ 3200]\n",
      "loss: 1.032796  [ 1840/ 3200]\n",
      "loss: 1.016346  [ 1856/ 3200]\n",
      "loss: 1.118433  [ 1872/ 3200]\n",
      "loss: 1.161495  [ 1888/ 3200]\n",
      "loss: 1.221861  [ 1904/ 3200]\n",
      "loss: 1.262325  [ 1920/ 3200]\n",
      "loss: 1.168799  [ 1936/ 3200]\n",
      "loss: 1.123738  [ 1952/ 3200]\n",
      "loss: 1.230297  [ 1968/ 3200]\n",
      "loss: 1.101138  [ 1984/ 3200]\n",
      "loss: 1.186757  [ 2000/ 3200]\n",
      "loss: 1.109810  [ 2016/ 3200]\n",
      "loss: 1.286498  [ 2032/ 3200]\n",
      "loss: 1.114206  [ 2048/ 3200]\n",
      "loss: 1.132601  [ 2064/ 3200]\n",
      "loss: 1.188973  [ 2080/ 3200]\n",
      "loss: 1.249413  [ 2096/ 3200]\n",
      "loss: 0.886491  [ 2112/ 3200]\n",
      "loss: 1.164559  [ 2128/ 3200]\n",
      "loss: 1.076452  [ 2144/ 3200]\n",
      "loss: 0.987928  [ 2160/ 3200]\n",
      "loss: 1.089710  [ 2176/ 3200]\n",
      "loss: 0.956899  [ 2192/ 3200]\n",
      "loss: 1.006739  [ 2208/ 3200]\n",
      "loss: 1.003533  [ 2224/ 3200]\n",
      "loss: 0.803018  [ 2240/ 3200]\n",
      "loss: 1.622619  [ 2256/ 3200]\n",
      "loss: 1.048147  [ 2272/ 3200]\n",
      "loss: 1.077835  [ 2288/ 3200]\n",
      "loss: 1.278565  [ 2304/ 3200]\n",
      "loss: 1.052540  [ 2320/ 3200]\n",
      "loss: 1.198540  [ 2336/ 3200]\n",
      "loss: 1.161482  [ 2352/ 3200]\n",
      "loss: 1.304855  [ 2368/ 3200]\n",
      "loss: 1.059430  [ 2384/ 3200]\n",
      "loss: 1.231607  [ 2400/ 3200]\n",
      "loss: 1.127579  [ 2416/ 3200]\n",
      "loss: 1.239829  [ 2432/ 3200]\n",
      "loss: 1.000406  [ 2448/ 3200]\n",
      "loss: 1.232389  [ 2464/ 3200]\n",
      "loss: 1.098944  [ 2480/ 3200]\n",
      "loss: 1.055171  [ 2496/ 3200]\n",
      "loss: 0.962163  [ 2512/ 3200]\n",
      "loss: 1.189785  [ 2528/ 3200]\n",
      "loss: 0.907092  [ 2544/ 3200]\n",
      "loss: 1.136973  [ 2560/ 3200]\n",
      "loss: 0.998029  [ 2576/ 3200]\n",
      "loss: 1.064615  [ 2592/ 3200]\n",
      "loss: 1.346220  [ 2608/ 3200]\n",
      "loss: 1.220559  [ 2624/ 3200]\n",
      "loss: 1.182245  [ 2640/ 3200]\n",
      "loss: 1.114588  [ 2656/ 3200]\n",
      "loss: 1.092274  [ 2672/ 3200]\n",
      "loss: 1.201308  [ 2688/ 3200]\n",
      "loss: 1.190420  [ 2704/ 3200]\n",
      "loss: 1.015688  [ 2720/ 3200]\n",
      "loss: 1.168675  [ 2736/ 3200]\n",
      "loss: 1.203542  [ 2752/ 3200]\n",
      "loss: 0.942775  [ 2768/ 3200]\n",
      "loss: 1.137705  [ 2784/ 3200]\n",
      "loss: 1.112446  [ 2800/ 3200]\n",
      "loss: 1.142685  [ 2816/ 3200]\n",
      "loss: 1.272334  [ 2832/ 3200]\n",
      "loss: 1.251296  [ 2848/ 3200]\n",
      "loss: 0.985876  [ 2864/ 3200]\n",
      "loss: 0.831278  [ 2880/ 3200]\n",
      "loss: 1.175734  [ 2896/ 3200]\n",
      "loss: 1.417493  [ 2912/ 3200]\n",
      "loss: 1.065223  [ 2928/ 3200]\n",
      "loss: 0.927124  [ 2944/ 3200]\n",
      "loss: 0.948243  [ 2960/ 3200]\n",
      "loss: 1.047114  [ 2976/ 3200]\n",
      "loss: 1.234132  [ 2992/ 3200]\n",
      "loss: 0.849219  [ 3008/ 3200]\n",
      "loss: 1.089054  [ 3024/ 3200]\n",
      "loss: 1.059442  [ 3040/ 3200]\n",
      "loss: 0.929995  [ 3056/ 3200]\n",
      "loss: 1.067602  [ 3072/ 3200]\n",
      "loss: 0.992574  [ 3088/ 3200]\n",
      "loss: 0.908588  [ 3104/ 3200]\n",
      "loss: 1.132178  [ 3120/ 3200]\n",
      "loss: 1.041273  [ 3136/ 3200]\n",
      "loss: 1.091055  [ 3152/ 3200]\n",
      "loss: 1.043985  [ 3168/ 3200]\n",
      "loss: 1.369573  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 1.317675  [    0/ 3200]\n",
      "loss: 1.263910  [   16/ 3200]\n",
      "loss: 1.152698  [   32/ 3200]\n",
      "loss: 1.148958  [   48/ 3200]\n",
      "loss: 1.027779  [   64/ 3200]\n",
      "loss: 0.783096  [   80/ 3200]\n",
      "loss: 0.983687  [   96/ 3200]\n",
      "loss: 1.042456  [  112/ 3200]\n",
      "loss: 1.530467  [  128/ 3200]\n",
      "loss: 1.589355  [  144/ 3200]\n",
      "loss: 1.143243  [  160/ 3200]\n",
      "loss: 0.856310  [  176/ 3200]\n",
      "loss: 0.994822  [  192/ 3200]\n",
      "loss: 1.009660  [  208/ 3200]\n",
      "loss: 1.055673  [  224/ 3200]\n",
      "loss: 1.116011  [  240/ 3200]\n",
      "loss: 0.939142  [  256/ 3200]\n",
      "loss: 1.112688  [  272/ 3200]\n",
      "loss: 1.072858  [  288/ 3200]\n",
      "loss: 1.180400  [  304/ 3200]\n",
      "loss: 1.057736  [  320/ 3200]\n",
      "loss: 1.150070  [  336/ 3200]\n",
      "loss: 1.071588  [  352/ 3200]\n",
      "loss: 1.191386  [  368/ 3200]\n",
      "loss: 1.213696  [  384/ 3200]\n",
      "loss: 1.085402  [  400/ 3200]\n",
      "loss: 1.474579  [  416/ 3200]\n",
      "loss: 0.996227  [  432/ 3200]\n",
      "loss: 1.080068  [  448/ 3200]\n",
      "loss: 1.126544  [  464/ 3200]\n",
      "loss: 1.078649  [  480/ 3200]\n",
      "loss: 1.079979  [  496/ 3200]\n",
      "loss: 0.980154  [  512/ 3200]\n",
      "loss: 0.885526  [  528/ 3200]\n",
      "loss: 1.403508  [  544/ 3200]\n",
      "loss: 1.194871  [  560/ 3200]\n",
      "loss: 1.145413  [  576/ 3200]\n",
      "loss: 1.262091  [  592/ 3200]\n",
      "loss: 1.177323  [  608/ 3200]\n",
      "loss: 1.038416  [  624/ 3200]\n",
      "loss: 1.274270  [  640/ 3200]\n",
      "loss: 1.116275  [  656/ 3200]\n",
      "loss: 1.062112  [  672/ 3200]\n",
      "loss: 1.293332  [  688/ 3200]\n",
      "loss: 1.010523  [  704/ 3200]\n",
      "loss: 0.857152  [  720/ 3200]\n",
      "loss: 1.099082  [  736/ 3200]\n",
      "loss: 1.340973  [  752/ 3200]\n",
      "loss: 0.894472  [  768/ 3200]\n",
      "loss: 0.985183  [  784/ 3200]\n",
      "loss: 0.962487  [  800/ 3200]\n",
      "loss: 1.156831  [  816/ 3200]\n",
      "loss: 1.068564  [  832/ 3200]\n",
      "loss: 1.307809  [  848/ 3200]\n",
      "loss: 1.065997  [  864/ 3200]\n",
      "loss: 0.910064  [  880/ 3200]\n",
      "loss: 0.991672  [  896/ 3200]\n",
      "loss: 0.859913  [  912/ 3200]\n",
      "loss: 0.941178  [  928/ 3200]\n",
      "loss: 0.953194  [  944/ 3200]\n",
      "loss: 1.280570  [  960/ 3200]\n",
      "loss: 1.194706  [  976/ 3200]\n",
      "loss: 0.956767  [  992/ 3200]\n",
      "loss: 1.089713  [ 1008/ 3200]\n",
      "loss: 1.074368  [ 1024/ 3200]\n",
      "loss: 1.168502  [ 1040/ 3200]\n",
      "loss: 1.049229  [ 1056/ 3200]\n",
      "loss: 1.360739  [ 1072/ 3200]\n",
      "loss: 1.105692  [ 1088/ 3200]\n",
      "loss: 0.911236  [ 1104/ 3200]\n",
      "loss: 1.229533  [ 1120/ 3200]\n",
      "loss: 1.009410  [ 1136/ 3200]\n",
      "loss: 1.103613  [ 1152/ 3200]\n",
      "loss: 1.133463  [ 1168/ 3200]\n",
      "loss: 1.254289  [ 1184/ 3200]\n",
      "loss: 1.158276  [ 1200/ 3200]\n",
      "loss: 1.028219  [ 1216/ 3200]\n",
      "loss: 1.110981  [ 1232/ 3200]\n",
      "loss: 1.071578  [ 1248/ 3200]\n",
      "loss: 1.196134  [ 1264/ 3200]\n",
      "loss: 1.036838  [ 1280/ 3200]\n",
      "loss: 0.930835  [ 1296/ 3200]\n",
      "loss: 1.314577  [ 1312/ 3200]\n",
      "loss: 1.097565  [ 1328/ 3200]\n",
      "loss: 1.282902  [ 1344/ 3200]\n",
      "loss: 0.988955  [ 1360/ 3200]\n",
      "loss: 1.272141  [ 1376/ 3200]\n",
      "loss: 1.014404  [ 1392/ 3200]\n",
      "loss: 1.086448  [ 1408/ 3200]\n",
      "loss: 0.900472  [ 1424/ 3200]\n",
      "loss: 1.225441  [ 1440/ 3200]\n",
      "loss: 1.083270  [ 1456/ 3200]\n",
      "loss: 1.118432  [ 1472/ 3200]\n",
      "loss: 1.150215  [ 1488/ 3200]\n",
      "loss: 1.242870  [ 1504/ 3200]\n",
      "loss: 0.886607  [ 1520/ 3200]\n",
      "loss: 1.173542  [ 1536/ 3200]\n",
      "loss: 1.202644  [ 1552/ 3200]\n",
      "loss: 0.956540  [ 1568/ 3200]\n",
      "loss: 1.154413  [ 1584/ 3200]\n",
      "loss: 1.110283  [ 1600/ 3200]\n",
      "loss: 0.911237  [ 1616/ 3200]\n",
      "loss: 0.885084  [ 1632/ 3200]\n",
      "loss: 1.052559  [ 1648/ 3200]\n",
      "loss: 1.139564  [ 1664/ 3200]\n",
      "loss: 0.976891  [ 1680/ 3200]\n",
      "loss: 1.011028  [ 1696/ 3200]\n",
      "loss: 1.056638  [ 1712/ 3200]\n",
      "loss: 0.878595  [ 1728/ 3200]\n",
      "loss: 1.200068  [ 1744/ 3200]\n",
      "loss: 1.075857  [ 1760/ 3200]\n",
      "loss: 1.099379  [ 1776/ 3200]\n",
      "loss: 1.038143  [ 1792/ 3200]\n",
      "loss: 1.009563  [ 1808/ 3200]\n",
      "loss: 0.881590  [ 1824/ 3200]\n",
      "loss: 1.094829  [ 1840/ 3200]\n",
      "loss: 1.048529  [ 1856/ 3200]\n",
      "loss: 1.032737  [ 1872/ 3200]\n",
      "loss: 1.113669  [ 1888/ 3200]\n",
      "loss: 1.004259  [ 1904/ 3200]\n",
      "loss: 1.080029  [ 1920/ 3200]\n",
      "loss: 1.026300  [ 1936/ 3200]\n",
      "loss: 1.280171  [ 1952/ 3200]\n",
      "loss: 0.991792  [ 1968/ 3200]\n",
      "loss: 1.025155  [ 1984/ 3200]\n",
      "loss: 1.030610  [ 2000/ 3200]\n",
      "loss: 0.996365  [ 2016/ 3200]\n",
      "loss: 1.120208  [ 2032/ 3200]\n",
      "loss: 1.017213  [ 2048/ 3200]\n",
      "loss: 1.309539  [ 2064/ 3200]\n",
      "loss: 1.170719  [ 2080/ 3200]\n",
      "loss: 1.156233  [ 2096/ 3200]\n",
      "loss: 0.944176  [ 2112/ 3200]\n",
      "loss: 1.007623  [ 2128/ 3200]\n",
      "loss: 0.903094  [ 2144/ 3200]\n",
      "loss: 0.796828  [ 2160/ 3200]\n",
      "loss: 1.093399  [ 2176/ 3200]\n",
      "loss: 1.048165  [ 2192/ 3200]\n",
      "loss: 1.002712  [ 2208/ 3200]\n",
      "loss: 0.981876  [ 2224/ 3200]\n",
      "loss: 0.908614  [ 2240/ 3200]\n",
      "loss: 0.957816  [ 2256/ 3200]\n",
      "loss: 1.055845  [ 2272/ 3200]\n",
      "loss: 1.490007  [ 2288/ 3200]\n",
      "loss: 1.197190  [ 2304/ 3200]\n",
      "loss: 1.230790  [ 2320/ 3200]\n",
      "loss: 1.391522  [ 2336/ 3200]\n",
      "loss: 1.089082  [ 2352/ 3200]\n",
      "loss: 1.031158  [ 2368/ 3200]\n",
      "loss: 0.861664  [ 2384/ 3200]\n",
      "loss: 1.121848  [ 2400/ 3200]\n",
      "loss: 1.104093  [ 2416/ 3200]\n",
      "loss: 0.912460  [ 2432/ 3200]\n",
      "loss: 1.229721  [ 2448/ 3200]\n",
      "loss: 1.026847  [ 2464/ 3200]\n",
      "loss: 1.074458  [ 2480/ 3200]\n",
      "loss: 1.294923  [ 2496/ 3200]\n",
      "loss: 0.931386  [ 2512/ 3200]\n",
      "loss: 1.044614  [ 2528/ 3200]\n",
      "loss: 1.168497  [ 2544/ 3200]\n",
      "loss: 1.071452  [ 2560/ 3200]\n",
      "loss: 0.949789  [ 2576/ 3200]\n",
      "loss: 1.106425  [ 2592/ 3200]\n",
      "loss: 1.385144  [ 2608/ 3200]\n",
      "loss: 0.985856  [ 2624/ 3200]\n",
      "loss: 1.235828  [ 2640/ 3200]\n",
      "loss: 1.033660  [ 2656/ 3200]\n",
      "loss: 1.144547  [ 2672/ 3200]\n",
      "loss: 0.978637  [ 2688/ 3200]\n",
      "loss: 1.367475  [ 2704/ 3200]\n",
      "loss: 0.975979  [ 2720/ 3200]\n",
      "loss: 0.912049  [ 2736/ 3200]\n",
      "loss: 0.878193  [ 2752/ 3200]\n",
      "loss: 1.044056  [ 2768/ 3200]\n",
      "loss: 1.351650  [ 2784/ 3200]\n",
      "loss: 1.104782  [ 2800/ 3200]\n",
      "loss: 1.050549  [ 2816/ 3200]\n",
      "loss: 1.177209  [ 2832/ 3200]\n",
      "loss: 0.931456  [ 2848/ 3200]\n",
      "loss: 1.298889  [ 2864/ 3200]\n",
      "loss: 0.976327  [ 2880/ 3200]\n",
      "loss: 0.767942  [ 2896/ 3200]\n",
      "loss: 1.041642  [ 2912/ 3200]\n",
      "loss: 1.122677  [ 2928/ 3200]\n",
      "loss: 0.849503  [ 2944/ 3200]\n",
      "loss: 1.098718  [ 2960/ 3200]\n",
      "loss: 1.070386  [ 2976/ 3200]\n",
      "loss: 1.223748  [ 2992/ 3200]\n",
      "loss: 1.119274  [ 3008/ 3200]\n",
      "loss: 1.010109  [ 3024/ 3200]\n",
      "loss: 1.174835  [ 3040/ 3200]\n",
      "loss: 1.305790  [ 3056/ 3200]\n",
      "loss: 0.970594  [ 3072/ 3200]\n",
      "loss: 1.134093  [ 3088/ 3200]\n",
      "loss: 1.257736  [ 3104/ 3200]\n",
      "loss: 1.074360  [ 3120/ 3200]\n",
      "loss: 1.112252  [ 3136/ 3200]\n",
      "loss: 1.334733  [ 3152/ 3200]\n",
      "loss: 1.075264  [ 3168/ 3200]\n",
      "loss: 0.867299  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 1.381927  [    0/ 3200]\n",
      "loss: 0.982139  [   16/ 3200]\n",
      "loss: 0.998120  [   32/ 3200]\n",
      "loss: 0.965892  [   48/ 3200]\n",
      "loss: 1.094402  [   64/ 3200]\n",
      "loss: 1.216421  [   80/ 3200]\n",
      "loss: 0.941007  [   96/ 3200]\n",
      "loss: 1.297609  [  112/ 3200]\n",
      "loss: 1.131263  [  128/ 3200]\n",
      "loss: 0.984080  [  144/ 3200]\n",
      "loss: 1.108853  [  160/ 3200]\n",
      "loss: 0.990054  [  176/ 3200]\n",
      "loss: 1.064330  [  192/ 3200]\n",
      "loss: 1.056352  [  208/ 3200]\n",
      "loss: 1.061167  [  224/ 3200]\n",
      "loss: 1.110785  [  240/ 3200]\n",
      "loss: 0.866351  [  256/ 3200]\n",
      "loss: 1.037595  [  272/ 3200]\n",
      "loss: 1.037468  [  288/ 3200]\n",
      "loss: 0.921534  [  304/ 3200]\n",
      "loss: 1.110053  [  320/ 3200]\n",
      "loss: 0.970623  [  336/ 3200]\n",
      "loss: 0.982450  [  352/ 3200]\n",
      "loss: 1.008014  [  368/ 3200]\n",
      "loss: 0.813643  [  384/ 3200]\n",
      "loss: 1.590469  [  400/ 3200]\n",
      "loss: 1.334004  [  416/ 3200]\n",
      "loss: 1.158481  [  432/ 3200]\n",
      "loss: 0.928504  [  448/ 3200]\n",
      "loss: 1.718167  [  464/ 3200]\n",
      "loss: 1.099773  [  480/ 3200]\n",
      "loss: 1.052627  [  496/ 3200]\n",
      "loss: 1.275011  [  512/ 3200]\n",
      "loss: 1.134393  [  528/ 3200]\n",
      "loss: 1.062727  [  544/ 3200]\n",
      "loss: 1.098894  [  560/ 3200]\n",
      "loss: 1.074718  [  576/ 3200]\n",
      "loss: 1.146141  [  592/ 3200]\n",
      "loss: 1.102259  [  608/ 3200]\n",
      "loss: 1.024685  [  624/ 3200]\n",
      "loss: 1.236364  [  640/ 3200]\n",
      "loss: 1.036847  [  656/ 3200]\n",
      "loss: 0.796588  [  672/ 3200]\n",
      "loss: 1.107375  [  688/ 3200]\n",
      "loss: 0.970335  [  704/ 3200]\n",
      "loss: 1.178184  [  720/ 3200]\n",
      "loss: 0.979095  [  736/ 3200]\n",
      "loss: 1.219936  [  752/ 3200]\n",
      "loss: 0.988320  [  768/ 3200]\n",
      "loss: 0.975297  [  784/ 3200]\n",
      "loss: 0.958460  [  800/ 3200]\n",
      "loss: 1.068343  [  816/ 3200]\n",
      "loss: 1.116820  [  832/ 3200]\n",
      "loss: 0.944739  [  848/ 3200]\n",
      "loss: 1.199586  [  864/ 3200]\n",
      "loss: 1.159052  [  880/ 3200]\n",
      "loss: 0.807429  [  896/ 3200]\n",
      "loss: 0.905601  [  912/ 3200]\n",
      "loss: 1.414576  [  928/ 3200]\n",
      "loss: 1.090546  [  944/ 3200]\n",
      "loss: 1.007890  [  960/ 3200]\n",
      "loss: 1.171979  [  976/ 3200]\n",
      "loss: 0.764793  [  992/ 3200]\n",
      "loss: 1.046073  [ 1008/ 3200]\n",
      "loss: 0.939557  [ 1024/ 3200]\n",
      "loss: 1.080572  [ 1040/ 3200]\n",
      "loss: 1.050449  [ 1056/ 3200]\n",
      "loss: 1.081790  [ 1072/ 3200]\n",
      "loss: 1.032453  [ 1088/ 3200]\n",
      "loss: 0.946069  [ 1104/ 3200]\n",
      "loss: 0.963881  [ 1120/ 3200]\n",
      "loss: 1.119470  [ 1136/ 3200]\n",
      "loss: 0.797704  [ 1152/ 3200]\n",
      "loss: 1.248755  [ 1168/ 3200]\n",
      "loss: 1.031484  [ 1184/ 3200]\n",
      "loss: 0.855760  [ 1200/ 3200]\n",
      "loss: 1.116261  [ 1216/ 3200]\n",
      "loss: 1.000417  [ 1232/ 3200]\n",
      "loss: 1.076917  [ 1248/ 3200]\n",
      "loss: 0.937176  [ 1264/ 3200]\n",
      "loss: 1.065467  [ 1280/ 3200]\n",
      "loss: 1.201665  [ 1296/ 3200]\n",
      "loss: 1.098642  [ 1312/ 3200]\n",
      "loss: 1.025308  [ 1328/ 3200]\n",
      "loss: 0.828401  [ 1344/ 3200]\n",
      "loss: 0.931521  [ 1360/ 3200]\n",
      "loss: 1.011172  [ 1376/ 3200]\n",
      "loss: 1.021229  [ 1392/ 3200]\n",
      "loss: 0.990113  [ 1408/ 3200]\n",
      "loss: 0.891345  [ 1424/ 3200]\n",
      "loss: 1.123957  [ 1440/ 3200]\n",
      "loss: 0.870393  [ 1456/ 3200]\n",
      "loss: 1.089372  [ 1472/ 3200]\n",
      "loss: 0.947920  [ 1488/ 3200]\n",
      "loss: 0.968064  [ 1504/ 3200]\n",
      "loss: 1.223885  [ 1520/ 3200]\n",
      "loss: 1.054504  [ 1536/ 3200]\n",
      "loss: 1.011778  [ 1552/ 3200]\n",
      "loss: 1.172641  [ 1568/ 3200]\n",
      "loss: 1.517392  [ 1584/ 3200]\n",
      "loss: 1.220984  [ 1600/ 3200]\n",
      "loss: 1.235806  [ 1616/ 3200]\n",
      "loss: 1.034418  [ 1632/ 3200]\n",
      "loss: 1.028888  [ 1648/ 3200]\n",
      "loss: 1.028400  [ 1664/ 3200]\n",
      "loss: 1.195457  [ 1680/ 3200]\n",
      "loss: 1.081647  [ 1696/ 3200]\n",
      "loss: 1.023509  [ 1712/ 3200]\n",
      "loss: 0.798931  [ 1728/ 3200]\n",
      "loss: 0.949118  [ 1744/ 3200]\n",
      "loss: 1.080703  [ 1760/ 3200]\n",
      "loss: 0.998590  [ 1776/ 3200]\n",
      "loss: 0.920502  [ 1792/ 3200]\n",
      "loss: 1.190752  [ 1808/ 3200]\n",
      "loss: 1.029655  [ 1824/ 3200]\n",
      "loss: 1.175077  [ 1840/ 3200]\n",
      "loss: 1.266029  [ 1856/ 3200]\n",
      "loss: 0.846009  [ 1872/ 3200]\n",
      "loss: 0.896215  [ 1888/ 3200]\n",
      "loss: 1.179522  [ 1904/ 3200]\n",
      "loss: 0.938042  [ 1920/ 3200]\n",
      "loss: 0.872791  [ 1936/ 3200]\n",
      "loss: 1.159995  [ 1952/ 3200]\n",
      "loss: 0.971552  [ 1968/ 3200]\n",
      "loss: 1.052129  [ 1984/ 3200]\n",
      "loss: 0.883069  [ 2000/ 3200]\n",
      "loss: 0.967380  [ 2016/ 3200]\n",
      "loss: 1.426039  [ 2032/ 3200]\n",
      "loss: 1.187788  [ 2048/ 3200]\n",
      "loss: 0.967546  [ 2064/ 3200]\n",
      "loss: 0.887187  [ 2080/ 3200]\n",
      "loss: 0.977531  [ 2096/ 3200]\n",
      "loss: 1.237287  [ 2112/ 3200]\n",
      "loss: 0.881891  [ 2128/ 3200]\n",
      "loss: 1.023086  [ 2144/ 3200]\n",
      "loss: 1.165011  [ 2160/ 3200]\n",
      "loss: 0.697693  [ 2176/ 3200]\n",
      "loss: 1.071252  [ 2192/ 3200]\n",
      "loss: 0.963457  [ 2208/ 3200]\n",
      "loss: 0.991080  [ 2224/ 3200]\n",
      "loss: 1.321457  [ 2240/ 3200]\n",
      "loss: 1.005574  [ 2256/ 3200]\n",
      "loss: 1.146782  [ 2272/ 3200]\n",
      "loss: 1.017670  [ 2288/ 3200]\n",
      "loss: 1.128691  [ 2304/ 3200]\n",
      "loss: 1.135707  [ 2320/ 3200]\n",
      "loss: 0.934106  [ 2336/ 3200]\n",
      "loss: 1.089077  [ 2352/ 3200]\n",
      "loss: 1.037944  [ 2368/ 3200]\n",
      "loss: 1.209008  [ 2384/ 3200]\n",
      "loss: 0.957294  [ 2400/ 3200]\n",
      "loss: 0.875258  [ 2416/ 3200]\n",
      "loss: 0.915664  [ 2432/ 3200]\n",
      "loss: 1.083173  [ 2448/ 3200]\n",
      "loss: 0.909225  [ 2464/ 3200]\n",
      "loss: 1.060128  [ 2480/ 3200]\n",
      "loss: 1.225541  [ 2496/ 3200]\n",
      "loss: 1.169010  [ 2512/ 3200]\n",
      "loss: 1.096810  [ 2528/ 3200]\n",
      "loss: 1.029980  [ 2544/ 3200]\n",
      "loss: 0.834341  [ 2560/ 3200]\n",
      "loss: 0.800918  [ 2576/ 3200]\n",
      "loss: 0.963224  [ 2592/ 3200]\n",
      "loss: 0.698478  [ 2608/ 3200]\n",
      "loss: 1.358940  [ 2624/ 3200]\n",
      "loss: 1.238492  [ 2640/ 3200]\n",
      "loss: 1.183836  [ 2656/ 3200]\n",
      "loss: 1.054425  [ 2672/ 3200]\n",
      "loss: 1.130308  [ 2688/ 3200]\n",
      "loss: 1.227282  [ 2704/ 3200]\n",
      "loss: 1.018732  [ 2720/ 3200]\n",
      "loss: 1.147866  [ 2736/ 3200]\n",
      "loss: 0.942436  [ 2752/ 3200]\n",
      "loss: 1.090839  [ 2768/ 3200]\n",
      "loss: 1.015436  [ 2784/ 3200]\n",
      "loss: 1.279765  [ 2800/ 3200]\n",
      "loss: 1.218802  [ 2816/ 3200]\n",
      "loss: 0.987278  [ 2832/ 3200]\n",
      "loss: 0.802928  [ 2848/ 3200]\n",
      "loss: 1.182254  [ 2864/ 3200]\n",
      "loss: 1.138980  [ 2880/ 3200]\n",
      "loss: 1.157444  [ 2896/ 3200]\n",
      "loss: 0.969812  [ 2912/ 3200]\n",
      "loss: 0.866949  [ 2928/ 3200]\n",
      "loss: 1.105136  [ 2944/ 3200]\n",
      "loss: 0.895145  [ 2960/ 3200]\n",
      "loss: 1.054253  [ 2976/ 3200]\n",
      "loss: 1.098282  [ 2992/ 3200]\n",
      "loss: 0.951797  [ 3008/ 3200]\n",
      "loss: 1.271728  [ 3024/ 3200]\n",
      "loss: 1.059065  [ 3040/ 3200]\n",
      "loss: 1.272401  [ 3056/ 3200]\n",
      "loss: 1.211380  [ 3072/ 3200]\n",
      "loss: 1.083114  [ 3088/ 3200]\n",
      "loss: 1.371832  [ 3104/ 3200]\n",
      "loss: 1.139346  [ 3120/ 3200]\n",
      "loss: 1.089217  [ 3136/ 3200]\n",
      "loss: 0.976257  [ 3152/ 3200]\n",
      "loss: 1.117440  [ 3168/ 3200]\n",
      "loss: 0.941537  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 1.136348  [    0/ 3200]\n",
      "loss: 0.845942  [   16/ 3200]\n",
      "loss: 1.014438  [   32/ 3200]\n",
      "loss: 0.971532  [   48/ 3200]\n",
      "loss: 0.853566  [   64/ 3200]\n",
      "loss: 0.941308  [   80/ 3200]\n",
      "loss: 0.757758  [   96/ 3200]\n",
      "loss: 0.911034  [  112/ 3200]\n",
      "loss: 0.978953  [  128/ 3200]\n",
      "loss: 1.141343  [  144/ 3200]\n",
      "loss: 1.074498  [  160/ 3200]\n",
      "loss: 0.784546  [  176/ 3200]\n",
      "loss: 1.117877  [  192/ 3200]\n",
      "loss: 1.025774  [  208/ 3200]\n",
      "loss: 1.381839  [  224/ 3200]\n",
      "loss: 0.950043  [  240/ 3200]\n",
      "loss: 0.913934  [  256/ 3200]\n",
      "loss: 1.074518  [  272/ 3200]\n",
      "loss: 1.110004  [  288/ 3200]\n",
      "loss: 0.882920  [  304/ 3200]\n",
      "loss: 1.053977  [  320/ 3200]\n",
      "loss: 1.334673  [  336/ 3200]\n",
      "loss: 1.245720  [  352/ 3200]\n",
      "loss: 0.923538  [  368/ 3200]\n",
      "loss: 0.821913  [  384/ 3200]\n",
      "loss: 1.281435  [  400/ 3200]\n",
      "loss: 0.956177  [  416/ 3200]\n",
      "loss: 0.822739  [  432/ 3200]\n",
      "loss: 1.163032  [  448/ 3200]\n",
      "loss: 1.019269  [  464/ 3200]\n",
      "loss: 0.868549  [  480/ 3200]\n",
      "loss: 1.036971  [  496/ 3200]\n",
      "loss: 1.280530  [  512/ 3200]\n",
      "loss: 0.926193  [  528/ 3200]\n",
      "loss: 0.874571  [  544/ 3200]\n",
      "loss: 0.983697  [  560/ 3200]\n",
      "loss: 1.311137  [  576/ 3200]\n",
      "loss: 1.041602  [  592/ 3200]\n",
      "loss: 1.227935  [  608/ 3200]\n",
      "loss: 1.093566  [  624/ 3200]\n",
      "loss: 0.935515  [  640/ 3200]\n",
      "loss: 1.035375  [  656/ 3200]\n",
      "loss: 1.036374  [  672/ 3200]\n",
      "loss: 0.960144  [  688/ 3200]\n",
      "loss: 1.123599  [  704/ 3200]\n",
      "loss: 1.050592  [  720/ 3200]\n",
      "loss: 1.260816  [  736/ 3200]\n",
      "loss: 1.400456  [  752/ 3200]\n",
      "loss: 0.998005  [  768/ 3200]\n",
      "loss: 1.339789  [  784/ 3200]\n",
      "loss: 1.355276  [  800/ 3200]\n",
      "loss: 1.044070  [  816/ 3200]\n",
      "loss: 1.072238  [  832/ 3200]\n",
      "loss: 0.934765  [  848/ 3200]\n",
      "loss: 0.936018  [  864/ 3200]\n",
      "loss: 0.969679  [  880/ 3200]\n",
      "loss: 1.350923  [  896/ 3200]\n",
      "loss: 1.048863  [  912/ 3200]\n",
      "loss: 0.961057  [  928/ 3200]\n",
      "loss: 0.769190  [  944/ 3200]\n",
      "loss: 1.063837  [  960/ 3200]\n",
      "loss: 0.931291  [  976/ 3200]\n",
      "loss: 1.004366  [  992/ 3200]\n",
      "loss: 0.976835  [ 1008/ 3200]\n",
      "loss: 1.118678  [ 1024/ 3200]\n",
      "loss: 0.984688  [ 1040/ 3200]\n",
      "loss: 1.053621  [ 1056/ 3200]\n",
      "loss: 0.836492  [ 1072/ 3200]\n",
      "loss: 0.986517  [ 1088/ 3200]\n",
      "loss: 0.921964  [ 1104/ 3200]\n",
      "loss: 1.469277  [ 1120/ 3200]\n",
      "loss: 0.916146  [ 1136/ 3200]\n",
      "loss: 0.832954  [ 1152/ 3200]\n",
      "loss: 0.992127  [ 1168/ 3200]\n",
      "loss: 1.201725  [ 1184/ 3200]\n",
      "loss: 0.914533  [ 1200/ 3200]\n",
      "loss: 1.280167  [ 1216/ 3200]\n",
      "loss: 1.027470  [ 1232/ 3200]\n",
      "loss: 1.168922  [ 1248/ 3200]\n",
      "loss: 0.951833  [ 1264/ 3200]\n",
      "loss: 0.818304  [ 1280/ 3200]\n",
      "loss: 0.867017  [ 1296/ 3200]\n",
      "loss: 1.277188  [ 1312/ 3200]\n",
      "loss: 1.238940  [ 1328/ 3200]\n",
      "loss: 1.236730  [ 1344/ 3200]\n",
      "loss: 1.119215  [ 1360/ 3200]\n",
      "loss: 1.047674  [ 1376/ 3200]\n",
      "loss: 1.040874  [ 1392/ 3200]\n",
      "loss: 0.967480  [ 1408/ 3200]\n",
      "loss: 1.094374  [ 1424/ 3200]\n",
      "loss: 0.772679  [ 1440/ 3200]\n",
      "loss: 1.116847  [ 1456/ 3200]\n",
      "loss: 1.037620  [ 1472/ 3200]\n",
      "loss: 1.086954  [ 1488/ 3200]\n",
      "loss: 1.028311  [ 1504/ 3200]\n",
      "loss: 0.731925  [ 1520/ 3200]\n",
      "loss: 1.037678  [ 1536/ 3200]\n",
      "loss: 0.961405  [ 1552/ 3200]\n",
      "loss: 1.173300  [ 1568/ 3200]\n",
      "loss: 0.960608  [ 1584/ 3200]\n",
      "loss: 1.103639  [ 1600/ 3200]\n",
      "loss: 0.984453  [ 1616/ 3200]\n",
      "loss: 0.983032  [ 1632/ 3200]\n",
      "loss: 1.403762  [ 1648/ 3200]\n",
      "loss: 0.992441  [ 1664/ 3200]\n",
      "loss: 0.957664  [ 1680/ 3200]\n",
      "loss: 1.192919  [ 1696/ 3200]\n",
      "loss: 1.353226  [ 1712/ 3200]\n",
      "loss: 0.905354  [ 1728/ 3200]\n",
      "loss: 1.059594  [ 1744/ 3200]\n",
      "loss: 0.951502  [ 1760/ 3200]\n",
      "loss: 1.078023  [ 1776/ 3200]\n",
      "loss: 0.998841  [ 1792/ 3200]\n",
      "loss: 1.012064  [ 1808/ 3200]\n",
      "loss: 0.968704  [ 1824/ 3200]\n",
      "loss: 1.279455  [ 1840/ 3200]\n",
      "loss: 1.029911  [ 1856/ 3200]\n",
      "loss: 1.278171  [ 1872/ 3200]\n",
      "loss: 1.009705  [ 1888/ 3200]\n",
      "loss: 1.002257  [ 1904/ 3200]\n",
      "loss: 1.170711  [ 1920/ 3200]\n",
      "loss: 1.149506  [ 1936/ 3200]\n",
      "loss: 1.167947  [ 1952/ 3200]\n",
      "loss: 1.096091  [ 1968/ 3200]\n",
      "loss: 1.105180  [ 1984/ 3200]\n",
      "loss: 1.030519  [ 2000/ 3200]\n",
      "loss: 1.227612  [ 2016/ 3200]\n",
      "loss: 1.060066  [ 2032/ 3200]\n",
      "loss: 0.820600  [ 2048/ 3200]\n",
      "loss: 1.127877  [ 2064/ 3200]\n",
      "loss: 0.928011  [ 2080/ 3200]\n",
      "loss: 1.017039  [ 2096/ 3200]\n",
      "loss: 1.212715  [ 2112/ 3200]\n",
      "loss: 1.157658  [ 2128/ 3200]\n",
      "loss: 1.230425  [ 2144/ 3200]\n",
      "loss: 0.976120  [ 2160/ 3200]\n",
      "loss: 1.281450  [ 2176/ 3200]\n",
      "loss: 1.026940  [ 2192/ 3200]\n",
      "loss: 1.055244  [ 2208/ 3200]\n",
      "loss: 0.946226  [ 2224/ 3200]\n",
      "loss: 1.093397  [ 2240/ 3200]\n",
      "loss: 0.865185  [ 2256/ 3200]\n",
      "loss: 1.255814  [ 2272/ 3200]\n",
      "loss: 0.920936  [ 2288/ 3200]\n",
      "loss: 0.980051  [ 2304/ 3200]\n",
      "loss: 1.025330  [ 2320/ 3200]\n",
      "loss: 1.226836  [ 2336/ 3200]\n",
      "loss: 0.957463  [ 2352/ 3200]\n",
      "loss: 1.319548  [ 2368/ 3200]\n",
      "loss: 0.851346  [ 2384/ 3200]\n",
      "loss: 0.900800  [ 2400/ 3200]\n",
      "loss: 1.060760  [ 2416/ 3200]\n",
      "loss: 1.408013  [ 2432/ 3200]\n",
      "loss: 0.857759  [ 2448/ 3200]\n",
      "loss: 0.728190  [ 2464/ 3200]\n",
      "loss: 0.877667  [ 2480/ 3200]\n",
      "loss: 1.007935  [ 2496/ 3200]\n",
      "loss: 1.123433  [ 2512/ 3200]\n",
      "loss: 1.123807  [ 2528/ 3200]\n",
      "loss: 1.041589  [ 2544/ 3200]\n",
      "loss: 1.344453  [ 2560/ 3200]\n",
      "loss: 1.071534  [ 2576/ 3200]\n",
      "loss: 0.977818  [ 2592/ 3200]\n",
      "loss: 1.044414  [ 2608/ 3200]\n",
      "loss: 0.865159  [ 2624/ 3200]\n",
      "loss: 1.090634  [ 2640/ 3200]\n",
      "loss: 0.838086  [ 2656/ 3200]\n",
      "loss: 1.039921  [ 2672/ 3200]\n",
      "loss: 1.207944  [ 2688/ 3200]\n",
      "loss: 0.993883  [ 2704/ 3200]\n",
      "loss: 0.973261  [ 2720/ 3200]\n",
      "loss: 0.743746  [ 2736/ 3200]\n",
      "loss: 0.872663  [ 2752/ 3200]\n",
      "loss: 1.003566  [ 2768/ 3200]\n",
      "loss: 0.947791  [ 2784/ 3200]\n",
      "loss: 1.027187  [ 2800/ 3200]\n",
      "loss: 1.021333  [ 2816/ 3200]\n",
      "loss: 1.060193  [ 2832/ 3200]\n",
      "loss: 0.853978  [ 2848/ 3200]\n",
      "loss: 1.163216  [ 2864/ 3200]\n",
      "loss: 0.840733  [ 2880/ 3200]\n",
      "loss: 1.063120  [ 2896/ 3200]\n",
      "loss: 1.141801  [ 2912/ 3200]\n",
      "loss: 1.053245  [ 2928/ 3200]\n",
      "loss: 1.198953  [ 2944/ 3200]\n",
      "loss: 1.015464  [ 2960/ 3200]\n",
      "loss: 1.100079  [ 2976/ 3200]\n",
      "loss: 0.800038  [ 2992/ 3200]\n",
      "loss: 1.198854  [ 3008/ 3200]\n",
      "loss: 0.899615  [ 3024/ 3200]\n",
      "loss: 1.277076  [ 3040/ 3200]\n",
      "loss: 1.288392  [ 3056/ 3200]\n",
      "loss: 1.100371  [ 3072/ 3200]\n",
      "loss: 0.729778  [ 3088/ 3200]\n",
      "loss: 0.978775  [ 3104/ 3200]\n",
      "loss: 0.881924  [ 3120/ 3200]\n",
      "loss: 0.869775  [ 3136/ 3200]\n",
      "loss: 0.950170  [ 3152/ 3200]\n",
      "loss: 1.381914  [ 3168/ 3200]\n",
      "loss: 0.934162  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 1.033796  [    0/ 3200]\n",
      "loss: 1.074106  [   16/ 3200]\n",
      "loss: 0.826118  [   32/ 3200]\n",
      "loss: 1.022953  [   48/ 3200]\n",
      "loss: 1.057893  [   64/ 3200]\n",
      "loss: 1.052591  [   80/ 3200]\n",
      "loss: 0.957765  [   96/ 3200]\n",
      "loss: 0.861266  [  112/ 3200]\n",
      "loss: 1.216824  [  128/ 3200]\n",
      "loss: 1.029225  [  144/ 3200]\n",
      "loss: 1.066437  [  160/ 3200]\n",
      "loss: 0.905448  [  176/ 3200]\n",
      "loss: 1.103014  [  192/ 3200]\n",
      "loss: 0.946013  [  208/ 3200]\n",
      "loss: 0.818873  [  224/ 3200]\n",
      "loss: 1.001885  [  240/ 3200]\n",
      "loss: 1.004771  [  256/ 3200]\n",
      "loss: 0.930404  [  272/ 3200]\n",
      "loss: 0.730559  [  288/ 3200]\n",
      "loss: 1.085625  [  304/ 3200]\n",
      "loss: 1.386812  [  320/ 3200]\n",
      "loss: 1.255614  [  336/ 3200]\n",
      "loss: 1.471360  [  352/ 3200]\n",
      "loss: 1.131842  [  368/ 3200]\n",
      "loss: 1.170694  [  384/ 3200]\n",
      "loss: 0.960704  [  400/ 3200]\n",
      "loss: 1.082987  [  416/ 3200]\n",
      "loss: 0.962332  [  432/ 3200]\n",
      "loss: 1.029529  [  448/ 3200]\n",
      "loss: 0.836918  [  464/ 3200]\n",
      "loss: 1.091315  [  480/ 3200]\n",
      "loss: 1.074979  [  496/ 3200]\n",
      "loss: 0.914709  [  512/ 3200]\n",
      "loss: 1.016633  [  528/ 3200]\n",
      "loss: 1.097963  [  544/ 3200]\n",
      "loss: 1.815522  [  560/ 3200]\n",
      "loss: 0.969933  [  576/ 3200]\n",
      "loss: 0.976252  [  592/ 3200]\n",
      "loss: 1.209120  [  608/ 3200]\n",
      "loss: 1.152823  [  624/ 3200]\n",
      "loss: 1.148765  [  640/ 3200]\n",
      "loss: 1.061792  [  656/ 3200]\n",
      "loss: 0.891333  [  672/ 3200]\n",
      "loss: 0.957659  [  688/ 3200]\n",
      "loss: 1.112153  [  704/ 3200]\n",
      "loss: 1.077495  [  720/ 3200]\n",
      "loss: 0.950124  [  736/ 3200]\n",
      "loss: 0.801072  [  752/ 3200]\n",
      "loss: 0.982615  [  768/ 3200]\n",
      "loss: 1.250222  [  784/ 3200]\n",
      "loss: 0.795871  [  800/ 3200]\n",
      "loss: 1.069657  [  816/ 3200]\n",
      "loss: 1.128400  [  832/ 3200]\n",
      "loss: 0.899909  [  848/ 3200]\n",
      "loss: 0.902383  [  864/ 3200]\n",
      "loss: 0.741801  [  880/ 3200]\n",
      "loss: 1.177124  [  896/ 3200]\n",
      "loss: 1.382146  [  912/ 3200]\n",
      "loss: 0.949825  [  928/ 3200]\n",
      "loss: 0.930629  [  944/ 3200]\n",
      "loss: 1.051199  [  960/ 3200]\n",
      "loss: 1.393980  [  976/ 3200]\n",
      "loss: 1.137207  [  992/ 3200]\n",
      "loss: 1.068239  [ 1008/ 3200]\n",
      "loss: 1.134176  [ 1024/ 3200]\n",
      "loss: 0.925048  [ 1040/ 3200]\n",
      "loss: 0.773613  [ 1056/ 3200]\n",
      "loss: 0.979572  [ 1072/ 3200]\n",
      "loss: 1.309618  [ 1088/ 3200]\n",
      "loss: 0.748310  [ 1104/ 3200]\n",
      "loss: 0.937029  [ 1120/ 3200]\n",
      "loss: 0.970369  [ 1136/ 3200]\n",
      "loss: 0.855686  [ 1152/ 3200]\n",
      "loss: 0.922576  [ 1168/ 3200]\n",
      "loss: 0.974075  [ 1184/ 3200]\n",
      "loss: 1.123462  [ 1200/ 3200]\n",
      "loss: 1.131970  [ 1216/ 3200]\n",
      "loss: 1.192527  [ 1232/ 3200]\n",
      "loss: 0.853434  [ 1248/ 3200]\n",
      "loss: 1.033549  [ 1264/ 3200]\n",
      "loss: 1.027812  [ 1280/ 3200]\n",
      "loss: 0.669115  [ 1296/ 3200]\n",
      "loss: 0.949812  [ 1312/ 3200]\n",
      "loss: 1.061888  [ 1328/ 3200]\n",
      "loss: 0.814420  [ 1344/ 3200]\n",
      "loss: 1.239291  [ 1360/ 3200]\n",
      "loss: 1.242709  [ 1376/ 3200]\n",
      "loss: 1.344441  [ 1392/ 3200]\n",
      "loss: 1.135831  [ 1408/ 3200]\n",
      "loss: 1.073256  [ 1424/ 3200]\n",
      "loss: 0.981508  [ 1440/ 3200]\n",
      "loss: 0.698163  [ 1456/ 3200]\n",
      "loss: 1.205732  [ 1472/ 3200]\n",
      "loss: 0.963154  [ 1488/ 3200]\n",
      "loss: 1.112804  [ 1504/ 3200]\n",
      "loss: 1.047002  [ 1520/ 3200]\n",
      "loss: 1.039716  [ 1536/ 3200]\n",
      "loss: 1.182731  [ 1552/ 3200]\n",
      "loss: 1.038674  [ 1568/ 3200]\n",
      "loss: 1.102642  [ 1584/ 3200]\n",
      "loss: 1.012885  [ 1600/ 3200]\n",
      "loss: 1.160023  [ 1616/ 3200]\n",
      "loss: 0.910615  [ 1632/ 3200]\n",
      "loss: 0.814137  [ 1648/ 3200]\n",
      "loss: 0.834800  [ 1664/ 3200]\n",
      "loss: 1.028563  [ 1680/ 3200]\n",
      "loss: 0.998834  [ 1696/ 3200]\n",
      "loss: 0.866829  [ 1712/ 3200]\n",
      "loss: 1.305296  [ 1728/ 3200]\n",
      "loss: 1.171688  [ 1744/ 3200]\n",
      "loss: 1.064912  [ 1760/ 3200]\n",
      "loss: 0.956664  [ 1776/ 3200]\n",
      "loss: 1.240516  [ 1792/ 3200]\n",
      "loss: 0.953481  [ 1808/ 3200]\n",
      "loss: 0.911454  [ 1824/ 3200]\n",
      "loss: 1.049529  [ 1840/ 3200]\n",
      "loss: 0.925766  [ 1856/ 3200]\n",
      "loss: 1.073203  [ 1872/ 3200]\n",
      "loss: 0.700349  [ 1888/ 3200]\n",
      "loss: 1.250116  [ 1904/ 3200]\n",
      "loss: 0.913390  [ 1920/ 3200]\n",
      "loss: 0.891295  [ 1936/ 3200]\n",
      "loss: 1.064737  [ 1952/ 3200]\n",
      "loss: 0.837508  [ 1968/ 3200]\n",
      "loss: 0.974089  [ 1984/ 3200]\n",
      "loss: 1.178267  [ 2000/ 3200]\n",
      "loss: 1.028526  [ 2016/ 3200]\n",
      "loss: 0.817830  [ 2032/ 3200]\n",
      "loss: 0.898243  [ 2048/ 3200]\n",
      "loss: 0.880908  [ 2064/ 3200]\n",
      "loss: 1.117597  [ 2080/ 3200]\n",
      "loss: 0.905189  [ 2096/ 3200]\n",
      "loss: 1.262971  [ 2112/ 3200]\n",
      "loss: 1.057465  [ 2128/ 3200]\n",
      "loss: 1.059615  [ 2144/ 3200]\n",
      "loss: 0.994479  [ 2160/ 3200]\n",
      "loss: 0.779156  [ 2176/ 3200]\n",
      "loss: 1.036917  [ 2192/ 3200]\n",
      "loss: 1.307381  [ 2208/ 3200]\n",
      "loss: 1.391542  [ 2224/ 3200]\n",
      "loss: 1.048394  [ 2240/ 3200]\n",
      "loss: 1.045466  [ 2256/ 3200]\n",
      "loss: 1.033096  [ 2272/ 3200]\n",
      "loss: 1.091857  [ 2288/ 3200]\n",
      "loss: 0.799037  [ 2304/ 3200]\n",
      "loss: 0.933177  [ 2320/ 3200]\n",
      "loss: 0.882276  [ 2336/ 3200]\n",
      "loss: 1.072625  [ 2352/ 3200]\n",
      "loss: 0.840344  [ 2368/ 3200]\n",
      "loss: 1.245491  [ 2384/ 3200]\n",
      "loss: 1.020562  [ 2400/ 3200]\n",
      "loss: 0.879441  [ 2416/ 3200]\n",
      "loss: 1.098990  [ 2432/ 3200]\n",
      "loss: 1.008216  [ 2448/ 3200]\n",
      "loss: 1.312712  [ 2464/ 3200]\n",
      "loss: 0.853436  [ 2480/ 3200]\n",
      "loss: 0.839495  [ 2496/ 3200]\n",
      "loss: 0.810799  [ 2512/ 3200]\n",
      "loss: 1.032809  [ 2528/ 3200]\n",
      "loss: 1.040119  [ 2544/ 3200]\n",
      "loss: 1.114782  [ 2560/ 3200]\n",
      "loss: 0.634174  [ 2576/ 3200]\n",
      "loss: 1.133984  [ 2592/ 3200]\n",
      "loss: 1.038350  [ 2608/ 3200]\n",
      "loss: 1.154304  [ 2624/ 3200]\n",
      "loss: 0.888199  [ 2640/ 3200]\n",
      "loss: 1.279869  [ 2656/ 3200]\n",
      "loss: 1.045589  [ 2672/ 3200]\n",
      "loss: 0.976005  [ 2688/ 3200]\n",
      "loss: 0.946449  [ 2704/ 3200]\n",
      "loss: 0.844845  [ 2720/ 3200]\n",
      "loss: 0.821549  [ 2736/ 3200]\n",
      "loss: 0.943821  [ 2752/ 3200]\n",
      "loss: 1.213409  [ 2768/ 3200]\n",
      "loss: 0.848312  [ 2784/ 3200]\n",
      "loss: 1.014080  [ 2800/ 3200]\n",
      "loss: 1.328202  [ 2816/ 3200]\n",
      "loss: 1.091707  [ 2832/ 3200]\n",
      "loss: 1.251858  [ 2848/ 3200]\n",
      "loss: 1.238713  [ 2864/ 3200]\n",
      "loss: 1.020062  [ 2880/ 3200]\n",
      "loss: 0.839526  [ 2896/ 3200]\n",
      "loss: 0.966248  [ 2912/ 3200]\n",
      "loss: 1.296978  [ 2928/ 3200]\n",
      "loss: 0.879666  [ 2944/ 3200]\n",
      "loss: 0.989426  [ 2960/ 3200]\n",
      "loss: 0.654490  [ 2976/ 3200]\n",
      "loss: 1.075524  [ 2992/ 3200]\n",
      "loss: 0.909040  [ 3008/ 3200]\n",
      "loss: 1.174480  [ 3024/ 3200]\n",
      "loss: 1.174843  [ 3040/ 3200]\n",
      "loss: 0.906105  [ 3056/ 3200]\n",
      "loss: 1.033771  [ 3072/ 3200]\n",
      "loss: 0.911010  [ 3088/ 3200]\n",
      "loss: 1.073653  [ 3104/ 3200]\n",
      "loss: 0.877393  [ 3120/ 3200]\n",
      "loss: 1.201179  [ 3136/ 3200]\n",
      "loss: 1.076485  [ 3152/ 3200]\n",
      "loss: 0.888798  [ 3168/ 3200]\n",
      "loss: 1.111513  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 1.037232  [    0/ 3200]\n",
      "loss: 0.894807  [   16/ 3200]\n",
      "loss: 0.825845  [   32/ 3200]\n",
      "loss: 1.029294  [   48/ 3200]\n",
      "loss: 0.953515  [   64/ 3200]\n",
      "loss: 1.026562  [   80/ 3200]\n",
      "loss: 0.849543  [   96/ 3200]\n",
      "loss: 0.867166  [  112/ 3200]\n",
      "loss: 1.175301  [  128/ 3200]\n",
      "loss: 1.041551  [  144/ 3200]\n",
      "loss: 0.992001  [  160/ 3200]\n",
      "loss: 0.840259  [  176/ 3200]\n",
      "loss: 0.984850  [  192/ 3200]\n",
      "loss: 1.007973  [  208/ 3200]\n",
      "loss: 1.008414  [  224/ 3200]\n",
      "loss: 0.809249  [  240/ 3200]\n",
      "loss: 1.051623  [  256/ 3200]\n",
      "loss: 1.164256  [  272/ 3200]\n",
      "loss: 1.179940  [  288/ 3200]\n",
      "loss: 1.256454  [  304/ 3200]\n",
      "loss: 0.959468  [  320/ 3200]\n",
      "loss: 1.209230  [  336/ 3200]\n",
      "loss: 1.119800  [  352/ 3200]\n",
      "loss: 1.040032  [  368/ 3200]\n",
      "loss: 1.014227  [  384/ 3200]\n",
      "loss: 1.007829  [  400/ 3200]\n",
      "loss: 1.242231  [  416/ 3200]\n",
      "loss: 0.971423  [  432/ 3200]\n",
      "loss: 1.000439  [  448/ 3200]\n",
      "loss: 0.973409  [  464/ 3200]\n",
      "loss: 0.924643  [  480/ 3200]\n",
      "loss: 1.118662  [  496/ 3200]\n",
      "loss: 1.135328  [  512/ 3200]\n",
      "loss: 1.226061  [  528/ 3200]\n",
      "loss: 0.956010  [  544/ 3200]\n",
      "loss: 1.066937  [  560/ 3200]\n",
      "loss: 0.761624  [  576/ 3200]\n",
      "loss: 0.985414  [  592/ 3200]\n",
      "loss: 1.230585  [  608/ 3200]\n",
      "loss: 1.321512  [  624/ 3200]\n",
      "loss: 1.213722  [  640/ 3200]\n",
      "loss: 0.831161  [  656/ 3200]\n",
      "loss: 1.152385  [  672/ 3200]\n",
      "loss: 1.049751  [  688/ 3200]\n",
      "loss: 0.877119  [  704/ 3200]\n",
      "loss: 1.099327  [  720/ 3200]\n",
      "loss: 1.083095  [  736/ 3200]\n",
      "loss: 1.044629  [  752/ 3200]\n",
      "loss: 1.163345  [  768/ 3200]\n",
      "loss: 1.010824  [  784/ 3200]\n",
      "loss: 1.126485  [  800/ 3200]\n",
      "loss: 1.327727  [  816/ 3200]\n",
      "loss: 1.013761  [  832/ 3200]\n",
      "loss: 1.114522  [  848/ 3200]\n",
      "loss: 0.800084  [  864/ 3200]\n",
      "loss: 1.104549  [  880/ 3200]\n",
      "loss: 0.888374  [  896/ 3200]\n",
      "loss: 0.880002  [  912/ 3200]\n",
      "loss: 1.105970  [  928/ 3200]\n",
      "loss: 0.881932  [  944/ 3200]\n",
      "loss: 0.852773  [  960/ 3200]\n",
      "loss: 1.138017  [  976/ 3200]\n",
      "loss: 1.307144  [  992/ 3200]\n",
      "loss: 1.210162  [ 1008/ 3200]\n",
      "loss: 0.746053  [ 1024/ 3200]\n",
      "loss: 0.798831  [ 1040/ 3200]\n",
      "loss: 0.841963  [ 1056/ 3200]\n",
      "loss: 0.818748  [ 1072/ 3200]\n",
      "loss: 1.415905  [ 1088/ 3200]\n",
      "loss: 1.003248  [ 1104/ 3200]\n",
      "loss: 0.894841  [ 1120/ 3200]\n",
      "loss: 0.961592  [ 1136/ 3200]\n",
      "loss: 1.309136  [ 1152/ 3200]\n",
      "loss: 0.815228  [ 1168/ 3200]\n",
      "loss: 1.019564  [ 1184/ 3200]\n",
      "loss: 0.899672  [ 1200/ 3200]\n",
      "loss: 1.031219  [ 1216/ 3200]\n",
      "loss: 0.984608  [ 1232/ 3200]\n",
      "loss: 1.354880  [ 1248/ 3200]\n",
      "loss: 0.973809  [ 1264/ 3200]\n",
      "loss: 0.726089  [ 1280/ 3200]\n",
      "loss: 0.784506  [ 1296/ 3200]\n",
      "loss: 0.806628  [ 1312/ 3200]\n",
      "loss: 1.111642  [ 1328/ 3200]\n",
      "loss: 1.017565  [ 1344/ 3200]\n",
      "loss: 1.043810  [ 1360/ 3200]\n",
      "loss: 0.962413  [ 1376/ 3200]\n",
      "loss: 0.903712  [ 1392/ 3200]\n",
      "loss: 1.023748  [ 1408/ 3200]\n",
      "loss: 0.831682  [ 1424/ 3200]\n",
      "loss: 1.183365  [ 1440/ 3200]\n",
      "loss: 0.964424  [ 1456/ 3200]\n",
      "loss: 0.847453  [ 1472/ 3200]\n",
      "loss: 1.222389  [ 1488/ 3200]\n",
      "loss: 1.038988  [ 1504/ 3200]\n",
      "loss: 0.766497  [ 1520/ 3200]\n",
      "loss: 0.789387  [ 1536/ 3200]\n",
      "loss: 1.082429  [ 1552/ 3200]\n",
      "loss: 0.996255  [ 1568/ 3200]\n",
      "loss: 1.101392  [ 1584/ 3200]\n",
      "loss: 1.240812  [ 1600/ 3200]\n",
      "loss: 1.064134  [ 1616/ 3200]\n",
      "loss: 1.130432  [ 1632/ 3200]\n",
      "loss: 1.104121  [ 1648/ 3200]\n",
      "loss: 0.974985  [ 1664/ 3200]\n",
      "loss: 0.754780  [ 1680/ 3200]\n",
      "loss: 0.925185  [ 1696/ 3200]\n",
      "loss: 1.042642  [ 1712/ 3200]\n",
      "loss: 1.060790  [ 1728/ 3200]\n",
      "loss: 0.924547  [ 1744/ 3200]\n",
      "loss: 0.964205  [ 1760/ 3200]\n",
      "loss: 0.932440  [ 1776/ 3200]\n",
      "loss: 0.839356  [ 1792/ 3200]\n",
      "loss: 0.868605  [ 1808/ 3200]\n",
      "loss: 1.287223  [ 1824/ 3200]\n",
      "loss: 1.055821  [ 1840/ 3200]\n",
      "loss: 0.849391  [ 1856/ 3200]\n",
      "loss: 1.001442  [ 1872/ 3200]\n",
      "loss: 0.973733  [ 1888/ 3200]\n",
      "loss: 0.727845  [ 1904/ 3200]\n",
      "loss: 0.980154  [ 1920/ 3200]\n",
      "loss: 1.123200  [ 1936/ 3200]\n",
      "loss: 1.543721  [ 1952/ 3200]\n",
      "loss: 0.970758  [ 1968/ 3200]\n",
      "loss: 1.004789  [ 1984/ 3200]\n",
      "loss: 1.041601  [ 2000/ 3200]\n",
      "loss: 0.948574  [ 2016/ 3200]\n",
      "loss: 1.121523  [ 2032/ 3200]\n",
      "loss: 1.032506  [ 2048/ 3200]\n",
      "loss: 0.930634  [ 2064/ 3200]\n",
      "loss: 0.825138  [ 2080/ 3200]\n",
      "loss: 1.171745  [ 2096/ 3200]\n",
      "loss: 0.761163  [ 2112/ 3200]\n",
      "loss: 1.025870  [ 2128/ 3200]\n",
      "loss: 0.849802  [ 2144/ 3200]\n",
      "loss: 1.161552  [ 2160/ 3200]\n",
      "loss: 1.000996  [ 2176/ 3200]\n",
      "loss: 0.697379  [ 2192/ 3200]\n",
      "loss: 1.251646  [ 2208/ 3200]\n",
      "loss: 1.001330  [ 2224/ 3200]\n",
      "loss: 0.952217  [ 2240/ 3200]\n",
      "loss: 0.826639  [ 2256/ 3200]\n",
      "loss: 0.989929  [ 2272/ 3200]\n",
      "loss: 1.037476  [ 2288/ 3200]\n",
      "loss: 1.004773  [ 2304/ 3200]\n",
      "loss: 0.866412  [ 2320/ 3200]\n",
      "loss: 0.729797  [ 2336/ 3200]\n",
      "loss: 1.253458  [ 2352/ 3200]\n",
      "loss: 1.054670  [ 2368/ 3200]\n",
      "loss: 1.139910  [ 2384/ 3200]\n",
      "loss: 0.927126  [ 2400/ 3200]\n",
      "loss: 1.004165  [ 2416/ 3200]\n",
      "loss: 0.945758  [ 2432/ 3200]\n",
      "loss: 0.819753  [ 2448/ 3200]\n",
      "loss: 0.993812  [ 2464/ 3200]\n",
      "loss: 0.834630  [ 2480/ 3200]\n",
      "loss: 1.173660  [ 2496/ 3200]\n",
      "loss: 1.309760  [ 2512/ 3200]\n",
      "loss: 0.789463  [ 2528/ 3200]\n",
      "loss: 1.017879  [ 2544/ 3200]\n",
      "loss: 0.883799  [ 2560/ 3200]\n",
      "loss: 0.887366  [ 2576/ 3200]\n",
      "loss: 0.810025  [ 2592/ 3200]\n",
      "loss: 1.428609  [ 2608/ 3200]\n",
      "loss: 1.061975  [ 2624/ 3200]\n",
      "loss: 1.051053  [ 2640/ 3200]\n",
      "loss: 1.138565  [ 2656/ 3200]\n",
      "loss: 0.955591  [ 2672/ 3200]\n",
      "loss: 0.829168  [ 2688/ 3200]\n",
      "loss: 1.026530  [ 2704/ 3200]\n",
      "loss: 1.013573  [ 2720/ 3200]\n",
      "loss: 0.867377  [ 2736/ 3200]\n",
      "loss: 1.050446  [ 2752/ 3200]\n",
      "loss: 0.803452  [ 2768/ 3200]\n",
      "loss: 0.712826  [ 2784/ 3200]\n",
      "loss: 1.119402  [ 2800/ 3200]\n",
      "loss: 0.562525  [ 2816/ 3200]\n",
      "loss: 0.797257  [ 2832/ 3200]\n",
      "loss: 1.314319  [ 2848/ 3200]\n",
      "loss: 0.874518  [ 2864/ 3200]\n",
      "loss: 1.103908  [ 2880/ 3200]\n",
      "loss: 0.731463  [ 2896/ 3200]\n",
      "loss: 0.940165  [ 2912/ 3200]\n",
      "loss: 0.994972  [ 2928/ 3200]\n",
      "loss: 0.889953  [ 2944/ 3200]\n",
      "loss: 0.833348  [ 2960/ 3200]\n",
      "loss: 1.036077  [ 2976/ 3200]\n",
      "loss: 1.059424  [ 2992/ 3200]\n",
      "loss: 0.855381  [ 3008/ 3200]\n",
      "loss: 1.142722  [ 3024/ 3200]\n",
      "loss: 0.877614  [ 3040/ 3200]\n",
      "loss: 1.145547  [ 3056/ 3200]\n",
      "loss: 0.979366  [ 3072/ 3200]\n",
      "loss: 0.902378  [ 3088/ 3200]\n",
      "loss: 1.257903  [ 3104/ 3200]\n",
      "loss: 0.957505  [ 3120/ 3200]\n",
      "loss: 0.969074  [ 3136/ 3200]\n",
      "loss: 0.809022  [ 3152/ 3200]\n",
      "loss: 0.984873  [ 3168/ 3200]\n",
      "loss: 1.057456  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 0.892654  [    0/ 3200]\n",
      "loss: 1.062722  [   16/ 3200]\n",
      "loss: 1.146003  [   32/ 3200]\n",
      "loss: 1.022650  [   48/ 3200]\n",
      "loss: 0.987406  [   64/ 3200]\n",
      "loss: 0.844937  [   80/ 3200]\n",
      "loss: 1.082798  [   96/ 3200]\n",
      "loss: 1.129175  [  112/ 3200]\n",
      "loss: 0.979472  [  128/ 3200]\n",
      "loss: 0.992369  [  144/ 3200]\n",
      "loss: 0.830033  [  160/ 3200]\n",
      "loss: 0.919944  [  176/ 3200]\n",
      "loss: 0.920367  [  192/ 3200]\n",
      "loss: 0.828899  [  208/ 3200]\n",
      "loss: 0.791328  [  224/ 3200]\n",
      "loss: 0.849278  [  240/ 3200]\n",
      "loss: 1.329055  [  256/ 3200]\n",
      "loss: 1.063027  [  272/ 3200]\n",
      "loss: 0.881032  [  288/ 3200]\n",
      "loss: 1.093043  [  304/ 3200]\n",
      "loss: 0.864060  [  320/ 3200]\n",
      "loss: 1.084345  [  336/ 3200]\n",
      "loss: 1.030631  [  352/ 3200]\n",
      "loss: 1.026679  [  368/ 3200]\n",
      "loss: 1.248059  [  384/ 3200]\n",
      "loss: 0.892184  [  400/ 3200]\n",
      "loss: 0.900655  [  416/ 3200]\n",
      "loss: 1.091147  [  432/ 3200]\n",
      "loss: 0.907976  [  448/ 3200]\n",
      "loss: 0.904262  [  464/ 3200]\n",
      "loss: 1.195920  [  480/ 3200]\n",
      "loss: 0.959168  [  496/ 3200]\n",
      "loss: 0.827916  [  512/ 3200]\n",
      "loss: 1.000020  [  528/ 3200]\n",
      "loss: 1.169993  [  544/ 3200]\n",
      "loss: 0.978972  [  560/ 3200]\n",
      "loss: 0.942340  [  576/ 3200]\n",
      "loss: 0.757582  [  592/ 3200]\n",
      "loss: 0.890440  [  608/ 3200]\n",
      "loss: 0.689939  [  624/ 3200]\n",
      "loss: 0.849711  [  640/ 3200]\n",
      "loss: 0.896196  [  656/ 3200]\n",
      "loss: 1.220982  [  672/ 3200]\n",
      "loss: 1.056553  [  688/ 3200]\n",
      "loss: 0.920935  [  704/ 3200]\n",
      "loss: 0.857907  [  720/ 3200]\n",
      "loss: 1.578130  [  736/ 3200]\n",
      "loss: 0.919847  [  752/ 3200]\n",
      "loss: 1.052729  [  768/ 3200]\n",
      "loss: 1.147836  [  784/ 3200]\n",
      "loss: 0.940513  [  800/ 3200]\n",
      "loss: 0.753911  [  816/ 3200]\n",
      "loss: 0.962589  [  832/ 3200]\n",
      "loss: 1.209807  [  848/ 3200]\n",
      "loss: 1.145139  [  864/ 3200]\n",
      "loss: 1.299824  [  880/ 3200]\n",
      "loss: 0.926083  [  896/ 3200]\n",
      "loss: 1.202548  [  912/ 3200]\n",
      "loss: 1.225257  [  928/ 3200]\n",
      "loss: 0.801146  [  944/ 3200]\n",
      "loss: 0.931945  [  960/ 3200]\n",
      "loss: 0.948933  [  976/ 3200]\n",
      "loss: 0.897661  [  992/ 3200]\n",
      "loss: 0.883058  [ 1008/ 3200]\n",
      "loss: 1.185289  [ 1024/ 3200]\n",
      "loss: 0.613269  [ 1040/ 3200]\n",
      "loss: 0.965304  [ 1056/ 3200]\n",
      "loss: 0.976350  [ 1072/ 3200]\n",
      "loss: 1.208872  [ 1088/ 3200]\n",
      "loss: 0.951853  [ 1104/ 3200]\n",
      "loss: 1.121118  [ 1120/ 3200]\n",
      "loss: 0.901116  [ 1136/ 3200]\n",
      "loss: 0.949904  [ 1152/ 3200]\n",
      "loss: 0.783050  [ 1168/ 3200]\n",
      "loss: 0.886886  [ 1184/ 3200]\n",
      "loss: 0.784521  [ 1200/ 3200]\n",
      "loss: 1.114737  [ 1216/ 3200]\n",
      "loss: 0.943787  [ 1232/ 3200]\n",
      "loss: 0.817644  [ 1248/ 3200]\n",
      "loss: 0.842370  [ 1264/ 3200]\n",
      "loss: 0.744595  [ 1280/ 3200]\n",
      "loss: 1.037246  [ 1296/ 3200]\n",
      "loss: 1.404892  [ 1312/ 3200]\n",
      "loss: 1.072235  [ 1328/ 3200]\n",
      "loss: 1.161617  [ 1344/ 3200]\n",
      "loss: 0.834371  [ 1360/ 3200]\n",
      "loss: 1.091002  [ 1376/ 3200]\n",
      "loss: 0.920271  [ 1392/ 3200]\n",
      "loss: 1.148838  [ 1408/ 3200]\n",
      "loss: 1.239946  [ 1424/ 3200]\n",
      "loss: 1.078632  [ 1440/ 3200]\n",
      "loss: 1.282423  [ 1456/ 3200]\n",
      "loss: 0.940004  [ 1472/ 3200]\n",
      "loss: 0.925943  [ 1488/ 3200]\n",
      "loss: 1.116233  [ 1504/ 3200]\n",
      "loss: 1.111154  [ 1520/ 3200]\n",
      "loss: 1.033483  [ 1536/ 3200]\n",
      "loss: 1.320977  [ 1552/ 3200]\n",
      "loss: 1.052724  [ 1568/ 3200]\n",
      "loss: 1.047098  [ 1584/ 3200]\n",
      "loss: 0.859339  [ 1600/ 3200]\n",
      "loss: 0.978500  [ 1616/ 3200]\n",
      "loss: 0.788788  [ 1632/ 3200]\n",
      "loss: 0.877208  [ 1648/ 3200]\n",
      "loss: 0.995153  [ 1664/ 3200]\n",
      "loss: 0.724260  [ 1680/ 3200]\n",
      "loss: 0.882551  [ 1696/ 3200]\n",
      "loss: 1.352771  [ 1712/ 3200]\n",
      "loss: 1.102241  [ 1728/ 3200]\n",
      "loss: 1.030000  [ 1744/ 3200]\n",
      "loss: 1.118540  [ 1760/ 3200]\n",
      "loss: 0.903853  [ 1776/ 3200]\n",
      "loss: 0.916381  [ 1792/ 3200]\n",
      "loss: 0.866730  [ 1808/ 3200]\n",
      "loss: 0.993759  [ 1824/ 3200]\n",
      "loss: 0.718315  [ 1840/ 3200]\n",
      "loss: 1.084867  [ 1856/ 3200]\n",
      "loss: 1.116336  [ 1872/ 3200]\n",
      "loss: 0.828053  [ 1888/ 3200]\n",
      "loss: 1.040208  [ 1904/ 3200]\n",
      "loss: 0.724818  [ 1920/ 3200]\n",
      "loss: 1.155523  [ 1936/ 3200]\n",
      "loss: 1.033440  [ 1952/ 3200]\n",
      "loss: 1.122412  [ 1968/ 3200]\n",
      "loss: 0.891861  [ 1984/ 3200]\n",
      "loss: 1.012046  [ 2000/ 3200]\n",
      "loss: 0.921661  [ 2016/ 3200]\n",
      "loss: 1.039980  [ 2032/ 3200]\n",
      "loss: 1.051987  [ 2048/ 3200]\n",
      "loss: 1.137326  [ 2064/ 3200]\n",
      "loss: 0.837211  [ 2080/ 3200]\n",
      "loss: 0.921034  [ 2096/ 3200]\n",
      "loss: 1.463243  [ 2112/ 3200]\n",
      "loss: 0.868121  [ 2128/ 3200]\n",
      "loss: 1.304774  [ 2144/ 3200]\n",
      "loss: 1.085034  [ 2160/ 3200]\n",
      "loss: 1.085204  [ 2176/ 3200]\n",
      "loss: 0.918667  [ 2192/ 3200]\n",
      "loss: 0.829942  [ 2208/ 3200]\n",
      "loss: 0.973721  [ 2224/ 3200]\n",
      "loss: 0.848209  [ 2240/ 3200]\n",
      "loss: 0.899776  [ 2256/ 3200]\n",
      "loss: 0.856015  [ 2272/ 3200]\n",
      "loss: 0.958558  [ 2288/ 3200]\n",
      "loss: 0.912368  [ 2304/ 3200]\n",
      "loss: 1.045956  [ 2320/ 3200]\n",
      "loss: 1.045181  [ 2336/ 3200]\n",
      "loss: 0.964687  [ 2352/ 3200]\n",
      "loss: 0.978035  [ 2368/ 3200]\n",
      "loss: 0.932454  [ 2384/ 3200]\n",
      "loss: 0.809165  [ 2400/ 3200]\n",
      "loss: 1.152798  [ 2416/ 3200]\n",
      "loss: 1.051287  [ 2432/ 3200]\n",
      "loss: 0.887766  [ 2448/ 3200]\n",
      "loss: 0.662935  [ 2464/ 3200]\n",
      "loss: 1.354184  [ 2480/ 3200]\n",
      "loss: 0.896134  [ 2496/ 3200]\n",
      "loss: 0.737718  [ 2512/ 3200]\n",
      "loss: 1.199467  [ 2528/ 3200]\n",
      "loss: 0.763159  [ 2544/ 3200]\n",
      "loss: 1.124582  [ 2560/ 3200]\n",
      "loss: 1.004416  [ 2576/ 3200]\n",
      "loss: 0.887915  [ 2592/ 3200]\n",
      "loss: 0.856962  [ 2608/ 3200]\n",
      "loss: 1.006598  [ 2624/ 3200]\n",
      "loss: 1.237426  [ 2640/ 3200]\n",
      "loss: 0.896496  [ 2656/ 3200]\n",
      "loss: 0.951502  [ 2672/ 3200]\n",
      "loss: 0.849075  [ 2688/ 3200]\n",
      "loss: 1.319681  [ 2704/ 3200]\n",
      "loss: 0.924681  [ 2720/ 3200]\n",
      "loss: 0.934836  [ 2736/ 3200]\n",
      "loss: 1.081084  [ 2752/ 3200]\n",
      "loss: 1.338393  [ 2768/ 3200]\n",
      "loss: 1.207362  [ 2784/ 3200]\n",
      "loss: 1.213567  [ 2800/ 3200]\n",
      "loss: 1.066908  [ 2816/ 3200]\n",
      "loss: 1.133402  [ 2832/ 3200]\n",
      "loss: 0.626117  [ 2848/ 3200]\n",
      "loss: 0.885148  [ 2864/ 3200]\n",
      "loss: 1.003941  [ 2880/ 3200]\n",
      "loss: 0.938712  [ 2896/ 3200]\n",
      "loss: 1.317928  [ 2912/ 3200]\n",
      "loss: 0.875160  [ 2928/ 3200]\n",
      "loss: 0.921404  [ 2944/ 3200]\n",
      "loss: 1.063702  [ 2960/ 3200]\n",
      "loss: 1.049217  [ 2976/ 3200]\n",
      "loss: 1.245234  [ 2992/ 3200]\n",
      "loss: 1.132539  [ 3008/ 3200]\n",
      "loss: 0.777380  [ 3024/ 3200]\n",
      "loss: 0.895404  [ 3040/ 3200]\n",
      "loss: 0.947026  [ 3056/ 3200]\n",
      "loss: 0.964879  [ 3072/ 3200]\n",
      "loss: 0.950696  [ 3088/ 3200]\n",
      "loss: 0.900195  [ 3104/ 3200]\n",
      "loss: 0.858227  [ 3120/ 3200]\n",
      "loss: 0.742414  [ 3136/ 3200]\n",
      "loss: 0.938579  [ 3152/ 3200]\n",
      "loss: 0.817450  [ 3168/ 3200]\n",
      "loss: 0.940811  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.802510  [    0/ 3200]\n",
      "loss: 0.708114  [   16/ 3200]\n",
      "loss: 0.742080  [   32/ 3200]\n",
      "loss: 0.974576  [   48/ 3200]\n",
      "loss: 1.053831  [   64/ 3200]\n",
      "loss: 1.010954  [   80/ 3200]\n",
      "loss: 1.040809  [   96/ 3200]\n",
      "loss: 0.837971  [  112/ 3200]\n",
      "loss: 1.111257  [  128/ 3200]\n",
      "loss: 0.780742  [  144/ 3200]\n",
      "loss: 0.806627  [  160/ 3200]\n",
      "loss: 1.073418  [  176/ 3200]\n",
      "loss: 1.123633  [  192/ 3200]\n",
      "loss: 1.056905  [  208/ 3200]\n",
      "loss: 0.953114  [  224/ 3200]\n",
      "loss: 0.865296  [  240/ 3200]\n",
      "loss: 0.861741  [  256/ 3200]\n",
      "loss: 1.075615  [  272/ 3200]\n",
      "loss: 0.827456  [  288/ 3200]\n",
      "loss: 0.763771  [  304/ 3200]\n",
      "loss: 0.716161  [  320/ 3200]\n",
      "loss: 0.867774  [  336/ 3200]\n",
      "loss: 0.864937  [  352/ 3200]\n",
      "loss: 0.777812  [  368/ 3200]\n",
      "loss: 0.931418  [  384/ 3200]\n",
      "loss: 0.938724  [  400/ 3200]\n",
      "loss: 1.228495  [  416/ 3200]\n",
      "loss: 1.220046  [  432/ 3200]\n",
      "loss: 1.090633  [  448/ 3200]\n",
      "loss: 0.748022  [  464/ 3200]\n",
      "loss: 1.260517  [  480/ 3200]\n",
      "loss: 1.106815  [  496/ 3200]\n",
      "loss: 1.327352  [  512/ 3200]\n",
      "loss: 0.901076  [  528/ 3200]\n",
      "loss: 0.787860  [  544/ 3200]\n",
      "loss: 1.125656  [  560/ 3200]\n",
      "loss: 1.150145  [  576/ 3200]\n",
      "loss: 0.952244  [  592/ 3200]\n",
      "loss: 1.098674  [  608/ 3200]\n",
      "loss: 0.786244  [  624/ 3200]\n",
      "loss: 1.033627  [  640/ 3200]\n",
      "loss: 0.951765  [  656/ 3200]\n",
      "loss: 0.994822  [  672/ 3200]\n",
      "loss: 0.879138  [  688/ 3200]\n",
      "loss: 1.398603  [  704/ 3200]\n",
      "loss: 0.975272  [  720/ 3200]\n",
      "loss: 0.787274  [  736/ 3200]\n",
      "loss: 0.986230  [  752/ 3200]\n",
      "loss: 1.006702  [  768/ 3200]\n",
      "loss: 1.020737  [  784/ 3200]\n",
      "loss: 1.059035  [  800/ 3200]\n",
      "loss: 0.957913  [  816/ 3200]\n",
      "loss: 0.855735  [  832/ 3200]\n",
      "loss: 0.794784  [  848/ 3200]\n",
      "loss: 1.048048  [  864/ 3200]\n",
      "loss: 1.339984  [  880/ 3200]\n",
      "loss: 1.000892  [  896/ 3200]\n",
      "loss: 1.061179  [  912/ 3200]\n",
      "loss: 0.770932  [  928/ 3200]\n",
      "loss: 0.857951  [  944/ 3200]\n",
      "loss: 0.732617  [  960/ 3200]\n",
      "loss: 1.161847  [  976/ 3200]\n",
      "loss: 0.919134  [  992/ 3200]\n",
      "loss: 1.136801  [ 1008/ 3200]\n",
      "loss: 0.973330  [ 1024/ 3200]\n",
      "loss: 0.954362  [ 1040/ 3200]\n",
      "loss: 1.144521  [ 1056/ 3200]\n",
      "loss: 1.185110  [ 1072/ 3200]\n",
      "loss: 0.877501  [ 1088/ 3200]\n",
      "loss: 1.090575  [ 1104/ 3200]\n",
      "loss: 0.961432  [ 1120/ 3200]\n",
      "loss: 1.000087  [ 1136/ 3200]\n",
      "loss: 1.034993  [ 1152/ 3200]\n",
      "loss: 0.834363  [ 1168/ 3200]\n",
      "loss: 0.901243  [ 1184/ 3200]\n",
      "loss: 1.380706  [ 1200/ 3200]\n",
      "loss: 1.209812  [ 1216/ 3200]\n",
      "loss: 0.828486  [ 1232/ 3200]\n",
      "loss: 0.923620  [ 1248/ 3200]\n",
      "loss: 0.898330  [ 1264/ 3200]\n",
      "loss: 0.932044  [ 1280/ 3200]\n",
      "loss: 0.927089  [ 1296/ 3200]\n",
      "loss: 0.901674  [ 1312/ 3200]\n",
      "loss: 0.744241  [ 1328/ 3200]\n",
      "loss: 0.843517  [ 1344/ 3200]\n",
      "loss: 1.064466  [ 1360/ 3200]\n",
      "loss: 1.019025  [ 1376/ 3200]\n",
      "loss: 0.766634  [ 1392/ 3200]\n",
      "loss: 1.097843  [ 1408/ 3200]\n",
      "loss: 1.105274  [ 1424/ 3200]\n",
      "loss: 0.860291  [ 1440/ 3200]\n",
      "loss: 0.799257  [ 1456/ 3200]\n",
      "loss: 1.237187  [ 1472/ 3200]\n",
      "loss: 1.115828  [ 1488/ 3200]\n",
      "loss: 0.876581  [ 1504/ 3200]\n",
      "loss: 0.942835  [ 1520/ 3200]\n",
      "loss: 1.051963  [ 1536/ 3200]\n",
      "loss: 0.875756  [ 1552/ 3200]\n",
      "loss: 1.058864  [ 1568/ 3200]\n",
      "loss: 0.989967  [ 1584/ 3200]\n",
      "loss: 0.851375  [ 1600/ 3200]\n",
      "loss: 0.964765  [ 1616/ 3200]\n",
      "loss: 1.227280  [ 1632/ 3200]\n",
      "loss: 1.285778  [ 1648/ 3200]\n",
      "loss: 0.873312  [ 1664/ 3200]\n",
      "loss: 1.086331  [ 1680/ 3200]\n",
      "loss: 0.832815  [ 1696/ 3200]\n",
      "loss: 0.805398  [ 1712/ 3200]\n",
      "loss: 1.101040  [ 1728/ 3200]\n",
      "loss: 1.069428  [ 1744/ 3200]\n",
      "loss: 1.217123  [ 1760/ 3200]\n",
      "loss: 0.760237  [ 1776/ 3200]\n",
      "loss: 0.977041  [ 1792/ 3200]\n",
      "loss: 1.160859  [ 1808/ 3200]\n",
      "loss: 0.800406  [ 1824/ 3200]\n",
      "loss: 0.922575  [ 1840/ 3200]\n",
      "loss: 0.715101  [ 1856/ 3200]\n",
      "loss: 1.095822  [ 1872/ 3200]\n",
      "loss: 1.034417  [ 1888/ 3200]\n",
      "loss: 0.873358  [ 1904/ 3200]\n",
      "loss: 0.787605  [ 1920/ 3200]\n",
      "loss: 1.185450  [ 1936/ 3200]\n",
      "loss: 0.872895  [ 1952/ 3200]\n",
      "loss: 0.866242  [ 1968/ 3200]\n",
      "loss: 0.784192  [ 1984/ 3200]\n",
      "loss: 1.137200  [ 2000/ 3200]\n",
      "loss: 1.140272  [ 2016/ 3200]\n",
      "loss: 0.801681  [ 2032/ 3200]\n",
      "loss: 0.919056  [ 2048/ 3200]\n",
      "loss: 0.952390  [ 2064/ 3200]\n",
      "loss: 0.973171  [ 2080/ 3200]\n",
      "loss: 0.777668  [ 2096/ 3200]\n",
      "loss: 1.081920  [ 2112/ 3200]\n",
      "loss: 0.854266  [ 2128/ 3200]\n",
      "loss: 0.880028  [ 2144/ 3200]\n",
      "loss: 0.819160  [ 2160/ 3200]\n",
      "loss: 0.849565  [ 2176/ 3200]\n",
      "loss: 0.733315  [ 2192/ 3200]\n",
      "loss: 0.735451  [ 2208/ 3200]\n",
      "loss: 0.846776  [ 2224/ 3200]\n",
      "loss: 0.877484  [ 2240/ 3200]\n",
      "loss: 1.050946  [ 2256/ 3200]\n",
      "loss: 0.995537  [ 2272/ 3200]\n",
      "loss: 0.832301  [ 2288/ 3200]\n",
      "loss: 1.104456  [ 2304/ 3200]\n",
      "loss: 0.822297  [ 2320/ 3200]\n",
      "loss: 1.207320  [ 2336/ 3200]\n",
      "loss: 1.062088  [ 2352/ 3200]\n",
      "loss: 1.162726  [ 2368/ 3200]\n",
      "loss: 0.863155  [ 2384/ 3200]\n",
      "loss: 0.799160  [ 2400/ 3200]\n",
      "loss: 1.094049  [ 2416/ 3200]\n",
      "loss: 0.988378  [ 2432/ 3200]\n",
      "loss: 0.816376  [ 2448/ 3200]\n",
      "loss: 1.745226  [ 2464/ 3200]\n",
      "loss: 1.578644  [ 2480/ 3200]\n",
      "loss: 1.023779  [ 2496/ 3200]\n",
      "loss: 0.835592  [ 2512/ 3200]\n",
      "loss: 0.799004  [ 2528/ 3200]\n",
      "loss: 1.011798  [ 2544/ 3200]\n",
      "loss: 0.786766  [ 2560/ 3200]\n",
      "loss: 0.903339  [ 2576/ 3200]\n",
      "loss: 0.727803  [ 2592/ 3200]\n",
      "loss: 1.254611  [ 2608/ 3200]\n",
      "loss: 1.027892  [ 2624/ 3200]\n",
      "loss: 1.128005  [ 2640/ 3200]\n",
      "loss: 0.935897  [ 2656/ 3200]\n",
      "loss: 1.220458  [ 2672/ 3200]\n",
      "loss: 0.952021  [ 2688/ 3200]\n",
      "loss: 0.820303  [ 2704/ 3200]\n",
      "loss: 0.962926  [ 2720/ 3200]\n",
      "loss: 1.138245  [ 2736/ 3200]\n",
      "loss: 0.995867  [ 2752/ 3200]\n",
      "loss: 0.867201  [ 2768/ 3200]\n",
      "loss: 0.937790  [ 2784/ 3200]\n",
      "loss: 1.122068  [ 2800/ 3200]\n",
      "loss: 0.768831  [ 2816/ 3200]\n",
      "loss: 1.012901  [ 2832/ 3200]\n",
      "loss: 0.979805  [ 2848/ 3200]\n",
      "loss: 0.757177  [ 2864/ 3200]\n",
      "loss: 1.034676  [ 2880/ 3200]\n",
      "loss: 0.929832  [ 2896/ 3200]\n",
      "loss: 1.130406  [ 2912/ 3200]\n",
      "loss: 1.108810  [ 2928/ 3200]\n",
      "loss: 1.147842  [ 2944/ 3200]\n",
      "loss: 0.894203  [ 2960/ 3200]\n",
      "loss: 0.918211  [ 2976/ 3200]\n",
      "loss: 1.033008  [ 2992/ 3200]\n",
      "loss: 0.955760  [ 3008/ 3200]\n",
      "loss: 1.038891  [ 3024/ 3200]\n",
      "loss: 0.976606  [ 3040/ 3200]\n",
      "loss: 0.932473  [ 3056/ 3200]\n",
      "loss: 1.057917  [ 3072/ 3200]\n",
      "loss: 0.912668  [ 3088/ 3200]\n",
      "loss: 0.904377  [ 3104/ 3200]\n",
      "loss: 1.086590  [ 3120/ 3200]\n",
      "loss: 0.889754  [ 3136/ 3200]\n",
      "loss: 1.123290  [ 3152/ 3200]\n",
      "loss: 1.047560  [ 3168/ 3200]\n",
      "loss: 1.167812  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 0.849717  [    0/ 3200]\n",
      "loss: 0.888874  [   16/ 3200]\n",
      "loss: 0.947964  [   32/ 3200]\n",
      "loss: 0.988361  [   48/ 3200]\n",
      "loss: 1.063976  [   64/ 3200]\n",
      "loss: 1.025035  [   80/ 3200]\n",
      "loss: 0.621466  [   96/ 3200]\n",
      "loss: 0.991253  [  112/ 3200]\n",
      "loss: 1.117210  [  128/ 3200]\n",
      "loss: 1.249510  [  144/ 3200]\n",
      "loss: 1.144775  [  160/ 3200]\n",
      "loss: 1.003644  [  176/ 3200]\n",
      "loss: 0.757095  [  192/ 3200]\n",
      "loss: 1.080230  [  208/ 3200]\n",
      "loss: 0.964555  [  224/ 3200]\n",
      "loss: 0.839159  [  240/ 3200]\n",
      "loss: 0.920633  [  256/ 3200]\n",
      "loss: 0.982856  [  272/ 3200]\n",
      "loss: 0.962429  [  288/ 3200]\n",
      "loss: 0.810647  [  304/ 3200]\n",
      "loss: 1.018637  [  320/ 3200]\n",
      "loss: 0.703986  [  336/ 3200]\n",
      "loss: 1.144591  [  352/ 3200]\n",
      "loss: 1.237097  [  368/ 3200]\n",
      "loss: 1.178902  [  384/ 3200]\n",
      "loss: 0.901169  [  400/ 3200]\n",
      "loss: 0.711797  [  416/ 3200]\n",
      "loss: 0.894853  [  432/ 3200]\n",
      "loss: 1.165870  [  448/ 3200]\n",
      "loss: 1.088896  [  464/ 3200]\n",
      "loss: 1.033657  [  480/ 3200]\n",
      "loss: 0.875381  [  496/ 3200]\n",
      "loss: 0.905565  [  512/ 3200]\n",
      "loss: 1.099639  [  528/ 3200]\n",
      "loss: 1.142963  [  544/ 3200]\n",
      "loss: 0.996674  [  560/ 3200]\n",
      "loss: 0.938307  [  576/ 3200]\n",
      "loss: 0.939243  [  592/ 3200]\n",
      "loss: 1.074617  [  608/ 3200]\n",
      "loss: 0.769231  [  624/ 3200]\n",
      "loss: 0.856701  [  640/ 3200]\n",
      "loss: 0.896758  [  656/ 3200]\n",
      "loss: 1.001732  [  672/ 3200]\n",
      "loss: 0.948018  [  688/ 3200]\n",
      "loss: 0.688964  [  704/ 3200]\n",
      "loss: 0.958620  [  720/ 3200]\n",
      "loss: 1.195119  [  736/ 3200]\n",
      "loss: 1.131631  [  752/ 3200]\n",
      "loss: 0.993552  [  768/ 3200]\n",
      "loss: 0.803368  [  784/ 3200]\n",
      "loss: 1.000309  [  800/ 3200]\n",
      "loss: 0.916054  [  816/ 3200]\n",
      "loss: 0.774387  [  832/ 3200]\n",
      "loss: 0.838681  [  848/ 3200]\n",
      "loss: 0.641870  [  864/ 3200]\n",
      "loss: 1.033539  [  880/ 3200]\n",
      "loss: 0.898656  [  896/ 3200]\n",
      "loss: 0.934038  [  912/ 3200]\n",
      "loss: 0.975726  [  928/ 3200]\n",
      "loss: 1.121854  [  944/ 3200]\n",
      "loss: 0.896343  [  960/ 3200]\n",
      "loss: 0.833830  [  976/ 3200]\n",
      "loss: 0.925325  [  992/ 3200]\n",
      "loss: 0.953545  [ 1008/ 3200]\n",
      "loss: 0.745602  [ 1024/ 3200]\n",
      "loss: 1.023396  [ 1040/ 3200]\n",
      "loss: 0.757812  [ 1056/ 3200]\n",
      "loss: 0.882547  [ 1072/ 3200]\n",
      "loss: 1.069048  [ 1088/ 3200]\n",
      "loss: 1.121100  [ 1104/ 3200]\n",
      "loss: 1.086977  [ 1120/ 3200]\n",
      "loss: 0.864120  [ 1136/ 3200]\n",
      "loss: 1.031762  [ 1152/ 3200]\n",
      "loss: 0.752467  [ 1168/ 3200]\n",
      "loss: 0.803239  [ 1184/ 3200]\n",
      "loss: 1.246551  [ 1200/ 3200]\n",
      "loss: 1.155549  [ 1216/ 3200]\n",
      "loss: 1.018106  [ 1232/ 3200]\n",
      "loss: 0.909944  [ 1248/ 3200]\n",
      "loss: 0.945022  [ 1264/ 3200]\n",
      "loss: 1.095958  [ 1280/ 3200]\n",
      "loss: 0.903996  [ 1296/ 3200]\n",
      "loss: 0.644849  [ 1312/ 3200]\n",
      "loss: 1.010900  [ 1328/ 3200]\n",
      "loss: 0.942967  [ 1344/ 3200]\n",
      "loss: 0.835203  [ 1360/ 3200]\n",
      "loss: 0.888453  [ 1376/ 3200]\n",
      "loss: 0.985000  [ 1392/ 3200]\n",
      "loss: 0.814086  [ 1408/ 3200]\n",
      "loss: 1.256095  [ 1424/ 3200]\n",
      "loss: 1.058859  [ 1440/ 3200]\n",
      "loss: 0.871801  [ 1456/ 3200]\n",
      "loss: 1.015402  [ 1472/ 3200]\n",
      "loss: 0.575981  [ 1488/ 3200]\n",
      "loss: 1.180105  [ 1504/ 3200]\n",
      "loss: 0.882968  [ 1520/ 3200]\n",
      "loss: 0.889357  [ 1536/ 3200]\n",
      "loss: 1.076590  [ 1552/ 3200]\n",
      "loss: 1.261705  [ 1568/ 3200]\n",
      "loss: 0.955678  [ 1584/ 3200]\n",
      "loss: 0.989093  [ 1600/ 3200]\n",
      "loss: 1.210518  [ 1616/ 3200]\n",
      "loss: 1.186077  [ 1632/ 3200]\n",
      "loss: 1.202506  [ 1648/ 3200]\n",
      "loss: 1.094461  [ 1664/ 3200]\n",
      "loss: 0.842070  [ 1680/ 3200]\n",
      "loss: 1.037016  [ 1696/ 3200]\n",
      "loss: 0.886551  [ 1712/ 3200]\n",
      "loss: 1.101128  [ 1728/ 3200]\n",
      "loss: 0.979878  [ 1744/ 3200]\n",
      "loss: 1.306964  [ 1760/ 3200]\n",
      "loss: 0.950232  [ 1776/ 3200]\n",
      "loss: 0.718121  [ 1792/ 3200]\n",
      "loss: 0.870063  [ 1808/ 3200]\n",
      "loss: 0.743394  [ 1824/ 3200]\n",
      "loss: 1.032600  [ 1840/ 3200]\n",
      "loss: 0.655998  [ 1856/ 3200]\n",
      "loss: 1.133154  [ 1872/ 3200]\n",
      "loss: 1.066336  [ 1888/ 3200]\n",
      "loss: 0.772223  [ 1904/ 3200]\n",
      "loss: 1.261203  [ 1920/ 3200]\n",
      "loss: 1.092572  [ 1936/ 3200]\n",
      "loss: 0.929051  [ 1952/ 3200]\n",
      "loss: 1.055236  [ 1968/ 3200]\n",
      "loss: 0.741672  [ 1984/ 3200]\n",
      "loss: 0.944609  [ 2000/ 3200]\n",
      "loss: 1.469539  [ 2016/ 3200]\n",
      "loss: 1.331168  [ 2032/ 3200]\n",
      "loss: 0.897347  [ 2048/ 3200]\n",
      "loss: 1.028537  [ 2064/ 3200]\n",
      "loss: 0.942281  [ 2080/ 3200]\n",
      "loss: 1.052864  [ 2096/ 3200]\n",
      "loss: 0.769212  [ 2112/ 3200]\n",
      "loss: 0.692774  [ 2128/ 3200]\n",
      "loss: 0.963293  [ 2144/ 3200]\n",
      "loss: 0.804539  [ 2160/ 3200]\n",
      "loss: 0.825354  [ 2176/ 3200]\n",
      "loss: 1.023499  [ 2192/ 3200]\n",
      "loss: 0.686798  [ 2208/ 3200]\n",
      "loss: 0.878608  [ 2224/ 3200]\n",
      "loss: 0.906824  [ 2240/ 3200]\n",
      "loss: 1.145404  [ 2256/ 3200]\n",
      "loss: 0.890718  [ 2272/ 3200]\n",
      "loss: 1.151083  [ 2288/ 3200]\n",
      "loss: 0.999122  [ 2304/ 3200]\n",
      "loss: 1.061316  [ 2320/ 3200]\n",
      "loss: 0.824435  [ 2336/ 3200]\n",
      "loss: 1.014978  [ 2352/ 3200]\n",
      "loss: 0.935181  [ 2368/ 3200]\n",
      "loss: 1.067076  [ 2384/ 3200]\n",
      "loss: 0.659354  [ 2400/ 3200]\n",
      "loss: 0.799683  [ 2416/ 3200]\n",
      "loss: 0.990951  [ 2432/ 3200]\n",
      "loss: 0.823924  [ 2448/ 3200]\n",
      "loss: 0.817308  [ 2464/ 3200]\n",
      "loss: 1.015814  [ 2480/ 3200]\n",
      "loss: 0.905145  [ 2496/ 3200]\n",
      "loss: 1.220989  [ 2512/ 3200]\n",
      "loss: 0.674788  [ 2528/ 3200]\n",
      "loss: 0.888144  [ 2544/ 3200]\n",
      "loss: 0.855588  [ 2560/ 3200]\n",
      "loss: 0.824002  [ 2576/ 3200]\n",
      "loss: 1.065628  [ 2592/ 3200]\n",
      "loss: 1.119410  [ 2608/ 3200]\n",
      "loss: 0.906655  [ 2624/ 3200]\n",
      "loss: 1.068998  [ 2640/ 3200]\n",
      "loss: 0.736366  [ 2656/ 3200]\n",
      "loss: 1.102648  [ 2672/ 3200]\n",
      "loss: 0.882037  [ 2688/ 3200]\n",
      "loss: 0.891134  [ 2704/ 3200]\n",
      "loss: 0.878369  [ 2720/ 3200]\n",
      "loss: 0.819844  [ 2736/ 3200]\n",
      "loss: 0.876200  [ 2752/ 3200]\n",
      "loss: 0.611647  [ 2768/ 3200]\n",
      "loss: 0.792002  [ 2784/ 3200]\n",
      "loss: 0.819920  [ 2800/ 3200]\n",
      "loss: 0.891809  [ 2816/ 3200]\n",
      "loss: 0.938256  [ 2832/ 3200]\n",
      "loss: 0.651901  [ 2848/ 3200]\n",
      "loss: 1.305219  [ 2864/ 3200]\n",
      "loss: 1.211213  [ 2880/ 3200]\n",
      "loss: 0.810670  [ 2896/ 3200]\n",
      "loss: 0.771635  [ 2912/ 3200]\n",
      "loss: 1.000658  [ 2928/ 3200]\n",
      "loss: 1.087634  [ 2944/ 3200]\n",
      "loss: 0.970664  [ 2960/ 3200]\n",
      "loss: 1.136408  [ 2976/ 3200]\n",
      "loss: 0.879491  [ 2992/ 3200]\n",
      "loss: 0.856943  [ 3008/ 3200]\n",
      "loss: 1.195359  [ 3024/ 3200]\n",
      "loss: 1.168979  [ 3040/ 3200]\n",
      "loss: 0.682556  [ 3056/ 3200]\n",
      "loss: 0.909658  [ 3072/ 3200]\n",
      "loss: 0.975823  [ 3088/ 3200]\n",
      "loss: 0.851415  [ 3104/ 3200]\n",
      "loss: 0.855422  [ 3120/ 3200]\n",
      "loss: 0.991342  [ 3136/ 3200]\n",
      "loss: 0.785999  [ 3152/ 3200]\n",
      "loss: 0.881009  [ 3168/ 3200]\n",
      "loss: 0.701210  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 1.137966  [    0/ 3200]\n",
      "loss: 0.863744  [   16/ 3200]\n",
      "loss: 0.999669  [   32/ 3200]\n",
      "loss: 0.857043  [   48/ 3200]\n",
      "loss: 0.860991  [   64/ 3200]\n",
      "loss: 0.918080  [   80/ 3200]\n",
      "loss: 0.813064  [   96/ 3200]\n",
      "loss: 0.890852  [  112/ 3200]\n",
      "loss: 0.922461  [  128/ 3200]\n",
      "loss: 0.995035  [  144/ 3200]\n",
      "loss: 1.273899  [  160/ 3200]\n",
      "loss: 0.898512  [  176/ 3200]\n",
      "loss: 0.984249  [  192/ 3200]\n",
      "loss: 1.011550  [  208/ 3200]\n",
      "loss: 0.861844  [  224/ 3200]\n",
      "loss: 0.816897  [  240/ 3200]\n",
      "loss: 1.379781  [  256/ 3200]\n",
      "loss: 0.855533  [  272/ 3200]\n",
      "loss: 0.888419  [  288/ 3200]\n",
      "loss: 1.054643  [  304/ 3200]\n",
      "loss: 0.847598  [  320/ 3200]\n",
      "loss: 1.167371  [  336/ 3200]\n",
      "loss: 0.704445  [  352/ 3200]\n",
      "loss: 0.987410  [  368/ 3200]\n",
      "loss: 0.960258  [  384/ 3200]\n",
      "loss: 1.071314  [  400/ 3200]\n",
      "loss: 0.894870  [  416/ 3200]\n",
      "loss: 0.817026  [  432/ 3200]\n",
      "loss: 1.360064  [  448/ 3200]\n",
      "loss: 0.874794  [  464/ 3200]\n",
      "loss: 0.927793  [  480/ 3200]\n",
      "loss: 0.955344  [  496/ 3200]\n",
      "loss: 1.001440  [  512/ 3200]\n",
      "loss: 0.850351  [  528/ 3200]\n",
      "loss: 1.242831  [  544/ 3200]\n",
      "loss: 1.193279  [  560/ 3200]\n",
      "loss: 1.084476  [  576/ 3200]\n",
      "loss: 1.215953  [  592/ 3200]\n",
      "loss: 1.063460  [  608/ 3200]\n",
      "loss: 0.729109  [  624/ 3200]\n",
      "loss: 0.722739  [  640/ 3200]\n",
      "loss: 0.812664  [  656/ 3200]\n",
      "loss: 1.051203  [  672/ 3200]\n",
      "loss: 0.681317  [  688/ 3200]\n",
      "loss: 0.624298  [  704/ 3200]\n",
      "loss: 0.645800  [  720/ 3200]\n",
      "loss: 0.975332  [  736/ 3200]\n",
      "loss: 0.882248  [  752/ 3200]\n",
      "loss: 0.772180  [  768/ 3200]\n",
      "loss: 1.078536  [  784/ 3200]\n",
      "loss: 0.661929  [  800/ 3200]\n",
      "loss: 1.116213  [  816/ 3200]\n",
      "loss: 0.696592  [  832/ 3200]\n",
      "loss: 1.205992  [  848/ 3200]\n",
      "loss: 1.041327  [  864/ 3200]\n",
      "loss: 0.973990  [  880/ 3200]\n",
      "loss: 0.761267  [  896/ 3200]\n",
      "loss: 0.731273  [  912/ 3200]\n",
      "loss: 1.278225  [  928/ 3200]\n",
      "loss: 1.367830  [  944/ 3200]\n",
      "loss: 0.773195  [  960/ 3200]\n",
      "loss: 0.877887  [  976/ 3200]\n",
      "loss: 0.840034  [  992/ 3200]\n",
      "loss: 0.675334  [ 1008/ 3200]\n",
      "loss: 1.003921  [ 1024/ 3200]\n",
      "loss: 0.988044  [ 1040/ 3200]\n",
      "loss: 1.021904  [ 1056/ 3200]\n",
      "loss: 0.850407  [ 1072/ 3200]\n",
      "loss: 1.002971  [ 1088/ 3200]\n",
      "loss: 0.989975  [ 1104/ 3200]\n",
      "loss: 1.102335  [ 1120/ 3200]\n",
      "loss: 0.791422  [ 1136/ 3200]\n",
      "loss: 0.785034  [ 1152/ 3200]\n",
      "loss: 0.702873  [ 1168/ 3200]\n",
      "loss: 0.938503  [ 1184/ 3200]\n",
      "loss: 1.029128  [ 1200/ 3200]\n",
      "loss: 1.049051  [ 1216/ 3200]\n",
      "loss: 1.187724  [ 1232/ 3200]\n",
      "loss: 0.949869  [ 1248/ 3200]\n",
      "loss: 0.927528  [ 1264/ 3200]\n",
      "loss: 0.894458  [ 1280/ 3200]\n",
      "loss: 1.019254  [ 1296/ 3200]\n",
      "loss: 1.079889  [ 1312/ 3200]\n",
      "loss: 0.852578  [ 1328/ 3200]\n",
      "loss: 0.848936  [ 1344/ 3200]\n",
      "loss: 0.811868  [ 1360/ 3200]\n",
      "loss: 1.391258  [ 1376/ 3200]\n",
      "loss: 0.880020  [ 1392/ 3200]\n",
      "loss: 0.981734  [ 1408/ 3200]\n",
      "loss: 0.954314  [ 1424/ 3200]\n",
      "loss: 0.881132  [ 1440/ 3200]\n",
      "loss: 0.730281  [ 1456/ 3200]\n",
      "loss: 1.409780  [ 1472/ 3200]\n",
      "loss: 1.299388  [ 1488/ 3200]\n",
      "loss: 1.084996  [ 1504/ 3200]\n",
      "loss: 0.960396  [ 1520/ 3200]\n",
      "loss: 0.834477  [ 1536/ 3200]\n",
      "loss: 0.747802  [ 1552/ 3200]\n",
      "loss: 0.906622  [ 1568/ 3200]\n",
      "loss: 1.158082  [ 1584/ 3200]\n",
      "loss: 0.965225  [ 1600/ 3200]\n",
      "loss: 0.778157  [ 1616/ 3200]\n",
      "loss: 0.771847  [ 1632/ 3200]\n",
      "loss: 0.855074  [ 1648/ 3200]\n",
      "loss: 0.899502  [ 1664/ 3200]\n",
      "loss: 0.794903  [ 1680/ 3200]\n",
      "loss: 0.741948  [ 1696/ 3200]\n",
      "loss: 0.667361  [ 1712/ 3200]\n",
      "loss: 0.835562  [ 1728/ 3200]\n",
      "loss: 0.887843  [ 1744/ 3200]\n",
      "loss: 0.890200  [ 1760/ 3200]\n",
      "loss: 0.893053  [ 1776/ 3200]\n",
      "loss: 0.808056  [ 1792/ 3200]\n",
      "loss: 0.711765  [ 1808/ 3200]\n",
      "loss: 1.064584  [ 1824/ 3200]\n",
      "loss: 0.830910  [ 1840/ 3200]\n",
      "loss: 0.885867  [ 1856/ 3200]\n",
      "loss: 0.923080  [ 1872/ 3200]\n",
      "loss: 1.231598  [ 1888/ 3200]\n",
      "loss: 1.626385  [ 1904/ 3200]\n",
      "loss: 1.357707  [ 1920/ 3200]\n",
      "loss: 1.403173  [ 1936/ 3200]\n",
      "loss: 0.929996  [ 1952/ 3200]\n",
      "loss: 0.850890  [ 1968/ 3200]\n",
      "loss: 0.824715  [ 1984/ 3200]\n",
      "loss: 1.069845  [ 2000/ 3200]\n",
      "loss: 0.681723  [ 2016/ 3200]\n",
      "loss: 1.268957  [ 2032/ 3200]\n",
      "loss: 0.888914  [ 2048/ 3200]\n",
      "loss: 1.032431  [ 2064/ 3200]\n",
      "loss: 0.972497  [ 2080/ 3200]\n",
      "loss: 1.001045  [ 2096/ 3200]\n",
      "loss: 1.030826  [ 2112/ 3200]\n",
      "loss: 1.122766  [ 2128/ 3200]\n",
      "loss: 0.928314  [ 2144/ 3200]\n",
      "loss: 0.901483  [ 2160/ 3200]\n",
      "loss: 0.817201  [ 2176/ 3200]\n",
      "loss: 0.865149  [ 2192/ 3200]\n",
      "loss: 1.027858  [ 2208/ 3200]\n",
      "loss: 1.022495  [ 2224/ 3200]\n",
      "loss: 0.765321  [ 2240/ 3200]\n",
      "loss: 0.840619  [ 2256/ 3200]\n",
      "loss: 0.921385  [ 2272/ 3200]\n",
      "loss: 1.029171  [ 2288/ 3200]\n",
      "loss: 1.042756  [ 2304/ 3200]\n",
      "loss: 0.749468  [ 2320/ 3200]\n",
      "loss: 0.816851  [ 2336/ 3200]\n",
      "loss: 0.686208  [ 2352/ 3200]\n",
      "loss: 1.361226  [ 2368/ 3200]\n",
      "loss: 0.986229  [ 2384/ 3200]\n",
      "loss: 0.942348  [ 2400/ 3200]\n",
      "loss: 0.950885  [ 2416/ 3200]\n",
      "loss: 0.842385  [ 2432/ 3200]\n",
      "loss: 0.817068  [ 2448/ 3200]\n",
      "loss: 0.969321  [ 2464/ 3200]\n",
      "loss: 0.956314  [ 2480/ 3200]\n",
      "loss: 1.021674  [ 2496/ 3200]\n",
      "loss: 0.959168  [ 2512/ 3200]\n",
      "loss: 1.023265  [ 2528/ 3200]\n",
      "loss: 0.848572  [ 2544/ 3200]\n",
      "loss: 0.670537  [ 2560/ 3200]\n",
      "loss: 0.808867  [ 2576/ 3200]\n",
      "loss: 0.763012  [ 2592/ 3200]\n",
      "loss: 0.686753  [ 2608/ 3200]\n",
      "loss: 0.908989  [ 2624/ 3200]\n",
      "loss: 0.949473  [ 2640/ 3200]\n",
      "loss: 0.853326  [ 2656/ 3200]\n",
      "loss: 0.913307  [ 2672/ 3200]\n",
      "loss: 0.904637  [ 2688/ 3200]\n",
      "loss: 0.786552  [ 2704/ 3200]\n",
      "loss: 0.894876  [ 2720/ 3200]\n",
      "loss: 0.596699  [ 2736/ 3200]\n",
      "loss: 0.817814  [ 2752/ 3200]\n",
      "loss: 0.880418  [ 2768/ 3200]\n",
      "loss: 1.026046  [ 2784/ 3200]\n",
      "loss: 0.790815  [ 2800/ 3200]\n",
      "loss: 0.852135  [ 2816/ 3200]\n",
      "loss: 0.910576  [ 2832/ 3200]\n",
      "loss: 1.104629  [ 2848/ 3200]\n",
      "loss: 1.323203  [ 2864/ 3200]\n",
      "loss: 1.171064  [ 2880/ 3200]\n",
      "loss: 1.373289  [ 2896/ 3200]\n",
      "loss: 0.713304  [ 2912/ 3200]\n",
      "loss: 0.954913  [ 2928/ 3200]\n",
      "loss: 0.815660  [ 2944/ 3200]\n",
      "loss: 0.745548  [ 2960/ 3200]\n",
      "loss: 0.543491  [ 2976/ 3200]\n",
      "loss: 1.158418  [ 2992/ 3200]\n",
      "loss: 0.906018  [ 3008/ 3200]\n",
      "loss: 0.745003  [ 3024/ 3200]\n",
      "loss: 0.733775  [ 3040/ 3200]\n",
      "loss: 0.636091  [ 3056/ 3200]\n",
      "loss: 0.617575  [ 3072/ 3200]\n",
      "loss: 1.051467  [ 3088/ 3200]\n",
      "loss: 0.981506  [ 3104/ 3200]\n",
      "loss: 0.891513  [ 3120/ 3200]\n",
      "loss: 1.184751  [ 3136/ 3200]\n",
      "loss: 1.145566  [ 3152/ 3200]\n",
      "loss: 0.821714  [ 3168/ 3200]\n",
      "loss: 0.734001  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 0.810724  [    0/ 3200]\n",
      "loss: 0.871506  [   16/ 3200]\n",
      "loss: 1.146299  [   32/ 3200]\n",
      "loss: 0.892901  [   48/ 3200]\n",
      "loss: 0.502670  [   64/ 3200]\n",
      "loss: 0.717736  [   80/ 3200]\n",
      "loss: 0.992030  [   96/ 3200]\n",
      "loss: 1.157375  [  112/ 3200]\n",
      "loss: 1.042814  [  128/ 3200]\n",
      "loss: 0.826216  [  144/ 3200]\n",
      "loss: 0.885134  [  160/ 3200]\n",
      "loss: 1.417241  [  176/ 3200]\n",
      "loss: 0.856010  [  192/ 3200]\n",
      "loss: 1.270585  [  208/ 3200]\n",
      "loss: 1.003512  [  224/ 3200]\n",
      "loss: 1.185062  [  240/ 3200]\n",
      "loss: 0.970497  [  256/ 3200]\n",
      "loss: 0.843831  [  272/ 3200]\n",
      "loss: 0.806509  [  288/ 3200]\n",
      "loss: 0.896409  [  304/ 3200]\n",
      "loss: 0.956681  [  320/ 3200]\n",
      "loss: 1.079961  [  336/ 3200]\n",
      "loss: 0.853538  [  352/ 3200]\n",
      "loss: 0.789088  [  368/ 3200]\n",
      "loss: 0.787243  [  384/ 3200]\n",
      "loss: 0.757993  [  400/ 3200]\n",
      "loss: 0.794770  [  416/ 3200]\n",
      "loss: 0.742213  [  432/ 3200]\n",
      "loss: 0.994472  [  448/ 3200]\n",
      "loss: 1.081909  [  464/ 3200]\n",
      "loss: 0.785626  [  480/ 3200]\n",
      "loss: 1.162000  [  496/ 3200]\n",
      "loss: 1.294534  [  512/ 3200]\n",
      "loss: 1.086231  [  528/ 3200]\n",
      "loss: 0.861301  [  544/ 3200]\n",
      "loss: 0.966309  [  560/ 3200]\n",
      "loss: 1.094794  [  576/ 3200]\n",
      "loss: 0.784686  [  592/ 3200]\n",
      "loss: 1.062708  [  608/ 3200]\n",
      "loss: 1.081011  [  624/ 3200]\n",
      "loss: 1.027642  [  640/ 3200]\n",
      "loss: 0.861497  [  656/ 3200]\n",
      "loss: 0.711038  [  672/ 3200]\n",
      "loss: 1.108912  [  688/ 3200]\n",
      "loss: 1.017915  [  704/ 3200]\n",
      "loss: 0.971605  [  720/ 3200]\n",
      "loss: 0.866512  [  736/ 3200]\n",
      "loss: 0.917525  [  752/ 3200]\n",
      "loss: 0.547905  [  768/ 3200]\n",
      "loss: 0.817074  [  784/ 3200]\n",
      "loss: 0.919554  [  800/ 3200]\n",
      "loss: 0.982001  [  816/ 3200]\n",
      "loss: 0.728093  [  832/ 3200]\n",
      "loss: 1.214034  [  848/ 3200]\n",
      "loss: 0.910878  [  864/ 3200]\n",
      "loss: 1.014354  [  880/ 3200]\n",
      "loss: 0.797880  [  896/ 3200]\n",
      "loss: 0.942149  [  912/ 3200]\n",
      "loss: 1.008672  [  928/ 3200]\n",
      "loss: 0.739056  [  944/ 3200]\n",
      "loss: 0.855050  [  960/ 3200]\n",
      "loss: 0.914604  [  976/ 3200]\n",
      "loss: 1.153946  [  992/ 3200]\n",
      "loss: 0.880732  [ 1008/ 3200]\n",
      "loss: 0.908418  [ 1024/ 3200]\n",
      "loss: 0.795879  [ 1040/ 3200]\n",
      "loss: 0.712035  [ 1056/ 3200]\n",
      "loss: 0.749756  [ 1072/ 3200]\n",
      "loss: 0.659123  [ 1088/ 3200]\n",
      "loss: 0.885383  [ 1104/ 3200]\n",
      "loss: 1.299605  [ 1120/ 3200]\n",
      "loss: 0.685561  [ 1136/ 3200]\n",
      "loss: 0.712269  [ 1152/ 3200]\n",
      "loss: 0.798812  [ 1168/ 3200]\n",
      "loss: 0.784677  [ 1184/ 3200]\n",
      "loss: 0.919488  [ 1200/ 3200]\n",
      "loss: 0.900693  [ 1216/ 3200]\n",
      "loss: 0.675451  [ 1232/ 3200]\n",
      "loss: 1.310057  [ 1248/ 3200]\n",
      "loss: 0.590736  [ 1264/ 3200]\n",
      "loss: 1.149336  [ 1280/ 3200]\n",
      "loss: 1.014173  [ 1296/ 3200]\n",
      "loss: 1.379473  [ 1312/ 3200]\n",
      "loss: 1.073468  [ 1328/ 3200]\n",
      "loss: 1.141145  [ 1344/ 3200]\n",
      "loss: 0.904090  [ 1360/ 3200]\n",
      "loss: 0.783057  [ 1376/ 3200]\n",
      "loss: 0.758100  [ 1392/ 3200]\n",
      "loss: 0.991053  [ 1408/ 3200]\n",
      "loss: 1.083402  [ 1424/ 3200]\n",
      "loss: 0.799296  [ 1440/ 3200]\n",
      "loss: 1.024776  [ 1456/ 3200]\n",
      "loss: 0.983470  [ 1472/ 3200]\n",
      "loss: 0.800397  [ 1488/ 3200]\n",
      "loss: 0.957147  [ 1504/ 3200]\n",
      "loss: 1.124472  [ 1520/ 3200]\n",
      "loss: 0.801671  [ 1536/ 3200]\n",
      "loss: 0.978576  [ 1552/ 3200]\n",
      "loss: 0.751838  [ 1568/ 3200]\n",
      "loss: 1.071524  [ 1584/ 3200]\n",
      "loss: 1.061379  [ 1600/ 3200]\n",
      "loss: 0.716203  [ 1616/ 3200]\n",
      "loss: 0.908725  [ 1632/ 3200]\n",
      "loss: 0.691661  [ 1648/ 3200]\n",
      "loss: 1.064626  [ 1664/ 3200]\n",
      "loss: 0.846852  [ 1680/ 3200]\n",
      "loss: 0.802884  [ 1696/ 3200]\n",
      "loss: 0.768097  [ 1712/ 3200]\n",
      "loss: 0.765680  [ 1728/ 3200]\n",
      "loss: 0.898664  [ 1744/ 3200]\n",
      "loss: 0.957068  [ 1760/ 3200]\n",
      "loss: 0.784681  [ 1776/ 3200]\n",
      "loss: 0.893063  [ 1792/ 3200]\n",
      "loss: 0.829657  [ 1808/ 3200]\n",
      "loss: 0.885578  [ 1824/ 3200]\n",
      "loss: 1.018481  [ 1840/ 3200]\n",
      "loss: 1.118800  [ 1856/ 3200]\n",
      "loss: 0.715302  [ 1872/ 3200]\n",
      "loss: 0.518695  [ 1888/ 3200]\n",
      "loss: 0.559873  [ 1904/ 3200]\n",
      "loss: 0.829124  [ 1920/ 3200]\n",
      "loss: 0.714934  [ 1936/ 3200]\n",
      "loss: 0.747490  [ 1952/ 3200]\n",
      "loss: 0.670682  [ 1968/ 3200]\n",
      "loss: 0.859689  [ 1984/ 3200]\n",
      "loss: 0.866020  [ 2000/ 3200]\n",
      "loss: 0.561642  [ 2016/ 3200]\n",
      "loss: 0.699310  [ 2032/ 3200]\n",
      "loss: 1.011344  [ 2048/ 3200]\n",
      "loss: 0.725387  [ 2064/ 3200]\n",
      "loss: 1.018443  [ 2080/ 3200]\n",
      "loss: 1.082869  [ 2096/ 3200]\n",
      "loss: 0.677936  [ 2112/ 3200]\n",
      "loss: 1.118900  [ 2128/ 3200]\n",
      "loss: 0.709039  [ 2144/ 3200]\n",
      "loss: 0.824097  [ 2160/ 3200]\n",
      "loss: 0.829995  [ 2176/ 3200]\n",
      "loss: 0.857163  [ 2192/ 3200]\n",
      "loss: 0.887760  [ 2208/ 3200]\n",
      "loss: 1.149902  [ 2224/ 3200]\n",
      "loss: 1.274461  [ 2240/ 3200]\n",
      "loss: 0.847589  [ 2256/ 3200]\n",
      "loss: 1.041193  [ 2272/ 3200]\n",
      "loss: 0.728876  [ 2288/ 3200]\n",
      "loss: 0.811345  [ 2304/ 3200]\n",
      "loss: 1.029907  [ 2320/ 3200]\n",
      "loss: 0.919132  [ 2336/ 3200]\n",
      "loss: 1.226308  [ 2352/ 3200]\n",
      "loss: 1.188022  [ 2368/ 3200]\n",
      "loss: 1.141105  [ 2384/ 3200]\n",
      "loss: 0.872205  [ 2400/ 3200]\n",
      "loss: 0.639301  [ 2416/ 3200]\n",
      "loss: 0.678311  [ 2432/ 3200]\n",
      "loss: 0.591090  [ 2448/ 3200]\n",
      "loss: 0.885505  [ 2464/ 3200]\n",
      "loss: 0.753169  [ 2480/ 3200]\n",
      "loss: 0.767792  [ 2496/ 3200]\n",
      "loss: 0.672932  [ 2512/ 3200]\n",
      "loss: 0.969845  [ 2528/ 3200]\n",
      "loss: 0.976156  [ 2544/ 3200]\n",
      "loss: 1.083625  [ 2560/ 3200]\n",
      "loss: 0.977244  [ 2576/ 3200]\n",
      "loss: 1.320827  [ 2592/ 3200]\n",
      "loss: 0.981367  [ 2608/ 3200]\n",
      "loss: 0.744327  [ 2624/ 3200]\n",
      "loss: 1.075839  [ 2640/ 3200]\n",
      "loss: 0.596864  [ 2656/ 3200]\n",
      "loss: 0.784617  [ 2672/ 3200]\n",
      "loss: 0.834147  [ 2688/ 3200]\n",
      "loss: 0.660474  [ 2704/ 3200]\n",
      "loss: 0.774199  [ 2720/ 3200]\n",
      "loss: 0.878274  [ 2736/ 3200]\n",
      "loss: 0.696668  [ 2752/ 3200]\n",
      "loss: 1.174377  [ 2768/ 3200]\n",
      "loss: 0.813425  [ 2784/ 3200]\n",
      "loss: 0.745077  [ 2800/ 3200]\n",
      "loss: 1.150627  [ 2816/ 3200]\n",
      "loss: 0.986958  [ 2832/ 3200]\n",
      "loss: 1.101042  [ 2848/ 3200]\n",
      "loss: 0.647770  [ 2864/ 3200]\n",
      "loss: 0.962369  [ 2880/ 3200]\n",
      "loss: 1.060701  [ 2896/ 3200]\n",
      "loss: 1.210873  [ 2912/ 3200]\n",
      "loss: 1.020951  [ 2928/ 3200]\n",
      "loss: 0.851317  [ 2944/ 3200]\n",
      "loss: 0.884062  [ 2960/ 3200]\n",
      "loss: 1.027098  [ 2976/ 3200]\n",
      "loss: 1.135806  [ 2992/ 3200]\n",
      "loss: 0.602048  [ 3008/ 3200]\n",
      "loss: 0.949602  [ 3024/ 3200]\n",
      "loss: 0.550744  [ 3040/ 3200]\n",
      "loss: 0.922657  [ 3056/ 3200]\n",
      "loss: 0.731802  [ 3072/ 3200]\n",
      "loss: 1.048964  [ 3088/ 3200]\n",
      "loss: 0.647276  [ 3104/ 3200]\n",
      "loss: 0.986066  [ 3120/ 3200]\n",
      "loss: 0.946839  [ 3136/ 3200]\n",
      "loss: 1.057409  [ 3152/ 3200]\n",
      "loss: 0.892081  [ 3168/ 3200]\n",
      "loss: 0.984329  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 0.636224  [    0/ 3200]\n",
      "loss: 0.802428  [   16/ 3200]\n",
      "loss: 0.710416  [   32/ 3200]\n",
      "loss: 0.719351  [   48/ 3200]\n",
      "loss: 0.913927  [   64/ 3200]\n",
      "loss: 0.773670  [   80/ 3200]\n",
      "loss: 0.610140  [   96/ 3200]\n",
      "loss: 1.607240  [  112/ 3200]\n",
      "loss: 1.082036  [  128/ 3200]\n",
      "loss: 0.852671  [  144/ 3200]\n",
      "loss: 0.856466  [  160/ 3200]\n",
      "loss: 0.961472  [  176/ 3200]\n",
      "loss: 0.796502  [  192/ 3200]\n",
      "loss: 1.013605  [  208/ 3200]\n",
      "loss: 1.106209  [  224/ 3200]\n",
      "loss: 0.907931  [  240/ 3200]\n",
      "loss: 0.896370  [  256/ 3200]\n",
      "loss: 0.814108  [  272/ 3200]\n",
      "loss: 0.980668  [  288/ 3200]\n",
      "loss: 1.034747  [  304/ 3200]\n",
      "loss: 0.842546  [  320/ 3200]\n",
      "loss: 0.606497  [  336/ 3200]\n",
      "loss: 0.726297  [  352/ 3200]\n",
      "loss: 1.001362  [  368/ 3200]\n",
      "loss: 0.678456  [  384/ 3200]\n",
      "loss: 1.148877  [  400/ 3200]\n",
      "loss: 0.743538  [  416/ 3200]\n",
      "loss: 0.686830  [  432/ 3200]\n",
      "loss: 0.640477  [  448/ 3200]\n",
      "loss: 1.165559  [  464/ 3200]\n",
      "loss: 1.232650  [  480/ 3200]\n",
      "loss: 0.752648  [  496/ 3200]\n",
      "loss: 0.826489  [  512/ 3200]\n",
      "loss: 0.660597  [  528/ 3200]\n",
      "loss: 0.788945  [  544/ 3200]\n",
      "loss: 0.817672  [  560/ 3200]\n",
      "loss: 0.752529  [  576/ 3200]\n",
      "loss: 1.156556  [  592/ 3200]\n",
      "loss: 1.014470  [  608/ 3200]\n",
      "loss: 0.753371  [  624/ 3200]\n",
      "loss: 0.573893  [  640/ 3200]\n",
      "loss: 0.647821  [  656/ 3200]\n",
      "loss: 0.966111  [  672/ 3200]\n",
      "loss: 0.970578  [  688/ 3200]\n",
      "loss: 0.892618  [  704/ 3200]\n",
      "loss: 1.040543  [  720/ 3200]\n",
      "loss: 1.233706  [  736/ 3200]\n",
      "loss: 1.104138  [  752/ 3200]\n",
      "loss: 0.972512  [  768/ 3200]\n",
      "loss: 1.087768  [  784/ 3200]\n",
      "loss: 0.900314  [  800/ 3200]\n",
      "loss: 1.005587  [  816/ 3200]\n",
      "loss: 1.058887  [  832/ 3200]\n",
      "loss: 0.858430  [  848/ 3200]\n",
      "loss: 0.937794  [  864/ 3200]\n",
      "loss: 0.822061  [  880/ 3200]\n",
      "loss: 0.820846  [  896/ 3200]\n",
      "loss: 1.016782  [  912/ 3200]\n",
      "loss: 0.880357  [  928/ 3200]\n",
      "loss: 0.823685  [  944/ 3200]\n",
      "loss: 0.908190  [  960/ 3200]\n",
      "loss: 0.994478  [  976/ 3200]\n",
      "loss: 0.869082  [  992/ 3200]\n",
      "loss: 1.095607  [ 1008/ 3200]\n",
      "loss: 0.891002  [ 1024/ 3200]\n",
      "loss: 0.880802  [ 1040/ 3200]\n",
      "loss: 0.670665  [ 1056/ 3200]\n",
      "loss: 1.029888  [ 1072/ 3200]\n",
      "loss: 1.201062  [ 1088/ 3200]\n",
      "loss: 0.800149  [ 1104/ 3200]\n",
      "loss: 1.073639  [ 1120/ 3200]\n",
      "loss: 1.225901  [ 1136/ 3200]\n",
      "loss: 0.889979  [ 1152/ 3200]\n",
      "loss: 1.200520  [ 1168/ 3200]\n",
      "loss: 0.652838  [ 1184/ 3200]\n",
      "loss: 0.839425  [ 1200/ 3200]\n",
      "loss: 1.180096  [ 1216/ 3200]\n",
      "loss: 0.990210  [ 1232/ 3200]\n",
      "loss: 0.863625  [ 1248/ 3200]\n",
      "loss: 0.823640  [ 1264/ 3200]\n",
      "loss: 0.865768  [ 1280/ 3200]\n",
      "loss: 0.979453  [ 1296/ 3200]\n",
      "loss: 0.773997  [ 1312/ 3200]\n",
      "loss: 0.895044  [ 1328/ 3200]\n",
      "loss: 0.834795  [ 1344/ 3200]\n",
      "loss: 0.946652  [ 1360/ 3200]\n",
      "loss: 1.364754  [ 1376/ 3200]\n",
      "loss: 0.769707  [ 1392/ 3200]\n",
      "loss: 0.789427  [ 1408/ 3200]\n",
      "loss: 0.816734  [ 1424/ 3200]\n",
      "loss: 1.379314  [ 1440/ 3200]\n",
      "loss: 1.151141  [ 1456/ 3200]\n",
      "loss: 1.219471  [ 1472/ 3200]\n",
      "loss: 0.911356  [ 1488/ 3200]\n",
      "loss: 1.004822  [ 1504/ 3200]\n",
      "loss: 1.170109  [ 1520/ 3200]\n",
      "loss: 1.008531  [ 1536/ 3200]\n",
      "loss: 0.786479  [ 1552/ 3200]\n",
      "loss: 0.739380  [ 1568/ 3200]\n",
      "loss: 1.067463  [ 1584/ 3200]\n",
      "loss: 0.817044  [ 1600/ 3200]\n",
      "loss: 0.929032  [ 1616/ 3200]\n",
      "loss: 0.820460  [ 1632/ 3200]\n",
      "loss: 0.610314  [ 1648/ 3200]\n",
      "loss: 0.606263  [ 1664/ 3200]\n",
      "loss: 0.776297  [ 1680/ 3200]\n",
      "loss: 0.852477  [ 1696/ 3200]\n",
      "loss: 0.581754  [ 1712/ 3200]\n",
      "loss: 0.608378  [ 1728/ 3200]\n",
      "loss: 0.912596  [ 1744/ 3200]\n",
      "loss: 0.881346  [ 1760/ 3200]\n",
      "loss: 1.004826  [ 1776/ 3200]\n",
      "loss: 0.747107  [ 1792/ 3200]\n",
      "loss: 1.134207  [ 1808/ 3200]\n",
      "loss: 0.856706  [ 1824/ 3200]\n",
      "loss: 0.659851  [ 1840/ 3200]\n",
      "loss: 1.101463  [ 1856/ 3200]\n",
      "loss: 0.621990  [ 1872/ 3200]\n",
      "loss: 0.958395  [ 1888/ 3200]\n",
      "loss: 0.870665  [ 1904/ 3200]\n",
      "loss: 0.836272  [ 1920/ 3200]\n",
      "loss: 0.885951  [ 1936/ 3200]\n",
      "loss: 1.183312  [ 1952/ 3200]\n",
      "loss: 0.899841  [ 1968/ 3200]\n",
      "loss: 0.962838  [ 1984/ 3200]\n",
      "loss: 0.904721  [ 2000/ 3200]\n",
      "loss: 1.038348  [ 2016/ 3200]\n",
      "loss: 0.783859  [ 2032/ 3200]\n",
      "loss: 0.700120  [ 2048/ 3200]\n",
      "loss: 0.601604  [ 2064/ 3200]\n",
      "loss: 0.742946  [ 2080/ 3200]\n",
      "loss: 0.942045  [ 2096/ 3200]\n",
      "loss: 1.156964  [ 2112/ 3200]\n",
      "loss: 0.916347  [ 2128/ 3200]\n",
      "loss: 0.596564  [ 2144/ 3200]\n",
      "loss: 0.871165  [ 2160/ 3200]\n",
      "loss: 1.066936  [ 2176/ 3200]\n",
      "loss: 0.917648  [ 2192/ 3200]\n",
      "loss: 0.870278  [ 2208/ 3200]\n",
      "loss: 0.822210  [ 2224/ 3200]\n",
      "loss: 0.932284  [ 2240/ 3200]\n",
      "loss: 0.670349  [ 2256/ 3200]\n",
      "loss: 0.914702  [ 2272/ 3200]\n",
      "loss: 0.703389  [ 2288/ 3200]\n",
      "loss: 0.929810  [ 2304/ 3200]\n",
      "loss: 0.910136  [ 2320/ 3200]\n",
      "loss: 0.634181  [ 2336/ 3200]\n",
      "loss: 1.065818  [ 2352/ 3200]\n",
      "loss: 0.857809  [ 2368/ 3200]\n",
      "loss: 0.463095  [ 2384/ 3200]\n",
      "loss: 1.137866  [ 2400/ 3200]\n",
      "loss: 0.749597  [ 2416/ 3200]\n",
      "loss: 0.938350  [ 2432/ 3200]\n",
      "loss: 0.798670  [ 2448/ 3200]\n",
      "loss: 0.925147  [ 2464/ 3200]\n",
      "loss: 0.695336  [ 2480/ 3200]\n",
      "loss: 0.987483  [ 2496/ 3200]\n",
      "loss: 0.810425  [ 2512/ 3200]\n",
      "loss: 0.667353  [ 2528/ 3200]\n",
      "loss: 0.929409  [ 2544/ 3200]\n",
      "loss: 0.824962  [ 2560/ 3200]\n",
      "loss: 0.757510  [ 2576/ 3200]\n",
      "loss: 0.862129  [ 2592/ 3200]\n",
      "loss: 0.949030  [ 2608/ 3200]\n",
      "loss: 0.832988  [ 2624/ 3200]\n",
      "loss: 0.766074  [ 2640/ 3200]\n",
      "loss: 0.789313  [ 2656/ 3200]\n",
      "loss: 1.062703  [ 2672/ 3200]\n",
      "loss: 0.799385  [ 2688/ 3200]\n",
      "loss: 0.765381  [ 2704/ 3200]\n",
      "loss: 0.738600  [ 2720/ 3200]\n",
      "loss: 0.756075  [ 2736/ 3200]\n",
      "loss: 1.638062  [ 2752/ 3200]\n",
      "loss: 1.140480  [ 2768/ 3200]\n",
      "loss: 0.600242  [ 2784/ 3200]\n",
      "loss: 0.842056  [ 2800/ 3200]\n",
      "loss: 1.076208  [ 2816/ 3200]\n",
      "loss: 0.982722  [ 2832/ 3200]\n",
      "loss: 0.819217  [ 2848/ 3200]\n",
      "loss: 0.672942  [ 2864/ 3200]\n",
      "loss: 0.834139  [ 2880/ 3200]\n",
      "loss: 0.537192  [ 2896/ 3200]\n",
      "loss: 0.900190  [ 2912/ 3200]\n",
      "loss: 0.720285  [ 2928/ 3200]\n",
      "loss: 1.045566  [ 2944/ 3200]\n",
      "loss: 1.000483  [ 2960/ 3200]\n",
      "loss: 0.604788  [ 2976/ 3200]\n",
      "loss: 1.057751  [ 2992/ 3200]\n",
      "loss: 0.951319  [ 3008/ 3200]\n",
      "loss: 0.654687  [ 3024/ 3200]\n",
      "loss: 0.693421  [ 3040/ 3200]\n",
      "loss: 0.669214  [ 3056/ 3200]\n",
      "loss: 1.019019  [ 3072/ 3200]\n",
      "loss: 0.977757  [ 3088/ 3200]\n",
      "loss: 1.107674  [ 3104/ 3200]\n",
      "loss: 0.930136  [ 3120/ 3200]\n",
      "loss: 1.026595  [ 3136/ 3200]\n",
      "loss: 0.514134  [ 3152/ 3200]\n",
      "loss: 0.799881  [ 3168/ 3200]\n",
      "loss: 0.508249  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 0.820775  [    0/ 3200]\n",
      "loss: 0.820025  [   16/ 3200]\n",
      "loss: 0.793510  [   32/ 3200]\n",
      "loss: 0.926910  [   48/ 3200]\n",
      "loss: 0.916676  [   64/ 3200]\n",
      "loss: 0.880175  [   80/ 3200]\n",
      "loss: 0.915587  [   96/ 3200]\n",
      "loss: 0.956710  [  112/ 3200]\n",
      "loss: 0.989055  [  128/ 3200]\n",
      "loss: 0.646914  [  144/ 3200]\n",
      "loss: 0.490564  [  160/ 3200]\n",
      "loss: 0.803255  [  176/ 3200]\n",
      "loss: 0.904667  [  192/ 3200]\n",
      "loss: 0.815323  [  208/ 3200]\n",
      "loss: 0.952447  [  224/ 3200]\n",
      "loss: 0.984984  [  240/ 3200]\n",
      "loss: 1.088390  [  256/ 3200]\n",
      "loss: 0.839469  [  272/ 3200]\n",
      "loss: 0.573964  [  288/ 3200]\n",
      "loss: 0.978043  [  304/ 3200]\n",
      "loss: 0.763568  [  320/ 3200]\n",
      "loss: 0.717560  [  336/ 3200]\n",
      "loss: 0.760025  [  352/ 3200]\n",
      "loss: 0.944565  [  368/ 3200]\n",
      "loss: 0.971513  [  384/ 3200]\n",
      "loss: 0.666352  [  400/ 3200]\n",
      "loss: 0.812182  [  416/ 3200]\n",
      "loss: 1.058489  [  432/ 3200]\n",
      "loss: 0.941327  [  448/ 3200]\n",
      "loss: 0.904788  [  464/ 3200]\n",
      "loss: 0.657077  [  480/ 3200]\n",
      "loss: 0.829602  [  496/ 3200]\n",
      "loss: 0.921340  [  512/ 3200]\n",
      "loss: 0.829675  [  528/ 3200]\n",
      "loss: 1.029331  [  544/ 3200]\n",
      "loss: 0.636950  [  560/ 3200]\n",
      "loss: 0.925241  [  576/ 3200]\n",
      "loss: 0.734278  [  592/ 3200]\n",
      "loss: 0.529247  [  608/ 3200]\n",
      "loss: 1.129082  [  624/ 3200]\n",
      "loss: 1.096329  [  640/ 3200]\n",
      "loss: 0.914122  [  656/ 3200]\n",
      "loss: 0.807899  [  672/ 3200]\n",
      "loss: 0.867563  [  688/ 3200]\n",
      "loss: 0.822199  [  704/ 3200]\n",
      "loss: 0.786244  [  720/ 3200]\n",
      "loss: 0.709359  [  736/ 3200]\n",
      "loss: 0.790204  [  752/ 3200]\n",
      "loss: 1.016386  [  768/ 3200]\n",
      "loss: 1.055298  [  784/ 3200]\n",
      "loss: 0.866451  [  800/ 3200]\n",
      "loss: 0.640902  [  816/ 3200]\n",
      "loss: 0.826479  [  832/ 3200]\n",
      "loss: 0.527881  [  848/ 3200]\n",
      "loss: 0.943772  [  864/ 3200]\n",
      "loss: 1.070597  [  880/ 3200]\n",
      "loss: 0.769263  [  896/ 3200]\n",
      "loss: 1.087599  [  912/ 3200]\n",
      "loss: 0.840605  [  928/ 3200]\n",
      "loss: 0.740793  [  944/ 3200]\n",
      "loss: 0.917964  [  960/ 3200]\n",
      "loss: 1.061703  [  976/ 3200]\n",
      "loss: 0.867208  [  992/ 3200]\n",
      "loss: 0.909667  [ 1008/ 3200]\n",
      "loss: 1.033710  [ 1024/ 3200]\n",
      "loss: 0.891255  [ 1040/ 3200]\n",
      "loss: 0.921732  [ 1056/ 3200]\n",
      "loss: 0.749857  [ 1072/ 3200]\n",
      "loss: 0.861579  [ 1088/ 3200]\n",
      "loss: 0.800164  [ 1104/ 3200]\n",
      "loss: 0.652970  [ 1120/ 3200]\n",
      "loss: 0.713434  [ 1136/ 3200]\n",
      "loss: 1.118342  [ 1152/ 3200]\n",
      "loss: 0.777829  [ 1168/ 3200]\n",
      "loss: 0.794429  [ 1184/ 3200]\n",
      "loss: 0.727775  [ 1200/ 3200]\n",
      "loss: 0.551245  [ 1216/ 3200]\n",
      "loss: 1.080459  [ 1232/ 3200]\n",
      "loss: 1.012460  [ 1248/ 3200]\n",
      "loss: 1.072539  [ 1264/ 3200]\n",
      "loss: 0.527369  [ 1280/ 3200]\n",
      "loss: 0.706127  [ 1296/ 3200]\n",
      "loss: 1.157531  [ 1312/ 3200]\n",
      "loss: 1.044274  [ 1328/ 3200]\n",
      "loss: 0.969476  [ 1344/ 3200]\n",
      "loss: 0.822786  [ 1360/ 3200]\n",
      "loss: 0.802706  [ 1376/ 3200]\n",
      "loss: 0.774102  [ 1392/ 3200]\n",
      "loss: 0.800536  [ 1408/ 3200]\n",
      "loss: 0.661854  [ 1424/ 3200]\n",
      "loss: 0.985476  [ 1440/ 3200]\n",
      "loss: 0.728962  [ 1456/ 3200]\n",
      "loss: 0.863897  [ 1472/ 3200]\n",
      "loss: 0.970735  [ 1488/ 3200]\n",
      "loss: 0.787389  [ 1504/ 3200]\n",
      "loss: 0.823798  [ 1520/ 3200]\n",
      "loss: 0.715279  [ 1536/ 3200]\n",
      "loss: 0.705938  [ 1552/ 3200]\n",
      "loss: 0.749126  [ 1568/ 3200]\n",
      "loss: 1.018746  [ 1584/ 3200]\n",
      "loss: 0.636074  [ 1600/ 3200]\n",
      "loss: 0.767291  [ 1616/ 3200]\n",
      "loss: 0.944897  [ 1632/ 3200]\n",
      "loss: 0.728699  [ 1648/ 3200]\n",
      "loss: 0.730565  [ 1664/ 3200]\n",
      "loss: 0.562632  [ 1680/ 3200]\n",
      "loss: 1.631773  [ 1696/ 3200]\n",
      "loss: 1.017839  [ 1712/ 3200]\n",
      "loss: 0.973777  [ 1728/ 3200]\n",
      "loss: 1.032320  [ 1744/ 3200]\n",
      "loss: 0.804929  [ 1760/ 3200]\n",
      "loss: 0.936029  [ 1776/ 3200]\n",
      "loss: 1.028185  [ 1792/ 3200]\n",
      "loss: 0.849981  [ 1808/ 3200]\n",
      "loss: 0.774371  [ 1824/ 3200]\n",
      "loss: 0.861739  [ 1840/ 3200]\n",
      "loss: 0.841560  [ 1856/ 3200]\n",
      "loss: 1.130499  [ 1872/ 3200]\n",
      "loss: 0.743744  [ 1888/ 3200]\n",
      "loss: 0.857394  [ 1904/ 3200]\n",
      "loss: 0.775128  [ 1920/ 3200]\n",
      "loss: 0.849155  [ 1936/ 3200]\n",
      "loss: 0.790112  [ 1952/ 3200]\n",
      "loss: 0.915501  [ 1968/ 3200]\n",
      "loss: 0.917933  [ 1984/ 3200]\n",
      "loss: 1.025884  [ 2000/ 3200]\n",
      "loss: 1.128403  [ 2016/ 3200]\n",
      "loss: 1.044778  [ 2032/ 3200]\n",
      "loss: 0.961968  [ 2048/ 3200]\n",
      "loss: 0.676857  [ 2064/ 3200]\n",
      "loss: 0.598553  [ 2080/ 3200]\n",
      "loss: 0.921224  [ 2096/ 3200]\n",
      "loss: 1.256643  [ 2112/ 3200]\n",
      "loss: 1.061794  [ 2128/ 3200]\n",
      "loss: 0.737083  [ 2144/ 3200]\n",
      "loss: 0.897421  [ 2160/ 3200]\n",
      "loss: 0.900357  [ 2176/ 3200]\n",
      "loss: 0.697950  [ 2192/ 3200]\n",
      "loss: 0.739821  [ 2208/ 3200]\n",
      "loss: 0.795550  [ 2224/ 3200]\n",
      "loss: 0.738379  [ 2240/ 3200]\n",
      "loss: 0.954160  [ 2256/ 3200]\n",
      "loss: 0.876761  [ 2272/ 3200]\n",
      "loss: 0.812050  [ 2288/ 3200]\n",
      "loss: 0.895148  [ 2304/ 3200]\n",
      "loss: 1.050177  [ 2320/ 3200]\n",
      "loss: 0.982101  [ 2336/ 3200]\n",
      "loss: 1.014925  [ 2352/ 3200]\n",
      "loss: 1.011891  [ 2368/ 3200]\n",
      "loss: 0.591360  [ 2384/ 3200]\n",
      "loss: 1.066242  [ 2400/ 3200]\n",
      "loss: 0.662400  [ 2416/ 3200]\n",
      "loss: 1.006698  [ 2432/ 3200]\n",
      "loss: 1.003417  [ 2448/ 3200]\n",
      "loss: 1.159328  [ 2464/ 3200]\n",
      "loss: 0.850971  [ 2480/ 3200]\n",
      "loss: 0.487872  [ 2496/ 3200]\n",
      "loss: 0.861127  [ 2512/ 3200]\n",
      "loss: 0.772730  [ 2528/ 3200]\n",
      "loss: 1.055636  [ 2544/ 3200]\n",
      "loss: 0.784236  [ 2560/ 3200]\n",
      "loss: 0.924642  [ 2576/ 3200]\n",
      "loss: 0.911102  [ 2592/ 3200]\n",
      "loss: 0.856012  [ 2608/ 3200]\n",
      "loss: 0.912326  [ 2624/ 3200]\n",
      "loss: 0.783314  [ 2640/ 3200]\n",
      "loss: 0.566624  [ 2656/ 3200]\n",
      "loss: 0.694967  [ 2672/ 3200]\n",
      "loss: 0.615676  [ 2688/ 3200]\n",
      "loss: 0.528119  [ 2704/ 3200]\n",
      "loss: 0.641052  [ 2720/ 3200]\n",
      "loss: 0.942712  [ 2736/ 3200]\n",
      "loss: 0.635824  [ 2752/ 3200]\n",
      "loss: 1.090937  [ 2768/ 3200]\n",
      "loss: 0.575455  [ 2784/ 3200]\n",
      "loss: 0.667342  [ 2800/ 3200]\n",
      "loss: 1.528455  [ 2816/ 3200]\n",
      "loss: 0.674713  [ 2832/ 3200]\n",
      "loss: 1.363285  [ 2848/ 3200]\n",
      "loss: 1.071915  [ 2864/ 3200]\n",
      "loss: 0.841582  [ 2880/ 3200]\n",
      "loss: 0.834353  [ 2896/ 3200]\n",
      "loss: 0.904318  [ 2912/ 3200]\n",
      "loss: 1.025732  [ 2928/ 3200]\n",
      "loss: 0.749670  [ 2944/ 3200]\n",
      "loss: 0.655042  [ 2960/ 3200]\n",
      "loss: 1.042568  [ 2976/ 3200]\n",
      "loss: 1.114599  [ 2992/ 3200]\n",
      "loss: 0.864902  [ 3008/ 3200]\n",
      "loss: 0.590197  [ 3024/ 3200]\n",
      "loss: 0.656978  [ 3040/ 3200]\n",
      "loss: 0.854713  [ 3056/ 3200]\n",
      "loss: 0.829057  [ 3072/ 3200]\n",
      "loss: 1.214032  [ 3088/ 3200]\n",
      "loss: 1.304848  [ 3104/ 3200]\n",
      "loss: 0.588501  [ 3120/ 3200]\n",
      "loss: 1.011763  [ 3136/ 3200]\n",
      "loss: 0.779906  [ 3152/ 3200]\n",
      "loss: 0.622865  [ 3168/ 3200]\n",
      "loss: 0.859581  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.652940  [    0/ 3200]\n",
      "loss: 0.933217  [   16/ 3200]\n",
      "loss: 0.697217  [   32/ 3200]\n",
      "loss: 1.045766  [   48/ 3200]\n",
      "loss: 0.594373  [   64/ 3200]\n",
      "loss: 1.089370  [   80/ 3200]\n",
      "loss: 0.625529  [   96/ 3200]\n",
      "loss: 0.776223  [  112/ 3200]\n",
      "loss: 0.699733  [  128/ 3200]\n",
      "loss: 0.973138  [  144/ 3200]\n",
      "loss: 0.959973  [  160/ 3200]\n",
      "loss: 0.791506  [  176/ 3200]\n",
      "loss: 0.951212  [  192/ 3200]\n",
      "loss: 0.863823  [  208/ 3200]\n",
      "loss: 0.832504  [  224/ 3200]\n",
      "loss: 0.806720  [  240/ 3200]\n",
      "loss: 0.781155  [  256/ 3200]\n",
      "loss: 0.639269  [  272/ 3200]\n",
      "loss: 0.783388  [  288/ 3200]\n",
      "loss: 0.753200  [  304/ 3200]\n",
      "loss: 0.892553  [  320/ 3200]\n",
      "loss: 0.807450  [  336/ 3200]\n",
      "loss: 1.317512  [  352/ 3200]\n",
      "loss: 0.850765  [  368/ 3200]\n",
      "loss: 1.170994  [  384/ 3200]\n",
      "loss: 0.815686  [  400/ 3200]\n",
      "loss: 0.830139  [  416/ 3200]\n",
      "loss: 0.989372  [  432/ 3200]\n",
      "loss: 0.819379  [  448/ 3200]\n",
      "loss: 0.564944  [  464/ 3200]\n",
      "loss: 0.606981  [  480/ 3200]\n",
      "loss: 1.039972  [  496/ 3200]\n",
      "loss: 1.007269  [  512/ 3200]\n",
      "loss: 0.734653  [  528/ 3200]\n",
      "loss: 0.833712  [  544/ 3200]\n",
      "loss: 0.984960  [  560/ 3200]\n",
      "loss: 1.111465  [  576/ 3200]\n",
      "loss: 0.893896  [  592/ 3200]\n",
      "loss: 0.863974  [  608/ 3200]\n",
      "loss: 0.765702  [  624/ 3200]\n",
      "loss: 0.914924  [  640/ 3200]\n",
      "loss: 0.756187  [  656/ 3200]\n",
      "loss: 0.442424  [  672/ 3200]\n",
      "loss: 0.926378  [  688/ 3200]\n",
      "loss: 0.892173  [  704/ 3200]\n",
      "loss: 0.817434  [  720/ 3200]\n",
      "loss: 0.592550  [  736/ 3200]\n",
      "loss: 0.646233  [  752/ 3200]\n",
      "loss: 0.649645  [  768/ 3200]\n",
      "loss: 1.027377  [  784/ 3200]\n",
      "loss: 0.995605  [  800/ 3200]\n",
      "loss: 0.527118  [  816/ 3200]\n",
      "loss: 0.999711  [  832/ 3200]\n",
      "loss: 0.713762  [  848/ 3200]\n",
      "loss: 0.875639  [  864/ 3200]\n",
      "loss: 0.785521  [  880/ 3200]\n",
      "loss: 0.649461  [  896/ 3200]\n",
      "loss: 0.977244  [  912/ 3200]\n",
      "loss: 0.644008  [  928/ 3200]\n",
      "loss: 0.773878  [  944/ 3200]\n",
      "loss: 0.650418  [  960/ 3200]\n",
      "loss: 0.899081  [  976/ 3200]\n",
      "loss: 0.704993  [  992/ 3200]\n",
      "loss: 1.023847  [ 1008/ 3200]\n",
      "loss: 0.838180  [ 1024/ 3200]\n",
      "loss: 1.127527  [ 1040/ 3200]\n",
      "loss: 0.902931  [ 1056/ 3200]\n",
      "loss: 1.023973  [ 1072/ 3200]\n",
      "loss: 0.781490  [ 1088/ 3200]\n",
      "loss: 0.845416  [ 1104/ 3200]\n",
      "loss: 0.568458  [ 1120/ 3200]\n",
      "loss: 0.723427  [ 1136/ 3200]\n",
      "loss: 0.933889  [ 1152/ 3200]\n",
      "loss: 0.935019  [ 1168/ 3200]\n",
      "loss: 0.747164  [ 1184/ 3200]\n",
      "loss: 0.527422  [ 1200/ 3200]\n",
      "loss: 0.788810  [ 1216/ 3200]\n",
      "loss: 0.924540  [ 1232/ 3200]\n",
      "loss: 0.671662  [ 1248/ 3200]\n",
      "loss: 0.831271  [ 1264/ 3200]\n",
      "loss: 0.884243  [ 1280/ 3200]\n",
      "loss: 0.609592  [ 1296/ 3200]\n",
      "loss: 1.078131  [ 1312/ 3200]\n",
      "loss: 0.877046  [ 1328/ 3200]\n",
      "loss: 0.834671  [ 1344/ 3200]\n",
      "loss: 0.872157  [ 1360/ 3200]\n",
      "loss: 1.107406  [ 1376/ 3200]\n",
      "loss: 0.833948  [ 1392/ 3200]\n",
      "loss: 0.882345  [ 1408/ 3200]\n",
      "loss: 0.944093  [ 1424/ 3200]\n",
      "loss: 0.728526  [ 1440/ 3200]\n",
      "loss: 0.680523  [ 1456/ 3200]\n",
      "loss: 1.049630  [ 1472/ 3200]\n",
      "loss: 0.966486  [ 1488/ 3200]\n",
      "loss: 1.282830  [ 1504/ 3200]\n",
      "loss: 1.343463  [ 1520/ 3200]\n",
      "loss: 0.785314  [ 1536/ 3200]\n",
      "loss: 0.964357  [ 1552/ 3200]\n",
      "loss: 0.995673  [ 1568/ 3200]\n",
      "loss: 0.769794  [ 1584/ 3200]\n",
      "loss: 0.771819  [ 1600/ 3200]\n",
      "loss: 0.851451  [ 1616/ 3200]\n",
      "loss: 0.987676  [ 1632/ 3200]\n",
      "loss: 1.141986  [ 1648/ 3200]\n",
      "loss: 0.737256  [ 1664/ 3200]\n",
      "loss: 0.795722  [ 1680/ 3200]\n",
      "loss: 0.788079  [ 1696/ 3200]\n",
      "loss: 0.916934  [ 1712/ 3200]\n",
      "loss: 1.094680  [ 1728/ 3200]\n",
      "loss: 0.670108  [ 1744/ 3200]\n",
      "loss: 0.739019  [ 1760/ 3200]\n",
      "loss: 0.988842  [ 1776/ 3200]\n",
      "loss: 0.739818  [ 1792/ 3200]\n",
      "loss: 1.118043  [ 1808/ 3200]\n",
      "loss: 1.618263  [ 1824/ 3200]\n",
      "loss: 1.401506  [ 1840/ 3200]\n",
      "loss: 1.011371  [ 1856/ 3200]\n",
      "loss: 0.644533  [ 1872/ 3200]\n",
      "loss: 0.953343  [ 1888/ 3200]\n",
      "loss: 0.833224  [ 1904/ 3200]\n",
      "loss: 0.882346  [ 1920/ 3200]\n",
      "loss: 0.775670  [ 1936/ 3200]\n",
      "loss: 1.149276  [ 1952/ 3200]\n",
      "loss: 1.184723  [ 1968/ 3200]\n",
      "loss: 0.831285  [ 1984/ 3200]\n",
      "loss: 1.057831  [ 2000/ 3200]\n",
      "loss: 0.991191  [ 2016/ 3200]\n",
      "loss: 0.473399  [ 2032/ 3200]\n",
      "loss: 0.693714  [ 2048/ 3200]\n",
      "loss: 0.725505  [ 2064/ 3200]\n",
      "loss: 1.095902  [ 2080/ 3200]\n",
      "loss: 0.693514  [ 2096/ 3200]\n",
      "loss: 0.812610  [ 2112/ 3200]\n",
      "loss: 0.614962  [ 2128/ 3200]\n",
      "loss: 0.625467  [ 2144/ 3200]\n",
      "loss: 0.871000  [ 2160/ 3200]\n",
      "loss: 1.030106  [ 2176/ 3200]\n",
      "loss: 0.814858  [ 2192/ 3200]\n",
      "loss: 0.678456  [ 2208/ 3200]\n",
      "loss: 0.700476  [ 2224/ 3200]\n",
      "loss: 0.848853  [ 2240/ 3200]\n",
      "loss: 0.969944  [ 2256/ 3200]\n",
      "loss: 1.148646  [ 2272/ 3200]\n",
      "loss: 1.227232  [ 2288/ 3200]\n",
      "loss: 1.097928  [ 2304/ 3200]\n",
      "loss: 1.043779  [ 2320/ 3200]\n",
      "loss: 1.084634  [ 2336/ 3200]\n",
      "loss: 0.832692  [ 2352/ 3200]\n",
      "loss: 0.519300  [ 2368/ 3200]\n",
      "loss: 0.759444  [ 2384/ 3200]\n",
      "loss: 0.579728  [ 2400/ 3200]\n",
      "loss: 0.732244  [ 2416/ 3200]\n",
      "loss: 0.759409  [ 2432/ 3200]\n",
      "loss: 1.349335  [ 2448/ 3200]\n",
      "loss: 1.070678  [ 2464/ 3200]\n",
      "loss: 0.730165  [ 2480/ 3200]\n",
      "loss: 0.891035  [ 2496/ 3200]\n",
      "loss: 0.843320  [ 2512/ 3200]\n",
      "loss: 0.745976  [ 2528/ 3200]\n",
      "loss: 0.797513  [ 2544/ 3200]\n",
      "loss: 0.520571  [ 2560/ 3200]\n",
      "loss: 0.829863  [ 2576/ 3200]\n",
      "loss: 0.969689  [ 2592/ 3200]\n",
      "loss: 0.812915  [ 2608/ 3200]\n",
      "loss: 0.845080  [ 2624/ 3200]\n",
      "loss: 0.594390  [ 2640/ 3200]\n",
      "loss: 0.735518  [ 2656/ 3200]\n",
      "loss: 0.743356  [ 2672/ 3200]\n",
      "loss: 0.957152  [ 2688/ 3200]\n",
      "loss: 1.532450  [ 2704/ 3200]\n",
      "loss: 0.778066  [ 2720/ 3200]\n",
      "loss: 1.047688  [ 2736/ 3200]\n",
      "loss: 0.842349  [ 2752/ 3200]\n",
      "loss: 0.740434  [ 2768/ 3200]\n",
      "loss: 0.661502  [ 2784/ 3200]\n",
      "loss: 1.027400  [ 2800/ 3200]\n",
      "loss: 1.178748  [ 2816/ 3200]\n",
      "loss: 1.055576  [ 2832/ 3200]\n",
      "loss: 0.784497  [ 2848/ 3200]\n",
      "loss: 0.921188  [ 2864/ 3200]\n",
      "loss: 0.553787  [ 2880/ 3200]\n",
      "loss: 1.051689  [ 2896/ 3200]\n",
      "loss: 0.867605  [ 2912/ 3200]\n",
      "loss: 0.801935  [ 2928/ 3200]\n",
      "loss: 0.726767  [ 2944/ 3200]\n",
      "loss: 0.678161  [ 2960/ 3200]\n",
      "loss: 0.936070  [ 2976/ 3200]\n",
      "loss: 0.966946  [ 2992/ 3200]\n",
      "loss: 0.658122  [ 3008/ 3200]\n",
      "loss: 0.796660  [ 3024/ 3200]\n",
      "loss: 1.203197  [ 3040/ 3200]\n",
      "loss: 0.906300  [ 3056/ 3200]\n",
      "loss: 0.565598  [ 3072/ 3200]\n",
      "loss: 1.055664  [ 3088/ 3200]\n",
      "loss: 1.120574  [ 3104/ 3200]\n",
      "loss: 1.117364  [ 3120/ 3200]\n",
      "loss: 0.891042  [ 3136/ 3200]\n",
      "loss: 0.510125  [ 3152/ 3200]\n",
      "loss: 0.907983  [ 3168/ 3200]\n",
      "loss: 0.733420  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 0.597008  [    0/ 3200]\n",
      "loss: 0.704695  [   16/ 3200]\n",
      "loss: 0.649130  [   32/ 3200]\n",
      "loss: 0.757402  [   48/ 3200]\n",
      "loss: 0.789875  [   64/ 3200]\n",
      "loss: 1.096700  [   80/ 3200]\n",
      "loss: 0.660406  [   96/ 3200]\n",
      "loss: 0.883967  [  112/ 3200]\n",
      "loss: 0.827572  [  128/ 3200]\n",
      "loss: 0.914062  [  144/ 3200]\n",
      "loss: 1.041602  [  160/ 3200]\n",
      "loss: 0.759849  [  176/ 3200]\n",
      "loss: 0.746257  [  192/ 3200]\n",
      "loss: 0.692175  [  208/ 3200]\n",
      "loss: 1.107445  [  224/ 3200]\n",
      "loss: 0.736582  [  240/ 3200]\n",
      "loss: 0.911027  [  256/ 3200]\n",
      "loss: 0.712241  [  272/ 3200]\n",
      "loss: 1.105160  [  288/ 3200]\n",
      "loss: 0.824167  [  304/ 3200]\n",
      "loss: 0.962002  [  320/ 3200]\n",
      "loss: 0.944795  [  336/ 3200]\n",
      "loss: 0.893792  [  352/ 3200]\n",
      "loss: 0.886453  [  368/ 3200]\n",
      "loss: 0.927220  [  384/ 3200]\n",
      "loss: 0.625921  [  400/ 3200]\n",
      "loss: 0.943263  [  416/ 3200]\n",
      "loss: 0.657437  [  432/ 3200]\n",
      "loss: 0.939313  [  448/ 3200]\n",
      "loss: 0.779387  [  464/ 3200]\n",
      "loss: 1.095375  [  480/ 3200]\n",
      "loss: 1.370704  [  496/ 3200]\n",
      "loss: 1.057761  [  512/ 3200]\n",
      "loss: 0.830440  [  528/ 3200]\n",
      "loss: 1.161728  [  544/ 3200]\n",
      "loss: 0.888308  [  560/ 3200]\n",
      "loss: 0.712002  [  576/ 3200]\n",
      "loss: 0.758639  [  592/ 3200]\n",
      "loss: 0.921160  [  608/ 3200]\n",
      "loss: 0.755677  [  624/ 3200]\n",
      "loss: 0.538636  [  640/ 3200]\n",
      "loss: 1.098818  [  656/ 3200]\n",
      "loss: 1.034185  [  672/ 3200]\n",
      "loss: 0.805916  [  688/ 3200]\n",
      "loss: 1.098853  [  704/ 3200]\n",
      "loss: 0.672951  [  720/ 3200]\n",
      "loss: 0.782650  [  736/ 3200]\n",
      "loss: 0.863226  [  752/ 3200]\n",
      "loss: 0.971522  [  768/ 3200]\n",
      "loss: 0.544528  [  784/ 3200]\n",
      "loss: 0.850971  [  800/ 3200]\n",
      "loss: 0.852556  [  816/ 3200]\n",
      "loss: 1.007751  [  832/ 3200]\n",
      "loss: 0.771942  [  848/ 3200]\n",
      "loss: 0.855206  [  864/ 3200]\n",
      "loss: 0.856229  [  880/ 3200]\n",
      "loss: 0.885636  [  896/ 3200]\n",
      "loss: 0.695895  [  912/ 3200]\n",
      "loss: 0.957570  [  928/ 3200]\n",
      "loss: 1.158016  [  944/ 3200]\n",
      "loss: 1.115189  [  960/ 3200]\n",
      "loss: 0.816700  [  976/ 3200]\n",
      "loss: 1.015719  [  992/ 3200]\n",
      "loss: 1.040630  [ 1008/ 3200]\n",
      "loss: 1.030253  [ 1024/ 3200]\n",
      "loss: 0.747552  [ 1040/ 3200]\n",
      "loss: 0.820042  [ 1056/ 3200]\n",
      "loss: 0.791765  [ 1072/ 3200]\n",
      "loss: 0.705989  [ 1088/ 3200]\n",
      "loss: 1.081219  [ 1104/ 3200]\n",
      "loss: 0.612484  [ 1120/ 3200]\n",
      "loss: 0.917151  [ 1136/ 3200]\n",
      "loss: 1.017533  [ 1152/ 3200]\n",
      "loss: 0.928176  [ 1168/ 3200]\n",
      "loss: 0.767620  [ 1184/ 3200]\n",
      "loss: 0.847583  [ 1200/ 3200]\n",
      "loss: 0.776402  [ 1216/ 3200]\n",
      "loss: 0.892111  [ 1232/ 3200]\n",
      "loss: 0.980389  [ 1248/ 3200]\n",
      "loss: 0.665961  [ 1264/ 3200]\n",
      "loss: 0.679011  [ 1280/ 3200]\n",
      "loss: 0.622502  [ 1296/ 3200]\n",
      "loss: 0.976445  [ 1312/ 3200]\n",
      "loss: 0.903776  [ 1328/ 3200]\n",
      "loss: 0.840485  [ 1344/ 3200]\n",
      "loss: 0.675481  [ 1360/ 3200]\n",
      "loss: 1.000257  [ 1376/ 3200]\n",
      "loss: 0.935326  [ 1392/ 3200]\n",
      "loss: 0.843445  [ 1408/ 3200]\n",
      "loss: 0.713268  [ 1424/ 3200]\n",
      "loss: 0.765546  [ 1440/ 3200]\n",
      "loss: 1.182810  [ 1456/ 3200]\n",
      "loss: 0.478057  [ 1472/ 3200]\n",
      "loss: 0.750644  [ 1488/ 3200]\n",
      "loss: 0.749453  [ 1504/ 3200]\n",
      "loss: 1.189533  [ 1520/ 3200]\n",
      "loss: 0.551822  [ 1536/ 3200]\n",
      "loss: 0.760264  [ 1552/ 3200]\n",
      "loss: 0.549600  [ 1568/ 3200]\n",
      "loss: 0.825003  [ 1584/ 3200]\n",
      "loss: 0.606320  [ 1600/ 3200]\n",
      "loss: 0.864582  [ 1616/ 3200]\n",
      "loss: 1.090642  [ 1632/ 3200]\n",
      "loss: 0.816602  [ 1648/ 3200]\n",
      "loss: 0.798209  [ 1664/ 3200]\n",
      "loss: 1.211520  [ 1680/ 3200]\n",
      "loss: 0.698081  [ 1696/ 3200]\n",
      "loss: 0.850551  [ 1712/ 3200]\n",
      "loss: 0.865067  [ 1728/ 3200]\n",
      "loss: 0.828683  [ 1744/ 3200]\n",
      "loss: 0.667297  [ 1760/ 3200]\n",
      "loss: 0.813674  [ 1776/ 3200]\n",
      "loss: 0.857874  [ 1792/ 3200]\n",
      "loss: 1.106780  [ 1808/ 3200]\n",
      "loss: 0.533936  [ 1824/ 3200]\n",
      "loss: 1.204301  [ 1840/ 3200]\n",
      "loss: 0.843949  [ 1856/ 3200]\n",
      "loss: 1.103254  [ 1872/ 3200]\n",
      "loss: 0.746751  [ 1888/ 3200]\n",
      "loss: 0.512693  [ 1904/ 3200]\n",
      "loss: 0.588527  [ 1920/ 3200]\n",
      "loss: 0.819286  [ 1936/ 3200]\n",
      "loss: 0.656597  [ 1952/ 3200]\n",
      "loss: 0.696069  [ 1968/ 3200]\n",
      "loss: 0.854996  [ 1984/ 3200]\n",
      "loss: 0.825292  [ 2000/ 3200]\n",
      "loss: 0.837502  [ 2016/ 3200]\n",
      "loss: 0.624045  [ 2032/ 3200]\n",
      "loss: 0.583694  [ 2048/ 3200]\n",
      "loss: 0.780489  [ 2064/ 3200]\n",
      "loss: 0.782889  [ 2080/ 3200]\n",
      "loss: 0.901016  [ 2096/ 3200]\n",
      "loss: 1.119393  [ 2112/ 3200]\n",
      "loss: 0.844067  [ 2128/ 3200]\n",
      "loss: 0.816838  [ 2144/ 3200]\n",
      "loss: 0.524914  [ 2160/ 3200]\n",
      "loss: 0.706759  [ 2176/ 3200]\n",
      "loss: 0.833251  [ 2192/ 3200]\n",
      "loss: 1.233630  [ 2208/ 3200]\n",
      "loss: 1.115250  [ 2224/ 3200]\n",
      "loss: 0.689331  [ 2240/ 3200]\n",
      "loss: 0.628514  [ 2256/ 3200]\n",
      "loss: 0.758695  [ 2272/ 3200]\n",
      "loss: 0.929700  [ 2288/ 3200]\n",
      "loss: 0.750684  [ 2304/ 3200]\n",
      "loss: 0.689861  [ 2320/ 3200]\n",
      "loss: 0.816254  [ 2336/ 3200]\n",
      "loss: 0.542144  [ 2352/ 3200]\n",
      "loss: 0.807182  [ 2368/ 3200]\n",
      "loss: 0.591127  [ 2384/ 3200]\n",
      "loss: 0.830918  [ 2400/ 3200]\n",
      "loss: 0.872283  [ 2416/ 3200]\n",
      "loss: 1.079278  [ 2432/ 3200]\n",
      "loss: 0.946648  [ 2448/ 3200]\n",
      "loss: 0.695512  [ 2464/ 3200]\n",
      "loss: 0.736271  [ 2480/ 3200]\n",
      "loss: 0.895307  [ 2496/ 3200]\n",
      "loss: 1.085840  [ 2512/ 3200]\n",
      "loss: 1.030622  [ 2528/ 3200]\n",
      "loss: 0.974091  [ 2544/ 3200]\n",
      "loss: 0.642842  [ 2560/ 3200]\n",
      "loss: 0.778801  [ 2576/ 3200]\n",
      "loss: 0.668517  [ 2592/ 3200]\n",
      "loss: 0.624222  [ 2608/ 3200]\n",
      "loss: 0.649000  [ 2624/ 3200]\n",
      "loss: 1.417916  [ 2640/ 3200]\n",
      "loss: 0.805173  [ 2656/ 3200]\n",
      "loss: 0.557176  [ 2672/ 3200]\n",
      "loss: 1.253493  [ 2688/ 3200]\n",
      "loss: 0.508234  [ 2704/ 3200]\n",
      "loss: 0.707731  [ 2720/ 3200]\n",
      "loss: 0.731068  [ 2736/ 3200]\n",
      "loss: 0.801162  [ 2752/ 3200]\n",
      "loss: 0.603666  [ 2768/ 3200]\n",
      "loss: 0.887038  [ 2784/ 3200]\n",
      "loss: 0.803237  [ 2800/ 3200]\n",
      "loss: 0.909921  [ 2816/ 3200]\n",
      "loss: 0.995536  [ 2832/ 3200]\n",
      "loss: 0.561132  [ 2848/ 3200]\n",
      "loss: 1.240245  [ 2864/ 3200]\n",
      "loss: 1.178793  [ 2880/ 3200]\n",
      "loss: 0.986406  [ 2896/ 3200]\n",
      "loss: 1.102468  [ 2912/ 3200]\n",
      "loss: 0.917390  [ 2928/ 3200]\n",
      "loss: 0.644211  [ 2944/ 3200]\n",
      "loss: 1.099584  [ 2960/ 3200]\n",
      "loss: 1.093126  [ 2976/ 3200]\n",
      "loss: 0.833064  [ 2992/ 3200]\n",
      "loss: 0.986191  [ 3008/ 3200]\n",
      "loss: 0.640925  [ 3024/ 3200]\n",
      "loss: 0.835289  [ 3040/ 3200]\n",
      "loss: 0.569487  [ 3056/ 3200]\n",
      "loss: 0.813582  [ 3072/ 3200]\n",
      "loss: 0.598510  [ 3088/ 3200]\n",
      "loss: 0.870586  [ 3104/ 3200]\n",
      "loss: 1.073095  [ 3120/ 3200]\n",
      "loss: 0.682054  [ 3136/ 3200]\n",
      "loss: 0.746834  [ 3152/ 3200]\n",
      "loss: 0.514404  [ 3168/ 3200]\n",
      "loss: 0.706510  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.727387  [    0/ 3200]\n",
      "loss: 0.931319  [   16/ 3200]\n",
      "loss: 0.766534  [   32/ 3200]\n",
      "loss: 0.784552  [   48/ 3200]\n",
      "loss: 0.730357  [   64/ 3200]\n",
      "loss: 0.897310  [   80/ 3200]\n",
      "loss: 0.775039  [   96/ 3200]\n",
      "loss: 0.719932  [  112/ 3200]\n",
      "loss: 0.757982  [  128/ 3200]\n",
      "loss: 0.861047  [  144/ 3200]\n",
      "loss: 0.763281  [  160/ 3200]\n",
      "loss: 0.828545  [  176/ 3200]\n",
      "loss: 0.549015  [  192/ 3200]\n",
      "loss: 1.070600  [  208/ 3200]\n",
      "loss: 1.188192  [  224/ 3200]\n",
      "loss: 0.911028  [  240/ 3200]\n",
      "loss: 0.731396  [  256/ 3200]\n",
      "loss: 1.147127  [  272/ 3200]\n",
      "loss: 0.938512  [  288/ 3200]\n",
      "loss: 1.555088  [  304/ 3200]\n",
      "loss: 0.943907  [  320/ 3200]\n",
      "loss: 0.922504  [  336/ 3200]\n",
      "loss: 0.795956  [  352/ 3200]\n",
      "loss: 0.596822  [  368/ 3200]\n",
      "loss: 0.784971  [  384/ 3200]\n",
      "loss: 0.873128  [  400/ 3200]\n",
      "loss: 0.710478  [  416/ 3200]\n",
      "loss: 0.967924  [  432/ 3200]\n",
      "loss: 0.691659  [  448/ 3200]\n",
      "loss: 0.816882  [  464/ 3200]\n",
      "loss: 0.595021  [  480/ 3200]\n",
      "loss: 0.785028  [  496/ 3200]\n",
      "loss: 0.730948  [  512/ 3200]\n",
      "loss: 0.885642  [  528/ 3200]\n",
      "loss: 0.853001  [  544/ 3200]\n",
      "loss: 0.605574  [  560/ 3200]\n",
      "loss: 0.867268  [  576/ 3200]\n",
      "loss: 0.593218  [  592/ 3200]\n",
      "loss: 0.632171  [  608/ 3200]\n",
      "loss: 0.962267  [  624/ 3200]\n",
      "loss: 0.796062  [  640/ 3200]\n",
      "loss: 0.799627  [  656/ 3200]\n",
      "loss: 0.502286  [  672/ 3200]\n",
      "loss: 1.111557  [  688/ 3200]\n",
      "loss: 1.042144  [  704/ 3200]\n",
      "loss: 0.844404  [  720/ 3200]\n",
      "loss: 1.372101  [  736/ 3200]\n",
      "loss: 1.117597  [  752/ 3200]\n",
      "loss: 0.747964  [  768/ 3200]\n",
      "loss: 0.937577  [  784/ 3200]\n",
      "loss: 0.987413  [  800/ 3200]\n",
      "loss: 0.912258  [  816/ 3200]\n",
      "loss: 0.743411  [  832/ 3200]\n",
      "loss: 0.717718  [  848/ 3200]\n",
      "loss: 0.715728  [  864/ 3200]\n",
      "loss: 0.662078  [  880/ 3200]\n",
      "loss: 0.817737  [  896/ 3200]\n",
      "loss: 1.138218  [  912/ 3200]\n",
      "loss: 1.099785  [  928/ 3200]\n",
      "loss: 0.808308  [  944/ 3200]\n",
      "loss: 0.999727  [  960/ 3200]\n",
      "loss: 0.855031  [  976/ 3200]\n",
      "loss: 0.703792  [  992/ 3200]\n",
      "loss: 1.090443  [ 1008/ 3200]\n",
      "loss: 0.474595  [ 1024/ 3200]\n",
      "loss: 0.921631  [ 1040/ 3200]\n",
      "loss: 0.942951  [ 1056/ 3200]\n",
      "loss: 0.668746  [ 1072/ 3200]\n",
      "loss: 0.693552  [ 1088/ 3200]\n",
      "loss: 0.793372  [ 1104/ 3200]\n",
      "loss: 0.737481  [ 1120/ 3200]\n",
      "loss: 0.813336  [ 1136/ 3200]\n",
      "loss: 0.801712  [ 1152/ 3200]\n",
      "loss: 0.923539  [ 1168/ 3200]\n",
      "loss: 0.718144  [ 1184/ 3200]\n",
      "loss: 1.171421  [ 1200/ 3200]\n",
      "loss: 0.668835  [ 1216/ 3200]\n",
      "loss: 0.801333  [ 1232/ 3200]\n",
      "loss: 0.911363  [ 1248/ 3200]\n",
      "loss: 0.746451  [ 1264/ 3200]\n",
      "loss: 0.793696  [ 1280/ 3200]\n",
      "loss: 0.764943  [ 1296/ 3200]\n",
      "loss: 0.597094  [ 1312/ 3200]\n",
      "loss: 0.820580  [ 1328/ 3200]\n",
      "loss: 0.999462  [ 1344/ 3200]\n",
      "loss: 0.574212  [ 1360/ 3200]\n",
      "loss: 0.790880  [ 1376/ 3200]\n",
      "loss: 0.871072  [ 1392/ 3200]\n",
      "loss: 0.787915  [ 1408/ 3200]\n",
      "loss: 0.792531  [ 1424/ 3200]\n",
      "loss: 0.968801  [ 1440/ 3200]\n",
      "loss: 0.656344  [ 1456/ 3200]\n",
      "loss: 0.707383  [ 1472/ 3200]\n",
      "loss: 0.899976  [ 1488/ 3200]\n",
      "loss: 1.131514  [ 1504/ 3200]\n",
      "loss: 0.731378  [ 1520/ 3200]\n",
      "loss: 0.819137  [ 1536/ 3200]\n",
      "loss: 0.667273  [ 1552/ 3200]\n",
      "loss: 0.998938  [ 1568/ 3200]\n",
      "loss: 0.591454  [ 1584/ 3200]\n",
      "loss: 1.140279  [ 1600/ 3200]\n",
      "loss: 0.723307  [ 1616/ 3200]\n",
      "loss: 1.072817  [ 1632/ 3200]\n",
      "loss: 0.950624  [ 1648/ 3200]\n",
      "loss: 0.694793  [ 1664/ 3200]\n",
      "loss: 0.765423  [ 1680/ 3200]\n",
      "loss: 0.780223  [ 1696/ 3200]\n",
      "loss: 0.994837  [ 1712/ 3200]\n",
      "loss: 1.257311  [ 1728/ 3200]\n",
      "loss: 0.822909  [ 1744/ 3200]\n",
      "loss: 0.871406  [ 1760/ 3200]\n",
      "loss: 0.931197  [ 1776/ 3200]\n",
      "loss: 0.518830  [ 1792/ 3200]\n",
      "loss: 0.912121  [ 1808/ 3200]\n",
      "loss: 0.755933  [ 1824/ 3200]\n",
      "loss: 0.589567  [ 1840/ 3200]\n",
      "loss: 0.567266  [ 1856/ 3200]\n",
      "loss: 0.556040  [ 1872/ 3200]\n",
      "loss: 0.828427  [ 1888/ 3200]\n",
      "loss: 0.822266  [ 1904/ 3200]\n",
      "loss: 0.742085  [ 1920/ 3200]\n",
      "loss: 1.038921  [ 1936/ 3200]\n",
      "loss: 1.242862  [ 1952/ 3200]\n",
      "loss: 0.648763  [ 1968/ 3200]\n",
      "loss: 0.927567  [ 1984/ 3200]\n",
      "loss: 0.754856  [ 2000/ 3200]\n",
      "loss: 0.933553  [ 2016/ 3200]\n",
      "loss: 0.644567  [ 2032/ 3200]\n",
      "loss: 0.791131  [ 2048/ 3200]\n",
      "loss: 0.747801  [ 2064/ 3200]\n",
      "loss: 0.855472  [ 2080/ 3200]\n",
      "loss: 0.723707  [ 2096/ 3200]\n",
      "loss: 0.653850  [ 2112/ 3200]\n",
      "loss: 0.873151  [ 2128/ 3200]\n",
      "loss: 0.617808  [ 2144/ 3200]\n",
      "loss: 0.652358  [ 2160/ 3200]\n",
      "loss: 0.922633  [ 2176/ 3200]\n",
      "loss: 0.877698  [ 2192/ 3200]\n",
      "loss: 0.584978  [ 2208/ 3200]\n",
      "loss: 0.544697  [ 2224/ 3200]\n",
      "loss: 0.553036  [ 2240/ 3200]\n",
      "loss: 0.805024  [ 2256/ 3200]\n",
      "loss: 0.736168  [ 2272/ 3200]\n",
      "loss: 0.872939  [ 2288/ 3200]\n",
      "loss: 0.877762  [ 2304/ 3200]\n",
      "loss: 0.726676  [ 2320/ 3200]\n",
      "loss: 0.820097  [ 2336/ 3200]\n",
      "loss: 0.712757  [ 2352/ 3200]\n",
      "loss: 1.171524  [ 2368/ 3200]\n",
      "loss: 0.785287  [ 2384/ 3200]\n",
      "loss: 0.545129  [ 2400/ 3200]\n",
      "loss: 0.741555  [ 2416/ 3200]\n",
      "loss: 0.549172  [ 2432/ 3200]\n",
      "loss: 0.751842  [ 2448/ 3200]\n",
      "loss: 0.759660  [ 2464/ 3200]\n",
      "loss: 0.564335  [ 2480/ 3200]\n",
      "loss: 0.862479  [ 2496/ 3200]\n",
      "loss: 0.919295  [ 2512/ 3200]\n",
      "loss: 1.162439  [ 2528/ 3200]\n",
      "loss: 0.806826  [ 2544/ 3200]\n",
      "loss: 1.077295  [ 2560/ 3200]\n",
      "loss: 0.591643  [ 2576/ 3200]\n",
      "loss: 0.711904  [ 2592/ 3200]\n",
      "loss: 1.390372  [ 2608/ 3200]\n",
      "loss: 0.741195  [ 2624/ 3200]\n",
      "loss: 0.788129  [ 2640/ 3200]\n",
      "loss: 1.035797  [ 2656/ 3200]\n",
      "loss: 0.581179  [ 2672/ 3200]\n",
      "loss: 0.694041  [ 2688/ 3200]\n",
      "loss: 0.768677  [ 2704/ 3200]\n",
      "loss: 0.541240  [ 2720/ 3200]\n",
      "loss: 0.813391  [ 2736/ 3200]\n",
      "loss: 0.901970  [ 2752/ 3200]\n",
      "loss: 0.916904  [ 2768/ 3200]\n",
      "loss: 0.752639  [ 2784/ 3200]\n",
      "loss: 0.686141  [ 2800/ 3200]\n",
      "loss: 0.950594  [ 2816/ 3200]\n",
      "loss: 0.936776  [ 2832/ 3200]\n",
      "loss: 1.050310  [ 2848/ 3200]\n",
      "loss: 0.930493  [ 2864/ 3200]\n",
      "loss: 0.814262  [ 2880/ 3200]\n",
      "loss: 0.978737  [ 2896/ 3200]\n",
      "loss: 1.136971  [ 2912/ 3200]\n",
      "loss: 0.615003  [ 2928/ 3200]\n",
      "loss: 0.776347  [ 2944/ 3200]\n",
      "loss: 0.837979  [ 2960/ 3200]\n",
      "loss: 1.027311  [ 2976/ 3200]\n",
      "loss: 1.018825  [ 2992/ 3200]\n",
      "loss: 0.662952  [ 3008/ 3200]\n",
      "loss: 0.710643  [ 3024/ 3200]\n",
      "loss: 0.969161  [ 3040/ 3200]\n",
      "loss: 0.673032  [ 3056/ 3200]\n",
      "loss: 0.958761  [ 3072/ 3200]\n",
      "loss: 0.954807  [ 3088/ 3200]\n",
      "loss: 0.880674  [ 3104/ 3200]\n",
      "loss: 0.577958  [ 3120/ 3200]\n",
      "loss: 0.682868  [ 3136/ 3200]\n",
      "loss: 0.660289  [ 3152/ 3200]\n",
      "loss: 0.680614  [ 3168/ 3200]\n",
      "loss: 0.981972  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.843554  [    0/ 3200]\n",
      "loss: 1.037579  [   16/ 3200]\n",
      "loss: 0.986393  [   32/ 3200]\n",
      "loss: 0.463653  [   48/ 3200]\n",
      "loss: 0.529333  [   64/ 3200]\n",
      "loss: 0.682251  [   80/ 3200]\n",
      "loss: 1.329566  [   96/ 3200]\n",
      "loss: 1.069201  [  112/ 3200]\n",
      "loss: 0.796864  [  128/ 3200]\n",
      "loss: 0.697434  [  144/ 3200]\n",
      "loss: 0.958154  [  160/ 3200]\n",
      "loss: 0.776142  [  176/ 3200]\n",
      "loss: 0.719804  [  192/ 3200]\n",
      "loss: 0.926043  [  208/ 3200]\n",
      "loss: 0.819348  [  224/ 3200]\n",
      "loss: 0.813062  [  240/ 3200]\n",
      "loss: 0.750179  [  256/ 3200]\n",
      "loss: 0.566849  [  272/ 3200]\n",
      "loss: 0.768717  [  288/ 3200]\n",
      "loss: 0.986762  [  304/ 3200]\n",
      "loss: 0.698143  [  320/ 3200]\n",
      "loss: 0.718045  [  336/ 3200]\n",
      "loss: 0.990604  [  352/ 3200]\n",
      "loss: 1.114313  [  368/ 3200]\n",
      "loss: 1.002079  [  384/ 3200]\n",
      "loss: 0.828335  [  400/ 3200]\n",
      "loss: 1.046176  [  416/ 3200]\n",
      "loss: 0.812007  [  432/ 3200]\n",
      "loss: 0.624679  [  448/ 3200]\n",
      "loss: 0.658617  [  464/ 3200]\n",
      "loss: 0.616956  [  480/ 3200]\n",
      "loss: 0.691580  [  496/ 3200]\n",
      "loss: 0.907777  [  512/ 3200]\n",
      "loss: 1.097945  [  528/ 3200]\n",
      "loss: 0.848419  [  544/ 3200]\n",
      "loss: 0.780954  [  560/ 3200]\n",
      "loss: 0.741372  [  576/ 3200]\n",
      "loss: 0.814325  [  592/ 3200]\n",
      "loss: 0.666275  [  608/ 3200]\n",
      "loss: 1.428719  [  624/ 3200]\n",
      "loss: 0.803567  [  640/ 3200]\n",
      "loss: 0.641018  [  656/ 3200]\n",
      "loss: 0.938907  [  672/ 3200]\n",
      "loss: 0.780653  [  688/ 3200]\n",
      "loss: 0.900754  [  704/ 3200]\n",
      "loss: 0.619979  [  720/ 3200]\n",
      "loss: 0.764255  [  736/ 3200]\n",
      "loss: 0.759301  [  752/ 3200]\n",
      "loss: 0.610812  [  768/ 3200]\n",
      "loss: 0.905054  [  784/ 3200]\n",
      "loss: 0.794493  [  800/ 3200]\n",
      "loss: 1.034432  [  816/ 3200]\n",
      "loss: 0.721405  [  832/ 3200]\n",
      "loss: 0.649055  [  848/ 3200]\n",
      "loss: 0.834475  [  864/ 3200]\n",
      "loss: 0.643270  [  880/ 3200]\n",
      "loss: 0.663758  [  896/ 3200]\n",
      "loss: 0.814940  [  912/ 3200]\n",
      "loss: 0.983293  [  928/ 3200]\n",
      "loss: 1.014050  [  944/ 3200]\n",
      "loss: 1.296353  [  960/ 3200]\n",
      "loss: 0.957653  [  976/ 3200]\n",
      "loss: 0.830049  [  992/ 3200]\n",
      "loss: 0.761654  [ 1008/ 3200]\n",
      "loss: 0.963453  [ 1024/ 3200]\n",
      "loss: 0.677265  [ 1040/ 3200]\n",
      "loss: 0.856376  [ 1056/ 3200]\n",
      "loss: 0.596801  [ 1072/ 3200]\n",
      "loss: 0.775649  [ 1088/ 3200]\n",
      "loss: 0.565866  [ 1104/ 3200]\n",
      "loss: 1.214895  [ 1120/ 3200]\n",
      "loss: 0.853056  [ 1136/ 3200]\n",
      "loss: 0.953440  [ 1152/ 3200]\n",
      "loss: 0.697511  [ 1168/ 3200]\n",
      "loss: 0.555905  [ 1184/ 3200]\n",
      "loss: 0.674097  [ 1200/ 3200]\n",
      "loss: 0.829044  [ 1216/ 3200]\n",
      "loss: 0.838041  [ 1232/ 3200]\n",
      "loss: 0.767971  [ 1248/ 3200]\n",
      "loss: 0.929709  [ 1264/ 3200]\n",
      "loss: 0.890365  [ 1280/ 3200]\n",
      "loss: 0.546708  [ 1296/ 3200]\n",
      "loss: 0.921254  [ 1312/ 3200]\n",
      "loss: 0.775322  [ 1328/ 3200]\n",
      "loss: 1.083439  [ 1344/ 3200]\n",
      "loss: 0.916610  [ 1360/ 3200]\n",
      "loss: 1.009194  [ 1376/ 3200]\n",
      "loss: 0.983701  [ 1392/ 3200]\n",
      "loss: 0.865043  [ 1408/ 3200]\n",
      "loss: 0.854714  [ 1424/ 3200]\n",
      "loss: 0.881370  [ 1440/ 3200]\n",
      "loss: 0.724407  [ 1456/ 3200]\n",
      "loss: 0.804894  [ 1472/ 3200]\n",
      "loss: 0.760540  [ 1488/ 3200]\n",
      "loss: 1.055086  [ 1504/ 3200]\n",
      "loss: 0.810598  [ 1520/ 3200]\n",
      "loss: 1.173149  [ 1536/ 3200]\n",
      "loss: 0.899833  [ 1552/ 3200]\n",
      "loss: 1.157246  [ 1568/ 3200]\n",
      "loss: 1.008239  [ 1584/ 3200]\n",
      "loss: 0.907283  [ 1600/ 3200]\n",
      "loss: 0.728044  [ 1616/ 3200]\n",
      "loss: 0.874044  [ 1632/ 3200]\n",
      "loss: 1.038673  [ 1648/ 3200]\n",
      "loss: 0.738473  [ 1664/ 3200]\n",
      "loss: 0.640302  [ 1680/ 3200]\n",
      "loss: 0.784043  [ 1696/ 3200]\n",
      "loss: 0.690521  [ 1712/ 3200]\n",
      "loss: 1.076381  [ 1728/ 3200]\n",
      "loss: 0.914720  [ 1744/ 3200]\n",
      "loss: 0.893746  [ 1760/ 3200]\n",
      "loss: 0.887546  [ 1776/ 3200]\n",
      "loss: 0.686700  [ 1792/ 3200]\n",
      "loss: 0.583465  [ 1808/ 3200]\n",
      "loss: 0.685709  [ 1824/ 3200]\n",
      "loss: 0.683071  [ 1840/ 3200]\n",
      "loss: 0.997917  [ 1856/ 3200]\n",
      "loss: 0.673113  [ 1872/ 3200]\n",
      "loss: 0.828887  [ 1888/ 3200]\n",
      "loss: 0.732765  [ 1904/ 3200]\n",
      "loss: 1.062954  [ 1920/ 3200]\n",
      "loss: 0.596462  [ 1936/ 3200]\n",
      "loss: 0.585332  [ 1952/ 3200]\n",
      "loss: 0.701120  [ 1968/ 3200]\n",
      "loss: 0.780303  [ 1984/ 3200]\n",
      "loss: 0.524914  [ 2000/ 3200]\n",
      "loss: 0.749513  [ 2016/ 3200]\n",
      "loss: 0.901129  [ 2032/ 3200]\n",
      "loss: 0.944286  [ 2048/ 3200]\n",
      "loss: 1.159138  [ 2064/ 3200]\n",
      "loss: 1.087754  [ 2080/ 3200]\n",
      "loss: 0.605754  [ 2096/ 3200]\n",
      "loss: 0.746967  [ 2112/ 3200]\n",
      "loss: 0.781752  [ 2128/ 3200]\n",
      "loss: 0.993866  [ 2144/ 3200]\n",
      "loss: 0.500672  [ 2160/ 3200]\n",
      "loss: 0.810481  [ 2176/ 3200]\n",
      "loss: 1.003345  [ 2192/ 3200]\n",
      "loss: 0.599730  [ 2208/ 3200]\n",
      "loss: 0.865862  [ 2224/ 3200]\n",
      "loss: 0.316907  [ 2240/ 3200]\n",
      "loss: 1.020754  [ 2256/ 3200]\n",
      "loss: 0.848064  [ 2272/ 3200]\n",
      "loss: 0.963547  [ 2288/ 3200]\n",
      "loss: 0.662311  [ 2304/ 3200]\n",
      "loss: 1.294647  [ 2320/ 3200]\n",
      "loss: 0.876822  [ 2336/ 3200]\n",
      "loss: 0.794849  [ 2352/ 3200]\n",
      "loss: 0.611633  [ 2368/ 3200]\n",
      "loss: 0.895097  [ 2384/ 3200]\n",
      "loss: 0.803457  [ 2400/ 3200]\n",
      "loss: 0.751958  [ 2416/ 3200]\n",
      "loss: 0.899345  [ 2432/ 3200]\n",
      "loss: 0.833319  [ 2448/ 3200]\n",
      "loss: 0.717224  [ 2464/ 3200]\n",
      "loss: 0.717008  [ 2480/ 3200]\n",
      "loss: 0.601317  [ 2496/ 3200]\n",
      "loss: 0.436738  [ 2512/ 3200]\n",
      "loss: 0.762626  [ 2528/ 3200]\n",
      "loss: 0.733233  [ 2544/ 3200]\n",
      "loss: 0.929576  [ 2560/ 3200]\n",
      "loss: 0.852800  [ 2576/ 3200]\n",
      "loss: 0.809779  [ 2592/ 3200]\n",
      "loss: 0.930491  [ 2608/ 3200]\n",
      "loss: 0.800045  [ 2624/ 3200]\n",
      "loss: 0.932690  [ 2640/ 3200]\n",
      "loss: 0.709023  [ 2656/ 3200]\n",
      "loss: 0.892609  [ 2672/ 3200]\n",
      "loss: 0.881316  [ 2688/ 3200]\n",
      "loss: 0.734881  [ 2704/ 3200]\n",
      "loss: 1.094719  [ 2720/ 3200]\n",
      "loss: 0.698318  [ 2736/ 3200]\n",
      "loss: 0.952498  [ 2752/ 3200]\n",
      "loss: 0.761587  [ 2768/ 3200]\n",
      "loss: 0.625536  [ 2784/ 3200]\n",
      "loss: 0.903290  [ 2800/ 3200]\n",
      "loss: 0.576586  [ 2816/ 3200]\n",
      "loss: 0.748536  [ 2832/ 3200]\n",
      "loss: 0.736629  [ 2848/ 3200]\n",
      "loss: 0.687919  [ 2864/ 3200]\n",
      "loss: 0.562210  [ 2880/ 3200]\n",
      "loss: 0.944255  [ 2896/ 3200]\n",
      "loss: 0.610399  [ 2912/ 3200]\n",
      "loss: 0.736407  [ 2928/ 3200]\n",
      "loss: 0.508708  [ 2944/ 3200]\n",
      "loss: 0.626270  [ 2960/ 3200]\n",
      "loss: 0.633724  [ 2976/ 3200]\n",
      "loss: 0.993533  [ 2992/ 3200]\n",
      "loss: 1.098012  [ 3008/ 3200]\n",
      "loss: 0.822156  [ 3024/ 3200]\n",
      "loss: 0.835750  [ 3040/ 3200]\n",
      "loss: 0.720700  [ 3056/ 3200]\n",
      "loss: 0.848463  [ 3072/ 3200]\n",
      "loss: 1.052199  [ 3088/ 3200]\n",
      "loss: 0.673490  [ 3104/ 3200]\n",
      "loss: 0.700498  [ 3120/ 3200]\n",
      "loss: 0.469498  [ 3136/ 3200]\n",
      "loss: 1.054531  [ 3152/ 3200]\n",
      "loss: 0.600116  [ 3168/ 3200]\n",
      "loss: 0.860722  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.737353  [    0/ 3200]\n",
      "loss: 0.748704  [   16/ 3200]\n",
      "loss: 0.611370  [   32/ 3200]\n",
      "loss: 0.910861  [   48/ 3200]\n",
      "loss: 0.559851  [   64/ 3200]\n",
      "loss: 0.837338  [   80/ 3200]\n",
      "loss: 0.852150  [   96/ 3200]\n",
      "loss: 0.550080  [  112/ 3200]\n",
      "loss: 0.897068  [  128/ 3200]\n",
      "loss: 0.859363  [  144/ 3200]\n",
      "loss: 0.608581  [  160/ 3200]\n",
      "loss: 0.910031  [  176/ 3200]\n",
      "loss: 0.707564  [  192/ 3200]\n",
      "loss: 0.898004  [  208/ 3200]\n",
      "loss: 0.879165  [  224/ 3200]\n",
      "loss: 0.650480  [  240/ 3200]\n",
      "loss: 0.812535  [  256/ 3200]\n",
      "loss: 1.156782  [  272/ 3200]\n",
      "loss: 0.558926  [  288/ 3200]\n",
      "loss: 0.877828  [  304/ 3200]\n",
      "loss: 0.781362  [  320/ 3200]\n",
      "loss: 1.429525  [  336/ 3200]\n",
      "loss: 0.864919  [  352/ 3200]\n",
      "loss: 0.863517  [  368/ 3200]\n",
      "loss: 0.748813  [  384/ 3200]\n",
      "loss: 0.844147  [  400/ 3200]\n",
      "loss: 0.566323  [  416/ 3200]\n",
      "loss: 0.849111  [  432/ 3200]\n",
      "loss: 0.574018  [  448/ 3200]\n",
      "loss: 0.902485  [  464/ 3200]\n",
      "loss: 0.576325  [  480/ 3200]\n",
      "loss: 0.604859  [  496/ 3200]\n",
      "loss: 0.841824  [  512/ 3200]\n",
      "loss: 0.644336  [  528/ 3200]\n",
      "loss: 0.696790  [  544/ 3200]\n",
      "loss: 1.204820  [  560/ 3200]\n",
      "loss: 0.602640  [  576/ 3200]\n",
      "loss: 0.782728  [  592/ 3200]\n",
      "loss: 0.640193  [  608/ 3200]\n",
      "loss: 0.642266  [  624/ 3200]\n",
      "loss: 0.615765  [  640/ 3200]\n",
      "loss: 0.928377  [  656/ 3200]\n",
      "loss: 0.615143  [  672/ 3200]\n",
      "loss: 0.458589  [  688/ 3200]\n",
      "loss: 0.875219  [  704/ 3200]\n",
      "loss: 0.800888  [  720/ 3200]\n",
      "loss: 0.745402  [  736/ 3200]\n",
      "loss: 0.698455  [  752/ 3200]\n",
      "loss: 0.464336  [  768/ 3200]\n",
      "loss: 0.900342  [  784/ 3200]\n",
      "loss: 0.929787  [  800/ 3200]\n",
      "loss: 0.673564  [  816/ 3200]\n",
      "loss: 0.756470  [  832/ 3200]\n",
      "loss: 0.746040  [  848/ 3200]\n",
      "loss: 0.604183  [  864/ 3200]\n",
      "loss: 0.581974  [  880/ 3200]\n",
      "loss: 0.680033  [  896/ 3200]\n",
      "loss: 0.621879  [  912/ 3200]\n",
      "loss: 0.516907  [  928/ 3200]\n",
      "loss: 0.694164  [  944/ 3200]\n",
      "loss: 0.804686  [  960/ 3200]\n",
      "loss: 0.894981  [  976/ 3200]\n",
      "loss: 0.941235  [  992/ 3200]\n",
      "loss: 1.217879  [ 1008/ 3200]\n",
      "loss: 1.067768  [ 1024/ 3200]\n",
      "loss: 0.442182  [ 1040/ 3200]\n",
      "loss: 0.646539  [ 1056/ 3200]\n",
      "loss: 0.811398  [ 1072/ 3200]\n",
      "loss: 0.830862  [ 1088/ 3200]\n",
      "loss: 0.772411  [ 1104/ 3200]\n",
      "loss: 0.696673  [ 1120/ 3200]\n",
      "loss: 0.715471  [ 1136/ 3200]\n",
      "loss: 0.862301  [ 1152/ 3200]\n",
      "loss: 0.987395  [ 1168/ 3200]\n",
      "loss: 1.063138  [ 1184/ 3200]\n",
      "loss: 1.028800  [ 1200/ 3200]\n",
      "loss: 0.734091  [ 1216/ 3200]\n",
      "loss: 0.674001  [ 1232/ 3200]\n",
      "loss: 0.974755  [ 1248/ 3200]\n",
      "loss: 1.483827  [ 1264/ 3200]\n",
      "loss: 0.671040  [ 1280/ 3200]\n",
      "loss: 0.913879  [ 1296/ 3200]\n",
      "loss: 0.834610  [ 1312/ 3200]\n",
      "loss: 1.221025  [ 1328/ 3200]\n",
      "loss: 0.720072  [ 1344/ 3200]\n",
      "loss: 1.019672  [ 1360/ 3200]\n",
      "loss: 0.783308  [ 1376/ 3200]\n",
      "loss: 1.037043  [ 1392/ 3200]\n",
      "loss: 0.786558  [ 1408/ 3200]\n",
      "loss: 0.915940  [ 1424/ 3200]\n",
      "loss: 0.945285  [ 1440/ 3200]\n",
      "loss: 0.938479  [ 1456/ 3200]\n",
      "loss: 0.747123  [ 1472/ 3200]\n",
      "loss: 0.812728  [ 1488/ 3200]\n",
      "loss: 0.784444  [ 1504/ 3200]\n",
      "loss: 0.968153  [ 1520/ 3200]\n",
      "loss: 0.727479  [ 1536/ 3200]\n",
      "loss: 0.624762  [ 1552/ 3200]\n",
      "loss: 1.054860  [ 1568/ 3200]\n",
      "loss: 1.037418  [ 1584/ 3200]\n",
      "loss: 0.694607  [ 1600/ 3200]\n",
      "loss: 0.815625  [ 1616/ 3200]\n",
      "loss: 0.705537  [ 1632/ 3200]\n",
      "loss: 1.020728  [ 1648/ 3200]\n",
      "loss: 1.015023  [ 1664/ 3200]\n",
      "loss: 0.849831  [ 1680/ 3200]\n",
      "loss: 0.619469  [ 1696/ 3200]\n",
      "loss: 0.670215  [ 1712/ 3200]\n",
      "loss: 0.763859  [ 1728/ 3200]\n",
      "loss: 0.888214  [ 1744/ 3200]\n",
      "loss: 0.596035  [ 1760/ 3200]\n",
      "loss: 0.640965  [ 1776/ 3200]\n",
      "loss: 1.290243  [ 1792/ 3200]\n",
      "loss: 0.576918  [ 1808/ 3200]\n",
      "loss: 0.718347  [ 1824/ 3200]\n",
      "loss: 0.681517  [ 1840/ 3200]\n",
      "loss: 0.799524  [ 1856/ 3200]\n",
      "loss: 0.815152  [ 1872/ 3200]\n",
      "loss: 1.298244  [ 1888/ 3200]\n",
      "loss: 0.796352  [ 1904/ 3200]\n",
      "loss: 0.919182  [ 1920/ 3200]\n",
      "loss: 1.179045  [ 1936/ 3200]\n",
      "loss: 0.726113  [ 1952/ 3200]\n",
      "loss: 0.795676  [ 1968/ 3200]\n",
      "loss: 0.696639  [ 1984/ 3200]\n",
      "loss: 0.843794  [ 2000/ 3200]\n",
      "loss: 0.635176  [ 2016/ 3200]\n",
      "loss: 1.001115  [ 2032/ 3200]\n",
      "loss: 0.822977  [ 2048/ 3200]\n",
      "loss: 0.752841  [ 2064/ 3200]\n",
      "loss: 0.846594  [ 2080/ 3200]\n",
      "loss: 0.815524  [ 2096/ 3200]\n",
      "loss: 0.958989  [ 2112/ 3200]\n",
      "loss: 0.900912  [ 2128/ 3200]\n",
      "loss: 0.844798  [ 2144/ 3200]\n",
      "loss: 0.897638  [ 2160/ 3200]\n",
      "loss: 0.849259  [ 2176/ 3200]\n",
      "loss: 0.741360  [ 2192/ 3200]\n",
      "loss: 0.901942  [ 2208/ 3200]\n",
      "loss: 0.746124  [ 2224/ 3200]\n",
      "loss: 0.865471  [ 2240/ 3200]\n",
      "loss: 0.714567  [ 2256/ 3200]\n",
      "loss: 0.800307  [ 2272/ 3200]\n",
      "loss: 1.054619  [ 2288/ 3200]\n",
      "loss: 1.007454  [ 2304/ 3200]\n",
      "loss: 0.941753  [ 2320/ 3200]\n",
      "loss: 0.932255  [ 2336/ 3200]\n",
      "loss: 0.895936  [ 2352/ 3200]\n",
      "loss: 0.655247  [ 2368/ 3200]\n",
      "loss: 0.875022  [ 2384/ 3200]\n",
      "loss: 0.411892  [ 2400/ 3200]\n",
      "loss: 0.839669  [ 2416/ 3200]\n",
      "loss: 1.226842  [ 2432/ 3200]\n",
      "loss: 0.430023  [ 2448/ 3200]\n",
      "loss: 0.746419  [ 2464/ 3200]\n",
      "loss: 1.095845  [ 2480/ 3200]\n",
      "loss: 0.809616  [ 2496/ 3200]\n",
      "loss: 1.006552  [ 2512/ 3200]\n",
      "loss: 0.766265  [ 2528/ 3200]\n",
      "loss: 0.654690  [ 2544/ 3200]\n",
      "loss: 0.855386  [ 2560/ 3200]\n",
      "loss: 0.539756  [ 2576/ 3200]\n",
      "loss: 0.835964  [ 2592/ 3200]\n",
      "loss: 0.513648  [ 2608/ 3200]\n",
      "loss: 0.848269  [ 2624/ 3200]\n",
      "loss: 0.701887  [ 2640/ 3200]\n",
      "loss: 0.986413  [ 2656/ 3200]\n",
      "loss: 0.695799  [ 2672/ 3200]\n",
      "loss: 0.656582  [ 2688/ 3200]\n",
      "loss: 0.935782  [ 2704/ 3200]\n",
      "loss: 0.930663  [ 2720/ 3200]\n",
      "loss: 0.618806  [ 2736/ 3200]\n",
      "loss: 0.667116  [ 2752/ 3200]\n",
      "loss: 0.916463  [ 2768/ 3200]\n",
      "loss: 0.997510  [ 2784/ 3200]\n",
      "loss: 0.664806  [ 2800/ 3200]\n",
      "loss: 0.896548  [ 2816/ 3200]\n",
      "loss: 0.961670  [ 2832/ 3200]\n",
      "loss: 1.062741  [ 2848/ 3200]\n",
      "loss: 0.637067  [ 2864/ 3200]\n",
      "loss: 0.559126  [ 2880/ 3200]\n",
      "loss: 0.834779  [ 2896/ 3200]\n",
      "loss: 0.745335  [ 2912/ 3200]\n",
      "loss: 0.588800  [ 2928/ 3200]\n",
      "loss: 0.496879  [ 2944/ 3200]\n",
      "loss: 1.139498  [ 2960/ 3200]\n",
      "loss: 0.666497  [ 2976/ 3200]\n",
      "loss: 0.929029  [ 2992/ 3200]\n",
      "loss: 0.643123  [ 3008/ 3200]\n",
      "loss: 0.611842  [ 3024/ 3200]\n",
      "loss: 0.622505  [ 3040/ 3200]\n",
      "loss: 0.841708  [ 3056/ 3200]\n",
      "loss: 1.096911  [ 3072/ 3200]\n",
      "loss: 0.892215  [ 3088/ 3200]\n",
      "loss: 0.529416  [ 3104/ 3200]\n",
      "loss: 0.899903  [ 3120/ 3200]\n",
      "loss: 0.662836  [ 3136/ 3200]\n",
      "loss: 0.962571  [ 3152/ 3200]\n",
      "loss: 0.691131  [ 3168/ 3200]\n",
      "loss: 0.454129  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.991678  [    0/ 3200]\n",
      "loss: 0.960405  [   16/ 3200]\n",
      "loss: 0.456665  [   32/ 3200]\n",
      "loss: 0.916391  [   48/ 3200]\n",
      "loss: 0.810781  [   64/ 3200]\n",
      "loss: 0.712480  [   80/ 3200]\n",
      "loss: 0.821519  [   96/ 3200]\n",
      "loss: 0.826177  [  112/ 3200]\n",
      "loss: 0.494103  [  128/ 3200]\n",
      "loss: 0.793508  [  144/ 3200]\n",
      "loss: 0.553268  [  160/ 3200]\n",
      "loss: 0.946235  [  176/ 3200]\n",
      "loss: 0.507240  [  192/ 3200]\n",
      "loss: 0.821151  [  208/ 3200]\n",
      "loss: 0.834704  [  224/ 3200]\n",
      "loss: 0.623418  [  240/ 3200]\n",
      "loss: 0.570755  [  256/ 3200]\n",
      "loss: 1.028900  [  272/ 3200]\n",
      "loss: 0.615419  [  288/ 3200]\n",
      "loss: 0.923939  [  304/ 3200]\n",
      "loss: 0.744482  [  320/ 3200]\n",
      "loss: 0.742161  [  336/ 3200]\n",
      "loss: 0.821436  [  352/ 3200]\n",
      "loss: 0.613884  [  368/ 3200]\n",
      "loss: 0.752273  [  384/ 3200]\n",
      "loss: 1.287967  [  400/ 3200]\n",
      "loss: 1.106811  [  416/ 3200]\n",
      "loss: 0.788904  [  432/ 3200]\n",
      "loss: 0.547885  [  448/ 3200]\n",
      "loss: 0.845271  [  464/ 3200]\n",
      "loss: 0.839989  [  480/ 3200]\n",
      "loss: 0.753187  [  496/ 3200]\n",
      "loss: 0.909342  [  512/ 3200]\n",
      "loss: 1.017930  [  528/ 3200]\n",
      "loss: 0.939933  [  544/ 3200]\n",
      "loss: 0.752611  [  560/ 3200]\n",
      "loss: 0.921557  [  576/ 3200]\n",
      "loss: 0.574238  [  592/ 3200]\n",
      "loss: 0.509755  [  608/ 3200]\n",
      "loss: 0.760122  [  624/ 3200]\n",
      "loss: 0.939557  [  640/ 3200]\n",
      "loss: 0.767088  [  656/ 3200]\n",
      "loss: 0.696814  [  672/ 3200]\n",
      "loss: 0.687930  [  688/ 3200]\n",
      "loss: 0.921942  [  704/ 3200]\n",
      "loss: 0.547199  [  720/ 3200]\n",
      "loss: 0.603856  [  736/ 3200]\n",
      "loss: 0.830446  [  752/ 3200]\n",
      "loss: 0.472659  [  768/ 3200]\n",
      "loss: 0.775518  [  784/ 3200]\n",
      "loss: 0.508898  [  800/ 3200]\n",
      "loss: 0.540307  [  816/ 3200]\n",
      "loss: 0.766429  [  832/ 3200]\n",
      "loss: 0.754817  [  848/ 3200]\n",
      "loss: 0.632135  [  864/ 3200]\n",
      "loss: 0.747656  [  880/ 3200]\n",
      "loss: 0.734397  [  896/ 3200]\n",
      "loss: 0.889007  [  912/ 3200]\n",
      "loss: 0.711709  [  928/ 3200]\n",
      "loss: 0.817646  [  944/ 3200]\n",
      "loss: 1.251261  [  960/ 3200]\n",
      "loss: 0.627848  [  976/ 3200]\n",
      "loss: 0.561128  [  992/ 3200]\n",
      "loss: 0.857607  [ 1008/ 3200]\n",
      "loss: 0.796910  [ 1024/ 3200]\n",
      "loss: 0.673533  [ 1040/ 3200]\n",
      "loss: 0.537882  [ 1056/ 3200]\n",
      "loss: 0.599704  [ 1072/ 3200]\n",
      "loss: 0.784993  [ 1088/ 3200]\n",
      "loss: 0.735394  [ 1104/ 3200]\n",
      "loss: 0.608783  [ 1120/ 3200]\n",
      "loss: 0.606707  [ 1136/ 3200]\n",
      "loss: 0.573135  [ 1152/ 3200]\n",
      "loss: 0.968821  [ 1168/ 3200]\n",
      "loss: 0.762605  [ 1184/ 3200]\n",
      "loss: 0.817305  [ 1200/ 3200]\n",
      "loss: 1.102197  [ 1216/ 3200]\n",
      "loss: 0.968665  [ 1232/ 3200]\n",
      "loss: 0.731014  [ 1248/ 3200]\n",
      "loss: 0.971622  [ 1264/ 3200]\n",
      "loss: 0.800714  [ 1280/ 3200]\n",
      "loss: 0.835557  [ 1296/ 3200]\n",
      "loss: 0.861336  [ 1312/ 3200]\n",
      "loss: 0.868882  [ 1328/ 3200]\n",
      "loss: 0.565317  [ 1344/ 3200]\n",
      "loss: 0.742051  [ 1360/ 3200]\n",
      "loss: 0.633479  [ 1376/ 3200]\n",
      "loss: 1.177714  [ 1392/ 3200]\n",
      "loss: 0.826733  [ 1408/ 3200]\n",
      "loss: 0.924313  [ 1424/ 3200]\n",
      "loss: 0.710416  [ 1440/ 3200]\n",
      "loss: 0.990003  [ 1456/ 3200]\n",
      "loss: 0.722129  [ 1472/ 3200]\n",
      "loss: 0.716449  [ 1488/ 3200]\n",
      "loss: 0.650860  [ 1504/ 3200]\n",
      "loss: 0.501249  [ 1520/ 3200]\n",
      "loss: 0.839348  [ 1536/ 3200]\n",
      "loss: 0.861268  [ 1552/ 3200]\n",
      "loss: 0.554059  [ 1568/ 3200]\n",
      "loss: 0.472473  [ 1584/ 3200]\n",
      "loss: 0.818984  [ 1600/ 3200]\n",
      "loss: 0.967979  [ 1616/ 3200]\n",
      "loss: 0.875486  [ 1632/ 3200]\n",
      "loss: 0.827344  [ 1648/ 3200]\n",
      "loss: 0.781443  [ 1664/ 3200]\n",
      "loss: 0.605145  [ 1680/ 3200]\n",
      "loss: 0.567985  [ 1696/ 3200]\n",
      "loss: 0.578969  [ 1712/ 3200]\n",
      "loss: 1.066603  [ 1728/ 3200]\n",
      "loss: 0.676990  [ 1744/ 3200]\n",
      "loss: 0.909363  [ 1760/ 3200]\n",
      "loss: 0.929810  [ 1776/ 3200]\n",
      "loss: 0.753838  [ 1792/ 3200]\n",
      "loss: 0.683640  [ 1808/ 3200]\n",
      "loss: 0.649913  [ 1824/ 3200]\n",
      "loss: 0.725360  [ 1840/ 3200]\n",
      "loss: 1.013080  [ 1856/ 3200]\n",
      "loss: 0.572504  [ 1872/ 3200]\n",
      "loss: 0.880353  [ 1888/ 3200]\n",
      "loss: 1.039831  [ 1904/ 3200]\n",
      "loss: 0.569252  [ 1920/ 3200]\n",
      "loss: 0.865221  [ 1936/ 3200]\n",
      "loss: 0.628274  [ 1952/ 3200]\n",
      "loss: 0.681094  [ 1968/ 3200]\n",
      "loss: 1.024453  [ 1984/ 3200]\n",
      "loss: 0.537933  [ 2000/ 3200]\n",
      "loss: 0.544012  [ 2016/ 3200]\n",
      "loss: 0.472056  [ 2032/ 3200]\n",
      "loss: 0.972614  [ 2048/ 3200]\n",
      "loss: 0.820429  [ 2064/ 3200]\n",
      "loss: 0.772321  [ 2080/ 3200]\n",
      "loss: 0.538200  [ 2096/ 3200]\n",
      "loss: 0.969055  [ 2112/ 3200]\n",
      "loss: 0.564242  [ 2128/ 3200]\n",
      "loss: 0.695227  [ 2144/ 3200]\n",
      "loss: 0.898315  [ 2160/ 3200]\n",
      "loss: 0.968453  [ 2176/ 3200]\n",
      "loss: 0.673061  [ 2192/ 3200]\n",
      "loss: 0.670496  [ 2208/ 3200]\n",
      "loss: 0.736044  [ 2224/ 3200]\n",
      "loss: 0.877190  [ 2240/ 3200]\n",
      "loss: 0.710564  [ 2256/ 3200]\n",
      "loss: 0.643239  [ 2272/ 3200]\n",
      "loss: 0.652387  [ 2288/ 3200]\n",
      "loss: 1.094049  [ 2304/ 3200]\n",
      "loss: 1.280700  [ 2320/ 3200]\n",
      "loss: 1.090191  [ 2336/ 3200]\n",
      "loss: 0.906758  [ 2352/ 3200]\n",
      "loss: 0.979560  [ 2368/ 3200]\n",
      "loss: 0.743431  [ 2384/ 3200]\n",
      "loss: 0.912134  [ 2400/ 3200]\n",
      "loss: 0.488592  [ 2416/ 3200]\n",
      "loss: 0.717057  [ 2432/ 3200]\n",
      "loss: 0.763304  [ 2448/ 3200]\n",
      "loss: 0.410386  [ 2464/ 3200]\n",
      "loss: 0.784511  [ 2480/ 3200]\n",
      "loss: 0.729934  [ 2496/ 3200]\n",
      "loss: 0.875929  [ 2512/ 3200]\n",
      "loss: 1.049208  [ 2528/ 3200]\n",
      "loss: 0.895466  [ 2544/ 3200]\n",
      "loss: 0.774030  [ 2560/ 3200]\n",
      "loss: 0.503298  [ 2576/ 3200]\n",
      "loss: 0.885076  [ 2592/ 3200]\n",
      "loss: 0.971508  [ 2608/ 3200]\n",
      "loss: 0.843816  [ 2624/ 3200]\n",
      "loss: 0.832328  [ 2640/ 3200]\n",
      "loss: 0.952963  [ 2656/ 3200]\n",
      "loss: 0.803530  [ 2672/ 3200]\n",
      "loss: 0.953113  [ 2688/ 3200]\n",
      "loss: 0.968122  [ 2704/ 3200]\n",
      "loss: 0.750587  [ 2720/ 3200]\n",
      "loss: 0.980798  [ 2736/ 3200]\n",
      "loss: 0.643371  [ 2752/ 3200]\n",
      "loss: 1.312354  [ 2768/ 3200]\n",
      "loss: 0.852813  [ 2784/ 3200]\n",
      "loss: 0.787011  [ 2800/ 3200]\n",
      "loss: 0.806813  [ 2816/ 3200]\n",
      "loss: 0.831578  [ 2832/ 3200]\n",
      "loss: 0.988883  [ 2848/ 3200]\n",
      "loss: 0.735099  [ 2864/ 3200]\n",
      "loss: 0.992327  [ 2880/ 3200]\n",
      "loss: 0.984834  [ 2896/ 3200]\n",
      "loss: 0.878720  [ 2912/ 3200]\n",
      "loss: 0.793356  [ 2928/ 3200]\n",
      "loss: 0.766483  [ 2944/ 3200]\n",
      "loss: 0.677590  [ 2960/ 3200]\n",
      "loss: 0.562933  [ 2976/ 3200]\n",
      "loss: 0.659008  [ 2992/ 3200]\n",
      "loss: 0.865002  [ 3008/ 3200]\n",
      "loss: 0.723521  [ 3024/ 3200]\n",
      "loss: 1.003942  [ 3040/ 3200]\n",
      "loss: 1.049594  [ 3056/ 3200]\n",
      "loss: 0.597392  [ 3072/ 3200]\n",
      "loss: 0.679809  [ 3088/ 3200]\n",
      "loss: 0.712549  [ 3104/ 3200]\n",
      "loss: 0.606114  [ 3120/ 3200]\n",
      "loss: 0.922808  [ 3136/ 3200]\n",
      "loss: 1.092624  [ 3152/ 3200]\n",
      "loss: 0.656330  [ 3168/ 3200]\n",
      "loss: 0.787926  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 0.658331  [    0/ 3200]\n",
      "loss: 0.476720  [   16/ 3200]\n",
      "loss: 0.905747  [   32/ 3200]\n",
      "loss: 0.501412  [   48/ 3200]\n",
      "loss: 0.882193  [   64/ 3200]\n",
      "loss: 0.779577  [   80/ 3200]\n",
      "loss: 0.570478  [   96/ 3200]\n",
      "loss: 0.345907  [  112/ 3200]\n",
      "loss: 0.904272  [  128/ 3200]\n",
      "loss: 0.567158  [  144/ 3200]\n",
      "loss: 0.830217  [  160/ 3200]\n",
      "loss: 0.841393  [  176/ 3200]\n",
      "loss: 0.339155  [  192/ 3200]\n",
      "loss: 1.046481  [  208/ 3200]\n",
      "loss: 0.673453  [  224/ 3200]\n",
      "loss: 0.676373  [  240/ 3200]\n",
      "loss: 1.113676  [  256/ 3200]\n",
      "loss: 0.831876  [  272/ 3200]\n",
      "loss: 0.505220  [  288/ 3200]\n",
      "loss: 0.777761  [  304/ 3200]\n",
      "loss: 1.332813  [  320/ 3200]\n",
      "loss: 0.903244  [  336/ 3200]\n",
      "loss: 0.691869  [  352/ 3200]\n",
      "loss: 0.757515  [  368/ 3200]\n",
      "loss: 1.132739  [  384/ 3200]\n",
      "loss: 0.792536  [  400/ 3200]\n",
      "loss: 1.117399  [  416/ 3200]\n",
      "loss: 0.759492  [  432/ 3200]\n",
      "loss: 0.688216  [  448/ 3200]\n",
      "loss: 0.846918  [  464/ 3200]\n",
      "loss: 1.035609  [  480/ 3200]\n",
      "loss: 0.711820  [  496/ 3200]\n",
      "loss: 0.813578  [  512/ 3200]\n",
      "loss: 0.448362  [  528/ 3200]\n",
      "loss: 0.773124  [  544/ 3200]\n",
      "loss: 0.470148  [  560/ 3200]\n",
      "loss: 0.592647  [  576/ 3200]\n",
      "loss: 0.857081  [  592/ 3200]\n",
      "loss: 1.006160  [  608/ 3200]\n",
      "loss: 0.951316  [  624/ 3200]\n",
      "loss: 0.591953  [  640/ 3200]\n",
      "loss: 0.551659  [  656/ 3200]\n",
      "loss: 0.731814  [  672/ 3200]\n",
      "loss: 0.907010  [  688/ 3200]\n",
      "loss: 0.815367  [  704/ 3200]\n",
      "loss: 0.671260  [  720/ 3200]\n",
      "loss: 0.654798  [  736/ 3200]\n",
      "loss: 0.527660  [  752/ 3200]\n",
      "loss: 0.804896  [  768/ 3200]\n",
      "loss: 0.687704  [  784/ 3200]\n",
      "loss: 0.728160  [  800/ 3200]\n",
      "loss: 0.534422  [  816/ 3200]\n",
      "loss: 0.555988  [  832/ 3200]\n",
      "loss: 1.131394  [  848/ 3200]\n",
      "loss: 0.647855  [  864/ 3200]\n",
      "loss: 0.593082  [  880/ 3200]\n",
      "loss: 0.804190  [  896/ 3200]\n",
      "loss: 1.187345  [  912/ 3200]\n",
      "loss: 0.837381  [  928/ 3200]\n",
      "loss: 0.945254  [  944/ 3200]\n",
      "loss: 1.042998  [  960/ 3200]\n",
      "loss: 0.708508  [  976/ 3200]\n",
      "loss: 0.696732  [  992/ 3200]\n",
      "loss: 0.673894  [ 1008/ 3200]\n",
      "loss: 0.801418  [ 1024/ 3200]\n",
      "loss: 1.011502  [ 1040/ 3200]\n",
      "loss: 0.765707  [ 1056/ 3200]\n",
      "loss: 0.602488  [ 1072/ 3200]\n",
      "loss: 0.871334  [ 1088/ 3200]\n",
      "loss: 1.092067  [ 1104/ 3200]\n",
      "loss: 0.689643  [ 1120/ 3200]\n",
      "loss: 0.530373  [ 1136/ 3200]\n",
      "loss: 0.876094  [ 1152/ 3200]\n",
      "loss: 0.879321  [ 1168/ 3200]\n",
      "loss: 0.604130  [ 1184/ 3200]\n",
      "loss: 0.801277  [ 1200/ 3200]\n",
      "loss: 0.841670  [ 1216/ 3200]\n",
      "loss: 0.934551  [ 1232/ 3200]\n",
      "loss: 0.606377  [ 1248/ 3200]\n",
      "loss: 0.951592  [ 1264/ 3200]\n",
      "loss: 0.552310  [ 1280/ 3200]\n",
      "loss: 0.858889  [ 1296/ 3200]\n",
      "loss: 0.842353  [ 1312/ 3200]\n",
      "loss: 0.791729  [ 1328/ 3200]\n",
      "loss: 0.675282  [ 1344/ 3200]\n",
      "loss: 0.460052  [ 1360/ 3200]\n",
      "loss: 0.540406  [ 1376/ 3200]\n",
      "loss: 0.401595  [ 1392/ 3200]\n",
      "loss: 0.728686  [ 1408/ 3200]\n",
      "loss: 0.614853  [ 1424/ 3200]\n",
      "loss: 0.517843  [ 1440/ 3200]\n",
      "loss: 0.638987  [ 1456/ 3200]\n",
      "loss: 1.417255  [ 1472/ 3200]\n",
      "loss: 1.012551  [ 1488/ 3200]\n",
      "loss: 0.769691  [ 1504/ 3200]\n",
      "loss: 0.653893  [ 1520/ 3200]\n",
      "loss: 0.786290  [ 1536/ 3200]\n",
      "loss: 0.954064  [ 1552/ 3200]\n",
      "loss: 0.584507  [ 1568/ 3200]\n",
      "loss: 1.047159  [ 1584/ 3200]\n",
      "loss: 0.613344  [ 1600/ 3200]\n",
      "loss: 0.489956  [ 1616/ 3200]\n",
      "loss: 0.850658  [ 1632/ 3200]\n",
      "loss: 0.981143  [ 1648/ 3200]\n",
      "loss: 0.781273  [ 1664/ 3200]\n",
      "loss: 0.819429  [ 1680/ 3200]\n",
      "loss: 0.881723  [ 1696/ 3200]\n",
      "loss: 0.828338  [ 1712/ 3200]\n",
      "loss: 1.049185  [ 1728/ 3200]\n",
      "loss: 0.968526  [ 1744/ 3200]\n",
      "loss: 0.574845  [ 1760/ 3200]\n",
      "loss: 0.650305  [ 1776/ 3200]\n",
      "loss: 0.910189  [ 1792/ 3200]\n",
      "loss: 0.626559  [ 1808/ 3200]\n",
      "loss: 0.813673  [ 1824/ 3200]\n",
      "loss: 0.536082  [ 1840/ 3200]\n",
      "loss: 0.686707  [ 1856/ 3200]\n",
      "loss: 1.078643  [ 1872/ 3200]\n",
      "loss: 1.124998  [ 1888/ 3200]\n",
      "loss: 0.736973  [ 1904/ 3200]\n",
      "loss: 0.578574  [ 1920/ 3200]\n",
      "loss: 0.722939  [ 1936/ 3200]\n",
      "loss: 0.416380  [ 1952/ 3200]\n",
      "loss: 0.806642  [ 1968/ 3200]\n",
      "loss: 0.704966  [ 1984/ 3200]\n",
      "loss: 0.873208  [ 2000/ 3200]\n",
      "loss: 0.709683  [ 2016/ 3200]\n",
      "loss: 0.952036  [ 2032/ 3200]\n",
      "loss: 0.684524  [ 2048/ 3200]\n",
      "loss: 0.936009  [ 2064/ 3200]\n",
      "loss: 0.693177  [ 2080/ 3200]\n",
      "loss: 0.737290  [ 2096/ 3200]\n",
      "loss: 0.653871  [ 2112/ 3200]\n",
      "loss: 0.501387  [ 2128/ 3200]\n",
      "loss: 0.611651  [ 2144/ 3200]\n",
      "loss: 0.625680  [ 2160/ 3200]\n",
      "loss: 0.723132  [ 2176/ 3200]\n",
      "loss: 0.943193  [ 2192/ 3200]\n",
      "loss: 0.624229  [ 2208/ 3200]\n",
      "loss: 0.908966  [ 2224/ 3200]\n",
      "loss: 0.606931  [ 2240/ 3200]\n",
      "loss: 0.621343  [ 2256/ 3200]\n",
      "loss: 0.789275  [ 2272/ 3200]\n",
      "loss: 0.806250  [ 2288/ 3200]\n",
      "loss: 0.745529  [ 2304/ 3200]\n",
      "loss: 0.581631  [ 2320/ 3200]\n",
      "loss: 0.588428  [ 2336/ 3200]\n",
      "loss: 1.257464  [ 2352/ 3200]\n",
      "loss: 0.482989  [ 2368/ 3200]\n",
      "loss: 0.712955  [ 2384/ 3200]\n",
      "loss: 0.756732  [ 2400/ 3200]\n",
      "loss: 1.369096  [ 2416/ 3200]\n",
      "loss: 0.763849  [ 2432/ 3200]\n",
      "loss: 0.975688  [ 2448/ 3200]\n",
      "loss: 0.663278  [ 2464/ 3200]\n",
      "loss: 0.979020  [ 2480/ 3200]\n",
      "loss: 0.980505  [ 2496/ 3200]\n",
      "loss: 0.462676  [ 2512/ 3200]\n",
      "loss: 1.044167  [ 2528/ 3200]\n",
      "loss: 0.784280  [ 2544/ 3200]\n",
      "loss: 0.785836  [ 2560/ 3200]\n",
      "loss: 0.482327  [ 2576/ 3200]\n",
      "loss: 0.774572  [ 2592/ 3200]\n",
      "loss: 0.816020  [ 2608/ 3200]\n",
      "loss: 0.746749  [ 2624/ 3200]\n",
      "loss: 0.747376  [ 2640/ 3200]\n",
      "loss: 0.602345  [ 2656/ 3200]\n",
      "loss: 0.763376  [ 2672/ 3200]\n",
      "loss: 0.770602  [ 2688/ 3200]\n",
      "loss: 0.803042  [ 2704/ 3200]\n",
      "loss: 0.676859  [ 2720/ 3200]\n",
      "loss: 0.709205  [ 2736/ 3200]\n",
      "loss: 0.585385  [ 2752/ 3200]\n",
      "loss: 0.762814  [ 2768/ 3200]\n",
      "loss: 0.864069  [ 2784/ 3200]\n",
      "loss: 0.712896  [ 2800/ 3200]\n",
      "loss: 0.943624  [ 2816/ 3200]\n",
      "loss: 0.793077  [ 2832/ 3200]\n",
      "loss: 0.798327  [ 2848/ 3200]\n",
      "loss: 0.655160  [ 2864/ 3200]\n",
      "loss: 0.904791  [ 2880/ 3200]\n",
      "loss: 0.855721  [ 2896/ 3200]\n",
      "loss: 0.922032  [ 2912/ 3200]\n",
      "loss: 0.849401  [ 2928/ 3200]\n",
      "loss: 0.888140  [ 2944/ 3200]\n",
      "loss: 0.730941  [ 2960/ 3200]\n",
      "loss: 1.036174  [ 2976/ 3200]\n",
      "loss: 0.492556  [ 2992/ 3200]\n",
      "loss: 0.785106  [ 3008/ 3200]\n",
      "loss: 0.746376  [ 3024/ 3200]\n",
      "loss: 0.831904  [ 3040/ 3200]\n",
      "loss: 0.726296  [ 3056/ 3200]\n",
      "loss: 0.823391  [ 3072/ 3200]\n",
      "loss: 0.950434  [ 3088/ 3200]\n",
      "loss: 0.579936  [ 3104/ 3200]\n",
      "loss: 0.849284  [ 3120/ 3200]\n",
      "loss: 0.965907  [ 3136/ 3200]\n",
      "loss: 1.589381  [ 3152/ 3200]\n",
      "loss: 1.027662  [ 3168/ 3200]\n",
      "loss: 0.780156  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.608838  [    0/ 3200]\n",
      "loss: 1.065102  [   16/ 3200]\n",
      "loss: 0.527173  [   32/ 3200]\n",
      "loss: 0.709613  [   48/ 3200]\n",
      "loss: 0.858898  [   64/ 3200]\n",
      "loss: 0.525583  [   80/ 3200]\n",
      "loss: 1.001326  [   96/ 3200]\n",
      "loss: 0.639334  [  112/ 3200]\n",
      "loss: 0.893152  [  128/ 3200]\n",
      "loss: 1.107172  [  144/ 3200]\n",
      "loss: 0.897869  [  160/ 3200]\n",
      "loss: 0.501484  [  176/ 3200]\n",
      "loss: 0.747256  [  192/ 3200]\n",
      "loss: 0.568444  [  208/ 3200]\n",
      "loss: 0.667654  [  224/ 3200]\n",
      "loss: 0.888085  [  240/ 3200]\n",
      "loss: 0.751110  [  256/ 3200]\n",
      "loss: 0.847114  [  272/ 3200]\n",
      "loss: 0.777456  [  288/ 3200]\n",
      "loss: 0.689906  [  304/ 3200]\n",
      "loss: 0.530169  [  320/ 3200]\n",
      "loss: 0.931995  [  336/ 3200]\n",
      "loss: 0.376274  [  352/ 3200]\n",
      "loss: 0.701123  [  368/ 3200]\n",
      "loss: 0.768319  [  384/ 3200]\n",
      "loss: 0.933135  [  400/ 3200]\n",
      "loss: 0.763171  [  416/ 3200]\n",
      "loss: 0.691337  [  432/ 3200]\n",
      "loss: 0.593337  [  448/ 3200]\n",
      "loss: 0.878829  [  464/ 3200]\n",
      "loss: 1.046439  [  480/ 3200]\n",
      "loss: 0.989893  [  496/ 3200]\n",
      "loss: 0.626224  [  512/ 3200]\n",
      "loss: 0.812768  [  528/ 3200]\n",
      "loss: 0.789715  [  544/ 3200]\n",
      "loss: 0.762360  [  560/ 3200]\n",
      "loss: 0.848681  [  576/ 3200]\n",
      "loss: 0.763466  [  592/ 3200]\n",
      "loss: 0.777122  [  608/ 3200]\n",
      "loss: 0.776763  [  624/ 3200]\n",
      "loss: 1.116326  [  640/ 3200]\n",
      "loss: 0.530322  [  656/ 3200]\n",
      "loss: 0.913074  [  672/ 3200]\n",
      "loss: 0.787699  [  688/ 3200]\n",
      "loss: 0.651075  [  704/ 3200]\n",
      "loss: 0.543273  [  720/ 3200]\n",
      "loss: 0.871398  [  736/ 3200]\n",
      "loss: 0.661598  [  752/ 3200]\n",
      "loss: 0.858259  [  768/ 3200]\n",
      "loss: 0.748388  [  784/ 3200]\n",
      "loss: 0.420272  [  800/ 3200]\n",
      "loss: 0.511299  [  816/ 3200]\n",
      "loss: 0.647618  [  832/ 3200]\n",
      "loss: 0.750784  [  848/ 3200]\n",
      "loss: 0.948834  [  864/ 3200]\n",
      "loss: 0.457815  [  880/ 3200]\n",
      "loss: 0.922556  [  896/ 3200]\n",
      "loss: 1.073315  [  912/ 3200]\n",
      "loss: 0.931847  [  928/ 3200]\n",
      "loss: 0.587397  [  944/ 3200]\n",
      "loss: 1.051524  [  960/ 3200]\n",
      "loss: 1.012355  [  976/ 3200]\n",
      "loss: 0.686878  [  992/ 3200]\n",
      "loss: 0.819660  [ 1008/ 3200]\n",
      "loss: 0.518905  [ 1024/ 3200]\n",
      "loss: 1.131113  [ 1040/ 3200]\n",
      "loss: 0.923438  [ 1056/ 3200]\n",
      "loss: 0.888205  [ 1072/ 3200]\n",
      "loss: 0.782291  [ 1088/ 3200]\n",
      "loss: 0.864338  [ 1104/ 3200]\n",
      "loss: 0.889530  [ 1120/ 3200]\n",
      "loss: 0.671898  [ 1136/ 3200]\n",
      "loss: 0.635671  [ 1152/ 3200]\n",
      "loss: 0.653731  [ 1168/ 3200]\n",
      "loss: 0.851287  [ 1184/ 3200]\n",
      "loss: 0.558282  [ 1200/ 3200]\n",
      "loss: 0.851988  [ 1216/ 3200]\n",
      "loss: 0.756097  [ 1232/ 3200]\n",
      "loss: 0.588623  [ 1248/ 3200]\n",
      "loss: 0.695595  [ 1264/ 3200]\n",
      "loss: 0.900255  [ 1280/ 3200]\n",
      "loss: 0.799375  [ 1296/ 3200]\n",
      "loss: 0.877525  [ 1312/ 3200]\n",
      "loss: 0.905406  [ 1328/ 3200]\n",
      "loss: 0.838963  [ 1344/ 3200]\n",
      "loss: 0.902043  [ 1360/ 3200]\n",
      "loss: 0.424270  [ 1376/ 3200]\n",
      "loss: 0.645556  [ 1392/ 3200]\n",
      "loss: 0.678238  [ 1408/ 3200]\n",
      "loss: 0.673158  [ 1424/ 3200]\n",
      "loss: 0.690355  [ 1440/ 3200]\n",
      "loss: 1.175065  [ 1456/ 3200]\n",
      "loss: 0.795629  [ 1472/ 3200]\n",
      "loss: 1.069396  [ 1488/ 3200]\n",
      "loss: 0.636580  [ 1504/ 3200]\n",
      "loss: 0.613622  [ 1520/ 3200]\n",
      "loss: 0.848022  [ 1536/ 3200]\n",
      "loss: 0.671147  [ 1552/ 3200]\n",
      "loss: 1.087436  [ 1568/ 3200]\n",
      "loss: 0.596338  [ 1584/ 3200]\n",
      "loss: 1.185850  [ 1600/ 3200]\n",
      "loss: 0.993087  [ 1616/ 3200]\n",
      "loss: 0.834385  [ 1632/ 3200]\n",
      "loss: 0.822953  [ 1648/ 3200]\n",
      "loss: 0.657935  [ 1664/ 3200]\n",
      "loss: 0.929423  [ 1680/ 3200]\n",
      "loss: 1.133841  [ 1696/ 3200]\n",
      "loss: 0.486691  [ 1712/ 3200]\n",
      "loss: 0.788445  [ 1728/ 3200]\n",
      "loss: 0.642876  [ 1744/ 3200]\n",
      "loss: 1.322881  [ 1760/ 3200]\n",
      "loss: 0.718777  [ 1776/ 3200]\n",
      "loss: 0.651037  [ 1792/ 3200]\n",
      "loss: 0.622193  [ 1808/ 3200]\n",
      "loss: 0.697162  [ 1824/ 3200]\n",
      "loss: 0.716536  [ 1840/ 3200]\n",
      "loss: 0.549872  [ 1856/ 3200]\n",
      "loss: 0.670467  [ 1872/ 3200]\n",
      "loss: 0.627384  [ 1888/ 3200]\n",
      "loss: 0.430411  [ 1904/ 3200]\n",
      "loss: 0.875739  [ 1920/ 3200]\n",
      "loss: 0.695806  [ 1936/ 3200]\n",
      "loss: 0.604840  [ 1952/ 3200]\n",
      "loss: 1.256033  [ 1968/ 3200]\n",
      "loss: 0.790475  [ 1984/ 3200]\n",
      "loss: 0.831768  [ 2000/ 3200]\n",
      "loss: 0.806902  [ 2016/ 3200]\n",
      "loss: 0.589707  [ 2032/ 3200]\n",
      "loss: 0.742797  [ 2048/ 3200]\n",
      "loss: 0.951041  [ 2064/ 3200]\n",
      "loss: 0.759824  [ 2080/ 3200]\n",
      "loss: 0.609816  [ 2096/ 3200]\n",
      "loss: 0.931524  [ 2112/ 3200]\n",
      "loss: 0.981332  [ 2128/ 3200]\n",
      "loss: 0.693263  [ 2144/ 3200]\n",
      "loss: 0.794748  [ 2160/ 3200]\n",
      "loss: 0.476935  [ 2176/ 3200]\n",
      "loss: 0.955971  [ 2192/ 3200]\n",
      "loss: 0.748504  [ 2208/ 3200]\n",
      "loss: 0.715825  [ 2224/ 3200]\n",
      "loss: 1.108032  [ 2240/ 3200]\n",
      "loss: 0.687829  [ 2256/ 3200]\n",
      "loss: 1.123475  [ 2272/ 3200]\n",
      "loss: 0.649611  [ 2288/ 3200]\n",
      "loss: 0.861256  [ 2304/ 3200]\n",
      "loss: 1.016128  [ 2320/ 3200]\n",
      "loss: 0.838431  [ 2336/ 3200]\n",
      "loss: 0.889100  [ 2352/ 3200]\n",
      "loss: 0.705690  [ 2368/ 3200]\n",
      "loss: 0.884494  [ 2384/ 3200]\n",
      "loss: 1.007576  [ 2400/ 3200]\n",
      "loss: 0.971582  [ 2416/ 3200]\n",
      "loss: 0.712716  [ 2432/ 3200]\n",
      "loss: 0.743223  [ 2448/ 3200]\n",
      "loss: 0.525417  [ 2464/ 3200]\n",
      "loss: 0.599524  [ 2480/ 3200]\n",
      "loss: 0.649970  [ 2496/ 3200]\n",
      "loss: 0.573931  [ 2512/ 3200]\n",
      "loss: 0.877963  [ 2528/ 3200]\n",
      "loss: 0.632453  [ 2544/ 3200]\n",
      "loss: 0.659428  [ 2560/ 3200]\n",
      "loss: 0.828029  [ 2576/ 3200]\n",
      "loss: 0.565009  [ 2592/ 3200]\n",
      "loss: 0.825477  [ 2608/ 3200]\n",
      "loss: 0.525364  [ 2624/ 3200]\n",
      "loss: 0.726917  [ 2640/ 3200]\n",
      "loss: 0.634610  [ 2656/ 3200]\n",
      "loss: 0.942537  [ 2672/ 3200]\n",
      "loss: 1.089023  [ 2688/ 3200]\n",
      "loss: 0.558892  [ 2704/ 3200]\n",
      "loss: 0.366925  [ 2720/ 3200]\n",
      "loss: 0.530723  [ 2736/ 3200]\n",
      "loss: 0.936552  [ 2752/ 3200]\n",
      "loss: 0.746942  [ 2768/ 3200]\n",
      "loss: 0.487891  [ 2784/ 3200]\n",
      "loss: 0.610246  [ 2800/ 3200]\n",
      "loss: 0.626184  [ 2816/ 3200]\n",
      "loss: 0.857293  [ 2832/ 3200]\n",
      "loss: 0.510963  [ 2848/ 3200]\n",
      "loss: 0.748518  [ 2864/ 3200]\n",
      "loss: 0.673491  [ 2880/ 3200]\n",
      "loss: 0.564445  [ 2896/ 3200]\n",
      "loss: 1.269062  [ 2912/ 3200]\n",
      "loss: 0.588847  [ 2928/ 3200]\n",
      "loss: 0.818649  [ 2944/ 3200]\n",
      "loss: 0.637263  [ 2960/ 3200]\n",
      "loss: 0.717864  [ 2976/ 3200]\n",
      "loss: 0.688598  [ 2992/ 3200]\n",
      "loss: 0.645639  [ 3008/ 3200]\n",
      "loss: 0.896475  [ 3024/ 3200]\n",
      "loss: 0.573460  [ 3040/ 3200]\n",
      "loss: 0.566370  [ 3056/ 3200]\n",
      "loss: 0.824494  [ 3072/ 3200]\n",
      "loss: 0.585037  [ 3088/ 3200]\n",
      "loss: 0.970872  [ 3104/ 3200]\n",
      "loss: 0.487735  [ 3120/ 3200]\n",
      "loss: 0.640929  [ 3136/ 3200]\n",
      "loss: 0.760410  [ 3152/ 3200]\n",
      "loss: 0.879088  [ 3168/ 3200]\n",
      "loss: 0.907714  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 0.671513  [    0/ 3200]\n",
      "loss: 0.477040  [   16/ 3200]\n",
      "loss: 0.945719  [   32/ 3200]\n",
      "loss: 0.821982  [   48/ 3200]\n",
      "loss: 0.618935  [   64/ 3200]\n",
      "loss: 0.765669  [   80/ 3200]\n",
      "loss: 0.999029  [   96/ 3200]\n",
      "loss: 1.096760  [  112/ 3200]\n",
      "loss: 1.065230  [  128/ 3200]\n",
      "loss: 0.741554  [  144/ 3200]\n",
      "loss: 0.503180  [  160/ 3200]\n",
      "loss: 0.677673  [  176/ 3200]\n",
      "loss: 0.751471  [  192/ 3200]\n",
      "loss: 0.845364  [  208/ 3200]\n",
      "loss: 0.806488  [  224/ 3200]\n",
      "loss: 1.114761  [  240/ 3200]\n",
      "loss: 0.733846  [  256/ 3200]\n",
      "loss: 0.572502  [  272/ 3200]\n",
      "loss: 0.500206  [  288/ 3200]\n",
      "loss: 0.726822  [  304/ 3200]\n",
      "loss: 0.814153  [  320/ 3200]\n",
      "loss: 0.863887  [  336/ 3200]\n",
      "loss: 0.542143  [  352/ 3200]\n",
      "loss: 0.949081  [  368/ 3200]\n",
      "loss: 0.419911  [  384/ 3200]\n",
      "loss: 0.590120  [  400/ 3200]\n",
      "loss: 0.875288  [  416/ 3200]\n",
      "loss: 0.464376  [  432/ 3200]\n",
      "loss: 0.725954  [  448/ 3200]\n",
      "loss: 1.011274  [  464/ 3200]\n",
      "loss: 0.509799  [  480/ 3200]\n",
      "loss: 0.752824  [  496/ 3200]\n",
      "loss: 0.826736  [  512/ 3200]\n",
      "loss: 0.628529  [  528/ 3200]\n",
      "loss: 0.739775  [  544/ 3200]\n",
      "loss: 0.874084  [  560/ 3200]\n",
      "loss: 0.526748  [  576/ 3200]\n",
      "loss: 0.803555  [  592/ 3200]\n",
      "loss: 0.550795  [  608/ 3200]\n",
      "loss: 0.652508  [  624/ 3200]\n",
      "loss: 0.732770  [  640/ 3200]\n",
      "loss: 0.960019  [  656/ 3200]\n",
      "loss: 0.771390  [  672/ 3200]\n",
      "loss: 0.770896  [  688/ 3200]\n",
      "loss: 0.580152  [  704/ 3200]\n",
      "loss: 0.558319  [  720/ 3200]\n",
      "loss: 0.803185  [  736/ 3200]\n",
      "loss: 0.557049  [  752/ 3200]\n",
      "loss: 0.787824  [  768/ 3200]\n",
      "loss: 0.544185  [  784/ 3200]\n",
      "loss: 0.870880  [  800/ 3200]\n",
      "loss: 0.656087  [  816/ 3200]\n",
      "loss: 0.860572  [  832/ 3200]\n",
      "loss: 1.147950  [  848/ 3200]\n",
      "loss: 0.629341  [  864/ 3200]\n",
      "loss: 0.774612  [  880/ 3200]\n",
      "loss: 0.567212  [  896/ 3200]\n",
      "loss: 0.460751  [  912/ 3200]\n",
      "loss: 0.973486  [  928/ 3200]\n",
      "loss: 1.015194  [  944/ 3200]\n",
      "loss: 1.051147  [  960/ 3200]\n",
      "loss: 0.693510  [  976/ 3200]\n",
      "loss: 0.785437  [  992/ 3200]\n",
      "loss: 0.616687  [ 1008/ 3200]\n",
      "loss: 0.809355  [ 1024/ 3200]\n",
      "loss: 0.581815  [ 1040/ 3200]\n",
      "loss: 0.463007  [ 1056/ 3200]\n",
      "loss: 0.777993  [ 1072/ 3200]\n",
      "loss: 0.552276  [ 1088/ 3200]\n",
      "loss: 0.908123  [ 1104/ 3200]\n",
      "loss: 0.530957  [ 1120/ 3200]\n",
      "loss: 0.552562  [ 1136/ 3200]\n",
      "loss: 1.243775  [ 1152/ 3200]\n",
      "loss: 0.735551  [ 1168/ 3200]\n",
      "loss: 0.517551  [ 1184/ 3200]\n",
      "loss: 0.545119  [ 1200/ 3200]\n",
      "loss: 0.802797  [ 1216/ 3200]\n",
      "loss: 0.843731  [ 1232/ 3200]\n",
      "loss: 0.655071  [ 1248/ 3200]\n",
      "loss: 0.427014  [ 1264/ 3200]\n",
      "loss: 1.004985  [ 1280/ 3200]\n",
      "loss: 0.928108  [ 1296/ 3200]\n",
      "loss: 0.859678  [ 1312/ 3200]\n",
      "loss: 0.771126  [ 1328/ 3200]\n",
      "loss: 0.672567  [ 1344/ 3200]\n",
      "loss: 0.699980  [ 1360/ 3200]\n",
      "loss: 0.878445  [ 1376/ 3200]\n",
      "loss: 0.579107  [ 1392/ 3200]\n",
      "loss: 0.631289  [ 1408/ 3200]\n",
      "loss: 0.857028  [ 1424/ 3200]\n",
      "loss: 0.757761  [ 1440/ 3200]\n",
      "loss: 0.689053  [ 1456/ 3200]\n",
      "loss: 1.039325  [ 1472/ 3200]\n",
      "loss: 0.825199  [ 1488/ 3200]\n",
      "loss: 0.663896  [ 1504/ 3200]\n",
      "loss: 0.714582  [ 1520/ 3200]\n",
      "loss: 0.996313  [ 1536/ 3200]\n",
      "loss: 0.995420  [ 1552/ 3200]\n",
      "loss: 0.879503  [ 1568/ 3200]\n",
      "loss: 1.098205  [ 1584/ 3200]\n",
      "loss: 0.811310  [ 1600/ 3200]\n",
      "loss: 0.813667  [ 1616/ 3200]\n",
      "loss: 0.907871  [ 1632/ 3200]\n",
      "loss: 0.759515  [ 1648/ 3200]\n",
      "loss: 0.823450  [ 1664/ 3200]\n",
      "loss: 0.980664  [ 1680/ 3200]\n",
      "loss: 1.021985  [ 1696/ 3200]\n",
      "loss: 0.486406  [ 1712/ 3200]\n",
      "loss: 0.450510  [ 1728/ 3200]\n",
      "loss: 0.758086  [ 1744/ 3200]\n",
      "loss: 0.778132  [ 1760/ 3200]\n",
      "loss: 0.724636  [ 1776/ 3200]\n",
      "loss: 0.480134  [ 1792/ 3200]\n",
      "loss: 0.758809  [ 1808/ 3200]\n",
      "loss: 0.793235  [ 1824/ 3200]\n",
      "loss: 0.773997  [ 1840/ 3200]\n",
      "loss: 0.577914  [ 1856/ 3200]\n",
      "loss: 0.967920  [ 1872/ 3200]\n",
      "loss: 0.932974  [ 1888/ 3200]\n",
      "loss: 0.773104  [ 1904/ 3200]\n",
      "loss: 0.742486  [ 1920/ 3200]\n",
      "loss: 0.945855  [ 1936/ 3200]\n",
      "loss: 0.791598  [ 1952/ 3200]\n",
      "loss: 0.763219  [ 1968/ 3200]\n",
      "loss: 0.615336  [ 1984/ 3200]\n",
      "loss: 0.623460  [ 2000/ 3200]\n",
      "loss: 0.962534  [ 2016/ 3200]\n",
      "loss: 0.423073  [ 2032/ 3200]\n",
      "loss: 0.996246  [ 2048/ 3200]\n",
      "loss: 0.871338  [ 2064/ 3200]\n",
      "loss: 0.648674  [ 2080/ 3200]\n",
      "loss: 0.818466  [ 2096/ 3200]\n",
      "loss: 0.661169  [ 2112/ 3200]\n",
      "loss: 0.382211  [ 2128/ 3200]\n",
      "loss: 0.610848  [ 2144/ 3200]\n",
      "loss: 1.085874  [ 2160/ 3200]\n",
      "loss: 0.717109  [ 2176/ 3200]\n",
      "loss: 0.824681  [ 2192/ 3200]\n",
      "loss: 0.849058  [ 2208/ 3200]\n",
      "loss: 0.579451  [ 2224/ 3200]\n",
      "loss: 0.709752  [ 2240/ 3200]\n",
      "loss: 0.563550  [ 2256/ 3200]\n",
      "loss: 0.541715  [ 2272/ 3200]\n",
      "loss: 0.750912  [ 2288/ 3200]\n",
      "loss: 0.614937  [ 2304/ 3200]\n",
      "loss: 0.540906  [ 2320/ 3200]\n",
      "loss: 0.637642  [ 2336/ 3200]\n",
      "loss: 0.596858  [ 2352/ 3200]\n",
      "loss: 0.752382  [ 2368/ 3200]\n",
      "loss: 0.597103  [ 2384/ 3200]\n",
      "loss: 0.690903  [ 2400/ 3200]\n",
      "loss: 0.766591  [ 2416/ 3200]\n",
      "loss: 0.977894  [ 2432/ 3200]\n",
      "loss: 0.724740  [ 2448/ 3200]\n",
      "loss: 0.614694  [ 2464/ 3200]\n",
      "loss: 1.098868  [ 2480/ 3200]\n",
      "loss: 0.629901  [ 2496/ 3200]\n",
      "loss: 0.645213  [ 2512/ 3200]\n",
      "loss: 0.671066  [ 2528/ 3200]\n",
      "loss: 0.705397  [ 2544/ 3200]\n",
      "loss: 0.763376  [ 2560/ 3200]\n",
      "loss: 0.694937  [ 2576/ 3200]\n",
      "loss: 1.081312  [ 2592/ 3200]\n",
      "loss: 0.475953  [ 2608/ 3200]\n",
      "loss: 0.776850  [ 2624/ 3200]\n",
      "loss: 0.938268  [ 2640/ 3200]\n",
      "loss: 0.479439  [ 2656/ 3200]\n",
      "loss: 0.495526  [ 2672/ 3200]\n",
      "loss: 0.551467  [ 2688/ 3200]\n",
      "loss: 1.080300  [ 2704/ 3200]\n",
      "loss: 0.609460  [ 2720/ 3200]\n",
      "loss: 1.026863  [ 2736/ 3200]\n",
      "loss: 0.459615  [ 2752/ 3200]\n",
      "loss: 0.628748  [ 2768/ 3200]\n",
      "loss: 0.695213  [ 2784/ 3200]\n",
      "loss: 0.805749  [ 2800/ 3200]\n",
      "loss: 0.721240  [ 2816/ 3200]\n",
      "loss: 0.701808  [ 2832/ 3200]\n",
      "loss: 0.947457  [ 2848/ 3200]\n",
      "loss: 0.944467  [ 2864/ 3200]\n",
      "loss: 0.919087  [ 2880/ 3200]\n",
      "loss: 0.706212  [ 2896/ 3200]\n",
      "loss: 0.937917  [ 2912/ 3200]\n",
      "loss: 0.837212  [ 2928/ 3200]\n",
      "loss: 0.626575  [ 2944/ 3200]\n",
      "loss: 0.699589  [ 2960/ 3200]\n",
      "loss: 0.431816  [ 2976/ 3200]\n",
      "loss: 0.457272  [ 2992/ 3200]\n",
      "loss: 0.939606  [ 3008/ 3200]\n",
      "loss: 0.755579  [ 3024/ 3200]\n",
      "loss: 0.909733  [ 3040/ 3200]\n",
      "loss: 0.868376  [ 3056/ 3200]\n",
      "loss: 0.650118  [ 3072/ 3200]\n",
      "loss: 0.494544  [ 3088/ 3200]\n",
      "loss: 0.804989  [ 3104/ 3200]\n",
      "loss: 0.755306  [ 3120/ 3200]\n",
      "loss: 0.548104  [ 3136/ 3200]\n",
      "loss: 0.534246  [ 3152/ 3200]\n",
      "loss: 1.213510  [ 3168/ 3200]\n",
      "loss: 0.757998  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.672493  [    0/ 3200]\n",
      "loss: 0.679343  [   16/ 3200]\n",
      "loss: 0.435222  [   32/ 3200]\n",
      "loss: 0.552841  [   48/ 3200]\n",
      "loss: 0.636684  [   64/ 3200]\n",
      "loss: 0.960782  [   80/ 3200]\n",
      "loss: 1.261108  [   96/ 3200]\n",
      "loss: 0.999411  [  112/ 3200]\n",
      "loss: 0.852726  [  128/ 3200]\n",
      "loss: 0.681735  [  144/ 3200]\n",
      "loss: 0.458530  [  160/ 3200]\n",
      "loss: 0.660654  [  176/ 3200]\n",
      "loss: 0.528401  [  192/ 3200]\n",
      "loss: 0.729663  [  208/ 3200]\n",
      "loss: 0.718952  [  224/ 3200]\n",
      "loss: 0.490624  [  240/ 3200]\n",
      "loss: 0.792436  [  256/ 3200]\n",
      "loss: 0.734569  [  272/ 3200]\n",
      "loss: 0.641445  [  288/ 3200]\n",
      "loss: 0.767465  [  304/ 3200]\n",
      "loss: 0.824929  [  320/ 3200]\n",
      "loss: 0.468311  [  336/ 3200]\n",
      "loss: 0.768536  [  352/ 3200]\n",
      "loss: 0.700749  [  368/ 3200]\n",
      "loss: 0.606156  [  384/ 3200]\n",
      "loss: 0.545030  [  400/ 3200]\n",
      "loss: 0.674985  [  416/ 3200]\n",
      "loss: 0.764532  [  432/ 3200]\n",
      "loss: 0.998375  [  448/ 3200]\n",
      "loss: 0.822742  [  464/ 3200]\n",
      "loss: 0.982038  [  480/ 3200]\n",
      "loss: 0.763334  [  496/ 3200]\n",
      "loss: 0.590222  [  512/ 3200]\n",
      "loss: 0.717508  [  528/ 3200]\n",
      "loss: 0.597505  [  544/ 3200]\n",
      "loss: 0.576595  [  560/ 3200]\n",
      "loss: 0.588912  [  576/ 3200]\n",
      "loss: 0.706060  [  592/ 3200]\n",
      "loss: 0.666325  [  608/ 3200]\n",
      "loss: 0.485579  [  624/ 3200]\n",
      "loss: 0.572843  [  640/ 3200]\n",
      "loss: 0.593723  [  656/ 3200]\n",
      "loss: 0.973230  [  672/ 3200]\n",
      "loss: 0.676421  [  688/ 3200]\n",
      "loss: 0.846667  [  704/ 3200]\n",
      "loss: 0.863051  [  720/ 3200]\n",
      "loss: 0.602953  [  736/ 3200]\n",
      "loss: 0.643624  [  752/ 3200]\n",
      "loss: 0.498450  [  768/ 3200]\n",
      "loss: 0.468942  [  784/ 3200]\n",
      "loss: 0.864285  [  800/ 3200]\n",
      "loss: 0.635385  [  816/ 3200]\n",
      "loss: 0.553872  [  832/ 3200]\n",
      "loss: 0.760866  [  848/ 3200]\n",
      "loss: 0.478312  [  864/ 3200]\n",
      "loss: 0.812759  [  880/ 3200]\n",
      "loss: 0.798690  [  896/ 3200]\n",
      "loss: 0.374890  [  912/ 3200]\n",
      "loss: 0.515236  [  928/ 3200]\n",
      "loss: 0.749971  [  944/ 3200]\n",
      "loss: 0.799576  [  960/ 3200]\n",
      "loss: 0.628665  [  976/ 3200]\n",
      "loss: 1.006782  [  992/ 3200]\n",
      "loss: 0.747347  [ 1008/ 3200]\n",
      "loss: 0.713066  [ 1024/ 3200]\n",
      "loss: 0.815569  [ 1040/ 3200]\n",
      "loss: 0.711631  [ 1056/ 3200]\n",
      "loss: 0.946462  [ 1072/ 3200]\n",
      "loss: 0.468531  [ 1088/ 3200]\n",
      "loss: 0.751681  [ 1104/ 3200]\n",
      "loss: 0.660885  [ 1120/ 3200]\n",
      "loss: 0.785042  [ 1136/ 3200]\n",
      "loss: 0.370518  [ 1152/ 3200]\n",
      "loss: 1.052985  [ 1168/ 3200]\n",
      "loss: 0.809518  [ 1184/ 3200]\n",
      "loss: 0.667751  [ 1200/ 3200]\n",
      "loss: 0.720269  [ 1216/ 3200]\n",
      "loss: 1.204813  [ 1232/ 3200]\n",
      "loss: 0.688214  [ 1248/ 3200]\n",
      "loss: 0.613859  [ 1264/ 3200]\n",
      "loss: 0.452902  [ 1280/ 3200]\n",
      "loss: 0.770407  [ 1296/ 3200]\n",
      "loss: 0.934313  [ 1312/ 3200]\n",
      "loss: 0.913040  [ 1328/ 3200]\n",
      "loss: 0.578734  [ 1344/ 3200]\n",
      "loss: 0.541579  [ 1360/ 3200]\n",
      "loss: 0.934900  [ 1376/ 3200]\n",
      "loss: 0.518366  [ 1392/ 3200]\n",
      "loss: 1.216317  [ 1408/ 3200]\n",
      "loss: 0.696842  [ 1424/ 3200]\n",
      "loss: 0.852035  [ 1440/ 3200]\n",
      "loss: 0.662970  [ 1456/ 3200]\n",
      "loss: 0.895493  [ 1472/ 3200]\n",
      "loss: 1.430450  [ 1488/ 3200]\n",
      "loss: 0.624630  [ 1504/ 3200]\n",
      "loss: 0.693439  [ 1520/ 3200]\n",
      "loss: 0.863369  [ 1536/ 3200]\n",
      "loss: 1.235365  [ 1552/ 3200]\n",
      "loss: 0.807451  [ 1568/ 3200]\n",
      "loss: 0.827556  [ 1584/ 3200]\n",
      "loss: 0.738501  [ 1600/ 3200]\n",
      "loss: 0.647938  [ 1616/ 3200]\n",
      "loss: 0.814924  [ 1632/ 3200]\n",
      "loss: 0.687236  [ 1648/ 3200]\n",
      "loss: 0.897855  [ 1664/ 3200]\n",
      "loss: 0.783239  [ 1680/ 3200]\n",
      "loss: 0.435226  [ 1696/ 3200]\n",
      "loss: 0.640694  [ 1712/ 3200]\n",
      "loss: 0.599879  [ 1728/ 3200]\n",
      "loss: 0.855906  [ 1744/ 3200]\n",
      "loss: 0.841826  [ 1760/ 3200]\n",
      "loss: 0.658580  [ 1776/ 3200]\n",
      "loss: 0.843004  [ 1792/ 3200]\n",
      "loss: 0.762798  [ 1808/ 3200]\n",
      "loss: 0.652513  [ 1824/ 3200]\n",
      "loss: 0.871164  [ 1840/ 3200]\n",
      "loss: 0.655919  [ 1856/ 3200]\n",
      "loss: 0.726884  [ 1872/ 3200]\n",
      "loss: 0.817941  [ 1888/ 3200]\n",
      "loss: 0.620575  [ 1904/ 3200]\n",
      "loss: 0.705298  [ 1920/ 3200]\n",
      "loss: 1.019538  [ 1936/ 3200]\n",
      "loss: 0.793159  [ 1952/ 3200]\n",
      "loss: 0.390333  [ 1968/ 3200]\n",
      "loss: 1.012081  [ 1984/ 3200]\n",
      "loss: 1.124739  [ 2000/ 3200]\n",
      "loss: 1.023768  [ 2016/ 3200]\n",
      "loss: 0.638513  [ 2032/ 3200]\n",
      "loss: 0.524858  [ 2048/ 3200]\n",
      "loss: 0.975007  [ 2064/ 3200]\n",
      "loss: 0.807580  [ 2080/ 3200]\n",
      "loss: 0.942842  [ 2096/ 3200]\n",
      "loss: 0.532576  [ 2112/ 3200]\n",
      "loss: 0.566231  [ 2128/ 3200]\n",
      "loss: 0.755175  [ 2144/ 3200]\n",
      "loss: 1.196747  [ 2160/ 3200]\n",
      "loss: 0.602280  [ 2176/ 3200]\n",
      "loss: 0.736680  [ 2192/ 3200]\n",
      "loss: 0.835497  [ 2208/ 3200]\n",
      "loss: 0.733025  [ 2224/ 3200]\n",
      "loss: 0.930701  [ 2240/ 3200]\n",
      "loss: 0.857028  [ 2256/ 3200]\n",
      "loss: 0.787462  [ 2272/ 3200]\n",
      "loss: 0.747129  [ 2288/ 3200]\n",
      "loss: 0.749859  [ 2304/ 3200]\n",
      "loss: 0.513930  [ 2320/ 3200]\n",
      "loss: 0.807001  [ 2336/ 3200]\n",
      "loss: 0.918337  [ 2352/ 3200]\n",
      "loss: 1.207448  [ 2368/ 3200]\n",
      "loss: 0.564347  [ 2384/ 3200]\n",
      "loss: 0.505368  [ 2400/ 3200]\n",
      "loss: 0.859086  [ 2416/ 3200]\n",
      "loss: 0.575642  [ 2432/ 3200]\n",
      "loss: 0.643439  [ 2448/ 3200]\n",
      "loss: 0.630413  [ 2464/ 3200]\n",
      "loss: 0.561640  [ 2480/ 3200]\n",
      "loss: 0.798214  [ 2496/ 3200]\n",
      "loss: 0.545724  [ 2512/ 3200]\n",
      "loss: 0.627669  [ 2528/ 3200]\n",
      "loss: 0.288032  [ 2544/ 3200]\n",
      "loss: 0.669517  [ 2560/ 3200]\n",
      "loss: 0.754405  [ 2576/ 3200]\n",
      "loss: 0.968827  [ 2592/ 3200]\n",
      "loss: 0.685284  [ 2608/ 3200]\n",
      "loss: 0.700143  [ 2624/ 3200]\n",
      "loss: 0.493662  [ 2640/ 3200]\n",
      "loss: 1.182821  [ 2656/ 3200]\n",
      "loss: 0.610571  [ 2672/ 3200]\n",
      "loss: 0.659418  [ 2688/ 3200]\n",
      "loss: 0.912556  [ 2704/ 3200]\n",
      "loss: 0.573664  [ 2720/ 3200]\n",
      "loss: 0.475127  [ 2736/ 3200]\n",
      "loss: 0.852940  [ 2752/ 3200]\n",
      "loss: 0.710837  [ 2768/ 3200]\n",
      "loss: 0.645525  [ 2784/ 3200]\n",
      "loss: 0.464064  [ 2800/ 3200]\n",
      "loss: 1.210012  [ 2816/ 3200]\n",
      "loss: 0.783185  [ 2832/ 3200]\n",
      "loss: 0.390240  [ 2848/ 3200]\n",
      "loss: 0.669296  [ 2864/ 3200]\n",
      "loss: 0.531138  [ 2880/ 3200]\n",
      "loss: 0.652297  [ 2896/ 3200]\n",
      "loss: 0.620000  [ 2912/ 3200]\n",
      "loss: 0.523799  [ 2928/ 3200]\n",
      "loss: 0.619537  [ 2944/ 3200]\n",
      "loss: 1.036308  [ 2960/ 3200]\n",
      "loss: 1.085552  [ 2976/ 3200]\n",
      "loss: 0.833912  [ 2992/ 3200]\n",
      "loss: 0.822662  [ 3008/ 3200]\n",
      "loss: 0.667986  [ 3024/ 3200]\n",
      "loss: 0.716192  [ 3040/ 3200]\n",
      "loss: 0.535388  [ 3056/ 3200]\n",
      "loss: 1.004824  [ 3072/ 3200]\n",
      "loss: 0.625720  [ 3088/ 3200]\n",
      "loss: 0.977487  [ 3104/ 3200]\n",
      "loss: 1.213852  [ 3120/ 3200]\n",
      "loss: 0.882944  [ 3136/ 3200]\n",
      "loss: 0.950912  [ 3152/ 3200]\n",
      "loss: 0.768647  [ 3168/ 3200]\n",
      "loss: 0.725076  [ 3184/ 3200]\n",
      "\n",
      "CPU times: user 21.8 s, sys: 1.22 s, total: 23.1 s\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cnn_model = train_convolutional_neural_network(epochs, optimizer, train_dataloader, loss_function, cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msDDSIpy5PDc"
   },
   "source": [
    "Test our Convolutional Neural Network on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUh8mutZ5fz0",
    "outputId": "6113b995-9817-46ed-cdd4-bff9c9f7d830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:\n",
      "Avg loss               : 0.059207\n",
      "f1 macro averaged score: 0.620641\n",
      "Accuracy               : 62.0%\n",
      "Confusion matrix       :\n",
      "tensor([[163,  99,  32,   3],\n",
      "        [  2, 214,  61,  47],\n",
      "        [  1,  27, 309,  19],\n",
      "        [  5, 117, 110, 167]], device='cuda:0')\n",
      "CPU times: user 149 ms, sys: 4.47 ms, total: 153 ms\n",
      "Wall time: 184 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = test_convolutional_neural_network(test_dataloader, loss_function, cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J89-X8kB_P87"
   },
   "source": [
    "Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CkJ8Y_HB3qW"
   },
   "source": [
    "Initialize model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ummo7i74B-Eb",
    "outputId": "66ca08c9-6d7e-4f96-8dca-e4f0359cfef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (dense3): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (dense4): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Net().to(device)\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcMEQ3cHCBru"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "optimizer = SGD(params=cnn_model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTQSedTV_Rka",
    "outputId": "664872d4-b654-48a7-91e3-e27e7bf18436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 1.311169  [ 1424/ 3200]\n",
      "loss: 0.950143  [ 1440/ 3200]\n",
      "loss: 0.911601  [ 1456/ 3200]\n",
      "loss: 1.211442  [ 1472/ 3200]\n",
      "loss: 1.284917  [ 1488/ 3200]\n",
      "loss: 1.213743  [ 1504/ 3200]\n",
      "loss: 1.159529  [ 1520/ 3200]\n",
      "loss: 1.040693  [ 1536/ 3200]\n",
      "loss: 1.174513  [ 1552/ 3200]\n",
      "loss: 1.054895  [ 1568/ 3200]\n",
      "loss: 1.368127  [ 1584/ 3200]\n",
      "loss: 1.009333  [ 1600/ 3200]\n",
      "loss: 1.193585  [ 1616/ 3200]\n",
      "loss: 1.313507  [ 1632/ 3200]\n",
      "loss: 1.044938  [ 1648/ 3200]\n",
      "loss: 1.265589  [ 1664/ 3200]\n",
      "loss: 0.948670  [ 1680/ 3200]\n",
      "loss: 0.989446  [ 1696/ 3200]\n",
      "loss: 1.072894  [ 1712/ 3200]\n",
      "loss: 0.965374  [ 1728/ 3200]\n",
      "loss: 0.907669  [ 1744/ 3200]\n",
      "loss: 1.215414  [ 1760/ 3200]\n",
      "loss: 0.959877  [ 1776/ 3200]\n",
      "loss: 0.966269  [ 1792/ 3200]\n",
      "loss: 0.879849  [ 1808/ 3200]\n",
      "loss: 1.105601  [ 1824/ 3200]\n",
      "loss: 0.971323  [ 1840/ 3200]\n",
      "loss: 1.091528  [ 1856/ 3200]\n",
      "loss: 0.834543  [ 1872/ 3200]\n",
      "loss: 1.087493  [ 1888/ 3200]\n",
      "loss: 0.842174  [ 1904/ 3200]\n",
      "loss: 1.071032  [ 1920/ 3200]\n",
      "loss: 1.202731  [ 1936/ 3200]\n",
      "loss: 1.038024  [ 1952/ 3200]\n",
      "loss: 1.158259  [ 1968/ 3200]\n",
      "loss: 0.977654  [ 1984/ 3200]\n",
      "loss: 0.855460  [ 2000/ 3200]\n",
      "loss: 1.371108  [ 2016/ 3200]\n",
      "loss: 1.225818  [ 2032/ 3200]\n",
      "loss: 0.884278  [ 2048/ 3200]\n",
      "loss: 0.999968  [ 2064/ 3200]\n",
      "loss: 1.142059  [ 2080/ 3200]\n",
      "loss: 1.189297  [ 2096/ 3200]\n",
      "loss: 1.034256  [ 2112/ 3200]\n",
      "loss: 0.986925  [ 2128/ 3200]\n",
      "loss: 0.923309  [ 2144/ 3200]\n",
      "loss: 1.029945  [ 2160/ 3200]\n",
      "loss: 1.061287  [ 2176/ 3200]\n",
      "loss: 0.648841  [ 2192/ 3200]\n",
      "loss: 1.030097  [ 2208/ 3200]\n",
      "loss: 1.365796  [ 2224/ 3200]\n",
      "loss: 0.967493  [ 2240/ 3200]\n",
      "loss: 1.216336  [ 2256/ 3200]\n",
      "loss: 0.970889  [ 2272/ 3200]\n",
      "loss: 0.770021  [ 2288/ 3200]\n",
      "loss: 1.111390  [ 2304/ 3200]\n",
      "loss: 1.129107  [ 2320/ 3200]\n",
      "loss: 1.011771  [ 2336/ 3200]\n",
      "loss: 1.177818  [ 2352/ 3200]\n",
      "loss: 1.142165  [ 2368/ 3200]\n",
      "loss: 1.152537  [ 2384/ 3200]\n",
      "loss: 0.918953  [ 2400/ 3200]\n",
      "loss: 1.109021  [ 2416/ 3200]\n",
      "loss: 1.059405  [ 2432/ 3200]\n",
      "loss: 0.988737  [ 2448/ 3200]\n",
      "loss: 1.377581  [ 2464/ 3200]\n",
      "loss: 0.988915  [ 2480/ 3200]\n",
      "loss: 0.816038  [ 2496/ 3200]\n",
      "loss: 1.290625  [ 2512/ 3200]\n",
      "loss: 1.289266  [ 2528/ 3200]\n",
      "loss: 0.896354  [ 2544/ 3200]\n",
      "loss: 1.310305  [ 2560/ 3200]\n",
      "loss: 1.115058  [ 2576/ 3200]\n",
      "loss: 1.262133  [ 2592/ 3200]\n",
      "loss: 1.283890  [ 2608/ 3200]\n",
      "loss: 0.857764  [ 2624/ 3200]\n",
      "loss: 1.327465  [ 2640/ 3200]\n",
      "loss: 1.328607  [ 2656/ 3200]\n",
      "loss: 0.860596  [ 2672/ 3200]\n",
      "loss: 1.043913  [ 2688/ 3200]\n",
      "loss: 0.892500  [ 2704/ 3200]\n",
      "loss: 0.973533  [ 2720/ 3200]\n",
      "loss: 1.042538  [ 2736/ 3200]\n",
      "loss: 1.260781  [ 2752/ 3200]\n",
      "loss: 0.946487  [ 2768/ 3200]\n",
      "loss: 1.367916  [ 2784/ 3200]\n",
      "loss: 1.085212  [ 2800/ 3200]\n",
      "loss: 1.035182  [ 2816/ 3200]\n",
      "loss: 1.319870  [ 2832/ 3200]\n",
      "loss: 1.246622  [ 2848/ 3200]\n",
      "loss: 1.142167  [ 2864/ 3200]\n",
      "loss: 0.821517  [ 2880/ 3200]\n",
      "loss: 0.856352  [ 2896/ 3200]\n",
      "loss: 0.927821  [ 2912/ 3200]\n",
      "loss: 1.345543  [ 2928/ 3200]\n",
      "loss: 0.990734  [ 2944/ 3200]\n",
      "loss: 0.929774  [ 2960/ 3200]\n",
      "loss: 1.206038  [ 2976/ 3200]\n",
      "loss: 1.217875  [ 2992/ 3200]\n",
      "loss: 1.004152  [ 3008/ 3200]\n",
      "loss: 1.065980  [ 3024/ 3200]\n",
      "loss: 1.037652  [ 3040/ 3200]\n",
      "loss: 0.981573  [ 3056/ 3200]\n",
      "loss: 0.886786  [ 3072/ 3200]\n",
      "loss: 1.048207  [ 3088/ 3200]\n",
      "loss: 0.819595  [ 3104/ 3200]\n",
      "loss: 0.839173  [ 3120/ 3200]\n",
      "loss: 1.006180  [ 3136/ 3200]\n",
      "loss: 1.293869  [ 3152/ 3200]\n",
      "loss: 1.054292  [ 3168/ 3200]\n",
      "loss: 0.867718  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.071281\n",
      "f1 macro averaged score: 0.388203\n",
      "Accuracy               : 46.5%\n",
      "Confusion matrix       :\n",
      "tensor([[188,  11,   0,   1],\n",
      "        [ 99,  69,  31,   1],\n",
      "        [ 42,  45, 113,   0],\n",
      "        [ 36,  70,  92,   2]], device='cuda:0')\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 0.912948  [    0/ 3200]\n",
      "loss: 1.186934  [   16/ 3200]\n",
      "loss: 1.191917  [   32/ 3200]\n",
      "loss: 0.764150  [   48/ 3200]\n",
      "loss: 1.127881  [   64/ 3200]\n",
      "loss: 0.717016  [   80/ 3200]\n",
      "loss: 1.033279  [   96/ 3200]\n",
      "loss: 1.133808  [  112/ 3200]\n",
      "loss: 1.332701  [  128/ 3200]\n",
      "loss: 1.001612  [  144/ 3200]\n",
      "loss: 1.192001  [  160/ 3200]\n",
      "loss: 0.837971  [  176/ 3200]\n",
      "loss: 1.036256  [  192/ 3200]\n",
      "loss: 1.287851  [  208/ 3200]\n",
      "loss: 1.065132  [  224/ 3200]\n",
      "loss: 0.960466  [  240/ 3200]\n",
      "loss: 0.921505  [  256/ 3200]\n",
      "loss: 1.101182  [  272/ 3200]\n",
      "loss: 0.949326  [  288/ 3200]\n",
      "loss: 1.208971  [  304/ 3200]\n",
      "loss: 0.961329  [  320/ 3200]\n",
      "loss: 1.175054  [  336/ 3200]\n",
      "loss: 1.007440  [  352/ 3200]\n",
      "loss: 0.986677  [  368/ 3200]\n",
      "loss: 1.023615  [  384/ 3200]\n",
      "loss: 0.823147  [  400/ 3200]\n",
      "loss: 0.926164  [  416/ 3200]\n",
      "loss: 1.258620  [  432/ 3200]\n",
      "loss: 1.033432  [  448/ 3200]\n",
      "loss: 1.478035  [  464/ 3200]\n",
      "loss: 1.088977  [  480/ 3200]\n",
      "loss: 0.818717  [  496/ 3200]\n",
      "loss: 1.283970  [  512/ 3200]\n",
      "loss: 1.021223  [  528/ 3200]\n",
      "loss: 1.095123  [  544/ 3200]\n",
      "loss: 1.174969  [  560/ 3200]\n",
      "loss: 0.999420  [  576/ 3200]\n",
      "loss: 0.860900  [  592/ 3200]\n",
      "loss: 0.985371  [  608/ 3200]\n",
      "loss: 1.170440  [  624/ 3200]\n",
      "loss: 0.989110  [  640/ 3200]\n",
      "loss: 1.151706  [  656/ 3200]\n",
      "loss: 0.843978  [  672/ 3200]\n",
      "loss: 1.157963  [  688/ 3200]\n",
      "loss: 1.155513  [  704/ 3200]\n",
      "loss: 0.951166  [  720/ 3200]\n",
      "loss: 0.919493  [  736/ 3200]\n",
      "loss: 1.347252  [  752/ 3200]\n",
      "loss: 1.312626  [  768/ 3200]\n",
      "loss: 1.116328  [  784/ 3200]\n",
      "loss: 0.761756  [  800/ 3200]\n",
      "loss: 0.899325  [  816/ 3200]\n",
      "loss: 1.439355  [  832/ 3200]\n",
      "loss: 1.029022  [  848/ 3200]\n",
      "loss: 1.171664  [  864/ 3200]\n",
      "loss: 1.317767  [  880/ 3200]\n",
      "loss: 1.002303  [  896/ 3200]\n",
      "loss: 0.817937  [  912/ 3200]\n",
      "loss: 0.910885  [  928/ 3200]\n",
      "loss: 0.800238  [  944/ 3200]\n",
      "loss: 1.230455  [  960/ 3200]\n",
      "loss: 0.720563  [  976/ 3200]\n",
      "loss: 1.095675  [  992/ 3200]\n",
      "loss: 1.199094  [ 1008/ 3200]\n",
      "loss: 1.273585  [ 1024/ 3200]\n",
      "loss: 1.013028  [ 1040/ 3200]\n",
      "loss: 0.852656  [ 1056/ 3200]\n",
      "loss: 1.178779  [ 1072/ 3200]\n",
      "loss: 1.214930  [ 1088/ 3200]\n",
      "loss: 1.077885  [ 1104/ 3200]\n",
      "loss: 1.029207  [ 1120/ 3200]\n",
      "loss: 1.189405  [ 1136/ 3200]\n",
      "loss: 1.084170  [ 1152/ 3200]\n",
      "loss: 1.151885  [ 1168/ 3200]\n",
      "loss: 1.022900  [ 1184/ 3200]\n",
      "loss: 0.819178  [ 1200/ 3200]\n",
      "loss: 1.389017  [ 1216/ 3200]\n",
      "loss: 1.152026  [ 1232/ 3200]\n",
      "loss: 0.855016  [ 1248/ 3200]\n",
      "loss: 0.936592  [ 1264/ 3200]\n",
      "loss: 1.238781  [ 1280/ 3200]\n",
      "loss: 1.052503  [ 1296/ 3200]\n",
      "loss: 1.039699  [ 1312/ 3200]\n",
      "loss: 1.182396  [ 1328/ 3200]\n",
      "loss: 1.414354  [ 1344/ 3200]\n",
      "loss: 1.112896  [ 1360/ 3200]\n",
      "loss: 1.091505  [ 1376/ 3200]\n",
      "loss: 1.048421  [ 1392/ 3200]\n",
      "loss: 0.829728  [ 1408/ 3200]\n",
      "loss: 0.959583  [ 1424/ 3200]\n",
      "loss: 1.116637  [ 1440/ 3200]\n",
      "loss: 1.010495  [ 1456/ 3200]\n",
      "loss: 0.929092  [ 1472/ 3200]\n",
      "loss: 1.213442  [ 1488/ 3200]\n",
      "loss: 0.945294  [ 1504/ 3200]\n",
      "loss: 0.937934  [ 1520/ 3200]\n",
      "loss: 0.998928  [ 1536/ 3200]\n",
      "loss: 1.332305  [ 1552/ 3200]\n",
      "loss: 1.012379  [ 1568/ 3200]\n",
      "loss: 1.055999  [ 1584/ 3200]\n",
      "loss: 1.060911  [ 1600/ 3200]\n",
      "loss: 1.002207  [ 1616/ 3200]\n",
      "loss: 1.145880  [ 1632/ 3200]\n",
      "loss: 1.094975  [ 1648/ 3200]\n",
      "loss: 1.292295  [ 1664/ 3200]\n",
      "loss: 0.716049  [ 1680/ 3200]\n",
      "loss: 1.138515  [ 1696/ 3200]\n",
      "loss: 1.205721  [ 1712/ 3200]\n",
      "loss: 1.078926  [ 1728/ 3200]\n",
      "loss: 1.141375  [ 1744/ 3200]\n",
      "loss: 1.139576  [ 1760/ 3200]\n",
      "loss: 1.031058  [ 1776/ 3200]\n",
      "loss: 0.978792  [ 1792/ 3200]\n",
      "loss: 0.848379  [ 1808/ 3200]\n",
      "loss: 0.935439  [ 1824/ 3200]\n",
      "loss: 0.995556  [ 1840/ 3200]\n",
      "loss: 1.035809  [ 1856/ 3200]\n",
      "loss: 1.000771  [ 1872/ 3200]\n",
      "loss: 0.959765  [ 1888/ 3200]\n",
      "loss: 1.030606  [ 1904/ 3200]\n",
      "loss: 1.055224  [ 1920/ 3200]\n",
      "loss: 0.839953  [ 1936/ 3200]\n",
      "loss: 0.973563  [ 1952/ 3200]\n",
      "loss: 0.921001  [ 1968/ 3200]\n",
      "loss: 1.200609  [ 1984/ 3200]\n",
      "loss: 1.117973  [ 2000/ 3200]\n",
      "loss: 0.917593  [ 2016/ 3200]\n",
      "loss: 0.833173  [ 2032/ 3200]\n",
      "loss: 0.931161  [ 2048/ 3200]\n",
      "loss: 0.963335  [ 2064/ 3200]\n",
      "loss: 1.150384  [ 2080/ 3200]\n",
      "loss: 1.202076  [ 2096/ 3200]\n",
      "loss: 1.186828  [ 2112/ 3200]\n",
      "loss: 1.015692  [ 2128/ 3200]\n",
      "loss: 1.050014  [ 2144/ 3200]\n",
      "loss: 0.988305  [ 2160/ 3200]\n",
      "loss: 0.893521  [ 2176/ 3200]\n",
      "loss: 0.914712  [ 2192/ 3200]\n",
      "loss: 1.252960  [ 2208/ 3200]\n",
      "loss: 1.395232  [ 2224/ 3200]\n",
      "loss: 0.798988  [ 2240/ 3200]\n",
      "loss: 0.839628  [ 2256/ 3200]\n",
      "loss: 1.270861  [ 2272/ 3200]\n",
      "loss: 1.035868  [ 2288/ 3200]\n",
      "loss: 0.900538  [ 2304/ 3200]\n",
      "loss: 1.051525  [ 2320/ 3200]\n",
      "loss: 1.121361  [ 2336/ 3200]\n",
      "loss: 1.320706  [ 2352/ 3200]\n",
      "loss: 1.121856  [ 2368/ 3200]\n",
      "loss: 0.919290  [ 2384/ 3200]\n",
      "loss: 1.274483  [ 2400/ 3200]\n",
      "loss: 1.084291  [ 2416/ 3200]\n",
      "loss: 0.770824  [ 2432/ 3200]\n",
      "loss: 1.071751  [ 2448/ 3200]\n",
      "loss: 0.894915  [ 2464/ 3200]\n",
      "loss: 1.103713  [ 2480/ 3200]\n",
      "loss: 0.785776  [ 2496/ 3200]\n",
      "loss: 1.067502  [ 2512/ 3200]\n",
      "loss: 1.008521  [ 2528/ 3200]\n",
      "loss: 1.040114  [ 2544/ 3200]\n",
      "loss: 1.002376  [ 2560/ 3200]\n",
      "loss: 1.091946  [ 2576/ 3200]\n",
      "loss: 0.814205  [ 2592/ 3200]\n",
      "loss: 0.846671  [ 2608/ 3200]\n",
      "loss: 1.053440  [ 2624/ 3200]\n",
      "loss: 1.062645  [ 2640/ 3200]\n",
      "loss: 0.983653  [ 2656/ 3200]\n",
      "loss: 1.082965  [ 2672/ 3200]\n",
      "loss: 0.956241  [ 2688/ 3200]\n",
      "loss: 1.148505  [ 2704/ 3200]\n",
      "loss: 1.045171  [ 2720/ 3200]\n",
      "loss: 0.790841  [ 2736/ 3200]\n",
      "loss: 0.977517  [ 2752/ 3200]\n",
      "loss: 1.189144  [ 2768/ 3200]\n",
      "loss: 0.919330  [ 2784/ 3200]\n",
      "loss: 0.984962  [ 2800/ 3200]\n",
      "loss: 1.196681  [ 2816/ 3200]\n",
      "loss: 0.915849  [ 2832/ 3200]\n",
      "loss: 0.956762  [ 2848/ 3200]\n",
      "loss: 0.814978  [ 2864/ 3200]\n",
      "loss: 0.803030  [ 2880/ 3200]\n",
      "loss: 0.880966  [ 2896/ 3200]\n",
      "loss: 1.216099  [ 2912/ 3200]\n",
      "loss: 0.819457  [ 2928/ 3200]\n",
      "loss: 1.038773  [ 2944/ 3200]\n",
      "loss: 1.037853  [ 2960/ 3200]\n",
      "loss: 1.133726  [ 2976/ 3200]\n",
      "loss: 0.959495  [ 2992/ 3200]\n",
      "loss: 0.935911  [ 3008/ 3200]\n",
      "loss: 0.845000  [ 3024/ 3200]\n",
      "loss: 0.947481  [ 3040/ 3200]\n",
      "loss: 0.724230  [ 3056/ 3200]\n",
      "loss: 1.008352  [ 3072/ 3200]\n",
      "loss: 0.847370  [ 3088/ 3200]\n",
      "loss: 1.088333  [ 3104/ 3200]\n",
      "loss: 1.638353  [ 3120/ 3200]\n",
      "loss: 1.403821  [ 3136/ 3200]\n",
      "loss: 1.062251  [ 3152/ 3200]\n",
      "loss: 1.112570  [ 3168/ 3200]\n",
      "loss: 0.942426  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.065009\n",
      "f1 macro averaged score: 0.522625\n",
      "Accuracy               : 52.8%\n",
      "Confusion matrix       :\n",
      "tensor([[132,  54,  12,   2],\n",
      "        [ 38,  78,  57,  27],\n",
      "        [  7,  25, 142,  26],\n",
      "        [  2,  46,  82,  70]], device='cuda:0')\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 0.967541  [    0/ 3200]\n",
      "loss: 0.783181  [   16/ 3200]\n",
      "loss: 1.325101  [   32/ 3200]\n",
      "loss: 0.947570  [   48/ 3200]\n",
      "loss: 0.858059  [   64/ 3200]\n",
      "loss: 1.033434  [   80/ 3200]\n",
      "loss: 0.934514  [   96/ 3200]\n",
      "loss: 0.787635  [  112/ 3200]\n",
      "loss: 1.200971  [  128/ 3200]\n",
      "loss: 1.138047  [  144/ 3200]\n",
      "loss: 1.172455  [  160/ 3200]\n",
      "loss: 1.156389  [  176/ 3200]\n",
      "loss: 1.315705  [  192/ 3200]\n",
      "loss: 0.946941  [  208/ 3200]\n",
      "loss: 1.052047  [  224/ 3200]\n",
      "loss: 1.166422  [  240/ 3200]\n",
      "loss: 1.185460  [  256/ 3200]\n",
      "loss: 0.948776  [  272/ 3200]\n",
      "loss: 1.221690  [  288/ 3200]\n",
      "loss: 0.942852  [  304/ 3200]\n",
      "loss: 0.992720  [  320/ 3200]\n",
      "loss: 0.870417  [  336/ 3200]\n",
      "loss: 0.992699  [  352/ 3200]\n",
      "loss: 0.897278  [  368/ 3200]\n",
      "loss: 0.773238  [  384/ 3200]\n",
      "loss: 0.834696  [  400/ 3200]\n",
      "loss: 0.997830  [  416/ 3200]\n",
      "loss: 0.954131  [  432/ 3200]\n",
      "loss: 0.794856  [  448/ 3200]\n",
      "loss: 1.051335  [  464/ 3200]\n",
      "loss: 0.874905  [  480/ 3200]\n",
      "loss: 1.150173  [  496/ 3200]\n",
      "loss: 0.877515  [  512/ 3200]\n",
      "loss: 1.425625  [  528/ 3200]\n",
      "loss: 0.870575  [  544/ 3200]\n",
      "loss: 1.158181  [  560/ 3200]\n",
      "loss: 1.047499  [  576/ 3200]\n",
      "loss: 1.121048  [  592/ 3200]\n",
      "loss: 0.913555  [  608/ 3200]\n",
      "loss: 1.136122  [  624/ 3200]\n",
      "loss: 0.786361  [  640/ 3200]\n",
      "loss: 0.973718  [  656/ 3200]\n",
      "loss: 1.009104  [  672/ 3200]\n",
      "loss: 0.942587  [  688/ 3200]\n",
      "loss: 0.876289  [  704/ 3200]\n",
      "loss: 1.047949  [  720/ 3200]\n",
      "loss: 0.856988  [  736/ 3200]\n",
      "loss: 1.361531  [  752/ 3200]\n",
      "loss: 1.156396  [  768/ 3200]\n",
      "loss: 1.304943  [  784/ 3200]\n",
      "loss: 0.756831  [  800/ 3200]\n",
      "loss: 1.024628  [  816/ 3200]\n",
      "loss: 0.982994  [  832/ 3200]\n",
      "loss: 0.854454  [  848/ 3200]\n",
      "loss: 0.944048  [  864/ 3200]\n",
      "loss: 1.188488  [  880/ 3200]\n",
      "loss: 1.228864  [  896/ 3200]\n",
      "loss: 1.043329  [  912/ 3200]\n",
      "loss: 1.045462  [  928/ 3200]\n",
      "loss: 0.991372  [  944/ 3200]\n",
      "loss: 1.181146  [  960/ 3200]\n",
      "loss: 1.010506  [  976/ 3200]\n",
      "loss: 1.152327  [  992/ 3200]\n",
      "loss: 1.167184  [ 1008/ 3200]\n",
      "loss: 1.004374  [ 1024/ 3200]\n",
      "loss: 0.772833  [ 1040/ 3200]\n",
      "loss: 1.421281  [ 1056/ 3200]\n",
      "loss: 0.858981  [ 1072/ 3200]\n",
      "loss: 0.964851  [ 1088/ 3200]\n",
      "loss: 0.949856  [ 1104/ 3200]\n",
      "loss: 1.147289  [ 1120/ 3200]\n",
      "loss: 0.958474  [ 1136/ 3200]\n",
      "loss: 0.687222  [ 1152/ 3200]\n",
      "loss: 0.969186  [ 1168/ 3200]\n",
      "loss: 1.149250  [ 1184/ 3200]\n",
      "loss: 1.178001  [ 1200/ 3200]\n",
      "loss: 1.017041  [ 1216/ 3200]\n",
      "loss: 1.362548  [ 1232/ 3200]\n",
      "loss: 0.787614  [ 1248/ 3200]\n",
      "loss: 1.083448  [ 1264/ 3200]\n",
      "loss: 0.960119  [ 1280/ 3200]\n",
      "loss: 0.817804  [ 1296/ 3200]\n",
      "loss: 0.921974  [ 1312/ 3200]\n",
      "loss: 1.243437  [ 1328/ 3200]\n",
      "loss: 0.761392  [ 1344/ 3200]\n",
      "loss: 0.929487  [ 1360/ 3200]\n",
      "loss: 0.992132  [ 1376/ 3200]\n",
      "loss: 0.949450  [ 1392/ 3200]\n",
      "loss: 0.976848  [ 1408/ 3200]\n",
      "loss: 0.828728  [ 1424/ 3200]\n",
      "loss: 1.011562  [ 1440/ 3200]\n",
      "loss: 0.956389  [ 1456/ 3200]\n",
      "loss: 1.271501  [ 1472/ 3200]\n",
      "loss: 1.116336  [ 1488/ 3200]\n",
      "loss: 1.113110  [ 1504/ 3200]\n",
      "loss: 1.211164  [ 1520/ 3200]\n",
      "loss: 0.936055  [ 1536/ 3200]\n",
      "loss: 1.352479  [ 1552/ 3200]\n",
      "loss: 1.046424  [ 1568/ 3200]\n",
      "loss: 0.716150  [ 1584/ 3200]\n",
      "loss: 1.061516  [ 1600/ 3200]\n",
      "loss: 0.761424  [ 1616/ 3200]\n",
      "loss: 1.404375  [ 1632/ 3200]\n",
      "loss: 0.886893  [ 1648/ 3200]\n",
      "loss: 1.016490  [ 1664/ 3200]\n",
      "loss: 1.148653  [ 1680/ 3200]\n",
      "loss: 1.050790  [ 1696/ 3200]\n",
      "loss: 0.858684  [ 1712/ 3200]\n",
      "loss: 0.840291  [ 1728/ 3200]\n",
      "loss: 1.049206  [ 1744/ 3200]\n",
      "loss: 1.041341  [ 1760/ 3200]\n",
      "loss: 0.865162  [ 1776/ 3200]\n",
      "loss: 1.172204  [ 1792/ 3200]\n",
      "loss: 0.887531  [ 1808/ 3200]\n",
      "loss: 1.080657  [ 1824/ 3200]\n",
      "loss: 0.765873  [ 1840/ 3200]\n",
      "loss: 0.760052  [ 1856/ 3200]\n",
      "loss: 0.873106  [ 1872/ 3200]\n",
      "loss: 1.064085  [ 1888/ 3200]\n",
      "loss: 1.023362  [ 1904/ 3200]\n",
      "loss: 1.117787  [ 1920/ 3200]\n",
      "loss: 0.748073  [ 1936/ 3200]\n",
      "loss: 1.110477  [ 1952/ 3200]\n",
      "loss: 0.745266  [ 1968/ 3200]\n",
      "loss: 0.992725  [ 1984/ 3200]\n",
      "loss: 0.989143  [ 2000/ 3200]\n",
      "loss: 0.969154  [ 2016/ 3200]\n",
      "loss: 1.247452  [ 2032/ 3200]\n",
      "loss: 1.053357  [ 2048/ 3200]\n",
      "loss: 1.039941  [ 2064/ 3200]\n",
      "loss: 1.134106  [ 2080/ 3200]\n",
      "loss: 0.860371  [ 2096/ 3200]\n",
      "loss: 1.160916  [ 2112/ 3200]\n",
      "loss: 1.318216  [ 2128/ 3200]\n",
      "loss: 1.160101  [ 2144/ 3200]\n",
      "loss: 1.099265  [ 2160/ 3200]\n",
      "loss: 0.848239  [ 2176/ 3200]\n",
      "loss: 0.645775  [ 2192/ 3200]\n",
      "loss: 0.911133  [ 2208/ 3200]\n",
      "loss: 0.779676  [ 2224/ 3200]\n",
      "loss: 0.734625  [ 2240/ 3200]\n",
      "loss: 0.924376  [ 2256/ 3200]\n",
      "loss: 1.073574  [ 2272/ 3200]\n",
      "loss: 0.993643  [ 2288/ 3200]\n",
      "loss: 0.974153  [ 2304/ 3200]\n",
      "loss: 1.108318  [ 2320/ 3200]\n",
      "loss: 0.986126  [ 2336/ 3200]\n",
      "loss: 1.020268  [ 2352/ 3200]\n",
      "loss: 1.101061  [ 2368/ 3200]\n",
      "loss: 1.194457  [ 2384/ 3200]\n",
      "loss: 1.019423  [ 2400/ 3200]\n",
      "loss: 0.822399  [ 2416/ 3200]\n",
      "loss: 0.703589  [ 2432/ 3200]\n",
      "loss: 0.810139  [ 2448/ 3200]\n",
      "loss: 0.979983  [ 2464/ 3200]\n",
      "loss: 1.012274  [ 2480/ 3200]\n",
      "loss: 0.677898  [ 2496/ 3200]\n",
      "loss: 1.066632  [ 2512/ 3200]\n",
      "loss: 1.367350  [ 2528/ 3200]\n",
      "loss: 0.851380  [ 2544/ 3200]\n",
      "loss: 0.890551  [ 2560/ 3200]\n",
      "loss: 1.193596  [ 2576/ 3200]\n",
      "loss: 0.789417  [ 2592/ 3200]\n",
      "loss: 0.921693  [ 2608/ 3200]\n",
      "loss: 1.060131  [ 2624/ 3200]\n",
      "loss: 0.916388  [ 2640/ 3200]\n",
      "loss: 1.124744  [ 2656/ 3200]\n",
      "loss: 0.878845  [ 2672/ 3200]\n",
      "loss: 0.936914  [ 2688/ 3200]\n",
      "loss: 0.893553  [ 2704/ 3200]\n",
      "loss: 0.939418  [ 2720/ 3200]\n",
      "loss: 0.931980  [ 2736/ 3200]\n",
      "loss: 1.129060  [ 2752/ 3200]\n",
      "loss: 1.341449  [ 2768/ 3200]\n",
      "loss: 0.938836  [ 2784/ 3200]\n",
      "loss: 0.975210  [ 2800/ 3200]\n",
      "loss: 0.960899  [ 2816/ 3200]\n",
      "loss: 0.918518  [ 2832/ 3200]\n",
      "loss: 0.678969  [ 2848/ 3200]\n",
      "loss: 0.891029  [ 2864/ 3200]\n",
      "loss: 1.097023  [ 2880/ 3200]\n",
      "loss: 1.145639  [ 2896/ 3200]\n",
      "loss: 1.208686  [ 2912/ 3200]\n",
      "loss: 0.745239  [ 2928/ 3200]\n",
      "loss: 1.136029  [ 2944/ 3200]\n",
      "loss: 0.953319  [ 2960/ 3200]\n",
      "loss: 0.999868  [ 2976/ 3200]\n",
      "loss: 1.048913  [ 2992/ 3200]\n",
      "loss: 0.672490  [ 3008/ 3200]\n",
      "loss: 0.919643  [ 3024/ 3200]\n",
      "loss: 1.094800  [ 3040/ 3200]\n",
      "loss: 0.741011  [ 3056/ 3200]\n",
      "loss: 1.051121  [ 3072/ 3200]\n",
      "loss: 1.218856  [ 3088/ 3200]\n",
      "loss: 0.727121  [ 3104/ 3200]\n",
      "loss: 0.961432  [ 3120/ 3200]\n",
      "loss: 0.940946  [ 3136/ 3200]\n",
      "loss: 1.208943  [ 3152/ 3200]\n",
      "loss: 1.143902  [ 3168/ 3200]\n",
      "loss: 1.530409  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.064816\n",
      "f1 macro averaged score: 0.513059\n",
      "Accuracy               : 53.6%\n",
      "Confusion matrix       :\n",
      "tensor([[152,  22,   8,  18],\n",
      "        [ 48,  38,  38,  76],\n",
      "        [  9,  10,  93,  88],\n",
      "        [  7,  16,  31, 146]], device='cuda:0')\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 1.072316  [    0/ 3200]\n",
      "loss: 1.446302  [   16/ 3200]\n",
      "loss: 1.179662  [   32/ 3200]\n",
      "loss: 1.082071  [   48/ 3200]\n",
      "loss: 0.772637  [   64/ 3200]\n",
      "loss: 0.978868  [   80/ 3200]\n",
      "loss: 0.795224  [   96/ 3200]\n",
      "loss: 1.063653  [  112/ 3200]\n",
      "loss: 0.684726  [  128/ 3200]\n",
      "loss: 0.847508  [  144/ 3200]\n",
      "loss: 1.065156  [  160/ 3200]\n",
      "loss: 0.919411  [  176/ 3200]\n",
      "loss: 0.952129  [  192/ 3200]\n",
      "loss: 1.348411  [  208/ 3200]\n",
      "loss: 0.853246  [  224/ 3200]\n",
      "loss: 1.184993  [  240/ 3200]\n",
      "loss: 0.721309  [  256/ 3200]\n",
      "loss: 0.922473  [  272/ 3200]\n",
      "loss: 0.900898  [  288/ 3200]\n",
      "loss: 0.914731  [  304/ 3200]\n",
      "loss: 0.829407  [  320/ 3200]\n",
      "loss: 1.097419  [  336/ 3200]\n",
      "loss: 0.866076  [  352/ 3200]\n",
      "loss: 1.161357  [  368/ 3200]\n",
      "loss: 0.937704  [  384/ 3200]\n",
      "loss: 0.956206  [  400/ 3200]\n",
      "loss: 0.952411  [  416/ 3200]\n",
      "loss: 0.935606  [  432/ 3200]\n",
      "loss: 0.798928  [  448/ 3200]\n",
      "loss: 0.821691  [  464/ 3200]\n",
      "loss: 1.000075  [  480/ 3200]\n",
      "loss: 0.840692  [  496/ 3200]\n",
      "loss: 1.439942  [  512/ 3200]\n",
      "loss: 1.084367  [  528/ 3200]\n",
      "loss: 0.910081  [  544/ 3200]\n",
      "loss: 0.963513  [  560/ 3200]\n",
      "loss: 0.921780  [  576/ 3200]\n",
      "loss: 1.118879  [  592/ 3200]\n",
      "loss: 0.897590  [  608/ 3200]\n",
      "loss: 0.848936  [  624/ 3200]\n",
      "loss: 1.277791  [  640/ 3200]\n",
      "loss: 1.061315  [  656/ 3200]\n",
      "loss: 0.725070  [  672/ 3200]\n",
      "loss: 1.136337  [  688/ 3200]\n",
      "loss: 1.140860  [  704/ 3200]\n",
      "loss: 1.350120  [  720/ 3200]\n",
      "loss: 0.746168  [  736/ 3200]\n",
      "loss: 0.991900  [  752/ 3200]\n",
      "loss: 1.015117  [  768/ 3200]\n",
      "loss: 1.044199  [  784/ 3200]\n",
      "loss: 0.747343  [  800/ 3200]\n",
      "loss: 0.975634  [  816/ 3200]\n",
      "loss: 1.202203  [  832/ 3200]\n",
      "loss: 0.644246  [  848/ 3200]\n",
      "loss: 1.152917  [  864/ 3200]\n",
      "loss: 0.869981  [  880/ 3200]\n",
      "loss: 1.191710  [  896/ 3200]\n",
      "loss: 0.899012  [  912/ 3200]\n",
      "loss: 0.704386  [  928/ 3200]\n",
      "loss: 1.071497  [  944/ 3200]\n",
      "loss: 0.918497  [  960/ 3200]\n",
      "loss: 0.679060  [  976/ 3200]\n",
      "loss: 0.935930  [  992/ 3200]\n",
      "loss: 0.917072  [ 1008/ 3200]\n",
      "loss: 0.788312  [ 1024/ 3200]\n",
      "loss: 1.178097  [ 1040/ 3200]\n",
      "loss: 0.780849  [ 1056/ 3200]\n",
      "loss: 1.486965  [ 1072/ 3200]\n",
      "loss: 0.675050  [ 1088/ 3200]\n",
      "loss: 1.135064  [ 1104/ 3200]\n",
      "loss: 0.951113  [ 1120/ 3200]\n",
      "loss: 0.891112  [ 1136/ 3200]\n",
      "loss: 0.804655  [ 1152/ 3200]\n",
      "loss: 1.407336  [ 1168/ 3200]\n",
      "loss: 1.210851  [ 1184/ 3200]\n",
      "loss: 0.998564  [ 1200/ 3200]\n",
      "loss: 1.021886  [ 1216/ 3200]\n",
      "loss: 1.445167  [ 1232/ 3200]\n",
      "loss: 0.839889  [ 1248/ 3200]\n",
      "loss: 0.774006  [ 1264/ 3200]\n",
      "loss: 0.975490  [ 1280/ 3200]\n",
      "loss: 0.800922  [ 1296/ 3200]\n",
      "loss: 0.924527  [ 1312/ 3200]\n",
      "loss: 1.247652  [ 1328/ 3200]\n",
      "loss: 1.081229  [ 1344/ 3200]\n",
      "loss: 0.924958  [ 1360/ 3200]\n",
      "loss: 0.886823  [ 1376/ 3200]\n",
      "loss: 0.768774  [ 1392/ 3200]\n",
      "loss: 1.034922  [ 1408/ 3200]\n",
      "loss: 0.778120  [ 1424/ 3200]\n",
      "loss: 0.944275  [ 1440/ 3200]\n",
      "loss: 0.978544  [ 1456/ 3200]\n",
      "loss: 1.024994  [ 1472/ 3200]\n",
      "loss: 1.033226  [ 1488/ 3200]\n",
      "loss: 1.407367  [ 1504/ 3200]\n",
      "loss: 0.818689  [ 1520/ 3200]\n",
      "loss: 1.247663  [ 1536/ 3200]\n",
      "loss: 0.851391  [ 1552/ 3200]\n",
      "loss: 1.271505  [ 1568/ 3200]\n",
      "loss: 1.176089  [ 1584/ 3200]\n",
      "loss: 0.908126  [ 1600/ 3200]\n",
      "loss: 0.794650  [ 1616/ 3200]\n",
      "loss: 0.998496  [ 1632/ 3200]\n",
      "loss: 0.795843  [ 1648/ 3200]\n",
      "loss: 0.818069  [ 1664/ 3200]\n",
      "loss: 0.963294  [ 1680/ 3200]\n",
      "loss: 0.863489  [ 1696/ 3200]\n",
      "loss: 1.225599  [ 1712/ 3200]\n",
      "loss: 0.794799  [ 1728/ 3200]\n",
      "loss: 1.086248  [ 1744/ 3200]\n",
      "loss: 0.965815  [ 1760/ 3200]\n",
      "loss: 0.942816  [ 1776/ 3200]\n",
      "loss: 0.970821  [ 1792/ 3200]\n",
      "loss: 0.837682  [ 1808/ 3200]\n",
      "loss: 1.278675  [ 1824/ 3200]\n",
      "loss: 0.929877  [ 1840/ 3200]\n",
      "loss: 0.890699  [ 1856/ 3200]\n",
      "loss: 0.873320  [ 1872/ 3200]\n",
      "loss: 0.717313  [ 1888/ 3200]\n",
      "loss: 1.292353  [ 1904/ 3200]\n",
      "loss: 0.960540  [ 1920/ 3200]\n",
      "loss: 0.740460  [ 1936/ 3200]\n",
      "loss: 1.034767  [ 1952/ 3200]\n",
      "loss: 1.042027  [ 1968/ 3200]\n",
      "loss: 0.869813  [ 1984/ 3200]\n",
      "loss: 1.150443  [ 2000/ 3200]\n",
      "loss: 0.731250  [ 2016/ 3200]\n",
      "loss: 0.715663  [ 2032/ 3200]\n",
      "loss: 0.703015  [ 2048/ 3200]\n",
      "loss: 0.780720  [ 2064/ 3200]\n",
      "loss: 1.101289  [ 2080/ 3200]\n",
      "loss: 0.877200  [ 2096/ 3200]\n",
      "loss: 0.841742  [ 2112/ 3200]\n",
      "loss: 0.752154  [ 2128/ 3200]\n",
      "loss: 0.888886  [ 2144/ 3200]\n",
      "loss: 0.886171  [ 2160/ 3200]\n",
      "loss: 1.114518  [ 2176/ 3200]\n",
      "loss: 0.971346  [ 2192/ 3200]\n",
      "loss: 0.896481  [ 2208/ 3200]\n",
      "loss: 0.762700  [ 2224/ 3200]\n",
      "loss: 1.021038  [ 2240/ 3200]\n",
      "loss: 1.012720  [ 2256/ 3200]\n",
      "loss: 1.169960  [ 2272/ 3200]\n",
      "loss: 1.028966  [ 2288/ 3200]\n",
      "loss: 0.866415  [ 2304/ 3200]\n",
      "loss: 0.931968  [ 2320/ 3200]\n",
      "loss: 0.977501  [ 2336/ 3200]\n",
      "loss: 0.911556  [ 2352/ 3200]\n",
      "loss: 1.022594  [ 2368/ 3200]\n",
      "loss: 1.156275  [ 2384/ 3200]\n",
      "loss: 1.309302  [ 2400/ 3200]\n",
      "loss: 0.987044  [ 2416/ 3200]\n",
      "loss: 0.921303  [ 2432/ 3200]\n",
      "loss: 0.807527  [ 2448/ 3200]\n",
      "loss: 1.081542  [ 2464/ 3200]\n",
      "loss: 0.995813  [ 2480/ 3200]\n",
      "loss: 0.912079  [ 2496/ 3200]\n",
      "loss: 0.994892  [ 2512/ 3200]\n",
      "loss: 0.789249  [ 2528/ 3200]\n",
      "loss: 0.915478  [ 2544/ 3200]\n",
      "loss: 1.069830  [ 2560/ 3200]\n",
      "loss: 0.695616  [ 2576/ 3200]\n",
      "loss: 0.990226  [ 2592/ 3200]\n",
      "loss: 0.620594  [ 2608/ 3200]\n",
      "loss: 1.028851  [ 2624/ 3200]\n",
      "loss: 1.035668  [ 2640/ 3200]\n",
      "loss: 1.011011  [ 2656/ 3200]\n",
      "loss: 1.152816  [ 2672/ 3200]\n",
      "loss: 0.827090  [ 2688/ 3200]\n",
      "loss: 0.790715  [ 2704/ 3200]\n",
      "loss: 0.685051  [ 2720/ 3200]\n",
      "loss: 0.717643  [ 2736/ 3200]\n",
      "loss: 1.497746  [ 2752/ 3200]\n",
      "loss: 0.799413  [ 2768/ 3200]\n",
      "loss: 1.122389  [ 2784/ 3200]\n",
      "loss: 1.547912  [ 2800/ 3200]\n",
      "loss: 1.047389  [ 2816/ 3200]\n",
      "loss: 1.086074  [ 2832/ 3200]\n",
      "loss: 1.089733  [ 2848/ 3200]\n",
      "loss: 0.863142  [ 2864/ 3200]\n",
      "loss: 0.768412  [ 2880/ 3200]\n",
      "loss: 0.991279  [ 2896/ 3200]\n",
      "loss: 0.733676  [ 2912/ 3200]\n",
      "loss: 1.197973  [ 2928/ 3200]\n",
      "loss: 0.854335  [ 2944/ 3200]\n",
      "loss: 1.330603  [ 2960/ 3200]\n",
      "loss: 0.876250  [ 2976/ 3200]\n",
      "loss: 1.038671  [ 2992/ 3200]\n",
      "loss: 0.781138  [ 3008/ 3200]\n",
      "loss: 0.718713  [ 3024/ 3200]\n",
      "loss: 0.744638  [ 3040/ 3200]\n",
      "loss: 0.711667  [ 3056/ 3200]\n",
      "loss: 0.850571  [ 3072/ 3200]\n",
      "loss: 0.942993  [ 3088/ 3200]\n",
      "loss: 1.084234  [ 3104/ 3200]\n",
      "loss: 0.852623  [ 3120/ 3200]\n",
      "loss: 1.312767  [ 3136/ 3200]\n",
      "loss: 0.856872  [ 3152/ 3200]\n",
      "loss: 0.866210  [ 3168/ 3200]\n",
      "loss: 1.119560  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.064705\n",
      "f1 macro averaged score: 0.500777\n",
      "Accuracy               : 52.8%\n",
      "Confusion matrix       :\n",
      "tensor([[164,  34,   0,   2],\n",
      "        [ 58,  91,   5,  46],\n",
      "        [ 12,  59,  36,  93],\n",
      "        [ 10,  58,   1, 131]], device='cuda:0')\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 1.212626  [    0/ 3200]\n",
      "loss: 1.018201  [   16/ 3200]\n",
      "loss: 0.914173  [   32/ 3200]\n",
      "loss: 0.874064  [   48/ 3200]\n",
      "loss: 0.650903  [   64/ 3200]\n",
      "loss: 1.151842  [   80/ 3200]\n",
      "loss: 0.988771  [   96/ 3200]\n",
      "loss: 0.802982  [  112/ 3200]\n",
      "loss: 1.258821  [  128/ 3200]\n",
      "loss: 1.009597  [  144/ 3200]\n",
      "loss: 1.018971  [  160/ 3200]\n",
      "loss: 0.963746  [  176/ 3200]\n",
      "loss: 0.901543  [  192/ 3200]\n",
      "loss: 0.995139  [  208/ 3200]\n",
      "loss: 0.712173  [  224/ 3200]\n",
      "loss: 0.883796  [  240/ 3200]\n",
      "loss: 0.611978  [  256/ 3200]\n",
      "loss: 0.994612  [  272/ 3200]\n",
      "loss: 0.885693  [  288/ 3200]\n",
      "loss: 1.072758  [  304/ 3200]\n",
      "loss: 0.924818  [  320/ 3200]\n",
      "loss: 0.980142  [  336/ 3200]\n",
      "loss: 0.935011  [  352/ 3200]\n",
      "loss: 0.782310  [  368/ 3200]\n",
      "loss: 1.094888  [  384/ 3200]\n",
      "loss: 0.885209  [  400/ 3200]\n",
      "loss: 0.976993  [  416/ 3200]\n",
      "loss: 0.806292  [  432/ 3200]\n",
      "loss: 0.937497  [  448/ 3200]\n",
      "loss: 1.094022  [  464/ 3200]\n",
      "loss: 0.760305  [  480/ 3200]\n",
      "loss: 0.785260  [  496/ 3200]\n",
      "loss: 0.951103  [  512/ 3200]\n",
      "loss: 0.899966  [  528/ 3200]\n",
      "loss: 1.077186  [  544/ 3200]\n",
      "loss: 1.008369  [  560/ 3200]\n",
      "loss: 0.732896  [  576/ 3200]\n",
      "loss: 0.793601  [  592/ 3200]\n",
      "loss: 1.036799  [  608/ 3200]\n",
      "loss: 0.956942  [  624/ 3200]\n",
      "loss: 1.205896  [  640/ 3200]\n",
      "loss: 0.934665  [  656/ 3200]\n",
      "loss: 1.091361  [  672/ 3200]\n",
      "loss: 0.960576  [  688/ 3200]\n",
      "loss: 1.070376  [  704/ 3200]\n",
      "loss: 1.190053  [  720/ 3200]\n",
      "loss: 0.714803  [  736/ 3200]\n",
      "loss: 1.266823  [  752/ 3200]\n",
      "loss: 1.051129  [  768/ 3200]\n",
      "loss: 1.028722  [  784/ 3200]\n",
      "loss: 0.709115  [  800/ 3200]\n",
      "loss: 0.720868  [  816/ 3200]\n",
      "loss: 0.761476  [  832/ 3200]\n",
      "loss: 1.099323  [  848/ 3200]\n",
      "loss: 0.867107  [  864/ 3200]\n",
      "loss: 0.869655  [  880/ 3200]\n",
      "loss: 0.948236  [  896/ 3200]\n",
      "loss: 0.992566  [  912/ 3200]\n",
      "loss: 0.637580  [  928/ 3200]\n",
      "loss: 0.886673  [  944/ 3200]\n",
      "loss: 0.835782  [  960/ 3200]\n",
      "loss: 0.818427  [  976/ 3200]\n",
      "loss: 0.806348  [  992/ 3200]\n",
      "loss: 0.780301  [ 1008/ 3200]\n",
      "loss: 1.171867  [ 1024/ 3200]\n",
      "loss: 0.728433  [ 1040/ 3200]\n",
      "loss: 0.854686  [ 1056/ 3200]\n",
      "loss: 1.364225  [ 1072/ 3200]\n",
      "loss: 1.067455  [ 1088/ 3200]\n",
      "loss: 0.851151  [ 1104/ 3200]\n",
      "loss: 0.699829  [ 1120/ 3200]\n",
      "loss: 1.025967  [ 1136/ 3200]\n",
      "loss: 1.100697  [ 1152/ 3200]\n",
      "loss: 0.831919  [ 1168/ 3200]\n",
      "loss: 1.158331  [ 1184/ 3200]\n",
      "loss: 1.155815  [ 1200/ 3200]\n",
      "loss: 1.019402  [ 1216/ 3200]\n",
      "loss: 0.831658  [ 1232/ 3200]\n",
      "loss: 0.991667  [ 1248/ 3200]\n",
      "loss: 0.691980  [ 1264/ 3200]\n",
      "loss: 0.956612  [ 1280/ 3200]\n",
      "loss: 1.077478  [ 1296/ 3200]\n",
      "loss: 0.963436  [ 1312/ 3200]\n",
      "loss: 1.274000  [ 1328/ 3200]\n",
      "loss: 0.978321  [ 1344/ 3200]\n",
      "loss: 1.133274  [ 1360/ 3200]\n",
      "loss: 0.926829  [ 1376/ 3200]\n",
      "loss: 0.874443  [ 1392/ 3200]\n",
      "loss: 1.017709  [ 1408/ 3200]\n",
      "loss: 0.663290  [ 1424/ 3200]\n",
      "loss: 0.853209  [ 1440/ 3200]\n",
      "loss: 0.904437  [ 1456/ 3200]\n",
      "loss: 0.951210  [ 1472/ 3200]\n",
      "loss: 0.732365  [ 1488/ 3200]\n",
      "loss: 1.046521  [ 1504/ 3200]\n",
      "loss: 0.817313  [ 1520/ 3200]\n",
      "loss: 0.844010  [ 1536/ 3200]\n",
      "loss: 0.849679  [ 1552/ 3200]\n",
      "loss: 0.793082  [ 1568/ 3200]\n",
      "loss: 1.068472  [ 1584/ 3200]\n",
      "loss: 1.002119  [ 1600/ 3200]\n",
      "loss: 0.717195  [ 1616/ 3200]\n",
      "loss: 1.092632  [ 1632/ 3200]\n",
      "loss: 0.990217  [ 1648/ 3200]\n",
      "loss: 1.002871  [ 1664/ 3200]\n",
      "loss: 1.115411  [ 1680/ 3200]\n",
      "loss: 0.883680  [ 1696/ 3200]\n",
      "loss: 0.885032  [ 1712/ 3200]\n",
      "loss: 0.943611  [ 1728/ 3200]\n",
      "loss: 0.892765  [ 1744/ 3200]\n",
      "loss: 0.860943  [ 1760/ 3200]\n",
      "loss: 0.723414  [ 1776/ 3200]\n",
      "loss: 1.039850  [ 1792/ 3200]\n",
      "loss: 0.793298  [ 1808/ 3200]\n",
      "loss: 0.939992  [ 1824/ 3200]\n",
      "loss: 0.994118  [ 1840/ 3200]\n",
      "loss: 1.211991  [ 1856/ 3200]\n",
      "loss: 1.094282  [ 1872/ 3200]\n",
      "loss: 0.974451  [ 1888/ 3200]\n",
      "loss: 0.783201  [ 1904/ 3200]\n",
      "loss: 1.065195  [ 1920/ 3200]\n",
      "loss: 0.777634  [ 1936/ 3200]\n",
      "loss: 0.520637  [ 1952/ 3200]\n",
      "loss: 1.049006  [ 1968/ 3200]\n",
      "loss: 0.671101  [ 1984/ 3200]\n",
      "loss: 0.917521  [ 2000/ 3200]\n",
      "loss: 1.112824  [ 2016/ 3200]\n",
      "loss: 1.037023  [ 2032/ 3200]\n",
      "loss: 1.068747  [ 2048/ 3200]\n",
      "loss: 0.818772  [ 2064/ 3200]\n",
      "loss: 0.907422  [ 2080/ 3200]\n",
      "loss: 0.945612  [ 2096/ 3200]\n",
      "loss: 0.902694  [ 2112/ 3200]\n",
      "loss: 0.795069  [ 2128/ 3200]\n",
      "loss: 1.198388  [ 2144/ 3200]\n",
      "loss: 0.577741  [ 2160/ 3200]\n",
      "loss: 1.132715  [ 2176/ 3200]\n",
      "loss: 0.888019  [ 2192/ 3200]\n",
      "loss: 0.737193  [ 2208/ 3200]\n",
      "loss: 0.973397  [ 2224/ 3200]\n",
      "loss: 0.657780  [ 2240/ 3200]\n",
      "loss: 0.877142  [ 2256/ 3200]\n",
      "loss: 0.872686  [ 2272/ 3200]\n",
      "loss: 0.758928  [ 2288/ 3200]\n",
      "loss: 0.992895  [ 2304/ 3200]\n",
      "loss: 0.972422  [ 2320/ 3200]\n",
      "loss: 0.821341  [ 2336/ 3200]\n",
      "loss: 0.970650  [ 2352/ 3200]\n",
      "loss: 0.988981  [ 2368/ 3200]\n",
      "loss: 0.920436  [ 2384/ 3200]\n",
      "loss: 0.860193  [ 2400/ 3200]\n",
      "loss: 1.000836  [ 2416/ 3200]\n",
      "loss: 1.112261  [ 2432/ 3200]\n",
      "loss: 0.862270  [ 2448/ 3200]\n",
      "loss: 1.146556  [ 2464/ 3200]\n",
      "loss: 0.845267  [ 2480/ 3200]\n",
      "loss: 0.995632  [ 2496/ 3200]\n",
      "loss: 0.930658  [ 2512/ 3200]\n",
      "loss: 0.945266  [ 2528/ 3200]\n",
      "loss: 0.668679  [ 2544/ 3200]\n",
      "loss: 1.192995  [ 2560/ 3200]\n",
      "loss: 0.658393  [ 2576/ 3200]\n",
      "loss: 0.669688  [ 2592/ 3200]\n",
      "loss: 0.793215  [ 2608/ 3200]\n",
      "loss: 0.844397  [ 2624/ 3200]\n",
      "loss: 0.668416  [ 2640/ 3200]\n",
      "loss: 0.932069  [ 2656/ 3200]\n",
      "loss: 0.441949  [ 2672/ 3200]\n",
      "loss: 0.676345  [ 2688/ 3200]\n",
      "loss: 0.665494  [ 2704/ 3200]\n",
      "loss: 0.693240  [ 2720/ 3200]\n",
      "loss: 1.042988  [ 2736/ 3200]\n",
      "loss: 0.926961  [ 2752/ 3200]\n",
      "loss: 1.418830  [ 2768/ 3200]\n",
      "loss: 0.911536  [ 2784/ 3200]\n",
      "loss: 0.776030  [ 2800/ 3200]\n",
      "loss: 0.956491  [ 2816/ 3200]\n",
      "loss: 1.340783  [ 2832/ 3200]\n",
      "loss: 0.975413  [ 2848/ 3200]\n",
      "loss: 0.988411  [ 2864/ 3200]\n",
      "loss: 0.949900  [ 2880/ 3200]\n",
      "loss: 0.679134  [ 2896/ 3200]\n",
      "loss: 1.535210  [ 2912/ 3200]\n",
      "loss: 0.954531  [ 2928/ 3200]\n",
      "loss: 1.099654  [ 2944/ 3200]\n",
      "loss: 0.844837  [ 2960/ 3200]\n",
      "loss: 0.819007  [ 2976/ 3200]\n",
      "loss: 1.002347  [ 2992/ 3200]\n",
      "loss: 1.056178  [ 3008/ 3200]\n",
      "loss: 0.920649  [ 3024/ 3200]\n",
      "loss: 0.790108  [ 3040/ 3200]\n",
      "loss: 0.905000  [ 3056/ 3200]\n",
      "loss: 0.934058  [ 3072/ 3200]\n",
      "loss: 0.986111  [ 3088/ 3200]\n",
      "loss: 1.057604  [ 3104/ 3200]\n",
      "loss: 1.001719  [ 3120/ 3200]\n",
      "loss: 0.999391  [ 3136/ 3200]\n",
      "loss: 1.044231  [ 3152/ 3200]\n",
      "loss: 0.742636  [ 3168/ 3200]\n",
      "loss: 0.832773  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.059599\n",
      "f1 macro averaged score: 0.551579\n",
      "Accuracy               : 56.8%\n",
      "Confusion matrix       :\n",
      "tensor([[155,  28,   6,  11],\n",
      "        [ 42,  44,  43,  71],\n",
      "        [  1,  15, 122,  62],\n",
      "        [  2,  21,  44, 133]], device='cuda:0')\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 0.920376  [    0/ 3200]\n",
      "loss: 0.640873  [   16/ 3200]\n",
      "loss: 0.653736  [   32/ 3200]\n",
      "loss: 0.725733  [   48/ 3200]\n",
      "loss: 1.053422  [   64/ 3200]\n",
      "loss: 0.931217  [   80/ 3200]\n",
      "loss: 0.749655  [   96/ 3200]\n",
      "loss: 0.912602  [  112/ 3200]\n",
      "loss: 0.875312  [  128/ 3200]\n",
      "loss: 1.064572  [  144/ 3200]\n",
      "loss: 0.981557  [  160/ 3200]\n",
      "loss: 0.991346  [  176/ 3200]\n",
      "loss: 0.759583  [  192/ 3200]\n",
      "loss: 1.110810  [  208/ 3200]\n",
      "loss: 0.680099  [  224/ 3200]\n",
      "loss: 0.597290  [  240/ 3200]\n",
      "loss: 0.588797  [  256/ 3200]\n",
      "loss: 0.991312  [  272/ 3200]\n",
      "loss: 0.985212  [  288/ 3200]\n",
      "loss: 0.768157  [  304/ 3200]\n",
      "loss: 0.756080  [  320/ 3200]\n",
      "loss: 1.307273  [  336/ 3200]\n",
      "loss: 1.144626  [  352/ 3200]\n",
      "loss: 0.977524  [  368/ 3200]\n",
      "loss: 0.697522  [  384/ 3200]\n",
      "loss: 0.750425  [  400/ 3200]\n",
      "loss: 0.785867  [  416/ 3200]\n",
      "loss: 0.902991  [  432/ 3200]\n",
      "loss: 1.050715  [  448/ 3200]\n",
      "loss: 1.140720  [  464/ 3200]\n",
      "loss: 0.859959  [  480/ 3200]\n",
      "loss: 0.831392  [  496/ 3200]\n",
      "loss: 1.093711  [  512/ 3200]\n",
      "loss: 1.049487  [  528/ 3200]\n",
      "loss: 0.910130  [  544/ 3200]\n",
      "loss: 0.916878  [  560/ 3200]\n",
      "loss: 0.984898  [  576/ 3200]\n",
      "loss: 0.894026  [  592/ 3200]\n",
      "loss: 0.614402  [  608/ 3200]\n",
      "loss: 0.941433  [  624/ 3200]\n",
      "loss: 0.928033  [  640/ 3200]\n",
      "loss: 0.609069  [  656/ 3200]\n",
      "loss: 1.061327  [  672/ 3200]\n",
      "loss: 0.836830  [  688/ 3200]\n",
      "loss: 0.980545  [  704/ 3200]\n",
      "loss: 0.993992  [  720/ 3200]\n",
      "loss: 1.165562  [  736/ 3200]\n",
      "loss: 0.744534  [  752/ 3200]\n",
      "loss: 0.944842  [  768/ 3200]\n",
      "loss: 0.825260  [  784/ 3200]\n",
      "loss: 1.066914  [  800/ 3200]\n",
      "loss: 0.793585  [  816/ 3200]\n",
      "loss: 0.914266  [  832/ 3200]\n",
      "loss: 0.847952  [  848/ 3200]\n",
      "loss: 0.880489  [  864/ 3200]\n",
      "loss: 0.895985  [  880/ 3200]\n",
      "loss: 0.996331  [  896/ 3200]\n",
      "loss: 1.246013  [  912/ 3200]\n",
      "loss: 0.881160  [  928/ 3200]\n",
      "loss: 0.776397  [  944/ 3200]\n",
      "loss: 0.967332  [  960/ 3200]\n",
      "loss: 1.070161  [  976/ 3200]\n",
      "loss: 0.651135  [  992/ 3200]\n",
      "loss: 0.733043  [ 1008/ 3200]\n",
      "loss: 0.963171  [ 1024/ 3200]\n",
      "loss: 0.809185  [ 1040/ 3200]\n",
      "loss: 0.663318  [ 1056/ 3200]\n",
      "loss: 0.881854  [ 1072/ 3200]\n",
      "loss: 0.949581  [ 1088/ 3200]\n",
      "loss: 0.895367  [ 1104/ 3200]\n",
      "loss: 0.954839  [ 1120/ 3200]\n",
      "loss: 1.207464  [ 1136/ 3200]\n",
      "loss: 0.730490  [ 1152/ 3200]\n",
      "loss: 0.802655  [ 1168/ 3200]\n",
      "loss: 0.828049  [ 1184/ 3200]\n",
      "loss: 0.821762  [ 1200/ 3200]\n",
      "loss: 0.830836  [ 1216/ 3200]\n",
      "loss: 1.495683  [ 1232/ 3200]\n",
      "loss: 0.802120  [ 1248/ 3200]\n",
      "loss: 0.700948  [ 1264/ 3200]\n",
      "loss: 0.692259  [ 1280/ 3200]\n",
      "loss: 1.335243  [ 1296/ 3200]\n",
      "loss: 0.700921  [ 1312/ 3200]\n",
      "loss: 0.732481  [ 1328/ 3200]\n",
      "loss: 0.806669  [ 1344/ 3200]\n",
      "loss: 0.770744  [ 1360/ 3200]\n",
      "loss: 0.884495  [ 1376/ 3200]\n",
      "loss: 0.643425  [ 1392/ 3200]\n",
      "loss: 0.637463  [ 1408/ 3200]\n",
      "loss: 1.212194  [ 1424/ 3200]\n",
      "loss: 0.802010  [ 1440/ 3200]\n",
      "loss: 1.051112  [ 1456/ 3200]\n",
      "loss: 1.122772  [ 1472/ 3200]\n",
      "loss: 0.649529  [ 1488/ 3200]\n",
      "loss: 0.808685  [ 1504/ 3200]\n",
      "loss: 0.752955  [ 1520/ 3200]\n",
      "loss: 0.863214  [ 1536/ 3200]\n",
      "loss: 0.704091  [ 1552/ 3200]\n",
      "loss: 0.983305  [ 1568/ 3200]\n",
      "loss: 0.933229  [ 1584/ 3200]\n",
      "loss: 0.654471  [ 1600/ 3200]\n",
      "loss: 0.954184  [ 1616/ 3200]\n",
      "loss: 0.684275  [ 1632/ 3200]\n",
      "loss: 0.985873  [ 1648/ 3200]\n",
      "loss: 0.710255  [ 1664/ 3200]\n",
      "loss: 0.578932  [ 1680/ 3200]\n",
      "loss: 0.993855  [ 1696/ 3200]\n",
      "loss: 1.181304  [ 1712/ 3200]\n",
      "loss: 1.025911  [ 1728/ 3200]\n",
      "loss: 0.890507  [ 1744/ 3200]\n",
      "loss: 0.969046  [ 1760/ 3200]\n",
      "loss: 0.944727  [ 1776/ 3200]\n",
      "loss: 0.703459  [ 1792/ 3200]\n",
      "loss: 1.025448  [ 1808/ 3200]\n",
      "loss: 0.781102  [ 1824/ 3200]\n",
      "loss: 0.588707  [ 1840/ 3200]\n",
      "loss: 1.409511  [ 1856/ 3200]\n",
      "loss: 0.652532  [ 1872/ 3200]\n",
      "loss: 1.032607  [ 1888/ 3200]\n",
      "loss: 1.105339  [ 1904/ 3200]\n",
      "loss: 1.022470  [ 1920/ 3200]\n",
      "loss: 1.086058  [ 1936/ 3200]\n",
      "loss: 0.785798  [ 1952/ 3200]\n",
      "loss: 0.947961  [ 1968/ 3200]\n",
      "loss: 0.964046  [ 1984/ 3200]\n",
      "loss: 0.730654  [ 2000/ 3200]\n",
      "loss: 1.091025  [ 2016/ 3200]\n",
      "loss: 0.800699  [ 2032/ 3200]\n",
      "loss: 0.877625  [ 2048/ 3200]\n",
      "loss: 0.713145  [ 2064/ 3200]\n",
      "loss: 0.991097  [ 2080/ 3200]\n",
      "loss: 1.036478  [ 2096/ 3200]\n",
      "loss: 0.851140  [ 2112/ 3200]\n",
      "loss: 0.821055  [ 2128/ 3200]\n",
      "loss: 0.783698  [ 2144/ 3200]\n",
      "loss: 0.749365  [ 2160/ 3200]\n",
      "loss: 0.978222  [ 2176/ 3200]\n",
      "loss: 0.907755  [ 2192/ 3200]\n",
      "loss: 0.835809  [ 2208/ 3200]\n",
      "loss: 1.069999  [ 2224/ 3200]\n",
      "loss: 1.010844  [ 2240/ 3200]\n",
      "loss: 0.949672  [ 2256/ 3200]\n",
      "loss: 0.910656  [ 2272/ 3200]\n",
      "loss: 0.774760  [ 2288/ 3200]\n",
      "loss: 0.755358  [ 2304/ 3200]\n",
      "loss: 0.874833  [ 2320/ 3200]\n",
      "loss: 0.992567  [ 2336/ 3200]\n",
      "loss: 1.164755  [ 2352/ 3200]\n",
      "loss: 0.881917  [ 2368/ 3200]\n",
      "loss: 0.983258  [ 2384/ 3200]\n",
      "loss: 1.132325  [ 2400/ 3200]\n",
      "loss: 0.613642  [ 2416/ 3200]\n",
      "loss: 0.607545  [ 2432/ 3200]\n",
      "loss: 0.789240  [ 2448/ 3200]\n",
      "loss: 0.789471  [ 2464/ 3200]\n",
      "loss: 0.996341  [ 2480/ 3200]\n",
      "loss: 0.784517  [ 2496/ 3200]\n",
      "loss: 1.133104  [ 2512/ 3200]\n",
      "loss: 0.886464  [ 2528/ 3200]\n",
      "loss: 0.840972  [ 2544/ 3200]\n",
      "loss: 1.233847  [ 2560/ 3200]\n",
      "loss: 0.901333  [ 2576/ 3200]\n",
      "loss: 0.504695  [ 2592/ 3200]\n",
      "loss: 1.053858  [ 2608/ 3200]\n",
      "loss: 0.930456  [ 2624/ 3200]\n",
      "loss: 0.639377  [ 2640/ 3200]\n",
      "loss: 0.912886  [ 2656/ 3200]\n",
      "loss: 1.011416  [ 2672/ 3200]\n",
      "loss: 0.919630  [ 2688/ 3200]\n",
      "loss: 0.985967  [ 2704/ 3200]\n",
      "loss: 1.079546  [ 2720/ 3200]\n",
      "loss: 1.199438  [ 2736/ 3200]\n",
      "loss: 1.375897  [ 2752/ 3200]\n",
      "loss: 0.934362  [ 2768/ 3200]\n",
      "loss: 0.928180  [ 2784/ 3200]\n",
      "loss: 1.027373  [ 2800/ 3200]\n",
      "loss: 0.921479  [ 2816/ 3200]\n",
      "loss: 1.097404  [ 2832/ 3200]\n",
      "loss: 0.776115  [ 2848/ 3200]\n",
      "loss: 0.648467  [ 2864/ 3200]\n",
      "loss: 1.388041  [ 2880/ 3200]\n",
      "loss: 1.353377  [ 2896/ 3200]\n",
      "loss: 0.717451  [ 2912/ 3200]\n",
      "loss: 0.798428  [ 2928/ 3200]\n",
      "loss: 0.973558  [ 2944/ 3200]\n",
      "loss: 0.874876  [ 2960/ 3200]\n",
      "loss: 0.555678  [ 2976/ 3200]\n",
      "loss: 0.833744  [ 2992/ 3200]\n",
      "loss: 0.854337  [ 3008/ 3200]\n",
      "loss: 1.053032  [ 3024/ 3200]\n",
      "loss: 1.030079  [ 3040/ 3200]\n",
      "loss: 0.905316  [ 3056/ 3200]\n",
      "loss: 0.807515  [ 3072/ 3200]\n",
      "loss: 0.654026  [ 3088/ 3200]\n",
      "loss: 0.831939  [ 3104/ 3200]\n",
      "loss: 0.930993  [ 3120/ 3200]\n",
      "loss: 1.041275  [ 3136/ 3200]\n",
      "loss: 0.823591  [ 3152/ 3200]\n",
      "loss: 1.056978  [ 3168/ 3200]\n",
      "loss: 0.587382  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.059375\n",
      "f1 macro averaged score: 0.564812\n",
      "Accuracy               : 57.2%\n",
      "Confusion matrix       :\n",
      "tensor([[158,  33,   2,   7],\n",
      "        [ 44,  67,  25,  64],\n",
      "        [  5,  26,  92,  77],\n",
      "        [  5,  35,  19, 141]], device='cuda:0')\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 0.932708  [    0/ 3200]\n",
      "loss: 0.746292  [   16/ 3200]\n",
      "loss: 0.676817  [   32/ 3200]\n",
      "loss: 0.642317  [   48/ 3200]\n",
      "loss: 0.783344  [   64/ 3200]\n",
      "loss: 0.826884  [   80/ 3200]\n",
      "loss: 0.596061  [   96/ 3200]\n",
      "loss: 0.728857  [  112/ 3200]\n",
      "loss: 1.273673  [  128/ 3200]\n",
      "loss: 1.266129  [  144/ 3200]\n",
      "loss: 0.900419  [  160/ 3200]\n",
      "loss: 0.896391  [  176/ 3200]\n",
      "loss: 0.566235  [  192/ 3200]\n",
      "loss: 1.006252  [  208/ 3200]\n",
      "loss: 1.018270  [  224/ 3200]\n",
      "loss: 0.926439  [  240/ 3200]\n",
      "loss: 0.870556  [  256/ 3200]\n",
      "loss: 0.860307  [  272/ 3200]\n",
      "loss: 0.793804  [  288/ 3200]\n",
      "loss: 0.669960  [  304/ 3200]\n",
      "loss: 1.090913  [  320/ 3200]\n",
      "loss: 0.961861  [  336/ 3200]\n",
      "loss: 1.240463  [  352/ 3200]\n",
      "loss: 1.174691  [  368/ 3200]\n",
      "loss: 0.785170  [  384/ 3200]\n",
      "loss: 0.743340  [  400/ 3200]\n",
      "loss: 0.742075  [  416/ 3200]\n",
      "loss: 0.815343  [  432/ 3200]\n",
      "loss: 1.280475  [  448/ 3200]\n",
      "loss: 0.944647  [  464/ 3200]\n",
      "loss: 1.366651  [  480/ 3200]\n",
      "loss: 0.966670  [  496/ 3200]\n",
      "loss: 0.956464  [  512/ 3200]\n",
      "loss: 0.910344  [  528/ 3200]\n",
      "loss: 0.873042  [  544/ 3200]\n",
      "loss: 1.006541  [  560/ 3200]\n",
      "loss: 0.920480  [  576/ 3200]\n",
      "loss: 0.839373  [  592/ 3200]\n",
      "loss: 0.677721  [  608/ 3200]\n",
      "loss: 0.807047  [  624/ 3200]\n",
      "loss: 0.869927  [  640/ 3200]\n",
      "loss: 0.812821  [  656/ 3200]\n",
      "loss: 0.733972  [  672/ 3200]\n",
      "loss: 1.118285  [  688/ 3200]\n",
      "loss: 0.701627  [  704/ 3200]\n",
      "loss: 1.134488  [  720/ 3200]\n",
      "loss: 0.875183  [  736/ 3200]\n",
      "loss: 0.585592  [  752/ 3200]\n",
      "loss: 0.573128  [  768/ 3200]\n",
      "loss: 1.021618  [  784/ 3200]\n",
      "loss: 1.098543  [  800/ 3200]\n",
      "loss: 0.841802  [  816/ 3200]\n",
      "loss: 0.859430  [  832/ 3200]\n",
      "loss: 0.746516  [  848/ 3200]\n",
      "loss: 1.155763  [  864/ 3200]\n",
      "loss: 0.958725  [  880/ 3200]\n",
      "loss: 1.057647  [  896/ 3200]\n",
      "loss: 0.742820  [  912/ 3200]\n",
      "loss: 0.924260  [  928/ 3200]\n",
      "loss: 0.876521  [  944/ 3200]\n",
      "loss: 1.068006  [  960/ 3200]\n",
      "loss: 1.015227  [  976/ 3200]\n",
      "loss: 0.912848  [  992/ 3200]\n",
      "loss: 0.887379  [ 1008/ 3200]\n",
      "loss: 0.620529  [ 1024/ 3200]\n",
      "loss: 1.059233  [ 1040/ 3200]\n",
      "loss: 1.019799  [ 1056/ 3200]\n",
      "loss: 0.654911  [ 1072/ 3200]\n",
      "loss: 1.033957  [ 1088/ 3200]\n",
      "loss: 0.908985  [ 1104/ 3200]\n",
      "loss: 0.989117  [ 1120/ 3200]\n",
      "loss: 0.618836  [ 1136/ 3200]\n",
      "loss: 0.546973  [ 1152/ 3200]\n",
      "loss: 1.011847  [ 1168/ 3200]\n",
      "loss: 1.132211  [ 1184/ 3200]\n",
      "loss: 0.929594  [ 1200/ 3200]\n",
      "loss: 0.640211  [ 1216/ 3200]\n",
      "loss: 0.867891  [ 1232/ 3200]\n",
      "loss: 0.733198  [ 1248/ 3200]\n",
      "loss: 0.815683  [ 1264/ 3200]\n",
      "loss: 0.790993  [ 1280/ 3200]\n",
      "loss: 0.987803  [ 1296/ 3200]\n",
      "loss: 0.736936  [ 1312/ 3200]\n",
      "loss: 0.594709  [ 1328/ 3200]\n",
      "loss: 0.948145  [ 1344/ 3200]\n",
      "loss: 0.895779  [ 1360/ 3200]\n",
      "loss: 0.921903  [ 1376/ 3200]\n",
      "loss: 0.814882  [ 1392/ 3200]\n",
      "loss: 0.896842  [ 1408/ 3200]\n",
      "loss: 0.864731  [ 1424/ 3200]\n",
      "loss: 0.836989  [ 1440/ 3200]\n",
      "loss: 0.750962  [ 1456/ 3200]\n",
      "loss: 1.028845  [ 1472/ 3200]\n",
      "loss: 0.841446  [ 1488/ 3200]\n",
      "loss: 0.783036  [ 1504/ 3200]\n",
      "loss: 0.696854  [ 1520/ 3200]\n",
      "loss: 0.807557  [ 1536/ 3200]\n",
      "loss: 0.818732  [ 1552/ 3200]\n",
      "loss: 0.942559  [ 1568/ 3200]\n",
      "loss: 0.925129  [ 1584/ 3200]\n",
      "loss: 1.384860  [ 1600/ 3200]\n",
      "loss: 0.828667  [ 1616/ 3200]\n",
      "loss: 1.065135  [ 1632/ 3200]\n",
      "loss: 0.666637  [ 1648/ 3200]\n",
      "loss: 0.762866  [ 1664/ 3200]\n",
      "loss: 0.670519  [ 1680/ 3200]\n",
      "loss: 0.666643  [ 1696/ 3200]\n",
      "loss: 0.757021  [ 1712/ 3200]\n",
      "loss: 0.975803  [ 1728/ 3200]\n",
      "loss: 0.699065  [ 1744/ 3200]\n",
      "loss: 0.739784  [ 1760/ 3200]\n",
      "loss: 0.940308  [ 1776/ 3200]\n",
      "loss: 0.877160  [ 1792/ 3200]\n",
      "loss: 0.878089  [ 1808/ 3200]\n",
      "loss: 0.777855  [ 1824/ 3200]\n",
      "loss: 0.836690  [ 1840/ 3200]\n",
      "loss: 0.715886  [ 1856/ 3200]\n",
      "loss: 1.158450  [ 1872/ 3200]\n",
      "loss: 1.127697  [ 1888/ 3200]\n",
      "loss: 0.948256  [ 1904/ 3200]\n",
      "loss: 1.111000  [ 1920/ 3200]\n",
      "loss: 0.984525  [ 1936/ 3200]\n",
      "loss: 0.861406  [ 1952/ 3200]\n",
      "loss: 0.626356  [ 1968/ 3200]\n",
      "loss: 1.102995  [ 1984/ 3200]\n",
      "loss: 0.660108  [ 2000/ 3200]\n",
      "loss: 0.700957  [ 2016/ 3200]\n",
      "loss: 0.777455  [ 2032/ 3200]\n",
      "loss: 0.734203  [ 2048/ 3200]\n",
      "loss: 0.745239  [ 2064/ 3200]\n",
      "loss: 0.836592  [ 2080/ 3200]\n",
      "loss: 0.802056  [ 2096/ 3200]\n",
      "loss: 0.716591  [ 2112/ 3200]\n",
      "loss: 0.969859  [ 2128/ 3200]\n",
      "loss: 1.110774  [ 2144/ 3200]\n",
      "loss: 0.859368  [ 2160/ 3200]\n",
      "loss: 0.920944  [ 2176/ 3200]\n",
      "loss: 0.959796  [ 2192/ 3200]\n",
      "loss: 0.823796  [ 2208/ 3200]\n",
      "loss: 1.016965  [ 2224/ 3200]\n",
      "loss: 0.718206  [ 2240/ 3200]\n",
      "loss: 1.153875  [ 2256/ 3200]\n",
      "loss: 0.843947  [ 2272/ 3200]\n",
      "loss: 0.919096  [ 2288/ 3200]\n",
      "loss: 1.128257  [ 2304/ 3200]\n",
      "loss: 0.928951  [ 2320/ 3200]\n",
      "loss: 0.725586  [ 2336/ 3200]\n",
      "loss: 1.045641  [ 2352/ 3200]\n",
      "loss: 0.697957  [ 2368/ 3200]\n",
      "loss: 0.772424  [ 2384/ 3200]\n",
      "loss: 0.751398  [ 2400/ 3200]\n",
      "loss: 0.617766  [ 2416/ 3200]\n",
      "loss: 0.696217  [ 2432/ 3200]\n",
      "loss: 0.784863  [ 2448/ 3200]\n",
      "loss: 0.919799  [ 2464/ 3200]\n",
      "loss: 0.974462  [ 2480/ 3200]\n",
      "loss: 1.114902  [ 2496/ 3200]\n",
      "loss: 1.206753  [ 2512/ 3200]\n",
      "loss: 0.836516  [ 2528/ 3200]\n",
      "loss: 1.011524  [ 2544/ 3200]\n",
      "loss: 0.886917  [ 2560/ 3200]\n",
      "loss: 0.700211  [ 2576/ 3200]\n",
      "loss: 0.576553  [ 2592/ 3200]\n",
      "loss: 0.813371  [ 2608/ 3200]\n",
      "loss: 0.727761  [ 2624/ 3200]\n",
      "loss: 1.076241  [ 2640/ 3200]\n",
      "loss: 0.747670  [ 2656/ 3200]\n",
      "loss: 0.828796  [ 2672/ 3200]\n",
      "loss: 1.364830  [ 2688/ 3200]\n",
      "loss: 0.618710  [ 2704/ 3200]\n",
      "loss: 0.723283  [ 2720/ 3200]\n",
      "loss: 0.803077  [ 2736/ 3200]\n",
      "loss: 0.794286  [ 2752/ 3200]\n",
      "loss: 0.970137  [ 2768/ 3200]\n",
      "loss: 0.774063  [ 2784/ 3200]\n",
      "loss: 1.022547  [ 2800/ 3200]\n",
      "loss: 0.724104  [ 2816/ 3200]\n",
      "loss: 0.933733  [ 2832/ 3200]\n",
      "loss: 1.075201  [ 2848/ 3200]\n",
      "loss: 0.804848  [ 2864/ 3200]\n",
      "loss: 0.875052  [ 2880/ 3200]\n",
      "loss: 0.742559  [ 2896/ 3200]\n",
      "loss: 0.720567  [ 2912/ 3200]\n",
      "loss: 0.805745  [ 2928/ 3200]\n",
      "loss: 0.899700  [ 2944/ 3200]\n",
      "loss: 0.913025  [ 2960/ 3200]\n",
      "loss: 0.957449  [ 2976/ 3200]\n",
      "loss: 1.081957  [ 2992/ 3200]\n",
      "loss: 1.133780  [ 3008/ 3200]\n",
      "loss: 0.722487  [ 3024/ 3200]\n",
      "loss: 0.730981  [ 3040/ 3200]\n",
      "loss: 0.958288  [ 3056/ 3200]\n",
      "loss: 1.158104  [ 3072/ 3200]\n",
      "loss: 1.067952  [ 3088/ 3200]\n",
      "loss: 0.823992  [ 3104/ 3200]\n",
      "loss: 0.562826  [ 3120/ 3200]\n",
      "loss: 0.811815  [ 3136/ 3200]\n",
      "loss: 0.802098  [ 3152/ 3200]\n",
      "loss: 1.092696  [ 3168/ 3200]\n",
      "loss: 0.960733  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.064076\n",
      "f1 macro averaged score: 0.510586\n",
      "Accuracy               : 54.2%\n",
      "Confusion matrix       :\n",
      "tensor([[151,  25,   2,  22],\n",
      "        [ 38,  34,  20, 108],\n",
      "        [  1,  18,  67, 114],\n",
      "        [  2,   9,   7, 182]], device='cuda:0')\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 0.899531  [    0/ 3200]\n",
      "loss: 0.997997  [   16/ 3200]\n",
      "loss: 0.760720  [   32/ 3200]\n",
      "loss: 0.877341  [   48/ 3200]\n",
      "loss: 0.764269  [   64/ 3200]\n",
      "loss: 0.752059  [   80/ 3200]\n",
      "loss: 0.990972  [   96/ 3200]\n",
      "loss: 0.686881  [  112/ 3200]\n",
      "loss: 0.627256  [  128/ 3200]\n",
      "loss: 1.215907  [  144/ 3200]\n",
      "loss: 1.042307  [  160/ 3200]\n",
      "loss: 0.869452  [  176/ 3200]\n",
      "loss: 0.805802  [  192/ 3200]\n",
      "loss: 0.846661  [  208/ 3200]\n",
      "loss: 0.734327  [  224/ 3200]\n",
      "loss: 1.373089  [  240/ 3200]\n",
      "loss: 0.983494  [  256/ 3200]\n",
      "loss: 0.772836  [  272/ 3200]\n",
      "loss: 1.001075  [  288/ 3200]\n",
      "loss: 0.717934  [  304/ 3200]\n",
      "loss: 0.794062  [  320/ 3200]\n",
      "loss: 1.042969  [  336/ 3200]\n",
      "loss: 0.863675  [  352/ 3200]\n",
      "loss: 0.806319  [  368/ 3200]\n",
      "loss: 0.883584  [  384/ 3200]\n",
      "loss: 1.183084  [  400/ 3200]\n",
      "loss: 1.414916  [  416/ 3200]\n",
      "loss: 0.775087  [  432/ 3200]\n",
      "loss: 0.999965  [  448/ 3200]\n",
      "loss: 1.052754  [  464/ 3200]\n",
      "loss: 0.676791  [  480/ 3200]\n",
      "loss: 0.867932  [  496/ 3200]\n",
      "loss: 1.291722  [  512/ 3200]\n",
      "loss: 0.689506  [  528/ 3200]\n",
      "loss: 0.874948  [  544/ 3200]\n",
      "loss: 0.711025  [  560/ 3200]\n",
      "loss: 0.674678  [  576/ 3200]\n",
      "loss: 0.610835  [  592/ 3200]\n",
      "loss: 0.826487  [  608/ 3200]\n",
      "loss: 0.969290  [  624/ 3200]\n",
      "loss: 0.717839  [  640/ 3200]\n",
      "loss: 0.729201  [  656/ 3200]\n",
      "loss: 1.314038  [  672/ 3200]\n",
      "loss: 0.957370  [  688/ 3200]\n",
      "loss: 0.742901  [  704/ 3200]\n",
      "loss: 0.893085  [  720/ 3200]\n",
      "loss: 1.191932  [  736/ 3200]\n",
      "loss: 0.784486  [  752/ 3200]\n",
      "loss: 0.685104  [  768/ 3200]\n",
      "loss: 0.751368  [  784/ 3200]\n",
      "loss: 0.783580  [  800/ 3200]\n",
      "loss: 0.765274  [  816/ 3200]\n",
      "loss: 0.779353  [  832/ 3200]\n",
      "loss: 0.795457  [  848/ 3200]\n",
      "loss: 1.098970  [  864/ 3200]\n",
      "loss: 0.780109  [  880/ 3200]\n",
      "loss: 0.644244  [  896/ 3200]\n",
      "loss: 1.277996  [  912/ 3200]\n",
      "loss: 1.125040  [  928/ 3200]\n",
      "loss: 0.792254  [  944/ 3200]\n",
      "loss: 0.768811  [  960/ 3200]\n",
      "loss: 1.013671  [  976/ 3200]\n",
      "loss: 1.036262  [  992/ 3200]\n",
      "loss: 0.692949  [ 1008/ 3200]\n",
      "loss: 0.996033  [ 1024/ 3200]\n",
      "loss: 0.958417  [ 1040/ 3200]\n",
      "loss: 0.870995  [ 1056/ 3200]\n",
      "loss: 0.889033  [ 1072/ 3200]\n",
      "loss: 0.764195  [ 1088/ 3200]\n",
      "loss: 0.796976  [ 1104/ 3200]\n",
      "loss: 0.767905  [ 1120/ 3200]\n",
      "loss: 0.993543  [ 1136/ 3200]\n",
      "loss: 0.583240  [ 1152/ 3200]\n",
      "loss: 0.737164  [ 1168/ 3200]\n",
      "loss: 0.895955  [ 1184/ 3200]\n",
      "loss: 0.938033  [ 1200/ 3200]\n",
      "loss: 1.154677  [ 1216/ 3200]\n",
      "loss: 1.074342  [ 1232/ 3200]\n",
      "loss: 0.703197  [ 1248/ 3200]\n",
      "loss: 0.684595  [ 1264/ 3200]\n",
      "loss: 0.534827  [ 1280/ 3200]\n",
      "loss: 1.121871  [ 1296/ 3200]\n",
      "loss: 0.764275  [ 1312/ 3200]\n",
      "loss: 0.529530  [ 1328/ 3200]\n",
      "loss: 1.078234  [ 1344/ 3200]\n",
      "loss: 0.670466  [ 1360/ 3200]\n",
      "loss: 1.047150  [ 1376/ 3200]\n",
      "loss: 1.154149  [ 1392/ 3200]\n",
      "loss: 0.703605  [ 1408/ 3200]\n",
      "loss: 0.807221  [ 1424/ 3200]\n",
      "loss: 1.083110  [ 1440/ 3200]\n",
      "loss: 0.528511  [ 1456/ 3200]\n",
      "loss: 0.709727  [ 1472/ 3200]\n",
      "loss: 0.636124  [ 1488/ 3200]\n",
      "loss: 0.852135  [ 1504/ 3200]\n",
      "loss: 0.593244  [ 1520/ 3200]\n",
      "loss: 0.767715  [ 1536/ 3200]\n",
      "loss: 0.948950  [ 1552/ 3200]\n",
      "loss: 0.758980  [ 1568/ 3200]\n",
      "loss: 0.792224  [ 1584/ 3200]\n",
      "loss: 0.853149  [ 1600/ 3200]\n",
      "loss: 0.654843  [ 1616/ 3200]\n",
      "loss: 1.024639  [ 1632/ 3200]\n",
      "loss: 0.738467  [ 1648/ 3200]\n",
      "loss: 0.741167  [ 1664/ 3200]\n",
      "loss: 0.706150  [ 1680/ 3200]\n",
      "loss: 0.610126  [ 1696/ 3200]\n",
      "loss: 0.645308  [ 1712/ 3200]\n",
      "loss: 0.633699  [ 1728/ 3200]\n",
      "loss: 0.839041  [ 1744/ 3200]\n",
      "loss: 0.814598  [ 1760/ 3200]\n",
      "loss: 1.331275  [ 1776/ 3200]\n",
      "loss: 0.745980  [ 1792/ 3200]\n",
      "loss: 0.967666  [ 1808/ 3200]\n",
      "loss: 0.832086  [ 1824/ 3200]\n",
      "loss: 1.263240  [ 1840/ 3200]\n",
      "loss: 0.974887  [ 1856/ 3200]\n",
      "loss: 0.938170  [ 1872/ 3200]\n",
      "loss: 0.709309  [ 1888/ 3200]\n",
      "loss: 0.856621  [ 1904/ 3200]\n",
      "loss: 0.838469  [ 1920/ 3200]\n",
      "loss: 0.701865  [ 1936/ 3200]\n",
      "loss: 0.763935  [ 1952/ 3200]\n",
      "loss: 0.858556  [ 1968/ 3200]\n",
      "loss: 0.859939  [ 1984/ 3200]\n",
      "loss: 0.734960  [ 2000/ 3200]\n",
      "loss: 0.844543  [ 2016/ 3200]\n",
      "loss: 0.662829  [ 2032/ 3200]\n",
      "loss: 0.897100  [ 2048/ 3200]\n",
      "loss: 1.213990  [ 2064/ 3200]\n",
      "loss: 0.730241  [ 2080/ 3200]\n",
      "loss: 0.757681  [ 2096/ 3200]\n",
      "loss: 0.919432  [ 2112/ 3200]\n",
      "loss: 0.887513  [ 2128/ 3200]\n",
      "loss: 0.935965  [ 2144/ 3200]\n",
      "loss: 0.736313  [ 2160/ 3200]\n",
      "loss: 0.870381  [ 2176/ 3200]\n",
      "loss: 0.666170  [ 2192/ 3200]\n",
      "loss: 0.488277  [ 2208/ 3200]\n",
      "loss: 0.688045  [ 2224/ 3200]\n",
      "loss: 0.797563  [ 2240/ 3200]\n",
      "loss: 0.951016  [ 2256/ 3200]\n",
      "loss: 0.465769  [ 2272/ 3200]\n",
      "loss: 0.576002  [ 2288/ 3200]\n",
      "loss: 0.506804  [ 2304/ 3200]\n",
      "loss: 1.007517  [ 2320/ 3200]\n",
      "loss: 1.370329  [ 2336/ 3200]\n",
      "loss: 1.004688  [ 2352/ 3200]\n",
      "loss: 0.974320  [ 2368/ 3200]\n",
      "loss: 0.805753  [ 2384/ 3200]\n",
      "loss: 0.849407  [ 2400/ 3200]\n",
      "loss: 1.030502  [ 2416/ 3200]\n",
      "loss: 0.678466  [ 2432/ 3200]\n",
      "loss: 0.754852  [ 2448/ 3200]\n",
      "loss: 0.891392  [ 2464/ 3200]\n",
      "loss: 0.526036  [ 2480/ 3200]\n",
      "loss: 0.821910  [ 2496/ 3200]\n",
      "loss: 0.922078  [ 2512/ 3200]\n",
      "loss: 0.715786  [ 2528/ 3200]\n",
      "loss: 0.911120  [ 2544/ 3200]\n",
      "loss: 0.614799  [ 2560/ 3200]\n",
      "loss: 0.626535  [ 2576/ 3200]\n",
      "loss: 0.880537  [ 2592/ 3200]\n",
      "loss: 0.925974  [ 2608/ 3200]\n",
      "loss: 0.844485  [ 2624/ 3200]\n",
      "loss: 0.947735  [ 2640/ 3200]\n",
      "loss: 0.936546  [ 2656/ 3200]\n",
      "loss: 0.987873  [ 2672/ 3200]\n",
      "loss: 0.772975  [ 2688/ 3200]\n",
      "loss: 0.722495  [ 2704/ 3200]\n",
      "loss: 0.543075  [ 2720/ 3200]\n",
      "loss: 1.054920  [ 2736/ 3200]\n",
      "loss: 0.706129  [ 2752/ 3200]\n",
      "loss: 0.893785  [ 2768/ 3200]\n",
      "loss: 0.808882  [ 2784/ 3200]\n",
      "loss: 0.764068  [ 2800/ 3200]\n",
      "loss: 1.009630  [ 2816/ 3200]\n",
      "loss: 0.833965  [ 2832/ 3200]\n",
      "loss: 0.685064  [ 2848/ 3200]\n",
      "loss: 0.694665  [ 2864/ 3200]\n",
      "loss: 1.026237  [ 2880/ 3200]\n",
      "loss: 0.496030  [ 2896/ 3200]\n",
      "loss: 1.187766  [ 2912/ 3200]\n",
      "loss: 0.967947  [ 2928/ 3200]\n",
      "loss: 0.829285  [ 2944/ 3200]\n",
      "loss: 0.958814  [ 2960/ 3200]\n",
      "loss: 0.817374  [ 2976/ 3200]\n",
      "loss: 0.714394  [ 2992/ 3200]\n",
      "loss: 0.956995  [ 3008/ 3200]\n",
      "loss: 0.640447  [ 3024/ 3200]\n",
      "loss: 0.845768  [ 3040/ 3200]\n",
      "loss: 0.682350  [ 3056/ 3200]\n",
      "loss: 0.820810  [ 3072/ 3200]\n",
      "loss: 0.824902  [ 3088/ 3200]\n",
      "loss: 1.005408  [ 3104/ 3200]\n",
      "loss: 0.764439  [ 3120/ 3200]\n",
      "loss: 0.881248  [ 3136/ 3200]\n",
      "loss: 0.666694  [ 3152/ 3200]\n",
      "loss: 0.744010  [ 3168/ 3200]\n",
      "loss: 0.800425  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.056113\n",
      "f1 macro averaged score: 0.605368\n",
      "Accuracy               : 60.9%\n",
      "Confusion matrix       :\n",
      "tensor([[158,  36,   2,   4],\n",
      "        [ 41,  80,  32,  47],\n",
      "        [  5,  24, 127,  44],\n",
      "        [  5,  36,  37, 122]], device='cuda:0')\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.719697  [    0/ 3200]\n",
      "loss: 0.919092  [   16/ 3200]\n",
      "loss: 0.933330  [   32/ 3200]\n",
      "loss: 0.562385  [   48/ 3200]\n",
      "loss: 0.746700  [   64/ 3200]\n",
      "loss: 0.987776  [   80/ 3200]\n",
      "loss: 0.687415  [   96/ 3200]\n",
      "loss: 0.730258  [  112/ 3200]\n",
      "loss: 0.797913  [  128/ 3200]\n",
      "loss: 0.549725  [  144/ 3200]\n",
      "loss: 0.728952  [  160/ 3200]\n",
      "loss: 0.769045  [  176/ 3200]\n",
      "loss: 0.610087  [  192/ 3200]\n",
      "loss: 0.990227  [  208/ 3200]\n",
      "loss: 0.778424  [  224/ 3200]\n",
      "loss: 0.546478  [  240/ 3200]\n",
      "loss: 0.686149  [  256/ 3200]\n",
      "loss: 0.977940  [  272/ 3200]\n",
      "loss: 1.098709  [  288/ 3200]\n",
      "loss: 0.766198  [  304/ 3200]\n",
      "loss: 0.712470  [  320/ 3200]\n",
      "loss: 0.994770  [  336/ 3200]\n",
      "loss: 0.771989  [  352/ 3200]\n",
      "loss: 0.808217  [  368/ 3200]\n",
      "loss: 0.730022  [  384/ 3200]\n",
      "loss: 0.692069  [  400/ 3200]\n",
      "loss: 0.639931  [  416/ 3200]\n",
      "loss: 0.604245  [  432/ 3200]\n",
      "loss: 0.772904  [  448/ 3200]\n",
      "loss: 0.670853  [  464/ 3200]\n",
      "loss: 1.117469  [  480/ 3200]\n",
      "loss: 0.893864  [  496/ 3200]\n",
      "loss: 0.516850  [  512/ 3200]\n",
      "loss: 0.498327  [  528/ 3200]\n",
      "loss: 1.075782  [  544/ 3200]\n",
      "loss: 0.740784  [  560/ 3200]\n",
      "loss: 0.762628  [  576/ 3200]\n",
      "loss: 0.534533  [  592/ 3200]\n",
      "loss: 0.860587  [  608/ 3200]\n",
      "loss: 0.627923  [  624/ 3200]\n",
      "loss: 0.704590  [  640/ 3200]\n",
      "loss: 1.117940  [  656/ 3200]\n",
      "loss: 0.629479  [  672/ 3200]\n",
      "loss: 0.618323  [  688/ 3200]\n",
      "loss: 0.996568  [  704/ 3200]\n",
      "loss: 0.983495  [  720/ 3200]\n",
      "loss: 0.752859  [  736/ 3200]\n",
      "loss: 0.725009  [  752/ 3200]\n",
      "loss: 0.750481  [  768/ 3200]\n",
      "loss: 0.589317  [  784/ 3200]\n",
      "loss: 0.814362  [  800/ 3200]\n",
      "loss: 1.136307  [  816/ 3200]\n",
      "loss: 0.823358  [  832/ 3200]\n",
      "loss: 0.676260  [  848/ 3200]\n",
      "loss: 0.687228  [  864/ 3200]\n",
      "loss: 0.711417  [  880/ 3200]\n",
      "loss: 0.900264  [  896/ 3200]\n",
      "loss: 0.577832  [  912/ 3200]\n",
      "loss: 0.952866  [  928/ 3200]\n",
      "loss: 0.676306  [  944/ 3200]\n",
      "loss: 0.693391  [  960/ 3200]\n",
      "loss: 0.811197  [  976/ 3200]\n",
      "loss: 0.938991  [  992/ 3200]\n",
      "loss: 0.898359  [ 1008/ 3200]\n",
      "loss: 0.849712  [ 1024/ 3200]\n",
      "loss: 0.435788  [ 1040/ 3200]\n",
      "loss: 0.569822  [ 1056/ 3200]\n",
      "loss: 0.414063  [ 1072/ 3200]\n",
      "loss: 0.666975  [ 1088/ 3200]\n",
      "loss: 1.189758  [ 1104/ 3200]\n",
      "loss: 0.893605  [ 1120/ 3200]\n",
      "loss: 0.894847  [ 1136/ 3200]\n",
      "loss: 0.808249  [ 1152/ 3200]\n",
      "loss: 0.597194  [ 1168/ 3200]\n",
      "loss: 0.788078  [ 1184/ 3200]\n",
      "loss: 0.902006  [ 1200/ 3200]\n",
      "loss: 1.142762  [ 1216/ 3200]\n",
      "loss: 0.742731  [ 1232/ 3200]\n",
      "loss: 0.915734  [ 1248/ 3200]\n",
      "loss: 0.829235  [ 1264/ 3200]\n",
      "loss: 0.627689  [ 1280/ 3200]\n",
      "loss: 0.656489  [ 1296/ 3200]\n",
      "loss: 1.060016  [ 1312/ 3200]\n",
      "loss: 0.636495  [ 1328/ 3200]\n",
      "loss: 1.188612  [ 1344/ 3200]\n",
      "loss: 0.701380  [ 1360/ 3200]\n",
      "loss: 0.729888  [ 1376/ 3200]\n",
      "loss: 0.761640  [ 1392/ 3200]\n",
      "loss: 1.245429  [ 1408/ 3200]\n",
      "loss: 0.503669  [ 1424/ 3200]\n",
      "loss: 0.700202  [ 1440/ 3200]\n",
      "loss: 0.822599  [ 1456/ 3200]\n",
      "loss: 1.032413  [ 1472/ 3200]\n",
      "loss: 1.134058  [ 1488/ 3200]\n",
      "loss: 0.546210  [ 1504/ 3200]\n",
      "loss: 0.819154  [ 1520/ 3200]\n",
      "loss: 1.299641  [ 1536/ 3200]\n",
      "loss: 1.659298  [ 1552/ 3200]\n",
      "loss: 1.524016  [ 1568/ 3200]\n",
      "loss: 0.746374  [ 1584/ 3200]\n",
      "loss: 1.076075  [ 1600/ 3200]\n",
      "loss: 0.867441  [ 1616/ 3200]\n",
      "loss: 0.728690  [ 1632/ 3200]\n",
      "loss: 0.863278  [ 1648/ 3200]\n",
      "loss: 1.049183  [ 1664/ 3200]\n",
      "loss: 0.967398  [ 1680/ 3200]\n",
      "loss: 0.616988  [ 1696/ 3200]\n",
      "loss: 0.783080  [ 1712/ 3200]\n",
      "loss: 0.737669  [ 1728/ 3200]\n",
      "loss: 0.952048  [ 1744/ 3200]\n",
      "loss: 0.656804  [ 1760/ 3200]\n",
      "loss: 1.045811  [ 1776/ 3200]\n",
      "loss: 0.691756  [ 1792/ 3200]\n",
      "loss: 0.905039  [ 1808/ 3200]\n",
      "loss: 0.657758  [ 1824/ 3200]\n",
      "loss: 0.810305  [ 1840/ 3200]\n",
      "loss: 0.869664  [ 1856/ 3200]\n",
      "loss: 0.691302  [ 1872/ 3200]\n",
      "loss: 0.763182  [ 1888/ 3200]\n",
      "loss: 0.848961  [ 1904/ 3200]\n",
      "loss: 0.728015  [ 1920/ 3200]\n",
      "loss: 0.671926  [ 1936/ 3200]\n",
      "loss: 0.776401  [ 1952/ 3200]\n",
      "loss: 0.901297  [ 1968/ 3200]\n",
      "loss: 0.828253  [ 1984/ 3200]\n",
      "loss: 0.689735  [ 2000/ 3200]\n",
      "loss: 0.656225  [ 2016/ 3200]\n",
      "loss: 0.659561  [ 2032/ 3200]\n",
      "loss: 1.029684  [ 2048/ 3200]\n",
      "loss: 0.872327  [ 2064/ 3200]\n",
      "loss: 0.972237  [ 2080/ 3200]\n",
      "loss: 0.648222  [ 2096/ 3200]\n",
      "loss: 0.905029  [ 2112/ 3200]\n",
      "loss: 0.780156  [ 2128/ 3200]\n",
      "loss: 0.704435  [ 2144/ 3200]\n",
      "loss: 0.748260  [ 2160/ 3200]\n",
      "loss: 0.709823  [ 2176/ 3200]\n",
      "loss: 1.012630  [ 2192/ 3200]\n",
      "loss: 0.977151  [ 2208/ 3200]\n",
      "loss: 0.452615  [ 2224/ 3200]\n",
      "loss: 0.858652  [ 2240/ 3200]\n",
      "loss: 0.608800  [ 2256/ 3200]\n",
      "loss: 0.776973  [ 2272/ 3200]\n",
      "loss: 0.746135  [ 2288/ 3200]\n",
      "loss: 0.646822  [ 2304/ 3200]\n",
      "loss: 0.926462  [ 2320/ 3200]\n",
      "loss: 0.932136  [ 2336/ 3200]\n",
      "loss: 1.624806  [ 2352/ 3200]\n",
      "loss: 0.660646  [ 2368/ 3200]\n",
      "loss: 0.805573  [ 2384/ 3200]\n",
      "loss: 0.781711  [ 2400/ 3200]\n",
      "loss: 0.948975  [ 2416/ 3200]\n",
      "loss: 0.800464  [ 2432/ 3200]\n",
      "loss: 0.907403  [ 2448/ 3200]\n",
      "loss: 0.807688  [ 2464/ 3200]\n",
      "loss: 0.454877  [ 2480/ 3200]\n",
      "loss: 0.805244  [ 2496/ 3200]\n",
      "loss: 0.952708  [ 2512/ 3200]\n",
      "loss: 1.234486  [ 2528/ 3200]\n",
      "loss: 0.832346  [ 2544/ 3200]\n",
      "loss: 0.757558  [ 2560/ 3200]\n",
      "loss: 0.637570  [ 2576/ 3200]\n",
      "loss: 1.107429  [ 2592/ 3200]\n",
      "loss: 0.863615  [ 2608/ 3200]\n",
      "loss: 0.672377  [ 2624/ 3200]\n",
      "loss: 0.755707  [ 2640/ 3200]\n",
      "loss: 0.712567  [ 2656/ 3200]\n",
      "loss: 0.849199  [ 2672/ 3200]\n",
      "loss: 0.781546  [ 2688/ 3200]\n",
      "loss: 0.924134  [ 2704/ 3200]\n",
      "loss: 0.587790  [ 2720/ 3200]\n",
      "loss: 1.022157  [ 2736/ 3200]\n",
      "loss: 0.660210  [ 2752/ 3200]\n",
      "loss: 0.873541  [ 2768/ 3200]\n",
      "loss: 1.280009  [ 2784/ 3200]\n",
      "loss: 0.787498  [ 2800/ 3200]\n",
      "loss: 0.769967  [ 2816/ 3200]\n",
      "loss: 1.121563  [ 2832/ 3200]\n",
      "loss: 0.745953  [ 2848/ 3200]\n",
      "loss: 0.646234  [ 2864/ 3200]\n",
      "loss: 0.905872  [ 2880/ 3200]\n",
      "loss: 0.549803  [ 2896/ 3200]\n",
      "loss: 0.995363  [ 2912/ 3200]\n",
      "loss: 1.347452  [ 2928/ 3200]\n",
      "loss: 1.044432  [ 2944/ 3200]\n",
      "loss: 0.626610  [ 2960/ 3200]\n",
      "loss: 0.910943  [ 2976/ 3200]\n",
      "loss: 0.800703  [ 2992/ 3200]\n",
      "loss: 0.561823  [ 3008/ 3200]\n",
      "loss: 0.831488  [ 3024/ 3200]\n",
      "loss: 0.745284  [ 3040/ 3200]\n",
      "loss: 0.866240  [ 3056/ 3200]\n",
      "loss: 0.588801  [ 3072/ 3200]\n",
      "loss: 0.922634  [ 3088/ 3200]\n",
      "loss: 1.049619  [ 3104/ 3200]\n",
      "loss: 0.752522  [ 3120/ 3200]\n",
      "loss: 1.001734  [ 3136/ 3200]\n",
      "loss: 0.686952  [ 3152/ 3200]\n",
      "loss: 0.942647  [ 3168/ 3200]\n",
      "loss: 0.908148  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.062853\n",
      "f1 macro averaged score: 0.552587\n",
      "Accuracy               : 56.2%\n",
      "Confusion matrix       :\n",
      "tensor([[104,  63,  23,  10],\n",
      "        [ 18,  56,  73,  53],\n",
      "        [  0,   6, 163,  31],\n",
      "        [  0,  19,  54, 127]], device='cuda:0')\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 0.710311  [    0/ 3200]\n",
      "loss: 0.761361  [   16/ 3200]\n",
      "loss: 0.870326  [   32/ 3200]\n",
      "loss: 0.800903  [   48/ 3200]\n",
      "loss: 0.656805  [   64/ 3200]\n",
      "loss: 0.867959  [   80/ 3200]\n",
      "loss: 0.881067  [   96/ 3200]\n",
      "loss: 0.630092  [  112/ 3200]\n",
      "loss: 1.090475  [  128/ 3200]\n",
      "loss: 0.617206  [  144/ 3200]\n",
      "loss: 0.735123  [  160/ 3200]\n",
      "loss: 0.675254  [  176/ 3200]\n",
      "loss: 0.792257  [  192/ 3200]\n",
      "loss: 1.014859  [  208/ 3200]\n",
      "loss: 0.946949  [  224/ 3200]\n",
      "loss: 0.824394  [  240/ 3200]\n",
      "loss: 1.057369  [  256/ 3200]\n",
      "loss: 0.759013  [  272/ 3200]\n",
      "loss: 0.619669  [  288/ 3200]\n",
      "loss: 0.632911  [  304/ 3200]\n",
      "loss: 0.833901  [  320/ 3200]\n",
      "loss: 0.647542  [  336/ 3200]\n",
      "loss: 0.706474  [  352/ 3200]\n",
      "loss: 0.851751  [  368/ 3200]\n",
      "loss: 0.693795  [  384/ 3200]\n",
      "loss: 1.029289  [  400/ 3200]\n",
      "loss: 0.883142  [  416/ 3200]\n",
      "loss: 0.782248  [  432/ 3200]\n",
      "loss: 0.795137  [  448/ 3200]\n",
      "loss: 0.712250  [  464/ 3200]\n",
      "loss: 1.018018  [  480/ 3200]\n",
      "loss: 0.721441  [  496/ 3200]\n",
      "loss: 1.101790  [  512/ 3200]\n",
      "loss: 0.839419  [  528/ 3200]\n",
      "loss: 1.305184  [  544/ 3200]\n",
      "loss: 0.914217  [  560/ 3200]\n",
      "loss: 1.041735  [  576/ 3200]\n",
      "loss: 0.471661  [  592/ 3200]\n",
      "loss: 0.605576  [  608/ 3200]\n",
      "loss: 0.529876  [  624/ 3200]\n",
      "loss: 0.758443  [  640/ 3200]\n",
      "loss: 0.819273  [  656/ 3200]\n",
      "loss: 0.887034  [  672/ 3200]\n",
      "loss: 0.665632  [  688/ 3200]\n",
      "loss: 0.680598  [  704/ 3200]\n",
      "loss: 0.795748  [  720/ 3200]\n",
      "loss: 1.014737  [  736/ 3200]\n",
      "loss: 1.153933  [  752/ 3200]\n",
      "loss: 1.025041  [  768/ 3200]\n",
      "loss: 0.586750  [  784/ 3200]\n",
      "loss: 0.537373  [  800/ 3200]\n",
      "loss: 0.810838  [  816/ 3200]\n",
      "loss: 0.723557  [  832/ 3200]\n",
      "loss: 0.763085  [  848/ 3200]\n",
      "loss: 0.513790  [  864/ 3200]\n",
      "loss: 0.675903  [  880/ 3200]\n",
      "loss: 0.764695  [  896/ 3200]\n",
      "loss: 0.732254  [  912/ 3200]\n",
      "loss: 0.830637  [  928/ 3200]\n",
      "loss: 1.006096  [  944/ 3200]\n",
      "loss: 0.809583  [  960/ 3200]\n",
      "loss: 0.537645  [  976/ 3200]\n",
      "loss: 0.893990  [  992/ 3200]\n",
      "loss: 0.849828  [ 1008/ 3200]\n",
      "loss: 0.573408  [ 1024/ 3200]\n",
      "loss: 0.839227  [ 1040/ 3200]\n",
      "loss: 1.167458  [ 1056/ 3200]\n",
      "loss: 0.734091  [ 1072/ 3200]\n",
      "loss: 0.822281  [ 1088/ 3200]\n",
      "loss: 0.577646  [ 1104/ 3200]\n",
      "loss: 1.170938  [ 1120/ 3200]\n",
      "loss: 0.791048  [ 1136/ 3200]\n",
      "loss: 0.779359  [ 1152/ 3200]\n",
      "loss: 0.609292  [ 1168/ 3200]\n",
      "loss: 0.931481  [ 1184/ 3200]\n",
      "loss: 1.251170  [ 1200/ 3200]\n",
      "loss: 0.724912  [ 1216/ 3200]\n",
      "loss: 0.863821  [ 1232/ 3200]\n",
      "loss: 0.957653  [ 1248/ 3200]\n",
      "loss: 0.752318  [ 1264/ 3200]\n",
      "loss: 0.948272  [ 1280/ 3200]\n",
      "loss: 0.773182  [ 1296/ 3200]\n",
      "loss: 0.721310  [ 1312/ 3200]\n",
      "loss: 0.519830  [ 1328/ 3200]\n",
      "loss: 0.818160  [ 1344/ 3200]\n",
      "loss: 0.864281  [ 1360/ 3200]\n",
      "loss: 0.834095  [ 1376/ 3200]\n",
      "loss: 0.753902  [ 1392/ 3200]\n",
      "loss: 0.853387  [ 1408/ 3200]\n",
      "loss: 0.524670  [ 1424/ 3200]\n",
      "loss: 0.780844  [ 1440/ 3200]\n",
      "loss: 0.628818  [ 1456/ 3200]\n",
      "loss: 1.387380  [ 1472/ 3200]\n",
      "loss: 0.682606  [ 1488/ 3200]\n",
      "loss: 0.605435  [ 1504/ 3200]\n",
      "loss: 0.995534  [ 1520/ 3200]\n",
      "loss: 0.752211  [ 1536/ 3200]\n",
      "loss: 0.973856  [ 1552/ 3200]\n",
      "loss: 0.765746  [ 1568/ 3200]\n",
      "loss: 0.744531  [ 1584/ 3200]\n",
      "loss: 0.621495  [ 1600/ 3200]\n",
      "loss: 0.667306  [ 1616/ 3200]\n",
      "loss: 0.481306  [ 1632/ 3200]\n",
      "loss: 1.009932  [ 1648/ 3200]\n",
      "loss: 0.994392  [ 1664/ 3200]\n",
      "loss: 0.751614  [ 1680/ 3200]\n",
      "loss: 0.770027  [ 1696/ 3200]\n",
      "loss: 1.196041  [ 1712/ 3200]\n",
      "loss: 0.784853  [ 1728/ 3200]\n",
      "loss: 1.198872  [ 1744/ 3200]\n",
      "loss: 0.595006  [ 1760/ 3200]\n",
      "loss: 0.942859  [ 1776/ 3200]\n",
      "loss: 0.793098  [ 1792/ 3200]\n",
      "loss: 0.707231  [ 1808/ 3200]\n",
      "loss: 1.051864  [ 1824/ 3200]\n",
      "loss: 0.979938  [ 1840/ 3200]\n",
      "loss: 0.633045  [ 1856/ 3200]\n",
      "loss: 0.705708  [ 1872/ 3200]\n",
      "loss: 0.837401  [ 1888/ 3200]\n",
      "loss: 0.953919  [ 1904/ 3200]\n",
      "loss: 0.786626  [ 1920/ 3200]\n",
      "loss: 0.693058  [ 1936/ 3200]\n",
      "loss: 0.846414  [ 1952/ 3200]\n",
      "loss: 1.182696  [ 1968/ 3200]\n",
      "loss: 0.821915  [ 1984/ 3200]\n",
      "loss: 0.642862  [ 2000/ 3200]\n",
      "loss: 0.589073  [ 2016/ 3200]\n",
      "loss: 1.038526  [ 2032/ 3200]\n",
      "loss: 0.545095  [ 2048/ 3200]\n",
      "loss: 0.638129  [ 2064/ 3200]\n",
      "loss: 0.513815  [ 2080/ 3200]\n",
      "loss: 0.503800  [ 2096/ 3200]\n",
      "loss: 1.077590  [ 2112/ 3200]\n",
      "loss: 0.626571  [ 2128/ 3200]\n",
      "loss: 0.483016  [ 2144/ 3200]\n",
      "loss: 0.788967  [ 2160/ 3200]\n",
      "loss: 0.609452  [ 2176/ 3200]\n",
      "loss: 0.732058  [ 2192/ 3200]\n",
      "loss: 1.091745  [ 2208/ 3200]\n",
      "loss: 0.797643  [ 2224/ 3200]\n",
      "loss: 0.832631  [ 2240/ 3200]\n",
      "loss: 0.495184  [ 2256/ 3200]\n",
      "loss: 0.528282  [ 2272/ 3200]\n",
      "loss: 0.851810  [ 2288/ 3200]\n",
      "loss: 0.758295  [ 2304/ 3200]\n",
      "loss: 0.770224  [ 2320/ 3200]\n",
      "loss: 0.636550  [ 2336/ 3200]\n",
      "loss: 1.124528  [ 2352/ 3200]\n",
      "loss: 0.739107  [ 2368/ 3200]\n",
      "loss: 0.817900  [ 2384/ 3200]\n",
      "loss: 0.747191  [ 2400/ 3200]\n",
      "loss: 0.622706  [ 2416/ 3200]\n",
      "loss: 0.953436  [ 2432/ 3200]\n",
      "loss: 0.948018  [ 2448/ 3200]\n",
      "loss: 1.123520  [ 2464/ 3200]\n",
      "loss: 0.913534  [ 2480/ 3200]\n",
      "loss: 1.255607  [ 2496/ 3200]\n",
      "loss: 0.699209  [ 2512/ 3200]\n",
      "loss: 0.944409  [ 2528/ 3200]\n",
      "loss: 0.785734  [ 2544/ 3200]\n",
      "loss: 0.912104  [ 2560/ 3200]\n",
      "loss: 1.035714  [ 2576/ 3200]\n",
      "loss: 0.821580  [ 2592/ 3200]\n",
      "loss: 0.841374  [ 2608/ 3200]\n",
      "loss: 0.656688  [ 2624/ 3200]\n",
      "loss: 0.649142  [ 2640/ 3200]\n",
      "loss: 0.607584  [ 2656/ 3200]\n",
      "loss: 0.654532  [ 2672/ 3200]\n",
      "loss: 0.805774  [ 2688/ 3200]\n",
      "loss: 0.775762  [ 2704/ 3200]\n",
      "loss: 0.933791  [ 2720/ 3200]\n",
      "loss: 1.314087  [ 2736/ 3200]\n",
      "loss: 0.633263  [ 2752/ 3200]\n",
      "loss: 0.460695  [ 2768/ 3200]\n",
      "loss: 0.865682  [ 2784/ 3200]\n",
      "loss: 0.691691  [ 2800/ 3200]\n",
      "loss: 0.758990  [ 2816/ 3200]\n",
      "loss: 0.796071  [ 2832/ 3200]\n",
      "loss: 1.003271  [ 2848/ 3200]\n",
      "loss: 0.792516  [ 2864/ 3200]\n",
      "loss: 0.745704  [ 2880/ 3200]\n",
      "loss: 0.902924  [ 2896/ 3200]\n",
      "loss: 0.467842  [ 2912/ 3200]\n",
      "loss: 0.494288  [ 2928/ 3200]\n",
      "loss: 0.678478  [ 2944/ 3200]\n",
      "loss: 0.991666  [ 2960/ 3200]\n",
      "loss: 0.769263  [ 2976/ 3200]\n",
      "loss: 0.779802  [ 2992/ 3200]\n",
      "loss: 0.948530  [ 3008/ 3200]\n",
      "loss: 0.677449  [ 3024/ 3200]\n",
      "loss: 0.708560  [ 3040/ 3200]\n",
      "loss: 0.576104  [ 3056/ 3200]\n",
      "loss: 0.806442  [ 3072/ 3200]\n",
      "loss: 0.735527  [ 3088/ 3200]\n",
      "loss: 0.743754  [ 3104/ 3200]\n",
      "loss: 0.694477  [ 3120/ 3200]\n",
      "loss: 0.611560  [ 3136/ 3200]\n",
      "loss: 1.111213  [ 3152/ 3200]\n",
      "loss: 1.427297  [ 3168/ 3200]\n",
      "loss: 0.711561  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.059707\n",
      "f1 macro averaged score: 0.566760\n",
      "Accuracy               : 58.2%\n",
      "Confusion matrix       :\n",
      "tensor([[140,  37,   4,  19],\n",
      "        [ 30,  45,  34,  91],\n",
      "        [  0,  10, 118,  72],\n",
      "        [  2,   8,  27, 163]], device='cuda:0')\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 0.792963  [    0/ 3200]\n",
      "loss: 0.855051  [   16/ 3200]\n",
      "loss: 0.417245  [   32/ 3200]\n",
      "loss: 0.997067  [   48/ 3200]\n",
      "loss: 1.072158  [   64/ 3200]\n",
      "loss: 0.792889  [   80/ 3200]\n",
      "loss: 0.441493  [   96/ 3200]\n",
      "loss: 0.867851  [  112/ 3200]\n",
      "loss: 0.665244  [  128/ 3200]\n",
      "loss: 1.085415  [  144/ 3200]\n",
      "loss: 1.073137  [  160/ 3200]\n",
      "loss: 0.878670  [  176/ 3200]\n",
      "loss: 0.652866  [  192/ 3200]\n",
      "loss: 0.959576  [  208/ 3200]\n",
      "loss: 0.864387  [  224/ 3200]\n",
      "loss: 0.517065  [  240/ 3200]\n",
      "loss: 0.797202  [  256/ 3200]\n",
      "loss: 0.737891  [  272/ 3200]\n",
      "loss: 0.733570  [  288/ 3200]\n",
      "loss: 0.631371  [  304/ 3200]\n",
      "loss: 0.653243  [  320/ 3200]\n",
      "loss: 0.881963  [  336/ 3200]\n",
      "loss: 0.822336  [  352/ 3200]\n",
      "loss: 0.549114  [  368/ 3200]\n",
      "loss: 0.710827  [  384/ 3200]\n",
      "loss: 0.612006  [  400/ 3200]\n",
      "loss: 0.731117  [  416/ 3200]\n",
      "loss: 0.633514  [  432/ 3200]\n",
      "loss: 0.693168  [  448/ 3200]\n",
      "loss: 0.872788  [  464/ 3200]\n",
      "loss: 0.666124  [  480/ 3200]\n",
      "loss: 0.785705  [  496/ 3200]\n",
      "loss: 0.652910  [  512/ 3200]\n",
      "loss: 0.733493  [  528/ 3200]\n",
      "loss: 0.617391  [  544/ 3200]\n",
      "loss: 0.869529  [  560/ 3200]\n",
      "loss: 0.656972  [  576/ 3200]\n",
      "loss: 0.560031  [  592/ 3200]\n",
      "loss: 0.863866  [  608/ 3200]\n",
      "loss: 0.890927  [  624/ 3200]\n",
      "loss: 0.716338  [  640/ 3200]\n",
      "loss: 0.730916  [  656/ 3200]\n",
      "loss: 0.707676  [  672/ 3200]\n",
      "loss: 0.812873  [  688/ 3200]\n",
      "loss: 0.783318  [  704/ 3200]\n",
      "loss: 1.166476  [  720/ 3200]\n",
      "loss: 0.920988  [  736/ 3200]\n",
      "loss: 1.117383  [  752/ 3200]\n",
      "loss: 0.778584  [  768/ 3200]\n",
      "loss: 0.572594  [  784/ 3200]\n",
      "loss: 0.672195  [  800/ 3200]\n",
      "loss: 0.835935  [  816/ 3200]\n",
      "loss: 0.932143  [  832/ 3200]\n",
      "loss: 0.754929  [  848/ 3200]\n",
      "loss: 0.992227  [  864/ 3200]\n",
      "loss: 0.663818  [  880/ 3200]\n",
      "loss: 1.009544  [  896/ 3200]\n",
      "loss: 0.888121  [  912/ 3200]\n",
      "loss: 0.844912  [  928/ 3200]\n",
      "loss: 1.025779  [  944/ 3200]\n",
      "loss: 0.812581  [  960/ 3200]\n",
      "loss: 0.707423  [  976/ 3200]\n",
      "loss: 0.875668  [  992/ 3200]\n",
      "loss: 0.763675  [ 1008/ 3200]\n",
      "loss: 0.808376  [ 1024/ 3200]\n",
      "loss: 0.816058  [ 1040/ 3200]\n",
      "loss: 0.716771  [ 1056/ 3200]\n",
      "loss: 0.817993  [ 1072/ 3200]\n",
      "loss: 0.748524  [ 1088/ 3200]\n",
      "loss: 0.675332  [ 1104/ 3200]\n",
      "loss: 0.696769  [ 1120/ 3200]\n",
      "loss: 0.921968  [ 1136/ 3200]\n",
      "loss: 0.923655  [ 1152/ 3200]\n",
      "loss: 0.884090  [ 1168/ 3200]\n",
      "loss: 0.744851  [ 1184/ 3200]\n",
      "loss: 0.736310  [ 1200/ 3200]\n",
      "loss: 0.541409  [ 1216/ 3200]\n",
      "loss: 0.687350  [ 1232/ 3200]\n",
      "loss: 0.794941  [ 1248/ 3200]\n",
      "loss: 0.978583  [ 1264/ 3200]\n",
      "loss: 0.795600  [ 1280/ 3200]\n",
      "loss: 0.677384  [ 1296/ 3200]\n",
      "loss: 0.747336  [ 1312/ 3200]\n",
      "loss: 0.675394  [ 1328/ 3200]\n",
      "loss: 0.854620  [ 1344/ 3200]\n",
      "loss: 0.669977  [ 1360/ 3200]\n",
      "loss: 1.046816  [ 1376/ 3200]\n",
      "loss: 0.651041  [ 1392/ 3200]\n",
      "loss: 1.194232  [ 1408/ 3200]\n",
      "loss: 0.730697  [ 1424/ 3200]\n",
      "loss: 0.842936  [ 1440/ 3200]\n",
      "loss: 0.678276  [ 1456/ 3200]\n",
      "loss: 0.597497  [ 1472/ 3200]\n",
      "loss: 0.987291  [ 1488/ 3200]\n",
      "loss: 0.821402  [ 1504/ 3200]\n",
      "loss: 0.584004  [ 1520/ 3200]\n",
      "loss: 0.760753  [ 1536/ 3200]\n",
      "loss: 0.626345  [ 1552/ 3200]\n",
      "loss: 0.890156  [ 1568/ 3200]\n",
      "loss: 0.702628  [ 1584/ 3200]\n",
      "loss: 1.121831  [ 1600/ 3200]\n",
      "loss: 0.966539  [ 1616/ 3200]\n",
      "loss: 0.725953  [ 1632/ 3200]\n",
      "loss: 0.627216  [ 1648/ 3200]\n",
      "loss: 0.628374  [ 1664/ 3200]\n",
      "loss: 0.903445  [ 1680/ 3200]\n",
      "loss: 0.600798  [ 1696/ 3200]\n",
      "loss: 0.631108  [ 1712/ 3200]\n",
      "loss: 0.646334  [ 1728/ 3200]\n",
      "loss: 0.944798  [ 1744/ 3200]\n",
      "loss: 0.791646  [ 1760/ 3200]\n",
      "loss: 0.719771  [ 1776/ 3200]\n",
      "loss: 0.791692  [ 1792/ 3200]\n",
      "loss: 0.766457  [ 1808/ 3200]\n",
      "loss: 0.510008  [ 1824/ 3200]\n",
      "loss: 0.782149  [ 1840/ 3200]\n",
      "loss: 0.781428  [ 1856/ 3200]\n",
      "loss: 0.893314  [ 1872/ 3200]\n",
      "loss: 0.782886  [ 1888/ 3200]\n",
      "loss: 0.640849  [ 1904/ 3200]\n",
      "loss: 0.555983  [ 1920/ 3200]\n",
      "loss: 0.915590  [ 1936/ 3200]\n",
      "loss: 0.829726  [ 1952/ 3200]\n",
      "loss: 0.993231  [ 1968/ 3200]\n",
      "loss: 0.812857  [ 1984/ 3200]\n",
      "loss: 0.972941  [ 2000/ 3200]\n",
      "loss: 0.868460  [ 2016/ 3200]\n",
      "loss: 0.697770  [ 2032/ 3200]\n",
      "loss: 0.805954  [ 2048/ 3200]\n",
      "loss: 0.941329  [ 2064/ 3200]\n",
      "loss: 1.062904  [ 2080/ 3200]\n",
      "loss: 0.669096  [ 2096/ 3200]\n",
      "loss: 0.843787  [ 2112/ 3200]\n",
      "loss: 1.263564  [ 2128/ 3200]\n",
      "loss: 0.874603  [ 2144/ 3200]\n",
      "loss: 0.797022  [ 2160/ 3200]\n",
      "loss: 0.632966  [ 2176/ 3200]\n",
      "loss: 0.526548  [ 2192/ 3200]\n",
      "loss: 0.804908  [ 2208/ 3200]\n",
      "loss: 0.609356  [ 2224/ 3200]\n",
      "loss: 1.063773  [ 2240/ 3200]\n",
      "loss: 0.764841  [ 2256/ 3200]\n",
      "loss: 0.872603  [ 2272/ 3200]\n",
      "loss: 0.811898  [ 2288/ 3200]\n",
      "loss: 1.103677  [ 2304/ 3200]\n",
      "loss: 0.781733  [ 2320/ 3200]\n",
      "loss: 0.737043  [ 2336/ 3200]\n",
      "loss: 0.635506  [ 2352/ 3200]\n",
      "loss: 0.844934  [ 2368/ 3200]\n",
      "loss: 0.898009  [ 2384/ 3200]\n",
      "loss: 0.704005  [ 2400/ 3200]\n",
      "loss: 0.793332  [ 2416/ 3200]\n",
      "loss: 0.643816  [ 2432/ 3200]\n",
      "loss: 0.444051  [ 2448/ 3200]\n",
      "loss: 0.816164  [ 2464/ 3200]\n",
      "loss: 0.898036  [ 2480/ 3200]\n",
      "loss: 1.083122  [ 2496/ 3200]\n",
      "loss: 0.637148  [ 2512/ 3200]\n",
      "loss: 0.581198  [ 2528/ 3200]\n",
      "loss: 0.709431  [ 2544/ 3200]\n",
      "loss: 0.869950  [ 2560/ 3200]\n",
      "loss: 0.961893  [ 2576/ 3200]\n",
      "loss: 0.547793  [ 2592/ 3200]\n",
      "loss: 0.542569  [ 2608/ 3200]\n",
      "loss: 1.088415  [ 2624/ 3200]\n",
      "loss: 0.801012  [ 2640/ 3200]\n",
      "loss: 0.498828  [ 2656/ 3200]\n",
      "loss: 1.121794  [ 2672/ 3200]\n",
      "loss: 0.698511  [ 2688/ 3200]\n",
      "loss: 0.692065  [ 2704/ 3200]\n",
      "loss: 0.560973  [ 2720/ 3200]\n",
      "loss: 1.135359  [ 2736/ 3200]\n",
      "loss: 1.091842  [ 2752/ 3200]\n",
      "loss: 0.601454  [ 2768/ 3200]\n",
      "loss: 0.466370  [ 2784/ 3200]\n",
      "loss: 0.830180  [ 2800/ 3200]\n",
      "loss: 0.807358  [ 2816/ 3200]\n",
      "loss: 0.829590  [ 2832/ 3200]\n",
      "loss: 0.713914  [ 2848/ 3200]\n",
      "loss: 1.173873  [ 2864/ 3200]\n",
      "loss: 0.661116  [ 2880/ 3200]\n",
      "loss: 0.840471  [ 2896/ 3200]\n",
      "loss: 0.490474  [ 2912/ 3200]\n",
      "loss: 0.689651  [ 2928/ 3200]\n",
      "loss: 0.970771  [ 2944/ 3200]\n",
      "loss: 0.882833  [ 2960/ 3200]\n",
      "loss: 0.521498  [ 2976/ 3200]\n",
      "loss: 0.382870  [ 2992/ 3200]\n",
      "loss: 0.454279  [ 3008/ 3200]\n",
      "loss: 0.846141  [ 3024/ 3200]\n",
      "loss: 0.478766  [ 3040/ 3200]\n",
      "loss: 0.716473  [ 3056/ 3200]\n",
      "loss: 0.918327  [ 3072/ 3200]\n",
      "loss: 0.979143  [ 3088/ 3200]\n",
      "loss: 0.679714  [ 3104/ 3200]\n",
      "loss: 0.703752  [ 3120/ 3200]\n",
      "loss: 0.692579  [ 3136/ 3200]\n",
      "loss: 0.609154  [ 3152/ 3200]\n",
      "loss: 0.629741  [ 3168/ 3200]\n",
      "loss: 0.484296  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.057264\n",
      "f1 macro averaged score: 0.603865\n",
      "Accuracy               : 60.5%\n",
      "Confusion matrix       :\n",
      "tensor([[140,  46,   3,  11],\n",
      "        [ 22,  75,  38,  65],\n",
      "        [  0,  18, 122,  60],\n",
      "        [  2,  21,  30, 147]], device='cuda:0')\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 0.862913  [    0/ 3200]\n",
      "loss: 0.778611  [   16/ 3200]\n",
      "loss: 1.315461  [   32/ 3200]\n",
      "loss: 1.026914  [   48/ 3200]\n",
      "loss: 0.859274  [   64/ 3200]\n",
      "loss: 0.954566  [   80/ 3200]\n",
      "loss: 0.699505  [   96/ 3200]\n",
      "loss: 0.636274  [  112/ 3200]\n",
      "loss: 0.715450  [  128/ 3200]\n",
      "loss: 0.636240  [  144/ 3200]\n",
      "loss: 0.510920  [  160/ 3200]\n",
      "loss: 0.966013  [  176/ 3200]\n",
      "loss: 0.762152  [  192/ 3200]\n",
      "loss: 0.616904  [  208/ 3200]\n",
      "loss: 0.706372  [  224/ 3200]\n",
      "loss: 0.562622  [  240/ 3200]\n",
      "loss: 0.781756  [  256/ 3200]\n",
      "loss: 0.882346  [  272/ 3200]\n",
      "loss: 0.821868  [  288/ 3200]\n",
      "loss: 0.878238  [  304/ 3200]\n",
      "loss: 0.989471  [  320/ 3200]\n",
      "loss: 0.567828  [  336/ 3200]\n",
      "loss: 0.995975  [  352/ 3200]\n",
      "loss: 0.557153  [  368/ 3200]\n",
      "loss: 0.774238  [  384/ 3200]\n",
      "loss: 0.766253  [  400/ 3200]\n",
      "loss: 0.509677  [  416/ 3200]\n",
      "loss: 0.970492  [  432/ 3200]\n",
      "loss: 0.511338  [  448/ 3200]\n",
      "loss: 0.617290  [  464/ 3200]\n",
      "loss: 0.650779  [  480/ 3200]\n",
      "loss: 1.197632  [  496/ 3200]\n",
      "loss: 0.884448  [  512/ 3200]\n",
      "loss: 0.853147  [  528/ 3200]\n",
      "loss: 0.964371  [  544/ 3200]\n",
      "loss: 0.539803  [  560/ 3200]\n",
      "loss: 0.744763  [  576/ 3200]\n",
      "loss: 1.273848  [  592/ 3200]\n",
      "loss: 0.861048  [  608/ 3200]\n",
      "loss: 0.709503  [  624/ 3200]\n",
      "loss: 0.885963  [  640/ 3200]\n",
      "loss: 0.960870  [  656/ 3200]\n",
      "loss: 0.777333  [  672/ 3200]\n",
      "loss: 0.728474  [  688/ 3200]\n",
      "loss: 0.822994  [  704/ 3200]\n",
      "loss: 0.788048  [  720/ 3200]\n",
      "loss: 0.855482  [  736/ 3200]\n",
      "loss: 0.525948  [  752/ 3200]\n",
      "loss: 0.934836  [  768/ 3200]\n",
      "loss: 0.816820  [  784/ 3200]\n",
      "loss: 1.132868  [  800/ 3200]\n",
      "loss: 0.880174  [  816/ 3200]\n",
      "loss: 0.848148  [  832/ 3200]\n",
      "loss: 0.718548  [  848/ 3200]\n",
      "loss: 0.588856  [  864/ 3200]\n",
      "loss: 0.740394  [  880/ 3200]\n",
      "loss: 0.664404  [  896/ 3200]\n",
      "loss: 1.211902  [  912/ 3200]\n",
      "loss: 1.062670  [  928/ 3200]\n",
      "loss: 0.597003  [  944/ 3200]\n",
      "loss: 0.873543  [  960/ 3200]\n",
      "loss: 0.443241  [  976/ 3200]\n",
      "loss: 1.277460  [  992/ 3200]\n",
      "loss: 0.706696  [ 1008/ 3200]\n",
      "loss: 0.741525  [ 1024/ 3200]\n",
      "loss: 1.028098  [ 1040/ 3200]\n",
      "loss: 0.825767  [ 1056/ 3200]\n",
      "loss: 0.480792  [ 1072/ 3200]\n",
      "loss: 0.908565  [ 1088/ 3200]\n",
      "loss: 0.795765  [ 1104/ 3200]\n",
      "loss: 0.792269  [ 1120/ 3200]\n",
      "loss: 0.602190  [ 1136/ 3200]\n",
      "loss: 0.517073  [ 1152/ 3200]\n",
      "loss: 0.593613  [ 1168/ 3200]\n",
      "loss: 0.836368  [ 1184/ 3200]\n",
      "loss: 0.646281  [ 1200/ 3200]\n",
      "loss: 0.993913  [ 1216/ 3200]\n",
      "loss: 0.820524  [ 1232/ 3200]\n",
      "loss: 1.068451  [ 1248/ 3200]\n",
      "loss: 0.779717  [ 1264/ 3200]\n",
      "loss: 0.915248  [ 1280/ 3200]\n",
      "loss: 0.778619  [ 1296/ 3200]\n",
      "loss: 0.552662  [ 1312/ 3200]\n",
      "loss: 0.599485  [ 1328/ 3200]\n",
      "loss: 0.814359  [ 1344/ 3200]\n",
      "loss: 0.448543  [ 1360/ 3200]\n",
      "loss: 0.488052  [ 1376/ 3200]\n",
      "loss: 0.806425  [ 1392/ 3200]\n",
      "loss: 0.784902  [ 1408/ 3200]\n",
      "loss: 0.671619  [ 1424/ 3200]\n",
      "loss: 0.813732  [ 1440/ 3200]\n",
      "loss: 0.955301  [ 1456/ 3200]\n",
      "loss: 0.550465  [ 1472/ 3200]\n",
      "loss: 0.832180  [ 1488/ 3200]\n",
      "loss: 0.800843  [ 1504/ 3200]\n",
      "loss: 0.463652  [ 1520/ 3200]\n",
      "loss: 0.825399  [ 1536/ 3200]\n",
      "loss: 0.702329  [ 1552/ 3200]\n",
      "loss: 0.751694  [ 1568/ 3200]\n",
      "loss: 0.917904  [ 1584/ 3200]\n",
      "loss: 0.496085  [ 1600/ 3200]\n",
      "loss: 1.093890  [ 1616/ 3200]\n",
      "loss: 0.690445  [ 1632/ 3200]\n",
      "loss: 0.629753  [ 1648/ 3200]\n",
      "loss: 0.858783  [ 1664/ 3200]\n",
      "loss: 0.716883  [ 1680/ 3200]\n",
      "loss: 0.570085  [ 1696/ 3200]\n",
      "loss: 0.634592  [ 1712/ 3200]\n",
      "loss: 0.457968  [ 1728/ 3200]\n",
      "loss: 0.693140  [ 1744/ 3200]\n",
      "loss: 0.629350  [ 1760/ 3200]\n",
      "loss: 0.556680  [ 1776/ 3200]\n",
      "loss: 0.630490  [ 1792/ 3200]\n",
      "loss: 0.853904  [ 1808/ 3200]\n",
      "loss: 0.614579  [ 1824/ 3200]\n",
      "loss: 0.711323  [ 1840/ 3200]\n",
      "loss: 0.572487  [ 1856/ 3200]\n",
      "loss: 1.057803  [ 1872/ 3200]\n",
      "loss: 0.919501  [ 1888/ 3200]\n",
      "loss: 0.668099  [ 1904/ 3200]\n",
      "loss: 0.578840  [ 1920/ 3200]\n",
      "loss: 0.829419  [ 1936/ 3200]\n",
      "loss: 0.607322  [ 1952/ 3200]\n",
      "loss: 0.763863  [ 1968/ 3200]\n",
      "loss: 0.888051  [ 1984/ 3200]\n",
      "loss: 1.311266  [ 2000/ 3200]\n",
      "loss: 0.486767  [ 2016/ 3200]\n",
      "loss: 0.642937  [ 2032/ 3200]\n",
      "loss: 0.793795  [ 2048/ 3200]\n",
      "loss: 0.962659  [ 2064/ 3200]\n",
      "loss: 0.664956  [ 2080/ 3200]\n",
      "loss: 0.658533  [ 2096/ 3200]\n",
      "loss: 0.680784  [ 2112/ 3200]\n",
      "loss: 0.563027  [ 2128/ 3200]\n",
      "loss: 0.673073  [ 2144/ 3200]\n",
      "loss: 1.098902  [ 2160/ 3200]\n",
      "loss: 0.824293  [ 2176/ 3200]\n",
      "loss: 0.696937  [ 2192/ 3200]\n",
      "loss: 0.421013  [ 2208/ 3200]\n",
      "loss: 0.744240  [ 2224/ 3200]\n",
      "loss: 0.564905  [ 2240/ 3200]\n",
      "loss: 1.492203  [ 2256/ 3200]\n",
      "loss: 1.190286  [ 2272/ 3200]\n",
      "loss: 0.503487  [ 2288/ 3200]\n",
      "loss: 0.948178  [ 2304/ 3200]\n",
      "loss: 0.739308  [ 2320/ 3200]\n",
      "loss: 0.762360  [ 2336/ 3200]\n",
      "loss: 0.569679  [ 2352/ 3200]\n",
      "loss: 0.947662  [ 2368/ 3200]\n",
      "loss: 0.707082  [ 2384/ 3200]\n",
      "loss: 0.835373  [ 2400/ 3200]\n",
      "loss: 0.615841  [ 2416/ 3200]\n",
      "loss: 0.895295  [ 2432/ 3200]\n",
      "loss: 0.534053  [ 2448/ 3200]\n",
      "loss: 0.638084  [ 2464/ 3200]\n",
      "loss: 0.703011  [ 2480/ 3200]\n",
      "loss: 0.599119  [ 2496/ 3200]\n",
      "loss: 0.614183  [ 2512/ 3200]\n",
      "loss: 0.678635  [ 2528/ 3200]\n",
      "loss: 0.711621  [ 2544/ 3200]\n",
      "loss: 0.856853  [ 2560/ 3200]\n",
      "loss: 0.864000  [ 2576/ 3200]\n",
      "loss: 0.705010  [ 2592/ 3200]\n",
      "loss: 0.612911  [ 2608/ 3200]\n",
      "loss: 1.118364  [ 2624/ 3200]\n",
      "loss: 0.857523  [ 2640/ 3200]\n",
      "loss: 0.493278  [ 2656/ 3200]\n",
      "loss: 0.710411  [ 2672/ 3200]\n",
      "loss: 0.640559  [ 2688/ 3200]\n",
      "loss: 1.089634  [ 2704/ 3200]\n",
      "loss: 0.805437  [ 2720/ 3200]\n",
      "loss: 0.664103  [ 2736/ 3200]\n",
      "loss: 0.534123  [ 2752/ 3200]\n",
      "loss: 0.819974  [ 2768/ 3200]\n",
      "loss: 0.806807  [ 2784/ 3200]\n",
      "loss: 0.495209  [ 2800/ 3200]\n",
      "loss: 0.845743  [ 2816/ 3200]\n",
      "loss: 0.862672  [ 2832/ 3200]\n",
      "loss: 0.744868  [ 2848/ 3200]\n",
      "loss: 0.671408  [ 2864/ 3200]\n",
      "loss: 1.122671  [ 2880/ 3200]\n",
      "loss: 0.654772  [ 2896/ 3200]\n",
      "loss: 0.567511  [ 2912/ 3200]\n",
      "loss: 0.712470  [ 2928/ 3200]\n",
      "loss: 0.838586  [ 2944/ 3200]\n",
      "loss: 0.964438  [ 2960/ 3200]\n",
      "loss: 0.887655  [ 2976/ 3200]\n",
      "loss: 0.449418  [ 2992/ 3200]\n",
      "loss: 0.586917  [ 3008/ 3200]\n",
      "loss: 0.929470  [ 3024/ 3200]\n",
      "loss: 0.585842  [ 3040/ 3200]\n",
      "loss: 0.833504  [ 3056/ 3200]\n",
      "loss: 0.724825  [ 3072/ 3200]\n",
      "loss: 0.810987  [ 3088/ 3200]\n",
      "loss: 0.726625  [ 3104/ 3200]\n",
      "loss: 0.867096  [ 3120/ 3200]\n",
      "loss: 1.126160  [ 3136/ 3200]\n",
      "loss: 0.581964  [ 3152/ 3200]\n",
      "loss: 0.428241  [ 3168/ 3200]\n",
      "loss: 0.854815  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.059957\n",
      "f1 macro averaged score: 0.577963\n",
      "Accuracy               : 59.6%\n",
      "Confusion matrix       :\n",
      "tensor([[132,  38,  18,  12],\n",
      "        [ 18,  44,  81,  57],\n",
      "        [  0,   8, 168,  24],\n",
      "        [  1,   8,  58, 133]], device='cuda:0')\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 1.075665  [    0/ 3200]\n",
      "loss: 0.837519  [   16/ 3200]\n",
      "loss: 0.745327  [   32/ 3200]\n",
      "loss: 0.689491  [   48/ 3200]\n",
      "loss: 0.673489  [   64/ 3200]\n",
      "loss: 0.971846  [   80/ 3200]\n",
      "loss: 0.772633  [   96/ 3200]\n",
      "loss: 0.704933  [  112/ 3200]\n",
      "loss: 0.613497  [  128/ 3200]\n",
      "loss: 0.716814  [  144/ 3200]\n",
      "loss: 0.937808  [  160/ 3200]\n",
      "loss: 0.607505  [  176/ 3200]\n",
      "loss: 0.730473  [  192/ 3200]\n",
      "loss: 0.777148  [  208/ 3200]\n",
      "loss: 0.748118  [  224/ 3200]\n",
      "loss: 0.838330  [  240/ 3200]\n",
      "loss: 1.315909  [  256/ 3200]\n",
      "loss: 0.893268  [  272/ 3200]\n",
      "loss: 0.665717  [  288/ 3200]\n",
      "loss: 0.625995  [  304/ 3200]\n",
      "loss: 0.639653  [  320/ 3200]\n",
      "loss: 1.130174  [  336/ 3200]\n",
      "loss: 0.697559  [  352/ 3200]\n",
      "loss: 0.974807  [  368/ 3200]\n",
      "loss: 1.044682  [  384/ 3200]\n",
      "loss: 0.551778  [  400/ 3200]\n",
      "loss: 0.722243  [  416/ 3200]\n",
      "loss: 0.617327  [  432/ 3200]\n",
      "loss: 0.613650  [  448/ 3200]\n",
      "loss: 0.534768  [  464/ 3200]\n",
      "loss: 0.850059  [  480/ 3200]\n",
      "loss: 0.604946  [  496/ 3200]\n",
      "loss: 0.680396  [  512/ 3200]\n",
      "loss: 0.963302  [  528/ 3200]\n",
      "loss: 0.554321  [  544/ 3200]\n",
      "loss: 0.853741  [  560/ 3200]\n",
      "loss: 0.641435  [  576/ 3200]\n",
      "loss: 0.966681  [  592/ 3200]\n",
      "loss: 0.830236  [  608/ 3200]\n",
      "loss: 0.849831  [  624/ 3200]\n",
      "loss: 1.013271  [  640/ 3200]\n",
      "loss: 0.865845  [  656/ 3200]\n",
      "loss: 0.548849  [  672/ 3200]\n",
      "loss: 0.827297  [  688/ 3200]\n",
      "loss: 0.592753  [  704/ 3200]\n",
      "loss: 0.971782  [  720/ 3200]\n",
      "loss: 0.659746  [  736/ 3200]\n",
      "loss: 0.706465  [  752/ 3200]\n",
      "loss: 0.777089  [  768/ 3200]\n",
      "loss: 0.738505  [  784/ 3200]\n",
      "loss: 0.945193  [  800/ 3200]\n",
      "loss: 0.762069  [  816/ 3200]\n",
      "loss: 0.782086  [  832/ 3200]\n",
      "loss: 0.580634  [  848/ 3200]\n",
      "loss: 0.942813  [  864/ 3200]\n",
      "loss: 0.795679  [  880/ 3200]\n",
      "loss: 1.074804  [  896/ 3200]\n",
      "loss: 1.130999  [  912/ 3200]\n",
      "loss: 1.024487  [  928/ 3200]\n",
      "loss: 0.566231  [  944/ 3200]\n",
      "loss: 0.874191  [  960/ 3200]\n",
      "loss: 0.768910  [  976/ 3200]\n",
      "loss: 0.723047  [  992/ 3200]\n",
      "loss: 0.687149  [ 1008/ 3200]\n",
      "loss: 0.607520  [ 1024/ 3200]\n",
      "loss: 0.648973  [ 1040/ 3200]\n",
      "loss: 0.600802  [ 1056/ 3200]\n",
      "loss: 0.678527  [ 1072/ 3200]\n",
      "loss: 0.578610  [ 1088/ 3200]\n",
      "loss: 0.613529  [ 1104/ 3200]\n",
      "loss: 0.832768  [ 1120/ 3200]\n",
      "loss: 0.788109  [ 1136/ 3200]\n",
      "loss: 0.576468  [ 1152/ 3200]\n",
      "loss: 0.721043  [ 1168/ 3200]\n",
      "loss: 0.752784  [ 1184/ 3200]\n",
      "loss: 1.032073  [ 1200/ 3200]\n",
      "loss: 0.722497  [ 1216/ 3200]\n",
      "loss: 0.896191  [ 1232/ 3200]\n",
      "loss: 0.577994  [ 1248/ 3200]\n",
      "loss: 0.746366  [ 1264/ 3200]\n",
      "loss: 0.461062  [ 1280/ 3200]\n",
      "loss: 0.669859  [ 1296/ 3200]\n",
      "loss: 0.833805  [ 1312/ 3200]\n",
      "loss: 0.514690  [ 1328/ 3200]\n",
      "loss: 0.544229  [ 1344/ 3200]\n",
      "loss: 0.844954  [ 1360/ 3200]\n",
      "loss: 0.375869  [ 1376/ 3200]\n",
      "loss: 0.860911  [ 1392/ 3200]\n",
      "loss: 0.442791  [ 1408/ 3200]\n",
      "loss: 0.699321  [ 1424/ 3200]\n",
      "loss: 0.943199  [ 1440/ 3200]\n",
      "loss: 0.847158  [ 1456/ 3200]\n",
      "loss: 0.654617  [ 1472/ 3200]\n",
      "loss: 0.719806  [ 1488/ 3200]\n",
      "loss: 0.979318  [ 1504/ 3200]\n",
      "loss: 0.845553  [ 1520/ 3200]\n",
      "loss: 0.844068  [ 1536/ 3200]\n",
      "loss: 0.918273  [ 1552/ 3200]\n",
      "loss: 0.590065  [ 1568/ 3200]\n",
      "loss: 0.556325  [ 1584/ 3200]\n",
      "loss: 1.312928  [ 1600/ 3200]\n",
      "loss: 0.520761  [ 1616/ 3200]\n",
      "loss: 0.707190  [ 1632/ 3200]\n",
      "loss: 0.752760  [ 1648/ 3200]\n",
      "loss: 0.890398  [ 1664/ 3200]\n",
      "loss: 0.611261  [ 1680/ 3200]\n",
      "loss: 0.620615  [ 1696/ 3200]\n",
      "loss: 0.695761  [ 1712/ 3200]\n",
      "loss: 0.763459  [ 1728/ 3200]\n",
      "loss: 1.165713  [ 1744/ 3200]\n",
      "loss: 0.687506  [ 1760/ 3200]\n",
      "loss: 0.413058  [ 1776/ 3200]\n",
      "loss: 0.429102  [ 1792/ 3200]\n",
      "loss: 0.744677  [ 1808/ 3200]\n",
      "loss: 0.782034  [ 1824/ 3200]\n",
      "loss: 0.666569  [ 1840/ 3200]\n",
      "loss: 0.837945  [ 1856/ 3200]\n",
      "loss: 0.510080  [ 1872/ 3200]\n",
      "loss: 0.664026  [ 1888/ 3200]\n",
      "loss: 0.809666  [ 1904/ 3200]\n",
      "loss: 0.787174  [ 1920/ 3200]\n",
      "loss: 0.795953  [ 1936/ 3200]\n",
      "loss: 0.707982  [ 1952/ 3200]\n",
      "loss: 1.019073  [ 1968/ 3200]\n",
      "loss: 0.687552  [ 1984/ 3200]\n",
      "loss: 0.499895  [ 2000/ 3200]\n",
      "loss: 0.771136  [ 2016/ 3200]\n",
      "loss: 0.527363  [ 2032/ 3200]\n",
      "loss: 0.752296  [ 2048/ 3200]\n",
      "loss: 0.807716  [ 2064/ 3200]\n",
      "loss: 0.765920  [ 2080/ 3200]\n",
      "loss: 1.022877  [ 2096/ 3200]\n",
      "loss: 1.009202  [ 2112/ 3200]\n",
      "loss: 0.601951  [ 2128/ 3200]\n",
      "loss: 0.758223  [ 2144/ 3200]\n",
      "loss: 0.779032  [ 2160/ 3200]\n",
      "loss: 0.951002  [ 2176/ 3200]\n",
      "loss: 0.631610  [ 2192/ 3200]\n",
      "loss: 0.732037  [ 2208/ 3200]\n",
      "loss: 0.761270  [ 2224/ 3200]\n",
      "loss: 0.712803  [ 2240/ 3200]\n",
      "loss: 0.942086  [ 2256/ 3200]\n",
      "loss: 0.698548  [ 2272/ 3200]\n",
      "loss: 0.529290  [ 2288/ 3200]\n",
      "loss: 1.114357  [ 2304/ 3200]\n",
      "loss: 0.606146  [ 2320/ 3200]\n",
      "loss: 0.772380  [ 2336/ 3200]\n",
      "loss: 0.742986  [ 2352/ 3200]\n",
      "loss: 0.742510  [ 2368/ 3200]\n",
      "loss: 0.516135  [ 2384/ 3200]\n",
      "loss: 0.793392  [ 2400/ 3200]\n",
      "loss: 0.540517  [ 2416/ 3200]\n",
      "loss: 0.613473  [ 2432/ 3200]\n",
      "loss: 0.684036  [ 2448/ 3200]\n",
      "loss: 0.982803  [ 2464/ 3200]\n",
      "loss: 0.639614  [ 2480/ 3200]\n",
      "loss: 0.874318  [ 2496/ 3200]\n",
      "loss: 0.567911  [ 2512/ 3200]\n",
      "loss: 0.818470  [ 2528/ 3200]\n",
      "loss: 0.821618  [ 2544/ 3200]\n",
      "loss: 0.858784  [ 2560/ 3200]\n",
      "loss: 0.585964  [ 2576/ 3200]\n",
      "loss: 0.947458  [ 2592/ 3200]\n",
      "loss: 0.590721  [ 2608/ 3200]\n",
      "loss: 0.959368  [ 2624/ 3200]\n",
      "loss: 0.668502  [ 2640/ 3200]\n",
      "loss: 0.736368  [ 2656/ 3200]\n",
      "loss: 0.815177  [ 2672/ 3200]\n",
      "loss: 0.865789  [ 2688/ 3200]\n",
      "loss: 0.512259  [ 2704/ 3200]\n",
      "loss: 0.441224  [ 2720/ 3200]\n",
      "loss: 0.456041  [ 2736/ 3200]\n",
      "loss: 0.750623  [ 2752/ 3200]\n",
      "loss: 0.701204  [ 2768/ 3200]\n",
      "loss: 0.684294  [ 2784/ 3200]\n",
      "loss: 0.854002  [ 2800/ 3200]\n",
      "loss: 0.401320  [ 2816/ 3200]\n",
      "loss: 0.626408  [ 2832/ 3200]\n",
      "loss: 0.693459  [ 2848/ 3200]\n",
      "loss: 1.121005  [ 2864/ 3200]\n",
      "loss: 1.452902  [ 2880/ 3200]\n",
      "loss: 0.690400  [ 2896/ 3200]\n",
      "loss: 1.028595  [ 2912/ 3200]\n",
      "loss: 0.515473  [ 2928/ 3200]\n",
      "loss: 0.554523  [ 2944/ 3200]\n",
      "loss: 0.517949  [ 2960/ 3200]\n",
      "loss: 0.666735  [ 2976/ 3200]\n",
      "loss: 0.853635  [ 2992/ 3200]\n",
      "loss: 1.028295  [ 3008/ 3200]\n",
      "loss: 0.644879  [ 3024/ 3200]\n",
      "loss: 0.684155  [ 3040/ 3200]\n",
      "loss: 0.770721  [ 3056/ 3200]\n",
      "loss: 0.710827  [ 3072/ 3200]\n",
      "loss: 0.660367  [ 3088/ 3200]\n",
      "loss: 0.576260  [ 3104/ 3200]\n",
      "loss: 0.718735  [ 3120/ 3200]\n",
      "loss: 0.768027  [ 3136/ 3200]\n",
      "loss: 0.337559  [ 3152/ 3200]\n",
      "loss: 0.716380  [ 3168/ 3200]\n",
      "loss: 0.750453  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.054035\n",
      "f1 macro averaged score: 0.637032\n",
      "Accuracy               : 64.1%\n",
      "Confusion matrix       :\n",
      "tensor([[152,  34,   5,   9],\n",
      "        [ 26,  78,  42,  54],\n",
      "        [  0,  16, 142,  42],\n",
      "        [  2,  23,  34, 141]], device='cuda:0')\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 0.330632  [    0/ 3200]\n",
      "loss: 0.594917  [   16/ 3200]\n",
      "loss: 0.590470  [   32/ 3200]\n",
      "loss: 0.686733  [   48/ 3200]\n",
      "loss: 0.678455  [   64/ 3200]\n",
      "loss: 0.520320  [   80/ 3200]\n",
      "loss: 0.917429  [   96/ 3200]\n",
      "loss: 0.918302  [  112/ 3200]\n",
      "loss: 0.787668  [  128/ 3200]\n",
      "loss: 0.652238  [  144/ 3200]\n",
      "loss: 0.809744  [  160/ 3200]\n",
      "loss: 0.578425  [  176/ 3200]\n",
      "loss: 0.588006  [  192/ 3200]\n",
      "loss: 0.519841  [  208/ 3200]\n",
      "loss: 0.415181  [  224/ 3200]\n",
      "loss: 0.710048  [  240/ 3200]\n",
      "loss: 0.954420  [  256/ 3200]\n",
      "loss: 0.805805  [  272/ 3200]\n",
      "loss: 0.463206  [  288/ 3200]\n",
      "loss: 0.565582  [  304/ 3200]\n",
      "loss: 0.546390  [  320/ 3200]\n",
      "loss: 0.515489  [  336/ 3200]\n",
      "loss: 1.131611  [  352/ 3200]\n",
      "loss: 1.024228  [  368/ 3200]\n",
      "loss: 0.687765  [  384/ 3200]\n",
      "loss: 0.582679  [  400/ 3200]\n",
      "loss: 0.491325  [  416/ 3200]\n",
      "loss: 0.558292  [  432/ 3200]\n",
      "loss: 0.820009  [  448/ 3200]\n",
      "loss: 0.862142  [  464/ 3200]\n",
      "loss: 0.681723  [  480/ 3200]\n",
      "loss: 0.667615  [  496/ 3200]\n",
      "loss: 0.823391  [  512/ 3200]\n",
      "loss: 0.552784  [  528/ 3200]\n",
      "loss: 0.607925  [  544/ 3200]\n",
      "loss: 0.959644  [  560/ 3200]\n",
      "loss: 0.966763  [  576/ 3200]\n",
      "loss: 1.088833  [  592/ 3200]\n",
      "loss: 0.571422  [  608/ 3200]\n",
      "loss: 0.711498  [  624/ 3200]\n",
      "loss: 0.777533  [  640/ 3200]\n",
      "loss: 0.487762  [  656/ 3200]\n",
      "loss: 1.085055  [  672/ 3200]\n",
      "loss: 0.851385  [  688/ 3200]\n",
      "loss: 0.690284  [  704/ 3200]\n",
      "loss: 0.799285  [  720/ 3200]\n",
      "loss: 0.586436  [  736/ 3200]\n",
      "loss: 0.633986  [  752/ 3200]\n",
      "loss: 0.577121  [  768/ 3200]\n",
      "loss: 0.577544  [  784/ 3200]\n",
      "loss: 0.560354  [  800/ 3200]\n",
      "loss: 0.872031  [  816/ 3200]\n",
      "loss: 0.596063  [  832/ 3200]\n",
      "loss: 0.762593  [  848/ 3200]\n",
      "loss: 1.036213  [  864/ 3200]\n",
      "loss: 0.439564  [  880/ 3200]\n",
      "loss: 0.718439  [  896/ 3200]\n",
      "loss: 0.652572  [  912/ 3200]\n",
      "loss: 0.800789  [  928/ 3200]\n",
      "loss: 0.885007  [  944/ 3200]\n",
      "loss: 0.873390  [  960/ 3200]\n",
      "loss: 0.824557  [  976/ 3200]\n",
      "loss: 0.431111  [  992/ 3200]\n",
      "loss: 0.693027  [ 1008/ 3200]\n",
      "loss: 0.733539  [ 1024/ 3200]\n",
      "loss: 0.661768  [ 1040/ 3200]\n",
      "loss: 0.695030  [ 1056/ 3200]\n",
      "loss: 0.771961  [ 1072/ 3200]\n",
      "loss: 0.678173  [ 1088/ 3200]\n",
      "loss: 0.797786  [ 1104/ 3200]\n",
      "loss: 0.715539  [ 1120/ 3200]\n",
      "loss: 0.510118  [ 1136/ 3200]\n",
      "loss: 0.883155  [ 1152/ 3200]\n",
      "loss: 0.522132  [ 1168/ 3200]\n",
      "loss: 0.560489  [ 1184/ 3200]\n",
      "loss: 1.074497  [ 1200/ 3200]\n",
      "loss: 0.885630  [ 1216/ 3200]\n",
      "loss: 0.572984  [ 1232/ 3200]\n",
      "loss: 0.618771  [ 1248/ 3200]\n",
      "loss: 0.854329  [ 1264/ 3200]\n",
      "loss: 1.128303  [ 1280/ 3200]\n",
      "loss: 0.559001  [ 1296/ 3200]\n",
      "loss: 0.719763  [ 1312/ 3200]\n",
      "loss: 1.072129  [ 1328/ 3200]\n",
      "loss: 0.807846  [ 1344/ 3200]\n",
      "loss: 0.758365  [ 1360/ 3200]\n",
      "loss: 0.820017  [ 1376/ 3200]\n",
      "loss: 0.828157  [ 1392/ 3200]\n",
      "loss: 0.463845  [ 1408/ 3200]\n",
      "loss: 0.685289  [ 1424/ 3200]\n",
      "loss: 1.003098  [ 1440/ 3200]\n",
      "loss: 0.626660  [ 1456/ 3200]\n",
      "loss: 0.560691  [ 1472/ 3200]\n",
      "loss: 0.843350  [ 1488/ 3200]\n",
      "loss: 0.718299  [ 1504/ 3200]\n",
      "loss: 0.738612  [ 1520/ 3200]\n",
      "loss: 1.084100  [ 1536/ 3200]\n",
      "loss: 0.895537  [ 1552/ 3200]\n",
      "loss: 0.873456  [ 1568/ 3200]\n",
      "loss: 0.674893  [ 1584/ 3200]\n",
      "loss: 0.538714  [ 1600/ 3200]\n",
      "loss: 0.487224  [ 1616/ 3200]\n",
      "loss: 0.658541  [ 1632/ 3200]\n",
      "loss: 1.303914  [ 1648/ 3200]\n",
      "loss: 0.961219  [ 1664/ 3200]\n",
      "loss: 0.941438  [ 1680/ 3200]\n",
      "loss: 0.393293  [ 1696/ 3200]\n",
      "loss: 0.963030  [ 1712/ 3200]\n",
      "loss: 0.720711  [ 1728/ 3200]\n",
      "loss: 0.856562  [ 1744/ 3200]\n",
      "loss: 0.863564  [ 1760/ 3200]\n",
      "loss: 0.734884  [ 1776/ 3200]\n",
      "loss: 0.460310  [ 1792/ 3200]\n",
      "loss: 0.568286  [ 1808/ 3200]\n",
      "loss: 0.697794  [ 1824/ 3200]\n",
      "loss: 0.861732  [ 1840/ 3200]\n",
      "loss: 0.401546  [ 1856/ 3200]\n",
      "loss: 0.705571  [ 1872/ 3200]\n",
      "loss: 0.905953  [ 1888/ 3200]\n",
      "loss: 0.638102  [ 1904/ 3200]\n",
      "loss: 0.500230  [ 1920/ 3200]\n",
      "loss: 0.513671  [ 1936/ 3200]\n",
      "loss: 0.672424  [ 1952/ 3200]\n",
      "loss: 0.542908  [ 1968/ 3200]\n",
      "loss: 0.539253  [ 1984/ 3200]\n",
      "loss: 0.797002  [ 2000/ 3200]\n",
      "loss: 0.929515  [ 2016/ 3200]\n",
      "loss: 0.888389  [ 2032/ 3200]\n",
      "loss: 0.473336  [ 2048/ 3200]\n",
      "loss: 0.502836  [ 2064/ 3200]\n",
      "loss: 0.906361  [ 2080/ 3200]\n",
      "loss: 0.595799  [ 2096/ 3200]\n",
      "loss: 1.108141  [ 2112/ 3200]\n",
      "loss: 0.584429  [ 2128/ 3200]\n",
      "loss: 0.657233  [ 2144/ 3200]\n",
      "loss: 0.477742  [ 2160/ 3200]\n",
      "loss: 0.881832  [ 2176/ 3200]\n",
      "loss: 0.576507  [ 2192/ 3200]\n",
      "loss: 0.631849  [ 2208/ 3200]\n",
      "loss: 0.775039  [ 2224/ 3200]\n",
      "loss: 0.444520  [ 2240/ 3200]\n",
      "loss: 1.043196  [ 2256/ 3200]\n",
      "loss: 0.766180  [ 2272/ 3200]\n",
      "loss: 0.681982  [ 2288/ 3200]\n",
      "loss: 0.576775  [ 2304/ 3200]\n",
      "loss: 0.600668  [ 2320/ 3200]\n",
      "loss: 0.838093  [ 2336/ 3200]\n",
      "loss: 0.760984  [ 2352/ 3200]\n",
      "loss: 0.773660  [ 2368/ 3200]\n",
      "loss: 0.656253  [ 2384/ 3200]\n",
      "loss: 0.858514  [ 2400/ 3200]\n",
      "loss: 0.717465  [ 2416/ 3200]\n",
      "loss: 0.524345  [ 2432/ 3200]\n",
      "loss: 0.824275  [ 2448/ 3200]\n",
      "loss: 0.858132  [ 2464/ 3200]\n",
      "loss: 0.690408  [ 2480/ 3200]\n",
      "loss: 0.591272  [ 2496/ 3200]\n",
      "loss: 0.763618  [ 2512/ 3200]\n",
      "loss: 0.608235  [ 2528/ 3200]\n",
      "loss: 0.329829  [ 2544/ 3200]\n",
      "loss: 0.754255  [ 2560/ 3200]\n",
      "loss: 1.106554  [ 2576/ 3200]\n",
      "loss: 0.543394  [ 2592/ 3200]\n",
      "loss: 0.753470  [ 2608/ 3200]\n",
      "loss: 0.830116  [ 2624/ 3200]\n",
      "loss: 0.580278  [ 2640/ 3200]\n",
      "loss: 0.505885  [ 2656/ 3200]\n",
      "loss: 1.177946  [ 2672/ 3200]\n",
      "loss: 0.460075  [ 2688/ 3200]\n",
      "loss: 0.755764  [ 2704/ 3200]\n",
      "loss: 0.548650  [ 2720/ 3200]\n",
      "loss: 0.382777  [ 2736/ 3200]\n",
      "loss: 1.015006  [ 2752/ 3200]\n",
      "loss: 0.697609  [ 2768/ 3200]\n",
      "loss: 0.451382  [ 2784/ 3200]\n",
      "loss: 1.204816  [ 2800/ 3200]\n",
      "loss: 1.041814  [ 2816/ 3200]\n",
      "loss: 0.954822  [ 2832/ 3200]\n",
      "loss: 0.857257  [ 2848/ 3200]\n",
      "loss: 0.647379  [ 2864/ 3200]\n",
      "loss: 0.679031  [ 2880/ 3200]\n",
      "loss: 0.596769  [ 2896/ 3200]\n",
      "loss: 0.647852  [ 2912/ 3200]\n",
      "loss: 0.869473  [ 2928/ 3200]\n",
      "loss: 0.755512  [ 2944/ 3200]\n",
      "loss: 0.590453  [ 2960/ 3200]\n",
      "loss: 0.858217  [ 2976/ 3200]\n",
      "loss: 0.659778  [ 2992/ 3200]\n",
      "loss: 0.687144  [ 3008/ 3200]\n",
      "loss: 0.766735  [ 3024/ 3200]\n",
      "loss: 0.655059  [ 3040/ 3200]\n",
      "loss: 0.613018  [ 3056/ 3200]\n",
      "loss: 1.118743  [ 3072/ 3200]\n",
      "loss: 1.396622  [ 3088/ 3200]\n",
      "loss: 0.831678  [ 3104/ 3200]\n",
      "loss: 0.950084  [ 3120/ 3200]\n",
      "loss: 0.901896  [ 3136/ 3200]\n",
      "loss: 0.983279  [ 3152/ 3200]\n",
      "loss: 0.604376  [ 3168/ 3200]\n",
      "loss: 0.743655  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.052588\n",
      "f1 macro averaged score: 0.653174\n",
      "Accuracy               : 65.2%\n",
      "Confusion matrix       :\n",
      "tensor([[169,  26,   3,   2],\n",
      "        [ 34, 112,  25,  29],\n",
      "        [  0,  36, 125,  39],\n",
      "        [  5,  51,  28, 116]], device='cuda:0')\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.698502  [    0/ 3200]\n",
      "loss: 0.805826  [   16/ 3200]\n",
      "loss: 0.631340  [   32/ 3200]\n",
      "loss: 0.686597  [   48/ 3200]\n",
      "loss: 0.673429  [   64/ 3200]\n",
      "loss: 1.241001  [   80/ 3200]\n",
      "loss: 0.296176  [   96/ 3200]\n",
      "loss: 0.491246  [  112/ 3200]\n",
      "loss: 0.600222  [  128/ 3200]\n",
      "loss: 0.957185  [  144/ 3200]\n",
      "loss: 0.638466  [  160/ 3200]\n",
      "loss: 0.739383  [  176/ 3200]\n",
      "loss: 0.396599  [  192/ 3200]\n",
      "loss: 0.522546  [  208/ 3200]\n",
      "loss: 0.559402  [  224/ 3200]\n",
      "loss: 0.562043  [  240/ 3200]\n",
      "loss: 1.180357  [  256/ 3200]\n",
      "loss: 0.599858  [  272/ 3200]\n",
      "loss: 0.503895  [  288/ 3200]\n",
      "loss: 0.794193  [  304/ 3200]\n",
      "loss: 0.757372  [  320/ 3200]\n",
      "loss: 0.699899  [  336/ 3200]\n",
      "loss: 0.637142  [  352/ 3200]\n",
      "loss: 0.535678  [  368/ 3200]\n",
      "loss: 0.559126  [  384/ 3200]\n",
      "loss: 0.953003  [  400/ 3200]\n",
      "loss: 0.599837  [  416/ 3200]\n",
      "loss: 0.945303  [  432/ 3200]\n",
      "loss: 0.694843  [  448/ 3200]\n",
      "loss: 0.950834  [  464/ 3200]\n",
      "loss: 0.617045  [  480/ 3200]\n",
      "loss: 0.663038  [  496/ 3200]\n",
      "loss: 0.569892  [  512/ 3200]\n",
      "loss: 0.553169  [  528/ 3200]\n",
      "loss: 0.650894  [  544/ 3200]\n",
      "loss: 0.698866  [  560/ 3200]\n",
      "loss: 0.681753  [  576/ 3200]\n",
      "loss: 0.831601  [  592/ 3200]\n",
      "loss: 0.320150  [  608/ 3200]\n",
      "loss: 0.696691  [  624/ 3200]\n",
      "loss: 0.699204  [  640/ 3200]\n",
      "loss: 0.516838  [  656/ 3200]\n",
      "loss: 0.481918  [  672/ 3200]\n",
      "loss: 0.747169  [  688/ 3200]\n",
      "loss: 0.446975  [  704/ 3200]\n",
      "loss: 1.073432  [  720/ 3200]\n",
      "loss: 0.815855  [  736/ 3200]\n",
      "loss: 0.759090  [  752/ 3200]\n",
      "loss: 0.442977  [  768/ 3200]\n",
      "loss: 0.741737  [  784/ 3200]\n",
      "loss: 1.154264  [  800/ 3200]\n",
      "loss: 0.703613  [  816/ 3200]\n",
      "loss: 0.782972  [  832/ 3200]\n",
      "loss: 0.997953  [  848/ 3200]\n",
      "loss: 0.767555  [  864/ 3200]\n",
      "loss: 0.697792  [  880/ 3200]\n",
      "loss: 0.668841  [  896/ 3200]\n",
      "loss: 0.292029  [  912/ 3200]\n",
      "loss: 0.703917  [  928/ 3200]\n",
      "loss: 0.777590  [  944/ 3200]\n",
      "loss: 0.929351  [  960/ 3200]\n",
      "loss: 0.927765  [  976/ 3200]\n",
      "loss: 0.566932  [  992/ 3200]\n",
      "loss: 0.656356  [ 1008/ 3200]\n",
      "loss: 0.303920  [ 1024/ 3200]\n",
      "loss: 0.558184  [ 1040/ 3200]\n",
      "loss: 0.974688  [ 1056/ 3200]\n",
      "loss: 1.003568  [ 1072/ 3200]\n",
      "loss: 0.598722  [ 1088/ 3200]\n",
      "loss: 0.548713  [ 1104/ 3200]\n",
      "loss: 1.127820  [ 1120/ 3200]\n",
      "loss: 0.768220  [ 1136/ 3200]\n",
      "loss: 0.805470  [ 1152/ 3200]\n",
      "loss: 0.704051  [ 1168/ 3200]\n",
      "loss: 0.833436  [ 1184/ 3200]\n",
      "loss: 0.646603  [ 1200/ 3200]\n",
      "loss: 0.872466  [ 1216/ 3200]\n",
      "loss: 0.514253  [ 1232/ 3200]\n",
      "loss: 0.721294  [ 1248/ 3200]\n",
      "loss: 0.621960  [ 1264/ 3200]\n",
      "loss: 0.753013  [ 1280/ 3200]\n",
      "loss: 1.184865  [ 1296/ 3200]\n",
      "loss: 0.959645  [ 1312/ 3200]\n",
      "loss: 0.564058  [ 1328/ 3200]\n",
      "loss: 0.723341  [ 1344/ 3200]\n",
      "loss: 0.375447  [ 1360/ 3200]\n",
      "loss: 0.615017  [ 1376/ 3200]\n",
      "loss: 0.638103  [ 1392/ 3200]\n",
      "loss: 1.272513  [ 1408/ 3200]\n",
      "loss: 0.392297  [ 1424/ 3200]\n",
      "loss: 0.483284  [ 1440/ 3200]\n",
      "loss: 0.686842  [ 1456/ 3200]\n",
      "loss: 0.715400  [ 1472/ 3200]\n",
      "loss: 0.386342  [ 1488/ 3200]\n",
      "loss: 0.661979  [ 1504/ 3200]\n",
      "loss: 0.647637  [ 1520/ 3200]\n",
      "loss: 0.440136  [ 1536/ 3200]\n",
      "loss: 0.612711  [ 1552/ 3200]\n",
      "loss: 0.715771  [ 1568/ 3200]\n",
      "loss: 0.693905  [ 1584/ 3200]\n",
      "loss: 0.829619  [ 1600/ 3200]\n",
      "loss: 0.575621  [ 1616/ 3200]\n",
      "loss: 1.170790  [ 1632/ 3200]\n",
      "loss: 0.690606  [ 1648/ 3200]\n",
      "loss: 0.507271  [ 1664/ 3200]\n",
      "loss: 0.633310  [ 1680/ 3200]\n",
      "loss: 0.894039  [ 1696/ 3200]\n",
      "loss: 0.696158  [ 1712/ 3200]\n",
      "loss: 0.870955  [ 1728/ 3200]\n",
      "loss: 0.974587  [ 1744/ 3200]\n",
      "loss: 0.884597  [ 1760/ 3200]\n",
      "loss: 0.852903  [ 1776/ 3200]\n",
      "loss: 0.593507  [ 1792/ 3200]\n",
      "loss: 0.539633  [ 1808/ 3200]\n",
      "loss: 0.638971  [ 1824/ 3200]\n",
      "loss: 0.707733  [ 1840/ 3200]\n",
      "loss: 0.698266  [ 1856/ 3200]\n",
      "loss: 0.720698  [ 1872/ 3200]\n",
      "loss: 1.350439  [ 1888/ 3200]\n",
      "loss: 1.007537  [ 1904/ 3200]\n",
      "loss: 0.975843  [ 1920/ 3200]\n",
      "loss: 0.755631  [ 1936/ 3200]\n",
      "loss: 0.469207  [ 1952/ 3200]\n",
      "loss: 0.721992  [ 1968/ 3200]\n",
      "loss: 0.844886  [ 1984/ 3200]\n",
      "loss: 0.668847  [ 2000/ 3200]\n",
      "loss: 0.575186  [ 2016/ 3200]\n",
      "loss: 0.981830  [ 2032/ 3200]\n",
      "loss: 0.723872  [ 2048/ 3200]\n",
      "loss: 0.916911  [ 2064/ 3200]\n",
      "loss: 0.464886  [ 2080/ 3200]\n",
      "loss: 0.885135  [ 2096/ 3200]\n",
      "loss: 0.755282  [ 2112/ 3200]\n",
      "loss: 0.764343  [ 2128/ 3200]\n",
      "loss: 0.561373  [ 2144/ 3200]\n",
      "loss: 0.702926  [ 2160/ 3200]\n",
      "loss: 0.592255  [ 2176/ 3200]\n",
      "loss: 0.639870  [ 2192/ 3200]\n",
      "loss: 0.690784  [ 2208/ 3200]\n",
      "loss: 0.485716  [ 2224/ 3200]\n",
      "loss: 0.828398  [ 2240/ 3200]\n",
      "loss: 0.503582  [ 2256/ 3200]\n",
      "loss: 0.503289  [ 2272/ 3200]\n",
      "loss: 0.680347  [ 2288/ 3200]\n",
      "loss: 0.756730  [ 2304/ 3200]\n",
      "loss: 0.744198  [ 2320/ 3200]\n",
      "loss: 0.772943  [ 2336/ 3200]\n",
      "loss: 0.891489  [ 2352/ 3200]\n",
      "loss: 0.516197  [ 2368/ 3200]\n",
      "loss: 0.427329  [ 2384/ 3200]\n",
      "loss: 0.606364  [ 2400/ 3200]\n",
      "loss: 0.516760  [ 2416/ 3200]\n",
      "loss: 0.838041  [ 2432/ 3200]\n",
      "loss: 0.761066  [ 2448/ 3200]\n",
      "loss: 0.839326  [ 2464/ 3200]\n",
      "loss: 0.847545  [ 2480/ 3200]\n",
      "loss: 0.736309  [ 2496/ 3200]\n",
      "loss: 0.627886  [ 2512/ 3200]\n",
      "loss: 0.737010  [ 2528/ 3200]\n",
      "loss: 0.510926  [ 2544/ 3200]\n",
      "loss: 0.547547  [ 2560/ 3200]\n",
      "loss: 0.792366  [ 2576/ 3200]\n",
      "loss: 0.790520  [ 2592/ 3200]\n",
      "loss: 0.727130  [ 2608/ 3200]\n",
      "loss: 1.161101  [ 2624/ 3200]\n",
      "loss: 0.493322  [ 2640/ 3200]\n",
      "loss: 0.502066  [ 2656/ 3200]\n",
      "loss: 0.549638  [ 2672/ 3200]\n",
      "loss: 0.735395  [ 2688/ 3200]\n",
      "loss: 0.718697  [ 2704/ 3200]\n",
      "loss: 0.719928  [ 2720/ 3200]\n",
      "loss: 0.945984  [ 2736/ 3200]\n",
      "loss: 0.866766  [ 2752/ 3200]\n",
      "loss: 0.702383  [ 2768/ 3200]\n",
      "loss: 0.743912  [ 2784/ 3200]\n",
      "loss: 0.775710  [ 2800/ 3200]\n",
      "loss: 0.793627  [ 2816/ 3200]\n",
      "loss: 0.407789  [ 2832/ 3200]\n",
      "loss: 0.772854  [ 2848/ 3200]\n",
      "loss: 0.735538  [ 2864/ 3200]\n",
      "loss: 0.405208  [ 2880/ 3200]\n",
      "loss: 0.774291  [ 2896/ 3200]\n",
      "loss: 0.780268  [ 2912/ 3200]\n",
      "loss: 0.743631  [ 2928/ 3200]\n",
      "loss: 1.436917  [ 2944/ 3200]\n",
      "loss: 0.866227  [ 2960/ 3200]\n",
      "loss: 0.821349  [ 2976/ 3200]\n",
      "loss: 0.610532  [ 2992/ 3200]\n",
      "loss: 0.817935  [ 3008/ 3200]\n",
      "loss: 0.723603  [ 3024/ 3200]\n",
      "loss: 0.636543  [ 3040/ 3200]\n",
      "loss: 0.806494  [ 3056/ 3200]\n",
      "loss: 0.872599  [ 3072/ 3200]\n",
      "loss: 0.774175  [ 3088/ 3200]\n",
      "loss: 0.744756  [ 3104/ 3200]\n",
      "loss: 0.724927  [ 3120/ 3200]\n",
      "loss: 0.661439  [ 3136/ 3200]\n",
      "loss: 0.805448  [ 3152/ 3200]\n",
      "loss: 0.830286  [ 3168/ 3200]\n",
      "loss: 0.622370  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.053076\n",
      "f1 macro averaged score: 0.657385\n",
      "Accuracy               : 65.2%\n",
      "Confusion matrix       :\n",
      "tensor([[158,  38,   3,   1],\n",
      "        [ 23, 131,  22,  24],\n",
      "        [  0,  41, 122,  37],\n",
      "        [  2,  59,  28, 111]], device='cuda:0')\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 0.548865  [    0/ 3200]\n",
      "loss: 0.769041  [   16/ 3200]\n",
      "loss: 0.871760  [   32/ 3200]\n",
      "loss: 0.823914  [   48/ 3200]\n",
      "loss: 0.763680  [   64/ 3200]\n",
      "loss: 0.518803  [   80/ 3200]\n",
      "loss: 0.525966  [   96/ 3200]\n",
      "loss: 0.936256  [  112/ 3200]\n",
      "loss: 0.807368  [  128/ 3200]\n",
      "loss: 0.339304  [  144/ 3200]\n",
      "loss: 0.423130  [  160/ 3200]\n",
      "loss: 0.879038  [  176/ 3200]\n",
      "loss: 0.483442  [  192/ 3200]\n",
      "loss: 0.601037  [  208/ 3200]\n",
      "loss: 0.589225  [  224/ 3200]\n",
      "loss: 0.526041  [  240/ 3200]\n",
      "loss: 0.610796  [  256/ 3200]\n",
      "loss: 0.640510  [  272/ 3200]\n",
      "loss: 0.699557  [  288/ 3200]\n",
      "loss: 0.673036  [  304/ 3200]\n",
      "loss: 0.661895  [  320/ 3200]\n",
      "loss: 0.796615  [  336/ 3200]\n",
      "loss: 0.448845  [  352/ 3200]\n",
      "loss: 0.738191  [  368/ 3200]\n",
      "loss: 0.615679  [  384/ 3200]\n",
      "loss: 0.464920  [  400/ 3200]\n",
      "loss: 1.095545  [  416/ 3200]\n",
      "loss: 0.803747  [  432/ 3200]\n",
      "loss: 0.571608  [  448/ 3200]\n",
      "loss: 0.618318  [  464/ 3200]\n",
      "loss: 0.860740  [  480/ 3200]\n",
      "loss: 0.579616  [  496/ 3200]\n",
      "loss: 1.135755  [  512/ 3200]\n",
      "loss: 0.885588  [  528/ 3200]\n",
      "loss: 0.848866  [  544/ 3200]\n",
      "loss: 0.818045  [  560/ 3200]\n",
      "loss: 0.763643  [  576/ 3200]\n",
      "loss: 0.593220  [  592/ 3200]\n",
      "loss: 0.530410  [  608/ 3200]\n",
      "loss: 0.598128  [  624/ 3200]\n",
      "loss: 0.784630  [  640/ 3200]\n",
      "loss: 0.660312  [  656/ 3200]\n",
      "loss: 0.658666  [  672/ 3200]\n",
      "loss: 0.691371  [  688/ 3200]\n",
      "loss: 0.792687  [  704/ 3200]\n",
      "loss: 0.590761  [  720/ 3200]\n",
      "loss: 0.555433  [  736/ 3200]\n",
      "loss: 0.755981  [  752/ 3200]\n",
      "loss: 0.633938  [  768/ 3200]\n",
      "loss: 0.752460  [  784/ 3200]\n",
      "loss: 0.364819  [  800/ 3200]\n",
      "loss: 0.692896  [  816/ 3200]\n",
      "loss: 1.049672  [  832/ 3200]\n",
      "loss: 0.997606  [  848/ 3200]\n",
      "loss: 1.314902  [  864/ 3200]\n",
      "loss: 0.797657  [  880/ 3200]\n",
      "loss: 1.072196  [  896/ 3200]\n",
      "loss: 0.700667  [  912/ 3200]\n",
      "loss: 0.579664  [  928/ 3200]\n",
      "loss: 0.704364  [  944/ 3200]\n",
      "loss: 0.438172  [  960/ 3200]\n",
      "loss: 0.634203  [  976/ 3200]\n",
      "loss: 0.663921  [  992/ 3200]\n",
      "loss: 0.673621  [ 1008/ 3200]\n",
      "loss: 0.803044  [ 1024/ 3200]\n",
      "loss: 0.740850  [ 1040/ 3200]\n",
      "loss: 0.697662  [ 1056/ 3200]\n",
      "loss: 0.815433  [ 1072/ 3200]\n",
      "loss: 0.706665  [ 1088/ 3200]\n",
      "loss: 0.535998  [ 1104/ 3200]\n",
      "loss: 1.083993  [ 1120/ 3200]\n",
      "loss: 0.674908  [ 1136/ 3200]\n",
      "loss: 0.726946  [ 1152/ 3200]\n",
      "loss: 0.304584  [ 1168/ 3200]\n",
      "loss: 0.624399  [ 1184/ 3200]\n",
      "loss: 0.873831  [ 1200/ 3200]\n",
      "loss: 0.580525  [ 1216/ 3200]\n",
      "loss: 0.472387  [ 1232/ 3200]\n",
      "loss: 0.869824  [ 1248/ 3200]\n",
      "loss: 0.504398  [ 1264/ 3200]\n",
      "loss: 0.798829  [ 1280/ 3200]\n",
      "loss: 0.755560  [ 1296/ 3200]\n",
      "loss: 0.804441  [ 1312/ 3200]\n",
      "loss: 1.270413  [ 1328/ 3200]\n",
      "loss: 0.837706  [ 1344/ 3200]\n",
      "loss: 0.518312  [ 1360/ 3200]\n",
      "loss: 0.420678  [ 1376/ 3200]\n",
      "loss: 0.837527  [ 1392/ 3200]\n",
      "loss: 0.911656  [ 1408/ 3200]\n",
      "loss: 0.692192  [ 1424/ 3200]\n",
      "loss: 0.460610  [ 1440/ 3200]\n",
      "loss: 0.582699  [ 1456/ 3200]\n",
      "loss: 0.660115  [ 1472/ 3200]\n",
      "loss: 0.974970  [ 1488/ 3200]\n",
      "loss: 0.524738  [ 1504/ 3200]\n",
      "loss: 0.873754  [ 1520/ 3200]\n",
      "loss: 0.573313  [ 1536/ 3200]\n",
      "loss: 0.305646  [ 1552/ 3200]\n",
      "loss: 0.990437  [ 1568/ 3200]\n",
      "loss: 0.795582  [ 1584/ 3200]\n",
      "loss: 0.770822  [ 1600/ 3200]\n",
      "loss: 0.686005  [ 1616/ 3200]\n",
      "loss: 0.641430  [ 1632/ 3200]\n",
      "loss: 0.884992  [ 1648/ 3200]\n",
      "loss: 0.583562  [ 1664/ 3200]\n",
      "loss: 0.626206  [ 1680/ 3200]\n",
      "loss: 0.725099  [ 1696/ 3200]\n",
      "loss: 0.888119  [ 1712/ 3200]\n",
      "loss: 0.791397  [ 1728/ 3200]\n",
      "loss: 0.740261  [ 1744/ 3200]\n",
      "loss: 0.625836  [ 1760/ 3200]\n",
      "loss: 0.728158  [ 1776/ 3200]\n",
      "loss: 0.714399  [ 1792/ 3200]\n",
      "loss: 0.550389  [ 1808/ 3200]\n",
      "loss: 0.865910  [ 1824/ 3200]\n",
      "loss: 0.314976  [ 1840/ 3200]\n",
      "loss: 0.532147  [ 1856/ 3200]\n",
      "loss: 0.623417  [ 1872/ 3200]\n",
      "loss: 0.483863  [ 1888/ 3200]\n",
      "loss: 0.379189  [ 1904/ 3200]\n",
      "loss: 0.637075  [ 1920/ 3200]\n",
      "loss: 0.542454  [ 1936/ 3200]\n",
      "loss: 0.526123  [ 1952/ 3200]\n",
      "loss: 0.746530  [ 1968/ 3200]\n",
      "loss: 0.698969  [ 1984/ 3200]\n",
      "loss: 0.550952  [ 2000/ 3200]\n",
      "loss: 0.507102  [ 2016/ 3200]\n",
      "loss: 0.495521  [ 2032/ 3200]\n",
      "loss: 0.684669  [ 2048/ 3200]\n",
      "loss: 0.949473  [ 2064/ 3200]\n",
      "loss: 0.581201  [ 2080/ 3200]\n",
      "loss: 0.655542  [ 2096/ 3200]\n",
      "loss: 0.906907  [ 2112/ 3200]\n",
      "loss: 0.509243  [ 2128/ 3200]\n",
      "loss: 0.779502  [ 2144/ 3200]\n",
      "loss: 0.700875  [ 2160/ 3200]\n",
      "loss: 0.868730  [ 2176/ 3200]\n",
      "loss: 1.327162  [ 2192/ 3200]\n",
      "loss: 0.531558  [ 2208/ 3200]\n",
      "loss: 0.716198  [ 2224/ 3200]\n",
      "loss: 1.097019  [ 2240/ 3200]\n",
      "loss: 0.411418  [ 2256/ 3200]\n",
      "loss: 0.640234  [ 2272/ 3200]\n",
      "loss: 0.633220  [ 2288/ 3200]\n",
      "loss: 0.808901  [ 2304/ 3200]\n",
      "loss: 0.580330  [ 2320/ 3200]\n",
      "loss: 0.759831  [ 2336/ 3200]\n",
      "loss: 0.606030  [ 2352/ 3200]\n",
      "loss: 0.822160  [ 2368/ 3200]\n",
      "loss: 0.802831  [ 2384/ 3200]\n",
      "loss: 0.805178  [ 2400/ 3200]\n",
      "loss: 0.524299  [ 2416/ 3200]\n",
      "loss: 0.578856  [ 2432/ 3200]\n",
      "loss: 0.817920  [ 2448/ 3200]\n",
      "loss: 0.773264  [ 2464/ 3200]\n",
      "loss: 1.400348  [ 2480/ 3200]\n",
      "loss: 0.774775  [ 2496/ 3200]\n",
      "loss: 0.652244  [ 2512/ 3200]\n",
      "loss: 0.933457  [ 2528/ 3200]\n",
      "loss: 0.548803  [ 2544/ 3200]\n",
      "loss: 0.948277  [ 2560/ 3200]\n",
      "loss: 0.526137  [ 2576/ 3200]\n",
      "loss: 0.680110  [ 2592/ 3200]\n",
      "loss: 0.768071  [ 2608/ 3200]\n",
      "loss: 0.659051  [ 2624/ 3200]\n",
      "loss: 0.710383  [ 2640/ 3200]\n",
      "loss: 0.826215  [ 2656/ 3200]\n",
      "loss: 0.529224  [ 2672/ 3200]\n",
      "loss: 0.595011  [ 2688/ 3200]\n",
      "loss: 0.520379  [ 2704/ 3200]\n",
      "loss: 1.006528  [ 2720/ 3200]\n",
      "loss: 0.534545  [ 2736/ 3200]\n",
      "loss: 0.606347  [ 2752/ 3200]\n",
      "loss: 0.496990  [ 2768/ 3200]\n",
      "loss: 0.552515  [ 2784/ 3200]\n",
      "loss: 0.866660  [ 2800/ 3200]\n",
      "loss: 0.849699  [ 2816/ 3200]\n",
      "loss: 0.701055  [ 2832/ 3200]\n",
      "loss: 0.615615  [ 2848/ 3200]\n",
      "loss: 0.773331  [ 2864/ 3200]\n",
      "loss: 0.633487  [ 2880/ 3200]\n",
      "loss: 0.697809  [ 2896/ 3200]\n",
      "loss: 0.490922  [ 2912/ 3200]\n",
      "loss: 0.522641  [ 2928/ 3200]\n",
      "loss: 0.445424  [ 2944/ 3200]\n",
      "loss: 0.530390  [ 2960/ 3200]\n",
      "loss: 0.756431  [ 2976/ 3200]\n",
      "loss: 0.803924  [ 2992/ 3200]\n",
      "loss: 0.779441  [ 3008/ 3200]\n",
      "loss: 0.713516  [ 3024/ 3200]\n",
      "loss: 0.813382  [ 3040/ 3200]\n",
      "loss: 1.166895  [ 3056/ 3200]\n",
      "loss: 0.952974  [ 3072/ 3200]\n",
      "loss: 0.853045  [ 3088/ 3200]\n",
      "loss: 0.866281  [ 3104/ 3200]\n",
      "loss: 0.351129  [ 3120/ 3200]\n",
      "loss: 0.881283  [ 3136/ 3200]\n",
      "loss: 0.459898  [ 3152/ 3200]\n",
      "loss: 0.901803  [ 3168/ 3200]\n",
      "loss: 0.428855  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.053504\n",
      "f1 macro averaged score: 0.618237\n",
      "Accuracy               : 64.0%\n",
      "Confusion matrix       :\n",
      "tensor([[178,   7,   5,  10],\n",
      "        [ 53,  55,  34,  58],\n",
      "        [  5,  11, 134,  50],\n",
      "        [ 10,  19,  26, 145]], device='cuda:0')\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.500583  [    0/ 3200]\n",
      "loss: 0.771961  [   16/ 3200]\n",
      "loss: 0.664749  [   32/ 3200]\n",
      "loss: 1.096290  [   48/ 3200]\n",
      "loss: 0.732159  [   64/ 3200]\n",
      "loss: 0.369766  [   80/ 3200]\n",
      "loss: 0.500177  [   96/ 3200]\n",
      "loss: 0.703901  [  112/ 3200]\n",
      "loss: 0.736255  [  128/ 3200]\n",
      "loss: 0.392150  [  144/ 3200]\n",
      "loss: 0.752519  [  160/ 3200]\n",
      "loss: 0.772749  [  176/ 3200]\n",
      "loss: 0.887791  [  192/ 3200]\n",
      "loss: 0.823664  [  208/ 3200]\n",
      "loss: 0.849212  [  224/ 3200]\n",
      "loss: 1.186499  [  240/ 3200]\n",
      "loss: 0.893489  [  256/ 3200]\n",
      "loss: 0.718927  [  272/ 3200]\n",
      "loss: 0.749484  [  288/ 3200]\n",
      "loss: 0.432678  [  304/ 3200]\n",
      "loss: 1.173511  [  320/ 3200]\n",
      "loss: 0.966052  [  336/ 3200]\n",
      "loss: 0.519941  [  352/ 3200]\n",
      "loss: 0.898318  [  368/ 3200]\n",
      "loss: 0.527427  [  384/ 3200]\n",
      "loss: 0.927970  [  400/ 3200]\n",
      "loss: 0.712659  [  416/ 3200]\n",
      "loss: 0.612983  [  432/ 3200]\n",
      "loss: 0.870396  [  448/ 3200]\n",
      "loss: 0.443716  [  464/ 3200]\n",
      "loss: 0.931170  [  480/ 3200]\n",
      "loss: 0.735839  [  496/ 3200]\n",
      "loss: 0.665884  [  512/ 3200]\n",
      "loss: 0.708545  [  528/ 3200]\n",
      "loss: 0.843833  [  544/ 3200]\n",
      "loss: 0.720642  [  560/ 3200]\n",
      "loss: 0.907023  [  576/ 3200]\n",
      "loss: 0.615229  [  592/ 3200]\n",
      "loss: 0.524323  [  608/ 3200]\n",
      "loss: 1.061230  [  624/ 3200]\n",
      "loss: 0.707160  [  640/ 3200]\n",
      "loss: 0.745133  [  656/ 3200]\n",
      "loss: 0.553352  [  672/ 3200]\n",
      "loss: 0.499412  [  688/ 3200]\n",
      "loss: 0.941105  [  704/ 3200]\n",
      "loss: 0.682817  [  720/ 3200]\n",
      "loss: 0.556075  [  736/ 3200]\n",
      "loss: 0.610152  [  752/ 3200]\n",
      "loss: 0.863172  [  768/ 3200]\n",
      "loss: 0.489850  [  784/ 3200]\n",
      "loss: 0.811948  [  800/ 3200]\n",
      "loss: 0.786275  [  816/ 3200]\n",
      "loss: 0.534419  [  832/ 3200]\n",
      "loss: 0.464729  [  848/ 3200]\n",
      "loss: 0.656995  [  864/ 3200]\n",
      "loss: 0.602973  [  880/ 3200]\n",
      "loss: 0.925619  [  896/ 3200]\n",
      "loss: 0.957279  [  912/ 3200]\n",
      "loss: 0.368621  [  928/ 3200]\n",
      "loss: 0.739199  [  944/ 3200]\n",
      "loss: 0.632899  [  960/ 3200]\n",
      "loss: 0.310061  [  976/ 3200]\n",
      "loss: 0.900490  [  992/ 3200]\n",
      "loss: 0.571983  [ 1008/ 3200]\n",
      "loss: 0.681981  [ 1024/ 3200]\n",
      "loss: 0.444992  [ 1040/ 3200]\n",
      "loss: 0.664642  [ 1056/ 3200]\n",
      "loss: 0.757253  [ 1072/ 3200]\n",
      "loss: 0.615930  [ 1088/ 3200]\n",
      "loss: 0.552586  [ 1104/ 3200]\n",
      "loss: 1.127576  [ 1120/ 3200]\n",
      "loss: 0.728385  [ 1136/ 3200]\n",
      "loss: 0.442949  [ 1152/ 3200]\n",
      "loss: 0.724959  [ 1168/ 3200]\n",
      "loss: 0.762412  [ 1184/ 3200]\n",
      "loss: 0.530985  [ 1200/ 3200]\n",
      "loss: 0.619795  [ 1216/ 3200]\n",
      "loss: 0.934989  [ 1232/ 3200]\n",
      "loss: 0.634598  [ 1248/ 3200]\n",
      "loss: 0.552509  [ 1264/ 3200]\n",
      "loss: 0.746133  [ 1280/ 3200]\n",
      "loss: 0.241117  [ 1296/ 3200]\n",
      "loss: 0.540872  [ 1312/ 3200]\n",
      "loss: 0.820842  [ 1328/ 3200]\n",
      "loss: 0.591296  [ 1344/ 3200]\n",
      "loss: 0.492029  [ 1360/ 3200]\n",
      "loss: 0.685576  [ 1376/ 3200]\n",
      "loss: 0.866185  [ 1392/ 3200]\n",
      "loss: 0.650927  [ 1408/ 3200]\n",
      "loss: 0.907409  [ 1424/ 3200]\n",
      "loss: 0.683457  [ 1440/ 3200]\n",
      "loss: 0.610349  [ 1456/ 3200]\n",
      "loss: 0.475447  [ 1472/ 3200]\n",
      "loss: 0.622087  [ 1488/ 3200]\n",
      "loss: 0.693774  [ 1504/ 3200]\n",
      "loss: 0.579066  [ 1520/ 3200]\n",
      "loss: 0.732848  [ 1536/ 3200]\n",
      "loss: 0.696916  [ 1552/ 3200]\n",
      "loss: 0.926379  [ 1568/ 3200]\n",
      "loss: 1.016261  [ 1584/ 3200]\n",
      "loss: 0.860169  [ 1600/ 3200]\n",
      "loss: 0.499039  [ 1616/ 3200]\n",
      "loss: 0.490089  [ 1632/ 3200]\n",
      "loss: 0.741636  [ 1648/ 3200]\n",
      "loss: 0.952052  [ 1664/ 3200]\n",
      "loss: 0.826238  [ 1680/ 3200]\n",
      "loss: 0.720895  [ 1696/ 3200]\n",
      "loss: 0.642486  [ 1712/ 3200]\n",
      "loss: 0.851992  [ 1728/ 3200]\n",
      "loss: 0.933462  [ 1744/ 3200]\n",
      "loss: 0.431139  [ 1760/ 3200]\n",
      "loss: 0.718518  [ 1776/ 3200]\n",
      "loss: 0.500349  [ 1792/ 3200]\n",
      "loss: 0.778452  [ 1808/ 3200]\n",
      "loss: 0.696886  [ 1824/ 3200]\n",
      "loss: 0.591039  [ 1840/ 3200]\n",
      "loss: 0.426077  [ 1856/ 3200]\n",
      "loss: 0.675376  [ 1872/ 3200]\n",
      "loss: 0.930550  [ 1888/ 3200]\n",
      "loss: 0.719396  [ 1904/ 3200]\n",
      "loss: 0.587830  [ 1920/ 3200]\n",
      "loss: 0.509920  [ 1936/ 3200]\n",
      "loss: 0.388732  [ 1952/ 3200]\n",
      "loss: 1.046865  [ 1968/ 3200]\n",
      "loss: 0.651049  [ 1984/ 3200]\n",
      "loss: 0.541015  [ 2000/ 3200]\n",
      "loss: 0.620051  [ 2016/ 3200]\n",
      "loss: 0.683397  [ 2032/ 3200]\n",
      "loss: 0.589027  [ 2048/ 3200]\n",
      "loss: 0.606616  [ 2064/ 3200]\n",
      "loss: 0.897927  [ 2080/ 3200]\n",
      "loss: 0.532819  [ 2096/ 3200]\n",
      "loss: 0.569103  [ 2112/ 3200]\n",
      "loss: 0.676062  [ 2128/ 3200]\n",
      "loss: 0.856332  [ 2144/ 3200]\n",
      "loss: 0.515337  [ 2160/ 3200]\n",
      "loss: 0.386185  [ 2176/ 3200]\n",
      "loss: 0.528707  [ 2192/ 3200]\n",
      "loss: 0.881854  [ 2208/ 3200]\n",
      "loss: 0.643987  [ 2224/ 3200]\n",
      "loss: 0.826783  [ 2240/ 3200]\n",
      "loss: 0.832539  [ 2256/ 3200]\n",
      "loss: 0.490019  [ 2272/ 3200]\n",
      "loss: 0.536758  [ 2288/ 3200]\n",
      "loss: 0.358207  [ 2304/ 3200]\n",
      "loss: 0.366890  [ 2320/ 3200]\n",
      "loss: 1.045385  [ 2336/ 3200]\n",
      "loss: 0.410745  [ 2352/ 3200]\n",
      "loss: 0.665052  [ 2368/ 3200]\n",
      "loss: 0.540304  [ 2384/ 3200]\n",
      "loss: 0.746047  [ 2400/ 3200]\n",
      "loss: 0.446400  [ 2416/ 3200]\n",
      "loss: 0.723809  [ 2432/ 3200]\n",
      "loss: 1.023637  [ 2448/ 3200]\n",
      "loss: 0.880746  [ 2464/ 3200]\n",
      "loss: 0.744273  [ 2480/ 3200]\n",
      "loss: 0.896518  [ 2496/ 3200]\n",
      "loss: 0.966228  [ 2512/ 3200]\n",
      "loss: 0.539403  [ 2528/ 3200]\n",
      "loss: 0.842799  [ 2544/ 3200]\n",
      "loss: 0.597651  [ 2560/ 3200]\n",
      "loss: 0.833457  [ 2576/ 3200]\n",
      "loss: 0.816561  [ 2592/ 3200]\n",
      "loss: 0.603254  [ 2608/ 3200]\n",
      "loss: 0.749782  [ 2624/ 3200]\n",
      "loss: 0.719595  [ 2640/ 3200]\n",
      "loss: 0.853433  [ 2656/ 3200]\n",
      "loss: 0.425701  [ 2672/ 3200]\n",
      "loss: 0.831062  [ 2688/ 3200]\n",
      "loss: 1.055076  [ 2704/ 3200]\n",
      "loss: 0.543802  [ 2720/ 3200]\n",
      "loss: 0.525641  [ 2736/ 3200]\n",
      "loss: 0.576157  [ 2752/ 3200]\n",
      "loss: 0.664921  [ 2768/ 3200]\n",
      "loss: 0.590908  [ 2784/ 3200]\n",
      "loss: 0.541348  [ 2800/ 3200]\n",
      "loss: 0.861794  [ 2816/ 3200]\n",
      "loss: 0.655644  [ 2832/ 3200]\n",
      "loss: 0.676590  [ 2848/ 3200]\n",
      "loss: 0.870267  [ 2864/ 3200]\n",
      "loss: 0.705124  [ 2880/ 3200]\n",
      "loss: 0.548726  [ 2896/ 3200]\n",
      "loss: 0.709308  [ 2912/ 3200]\n",
      "loss: 0.545271  [ 2928/ 3200]\n",
      "loss: 0.706436  [ 2944/ 3200]\n",
      "loss: 0.849041  [ 2960/ 3200]\n",
      "loss: 0.218357  [ 2976/ 3200]\n",
      "loss: 0.475645  [ 2992/ 3200]\n",
      "loss: 0.409754  [ 3008/ 3200]\n",
      "loss: 0.763053  [ 3024/ 3200]\n",
      "loss: 0.708921  [ 3040/ 3200]\n",
      "loss: 0.715390  [ 3056/ 3200]\n",
      "loss: 0.681102  [ 3072/ 3200]\n",
      "loss: 0.826269  [ 3088/ 3200]\n",
      "loss: 0.617904  [ 3104/ 3200]\n",
      "loss: 0.701425  [ 3120/ 3200]\n",
      "loss: 0.570500  [ 3136/ 3200]\n",
      "loss: 0.458326  [ 3152/ 3200]\n",
      "loss: 0.551126  [ 3168/ 3200]\n",
      "loss: 0.917389  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.053206\n",
      "f1 macro averaged score: 0.641439\n",
      "Accuracy               : 64.5%\n",
      "Confusion matrix       :\n",
      "tensor([[169,  19,   4,   8],\n",
      "        [ 36,  91,  22,  51],\n",
      "        [  2,  28, 116,  54],\n",
      "        [  6,  32,  22, 140]], device='cuda:0')\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.518414  [    0/ 3200]\n",
      "loss: 0.751214  [   16/ 3200]\n",
      "loss: 0.740699  [   32/ 3200]\n",
      "loss: 0.804986  [   48/ 3200]\n",
      "loss: 0.945715  [   64/ 3200]\n",
      "loss: 0.608383  [   80/ 3200]\n",
      "loss: 0.663207  [   96/ 3200]\n",
      "loss: 0.357764  [  112/ 3200]\n",
      "loss: 0.428801  [  128/ 3200]\n",
      "loss: 0.679106  [  144/ 3200]\n",
      "loss: 0.540188  [  160/ 3200]\n",
      "loss: 0.751533  [  176/ 3200]\n",
      "loss: 0.920019  [  192/ 3200]\n",
      "loss: 0.804294  [  208/ 3200]\n",
      "loss: 0.728029  [  224/ 3200]\n",
      "loss: 0.595448  [  240/ 3200]\n",
      "loss: 0.516642  [  256/ 3200]\n",
      "loss: 0.524039  [  272/ 3200]\n",
      "loss: 0.760570  [  288/ 3200]\n",
      "loss: 0.787265  [  304/ 3200]\n",
      "loss: 0.646549  [  320/ 3200]\n",
      "loss: 0.667776  [  336/ 3200]\n",
      "loss: 0.534205  [  352/ 3200]\n",
      "loss: 0.298352  [  368/ 3200]\n",
      "loss: 0.770650  [  384/ 3200]\n",
      "loss: 0.817200  [  400/ 3200]\n",
      "loss: 0.511517  [  416/ 3200]\n",
      "loss: 0.645514  [  432/ 3200]\n",
      "loss: 0.636081  [  448/ 3200]\n",
      "loss: 0.819089  [  464/ 3200]\n",
      "loss: 0.626325  [  480/ 3200]\n",
      "loss: 0.950937  [  496/ 3200]\n",
      "loss: 0.593959  [  512/ 3200]\n",
      "loss: 0.553408  [  528/ 3200]\n",
      "loss: 0.729747  [  544/ 3200]\n",
      "loss: 0.520904  [  560/ 3200]\n",
      "loss: 1.102189  [  576/ 3200]\n",
      "loss: 0.504666  [  592/ 3200]\n",
      "loss: 0.570028  [  608/ 3200]\n",
      "loss: 1.072102  [  624/ 3200]\n",
      "loss: 0.484920  [  640/ 3200]\n",
      "loss: 0.630925  [  656/ 3200]\n",
      "loss: 0.639199  [  672/ 3200]\n",
      "loss: 0.444408  [  688/ 3200]\n",
      "loss: 0.549206  [  704/ 3200]\n",
      "loss: 0.870667  [  720/ 3200]\n",
      "loss: 0.580561  [  736/ 3200]\n",
      "loss: 0.478282  [  752/ 3200]\n",
      "loss: 0.926625  [  768/ 3200]\n",
      "loss: 0.550791  [  784/ 3200]\n",
      "loss: 0.611866  [  800/ 3200]\n",
      "loss: 0.334245  [  816/ 3200]\n",
      "loss: 0.827631  [  832/ 3200]\n",
      "loss: 0.631803  [  848/ 3200]\n",
      "loss: 1.019874  [  864/ 3200]\n",
      "loss: 0.914918  [  880/ 3200]\n",
      "loss: 0.660014  [  896/ 3200]\n",
      "loss: 0.551588  [  912/ 3200]\n",
      "loss: 0.483911  [  928/ 3200]\n",
      "loss: 1.190318  [  944/ 3200]\n",
      "loss: 1.041927  [  960/ 3200]\n",
      "loss: 0.717000  [  976/ 3200]\n",
      "loss: 0.571426  [  992/ 3200]\n",
      "loss: 0.608000  [ 1008/ 3200]\n",
      "loss: 0.733967  [ 1024/ 3200]\n",
      "loss: 0.926895  [ 1040/ 3200]\n",
      "loss: 1.029719  [ 1056/ 3200]\n",
      "loss: 0.412859  [ 1072/ 3200]\n",
      "loss: 0.601817  [ 1088/ 3200]\n",
      "loss: 0.544933  [ 1104/ 3200]\n",
      "loss: 0.566397  [ 1120/ 3200]\n",
      "loss: 0.386713  [ 1136/ 3200]\n",
      "loss: 0.570413  [ 1152/ 3200]\n",
      "loss: 0.496366  [ 1168/ 3200]\n",
      "loss: 0.935087  [ 1184/ 3200]\n",
      "loss: 0.731540  [ 1200/ 3200]\n",
      "loss: 0.724973  [ 1216/ 3200]\n",
      "loss: 0.928129  [ 1232/ 3200]\n",
      "loss: 0.595269  [ 1248/ 3200]\n",
      "loss: 0.730570  [ 1264/ 3200]\n",
      "loss: 0.828525  [ 1280/ 3200]\n",
      "loss: 0.483022  [ 1296/ 3200]\n",
      "loss: 0.685183  [ 1312/ 3200]\n",
      "loss: 0.521395  [ 1328/ 3200]\n",
      "loss: 0.535555  [ 1344/ 3200]\n",
      "loss: 1.205729  [ 1360/ 3200]\n",
      "loss: 0.485908  [ 1376/ 3200]\n",
      "loss: 0.611133  [ 1392/ 3200]\n",
      "loss: 0.550374  [ 1408/ 3200]\n",
      "loss: 0.625759  [ 1424/ 3200]\n",
      "loss: 0.475529  [ 1440/ 3200]\n",
      "loss: 0.835801  [ 1456/ 3200]\n",
      "loss: 0.640833  [ 1472/ 3200]\n",
      "loss: 0.520195  [ 1488/ 3200]\n",
      "loss: 0.775560  [ 1504/ 3200]\n",
      "loss: 0.655598  [ 1520/ 3200]\n",
      "loss: 0.694554  [ 1536/ 3200]\n",
      "loss: 0.705048  [ 1552/ 3200]\n",
      "loss: 0.547925  [ 1568/ 3200]\n",
      "loss: 0.652402  [ 1584/ 3200]\n",
      "loss: 0.515626  [ 1600/ 3200]\n",
      "loss: 0.521028  [ 1616/ 3200]\n",
      "loss: 0.491769  [ 1632/ 3200]\n",
      "loss: 0.760063  [ 1648/ 3200]\n",
      "loss: 1.195526  [ 1664/ 3200]\n",
      "loss: 0.756940  [ 1680/ 3200]\n",
      "loss: 0.827572  [ 1696/ 3200]\n",
      "loss: 0.469679  [ 1712/ 3200]\n",
      "loss: 0.704415  [ 1728/ 3200]\n",
      "loss: 0.706820  [ 1744/ 3200]\n",
      "loss: 0.600452  [ 1760/ 3200]\n",
      "loss: 0.535388  [ 1776/ 3200]\n",
      "loss: 0.993652  [ 1792/ 3200]\n",
      "loss: 0.523408  [ 1808/ 3200]\n",
      "loss: 0.669955  [ 1824/ 3200]\n",
      "loss: 0.709420  [ 1840/ 3200]\n",
      "loss: 0.579702  [ 1856/ 3200]\n",
      "loss: 0.894491  [ 1872/ 3200]\n",
      "loss: 0.980749  [ 1888/ 3200]\n",
      "loss: 0.776166  [ 1904/ 3200]\n",
      "loss: 0.911510  [ 1920/ 3200]\n",
      "loss: 1.131548  [ 1936/ 3200]\n",
      "loss: 0.609789  [ 1952/ 3200]\n",
      "loss: 0.653834  [ 1968/ 3200]\n",
      "loss: 0.606385  [ 1984/ 3200]\n",
      "loss: 0.481569  [ 2000/ 3200]\n",
      "loss: 0.493588  [ 2016/ 3200]\n",
      "loss: 0.643656  [ 2032/ 3200]\n",
      "loss: 0.322130  [ 2048/ 3200]\n",
      "loss: 0.813851  [ 2064/ 3200]\n",
      "loss: 0.603759  [ 2080/ 3200]\n",
      "loss: 0.827055  [ 2096/ 3200]\n",
      "loss: 0.777141  [ 2112/ 3200]\n",
      "loss: 0.464024  [ 2128/ 3200]\n",
      "loss: 0.856039  [ 2144/ 3200]\n",
      "loss: 0.898093  [ 2160/ 3200]\n",
      "loss: 0.890250  [ 2176/ 3200]\n",
      "loss: 0.743238  [ 2192/ 3200]\n",
      "loss: 0.608663  [ 2208/ 3200]\n",
      "loss: 0.893314  [ 2224/ 3200]\n",
      "loss: 0.551351  [ 2240/ 3200]\n",
      "loss: 0.618196  [ 2256/ 3200]\n",
      "loss: 0.693408  [ 2272/ 3200]\n",
      "loss: 0.754282  [ 2288/ 3200]\n",
      "loss: 0.910763  [ 2304/ 3200]\n",
      "loss: 0.692146  [ 2320/ 3200]\n",
      "loss: 1.022183  [ 2336/ 3200]\n",
      "loss: 0.585093  [ 2352/ 3200]\n",
      "loss: 0.708002  [ 2368/ 3200]\n",
      "loss: 0.832659  [ 2384/ 3200]\n",
      "loss: 1.153302  [ 2400/ 3200]\n",
      "loss: 0.818901  [ 2416/ 3200]\n",
      "loss: 0.555870  [ 2432/ 3200]\n",
      "loss: 0.912276  [ 2448/ 3200]\n",
      "loss: 0.820986  [ 2464/ 3200]\n",
      "loss: 0.596995  [ 2480/ 3200]\n",
      "loss: 0.609443  [ 2496/ 3200]\n",
      "loss: 0.708541  [ 2512/ 3200]\n",
      "loss: 0.796482  [ 2528/ 3200]\n",
      "loss: 0.602782  [ 2544/ 3200]\n",
      "loss: 0.422336  [ 2560/ 3200]\n",
      "loss: 0.792085  [ 2576/ 3200]\n",
      "loss: 0.813238  [ 2592/ 3200]\n",
      "loss: 0.519524  [ 2608/ 3200]\n",
      "loss: 0.412584  [ 2624/ 3200]\n",
      "loss: 0.451574  [ 2640/ 3200]\n",
      "loss: 0.720828  [ 2656/ 3200]\n",
      "loss: 0.725832  [ 2672/ 3200]\n",
      "loss: 0.704673  [ 2688/ 3200]\n",
      "loss: 0.719842  [ 2704/ 3200]\n",
      "loss: 0.830847  [ 2720/ 3200]\n",
      "loss: 0.638799  [ 2736/ 3200]\n",
      "loss: 0.942187  [ 2752/ 3200]\n",
      "loss: 0.675210  [ 2768/ 3200]\n",
      "loss: 0.433475  [ 2784/ 3200]\n",
      "loss: 0.801895  [ 2800/ 3200]\n",
      "loss: 0.606960  [ 2816/ 3200]\n",
      "loss: 0.843110  [ 2832/ 3200]\n",
      "loss: 0.801098  [ 2848/ 3200]\n",
      "loss: 0.312390  [ 2864/ 3200]\n",
      "loss: 0.625728  [ 2880/ 3200]\n",
      "loss: 0.532870  [ 2896/ 3200]\n",
      "loss: 0.670165  [ 2912/ 3200]\n",
      "loss: 0.415703  [ 2928/ 3200]\n",
      "loss: 0.703427  [ 2944/ 3200]\n",
      "loss: 0.829343  [ 2960/ 3200]\n",
      "loss: 0.634825  [ 2976/ 3200]\n",
      "loss: 0.580812  [ 2992/ 3200]\n",
      "loss: 0.731653  [ 3008/ 3200]\n",
      "loss: 1.028464  [ 3024/ 3200]\n",
      "loss: 0.522052  [ 3040/ 3200]\n",
      "loss: 0.420999  [ 3056/ 3200]\n",
      "loss: 0.548685  [ 3072/ 3200]\n",
      "loss: 0.367331  [ 3088/ 3200]\n",
      "loss: 0.518210  [ 3104/ 3200]\n",
      "loss: 0.664237  [ 3120/ 3200]\n",
      "loss: 0.623826  [ 3136/ 3200]\n",
      "loss: 0.326707  [ 3152/ 3200]\n",
      "loss: 0.538509  [ 3168/ 3200]\n",
      "loss: 0.702658  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.054335\n",
      "f1 macro averaged score: 0.641199\n",
      "Accuracy               : 64.5%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  17,   2,   7],\n",
      "        [ 41,  98,  12,  49],\n",
      "        [  2,  37, 107,  54],\n",
      "        [  8,  33,  22, 137]], device='cuda:0')\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.374970  [    0/ 3200]\n",
      "loss: 0.711229  [   16/ 3200]\n",
      "loss: 1.094361  [   32/ 3200]\n",
      "loss: 0.500009  [   48/ 3200]\n",
      "loss: 0.797935  [   64/ 3200]\n",
      "loss: 0.583651  [   80/ 3200]\n",
      "loss: 0.692932  [   96/ 3200]\n",
      "loss: 0.528873  [  112/ 3200]\n",
      "loss: 0.643961  [  128/ 3200]\n",
      "loss: 0.595720  [  144/ 3200]\n",
      "loss: 0.956243  [  160/ 3200]\n",
      "loss: 0.953729  [  176/ 3200]\n",
      "loss: 0.756670  [  192/ 3200]\n",
      "loss: 0.591670  [  208/ 3200]\n",
      "loss: 0.676419  [  224/ 3200]\n",
      "loss: 0.642956  [  240/ 3200]\n",
      "loss: 0.532352  [  256/ 3200]\n",
      "loss: 1.094232  [  272/ 3200]\n",
      "loss: 0.376498  [  288/ 3200]\n",
      "loss: 0.455469  [  304/ 3200]\n",
      "loss: 0.416214  [  320/ 3200]\n",
      "loss: 0.869559  [  336/ 3200]\n",
      "loss: 0.691794  [  352/ 3200]\n",
      "loss: 0.578819  [  368/ 3200]\n",
      "loss: 0.549360  [  384/ 3200]\n",
      "loss: 0.741342  [  400/ 3200]\n",
      "loss: 0.626596  [  416/ 3200]\n",
      "loss: 0.558276  [  432/ 3200]\n",
      "loss: 0.541473  [  448/ 3200]\n",
      "loss: 0.537660  [  464/ 3200]\n",
      "loss: 0.521668  [  480/ 3200]\n",
      "loss: 0.509237  [  496/ 3200]\n",
      "loss: 0.265201  [  512/ 3200]\n",
      "loss: 0.742194  [  528/ 3200]\n",
      "loss: 0.479248  [  544/ 3200]\n",
      "loss: 0.678002  [  560/ 3200]\n",
      "loss: 0.798261  [  576/ 3200]\n",
      "loss: 0.610874  [  592/ 3200]\n",
      "loss: 0.774749  [  608/ 3200]\n",
      "loss: 0.629725  [  624/ 3200]\n",
      "loss: 0.695342  [  640/ 3200]\n",
      "loss: 0.870034  [  656/ 3200]\n",
      "loss: 0.827265  [  672/ 3200]\n",
      "loss: 0.698138  [  688/ 3200]\n",
      "loss: 0.642641  [  704/ 3200]\n",
      "loss: 0.504642  [  720/ 3200]\n",
      "loss: 0.673164  [  736/ 3200]\n",
      "loss: 0.705795  [  752/ 3200]\n",
      "loss: 0.968031  [  768/ 3200]\n",
      "loss: 0.421090  [  784/ 3200]\n",
      "loss: 0.627923  [  800/ 3200]\n",
      "loss: 0.621982  [  816/ 3200]\n",
      "loss: 0.580806  [  832/ 3200]\n",
      "loss: 0.560919  [  848/ 3200]\n",
      "loss: 0.390994  [  864/ 3200]\n",
      "loss: 0.613291  [  880/ 3200]\n",
      "loss: 0.548259  [  896/ 3200]\n",
      "loss: 0.466188  [  912/ 3200]\n",
      "loss: 0.564072  [  928/ 3200]\n",
      "loss: 0.449892  [  944/ 3200]\n",
      "loss: 0.568653  [  960/ 3200]\n",
      "loss: 0.859300  [  976/ 3200]\n",
      "loss: 0.496899  [  992/ 3200]\n",
      "loss: 0.712423  [ 1008/ 3200]\n",
      "loss: 0.609138  [ 1024/ 3200]\n",
      "loss: 0.962256  [ 1040/ 3200]\n",
      "loss: 0.962118  [ 1056/ 3200]\n",
      "loss: 0.784820  [ 1072/ 3200]\n",
      "loss: 0.781119  [ 1088/ 3200]\n",
      "loss: 0.624489  [ 1104/ 3200]\n",
      "loss: 0.490226  [ 1120/ 3200]\n",
      "loss: 1.003677  [ 1136/ 3200]\n",
      "loss: 0.665991  [ 1152/ 3200]\n",
      "loss: 0.604769  [ 1168/ 3200]\n",
      "loss: 0.258163  [ 1184/ 3200]\n",
      "loss: 0.665049  [ 1200/ 3200]\n",
      "loss: 0.517058  [ 1216/ 3200]\n",
      "loss: 0.616319  [ 1232/ 3200]\n",
      "loss: 0.724753  [ 1248/ 3200]\n",
      "loss: 0.691293  [ 1264/ 3200]\n",
      "loss: 0.936674  [ 1280/ 3200]\n",
      "loss: 0.502234  [ 1296/ 3200]\n",
      "loss: 0.482896  [ 1312/ 3200]\n",
      "loss: 0.778276  [ 1328/ 3200]\n",
      "loss: 0.630263  [ 1344/ 3200]\n",
      "loss: 0.487286  [ 1360/ 3200]\n",
      "loss: 0.960540  [ 1376/ 3200]\n",
      "loss: 0.435030  [ 1392/ 3200]\n",
      "loss: 0.741969  [ 1408/ 3200]\n",
      "loss: 0.697973  [ 1424/ 3200]\n",
      "loss: 0.671461  [ 1440/ 3200]\n",
      "loss: 0.432941  [ 1456/ 3200]\n",
      "loss: 0.483883  [ 1472/ 3200]\n",
      "loss: 0.468779  [ 1488/ 3200]\n",
      "loss: 0.466154  [ 1504/ 3200]\n",
      "loss: 0.431612  [ 1520/ 3200]\n",
      "loss: 0.459959  [ 1536/ 3200]\n",
      "loss: 0.563232  [ 1552/ 3200]\n",
      "loss: 1.037955  [ 1568/ 3200]\n",
      "loss: 0.570709  [ 1584/ 3200]\n",
      "loss: 0.506868  [ 1600/ 3200]\n",
      "loss: 0.557033  [ 1616/ 3200]\n",
      "loss: 0.510044  [ 1632/ 3200]\n",
      "loss: 1.098857  [ 1648/ 3200]\n",
      "loss: 0.669553  [ 1664/ 3200]\n",
      "loss: 0.839480  [ 1680/ 3200]\n",
      "loss: 0.841780  [ 1696/ 3200]\n",
      "loss: 0.644129  [ 1712/ 3200]\n",
      "loss: 1.111078  [ 1728/ 3200]\n",
      "loss: 0.590176  [ 1744/ 3200]\n",
      "loss: 0.869982  [ 1760/ 3200]\n",
      "loss: 0.823209  [ 1776/ 3200]\n",
      "loss: 0.872741  [ 1792/ 3200]\n",
      "loss: 0.618610  [ 1808/ 3200]\n",
      "loss: 0.990097  [ 1824/ 3200]\n",
      "loss: 0.741595  [ 1840/ 3200]\n",
      "loss: 0.758520  [ 1856/ 3200]\n",
      "loss: 0.577359  [ 1872/ 3200]\n",
      "loss: 0.697844  [ 1888/ 3200]\n",
      "loss: 0.541565  [ 1904/ 3200]\n",
      "loss: 0.836695  [ 1920/ 3200]\n",
      "loss: 0.900432  [ 1936/ 3200]\n",
      "loss: 0.456522  [ 1952/ 3200]\n",
      "loss: 0.375681  [ 1968/ 3200]\n",
      "loss: 0.911025  [ 1984/ 3200]\n",
      "loss: 0.706325  [ 2000/ 3200]\n",
      "loss: 0.668074  [ 2016/ 3200]\n",
      "loss: 0.926921  [ 2032/ 3200]\n",
      "loss: 0.397359  [ 2048/ 3200]\n",
      "loss: 0.593512  [ 2064/ 3200]\n",
      "loss: 0.654774  [ 2080/ 3200]\n",
      "loss: 0.752133  [ 2096/ 3200]\n",
      "loss: 0.709140  [ 2112/ 3200]\n",
      "loss: 0.972184  [ 2128/ 3200]\n",
      "loss: 0.611460  [ 2144/ 3200]\n",
      "loss: 0.495411  [ 2160/ 3200]\n",
      "loss: 0.726297  [ 2176/ 3200]\n",
      "loss: 0.500045  [ 2192/ 3200]\n",
      "loss: 1.095934  [ 2208/ 3200]\n",
      "loss: 0.635236  [ 2224/ 3200]\n",
      "loss: 0.702257  [ 2240/ 3200]\n",
      "loss: 0.657815  [ 2256/ 3200]\n",
      "loss: 0.431044  [ 2272/ 3200]\n",
      "loss: 0.443601  [ 2288/ 3200]\n",
      "loss: 0.366865  [ 2304/ 3200]\n",
      "loss: 0.410688  [ 2320/ 3200]\n",
      "loss: 0.714808  [ 2336/ 3200]\n",
      "loss: 0.668838  [ 2352/ 3200]\n",
      "loss: 0.620117  [ 2368/ 3200]\n",
      "loss: 0.750576  [ 2384/ 3200]\n",
      "loss: 0.819214  [ 2400/ 3200]\n",
      "loss: 0.900634  [ 2416/ 3200]\n",
      "loss: 0.631521  [ 2432/ 3200]\n",
      "loss: 0.865758  [ 2448/ 3200]\n",
      "loss: 0.514400  [ 2464/ 3200]\n",
      "loss: 0.808905  [ 2480/ 3200]\n",
      "loss: 0.797132  [ 2496/ 3200]\n",
      "loss: 0.873331  [ 2512/ 3200]\n",
      "loss: 0.822601  [ 2528/ 3200]\n",
      "loss: 0.769873  [ 2544/ 3200]\n",
      "loss: 0.730564  [ 2560/ 3200]\n",
      "loss: 0.300780  [ 2576/ 3200]\n",
      "loss: 0.452577  [ 2592/ 3200]\n",
      "loss: 0.590328  [ 2608/ 3200]\n",
      "loss: 0.676089  [ 2624/ 3200]\n",
      "loss: 0.480583  [ 2640/ 3200]\n",
      "loss: 0.549048  [ 2656/ 3200]\n",
      "loss: 0.685958  [ 2672/ 3200]\n",
      "loss: 0.849646  [ 2688/ 3200]\n",
      "loss: 0.719904  [ 2704/ 3200]\n",
      "loss: 0.503950  [ 2720/ 3200]\n",
      "loss: 0.767397  [ 2736/ 3200]\n",
      "loss: 0.853281  [ 2752/ 3200]\n",
      "loss: 0.817079  [ 2768/ 3200]\n",
      "loss: 1.103532  [ 2784/ 3200]\n",
      "loss: 0.492455  [ 2800/ 3200]\n",
      "loss: 0.673973  [ 2816/ 3200]\n",
      "loss: 0.517443  [ 2832/ 3200]\n",
      "loss: 0.896348  [ 2848/ 3200]\n",
      "loss: 0.546865  [ 2864/ 3200]\n",
      "loss: 0.730450  [ 2880/ 3200]\n",
      "loss: 0.392092  [ 2896/ 3200]\n",
      "loss: 0.802512  [ 2912/ 3200]\n",
      "loss: 0.611042  [ 2928/ 3200]\n",
      "loss: 0.747983  [ 2944/ 3200]\n",
      "loss: 0.807053  [ 2960/ 3200]\n",
      "loss: 0.717474  [ 2976/ 3200]\n",
      "loss: 0.825880  [ 2992/ 3200]\n",
      "loss: 0.587840  [ 3008/ 3200]\n",
      "loss: 0.646144  [ 3024/ 3200]\n",
      "loss: 0.610496  [ 3040/ 3200]\n",
      "loss: 0.709693  [ 3056/ 3200]\n",
      "loss: 0.576575  [ 3072/ 3200]\n",
      "loss: 0.526327  [ 3088/ 3200]\n",
      "loss: 0.762708  [ 3104/ 3200]\n",
      "loss: 0.615115  [ 3120/ 3200]\n",
      "loss: 0.910162  [ 3136/ 3200]\n",
      "loss: 0.611028  [ 3152/ 3200]\n",
      "loss: 0.788526  [ 3168/ 3200]\n",
      "loss: 1.020259  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.052416\n",
      "f1 macro averaged score: 0.652442\n",
      "Accuracy               : 65.0%\n",
      "Confusion matrix       :\n",
      "tensor([[147,  45,   5,   3],\n",
      "        [ 22, 110,  47,  21],\n",
      "        [  0,  21, 157,  22],\n",
      "        [  1,  55,  38, 106]], device='cuda:0')\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.633367  [    0/ 3200]\n",
      "loss: 0.770712  [   16/ 3200]\n",
      "loss: 0.665705  [   32/ 3200]\n",
      "loss: 0.811267  [   48/ 3200]\n",
      "loss: 0.757860  [   64/ 3200]\n",
      "loss: 0.617290  [   80/ 3200]\n",
      "loss: 0.530722  [   96/ 3200]\n",
      "loss: 0.383644  [  112/ 3200]\n",
      "loss: 0.339049  [  128/ 3200]\n",
      "loss: 0.686654  [  144/ 3200]\n",
      "loss: 0.626141  [  160/ 3200]\n",
      "loss: 1.085609  [  176/ 3200]\n",
      "loss: 0.533190  [  192/ 3200]\n",
      "loss: 0.621925  [  208/ 3200]\n",
      "loss: 0.576908  [  224/ 3200]\n",
      "loss: 0.573205  [  240/ 3200]\n",
      "loss: 0.581201  [  256/ 3200]\n",
      "loss: 0.493790  [  272/ 3200]\n",
      "loss: 0.669886  [  288/ 3200]\n",
      "loss: 0.624124  [  304/ 3200]\n",
      "loss: 0.527359  [  320/ 3200]\n",
      "loss: 0.771845  [  336/ 3200]\n",
      "loss: 0.856321  [  352/ 3200]\n",
      "loss: 0.613126  [  368/ 3200]\n",
      "loss: 0.457449  [  384/ 3200]\n",
      "loss: 0.592576  [  400/ 3200]\n",
      "loss: 0.779338  [  416/ 3200]\n",
      "loss: 0.654732  [  432/ 3200]\n",
      "loss: 0.816849  [  448/ 3200]\n",
      "loss: 0.539681  [  464/ 3200]\n",
      "loss: 0.404348  [  480/ 3200]\n",
      "loss: 0.595866  [  496/ 3200]\n",
      "loss: 0.596677  [  512/ 3200]\n",
      "loss: 0.851709  [  528/ 3200]\n",
      "loss: 0.772477  [  544/ 3200]\n",
      "loss: 0.870743  [  560/ 3200]\n",
      "loss: 0.611036  [  576/ 3200]\n",
      "loss: 0.385040  [  592/ 3200]\n",
      "loss: 0.725533  [  608/ 3200]\n",
      "loss: 0.546452  [  624/ 3200]\n",
      "loss: 0.618772  [  640/ 3200]\n",
      "loss: 0.463049  [  656/ 3200]\n",
      "loss: 0.830150  [  672/ 3200]\n",
      "loss: 0.702270  [  688/ 3200]\n",
      "loss: 0.783150  [  704/ 3200]\n",
      "loss: 0.497593  [  720/ 3200]\n",
      "loss: 0.453963  [  736/ 3200]\n",
      "loss: 1.101577  [  752/ 3200]\n",
      "loss: 0.445389  [  768/ 3200]\n",
      "loss: 0.889134  [  784/ 3200]\n",
      "loss: 0.640764  [  800/ 3200]\n",
      "loss: 0.524491  [  816/ 3200]\n",
      "loss: 0.509803  [  832/ 3200]\n",
      "loss: 0.580571  [  848/ 3200]\n",
      "loss: 0.378749  [  864/ 3200]\n",
      "loss: 0.626547  [  880/ 3200]\n",
      "loss: 0.440330  [  896/ 3200]\n",
      "loss: 0.713997  [  912/ 3200]\n",
      "loss: 0.715752  [  928/ 3200]\n",
      "loss: 0.696587  [  944/ 3200]\n",
      "loss: 0.837823  [  960/ 3200]\n",
      "loss: 0.256468  [  976/ 3200]\n",
      "loss: 0.750957  [  992/ 3200]\n",
      "loss: 0.760516  [ 1008/ 3200]\n",
      "loss: 0.714101  [ 1024/ 3200]\n",
      "loss: 1.016775  [ 1040/ 3200]\n",
      "loss: 1.051269  [ 1056/ 3200]\n",
      "loss: 0.799699  [ 1072/ 3200]\n",
      "loss: 0.501865  [ 1088/ 3200]\n",
      "loss: 0.589934  [ 1104/ 3200]\n",
      "loss: 0.578034  [ 1120/ 3200]\n",
      "loss: 0.510008  [ 1136/ 3200]\n",
      "loss: 0.669240  [ 1152/ 3200]\n",
      "loss: 0.789800  [ 1168/ 3200]\n",
      "loss: 0.540684  [ 1184/ 3200]\n",
      "loss: 0.721808  [ 1200/ 3200]\n",
      "loss: 0.818774  [ 1216/ 3200]\n",
      "loss: 0.672488  [ 1232/ 3200]\n",
      "loss: 1.086483  [ 1248/ 3200]\n",
      "loss: 0.965494  [ 1264/ 3200]\n",
      "loss: 0.498277  [ 1280/ 3200]\n",
      "loss: 0.423294  [ 1296/ 3200]\n",
      "loss: 0.381410  [ 1312/ 3200]\n",
      "loss: 0.621597  [ 1328/ 3200]\n",
      "loss: 0.599151  [ 1344/ 3200]\n",
      "loss: 0.963933  [ 1360/ 3200]\n",
      "loss: 0.449189  [ 1376/ 3200]\n",
      "loss: 0.550498  [ 1392/ 3200]\n",
      "loss: 0.420859  [ 1408/ 3200]\n",
      "loss: 0.471690  [ 1424/ 3200]\n",
      "loss: 0.391692  [ 1440/ 3200]\n",
      "loss: 0.735957  [ 1456/ 3200]\n",
      "loss: 0.548974  [ 1472/ 3200]\n",
      "loss: 0.834987  [ 1488/ 3200]\n",
      "loss: 0.448958  [ 1504/ 3200]\n",
      "loss: 0.639263  [ 1520/ 3200]\n",
      "loss: 0.728847  [ 1536/ 3200]\n",
      "loss: 0.602575  [ 1552/ 3200]\n",
      "loss: 0.422976  [ 1568/ 3200]\n",
      "loss: 0.469295  [ 1584/ 3200]\n",
      "loss: 0.789547  [ 1600/ 3200]\n",
      "loss: 0.395200  [ 1616/ 3200]\n",
      "loss: 0.440365  [ 1632/ 3200]\n",
      "loss: 0.562398  [ 1648/ 3200]\n",
      "loss: 0.891254  [ 1664/ 3200]\n",
      "loss: 0.513803  [ 1680/ 3200]\n",
      "loss: 0.898803  [ 1696/ 3200]\n",
      "loss: 0.957905  [ 1712/ 3200]\n",
      "loss: 0.602823  [ 1728/ 3200]\n",
      "loss: 0.442164  [ 1744/ 3200]\n",
      "loss: 0.501838  [ 1760/ 3200]\n",
      "loss: 0.570338  [ 1776/ 3200]\n",
      "loss: 0.620635  [ 1792/ 3200]\n",
      "loss: 0.682674  [ 1808/ 3200]\n",
      "loss: 0.714422  [ 1824/ 3200]\n",
      "loss: 0.609470  [ 1840/ 3200]\n",
      "loss: 0.867747  [ 1856/ 3200]\n",
      "loss: 0.584298  [ 1872/ 3200]\n",
      "loss: 0.380437  [ 1888/ 3200]\n",
      "loss: 0.781142  [ 1904/ 3200]\n",
      "loss: 0.448144  [ 1920/ 3200]\n",
      "loss: 0.534677  [ 1936/ 3200]\n",
      "loss: 0.611825  [ 1952/ 3200]\n",
      "loss: 0.531633  [ 1968/ 3200]\n",
      "loss: 0.692009  [ 1984/ 3200]\n",
      "loss: 0.695612  [ 2000/ 3200]\n",
      "loss: 0.767785  [ 2016/ 3200]\n",
      "loss: 1.100445  [ 2032/ 3200]\n",
      "loss: 0.450942  [ 2048/ 3200]\n",
      "loss: 0.662956  [ 2064/ 3200]\n",
      "loss: 0.981735  [ 2080/ 3200]\n",
      "loss: 0.561872  [ 2096/ 3200]\n",
      "loss: 0.512204  [ 2112/ 3200]\n",
      "loss: 0.818355  [ 2128/ 3200]\n",
      "loss: 0.508784  [ 2144/ 3200]\n",
      "loss: 0.722980  [ 2160/ 3200]\n",
      "loss: 0.836318  [ 2176/ 3200]\n",
      "loss: 0.684143  [ 2192/ 3200]\n",
      "loss: 0.358795  [ 2208/ 3200]\n",
      "loss: 0.653266  [ 2224/ 3200]\n",
      "loss: 0.529553  [ 2240/ 3200]\n",
      "loss: 0.582412  [ 2256/ 3200]\n",
      "loss: 0.373297  [ 2272/ 3200]\n",
      "loss: 0.471731  [ 2288/ 3200]\n",
      "loss: 0.520102  [ 2304/ 3200]\n",
      "loss: 0.684736  [ 2320/ 3200]\n",
      "loss: 0.608425  [ 2336/ 3200]\n",
      "loss: 0.521695  [ 2352/ 3200]\n",
      "loss: 0.576034  [ 2368/ 3200]\n",
      "loss: 0.857973  [ 2384/ 3200]\n",
      "loss: 0.653694  [ 2400/ 3200]\n",
      "loss: 0.561459  [ 2416/ 3200]\n",
      "loss: 0.566701  [ 2432/ 3200]\n",
      "loss: 0.756619  [ 2448/ 3200]\n",
      "loss: 0.556797  [ 2464/ 3200]\n",
      "loss: 0.469115  [ 2480/ 3200]\n",
      "loss: 0.387368  [ 2496/ 3200]\n",
      "loss: 0.926138  [ 2512/ 3200]\n",
      "loss: 1.368972  [ 2528/ 3200]\n",
      "loss: 1.141628  [ 2544/ 3200]\n",
      "loss: 0.636288  [ 2560/ 3200]\n",
      "loss: 0.575547  [ 2576/ 3200]\n",
      "loss: 0.564713  [ 2592/ 3200]\n",
      "loss: 0.518341  [ 2608/ 3200]\n",
      "loss: 0.603825  [ 2624/ 3200]\n",
      "loss: 0.478952  [ 2640/ 3200]\n",
      "loss: 0.567102  [ 2656/ 3200]\n",
      "loss: 0.560944  [ 2672/ 3200]\n",
      "loss: 0.699897  [ 2688/ 3200]\n",
      "loss: 0.544176  [ 2704/ 3200]\n",
      "loss: 0.451801  [ 2720/ 3200]\n",
      "loss: 0.534472  [ 2736/ 3200]\n",
      "loss: 0.482117  [ 2752/ 3200]\n",
      "loss: 0.890687  [ 2768/ 3200]\n",
      "loss: 0.727125  [ 2784/ 3200]\n",
      "loss: 0.590966  [ 2800/ 3200]\n",
      "loss: 0.644717  [ 2816/ 3200]\n",
      "loss: 0.564111  [ 2832/ 3200]\n",
      "loss: 0.641019  [ 2848/ 3200]\n",
      "loss: 1.270973  [ 2864/ 3200]\n",
      "loss: 0.598935  [ 2880/ 3200]\n",
      "loss: 0.275559  [ 2896/ 3200]\n",
      "loss: 0.775449  [ 2912/ 3200]\n",
      "loss: 0.490952  [ 2928/ 3200]\n",
      "loss: 0.511471  [ 2944/ 3200]\n",
      "loss: 0.585002  [ 2960/ 3200]\n",
      "loss: 0.671518  [ 2976/ 3200]\n",
      "loss: 0.454029  [ 2992/ 3200]\n",
      "loss: 0.719892  [ 3008/ 3200]\n",
      "loss: 0.722488  [ 3024/ 3200]\n",
      "loss: 1.103864  [ 3040/ 3200]\n",
      "loss: 1.162634  [ 3056/ 3200]\n",
      "loss: 0.747302  [ 3072/ 3200]\n",
      "loss: 0.663516  [ 3088/ 3200]\n",
      "loss: 0.333140  [ 3104/ 3200]\n",
      "loss: 0.603391  [ 3120/ 3200]\n",
      "loss: 0.677723  [ 3136/ 3200]\n",
      "loss: 0.712635  [ 3152/ 3200]\n",
      "loss: 0.608465  [ 3168/ 3200]\n",
      "loss: 0.476776  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.051271\n",
      "f1 macro averaged score: 0.659729\n",
      "Accuracy               : 66.8%\n",
      "Confusion matrix       :\n",
      "tensor([[165,  22,   5,   8],\n",
      "        [ 27,  81,  52,  40],\n",
      "        [  0,  13, 161,  26],\n",
      "        [  4,  26,  43, 127]], device='cuda:0')\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 0.601192  [    0/ 3200]\n",
      "loss: 1.082619  [   16/ 3200]\n",
      "loss: 0.524243  [   32/ 3200]\n",
      "loss: 0.604500  [   48/ 3200]\n",
      "loss: 0.465123  [   64/ 3200]\n",
      "loss: 0.414753  [   80/ 3200]\n",
      "loss: 0.535037  [   96/ 3200]\n",
      "loss: 0.732369  [  112/ 3200]\n",
      "loss: 1.088472  [  128/ 3200]\n",
      "loss: 0.487394  [  144/ 3200]\n",
      "loss: 0.977243  [  160/ 3200]\n",
      "loss: 0.571096  [  176/ 3200]\n",
      "loss: 0.857359  [  192/ 3200]\n",
      "loss: 0.671358  [  208/ 3200]\n",
      "loss: 0.673024  [  224/ 3200]\n",
      "loss: 0.663598  [  240/ 3200]\n",
      "loss: 0.743322  [  256/ 3200]\n",
      "loss: 0.783657  [  272/ 3200]\n",
      "loss: 0.318727  [  288/ 3200]\n",
      "loss: 0.470866  [  304/ 3200]\n",
      "loss: 0.563273  [  320/ 3200]\n",
      "loss: 0.458913  [  336/ 3200]\n",
      "loss: 0.455106  [  352/ 3200]\n",
      "loss: 0.729325  [  368/ 3200]\n",
      "loss: 0.592862  [  384/ 3200]\n",
      "loss: 0.987924  [  400/ 3200]\n",
      "loss: 0.897536  [  416/ 3200]\n",
      "loss: 0.673227  [  432/ 3200]\n",
      "loss: 0.470902  [  448/ 3200]\n",
      "loss: 0.416487  [  464/ 3200]\n",
      "loss: 0.586699  [  480/ 3200]\n",
      "loss: 0.796609  [  496/ 3200]\n",
      "loss: 0.641077  [  512/ 3200]\n",
      "loss: 0.586306  [  528/ 3200]\n",
      "loss: 0.773070  [  544/ 3200]\n",
      "loss: 0.738032  [  560/ 3200]\n",
      "loss: 0.731618  [  576/ 3200]\n",
      "loss: 0.586034  [  592/ 3200]\n",
      "loss: 0.805058  [  608/ 3200]\n",
      "loss: 0.358662  [  624/ 3200]\n",
      "loss: 0.298804  [  640/ 3200]\n",
      "loss: 0.767709  [  656/ 3200]\n",
      "loss: 0.346972  [  672/ 3200]\n",
      "loss: 0.675672  [  688/ 3200]\n",
      "loss: 0.688439  [  704/ 3200]\n",
      "loss: 0.687169  [  720/ 3200]\n",
      "loss: 0.604217  [  736/ 3200]\n",
      "loss: 0.781268  [  752/ 3200]\n",
      "loss: 0.597281  [  768/ 3200]\n",
      "loss: 0.384680  [  784/ 3200]\n",
      "loss: 0.525651  [  800/ 3200]\n",
      "loss: 0.522582  [  816/ 3200]\n",
      "loss: 0.669702  [  832/ 3200]\n",
      "loss: 0.489260  [  848/ 3200]\n",
      "loss: 0.790268  [  864/ 3200]\n",
      "loss: 0.861505  [  880/ 3200]\n",
      "loss: 0.516438  [  896/ 3200]\n",
      "loss: 0.892098  [  912/ 3200]\n",
      "loss: 0.638078  [  928/ 3200]\n",
      "loss: 0.481184  [  944/ 3200]\n",
      "loss: 0.802302  [  960/ 3200]\n",
      "loss: 0.600247  [  976/ 3200]\n",
      "loss: 0.754350  [  992/ 3200]\n",
      "loss: 0.434431  [ 1008/ 3200]\n",
      "loss: 0.600311  [ 1024/ 3200]\n",
      "loss: 0.614215  [ 1040/ 3200]\n",
      "loss: 0.831245  [ 1056/ 3200]\n",
      "loss: 0.481132  [ 1072/ 3200]\n",
      "loss: 0.443773  [ 1088/ 3200]\n",
      "loss: 0.389192  [ 1104/ 3200]\n",
      "loss: 0.654655  [ 1120/ 3200]\n",
      "loss: 0.694566  [ 1136/ 3200]\n",
      "loss: 0.412843  [ 1152/ 3200]\n",
      "loss: 0.500008  [ 1168/ 3200]\n",
      "loss: 0.420956  [ 1184/ 3200]\n",
      "loss: 0.494666  [ 1200/ 3200]\n",
      "loss: 0.603095  [ 1216/ 3200]\n",
      "loss: 0.592361  [ 1232/ 3200]\n",
      "loss: 1.144656  [ 1248/ 3200]\n",
      "loss: 0.660428  [ 1264/ 3200]\n",
      "loss: 0.464832  [ 1280/ 3200]\n",
      "loss: 0.635085  [ 1296/ 3200]\n",
      "loss: 0.594662  [ 1312/ 3200]\n",
      "loss: 0.562073  [ 1328/ 3200]\n",
      "loss: 0.358166  [ 1344/ 3200]\n",
      "loss: 0.654227  [ 1360/ 3200]\n",
      "loss: 0.369716  [ 1376/ 3200]\n",
      "loss: 0.614663  [ 1392/ 3200]\n",
      "loss: 0.779776  [ 1408/ 3200]\n",
      "loss: 0.731132  [ 1424/ 3200]\n",
      "loss: 0.604379  [ 1440/ 3200]\n",
      "loss: 0.699717  [ 1456/ 3200]\n",
      "loss: 1.004497  [ 1472/ 3200]\n",
      "loss: 0.580470  [ 1488/ 3200]\n",
      "loss: 0.440151  [ 1504/ 3200]\n",
      "loss: 0.669326  [ 1520/ 3200]\n",
      "loss: 0.645397  [ 1536/ 3200]\n",
      "loss: 0.367990  [ 1552/ 3200]\n",
      "loss: 0.550598  [ 1568/ 3200]\n",
      "loss: 0.730219  [ 1584/ 3200]\n",
      "loss: 0.791542  [ 1600/ 3200]\n",
      "loss: 0.739350  [ 1616/ 3200]\n",
      "loss: 0.477599  [ 1632/ 3200]\n",
      "loss: 0.322029  [ 1648/ 3200]\n",
      "loss: 0.302680  [ 1664/ 3200]\n",
      "loss: 0.666325  [ 1680/ 3200]\n",
      "loss: 0.457076  [ 1696/ 3200]\n",
      "loss: 0.695661  [ 1712/ 3200]\n",
      "loss: 0.459176  [ 1728/ 3200]\n",
      "loss: 0.518705  [ 1744/ 3200]\n",
      "loss: 0.678724  [ 1760/ 3200]\n",
      "loss: 0.638045  [ 1776/ 3200]\n",
      "loss: 0.521594  [ 1792/ 3200]\n",
      "loss: 0.514617  [ 1808/ 3200]\n",
      "loss: 0.580393  [ 1824/ 3200]\n",
      "loss: 0.595039  [ 1840/ 3200]\n",
      "loss: 0.503510  [ 1856/ 3200]\n",
      "loss: 0.379323  [ 1872/ 3200]\n",
      "loss: 0.623310  [ 1888/ 3200]\n",
      "loss: 0.762595  [ 1904/ 3200]\n",
      "loss: 0.542749  [ 1920/ 3200]\n",
      "loss: 0.491010  [ 1936/ 3200]\n",
      "loss: 0.598230  [ 1952/ 3200]\n",
      "loss: 1.109034  [ 1968/ 3200]\n",
      "loss: 0.826104  [ 1984/ 3200]\n",
      "loss: 0.711862  [ 2000/ 3200]\n",
      "loss: 0.699072  [ 2016/ 3200]\n",
      "loss: 0.339569  [ 2032/ 3200]\n",
      "loss: 0.633230  [ 2048/ 3200]\n",
      "loss: 0.844697  [ 2064/ 3200]\n",
      "loss: 0.752506  [ 2080/ 3200]\n",
      "loss: 0.644939  [ 2096/ 3200]\n",
      "loss: 0.368515  [ 2112/ 3200]\n",
      "loss: 0.502845  [ 2128/ 3200]\n",
      "loss: 0.765068  [ 2144/ 3200]\n",
      "loss: 0.463131  [ 2160/ 3200]\n",
      "loss: 0.475173  [ 2176/ 3200]\n",
      "loss: 0.614187  [ 2192/ 3200]\n",
      "loss: 0.684148  [ 2208/ 3200]\n",
      "loss: 0.520952  [ 2224/ 3200]\n",
      "loss: 0.516119  [ 2240/ 3200]\n",
      "loss: 0.637318  [ 2256/ 3200]\n",
      "loss: 0.588115  [ 2272/ 3200]\n",
      "loss: 0.406389  [ 2288/ 3200]\n",
      "loss: 0.705242  [ 2304/ 3200]\n",
      "loss: 0.900840  [ 2320/ 3200]\n",
      "loss: 0.517518  [ 2336/ 3200]\n",
      "loss: 0.582948  [ 2352/ 3200]\n",
      "loss: 0.552421  [ 2368/ 3200]\n",
      "loss: 0.388731  [ 2384/ 3200]\n",
      "loss: 0.611979  [ 2400/ 3200]\n",
      "loss: 0.513706  [ 2416/ 3200]\n",
      "loss: 0.709080  [ 2432/ 3200]\n",
      "loss: 0.747948  [ 2448/ 3200]\n",
      "loss: 0.861712  [ 2464/ 3200]\n",
      "loss: 0.596439  [ 2480/ 3200]\n",
      "loss: 0.739074  [ 2496/ 3200]\n",
      "loss: 0.826693  [ 2512/ 3200]\n",
      "loss: 0.831002  [ 2528/ 3200]\n",
      "loss: 0.850545  [ 2544/ 3200]\n",
      "loss: 0.420977  [ 2560/ 3200]\n",
      "loss: 0.427025  [ 2576/ 3200]\n",
      "loss: 0.712758  [ 2592/ 3200]\n",
      "loss: 0.592201  [ 2608/ 3200]\n",
      "loss: 0.758348  [ 2624/ 3200]\n",
      "loss: 0.462229  [ 2640/ 3200]\n",
      "loss: 0.695710  [ 2656/ 3200]\n",
      "loss: 0.857779  [ 2672/ 3200]\n",
      "loss: 0.632628  [ 2688/ 3200]\n",
      "loss: 0.607903  [ 2704/ 3200]\n",
      "loss: 0.505100  [ 2720/ 3200]\n",
      "loss: 0.594198  [ 2736/ 3200]\n",
      "loss: 1.225764  [ 2752/ 3200]\n",
      "loss: 0.738346  [ 2768/ 3200]\n",
      "loss: 0.374035  [ 2784/ 3200]\n",
      "loss: 0.803118  [ 2800/ 3200]\n",
      "loss: 0.805625  [ 2816/ 3200]\n",
      "loss: 0.750042  [ 2832/ 3200]\n",
      "loss: 0.412786  [ 2848/ 3200]\n",
      "loss: 0.530932  [ 2864/ 3200]\n",
      "loss: 0.474591  [ 2880/ 3200]\n",
      "loss: 0.639179  [ 2896/ 3200]\n",
      "loss: 1.123392  [ 2912/ 3200]\n",
      "loss: 0.588056  [ 2928/ 3200]\n",
      "loss: 0.374241  [ 2944/ 3200]\n",
      "loss: 0.498343  [ 2960/ 3200]\n",
      "loss: 0.734728  [ 2976/ 3200]\n",
      "loss: 0.316640  [ 2992/ 3200]\n",
      "loss: 0.668376  [ 3008/ 3200]\n",
      "loss: 0.575602  [ 3024/ 3200]\n",
      "loss: 0.636834  [ 3040/ 3200]\n",
      "loss: 0.743199  [ 3056/ 3200]\n",
      "loss: 0.918998  [ 3072/ 3200]\n",
      "loss: 0.600815  [ 3088/ 3200]\n",
      "loss: 0.377202  [ 3104/ 3200]\n",
      "loss: 0.470389  [ 3120/ 3200]\n",
      "loss: 0.403728  [ 3136/ 3200]\n",
      "loss: 0.457603  [ 3152/ 3200]\n",
      "loss: 0.500231  [ 3168/ 3200]\n",
      "loss: 0.748886  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.056702\n",
      "f1 macro averaged score: 0.620851\n",
      "Accuracy               : 62.0%\n",
      "Confusion matrix       :\n",
      "tensor([[126,  62,   9,   3],\n",
      "        [ 16, 102,  68,  14],\n",
      "        [  0,  15, 173,  12],\n",
      "        [  1,  51,  53,  95]], device='cuda:0')\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.577026  [    0/ 3200]\n",
      "loss: 0.503536  [   16/ 3200]\n",
      "loss: 0.454766  [   32/ 3200]\n",
      "loss: 0.507070  [   48/ 3200]\n",
      "loss: 0.417866  [   64/ 3200]\n",
      "loss: 0.554491  [   80/ 3200]\n",
      "loss: 0.396943  [   96/ 3200]\n",
      "loss: 0.797405  [  112/ 3200]\n",
      "loss: 0.793886  [  128/ 3200]\n",
      "loss: 0.643319  [  144/ 3200]\n",
      "loss: 0.364389  [  160/ 3200]\n",
      "loss: 0.665299  [  176/ 3200]\n",
      "loss: 0.726446  [  192/ 3200]\n",
      "loss: 0.636014  [  208/ 3200]\n",
      "loss: 0.741652  [  224/ 3200]\n",
      "loss: 0.802258  [  240/ 3200]\n",
      "loss: 0.471281  [  256/ 3200]\n",
      "loss: 0.715908  [  272/ 3200]\n",
      "loss: 1.080026  [  288/ 3200]\n",
      "loss: 0.677236  [  304/ 3200]\n",
      "loss: 0.608006  [  320/ 3200]\n",
      "loss: 0.593571  [  336/ 3200]\n",
      "loss: 0.868580  [  352/ 3200]\n",
      "loss: 0.854341  [  368/ 3200]\n",
      "loss: 0.907391  [  384/ 3200]\n",
      "loss: 0.564827  [  400/ 3200]\n",
      "loss: 0.658752  [  416/ 3200]\n",
      "loss: 0.888982  [  432/ 3200]\n",
      "loss: 0.949311  [  448/ 3200]\n",
      "loss: 0.600346  [  464/ 3200]\n",
      "loss: 1.330258  [  480/ 3200]\n",
      "loss: 0.566064  [  496/ 3200]\n",
      "loss: 0.619157  [  512/ 3200]\n",
      "loss: 0.633387  [  528/ 3200]\n",
      "loss: 0.619958  [  544/ 3200]\n",
      "loss: 0.495582  [  560/ 3200]\n",
      "loss: 0.385970  [  576/ 3200]\n",
      "loss: 0.512717  [  592/ 3200]\n",
      "loss: 0.975298  [  608/ 3200]\n",
      "loss: 0.448797  [  624/ 3200]\n",
      "loss: 0.742183  [  640/ 3200]\n",
      "loss: 0.517840  [  656/ 3200]\n",
      "loss: 0.700617  [  672/ 3200]\n",
      "loss: 0.869058  [  688/ 3200]\n",
      "loss: 0.868370  [  704/ 3200]\n",
      "loss: 0.934198  [  720/ 3200]\n",
      "loss: 0.616892  [  736/ 3200]\n",
      "loss: 0.442885  [  752/ 3200]\n",
      "loss: 0.393853  [  768/ 3200]\n",
      "loss: 0.997988  [  784/ 3200]\n",
      "loss: 0.562525  [  800/ 3200]\n",
      "loss: 0.485773  [  816/ 3200]\n",
      "loss: 0.345878  [  832/ 3200]\n",
      "loss: 0.689343  [  848/ 3200]\n",
      "loss: 0.487929  [  864/ 3200]\n",
      "loss: 0.666162  [  880/ 3200]\n",
      "loss: 0.729670  [  896/ 3200]\n",
      "loss: 0.745958  [  912/ 3200]\n",
      "loss: 0.672346  [  928/ 3200]\n",
      "loss: 0.659376  [  944/ 3200]\n",
      "loss: 0.799749  [  960/ 3200]\n",
      "loss: 0.551757  [  976/ 3200]\n",
      "loss: 0.601258  [  992/ 3200]\n",
      "loss: 0.612883  [ 1008/ 3200]\n",
      "loss: 0.321434  [ 1024/ 3200]\n",
      "loss: 0.749204  [ 1040/ 3200]\n",
      "loss: 0.793783  [ 1056/ 3200]\n",
      "loss: 0.582785  [ 1072/ 3200]\n",
      "loss: 0.364700  [ 1088/ 3200]\n",
      "loss: 0.460053  [ 1104/ 3200]\n",
      "loss: 0.805538  [ 1120/ 3200]\n",
      "loss: 0.600454  [ 1136/ 3200]\n",
      "loss: 0.467640  [ 1152/ 3200]\n",
      "loss: 0.393487  [ 1168/ 3200]\n",
      "loss: 0.598968  [ 1184/ 3200]\n",
      "loss: 0.703268  [ 1200/ 3200]\n",
      "loss: 0.584508  [ 1216/ 3200]\n",
      "loss: 0.607379  [ 1232/ 3200]\n",
      "loss: 0.555886  [ 1248/ 3200]\n",
      "loss: 0.376595  [ 1264/ 3200]\n",
      "loss: 0.351346  [ 1280/ 3200]\n",
      "loss: 0.400096  [ 1296/ 3200]\n",
      "loss: 0.766823  [ 1312/ 3200]\n",
      "loss: 0.399716  [ 1328/ 3200]\n",
      "loss: 0.764436  [ 1344/ 3200]\n",
      "loss: 0.537326  [ 1360/ 3200]\n",
      "loss: 0.700565  [ 1376/ 3200]\n",
      "loss: 0.630804  [ 1392/ 3200]\n",
      "loss: 0.516657  [ 1408/ 3200]\n",
      "loss: 0.717210  [ 1424/ 3200]\n",
      "loss: 0.796195  [ 1440/ 3200]\n",
      "loss: 0.720349  [ 1456/ 3200]\n",
      "loss: 0.411380  [ 1472/ 3200]\n",
      "loss: 0.411816  [ 1488/ 3200]\n",
      "loss: 0.890372  [ 1504/ 3200]\n",
      "loss: 0.529264  [ 1520/ 3200]\n",
      "loss: 0.544368  [ 1536/ 3200]\n",
      "loss: 0.310178  [ 1552/ 3200]\n",
      "loss: 0.570306  [ 1568/ 3200]\n",
      "loss: 0.190949  [ 1584/ 3200]\n",
      "loss: 0.507525  [ 1600/ 3200]\n",
      "loss: 0.374851  [ 1616/ 3200]\n",
      "loss: 0.385674  [ 1632/ 3200]\n",
      "loss: 0.370795  [ 1648/ 3200]\n",
      "loss: 0.618911  [ 1664/ 3200]\n",
      "loss: 0.805674  [ 1680/ 3200]\n",
      "loss: 0.490285  [ 1696/ 3200]\n",
      "loss: 0.677427  [ 1712/ 3200]\n",
      "loss: 0.563927  [ 1728/ 3200]\n",
      "loss: 0.408648  [ 1744/ 3200]\n",
      "loss: 0.556525  [ 1760/ 3200]\n",
      "loss: 0.661800  [ 1776/ 3200]\n",
      "loss: 0.705388  [ 1792/ 3200]\n",
      "loss: 0.525229  [ 1808/ 3200]\n",
      "loss: 0.400949  [ 1824/ 3200]\n",
      "loss: 0.668599  [ 1840/ 3200]\n",
      "loss: 0.383970  [ 1856/ 3200]\n",
      "loss: 0.619988  [ 1872/ 3200]\n",
      "loss: 0.455206  [ 1888/ 3200]\n",
      "loss: 0.268773  [ 1904/ 3200]\n",
      "loss: 1.073639  [ 1920/ 3200]\n",
      "loss: 0.601023  [ 1936/ 3200]\n",
      "loss: 0.698696  [ 1952/ 3200]\n",
      "loss: 0.766194  [ 1968/ 3200]\n",
      "loss: 0.585347  [ 1984/ 3200]\n",
      "loss: 0.771257  [ 2000/ 3200]\n",
      "loss: 0.391555  [ 2016/ 3200]\n",
      "loss: 0.308129  [ 2032/ 3200]\n",
      "loss: 0.518813  [ 2048/ 3200]\n",
      "loss: 0.732639  [ 2064/ 3200]\n",
      "loss: 0.684655  [ 2080/ 3200]\n",
      "loss: 0.796021  [ 2096/ 3200]\n",
      "loss: 0.576281  [ 2112/ 3200]\n",
      "loss: 0.497445  [ 2128/ 3200]\n",
      "loss: 0.612893  [ 2144/ 3200]\n",
      "loss: 0.532052  [ 2160/ 3200]\n",
      "loss: 0.517751  [ 2176/ 3200]\n",
      "loss: 0.580033  [ 2192/ 3200]\n",
      "loss: 0.448856  [ 2208/ 3200]\n",
      "loss: 0.772670  [ 2224/ 3200]\n",
      "loss: 0.596996  [ 2240/ 3200]\n",
      "loss: 0.684590  [ 2256/ 3200]\n",
      "loss: 0.617155  [ 2272/ 3200]\n",
      "loss: 0.880937  [ 2288/ 3200]\n",
      "loss: 0.422923  [ 2304/ 3200]\n",
      "loss: 0.268950  [ 2320/ 3200]\n",
      "loss: 0.833483  [ 2336/ 3200]\n",
      "loss: 0.635155  [ 2352/ 3200]\n",
      "loss: 0.538763  [ 2368/ 3200]\n",
      "loss: 0.960904  [ 2384/ 3200]\n",
      "loss: 0.201556  [ 2400/ 3200]\n",
      "loss: 0.607718  [ 2416/ 3200]\n",
      "loss: 0.712529  [ 2432/ 3200]\n",
      "loss: 0.431993  [ 2448/ 3200]\n",
      "loss: 0.385709  [ 2464/ 3200]\n",
      "loss: 0.292069  [ 2480/ 3200]\n",
      "loss: 0.341362  [ 2496/ 3200]\n",
      "loss: 0.490517  [ 2512/ 3200]\n",
      "loss: 0.734641  [ 2528/ 3200]\n",
      "loss: 0.537624  [ 2544/ 3200]\n",
      "loss: 0.774889  [ 2560/ 3200]\n",
      "loss: 0.254112  [ 2576/ 3200]\n",
      "loss: 0.972960  [ 2592/ 3200]\n",
      "loss: 0.635689  [ 2608/ 3200]\n",
      "loss: 0.230935  [ 2624/ 3200]\n",
      "loss: 0.678269  [ 2640/ 3200]\n",
      "loss: 0.353663  [ 2656/ 3200]\n",
      "loss: 0.432636  [ 2672/ 3200]\n",
      "loss: 1.048152  [ 2688/ 3200]\n",
      "loss: 0.763552  [ 2704/ 3200]\n",
      "loss: 0.753017  [ 2720/ 3200]\n",
      "loss: 0.565476  [ 2736/ 3200]\n",
      "loss: 0.752956  [ 2752/ 3200]\n",
      "loss: 0.687041  [ 2768/ 3200]\n",
      "loss: 0.522102  [ 2784/ 3200]\n",
      "loss: 0.520658  [ 2800/ 3200]\n",
      "loss: 0.952923  [ 2816/ 3200]\n",
      "loss: 0.579879  [ 2832/ 3200]\n",
      "loss: 0.670923  [ 2848/ 3200]\n",
      "loss: 0.985094  [ 2864/ 3200]\n",
      "loss: 0.674255  [ 2880/ 3200]\n",
      "loss: 0.776435  [ 2896/ 3200]\n",
      "loss: 0.732400  [ 2912/ 3200]\n",
      "loss: 0.713507  [ 2928/ 3200]\n",
      "loss: 0.576820  [ 2944/ 3200]\n",
      "loss: 0.419847  [ 2960/ 3200]\n",
      "loss: 0.358660  [ 2976/ 3200]\n",
      "loss: 0.309584  [ 2992/ 3200]\n",
      "loss: 0.333122  [ 3008/ 3200]\n",
      "loss: 0.515762  [ 3024/ 3200]\n",
      "loss: 0.489114  [ 3040/ 3200]\n",
      "loss: 0.549253  [ 3056/ 3200]\n",
      "loss: 0.966686  [ 3072/ 3200]\n",
      "loss: 0.553643  [ 3088/ 3200]\n",
      "loss: 0.806143  [ 3104/ 3200]\n",
      "loss: 0.549213  [ 3120/ 3200]\n",
      "loss: 0.676187  [ 3136/ 3200]\n",
      "loss: 0.818600  [ 3152/ 3200]\n",
      "loss: 0.864432  [ 3168/ 3200]\n",
      "loss: 0.861893  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.051310\n",
      "f1 macro averaged score: 0.669131\n",
      "Accuracy               : 67.4%\n",
      "Confusion matrix       :\n",
      "tensor([[176,  14,   4,   6],\n",
      "        [ 40,  97,  15,  48],\n",
      "        [  4,  25, 132,  39],\n",
      "        [ 11,  30,  25, 134]], device='cuda:0')\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 0.752643  [    0/ 3200]\n",
      "loss: 0.560354  [   16/ 3200]\n",
      "loss: 0.265896  [   32/ 3200]\n",
      "loss: 0.590173  [   48/ 3200]\n",
      "loss: 0.681431  [   64/ 3200]\n",
      "loss: 0.357669  [   80/ 3200]\n",
      "loss: 0.363029  [   96/ 3200]\n",
      "loss: 0.644516  [  112/ 3200]\n",
      "loss: 0.424714  [  128/ 3200]\n",
      "loss: 0.614009  [  144/ 3200]\n",
      "loss: 0.432169  [  160/ 3200]\n",
      "loss: 0.529474  [  176/ 3200]\n",
      "loss: 0.712157  [  192/ 3200]\n",
      "loss: 0.798254  [  208/ 3200]\n",
      "loss: 0.550594  [  224/ 3200]\n",
      "loss: 0.584162  [  240/ 3200]\n",
      "loss: 0.426287  [  256/ 3200]\n",
      "loss: 0.576217  [  272/ 3200]\n",
      "loss: 0.727339  [  288/ 3200]\n",
      "loss: 0.539327  [  304/ 3200]\n",
      "loss: 0.435460  [  320/ 3200]\n",
      "loss: 0.407832  [  336/ 3200]\n",
      "loss: 0.473407  [  352/ 3200]\n",
      "loss: 0.979130  [  368/ 3200]\n",
      "loss: 0.652894  [  384/ 3200]\n",
      "loss: 0.649944  [  400/ 3200]\n",
      "loss: 0.652370  [  416/ 3200]\n",
      "loss: 0.815740  [  432/ 3200]\n",
      "loss: 0.438597  [  448/ 3200]\n",
      "loss: 0.779001  [  464/ 3200]\n",
      "loss: 0.572780  [  480/ 3200]\n",
      "loss: 0.677096  [  496/ 3200]\n",
      "loss: 0.827800  [  512/ 3200]\n",
      "loss: 0.523674  [  528/ 3200]\n",
      "loss: 0.354729  [  544/ 3200]\n",
      "loss: 0.387473  [  560/ 3200]\n",
      "loss: 0.680481  [  576/ 3200]\n",
      "loss: 0.537591  [  592/ 3200]\n",
      "loss: 0.956450  [  608/ 3200]\n",
      "loss: 0.748320  [  624/ 3200]\n",
      "loss: 0.440847  [  640/ 3200]\n",
      "loss: 0.380387  [  656/ 3200]\n",
      "loss: 0.577883  [  672/ 3200]\n",
      "loss: 0.577725  [  688/ 3200]\n",
      "loss: 0.587401  [  704/ 3200]\n",
      "loss: 0.803049  [  720/ 3200]\n",
      "loss: 0.396300  [  736/ 3200]\n",
      "loss: 0.622836  [  752/ 3200]\n",
      "loss: 0.333771  [  768/ 3200]\n",
      "loss: 0.617400  [  784/ 3200]\n",
      "loss: 0.370715  [  800/ 3200]\n",
      "loss: 0.376627  [  816/ 3200]\n",
      "loss: 0.483655  [  832/ 3200]\n",
      "loss: 0.834054  [  848/ 3200]\n",
      "loss: 0.724806  [  864/ 3200]\n",
      "loss: 0.513189  [  880/ 3200]\n",
      "loss: 0.701798  [  896/ 3200]\n",
      "loss: 0.609152  [  912/ 3200]\n",
      "loss: 0.675373  [  928/ 3200]\n",
      "loss: 0.636420  [  944/ 3200]\n",
      "loss: 0.462596  [  960/ 3200]\n",
      "loss: 0.539147  [  976/ 3200]\n",
      "loss: 0.470291  [  992/ 3200]\n",
      "loss: 0.514750  [ 1008/ 3200]\n",
      "loss: 0.590264  [ 1024/ 3200]\n",
      "loss: 0.784445  [ 1040/ 3200]\n",
      "loss: 0.600069  [ 1056/ 3200]\n",
      "loss: 0.631220  [ 1072/ 3200]\n",
      "loss: 0.524230  [ 1088/ 3200]\n",
      "loss: 0.476457  [ 1104/ 3200]\n",
      "loss: 0.365251  [ 1120/ 3200]\n",
      "loss: 0.671329  [ 1136/ 3200]\n",
      "loss: 0.390424  [ 1152/ 3200]\n",
      "loss: 0.503670  [ 1168/ 3200]\n",
      "loss: 0.473733  [ 1184/ 3200]\n",
      "loss: 0.779496  [ 1200/ 3200]\n",
      "loss: 0.583663  [ 1216/ 3200]\n",
      "loss: 0.571986  [ 1232/ 3200]\n",
      "loss: 0.491718  [ 1248/ 3200]\n",
      "loss: 0.474826  [ 1264/ 3200]\n",
      "loss: 0.496184  [ 1280/ 3200]\n",
      "loss: 0.843771  [ 1296/ 3200]\n",
      "loss: 0.838507  [ 1312/ 3200]\n",
      "loss: 0.693336  [ 1328/ 3200]\n",
      "loss: 0.579448  [ 1344/ 3200]\n",
      "loss: 0.448589  [ 1360/ 3200]\n",
      "loss: 0.406819  [ 1376/ 3200]\n",
      "loss: 0.660895  [ 1392/ 3200]\n",
      "loss: 0.483891  [ 1408/ 3200]\n",
      "loss: 0.608333  [ 1424/ 3200]\n",
      "loss: 0.492367  [ 1440/ 3200]\n",
      "loss: 0.285265  [ 1456/ 3200]\n",
      "loss: 0.365022  [ 1472/ 3200]\n",
      "loss: 0.361161  [ 1488/ 3200]\n",
      "loss: 0.638907  [ 1504/ 3200]\n",
      "loss: 0.564461  [ 1520/ 3200]\n",
      "loss: 0.226742  [ 1536/ 3200]\n",
      "loss: 0.834677  [ 1552/ 3200]\n",
      "loss: 0.519267  [ 1568/ 3200]\n",
      "loss: 0.597469  [ 1584/ 3200]\n",
      "loss: 0.565196  [ 1600/ 3200]\n",
      "loss: 0.588537  [ 1616/ 3200]\n",
      "loss: 0.505158  [ 1632/ 3200]\n",
      "loss: 0.765451  [ 1648/ 3200]\n",
      "loss: 0.775030  [ 1664/ 3200]\n",
      "loss: 0.863974  [ 1680/ 3200]\n",
      "loss: 0.645887  [ 1696/ 3200]\n",
      "loss: 0.585567  [ 1712/ 3200]\n",
      "loss: 0.534657  [ 1728/ 3200]\n",
      "loss: 0.643874  [ 1744/ 3200]\n",
      "loss: 0.914976  [ 1760/ 3200]\n",
      "loss: 0.532303  [ 1776/ 3200]\n",
      "loss: 0.378255  [ 1792/ 3200]\n",
      "loss: 0.955960  [ 1808/ 3200]\n",
      "loss: 0.670355  [ 1824/ 3200]\n",
      "loss: 0.394926  [ 1840/ 3200]\n",
      "loss: 0.405628  [ 1856/ 3200]\n",
      "loss: 0.648277  [ 1872/ 3200]\n",
      "loss: 0.569766  [ 1888/ 3200]\n",
      "loss: 0.468016  [ 1904/ 3200]\n",
      "loss: 0.604932  [ 1920/ 3200]\n",
      "loss: 0.814574  [ 1936/ 3200]\n",
      "loss: 0.766671  [ 1952/ 3200]\n",
      "loss: 0.531863  [ 1968/ 3200]\n",
      "loss: 0.586556  [ 1984/ 3200]\n",
      "loss: 0.629461  [ 2000/ 3200]\n",
      "loss: 0.501109  [ 2016/ 3200]\n",
      "loss: 0.452678  [ 2032/ 3200]\n",
      "loss: 0.564099  [ 2048/ 3200]\n",
      "loss: 0.618038  [ 2064/ 3200]\n",
      "loss: 0.574709  [ 2080/ 3200]\n",
      "loss: 0.664055  [ 2096/ 3200]\n",
      "loss: 0.880957  [ 2112/ 3200]\n",
      "loss: 0.475011  [ 2128/ 3200]\n",
      "loss: 0.579071  [ 2144/ 3200]\n",
      "loss: 0.971036  [ 2160/ 3200]\n",
      "loss: 0.842790  [ 2176/ 3200]\n",
      "loss: 0.783003  [ 2192/ 3200]\n",
      "loss: 0.883997  [ 2208/ 3200]\n",
      "loss: 0.763985  [ 2224/ 3200]\n",
      "loss: 0.480480  [ 2240/ 3200]\n",
      "loss: 0.523763  [ 2256/ 3200]\n",
      "loss: 0.654514  [ 2272/ 3200]\n",
      "loss: 0.634909  [ 2288/ 3200]\n",
      "loss: 0.742011  [ 2304/ 3200]\n",
      "loss: 0.703867  [ 2320/ 3200]\n",
      "loss: 0.395439  [ 2336/ 3200]\n",
      "loss: 0.564892  [ 2352/ 3200]\n",
      "loss: 0.357765  [ 2368/ 3200]\n",
      "loss: 0.590468  [ 2384/ 3200]\n",
      "loss: 0.488656  [ 2400/ 3200]\n",
      "loss: 0.466952  [ 2416/ 3200]\n",
      "loss: 0.587165  [ 2432/ 3200]\n",
      "loss: 0.532288  [ 2448/ 3200]\n",
      "loss: 0.332505  [ 2464/ 3200]\n",
      "loss: 0.889690  [ 2480/ 3200]\n",
      "loss: 1.080578  [ 2496/ 3200]\n",
      "loss: 0.399301  [ 2512/ 3200]\n",
      "loss: 0.348176  [ 2528/ 3200]\n",
      "loss: 0.513210  [ 2544/ 3200]\n",
      "loss: 1.339790  [ 2560/ 3200]\n",
      "loss: 0.915750  [ 2576/ 3200]\n",
      "loss: 0.571080  [ 2592/ 3200]\n",
      "loss: 0.533526  [ 2608/ 3200]\n",
      "loss: 0.414894  [ 2624/ 3200]\n",
      "loss: 0.583128  [ 2640/ 3200]\n",
      "loss: 0.438924  [ 2656/ 3200]\n",
      "loss: 0.371249  [ 2672/ 3200]\n",
      "loss: 0.948812  [ 2688/ 3200]\n",
      "loss: 0.817157  [ 2704/ 3200]\n",
      "loss: 0.549292  [ 2720/ 3200]\n",
      "loss: 0.491979  [ 2736/ 3200]\n",
      "loss: 0.521761  [ 2752/ 3200]\n",
      "loss: 0.602143  [ 2768/ 3200]\n",
      "loss: 0.482621  [ 2784/ 3200]\n",
      "loss: 0.358378  [ 2800/ 3200]\n",
      "loss: 0.656275  [ 2816/ 3200]\n",
      "loss: 0.895124  [ 2832/ 3200]\n",
      "loss: 0.463499  [ 2848/ 3200]\n",
      "loss: 0.588075  [ 2864/ 3200]\n",
      "loss: 0.440949  [ 2880/ 3200]\n",
      "loss: 0.890979  [ 2896/ 3200]\n",
      "loss: 0.930683  [ 2912/ 3200]\n",
      "loss: 0.811010  [ 2928/ 3200]\n",
      "loss: 0.711576  [ 2944/ 3200]\n",
      "loss: 0.690923  [ 2960/ 3200]\n",
      "loss: 0.378093  [ 2976/ 3200]\n",
      "loss: 0.527643  [ 2992/ 3200]\n",
      "loss: 0.576711  [ 3008/ 3200]\n",
      "loss: 0.321247  [ 3024/ 3200]\n",
      "loss: 0.492059  [ 3040/ 3200]\n",
      "loss: 0.458158  [ 3056/ 3200]\n",
      "loss: 0.731479  [ 3072/ 3200]\n",
      "loss: 0.587329  [ 3088/ 3200]\n",
      "loss: 0.489162  [ 3104/ 3200]\n",
      "loss: 0.692889  [ 3120/ 3200]\n",
      "loss: 0.600429  [ 3136/ 3200]\n",
      "loss: 0.523635  [ 3152/ 3200]\n",
      "loss: 0.486649  [ 3168/ 3200]\n",
      "loss: 0.748794  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.050261\n",
      "f1 macro averaged score: 0.681153\n",
      "Accuracy               : 68.2%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  16,   4,   5],\n",
      "        [ 35, 118,  24,  23],\n",
      "        [  2,  36, 139,  23],\n",
      "        [  8,  52,  26, 114]], device='cuda:0')\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.802822  [    0/ 3200]\n",
      "loss: 0.600519  [   16/ 3200]\n",
      "loss: 0.522411  [   32/ 3200]\n",
      "loss: 0.545288  [   48/ 3200]\n",
      "loss: 0.700276  [   64/ 3200]\n",
      "loss: 0.587682  [   80/ 3200]\n",
      "loss: 0.493293  [   96/ 3200]\n",
      "loss: 0.409986  [  112/ 3200]\n",
      "loss: 0.651408  [  128/ 3200]\n",
      "loss: 0.301970  [  144/ 3200]\n",
      "loss: 0.185540  [  160/ 3200]\n",
      "loss: 1.067053  [  176/ 3200]\n",
      "loss: 0.595096  [  192/ 3200]\n",
      "loss: 1.037563  [  208/ 3200]\n",
      "loss: 0.552089  [  224/ 3200]\n",
      "loss: 0.558999  [  240/ 3200]\n",
      "loss: 0.844655  [  256/ 3200]\n",
      "loss: 0.458873  [  272/ 3200]\n",
      "loss: 0.318999  [  288/ 3200]\n",
      "loss: 0.531018  [  304/ 3200]\n",
      "loss: 0.692961  [  320/ 3200]\n",
      "loss: 0.539524  [  336/ 3200]\n",
      "loss: 1.133185  [  352/ 3200]\n",
      "loss: 0.723689  [  368/ 3200]\n",
      "loss: 0.339285  [  384/ 3200]\n",
      "loss: 0.671421  [  400/ 3200]\n",
      "loss: 0.789993  [  416/ 3200]\n",
      "loss: 0.615152  [  432/ 3200]\n",
      "loss: 0.741912  [  448/ 3200]\n",
      "loss: 0.464461  [  464/ 3200]\n",
      "loss: 0.417564  [  480/ 3200]\n",
      "loss: 0.345041  [  496/ 3200]\n",
      "loss: 0.436658  [  512/ 3200]\n",
      "loss: 0.373627  [  528/ 3200]\n",
      "loss: 0.384449  [  544/ 3200]\n",
      "loss: 0.715049  [  560/ 3200]\n",
      "loss: 0.519498  [  576/ 3200]\n",
      "loss: 0.524317  [  592/ 3200]\n",
      "loss: 0.223848  [  608/ 3200]\n",
      "loss: 0.381204  [  624/ 3200]\n",
      "loss: 0.795018  [  640/ 3200]\n",
      "loss: 0.691962  [  656/ 3200]\n",
      "loss: 0.678175  [  672/ 3200]\n",
      "loss: 0.538022  [  688/ 3200]\n",
      "loss: 0.709312  [  704/ 3200]\n",
      "loss: 0.525776  [  720/ 3200]\n",
      "loss: 0.544320  [  736/ 3200]\n",
      "loss: 0.391064  [  752/ 3200]\n",
      "loss: 0.477351  [  768/ 3200]\n",
      "loss: 0.301405  [  784/ 3200]\n",
      "loss: 0.286069  [  800/ 3200]\n",
      "loss: 0.566508  [  816/ 3200]\n",
      "loss: 0.534689  [  832/ 3200]\n",
      "loss: 0.427714  [  848/ 3200]\n",
      "loss: 0.505744  [  864/ 3200]\n",
      "loss: 0.422439  [  880/ 3200]\n",
      "loss: 0.232449  [  896/ 3200]\n",
      "loss: 0.493089  [  912/ 3200]\n",
      "loss: 0.625012  [  928/ 3200]\n",
      "loss: 0.325823  [  944/ 3200]\n",
      "loss: 0.447090  [  960/ 3200]\n",
      "loss: 0.781217  [  976/ 3200]\n",
      "loss: 0.647321  [  992/ 3200]\n",
      "loss: 0.655395  [ 1008/ 3200]\n",
      "loss: 0.990072  [ 1024/ 3200]\n",
      "loss: 0.549603  [ 1040/ 3200]\n",
      "loss: 0.472021  [ 1056/ 3200]\n",
      "loss: 0.618298  [ 1072/ 3200]\n",
      "loss: 0.885643  [ 1088/ 3200]\n",
      "loss: 0.451687  [ 1104/ 3200]\n",
      "loss: 0.241740  [ 1120/ 3200]\n",
      "loss: 0.640748  [ 1136/ 3200]\n",
      "loss: 0.809896  [ 1152/ 3200]\n",
      "loss: 0.664776  [ 1168/ 3200]\n",
      "loss: 0.642964  [ 1184/ 3200]\n",
      "loss: 0.419199  [ 1200/ 3200]\n",
      "loss: 0.356286  [ 1216/ 3200]\n",
      "loss: 0.437838  [ 1232/ 3200]\n",
      "loss: 0.471520  [ 1248/ 3200]\n",
      "loss: 0.717349  [ 1264/ 3200]\n",
      "loss: 0.438148  [ 1280/ 3200]\n",
      "loss: 0.420676  [ 1296/ 3200]\n",
      "loss: 0.827963  [ 1312/ 3200]\n",
      "loss: 0.772466  [ 1328/ 3200]\n",
      "loss: 0.614049  [ 1344/ 3200]\n",
      "loss: 0.258846  [ 1360/ 3200]\n",
      "loss: 0.559929  [ 1376/ 3200]\n",
      "loss: 0.478013  [ 1392/ 3200]\n",
      "loss: 0.610820  [ 1408/ 3200]\n",
      "loss: 0.494930  [ 1424/ 3200]\n",
      "loss: 0.631847  [ 1440/ 3200]\n",
      "loss: 0.438124  [ 1456/ 3200]\n",
      "loss: 0.767306  [ 1472/ 3200]\n",
      "loss: 0.705989  [ 1488/ 3200]\n",
      "loss: 0.697506  [ 1504/ 3200]\n",
      "loss: 0.542592  [ 1520/ 3200]\n",
      "loss: 0.617685  [ 1536/ 3200]\n",
      "loss: 0.583195  [ 1552/ 3200]\n",
      "loss: 0.796873  [ 1568/ 3200]\n",
      "loss: 0.640632  [ 1584/ 3200]\n",
      "loss: 0.928325  [ 1600/ 3200]\n",
      "loss: 0.333570  [ 1616/ 3200]\n",
      "loss: 0.533565  [ 1632/ 3200]\n",
      "loss: 0.729147  [ 1648/ 3200]\n",
      "loss: 0.636463  [ 1664/ 3200]\n",
      "loss: 0.497351  [ 1680/ 3200]\n",
      "loss: 0.334884  [ 1696/ 3200]\n",
      "loss: 0.402294  [ 1712/ 3200]\n",
      "loss: 0.403274  [ 1728/ 3200]\n",
      "loss: 0.807237  [ 1744/ 3200]\n",
      "loss: 0.787374  [ 1760/ 3200]\n",
      "loss: 0.466260  [ 1776/ 3200]\n",
      "loss: 0.604920  [ 1792/ 3200]\n",
      "loss: 0.746058  [ 1808/ 3200]\n",
      "loss: 0.387400  [ 1824/ 3200]\n",
      "loss: 0.967847  [ 1840/ 3200]\n",
      "loss: 0.940641  [ 1856/ 3200]\n",
      "loss: 0.857312  [ 1872/ 3200]\n",
      "loss: 0.436572  [ 1888/ 3200]\n",
      "loss: 0.715370  [ 1904/ 3200]\n",
      "loss: 0.959818  [ 1920/ 3200]\n",
      "loss: 0.455986  [ 1936/ 3200]\n",
      "loss: 0.578125  [ 1952/ 3200]\n",
      "loss: 0.447970  [ 1968/ 3200]\n",
      "loss: 0.736694  [ 1984/ 3200]\n",
      "loss: 0.485230  [ 2000/ 3200]\n",
      "loss: 0.578621  [ 2016/ 3200]\n",
      "loss: 0.548682  [ 2032/ 3200]\n",
      "loss: 0.632350  [ 2048/ 3200]\n",
      "loss: 0.795318  [ 2064/ 3200]\n",
      "loss: 0.588202  [ 2080/ 3200]\n",
      "loss: 0.565781  [ 2096/ 3200]\n",
      "loss: 0.675052  [ 2112/ 3200]\n",
      "loss: 0.662855  [ 2128/ 3200]\n",
      "loss: 0.530010  [ 2144/ 3200]\n",
      "loss: 0.569858  [ 2160/ 3200]\n",
      "loss: 0.867208  [ 2176/ 3200]\n",
      "loss: 0.611722  [ 2192/ 3200]\n",
      "loss: 0.571886  [ 2208/ 3200]\n",
      "loss: 0.558272  [ 2224/ 3200]\n",
      "loss: 0.362811  [ 2240/ 3200]\n",
      "loss: 0.596294  [ 2256/ 3200]\n",
      "loss: 0.423268  [ 2272/ 3200]\n",
      "loss: 0.410270  [ 2288/ 3200]\n",
      "loss: 0.417015  [ 2304/ 3200]\n",
      "loss: 0.670995  [ 2320/ 3200]\n",
      "loss: 0.445593  [ 2336/ 3200]\n",
      "loss: 0.569421  [ 2352/ 3200]\n",
      "loss: 0.322407  [ 2368/ 3200]\n",
      "loss: 0.935372  [ 2384/ 3200]\n",
      "loss: 0.798470  [ 2400/ 3200]\n",
      "loss: 0.468386  [ 2416/ 3200]\n",
      "loss: 0.847498  [ 2432/ 3200]\n",
      "loss: 0.408040  [ 2448/ 3200]\n",
      "loss: 0.380113  [ 2464/ 3200]\n",
      "loss: 0.453606  [ 2480/ 3200]\n",
      "loss: 0.698497  [ 2496/ 3200]\n",
      "loss: 0.464044  [ 2512/ 3200]\n",
      "loss: 0.352856  [ 2528/ 3200]\n",
      "loss: 0.402628  [ 2544/ 3200]\n",
      "loss: 0.590548  [ 2560/ 3200]\n",
      "loss: 0.573506  [ 2576/ 3200]\n",
      "loss: 0.755016  [ 2592/ 3200]\n",
      "loss: 0.735469  [ 2608/ 3200]\n",
      "loss: 0.300901  [ 2624/ 3200]\n",
      "loss: 0.608315  [ 2640/ 3200]\n",
      "loss: 0.562556  [ 2656/ 3200]\n",
      "loss: 0.303033  [ 2672/ 3200]\n",
      "loss: 0.482175  [ 2688/ 3200]\n",
      "loss: 0.523704  [ 2704/ 3200]\n",
      "loss: 0.530434  [ 2720/ 3200]\n",
      "loss: 0.397195  [ 2736/ 3200]\n",
      "loss: 0.648610  [ 2752/ 3200]\n",
      "loss: 0.700588  [ 2768/ 3200]\n",
      "loss: 0.472073  [ 2784/ 3200]\n",
      "loss: 1.514268  [ 2800/ 3200]\n",
      "loss: 1.024148  [ 2816/ 3200]\n",
      "loss: 0.725537  [ 2832/ 3200]\n",
      "loss: 0.387574  [ 2848/ 3200]\n",
      "loss: 0.417549  [ 2864/ 3200]\n",
      "loss: 0.292619  [ 2880/ 3200]\n",
      "loss: 0.664499  [ 2896/ 3200]\n",
      "loss: 0.732175  [ 2912/ 3200]\n",
      "loss: 0.424112  [ 2928/ 3200]\n",
      "loss: 0.583685  [ 2944/ 3200]\n",
      "loss: 0.388677  [ 2960/ 3200]\n",
      "loss: 0.736388  [ 2976/ 3200]\n",
      "loss: 0.418298  [ 2992/ 3200]\n",
      "loss: 0.559205  [ 3008/ 3200]\n",
      "loss: 0.508210  [ 3024/ 3200]\n",
      "loss: 0.630892  [ 3040/ 3200]\n",
      "loss: 0.740176  [ 3056/ 3200]\n",
      "loss: 0.524340  [ 3072/ 3200]\n",
      "loss: 0.349002  [ 3088/ 3200]\n",
      "loss: 0.567075  [ 3104/ 3200]\n",
      "loss: 0.812050  [ 3120/ 3200]\n",
      "loss: 0.607684  [ 3136/ 3200]\n",
      "loss: 0.659031  [ 3152/ 3200]\n",
      "loss: 0.795795  [ 3168/ 3200]\n",
      "loss: 0.313937  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.051745\n",
      "f1 macro averaged score: 0.663077\n",
      "Accuracy               : 66.5%\n",
      "Confusion matrix       :\n",
      "tensor([[153,  35,   5,   7],\n",
      "        [ 22,  93,  50,  35],\n",
      "        [  0,  14, 161,  25],\n",
      "        [  2,  36,  37, 125]], device='cuda:0')\n",
      "\n",
      "Best epoch: 29 with f1 macro averaged score: 0.6811529397964478\n",
      "CPU times: user 22.8 s, sys: 649 ms, total: 23.4 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_model, f1_per_epoch = validate_convolutional_neural_network(epochs, optimizer, train_dataloader, val_dataloader, loss_function, cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkOTUQQFCRsF"
   },
   "source": [
    "The best epoch for this execution is the $29$th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NbZMGT8F7eg"
   },
   "source": [
    "Test the best Convolutional Neural Network on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_z812NIFvLr",
    "outputId": "560c3ced-ef3d-4b06-aac2-cc89ed0d6cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:\n",
      "Avg loss               : 0.052765\n",
      "f1 macro averaged score: 0.672850\n",
      "Accuracy               : 66.8%\n",
      "Confusion matrix       :\n",
      "tensor([[229,  41,  19,   8],\n",
      "        [ 12, 186,  65,  61],\n",
      "        [  2,  26, 301,  27],\n",
      "        [ 10,  92,  94, 203]], device='cuda:0')\n",
      "CPU times: user 155 ms, sys: 4.05 ms, total: 159 ms\n",
      "Wall time: 162 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = test_convolutional_neural_network(test_dataloader, loss_function, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGeeAxL25iH1"
   },
   "source": [
    "Pooling is a technique used to reduce the feature map size, which also reduces the number of parameters that must be computed during the training procedure.\n",
    "\n",
    "Max pooling keeps the features that are related to the the filter the most. [[2]](#reference)\n",
    "<br></br>\n",
    "\n",
    "Padding is used to control the reduction of the number of features, which can also be observed by the formula given in **Question 2 - Step 3**.\n",
    "\n",
    "Padding can increase accuracy when analysing data in a Convolutional Neural Network.\n",
    "[[3]](#reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-XLbBf6Fg8I"
   },
   "source": [
    "### Step 5 - Optimization algorithms\n",
    "\n",
    "We validate our Convolutional Neural Network for $30$ epochs and test it.\n",
    "\n",
    "The optimizers used are the following: Adadelta, Adagrad, Adam, AdamW, Adamax, ASGD, NAdam, RAdam, RMSprop, Rprop and SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e99eCnJQGmoZ",
    "outputId": "4cde0255-3270-4924-a851-5a85c0b730fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 1.083132  [ 1552/ 3200]\n",
      "loss: 1.276684  [ 1568/ 3200]\n",
      "loss: 1.222754  [ 1584/ 3200]\n",
      "loss: 1.008696  [ 1600/ 3200]\n",
      "loss: 1.195230  [ 1616/ 3200]\n",
      "loss: 1.065974  [ 1632/ 3200]\n",
      "loss: 1.057531  [ 1648/ 3200]\n",
      "loss: 1.176192  [ 1664/ 3200]\n",
      "loss: 0.922043  [ 1680/ 3200]\n",
      "loss: 1.072480  [ 1696/ 3200]\n",
      "loss: 1.173874  [ 1712/ 3200]\n",
      "loss: 1.300763  [ 1728/ 3200]\n",
      "loss: 0.943481  [ 1744/ 3200]\n",
      "loss: 1.156795  [ 1760/ 3200]\n",
      "loss: 1.094091  [ 1776/ 3200]\n",
      "loss: 1.153724  [ 1792/ 3200]\n",
      "loss: 1.142830  [ 1808/ 3200]\n",
      "loss: 1.165508  [ 1824/ 3200]\n",
      "loss: 1.024940  [ 1840/ 3200]\n",
      "loss: 1.026091  [ 1856/ 3200]\n",
      "loss: 1.047325  [ 1872/ 3200]\n",
      "loss: 1.214421  [ 1888/ 3200]\n",
      "loss: 1.302827  [ 1904/ 3200]\n",
      "loss: 1.169886  [ 1920/ 3200]\n",
      "loss: 1.044699  [ 1936/ 3200]\n",
      "loss: 1.239419  [ 1952/ 3200]\n",
      "loss: 0.853471  [ 1968/ 3200]\n",
      "loss: 1.168366  [ 1984/ 3200]\n",
      "loss: 1.245954  [ 2000/ 3200]\n",
      "loss: 1.047333  [ 2016/ 3200]\n",
      "loss: 0.917769  [ 2032/ 3200]\n",
      "loss: 1.043424  [ 2048/ 3200]\n",
      "loss: 1.092232  [ 2064/ 3200]\n",
      "loss: 1.060745  [ 2080/ 3200]\n",
      "loss: 1.221581  [ 2096/ 3200]\n",
      "loss: 1.147834  [ 2112/ 3200]\n",
      "loss: 1.202304  [ 2128/ 3200]\n",
      "loss: 1.082648  [ 2144/ 3200]\n",
      "loss: 1.047356  [ 2160/ 3200]\n",
      "loss: 1.262898  [ 2176/ 3200]\n",
      "loss: 1.264205  [ 2192/ 3200]\n",
      "loss: 1.364214  [ 2208/ 3200]\n",
      "loss: 1.267254  [ 2224/ 3200]\n",
      "loss: 1.138597  [ 2240/ 3200]\n",
      "loss: 1.224400  [ 2256/ 3200]\n",
      "loss: 1.163738  [ 2272/ 3200]\n",
      "loss: 0.901480  [ 2288/ 3200]\n",
      "loss: 1.101656  [ 2304/ 3200]\n",
      "loss: 1.109078  [ 2320/ 3200]\n",
      "loss: 1.319481  [ 2336/ 3200]\n",
      "loss: 1.091503  [ 2352/ 3200]\n",
      "loss: 1.150217  [ 2368/ 3200]\n",
      "loss: 1.120716  [ 2384/ 3200]\n",
      "loss: 1.194311  [ 2400/ 3200]\n",
      "loss: 1.119563  [ 2416/ 3200]\n",
      "loss: 1.010822  [ 2432/ 3200]\n",
      "loss: 1.151440  [ 2448/ 3200]\n",
      "loss: 1.237309  [ 2464/ 3200]\n",
      "loss: 1.181390  [ 2480/ 3200]\n",
      "loss: 1.232707  [ 2496/ 3200]\n",
      "loss: 1.000090  [ 2512/ 3200]\n",
      "loss: 1.159473  [ 2528/ 3200]\n",
      "loss: 1.078487  [ 2544/ 3200]\n",
      "loss: 0.984984  [ 2560/ 3200]\n",
      "loss: 1.089316  [ 2576/ 3200]\n",
      "loss: 1.087077  [ 2592/ 3200]\n",
      "loss: 1.324012  [ 2608/ 3200]\n",
      "loss: 1.081525  [ 2624/ 3200]\n",
      "loss: 1.053177  [ 2640/ 3200]\n",
      "loss: 1.294641  [ 2656/ 3200]\n",
      "loss: 1.771058  [ 2672/ 3200]\n",
      "loss: 1.210459  [ 2688/ 3200]\n",
      "loss: 1.243261  [ 2704/ 3200]\n",
      "loss: 0.966983  [ 2720/ 3200]\n",
      "loss: 1.087372  [ 2736/ 3200]\n",
      "loss: 0.774794  [ 2752/ 3200]\n",
      "loss: 1.215696  [ 2768/ 3200]\n",
      "loss: 1.098028  [ 2784/ 3200]\n",
      "loss: 1.270640  [ 2800/ 3200]\n",
      "loss: 1.178257  [ 2816/ 3200]\n",
      "loss: 1.011310  [ 2832/ 3200]\n",
      "loss: 0.955446  [ 2848/ 3200]\n",
      "loss: 1.337731  [ 2864/ 3200]\n",
      "loss: 1.132857  [ 2880/ 3200]\n",
      "loss: 1.103503  [ 2896/ 3200]\n",
      "loss: 1.036028  [ 2912/ 3200]\n",
      "loss: 1.313504  [ 2928/ 3200]\n",
      "loss: 1.214754  [ 2944/ 3200]\n",
      "loss: 1.119075  [ 2960/ 3200]\n",
      "loss: 1.090439  [ 2976/ 3200]\n",
      "loss: 0.965441  [ 2992/ 3200]\n",
      "loss: 1.295101  [ 3008/ 3200]\n",
      "loss: 1.043802  [ 3024/ 3200]\n",
      "loss: 1.454162  [ 3040/ 3200]\n",
      "loss: 1.566289  [ 3056/ 3200]\n",
      "loss: 1.025519  [ 3072/ 3200]\n",
      "loss: 0.958815  [ 3088/ 3200]\n",
      "loss: 1.157032  [ 3104/ 3200]\n",
      "loss: 1.013596  [ 3120/ 3200]\n",
      "loss: 1.150998  [ 3136/ 3200]\n",
      "loss: 1.166141  [ 3152/ 3200]\n",
      "loss: 1.045596  [ 3168/ 3200]\n",
      "loss: 1.026022  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.068310\n",
      "f1 macro averaged score: 0.400322\n",
      "Accuracy               : 46.8%\n",
      "Confusion matrix       :\n",
      "tensor([[145,  38,  17,   0],\n",
      "        [ 39,  67,  94,   0],\n",
      "        [  6,  32, 162,   0],\n",
      "        [ 10,  34, 156,   0]], device='cuda:0')\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 0.947168  [    0/ 3200]\n",
      "loss: 0.928840  [   16/ 3200]\n",
      "loss: 0.885683  [   32/ 3200]\n",
      "loss: 1.055068  [   48/ 3200]\n",
      "loss: 1.154900  [   64/ 3200]\n",
      "loss: 1.018795  [   80/ 3200]\n",
      "loss: 1.466634  [   96/ 3200]\n",
      "loss: 1.200263  [  112/ 3200]\n",
      "loss: 0.986368  [  128/ 3200]\n",
      "loss: 1.117635  [  144/ 3200]\n",
      "loss: 1.285664  [  160/ 3200]\n",
      "loss: 0.929650  [  176/ 3200]\n",
      "loss: 0.892563  [  192/ 3200]\n",
      "loss: 1.270174  [  208/ 3200]\n",
      "loss: 1.061264  [  224/ 3200]\n",
      "loss: 1.105755  [  240/ 3200]\n",
      "loss: 0.922193  [  256/ 3200]\n",
      "loss: 1.125492  [  272/ 3200]\n",
      "loss: 1.020872  [  288/ 3200]\n",
      "loss: 1.266997  [  304/ 3200]\n",
      "loss: 1.449917  [  320/ 3200]\n",
      "loss: 1.024047  [  336/ 3200]\n",
      "loss: 1.154519  [  352/ 3200]\n",
      "loss: 1.055522  [  368/ 3200]\n",
      "loss: 0.991295  [  384/ 3200]\n",
      "loss: 1.308805  [  400/ 3200]\n",
      "loss: 1.352571  [  416/ 3200]\n",
      "loss: 1.145849  [  432/ 3200]\n",
      "loss: 1.432139  [  448/ 3200]\n",
      "loss: 0.999969  [  464/ 3200]\n",
      "loss: 1.134114  [  480/ 3200]\n",
      "loss: 1.243552  [  496/ 3200]\n",
      "loss: 1.127760  [  512/ 3200]\n",
      "loss: 1.052118  [  528/ 3200]\n",
      "loss: 1.078158  [  544/ 3200]\n",
      "loss: 1.215477  [  560/ 3200]\n",
      "loss: 1.130917  [  576/ 3200]\n",
      "loss: 0.921207  [  592/ 3200]\n",
      "loss: 1.096696  [  608/ 3200]\n",
      "loss: 1.001571  [  624/ 3200]\n",
      "loss: 1.329748  [  640/ 3200]\n",
      "loss: 1.064236  [  656/ 3200]\n",
      "loss: 1.062588  [  672/ 3200]\n",
      "loss: 1.175816  [  688/ 3200]\n",
      "loss: 1.247844  [  704/ 3200]\n",
      "loss: 1.136437  [  720/ 3200]\n",
      "loss: 1.325094  [  736/ 3200]\n",
      "loss: 0.966333  [  752/ 3200]\n",
      "loss: 1.052886  [  768/ 3200]\n",
      "loss: 1.240991  [  784/ 3200]\n",
      "loss: 1.129994  [  800/ 3200]\n",
      "loss: 1.079522  [  816/ 3200]\n",
      "loss: 1.086330  [  832/ 3200]\n",
      "loss: 1.232327  [  848/ 3200]\n",
      "loss: 0.985039  [  864/ 3200]\n",
      "loss: 0.976296  [  880/ 3200]\n",
      "loss: 1.027559  [  896/ 3200]\n",
      "loss: 0.950130  [  912/ 3200]\n",
      "loss: 0.794985  [  928/ 3200]\n",
      "loss: 0.867909  [  944/ 3200]\n",
      "loss: 1.130133  [  960/ 3200]\n",
      "loss: 1.053543  [  976/ 3200]\n",
      "loss: 1.176833  [  992/ 3200]\n",
      "loss: 0.951379  [ 1008/ 3200]\n",
      "loss: 0.972634  [ 1024/ 3200]\n",
      "loss: 1.252627  [ 1040/ 3200]\n",
      "loss: 1.068915  [ 1056/ 3200]\n",
      "loss: 1.349281  [ 1072/ 3200]\n",
      "loss: 1.058141  [ 1088/ 3200]\n",
      "loss: 1.385663  [ 1104/ 3200]\n",
      "loss: 1.139286  [ 1120/ 3200]\n",
      "loss: 1.310287  [ 1136/ 3200]\n",
      "loss: 1.105030  [ 1152/ 3200]\n",
      "loss: 1.140436  [ 1168/ 3200]\n",
      "loss: 1.085285  [ 1184/ 3200]\n",
      "loss: 0.976911  [ 1200/ 3200]\n",
      "loss: 1.115655  [ 1216/ 3200]\n",
      "loss: 0.871030  [ 1232/ 3200]\n",
      "loss: 1.058289  [ 1248/ 3200]\n",
      "loss: 1.140999  [ 1264/ 3200]\n",
      "loss: 0.923507  [ 1280/ 3200]\n",
      "loss: 1.255116  [ 1296/ 3200]\n",
      "loss: 1.121313  [ 1312/ 3200]\n",
      "loss: 1.119998  [ 1328/ 3200]\n",
      "loss: 1.081974  [ 1344/ 3200]\n",
      "loss: 1.175650  [ 1360/ 3200]\n",
      "loss: 0.867639  [ 1376/ 3200]\n",
      "loss: 1.167380  [ 1392/ 3200]\n",
      "loss: 1.438915  [ 1408/ 3200]\n",
      "loss: 1.174278  [ 1424/ 3200]\n",
      "loss: 1.097101  [ 1440/ 3200]\n",
      "loss: 1.044939  [ 1456/ 3200]\n",
      "loss: 1.254529  [ 1472/ 3200]\n",
      "loss: 1.074395  [ 1488/ 3200]\n",
      "loss: 1.150783  [ 1504/ 3200]\n",
      "loss: 1.180911  [ 1520/ 3200]\n",
      "loss: 1.030748  [ 1536/ 3200]\n",
      "loss: 1.121885  [ 1552/ 3200]\n",
      "loss: 1.043163  [ 1568/ 3200]\n",
      "loss: 1.139601  [ 1584/ 3200]\n",
      "loss: 1.189999  [ 1600/ 3200]\n",
      "loss: 1.088110  [ 1616/ 3200]\n",
      "loss: 1.179568  [ 1632/ 3200]\n",
      "loss: 1.212258  [ 1648/ 3200]\n",
      "loss: 1.167253  [ 1664/ 3200]\n",
      "loss: 1.081452  [ 1680/ 3200]\n",
      "loss: 0.998584  [ 1696/ 3200]\n",
      "loss: 0.792835  [ 1712/ 3200]\n",
      "loss: 0.964275  [ 1728/ 3200]\n",
      "loss: 1.096622  [ 1744/ 3200]\n",
      "loss: 1.382537  [ 1760/ 3200]\n",
      "loss: 1.107241  [ 1776/ 3200]\n",
      "loss: 0.942646  [ 1792/ 3200]\n",
      "loss: 1.099064  [ 1808/ 3200]\n",
      "loss: 1.322497  [ 1824/ 3200]\n",
      "loss: 1.163636  [ 1840/ 3200]\n",
      "loss: 0.938074  [ 1856/ 3200]\n",
      "loss: 0.988825  [ 1872/ 3200]\n",
      "loss: 1.051826  [ 1888/ 3200]\n",
      "loss: 0.935148  [ 1904/ 3200]\n",
      "loss: 0.894392  [ 1920/ 3200]\n",
      "loss: 1.028969  [ 1936/ 3200]\n",
      "loss: 1.051660  [ 1952/ 3200]\n",
      "loss: 1.134572  [ 1968/ 3200]\n",
      "loss: 0.977091  [ 1984/ 3200]\n",
      "loss: 1.001370  [ 2000/ 3200]\n",
      "loss: 1.139422  [ 2016/ 3200]\n",
      "loss: 1.065693  [ 2032/ 3200]\n",
      "loss: 1.405743  [ 2048/ 3200]\n",
      "loss: 1.006844  [ 2064/ 3200]\n",
      "loss: 1.166310  [ 2080/ 3200]\n",
      "loss: 1.073200  [ 2096/ 3200]\n",
      "loss: 0.931120  [ 2112/ 3200]\n",
      "loss: 1.290611  [ 2128/ 3200]\n",
      "loss: 1.024363  [ 2144/ 3200]\n",
      "loss: 1.029136  [ 2160/ 3200]\n",
      "loss: 0.915074  [ 2176/ 3200]\n",
      "loss: 1.361440  [ 2192/ 3200]\n",
      "loss: 0.868870  [ 2208/ 3200]\n",
      "loss: 1.326560  [ 2224/ 3200]\n",
      "loss: 0.901532  [ 2240/ 3200]\n",
      "loss: 1.134065  [ 2256/ 3200]\n",
      "loss: 1.365366  [ 2272/ 3200]\n",
      "loss: 1.311064  [ 2288/ 3200]\n",
      "loss: 0.947279  [ 2304/ 3200]\n",
      "loss: 1.246430  [ 2320/ 3200]\n",
      "loss: 0.971618  [ 2336/ 3200]\n",
      "loss: 1.133413  [ 2352/ 3200]\n",
      "loss: 0.880391  [ 2368/ 3200]\n",
      "loss: 1.263219  [ 2384/ 3200]\n",
      "loss: 1.285362  [ 2400/ 3200]\n",
      "loss: 1.018212  [ 2416/ 3200]\n",
      "loss: 1.143800  [ 2432/ 3200]\n",
      "loss: 1.029010  [ 2448/ 3200]\n",
      "loss: 1.347001  [ 2464/ 3200]\n",
      "loss: 0.872656  [ 2480/ 3200]\n",
      "loss: 1.063294  [ 2496/ 3200]\n",
      "loss: 1.156067  [ 2512/ 3200]\n",
      "loss: 0.930914  [ 2528/ 3200]\n",
      "loss: 1.004712  [ 2544/ 3200]\n",
      "loss: 1.552074  [ 2560/ 3200]\n",
      "loss: 1.744268  [ 2576/ 3200]\n",
      "loss: 1.113082  [ 2592/ 3200]\n",
      "loss: 1.139979  [ 2608/ 3200]\n",
      "loss: 0.887838  [ 2624/ 3200]\n",
      "loss: 1.170203  [ 2640/ 3200]\n",
      "loss: 0.988675  [ 2656/ 3200]\n",
      "loss: 1.213470  [ 2672/ 3200]\n",
      "loss: 0.973332  [ 2688/ 3200]\n",
      "loss: 0.894116  [ 2704/ 3200]\n",
      "loss: 1.037249  [ 2720/ 3200]\n",
      "loss: 1.159595  [ 2736/ 3200]\n",
      "loss: 1.095754  [ 2752/ 3200]\n",
      "loss: 1.118096  [ 2768/ 3200]\n",
      "loss: 0.858112  [ 2784/ 3200]\n",
      "loss: 1.208119  [ 2800/ 3200]\n",
      "loss: 0.967948  [ 2816/ 3200]\n",
      "loss: 1.036772  [ 2832/ 3200]\n",
      "loss: 1.290567  [ 2848/ 3200]\n",
      "loss: 1.043531  [ 2864/ 3200]\n",
      "loss: 1.016844  [ 2880/ 3200]\n",
      "loss: 1.023381  [ 2896/ 3200]\n",
      "loss: 1.099242  [ 2912/ 3200]\n",
      "loss: 1.109102  [ 2928/ 3200]\n",
      "loss: 1.085985  [ 2944/ 3200]\n",
      "loss: 0.976762  [ 2960/ 3200]\n",
      "loss: 1.084852  [ 2976/ 3200]\n",
      "loss: 1.147987  [ 2992/ 3200]\n",
      "loss: 0.909073  [ 3008/ 3200]\n",
      "loss: 1.144158  [ 3024/ 3200]\n",
      "loss: 1.304212  [ 3040/ 3200]\n",
      "loss: 1.151188  [ 3056/ 3200]\n",
      "loss: 0.883108  [ 3072/ 3200]\n",
      "loss: 1.050832  [ 3088/ 3200]\n",
      "loss: 1.122046  [ 3104/ 3200]\n",
      "loss: 1.356635  [ 3120/ 3200]\n",
      "loss: 1.235947  [ 3136/ 3200]\n",
      "loss: 0.909033  [ 3152/ 3200]\n",
      "loss: 1.256329  [ 3168/ 3200]\n",
      "loss: 1.191177  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.067903\n",
      "f1 macro averaged score: 0.417379\n",
      "Accuracy               : 47.1%\n",
      "Confusion matrix       :\n",
      "tensor([[133,  56,  11,   0],\n",
      "        [ 30,  88,  82,   0],\n",
      "        [  4,  42, 153,   1],\n",
      "        [  4,  52, 141,   3]], device='cuda:0')\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 1.002430  [    0/ 3200]\n",
      "loss: 1.084138  [   16/ 3200]\n",
      "loss: 0.886957  [   32/ 3200]\n",
      "loss: 1.074423  [   48/ 3200]\n",
      "loss: 1.166844  [   64/ 3200]\n",
      "loss: 1.125536  [   80/ 3200]\n",
      "loss: 0.933840  [   96/ 3200]\n",
      "loss: 1.021618  [  112/ 3200]\n",
      "loss: 1.170289  [  128/ 3200]\n",
      "loss: 0.988748  [  144/ 3200]\n",
      "loss: 0.744525  [  160/ 3200]\n",
      "loss: 1.117831  [  176/ 3200]\n",
      "loss: 1.114267  [  192/ 3200]\n",
      "loss: 1.122689  [  208/ 3200]\n",
      "loss: 1.145352  [  224/ 3200]\n",
      "loss: 1.103533  [  240/ 3200]\n",
      "loss: 0.869906  [  256/ 3200]\n",
      "loss: 1.082027  [  272/ 3200]\n",
      "loss: 0.958695  [  288/ 3200]\n",
      "loss: 0.888794  [  304/ 3200]\n",
      "loss: 1.095721  [  320/ 3200]\n",
      "loss: 1.017827  [  336/ 3200]\n",
      "loss: 1.004532  [  352/ 3200]\n",
      "loss: 1.178063  [  368/ 3200]\n",
      "loss: 1.033073  [  384/ 3200]\n",
      "loss: 0.994646  [  400/ 3200]\n",
      "loss: 1.011839  [  416/ 3200]\n",
      "loss: 1.088746  [  432/ 3200]\n",
      "loss: 1.141550  [  448/ 3200]\n",
      "loss: 1.169393  [  464/ 3200]\n",
      "loss: 1.193514  [  480/ 3200]\n",
      "loss: 1.056634  [  496/ 3200]\n",
      "loss: 1.144663  [  512/ 3200]\n",
      "loss: 1.279611  [  528/ 3200]\n",
      "loss: 1.186178  [  544/ 3200]\n",
      "loss: 0.983355  [  560/ 3200]\n",
      "loss: 1.318754  [  576/ 3200]\n",
      "loss: 1.095483  [  592/ 3200]\n",
      "loss: 0.769894  [  608/ 3200]\n",
      "loss: 0.703609  [  624/ 3200]\n",
      "loss: 1.244282  [  640/ 3200]\n",
      "loss: 0.938325  [  656/ 3200]\n",
      "loss: 0.858407  [  672/ 3200]\n",
      "loss: 1.205719  [  688/ 3200]\n",
      "loss: 1.156128  [  704/ 3200]\n",
      "loss: 1.059784  [  720/ 3200]\n",
      "loss: 1.010997  [  736/ 3200]\n",
      "loss: 1.257641  [  752/ 3200]\n",
      "loss: 1.484566  [  768/ 3200]\n",
      "loss: 1.028157  [  784/ 3200]\n",
      "loss: 1.174659  [  800/ 3200]\n",
      "loss: 0.973925  [  816/ 3200]\n",
      "loss: 0.979005  [  832/ 3200]\n",
      "loss: 0.904477  [  848/ 3200]\n",
      "loss: 1.029636  [  864/ 3200]\n",
      "loss: 0.988028  [  880/ 3200]\n",
      "loss: 1.019030  [  896/ 3200]\n",
      "loss: 1.044586  [  912/ 3200]\n",
      "loss: 1.471431  [  928/ 3200]\n",
      "loss: 1.104042  [  944/ 3200]\n",
      "loss: 1.041486  [  960/ 3200]\n",
      "loss: 1.150075  [  976/ 3200]\n",
      "loss: 1.022750  [  992/ 3200]\n",
      "loss: 1.041815  [ 1008/ 3200]\n",
      "loss: 0.830458  [ 1024/ 3200]\n",
      "loss: 0.946896  [ 1040/ 3200]\n",
      "loss: 1.125947  [ 1056/ 3200]\n",
      "loss: 1.369823  [ 1072/ 3200]\n",
      "loss: 1.031155  [ 1088/ 3200]\n",
      "loss: 1.357993  [ 1104/ 3200]\n",
      "loss: 0.923069  [ 1120/ 3200]\n",
      "loss: 0.849038  [ 1136/ 3200]\n",
      "loss: 1.267614  [ 1152/ 3200]\n",
      "loss: 1.042837  [ 1168/ 3200]\n",
      "loss: 0.951475  [ 1184/ 3200]\n",
      "loss: 0.894689  [ 1200/ 3200]\n",
      "loss: 0.835194  [ 1216/ 3200]\n",
      "loss: 1.146702  [ 1232/ 3200]\n",
      "loss: 1.117572  [ 1248/ 3200]\n",
      "loss: 0.881145  [ 1264/ 3200]\n",
      "loss: 1.093537  [ 1280/ 3200]\n",
      "loss: 1.482110  [ 1296/ 3200]\n",
      "loss: 1.254565  [ 1312/ 3200]\n",
      "loss: 1.252483  [ 1328/ 3200]\n",
      "loss: 1.062714  [ 1344/ 3200]\n",
      "loss: 1.020455  [ 1360/ 3200]\n",
      "loss: 1.108653  [ 1376/ 3200]\n",
      "loss: 1.345425  [ 1392/ 3200]\n",
      "loss: 1.248498  [ 1408/ 3200]\n",
      "loss: 1.054443  [ 1424/ 3200]\n",
      "loss: 0.917763  [ 1440/ 3200]\n",
      "loss: 1.070543  [ 1456/ 3200]\n",
      "loss: 1.027862  [ 1472/ 3200]\n",
      "loss: 0.979991  [ 1488/ 3200]\n",
      "loss: 0.908183  [ 1504/ 3200]\n",
      "loss: 1.166489  [ 1520/ 3200]\n",
      "loss: 1.229379  [ 1536/ 3200]\n",
      "loss: 1.672955  [ 1552/ 3200]\n",
      "loss: 0.702733  [ 1568/ 3200]\n",
      "loss: 0.963066  [ 1584/ 3200]\n",
      "loss: 1.038304  [ 1600/ 3200]\n",
      "loss: 1.122026  [ 1616/ 3200]\n",
      "loss: 0.941468  [ 1632/ 3200]\n",
      "loss: 1.290965  [ 1648/ 3200]\n",
      "loss: 1.280166  [ 1664/ 3200]\n",
      "loss: 0.918934  [ 1680/ 3200]\n",
      "loss: 1.154033  [ 1696/ 3200]\n",
      "loss: 1.107306  [ 1712/ 3200]\n",
      "loss: 0.977491  [ 1728/ 3200]\n",
      "loss: 1.168885  [ 1744/ 3200]\n",
      "loss: 1.213032  [ 1760/ 3200]\n",
      "loss: 1.034579  [ 1776/ 3200]\n",
      "loss: 0.879019  [ 1792/ 3200]\n",
      "loss: 1.053803  [ 1808/ 3200]\n",
      "loss: 0.914761  [ 1824/ 3200]\n",
      "loss: 0.921894  [ 1840/ 3200]\n",
      "loss: 1.010457  [ 1856/ 3200]\n",
      "loss: 0.843750  [ 1872/ 3200]\n",
      "loss: 1.168230  [ 1888/ 3200]\n",
      "loss: 1.003572  [ 1904/ 3200]\n",
      "loss: 1.210633  [ 1920/ 3200]\n",
      "loss: 1.059083  [ 1936/ 3200]\n",
      "loss: 1.070782  [ 1952/ 3200]\n",
      "loss: 1.253279  [ 1968/ 3200]\n",
      "loss: 1.229088  [ 1984/ 3200]\n",
      "loss: 1.095643  [ 2000/ 3200]\n",
      "loss: 0.965716  [ 2016/ 3200]\n",
      "loss: 0.987310  [ 2032/ 3200]\n",
      "loss: 0.898468  [ 2048/ 3200]\n",
      "loss: 1.144997  [ 2064/ 3200]\n",
      "loss: 1.109533  [ 2080/ 3200]\n",
      "loss: 1.188280  [ 2096/ 3200]\n",
      "loss: 1.075280  [ 2112/ 3200]\n",
      "loss: 1.365176  [ 2128/ 3200]\n",
      "loss: 1.054702  [ 2144/ 3200]\n",
      "loss: 1.066246  [ 2160/ 3200]\n",
      "loss: 0.945416  [ 2176/ 3200]\n",
      "loss: 1.005963  [ 2192/ 3200]\n",
      "loss: 0.963678  [ 2208/ 3200]\n",
      "loss: 1.293206  [ 2224/ 3200]\n",
      "loss: 1.036318  [ 2240/ 3200]\n",
      "loss: 0.942856  [ 2256/ 3200]\n",
      "loss: 1.415325  [ 2272/ 3200]\n",
      "loss: 1.063154  [ 2288/ 3200]\n",
      "loss: 1.106880  [ 2304/ 3200]\n",
      "loss: 1.072365  [ 2320/ 3200]\n",
      "loss: 1.236655  [ 2336/ 3200]\n",
      "loss: 0.983233  [ 2352/ 3200]\n",
      "loss: 1.162879  [ 2368/ 3200]\n",
      "loss: 1.008724  [ 2384/ 3200]\n",
      "loss: 1.145265  [ 2400/ 3200]\n",
      "loss: 0.915283  [ 2416/ 3200]\n",
      "loss: 1.319496  [ 2432/ 3200]\n",
      "loss: 1.111781  [ 2448/ 3200]\n",
      "loss: 1.328227  [ 2464/ 3200]\n",
      "loss: 0.874082  [ 2480/ 3200]\n",
      "loss: 1.085183  [ 2496/ 3200]\n",
      "loss: 0.737027  [ 2512/ 3200]\n",
      "loss: 1.120548  [ 2528/ 3200]\n",
      "loss: 1.208190  [ 2544/ 3200]\n",
      "loss: 1.058029  [ 2560/ 3200]\n",
      "loss: 0.932147  [ 2576/ 3200]\n",
      "loss: 0.915676  [ 2592/ 3200]\n",
      "loss: 0.909606  [ 2608/ 3200]\n",
      "loss: 0.864275  [ 2624/ 3200]\n",
      "loss: 1.195782  [ 2640/ 3200]\n",
      "loss: 0.929514  [ 2656/ 3200]\n",
      "loss: 0.934314  [ 2672/ 3200]\n",
      "loss: 1.149399  [ 2688/ 3200]\n",
      "loss: 1.098269  [ 2704/ 3200]\n",
      "loss: 1.069079  [ 2720/ 3200]\n",
      "loss: 1.071860  [ 2736/ 3200]\n",
      "loss: 1.030389  [ 2752/ 3200]\n",
      "loss: 1.151352  [ 2768/ 3200]\n",
      "loss: 0.988869  [ 2784/ 3200]\n",
      "loss: 0.720920  [ 2800/ 3200]\n",
      "loss: 0.997152  [ 2816/ 3200]\n",
      "loss: 1.310255  [ 2832/ 3200]\n",
      "loss: 1.178431  [ 2848/ 3200]\n",
      "loss: 0.880635  [ 2864/ 3200]\n",
      "loss: 1.082229  [ 2880/ 3200]\n",
      "loss: 1.093231  [ 2896/ 3200]\n",
      "loss: 0.982138  [ 2912/ 3200]\n",
      "loss: 0.810956  [ 2928/ 3200]\n",
      "loss: 0.985723  [ 2944/ 3200]\n",
      "loss: 1.033865  [ 2960/ 3200]\n",
      "loss: 0.877390  [ 2976/ 3200]\n",
      "loss: 1.086971  [ 2992/ 3200]\n",
      "loss: 0.994453  [ 3008/ 3200]\n",
      "loss: 1.154494  [ 3024/ 3200]\n",
      "loss: 0.995340  [ 3040/ 3200]\n",
      "loss: 1.017665  [ 3056/ 3200]\n",
      "loss: 1.240456  [ 3072/ 3200]\n",
      "loss: 1.125141  [ 3088/ 3200]\n",
      "loss: 1.036679  [ 3104/ 3200]\n",
      "loss: 1.036012  [ 3120/ 3200]\n",
      "loss: 0.851281  [ 3136/ 3200]\n",
      "loss: 1.118035  [ 3152/ 3200]\n",
      "loss: 0.868112  [ 3168/ 3200]\n",
      "loss: 1.121367  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.066046\n",
      "f1 macro averaged score: 0.491956\n",
      "Accuracy               : 50.7%\n",
      "Confusion matrix       :\n",
      "tensor([[161,  37,   0,   2],\n",
      "        [ 66,  86,  14,  34],\n",
      "        [ 15,  59,  48,  78],\n",
      "        [ 13,  68,   8, 111]], device='cuda:0')\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 1.007603  [    0/ 3200]\n",
      "loss: 1.070815  [   16/ 3200]\n",
      "loss: 0.889116  [   32/ 3200]\n",
      "loss: 0.892269  [   48/ 3200]\n",
      "loss: 0.956377  [   64/ 3200]\n",
      "loss: 1.152073  [   80/ 3200]\n",
      "loss: 0.885899  [   96/ 3200]\n",
      "loss: 1.314857  [  112/ 3200]\n",
      "loss: 0.942994  [  128/ 3200]\n",
      "loss: 1.188966  [  144/ 3200]\n",
      "loss: 1.209946  [  160/ 3200]\n",
      "loss: 1.174189  [  176/ 3200]\n",
      "loss: 1.330642  [  192/ 3200]\n",
      "loss: 1.044652  [  208/ 3200]\n",
      "loss: 0.910907  [  224/ 3200]\n",
      "loss: 0.913980  [  240/ 3200]\n",
      "loss: 0.880451  [  256/ 3200]\n",
      "loss: 0.706986  [  272/ 3200]\n",
      "loss: 1.132297  [  288/ 3200]\n",
      "loss: 0.943222  [  304/ 3200]\n",
      "loss: 1.134335  [  320/ 3200]\n",
      "loss: 0.967242  [  336/ 3200]\n",
      "loss: 0.956790  [  352/ 3200]\n",
      "loss: 0.957292  [  368/ 3200]\n",
      "loss: 1.029311  [  384/ 3200]\n",
      "loss: 0.926754  [  400/ 3200]\n",
      "loss: 0.884986  [  416/ 3200]\n",
      "loss: 0.922123  [  432/ 3200]\n",
      "loss: 1.126274  [  448/ 3200]\n",
      "loss: 0.974513  [  464/ 3200]\n",
      "loss: 0.787495  [  480/ 3200]\n",
      "loss: 1.165118  [  496/ 3200]\n",
      "loss: 1.038566  [  512/ 3200]\n",
      "loss: 1.031651  [  528/ 3200]\n",
      "loss: 1.156771  [  544/ 3200]\n",
      "loss: 1.076158  [  560/ 3200]\n",
      "loss: 0.924130  [  576/ 3200]\n",
      "loss: 0.996214  [  592/ 3200]\n",
      "loss: 1.079586  [  608/ 3200]\n",
      "loss: 1.099998  [  624/ 3200]\n",
      "loss: 0.995852  [  640/ 3200]\n",
      "loss: 1.337742  [  656/ 3200]\n",
      "loss: 1.188576  [  672/ 3200]\n",
      "loss: 0.961837  [  688/ 3200]\n",
      "loss: 0.747171  [  704/ 3200]\n",
      "loss: 0.974326  [  720/ 3200]\n",
      "loss: 1.256805  [  736/ 3200]\n",
      "loss: 1.002952  [  752/ 3200]\n",
      "loss: 1.128672  [  768/ 3200]\n",
      "loss: 0.897485  [  784/ 3200]\n",
      "loss: 1.185565  [  800/ 3200]\n",
      "loss: 0.965653  [  816/ 3200]\n",
      "loss: 1.197793  [  832/ 3200]\n",
      "loss: 1.393120  [  848/ 3200]\n",
      "loss: 1.038766  [  864/ 3200]\n",
      "loss: 0.949282  [  880/ 3200]\n",
      "loss: 1.004338  [  896/ 3200]\n",
      "loss: 1.195290  [  912/ 3200]\n",
      "loss: 1.164452  [  928/ 3200]\n",
      "loss: 1.209769  [  944/ 3200]\n",
      "loss: 0.968242  [  960/ 3200]\n",
      "loss: 0.869838  [  976/ 3200]\n",
      "loss: 0.884637  [  992/ 3200]\n",
      "loss: 1.167353  [ 1008/ 3200]\n",
      "loss: 0.803808  [ 1024/ 3200]\n",
      "loss: 1.182066  [ 1040/ 3200]\n",
      "loss: 1.508483  [ 1056/ 3200]\n",
      "loss: 1.054801  [ 1072/ 3200]\n",
      "loss: 1.251840  [ 1088/ 3200]\n",
      "loss: 0.970659  [ 1104/ 3200]\n",
      "loss: 1.218732  [ 1120/ 3200]\n",
      "loss: 0.976864  [ 1136/ 3200]\n",
      "loss: 0.982084  [ 1152/ 3200]\n",
      "loss: 1.031783  [ 1168/ 3200]\n",
      "loss: 1.025649  [ 1184/ 3200]\n",
      "loss: 0.896504  [ 1200/ 3200]\n",
      "loss: 1.037258  [ 1216/ 3200]\n",
      "loss: 1.081236  [ 1232/ 3200]\n",
      "loss: 1.090192  [ 1248/ 3200]\n",
      "loss: 1.057514  [ 1264/ 3200]\n",
      "loss: 1.003425  [ 1280/ 3200]\n",
      "loss: 0.986981  [ 1296/ 3200]\n",
      "loss: 1.011080  [ 1312/ 3200]\n",
      "loss: 1.055906  [ 1328/ 3200]\n",
      "loss: 1.008032  [ 1344/ 3200]\n",
      "loss: 1.174401  [ 1360/ 3200]\n",
      "loss: 1.340813  [ 1376/ 3200]\n",
      "loss: 1.153331  [ 1392/ 3200]\n",
      "loss: 1.363098  [ 1408/ 3200]\n",
      "loss: 1.258115  [ 1424/ 3200]\n",
      "loss: 1.006717  [ 1440/ 3200]\n",
      "loss: 0.799984  [ 1456/ 3200]\n",
      "loss: 1.007024  [ 1472/ 3200]\n",
      "loss: 1.087083  [ 1488/ 3200]\n",
      "loss: 1.007535  [ 1504/ 3200]\n",
      "loss: 1.008980  [ 1520/ 3200]\n",
      "loss: 1.289867  [ 1536/ 3200]\n",
      "loss: 1.078075  [ 1552/ 3200]\n",
      "loss: 0.845406  [ 1568/ 3200]\n",
      "loss: 0.973443  [ 1584/ 3200]\n",
      "loss: 0.787265  [ 1600/ 3200]\n",
      "loss: 1.122769  [ 1616/ 3200]\n",
      "loss: 1.040047  [ 1632/ 3200]\n",
      "loss: 0.827668  [ 1648/ 3200]\n",
      "loss: 0.985413  [ 1664/ 3200]\n",
      "loss: 1.086173  [ 1680/ 3200]\n",
      "loss: 0.892026  [ 1696/ 3200]\n",
      "loss: 1.125681  [ 1712/ 3200]\n",
      "loss: 1.015315  [ 1728/ 3200]\n",
      "loss: 1.161854  [ 1744/ 3200]\n",
      "loss: 1.011922  [ 1760/ 3200]\n",
      "loss: 0.959457  [ 1776/ 3200]\n",
      "loss: 1.123494  [ 1792/ 3200]\n",
      "loss: 1.188818  [ 1808/ 3200]\n",
      "loss: 0.998037  [ 1824/ 3200]\n",
      "loss: 0.917865  [ 1840/ 3200]\n",
      "loss: 1.124404  [ 1856/ 3200]\n",
      "loss: 1.134952  [ 1872/ 3200]\n",
      "loss: 0.880908  [ 1888/ 3200]\n",
      "loss: 1.427658  [ 1904/ 3200]\n",
      "loss: 0.933117  [ 1920/ 3200]\n",
      "loss: 1.049343  [ 1936/ 3200]\n",
      "loss: 1.356023  [ 1952/ 3200]\n",
      "loss: 1.025886  [ 1968/ 3200]\n",
      "loss: 0.982499  [ 1984/ 3200]\n",
      "loss: 1.083143  [ 2000/ 3200]\n",
      "loss: 1.177584  [ 2016/ 3200]\n",
      "loss: 1.136824  [ 2032/ 3200]\n",
      "loss: 0.923657  [ 2048/ 3200]\n",
      "loss: 1.093798  [ 2064/ 3200]\n",
      "loss: 1.230537  [ 2080/ 3200]\n",
      "loss: 0.951047  [ 2096/ 3200]\n",
      "loss: 1.102659  [ 2112/ 3200]\n",
      "loss: 0.802453  [ 2128/ 3200]\n",
      "loss: 0.871408  [ 2144/ 3200]\n",
      "loss: 0.773604  [ 2160/ 3200]\n",
      "loss: 1.418721  [ 2176/ 3200]\n",
      "loss: 0.855379  [ 2192/ 3200]\n",
      "loss: 0.917861  [ 2208/ 3200]\n",
      "loss: 1.106490  [ 2224/ 3200]\n",
      "loss: 1.157496  [ 2240/ 3200]\n",
      "loss: 0.987529  [ 2256/ 3200]\n",
      "loss: 0.991164  [ 2272/ 3200]\n",
      "loss: 1.112766  [ 2288/ 3200]\n",
      "loss: 1.004236  [ 2304/ 3200]\n",
      "loss: 0.924886  [ 2320/ 3200]\n",
      "loss: 1.155313  [ 2336/ 3200]\n",
      "loss: 0.971154  [ 2352/ 3200]\n",
      "loss: 1.181540  [ 2368/ 3200]\n",
      "loss: 1.120229  [ 2384/ 3200]\n",
      "loss: 1.487288  [ 2400/ 3200]\n",
      "loss: 1.011100  [ 2416/ 3200]\n",
      "loss: 1.337714  [ 2432/ 3200]\n",
      "loss: 1.024077  [ 2448/ 3200]\n",
      "loss: 1.298392  [ 2464/ 3200]\n",
      "loss: 1.040251  [ 2480/ 3200]\n",
      "loss: 1.116746  [ 2496/ 3200]\n",
      "loss: 0.984383  [ 2512/ 3200]\n",
      "loss: 0.926725  [ 2528/ 3200]\n",
      "loss: 0.933699  [ 2544/ 3200]\n",
      "loss: 1.025077  [ 2560/ 3200]\n",
      "loss: 1.304009  [ 2576/ 3200]\n",
      "loss: 1.042708  [ 2592/ 3200]\n",
      "loss: 0.912523  [ 2608/ 3200]\n",
      "loss: 0.916726  [ 2624/ 3200]\n",
      "loss: 0.803757  [ 2640/ 3200]\n",
      "loss: 1.026624  [ 2656/ 3200]\n",
      "loss: 0.877534  [ 2672/ 3200]\n",
      "loss: 0.881747  [ 2688/ 3200]\n",
      "loss: 1.150293  [ 2704/ 3200]\n",
      "loss: 0.911721  [ 2720/ 3200]\n",
      "loss: 0.979320  [ 2736/ 3200]\n",
      "loss: 1.025424  [ 2752/ 3200]\n",
      "loss: 0.987015  [ 2768/ 3200]\n",
      "loss: 0.903105  [ 2784/ 3200]\n",
      "loss: 1.073628  [ 2800/ 3200]\n",
      "loss: 1.270224  [ 2816/ 3200]\n",
      "loss: 1.511520  [ 2832/ 3200]\n",
      "loss: 0.947295  [ 2848/ 3200]\n",
      "loss: 1.184142  [ 2864/ 3200]\n",
      "loss: 1.221569  [ 2880/ 3200]\n",
      "loss: 1.076870  [ 2896/ 3200]\n",
      "loss: 1.109341  [ 2912/ 3200]\n",
      "loss: 1.091795  [ 2928/ 3200]\n",
      "loss: 1.052365  [ 2944/ 3200]\n",
      "loss: 1.059809  [ 2960/ 3200]\n",
      "loss: 1.102572  [ 2976/ 3200]\n",
      "loss: 1.185410  [ 2992/ 3200]\n",
      "loss: 0.980079  [ 3008/ 3200]\n",
      "loss: 0.847592  [ 3024/ 3200]\n",
      "loss: 1.284890  [ 3040/ 3200]\n",
      "loss: 0.879829  [ 3056/ 3200]\n",
      "loss: 1.092596  [ 3072/ 3200]\n",
      "loss: 0.854345  [ 3088/ 3200]\n",
      "loss: 1.233080  [ 3104/ 3200]\n",
      "loss: 1.069589  [ 3120/ 3200]\n",
      "loss: 0.874402  [ 3136/ 3200]\n",
      "loss: 1.057578  [ 3152/ 3200]\n",
      "loss: 1.036857  [ 3168/ 3200]\n",
      "loss: 0.886820  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.072748\n",
      "f1 macro averaged score: 0.365374\n",
      "Accuracy               : 42.9%\n",
      "Confusion matrix       :\n",
      "tensor([[117,  30,  51,   2],\n",
      "        [ 24,  14, 148,  14],\n",
      "        [  1,   2, 192,   5],\n",
      "        [  1,   4, 175,  20]], device='cuda:0')\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 1.222173  [    0/ 3200]\n",
      "loss: 1.011236  [   16/ 3200]\n",
      "loss: 1.092033  [   32/ 3200]\n",
      "loss: 1.004036  [   48/ 3200]\n",
      "loss: 1.255354  [   64/ 3200]\n",
      "loss: 1.264404  [   80/ 3200]\n",
      "loss: 0.903734  [   96/ 3200]\n",
      "loss: 0.798018  [  112/ 3200]\n",
      "loss: 1.080893  [  128/ 3200]\n",
      "loss: 0.820155  [  144/ 3200]\n",
      "loss: 0.754187  [  160/ 3200]\n",
      "loss: 0.963705  [  176/ 3200]\n",
      "loss: 0.907427  [  192/ 3200]\n",
      "loss: 0.920452  [  208/ 3200]\n",
      "loss: 1.126075  [  224/ 3200]\n",
      "loss: 1.018453  [  240/ 3200]\n",
      "loss: 1.254040  [  256/ 3200]\n",
      "loss: 0.897950  [  272/ 3200]\n",
      "loss: 1.183376  [  288/ 3200]\n",
      "loss: 1.130117  [  304/ 3200]\n",
      "loss: 1.176013  [  320/ 3200]\n",
      "loss: 1.024312  [  336/ 3200]\n",
      "loss: 1.199004  [  352/ 3200]\n",
      "loss: 1.183112  [  368/ 3200]\n",
      "loss: 1.284392  [  384/ 3200]\n",
      "loss: 1.193785  [  400/ 3200]\n",
      "loss: 0.917773  [  416/ 3200]\n",
      "loss: 0.918813  [  432/ 3200]\n",
      "loss: 1.022748  [  448/ 3200]\n",
      "loss: 0.938896  [  464/ 3200]\n",
      "loss: 1.116665  [  480/ 3200]\n",
      "loss: 1.295232  [  496/ 3200]\n",
      "loss: 0.880254  [  512/ 3200]\n",
      "loss: 0.850853  [  528/ 3200]\n",
      "loss: 1.048693  [  544/ 3200]\n",
      "loss: 0.903738  [  560/ 3200]\n",
      "loss: 0.847853  [  576/ 3200]\n",
      "loss: 0.943054  [  592/ 3200]\n",
      "loss: 1.225628  [  608/ 3200]\n",
      "loss: 0.801801  [  624/ 3200]\n",
      "loss: 1.084635  [  640/ 3200]\n",
      "loss: 1.170756  [  656/ 3200]\n",
      "loss: 0.933801  [  672/ 3200]\n",
      "loss: 0.819715  [  688/ 3200]\n",
      "loss: 0.986687  [  704/ 3200]\n",
      "loss: 1.200812  [  720/ 3200]\n",
      "loss: 0.963557  [  736/ 3200]\n",
      "loss: 0.843599  [  752/ 3200]\n",
      "loss: 0.917322  [  768/ 3200]\n",
      "loss: 0.994478  [  784/ 3200]\n",
      "loss: 1.076157  [  800/ 3200]\n",
      "loss: 1.246465  [  816/ 3200]\n",
      "loss: 0.849090  [  832/ 3200]\n",
      "loss: 1.446083  [  848/ 3200]\n",
      "loss: 0.933038  [  864/ 3200]\n",
      "loss: 0.984754  [  880/ 3200]\n",
      "loss: 0.983849  [  896/ 3200]\n",
      "loss: 0.818134  [  912/ 3200]\n",
      "loss: 1.149426  [  928/ 3200]\n",
      "loss: 0.987100  [  944/ 3200]\n",
      "loss: 0.880465  [  960/ 3200]\n",
      "loss: 1.013605  [  976/ 3200]\n",
      "loss: 1.035150  [  992/ 3200]\n",
      "loss: 0.946466  [ 1008/ 3200]\n",
      "loss: 1.148182  [ 1024/ 3200]\n",
      "loss: 1.116818  [ 1040/ 3200]\n",
      "loss: 0.926807  [ 1056/ 3200]\n",
      "loss: 1.057340  [ 1072/ 3200]\n",
      "loss: 0.993253  [ 1088/ 3200]\n",
      "loss: 1.142077  [ 1104/ 3200]\n",
      "loss: 0.924451  [ 1120/ 3200]\n",
      "loss: 1.152006  [ 1136/ 3200]\n",
      "loss: 0.852653  [ 1152/ 3200]\n",
      "loss: 1.064959  [ 1168/ 3200]\n",
      "loss: 0.909326  [ 1184/ 3200]\n",
      "loss: 1.297501  [ 1200/ 3200]\n",
      "loss: 0.905314  [ 1216/ 3200]\n",
      "loss: 0.893828  [ 1232/ 3200]\n",
      "loss: 0.924680  [ 1248/ 3200]\n",
      "loss: 0.902718  [ 1264/ 3200]\n",
      "loss: 0.798934  [ 1280/ 3200]\n",
      "loss: 1.120810  [ 1296/ 3200]\n",
      "loss: 0.935687  [ 1312/ 3200]\n",
      "loss: 1.537566  [ 1328/ 3200]\n",
      "loss: 1.158519  [ 1344/ 3200]\n",
      "loss: 1.050989  [ 1360/ 3200]\n",
      "loss: 0.791377  [ 1376/ 3200]\n",
      "loss: 1.016254  [ 1392/ 3200]\n",
      "loss: 0.891411  [ 1408/ 3200]\n",
      "loss: 1.152575  [ 1424/ 3200]\n",
      "loss: 0.879068  [ 1440/ 3200]\n",
      "loss: 1.295917  [ 1456/ 3200]\n",
      "loss: 1.171827  [ 1472/ 3200]\n",
      "loss: 1.099291  [ 1488/ 3200]\n",
      "loss: 0.987839  [ 1504/ 3200]\n",
      "loss: 1.200970  [ 1520/ 3200]\n",
      "loss: 0.805623  [ 1536/ 3200]\n",
      "loss: 0.951936  [ 1552/ 3200]\n",
      "loss: 0.839057  [ 1568/ 3200]\n",
      "loss: 0.930073  [ 1584/ 3200]\n",
      "loss: 1.004502  [ 1600/ 3200]\n",
      "loss: 1.128183  [ 1616/ 3200]\n",
      "loss: 0.856138  [ 1632/ 3200]\n",
      "loss: 1.122458  [ 1648/ 3200]\n",
      "loss: 0.945262  [ 1664/ 3200]\n",
      "loss: 0.952267  [ 1680/ 3200]\n",
      "loss: 1.038946  [ 1696/ 3200]\n",
      "loss: 1.151762  [ 1712/ 3200]\n",
      "loss: 0.795969  [ 1728/ 3200]\n",
      "loss: 1.182576  [ 1744/ 3200]\n",
      "loss: 0.892967  [ 1760/ 3200]\n",
      "loss: 0.713001  [ 1776/ 3200]\n",
      "loss: 0.975719  [ 1792/ 3200]\n",
      "loss: 0.961486  [ 1808/ 3200]\n",
      "loss: 1.192237  [ 1824/ 3200]\n",
      "loss: 1.080327  [ 1840/ 3200]\n",
      "loss: 1.102570  [ 1856/ 3200]\n",
      "loss: 1.005190  [ 1872/ 3200]\n",
      "loss: 0.921176  [ 1888/ 3200]\n",
      "loss: 1.247947  [ 1904/ 3200]\n",
      "loss: 1.298883  [ 1920/ 3200]\n",
      "loss: 0.864036  [ 1936/ 3200]\n",
      "loss: 0.882366  [ 1952/ 3200]\n",
      "loss: 0.911457  [ 1968/ 3200]\n",
      "loss: 0.820179  [ 1984/ 3200]\n",
      "loss: 0.771886  [ 2000/ 3200]\n",
      "loss: 1.267031  [ 2016/ 3200]\n",
      "loss: 1.102747  [ 2032/ 3200]\n",
      "loss: 0.802076  [ 2048/ 3200]\n",
      "loss: 1.212187  [ 2064/ 3200]\n",
      "loss: 0.912924  [ 2080/ 3200]\n",
      "loss: 0.943820  [ 2096/ 3200]\n",
      "loss: 1.068554  [ 2112/ 3200]\n",
      "loss: 1.258967  [ 2128/ 3200]\n",
      "loss: 0.979248  [ 2144/ 3200]\n",
      "loss: 0.925694  [ 2160/ 3200]\n",
      "loss: 1.223376  [ 2176/ 3200]\n",
      "loss: 0.913457  [ 2192/ 3200]\n",
      "loss: 0.990693  [ 2208/ 3200]\n",
      "loss: 0.964597  [ 2224/ 3200]\n",
      "loss: 0.856532  [ 2240/ 3200]\n",
      "loss: 0.999537  [ 2256/ 3200]\n",
      "loss: 0.596678  [ 2272/ 3200]\n",
      "loss: 1.218879  [ 2288/ 3200]\n",
      "loss: 1.081126  [ 2304/ 3200]\n",
      "loss: 1.084313  [ 2320/ 3200]\n",
      "loss: 1.171830  [ 2336/ 3200]\n",
      "loss: 0.991229  [ 2352/ 3200]\n",
      "loss: 0.842410  [ 2368/ 3200]\n",
      "loss: 0.791159  [ 2384/ 3200]\n",
      "loss: 1.065780  [ 2400/ 3200]\n",
      "loss: 0.969471  [ 2416/ 3200]\n",
      "loss: 1.346183  [ 2432/ 3200]\n",
      "loss: 1.129576  [ 2448/ 3200]\n",
      "loss: 1.018819  [ 2464/ 3200]\n",
      "loss: 0.961458  [ 2480/ 3200]\n",
      "loss: 1.317308  [ 2496/ 3200]\n",
      "loss: 0.967315  [ 2512/ 3200]\n",
      "loss: 1.002918  [ 2528/ 3200]\n",
      "loss: 0.920464  [ 2544/ 3200]\n",
      "loss: 0.830284  [ 2560/ 3200]\n",
      "loss: 1.384153  [ 2576/ 3200]\n",
      "loss: 1.221448  [ 2592/ 3200]\n",
      "loss: 1.219955  [ 2608/ 3200]\n",
      "loss: 1.165803  [ 2624/ 3200]\n",
      "loss: 0.995192  [ 2640/ 3200]\n",
      "loss: 0.981777  [ 2656/ 3200]\n",
      "loss: 1.151347  [ 2672/ 3200]\n",
      "loss: 1.172547  [ 2688/ 3200]\n",
      "loss: 0.938488  [ 2704/ 3200]\n",
      "loss: 1.362945  [ 2720/ 3200]\n",
      "loss: 1.154500  [ 2736/ 3200]\n",
      "loss: 1.078582  [ 2752/ 3200]\n",
      "loss: 1.029129  [ 2768/ 3200]\n",
      "loss: 1.134299  [ 2784/ 3200]\n",
      "loss: 0.835228  [ 2800/ 3200]\n",
      "loss: 0.846750  [ 2816/ 3200]\n",
      "loss: 0.896411  [ 2832/ 3200]\n",
      "loss: 1.312933  [ 2848/ 3200]\n",
      "loss: 0.914595  [ 2864/ 3200]\n",
      "loss: 1.094997  [ 2880/ 3200]\n",
      "loss: 0.925159  [ 2896/ 3200]\n",
      "loss: 0.836545  [ 2912/ 3200]\n",
      "loss: 0.871228  [ 2928/ 3200]\n",
      "loss: 0.895771  [ 2944/ 3200]\n",
      "loss: 1.049529  [ 2960/ 3200]\n",
      "loss: 0.838264  [ 2976/ 3200]\n",
      "loss: 1.003374  [ 2992/ 3200]\n",
      "loss: 1.107834  [ 3008/ 3200]\n",
      "loss: 1.076004  [ 3024/ 3200]\n",
      "loss: 0.773272  [ 3040/ 3200]\n",
      "loss: 1.011298  [ 3056/ 3200]\n",
      "loss: 0.798339  [ 3072/ 3200]\n",
      "loss: 0.847292  [ 3088/ 3200]\n",
      "loss: 1.075634  [ 3104/ 3200]\n",
      "loss: 1.018916  [ 3120/ 3200]\n",
      "loss: 1.087639  [ 3136/ 3200]\n",
      "loss: 0.733222  [ 3152/ 3200]\n",
      "loss: 0.863880  [ 3168/ 3200]\n",
      "loss: 0.933537  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.062438\n",
      "f1 macro averaged score: 0.540790\n",
      "Accuracy               : 54.6%\n",
      "Confusion matrix       :\n",
      "tensor([[156,  42,   0,   2],\n",
      "        [ 47, 112,  28,  13],\n",
      "        [  7,  63, 110,  20],\n",
      "        [  4,  81,  56,  59]], device='cuda:0')\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 1.108069  [    0/ 3200]\n",
      "loss: 1.043535  [   16/ 3200]\n",
      "loss: 0.969890  [   32/ 3200]\n",
      "loss: 0.875834  [   48/ 3200]\n",
      "loss: 1.156130  [   64/ 3200]\n",
      "loss: 1.000057  [   80/ 3200]\n",
      "loss: 0.852565  [   96/ 3200]\n",
      "loss: 0.952615  [  112/ 3200]\n",
      "loss: 0.799328  [  128/ 3200]\n",
      "loss: 0.944778  [  144/ 3200]\n",
      "loss: 0.999962  [  160/ 3200]\n",
      "loss: 0.869686  [  176/ 3200]\n",
      "loss: 0.944354  [  192/ 3200]\n",
      "loss: 1.238731  [  208/ 3200]\n",
      "loss: 1.018038  [  224/ 3200]\n",
      "loss: 0.910969  [  240/ 3200]\n",
      "loss: 0.994683  [  256/ 3200]\n",
      "loss: 1.014275  [  272/ 3200]\n",
      "loss: 1.360992  [  288/ 3200]\n",
      "loss: 0.917851  [  304/ 3200]\n",
      "loss: 1.176726  [  320/ 3200]\n",
      "loss: 0.974945  [  336/ 3200]\n",
      "loss: 1.093092  [  352/ 3200]\n",
      "loss: 0.992258  [  368/ 3200]\n",
      "loss: 0.974240  [  384/ 3200]\n",
      "loss: 0.886896  [  400/ 3200]\n",
      "loss: 1.004616  [  416/ 3200]\n",
      "loss: 0.996888  [  432/ 3200]\n",
      "loss: 1.191331  [  448/ 3200]\n",
      "loss: 1.078982  [  464/ 3200]\n",
      "loss: 1.006777  [  480/ 3200]\n",
      "loss: 0.716598  [  496/ 3200]\n",
      "loss: 1.178443  [  512/ 3200]\n",
      "loss: 0.920482  [  528/ 3200]\n",
      "loss: 1.060343  [  544/ 3200]\n",
      "loss: 0.821945  [  560/ 3200]\n",
      "loss: 0.974793  [  576/ 3200]\n",
      "loss: 1.000395  [  592/ 3200]\n",
      "loss: 0.891062  [  608/ 3200]\n",
      "loss: 1.035138  [  624/ 3200]\n",
      "loss: 0.987489  [  640/ 3200]\n",
      "loss: 0.789456  [  656/ 3200]\n",
      "loss: 0.837589  [  672/ 3200]\n",
      "loss: 0.962496  [  688/ 3200]\n",
      "loss: 1.124213  [  704/ 3200]\n",
      "loss: 1.099793  [  720/ 3200]\n",
      "loss: 1.255944  [  736/ 3200]\n",
      "loss: 1.278196  [  752/ 3200]\n",
      "loss: 0.969839  [  768/ 3200]\n",
      "loss: 1.123627  [  784/ 3200]\n",
      "loss: 0.957944  [  800/ 3200]\n",
      "loss: 0.950634  [  816/ 3200]\n",
      "loss: 0.853179  [  832/ 3200]\n",
      "loss: 1.116733  [  848/ 3200]\n",
      "loss: 0.748075  [  864/ 3200]\n",
      "loss: 0.824790  [  880/ 3200]\n",
      "loss: 0.825189  [  896/ 3200]\n",
      "loss: 1.095194  [  912/ 3200]\n",
      "loss: 0.802385  [  928/ 3200]\n",
      "loss: 1.316308  [  944/ 3200]\n",
      "loss: 1.030464  [  960/ 3200]\n",
      "loss: 1.093771  [  976/ 3200]\n",
      "loss: 1.531635  [  992/ 3200]\n",
      "loss: 0.902816  [ 1008/ 3200]\n",
      "loss: 1.035143  [ 1024/ 3200]\n",
      "loss: 0.739976  [ 1040/ 3200]\n",
      "loss: 0.829777  [ 1056/ 3200]\n",
      "loss: 0.987294  [ 1072/ 3200]\n",
      "loss: 0.917670  [ 1088/ 3200]\n",
      "loss: 1.047218  [ 1104/ 3200]\n",
      "loss: 0.813156  [ 1120/ 3200]\n",
      "loss: 0.991864  [ 1136/ 3200]\n",
      "loss: 0.779432  [ 1152/ 3200]\n",
      "loss: 0.704091  [ 1168/ 3200]\n",
      "loss: 0.812517  [ 1184/ 3200]\n",
      "loss: 1.208644  [ 1200/ 3200]\n",
      "loss: 0.837095  [ 1216/ 3200]\n",
      "loss: 0.726754  [ 1232/ 3200]\n",
      "loss: 1.071369  [ 1248/ 3200]\n",
      "loss: 1.049943  [ 1264/ 3200]\n",
      "loss: 1.025575  [ 1280/ 3200]\n",
      "loss: 0.961211  [ 1296/ 3200]\n",
      "loss: 1.050182  [ 1312/ 3200]\n",
      "loss: 0.935120  [ 1328/ 3200]\n",
      "loss: 1.143417  [ 1344/ 3200]\n",
      "loss: 1.413505  [ 1360/ 3200]\n",
      "loss: 1.376550  [ 1376/ 3200]\n",
      "loss: 1.084337  [ 1392/ 3200]\n",
      "loss: 1.113513  [ 1408/ 3200]\n",
      "loss: 1.014145  [ 1424/ 3200]\n",
      "loss: 0.982282  [ 1440/ 3200]\n",
      "loss: 1.223203  [ 1456/ 3200]\n",
      "loss: 1.060788  [ 1472/ 3200]\n",
      "loss: 1.217652  [ 1488/ 3200]\n",
      "loss: 0.746673  [ 1504/ 3200]\n",
      "loss: 1.130811  [ 1520/ 3200]\n",
      "loss: 1.068420  [ 1536/ 3200]\n",
      "loss: 1.136632  [ 1552/ 3200]\n",
      "loss: 0.906312  [ 1568/ 3200]\n",
      "loss: 1.099971  [ 1584/ 3200]\n",
      "loss: 1.090285  [ 1600/ 3200]\n",
      "loss: 0.900011  [ 1616/ 3200]\n",
      "loss: 0.916704  [ 1632/ 3200]\n",
      "loss: 0.870632  [ 1648/ 3200]\n",
      "loss: 0.812779  [ 1664/ 3200]\n",
      "loss: 1.199816  [ 1680/ 3200]\n",
      "loss: 0.945172  [ 1696/ 3200]\n",
      "loss: 0.768448  [ 1712/ 3200]\n",
      "loss: 1.073818  [ 1728/ 3200]\n",
      "loss: 0.891056  [ 1744/ 3200]\n",
      "loss: 0.955439  [ 1760/ 3200]\n",
      "loss: 1.122306  [ 1776/ 3200]\n",
      "loss: 0.971851  [ 1792/ 3200]\n",
      "loss: 0.806891  [ 1808/ 3200]\n",
      "loss: 0.789926  [ 1824/ 3200]\n",
      "loss: 0.991167  [ 1840/ 3200]\n",
      "loss: 0.760839  [ 1856/ 3200]\n",
      "loss: 0.636515  [ 1872/ 3200]\n",
      "loss: 0.751776  [ 1888/ 3200]\n",
      "loss: 0.973336  [ 1904/ 3200]\n",
      "loss: 0.911647  [ 1920/ 3200]\n",
      "loss: 0.837148  [ 1936/ 3200]\n",
      "loss: 1.067239  [ 1952/ 3200]\n",
      "loss: 0.750564  [ 1968/ 3200]\n",
      "loss: 1.126152  [ 1984/ 3200]\n",
      "loss: 0.920183  [ 2000/ 3200]\n",
      "loss: 1.149762  [ 2016/ 3200]\n",
      "loss: 0.990104  [ 2032/ 3200]\n",
      "loss: 0.871726  [ 2048/ 3200]\n",
      "loss: 0.901497  [ 2064/ 3200]\n",
      "loss: 1.033818  [ 2080/ 3200]\n",
      "loss: 0.928029  [ 2096/ 3200]\n",
      "loss: 1.039224  [ 2112/ 3200]\n",
      "loss: 0.935986  [ 2128/ 3200]\n",
      "loss: 0.984206  [ 2144/ 3200]\n",
      "loss: 1.046231  [ 2160/ 3200]\n",
      "loss: 1.170422  [ 2176/ 3200]\n",
      "loss: 0.960570  [ 2192/ 3200]\n",
      "loss: 1.066439  [ 2208/ 3200]\n",
      "loss: 0.767523  [ 2224/ 3200]\n",
      "loss: 1.156039  [ 2240/ 3200]\n",
      "loss: 0.908006  [ 2256/ 3200]\n",
      "loss: 1.112719  [ 2272/ 3200]\n",
      "loss: 1.532216  [ 2288/ 3200]\n",
      "loss: 1.129744  [ 2304/ 3200]\n",
      "loss: 0.726533  [ 2320/ 3200]\n",
      "loss: 1.247834  [ 2336/ 3200]\n",
      "loss: 0.768722  [ 2352/ 3200]\n",
      "loss: 0.930508  [ 2368/ 3200]\n",
      "loss: 0.708760  [ 2384/ 3200]\n",
      "loss: 1.077498  [ 2400/ 3200]\n",
      "loss: 0.993978  [ 2416/ 3200]\n",
      "loss: 0.929840  [ 2432/ 3200]\n",
      "loss: 1.229747  [ 2448/ 3200]\n",
      "loss: 1.003172  [ 2464/ 3200]\n",
      "loss: 1.433564  [ 2480/ 3200]\n",
      "loss: 0.923579  [ 2496/ 3200]\n",
      "loss: 0.994778  [ 2512/ 3200]\n",
      "loss: 0.985560  [ 2528/ 3200]\n",
      "loss: 0.832784  [ 2544/ 3200]\n",
      "loss: 0.999545  [ 2560/ 3200]\n",
      "loss: 1.088659  [ 2576/ 3200]\n",
      "loss: 0.986533  [ 2592/ 3200]\n",
      "loss: 1.066660  [ 2608/ 3200]\n",
      "loss: 0.758121  [ 2624/ 3200]\n",
      "loss: 1.107937  [ 2640/ 3200]\n",
      "loss: 1.008244  [ 2656/ 3200]\n",
      "loss: 0.926696  [ 2672/ 3200]\n",
      "loss: 0.898624  [ 2688/ 3200]\n",
      "loss: 0.974913  [ 2704/ 3200]\n",
      "loss: 1.153675  [ 2720/ 3200]\n",
      "loss: 0.849339  [ 2736/ 3200]\n",
      "loss: 1.241244  [ 2752/ 3200]\n",
      "loss: 1.137043  [ 2768/ 3200]\n",
      "loss: 1.100419  [ 2784/ 3200]\n",
      "loss: 0.997909  [ 2800/ 3200]\n",
      "loss: 0.952025  [ 2816/ 3200]\n",
      "loss: 0.750707  [ 2832/ 3200]\n",
      "loss: 1.007425  [ 2848/ 3200]\n",
      "loss: 0.979398  [ 2864/ 3200]\n",
      "loss: 1.363612  [ 2880/ 3200]\n",
      "loss: 0.995846  [ 2896/ 3200]\n",
      "loss: 0.971370  [ 2912/ 3200]\n",
      "loss: 0.969159  [ 2928/ 3200]\n",
      "loss: 1.255381  [ 2944/ 3200]\n",
      "loss: 1.099391  [ 2960/ 3200]\n",
      "loss: 1.055630  [ 2976/ 3200]\n",
      "loss: 0.892030  [ 2992/ 3200]\n",
      "loss: 0.960984  [ 3008/ 3200]\n",
      "loss: 1.080783  [ 3024/ 3200]\n",
      "loss: 0.966506  [ 3040/ 3200]\n",
      "loss: 0.880446  [ 3056/ 3200]\n",
      "loss: 1.140293  [ 3072/ 3200]\n",
      "loss: 1.091027  [ 3088/ 3200]\n",
      "loss: 1.028306  [ 3104/ 3200]\n",
      "loss: 0.767978  [ 3120/ 3200]\n",
      "loss: 1.071554  [ 3136/ 3200]\n",
      "loss: 0.933920  [ 3152/ 3200]\n",
      "loss: 0.890162  [ 3168/ 3200]\n",
      "loss: 0.734358  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.063804\n",
      "f1 macro averaged score: 0.496191\n",
      "Accuracy               : 52.4%\n",
      "Confusion matrix       :\n",
      "tensor([[173,  25,   0,   2],\n",
      "        [ 66,  65,  13,  56],\n",
      "        [ 17,  36,  47, 100],\n",
      "        [ 13,  48,   5, 134]], device='cuda:0')\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 1.021886  [    0/ 3200]\n",
      "loss: 0.819062  [   16/ 3200]\n",
      "loss: 1.053488  [   32/ 3200]\n",
      "loss: 1.358222  [   48/ 3200]\n",
      "loss: 1.107942  [   64/ 3200]\n",
      "loss: 0.929029  [   80/ 3200]\n",
      "loss: 1.158088  [   96/ 3200]\n",
      "loss: 0.849127  [  112/ 3200]\n",
      "loss: 0.995690  [  128/ 3200]\n",
      "loss: 0.949351  [  144/ 3200]\n",
      "loss: 1.183232  [  160/ 3200]\n",
      "loss: 1.054130  [  176/ 3200]\n",
      "loss: 0.864299  [  192/ 3200]\n",
      "loss: 0.760796  [  208/ 3200]\n",
      "loss: 1.011382  [  224/ 3200]\n",
      "loss: 0.833029  [  240/ 3200]\n",
      "loss: 0.902073  [  256/ 3200]\n",
      "loss: 1.203924  [  272/ 3200]\n",
      "loss: 0.946877  [  288/ 3200]\n",
      "loss: 0.911566  [  304/ 3200]\n",
      "loss: 0.971699  [  320/ 3200]\n",
      "loss: 0.687931  [  336/ 3200]\n",
      "loss: 0.965175  [  352/ 3200]\n",
      "loss: 0.887031  [  368/ 3200]\n",
      "loss: 0.827339  [  384/ 3200]\n",
      "loss: 1.158768  [  400/ 3200]\n",
      "loss: 0.949622  [  416/ 3200]\n",
      "loss: 1.137454  [  432/ 3200]\n",
      "loss: 0.687649  [  448/ 3200]\n",
      "loss: 0.751744  [  464/ 3200]\n",
      "loss: 1.251645  [  480/ 3200]\n",
      "loss: 0.781882  [  496/ 3200]\n",
      "loss: 0.780053  [  512/ 3200]\n",
      "loss: 1.049501  [  528/ 3200]\n",
      "loss: 0.836239  [  544/ 3200]\n",
      "loss: 1.283888  [  560/ 3200]\n",
      "loss: 1.146676  [  576/ 3200]\n",
      "loss: 1.122610  [  592/ 3200]\n",
      "loss: 0.916163  [  608/ 3200]\n",
      "loss: 0.922514  [  624/ 3200]\n",
      "loss: 0.822145  [  640/ 3200]\n",
      "loss: 0.736721  [  656/ 3200]\n",
      "loss: 0.992401  [  672/ 3200]\n",
      "loss: 1.037185  [  688/ 3200]\n",
      "loss: 0.769084  [  704/ 3200]\n",
      "loss: 1.052955  [  720/ 3200]\n",
      "loss: 0.864061  [  736/ 3200]\n",
      "loss: 1.287463  [  752/ 3200]\n",
      "loss: 0.900925  [  768/ 3200]\n",
      "loss: 1.079921  [  784/ 3200]\n",
      "loss: 0.966185  [  800/ 3200]\n",
      "loss: 1.137950  [  816/ 3200]\n",
      "loss: 0.976959  [  832/ 3200]\n",
      "loss: 1.020010  [  848/ 3200]\n",
      "loss: 1.060041  [  864/ 3200]\n",
      "loss: 1.017751  [  880/ 3200]\n",
      "loss: 0.741345  [  896/ 3200]\n",
      "loss: 0.896736  [  912/ 3200]\n",
      "loss: 0.941616  [  928/ 3200]\n",
      "loss: 0.895951  [  944/ 3200]\n",
      "loss: 1.028126  [  960/ 3200]\n",
      "loss: 0.839504  [  976/ 3200]\n",
      "loss: 0.879853  [  992/ 3200]\n",
      "loss: 1.089305  [ 1008/ 3200]\n",
      "loss: 0.793249  [ 1024/ 3200]\n",
      "loss: 0.925521  [ 1040/ 3200]\n",
      "loss: 0.920840  [ 1056/ 3200]\n",
      "loss: 1.083119  [ 1072/ 3200]\n",
      "loss: 1.131743  [ 1088/ 3200]\n",
      "loss: 1.054427  [ 1104/ 3200]\n",
      "loss: 0.951473  [ 1120/ 3200]\n",
      "loss: 1.109097  [ 1136/ 3200]\n",
      "loss: 0.895567  [ 1152/ 3200]\n",
      "loss: 1.202314  [ 1168/ 3200]\n",
      "loss: 1.060042  [ 1184/ 3200]\n",
      "loss: 0.844161  [ 1200/ 3200]\n",
      "loss: 1.000361  [ 1216/ 3200]\n",
      "loss: 0.665505  [ 1232/ 3200]\n",
      "loss: 0.978325  [ 1248/ 3200]\n",
      "loss: 0.748753  [ 1264/ 3200]\n",
      "loss: 1.085591  [ 1280/ 3200]\n",
      "loss: 1.044714  [ 1296/ 3200]\n",
      "loss: 0.927789  [ 1312/ 3200]\n",
      "loss: 0.780924  [ 1328/ 3200]\n",
      "loss: 0.953636  [ 1344/ 3200]\n",
      "loss: 1.198312  [ 1360/ 3200]\n",
      "loss: 0.794551  [ 1376/ 3200]\n",
      "loss: 1.148172  [ 1392/ 3200]\n",
      "loss: 1.176733  [ 1408/ 3200]\n",
      "loss: 0.891114  [ 1424/ 3200]\n",
      "loss: 1.020245  [ 1440/ 3200]\n",
      "loss: 1.046440  [ 1456/ 3200]\n",
      "loss: 1.214819  [ 1472/ 3200]\n",
      "loss: 1.172576  [ 1488/ 3200]\n",
      "loss: 1.181329  [ 1504/ 3200]\n",
      "loss: 1.110611  [ 1520/ 3200]\n",
      "loss: 1.173527  [ 1536/ 3200]\n",
      "loss: 0.946233  [ 1552/ 3200]\n",
      "loss: 0.878267  [ 1568/ 3200]\n",
      "loss: 0.744764  [ 1584/ 3200]\n",
      "loss: 0.906292  [ 1600/ 3200]\n",
      "loss: 1.136059  [ 1616/ 3200]\n",
      "loss: 1.108563  [ 1632/ 3200]\n",
      "loss: 0.951741  [ 1648/ 3200]\n",
      "loss: 1.003659  [ 1664/ 3200]\n",
      "loss: 0.985161  [ 1680/ 3200]\n",
      "loss: 1.154075  [ 1696/ 3200]\n",
      "loss: 0.878003  [ 1712/ 3200]\n",
      "loss: 0.941331  [ 1728/ 3200]\n",
      "loss: 1.014594  [ 1744/ 3200]\n",
      "loss: 1.049715  [ 1760/ 3200]\n",
      "loss: 0.910786  [ 1776/ 3200]\n",
      "loss: 0.722060  [ 1792/ 3200]\n",
      "loss: 1.207841  [ 1808/ 3200]\n",
      "loss: 0.904673  [ 1824/ 3200]\n",
      "loss: 1.012271  [ 1840/ 3200]\n",
      "loss: 1.061604  [ 1856/ 3200]\n",
      "loss: 1.060385  [ 1872/ 3200]\n",
      "loss: 0.990197  [ 1888/ 3200]\n",
      "loss: 0.968163  [ 1904/ 3200]\n",
      "loss: 0.748733  [ 1920/ 3200]\n",
      "loss: 0.936822  [ 1936/ 3200]\n",
      "loss: 1.090274  [ 1952/ 3200]\n",
      "loss: 0.967234  [ 1968/ 3200]\n",
      "loss: 0.775476  [ 1984/ 3200]\n",
      "loss: 1.034337  [ 2000/ 3200]\n",
      "loss: 0.656188  [ 2016/ 3200]\n",
      "loss: 0.848863  [ 2032/ 3200]\n",
      "loss: 0.805256  [ 2048/ 3200]\n",
      "loss: 1.152927  [ 2064/ 3200]\n",
      "loss: 1.021305  [ 2080/ 3200]\n",
      "loss: 0.947340  [ 2096/ 3200]\n",
      "loss: 0.936958  [ 2112/ 3200]\n",
      "loss: 1.077857  [ 2128/ 3200]\n",
      "loss: 1.124390  [ 2144/ 3200]\n",
      "loss: 0.929717  [ 2160/ 3200]\n",
      "loss: 1.141963  [ 2176/ 3200]\n",
      "loss: 0.868621  [ 2192/ 3200]\n",
      "loss: 0.814499  [ 2208/ 3200]\n",
      "loss: 1.128988  [ 2224/ 3200]\n",
      "loss: 1.278089  [ 2240/ 3200]\n",
      "loss: 0.856387  [ 2256/ 3200]\n",
      "loss: 0.618670  [ 2272/ 3200]\n",
      "loss: 1.040136  [ 2288/ 3200]\n",
      "loss: 0.864107  [ 2304/ 3200]\n",
      "loss: 0.857438  [ 2320/ 3200]\n",
      "loss: 1.126713  [ 2336/ 3200]\n",
      "loss: 0.887482  [ 2352/ 3200]\n",
      "loss: 1.011161  [ 2368/ 3200]\n",
      "loss: 0.865358  [ 2384/ 3200]\n",
      "loss: 0.993399  [ 2400/ 3200]\n",
      "loss: 0.767605  [ 2416/ 3200]\n",
      "loss: 1.071499  [ 2432/ 3200]\n",
      "loss: 0.839237  [ 2448/ 3200]\n",
      "loss: 0.935354  [ 2464/ 3200]\n",
      "loss: 1.152627  [ 2480/ 3200]\n",
      "loss: 0.699062  [ 2496/ 3200]\n",
      "loss: 1.035061  [ 2512/ 3200]\n",
      "loss: 1.151594  [ 2528/ 3200]\n",
      "loss: 0.817640  [ 2544/ 3200]\n",
      "loss: 0.683735  [ 2560/ 3200]\n",
      "loss: 0.622753  [ 2576/ 3200]\n",
      "loss: 0.886488  [ 2592/ 3200]\n",
      "loss: 1.254769  [ 2608/ 3200]\n",
      "loss: 0.953916  [ 2624/ 3200]\n",
      "loss: 0.872030  [ 2640/ 3200]\n",
      "loss: 1.291044  [ 2656/ 3200]\n",
      "loss: 0.928677  [ 2672/ 3200]\n",
      "loss: 1.190834  [ 2688/ 3200]\n",
      "loss: 0.873676  [ 2704/ 3200]\n",
      "loss: 1.309084  [ 2720/ 3200]\n",
      "loss: 0.997882  [ 2736/ 3200]\n",
      "loss: 0.936425  [ 2752/ 3200]\n",
      "loss: 0.936399  [ 2768/ 3200]\n",
      "loss: 1.103249  [ 2784/ 3200]\n",
      "loss: 1.065837  [ 2800/ 3200]\n",
      "loss: 0.845464  [ 2816/ 3200]\n",
      "loss: 0.958850  [ 2832/ 3200]\n",
      "loss: 0.839424  [ 2848/ 3200]\n",
      "loss: 0.935565  [ 2864/ 3200]\n",
      "loss: 0.866804  [ 2880/ 3200]\n",
      "loss: 0.739343  [ 2896/ 3200]\n",
      "loss: 1.322945  [ 2912/ 3200]\n",
      "loss: 0.879116  [ 2928/ 3200]\n",
      "loss: 1.021741  [ 2944/ 3200]\n",
      "loss: 0.910002  [ 2960/ 3200]\n",
      "loss: 0.679711  [ 2976/ 3200]\n",
      "loss: 0.806616  [ 2992/ 3200]\n",
      "loss: 1.075695  [ 3008/ 3200]\n",
      "loss: 0.650977  [ 3024/ 3200]\n",
      "loss: 0.753314  [ 3040/ 3200]\n",
      "loss: 1.175819  [ 3056/ 3200]\n",
      "loss: 0.987340  [ 3072/ 3200]\n",
      "loss: 0.777777  [ 3088/ 3200]\n",
      "loss: 0.881949  [ 3104/ 3200]\n",
      "loss: 0.889698  [ 3120/ 3200]\n",
      "loss: 1.193371  [ 3136/ 3200]\n",
      "loss: 0.957589  [ 3152/ 3200]\n",
      "loss: 0.886204  [ 3168/ 3200]\n",
      "loss: 0.898669  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.063217\n",
      "f1 macro averaged score: 0.510034\n",
      "Accuracy               : 53.8%\n",
      "Confusion matrix       :\n",
      "tensor([[160,  34,   0,   6],\n",
      "        [ 48,  67,  12,  73],\n",
      "        [  4,  33,  43, 120],\n",
      "        [  4,  33,   3, 160]], device='cuda:0')\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 1.047738  [    0/ 3200]\n",
      "loss: 0.780277  [   16/ 3200]\n",
      "loss: 0.802974  [   32/ 3200]\n",
      "loss: 0.866296  [   48/ 3200]\n",
      "loss: 0.876152  [   64/ 3200]\n",
      "loss: 0.787185  [   80/ 3200]\n",
      "loss: 1.271079  [   96/ 3200]\n",
      "loss: 0.945972  [  112/ 3200]\n",
      "loss: 1.004989  [  128/ 3200]\n",
      "loss: 0.640662  [  144/ 3200]\n",
      "loss: 0.673217  [  160/ 3200]\n",
      "loss: 0.907215  [  176/ 3200]\n",
      "loss: 0.986782  [  192/ 3200]\n",
      "loss: 0.921022  [  208/ 3200]\n",
      "loss: 0.713135  [  224/ 3200]\n",
      "loss: 0.950062  [  240/ 3200]\n",
      "loss: 0.919739  [  256/ 3200]\n",
      "loss: 0.782768  [  272/ 3200]\n",
      "loss: 0.864006  [  288/ 3200]\n",
      "loss: 0.940000  [  304/ 3200]\n",
      "loss: 0.803332  [  320/ 3200]\n",
      "loss: 0.859863  [  336/ 3200]\n",
      "loss: 1.149746  [  352/ 3200]\n",
      "loss: 0.960553  [  368/ 3200]\n",
      "loss: 0.960587  [  384/ 3200]\n",
      "loss: 1.035025  [  400/ 3200]\n",
      "loss: 0.953152  [  416/ 3200]\n",
      "loss: 0.854695  [  432/ 3200]\n",
      "loss: 0.751432  [  448/ 3200]\n",
      "loss: 0.839516  [  464/ 3200]\n",
      "loss: 0.962810  [  480/ 3200]\n",
      "loss: 0.940149  [  496/ 3200]\n",
      "loss: 0.539617  [  512/ 3200]\n",
      "loss: 0.959542  [  528/ 3200]\n",
      "loss: 0.987632  [  544/ 3200]\n",
      "loss: 0.940424  [  560/ 3200]\n",
      "loss: 0.698727  [  576/ 3200]\n",
      "loss: 0.794490  [  592/ 3200]\n",
      "loss: 0.977403  [  608/ 3200]\n",
      "loss: 0.940015  [  624/ 3200]\n",
      "loss: 1.194525  [  640/ 3200]\n",
      "loss: 0.899242  [  656/ 3200]\n",
      "loss: 0.587385  [  672/ 3200]\n",
      "loss: 1.052231  [  688/ 3200]\n",
      "loss: 0.967763  [  704/ 3200]\n",
      "loss: 0.915174  [  720/ 3200]\n",
      "loss: 1.262402  [  736/ 3200]\n",
      "loss: 0.916442  [  752/ 3200]\n",
      "loss: 1.397145  [  768/ 3200]\n",
      "loss: 0.904547  [  784/ 3200]\n",
      "loss: 0.872661  [  800/ 3200]\n",
      "loss: 0.865696  [  816/ 3200]\n",
      "loss: 1.319594  [  832/ 3200]\n",
      "loss: 1.161206  [  848/ 3200]\n",
      "loss: 0.825994  [  864/ 3200]\n",
      "loss: 1.024252  [  880/ 3200]\n",
      "loss: 1.154794  [  896/ 3200]\n",
      "loss: 1.216676  [  912/ 3200]\n",
      "loss: 0.735855  [  928/ 3200]\n",
      "loss: 0.624722  [  944/ 3200]\n",
      "loss: 0.546126  [  960/ 3200]\n",
      "loss: 0.742608  [  976/ 3200]\n",
      "loss: 0.771724  [  992/ 3200]\n",
      "loss: 1.328806  [ 1008/ 3200]\n",
      "loss: 1.132553  [ 1024/ 3200]\n",
      "loss: 1.057932  [ 1040/ 3200]\n",
      "loss: 1.151986  [ 1056/ 3200]\n",
      "loss: 0.934313  [ 1072/ 3200]\n",
      "loss: 0.936487  [ 1088/ 3200]\n",
      "loss: 1.045064  [ 1104/ 3200]\n",
      "loss: 0.801231  [ 1120/ 3200]\n",
      "loss: 1.259950  [ 1136/ 3200]\n",
      "loss: 1.069518  [ 1152/ 3200]\n",
      "loss: 0.844514  [ 1168/ 3200]\n",
      "loss: 0.533694  [ 1184/ 3200]\n",
      "loss: 0.956765  [ 1200/ 3200]\n",
      "loss: 0.825033  [ 1216/ 3200]\n",
      "loss: 0.841917  [ 1232/ 3200]\n",
      "loss: 0.874996  [ 1248/ 3200]\n",
      "loss: 0.820365  [ 1264/ 3200]\n",
      "loss: 0.882303  [ 1280/ 3200]\n",
      "loss: 1.010816  [ 1296/ 3200]\n",
      "loss: 0.940974  [ 1312/ 3200]\n",
      "loss: 0.746216  [ 1328/ 3200]\n",
      "loss: 1.102844  [ 1344/ 3200]\n",
      "loss: 0.809146  [ 1360/ 3200]\n",
      "loss: 0.884453  [ 1376/ 3200]\n",
      "loss: 0.926176  [ 1392/ 3200]\n",
      "loss: 1.084582  [ 1408/ 3200]\n",
      "loss: 1.241843  [ 1424/ 3200]\n",
      "loss: 0.966671  [ 1440/ 3200]\n",
      "loss: 1.013596  [ 1456/ 3200]\n",
      "loss: 0.832105  [ 1472/ 3200]\n",
      "loss: 0.733293  [ 1488/ 3200]\n",
      "loss: 1.044234  [ 1504/ 3200]\n",
      "loss: 1.003649  [ 1520/ 3200]\n",
      "loss: 1.381177  [ 1536/ 3200]\n",
      "loss: 1.058590  [ 1552/ 3200]\n",
      "loss: 0.955429  [ 1568/ 3200]\n",
      "loss: 0.871689  [ 1584/ 3200]\n",
      "loss: 0.860316  [ 1600/ 3200]\n",
      "loss: 0.894234  [ 1616/ 3200]\n",
      "loss: 0.881231  [ 1632/ 3200]\n",
      "loss: 0.866822  [ 1648/ 3200]\n",
      "loss: 1.023356  [ 1664/ 3200]\n",
      "loss: 0.914967  [ 1680/ 3200]\n",
      "loss: 0.971596  [ 1696/ 3200]\n",
      "loss: 1.082056  [ 1712/ 3200]\n",
      "loss: 0.857960  [ 1728/ 3200]\n",
      "loss: 0.797564  [ 1744/ 3200]\n",
      "loss: 1.340314  [ 1760/ 3200]\n",
      "loss: 1.118678  [ 1776/ 3200]\n",
      "loss: 1.095919  [ 1792/ 3200]\n",
      "loss: 1.107099  [ 1808/ 3200]\n",
      "loss: 0.798417  [ 1824/ 3200]\n",
      "loss: 0.783247  [ 1840/ 3200]\n",
      "loss: 0.878886  [ 1856/ 3200]\n",
      "loss: 0.860776  [ 1872/ 3200]\n",
      "loss: 0.867216  [ 1888/ 3200]\n",
      "loss: 0.681647  [ 1904/ 3200]\n",
      "loss: 0.965343  [ 1920/ 3200]\n",
      "loss: 0.991888  [ 1936/ 3200]\n",
      "loss: 0.675784  [ 1952/ 3200]\n",
      "loss: 0.961941  [ 1968/ 3200]\n",
      "loss: 1.077503  [ 1984/ 3200]\n",
      "loss: 0.898730  [ 2000/ 3200]\n",
      "loss: 0.874329  [ 2016/ 3200]\n",
      "loss: 0.816169  [ 2032/ 3200]\n",
      "loss: 0.957026  [ 2048/ 3200]\n",
      "loss: 1.076277  [ 2064/ 3200]\n",
      "loss: 0.827047  [ 2080/ 3200]\n",
      "loss: 0.971336  [ 2096/ 3200]\n",
      "loss: 0.942019  [ 2112/ 3200]\n",
      "loss: 0.818806  [ 2128/ 3200]\n",
      "loss: 1.269160  [ 2144/ 3200]\n",
      "loss: 0.807949  [ 2160/ 3200]\n",
      "loss: 1.111861  [ 2176/ 3200]\n",
      "loss: 0.775422  [ 2192/ 3200]\n",
      "loss: 0.921357  [ 2208/ 3200]\n",
      "loss: 0.761735  [ 2224/ 3200]\n",
      "loss: 1.010804  [ 2240/ 3200]\n",
      "loss: 0.832887  [ 2256/ 3200]\n",
      "loss: 1.398506  [ 2272/ 3200]\n",
      "loss: 0.862613  [ 2288/ 3200]\n",
      "loss: 1.025980  [ 2304/ 3200]\n",
      "loss: 0.714003  [ 2320/ 3200]\n",
      "loss: 1.150930  [ 2336/ 3200]\n",
      "loss: 0.952652  [ 2352/ 3200]\n",
      "loss: 1.059189  [ 2368/ 3200]\n",
      "loss: 0.809683  [ 2384/ 3200]\n",
      "loss: 0.955402  [ 2400/ 3200]\n",
      "loss: 0.735880  [ 2416/ 3200]\n",
      "loss: 0.899375  [ 2432/ 3200]\n",
      "loss: 1.101061  [ 2448/ 3200]\n",
      "loss: 0.784671  [ 2464/ 3200]\n",
      "loss: 1.124407  [ 2480/ 3200]\n",
      "loss: 1.073987  [ 2496/ 3200]\n",
      "loss: 0.860012  [ 2512/ 3200]\n",
      "loss: 0.729551  [ 2528/ 3200]\n",
      "loss: 1.150808  [ 2544/ 3200]\n",
      "loss: 0.816074  [ 2560/ 3200]\n",
      "loss: 0.823007  [ 2576/ 3200]\n",
      "loss: 0.965847  [ 2592/ 3200]\n",
      "loss: 1.150394  [ 2608/ 3200]\n",
      "loss: 0.815301  [ 2624/ 3200]\n",
      "loss: 0.884246  [ 2640/ 3200]\n",
      "loss: 0.968902  [ 2656/ 3200]\n",
      "loss: 0.861160  [ 2672/ 3200]\n",
      "loss: 0.954473  [ 2688/ 3200]\n",
      "loss: 1.012763  [ 2704/ 3200]\n",
      "loss: 1.023532  [ 2720/ 3200]\n",
      "loss: 1.237883  [ 2736/ 3200]\n",
      "loss: 0.964743  [ 2752/ 3200]\n",
      "loss: 0.913312  [ 2768/ 3200]\n",
      "loss: 0.975366  [ 2784/ 3200]\n",
      "loss: 0.796670  [ 2800/ 3200]\n",
      "loss: 1.096211  [ 2816/ 3200]\n",
      "loss: 1.308849  [ 2832/ 3200]\n",
      "loss: 0.596684  [ 2848/ 3200]\n",
      "loss: 0.881137  [ 2864/ 3200]\n",
      "loss: 1.016542  [ 2880/ 3200]\n",
      "loss: 1.006096  [ 2896/ 3200]\n",
      "loss: 1.056553  [ 2912/ 3200]\n",
      "loss: 0.708594  [ 2928/ 3200]\n",
      "loss: 0.905244  [ 2944/ 3200]\n",
      "loss: 0.736806  [ 2960/ 3200]\n",
      "loss: 0.945028  [ 2976/ 3200]\n",
      "loss: 1.365241  [ 2992/ 3200]\n",
      "loss: 0.950684  [ 3008/ 3200]\n",
      "loss: 1.082230  [ 3024/ 3200]\n",
      "loss: 0.837685  [ 3040/ 3200]\n",
      "loss: 0.712490  [ 3056/ 3200]\n",
      "loss: 0.818414  [ 3072/ 3200]\n",
      "loss: 1.049868  [ 3088/ 3200]\n",
      "loss: 1.092314  [ 3104/ 3200]\n",
      "loss: 0.811441  [ 3120/ 3200]\n",
      "loss: 0.786489  [ 3136/ 3200]\n",
      "loss: 1.141597  [ 3152/ 3200]\n",
      "loss: 1.114264  [ 3168/ 3200]\n",
      "loss: 0.996177  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.068651\n",
      "f1 macro averaged score: 0.507217\n",
      "Accuracy               : 51.6%\n",
      "Confusion matrix       :\n",
      "tensor([[102,  67,  19,  12],\n",
      "        [ 14,  43,  75,  68],\n",
      "        [  0,   7, 149,  44],\n",
      "        [  0,  19,  62, 119]], device='cuda:0')\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 1.159357  [    0/ 3200]\n",
      "loss: 0.942649  [   16/ 3200]\n",
      "loss: 0.855757  [   32/ 3200]\n",
      "loss: 0.743154  [   48/ 3200]\n",
      "loss: 1.023233  [   64/ 3200]\n",
      "loss: 0.686837  [   80/ 3200]\n",
      "loss: 1.076723  [   96/ 3200]\n",
      "loss: 0.956674  [  112/ 3200]\n",
      "loss: 0.847024  [  128/ 3200]\n",
      "loss: 0.722890  [  144/ 3200]\n",
      "loss: 0.930124  [  160/ 3200]\n",
      "loss: 0.940386  [  176/ 3200]\n",
      "loss: 0.784522  [  192/ 3200]\n",
      "loss: 0.939895  [  208/ 3200]\n",
      "loss: 0.923910  [  224/ 3200]\n",
      "loss: 0.882448  [  240/ 3200]\n",
      "loss: 0.981037  [  256/ 3200]\n",
      "loss: 1.016605  [  272/ 3200]\n",
      "loss: 1.035667  [  288/ 3200]\n",
      "loss: 0.755323  [  304/ 3200]\n",
      "loss: 0.876790  [  320/ 3200]\n",
      "loss: 1.033417  [  336/ 3200]\n",
      "loss: 0.787506  [  352/ 3200]\n",
      "loss: 1.397981  [  368/ 3200]\n",
      "loss: 0.847573  [  384/ 3200]\n",
      "loss: 0.663982  [  400/ 3200]\n",
      "loss: 0.835199  [  416/ 3200]\n",
      "loss: 0.971820  [  432/ 3200]\n",
      "loss: 0.832561  [  448/ 3200]\n",
      "loss: 0.603149  [  464/ 3200]\n",
      "loss: 0.912106  [  480/ 3200]\n",
      "loss: 0.958164  [  496/ 3200]\n",
      "loss: 1.243920  [  512/ 3200]\n",
      "loss: 1.096385  [  528/ 3200]\n",
      "loss: 0.716354  [  544/ 3200]\n",
      "loss: 1.367082  [  560/ 3200]\n",
      "loss: 1.100560  [  576/ 3200]\n",
      "loss: 0.815758  [  592/ 3200]\n",
      "loss: 0.899967  [  608/ 3200]\n",
      "loss: 0.812310  [  624/ 3200]\n",
      "loss: 1.216886  [  640/ 3200]\n",
      "loss: 0.812103  [  656/ 3200]\n",
      "loss: 1.008571  [  672/ 3200]\n",
      "loss: 0.975136  [  688/ 3200]\n",
      "loss: 0.867580  [  704/ 3200]\n",
      "loss: 0.968560  [  720/ 3200]\n",
      "loss: 1.069663  [  736/ 3200]\n",
      "loss: 1.247213  [  752/ 3200]\n",
      "loss: 0.835240  [  768/ 3200]\n",
      "loss: 1.073733  [  784/ 3200]\n",
      "loss: 1.071537  [  800/ 3200]\n",
      "loss: 0.743584  [  816/ 3200]\n",
      "loss: 1.499247  [  832/ 3200]\n",
      "loss: 0.826417  [  848/ 3200]\n",
      "loss: 0.904936  [  864/ 3200]\n",
      "loss: 0.937671  [  880/ 3200]\n",
      "loss: 0.787034  [  896/ 3200]\n",
      "loss: 1.048495  [  912/ 3200]\n",
      "loss: 0.914381  [  928/ 3200]\n",
      "loss: 0.988160  [  944/ 3200]\n",
      "loss: 0.779851  [  960/ 3200]\n",
      "loss: 0.671185  [  976/ 3200]\n",
      "loss: 0.763420  [  992/ 3200]\n",
      "loss: 0.932084  [ 1008/ 3200]\n",
      "loss: 0.693956  [ 1024/ 3200]\n",
      "loss: 0.650417  [ 1040/ 3200]\n",
      "loss: 1.003012  [ 1056/ 3200]\n",
      "loss: 0.848525  [ 1072/ 3200]\n",
      "loss: 0.678714  [ 1088/ 3200]\n",
      "loss: 0.602597  [ 1104/ 3200]\n",
      "loss: 1.199927  [ 1120/ 3200]\n",
      "loss: 0.919771  [ 1136/ 3200]\n",
      "loss: 1.002535  [ 1152/ 3200]\n",
      "loss: 0.944480  [ 1168/ 3200]\n",
      "loss: 1.078393  [ 1184/ 3200]\n",
      "loss: 1.005772  [ 1200/ 3200]\n",
      "loss: 0.766360  [ 1216/ 3200]\n",
      "loss: 0.771250  [ 1232/ 3200]\n",
      "loss: 1.048098  [ 1248/ 3200]\n",
      "loss: 0.794849  [ 1264/ 3200]\n",
      "loss: 0.845163  [ 1280/ 3200]\n",
      "loss: 0.740337  [ 1296/ 3200]\n",
      "loss: 0.838099  [ 1312/ 3200]\n",
      "loss: 1.081075  [ 1328/ 3200]\n",
      "loss: 0.738345  [ 1344/ 3200]\n",
      "loss: 0.802862  [ 1360/ 3200]\n",
      "loss: 0.956220  [ 1376/ 3200]\n",
      "loss: 1.201927  [ 1392/ 3200]\n",
      "loss: 0.968255  [ 1408/ 3200]\n",
      "loss: 0.843753  [ 1424/ 3200]\n",
      "loss: 1.041407  [ 1440/ 3200]\n",
      "loss: 0.900413  [ 1456/ 3200]\n",
      "loss: 0.818707  [ 1472/ 3200]\n",
      "loss: 0.822944  [ 1488/ 3200]\n",
      "loss: 0.906967  [ 1504/ 3200]\n",
      "loss: 0.923713  [ 1520/ 3200]\n",
      "loss: 1.055630  [ 1536/ 3200]\n",
      "loss: 0.618793  [ 1552/ 3200]\n",
      "loss: 0.684740  [ 1568/ 3200]\n",
      "loss: 0.805595  [ 1584/ 3200]\n",
      "loss: 1.035192  [ 1600/ 3200]\n",
      "loss: 0.960859  [ 1616/ 3200]\n",
      "loss: 0.636204  [ 1632/ 3200]\n",
      "loss: 0.837008  [ 1648/ 3200]\n",
      "loss: 0.994823  [ 1664/ 3200]\n",
      "loss: 0.812530  [ 1680/ 3200]\n",
      "loss: 1.346398  [ 1696/ 3200]\n",
      "loss: 0.636557  [ 1712/ 3200]\n",
      "loss: 0.866171  [ 1728/ 3200]\n",
      "loss: 1.017985  [ 1744/ 3200]\n",
      "loss: 0.873447  [ 1760/ 3200]\n",
      "loss: 0.838660  [ 1776/ 3200]\n",
      "loss: 0.750039  [ 1792/ 3200]\n",
      "loss: 0.902684  [ 1808/ 3200]\n",
      "loss: 0.666543  [ 1824/ 3200]\n",
      "loss: 0.840720  [ 1840/ 3200]\n",
      "loss: 0.773733  [ 1856/ 3200]\n",
      "loss: 1.083071  [ 1872/ 3200]\n",
      "loss: 0.828194  [ 1888/ 3200]\n",
      "loss: 0.843948  [ 1904/ 3200]\n",
      "loss: 1.138697  [ 1920/ 3200]\n",
      "loss: 0.981260  [ 1936/ 3200]\n",
      "loss: 0.962578  [ 1952/ 3200]\n",
      "loss: 0.818515  [ 1968/ 3200]\n",
      "loss: 0.971034  [ 1984/ 3200]\n",
      "loss: 1.181639  [ 2000/ 3200]\n",
      "loss: 1.138080  [ 2016/ 3200]\n",
      "loss: 1.049801  [ 2032/ 3200]\n",
      "loss: 0.785139  [ 2048/ 3200]\n",
      "loss: 1.123630  [ 2064/ 3200]\n",
      "loss: 0.737887  [ 2080/ 3200]\n",
      "loss: 0.854164  [ 2096/ 3200]\n",
      "loss: 0.675747  [ 2112/ 3200]\n",
      "loss: 1.124708  [ 2128/ 3200]\n",
      "loss: 1.147492  [ 2144/ 3200]\n",
      "loss: 0.903631  [ 2160/ 3200]\n",
      "loss: 1.000050  [ 2176/ 3200]\n",
      "loss: 1.218266  [ 2192/ 3200]\n",
      "loss: 1.246285  [ 2208/ 3200]\n",
      "loss: 1.303285  [ 2224/ 3200]\n",
      "loss: 0.945215  [ 2240/ 3200]\n",
      "loss: 0.811968  [ 2256/ 3200]\n",
      "loss: 0.879331  [ 2272/ 3200]\n",
      "loss: 0.972762  [ 2288/ 3200]\n",
      "loss: 0.799204  [ 2304/ 3200]\n",
      "loss: 0.713664  [ 2320/ 3200]\n",
      "loss: 1.052674  [ 2336/ 3200]\n",
      "loss: 0.570394  [ 2352/ 3200]\n",
      "loss: 0.530335  [ 2368/ 3200]\n",
      "loss: 0.890517  [ 2384/ 3200]\n",
      "loss: 1.052800  [ 2400/ 3200]\n",
      "loss: 0.942115  [ 2416/ 3200]\n",
      "loss: 0.944644  [ 2432/ 3200]\n",
      "loss: 0.944189  [ 2448/ 3200]\n",
      "loss: 0.742918  [ 2464/ 3200]\n",
      "loss: 1.035121  [ 2480/ 3200]\n",
      "loss: 0.707460  [ 2496/ 3200]\n",
      "loss: 0.902397  [ 2512/ 3200]\n",
      "loss: 0.667566  [ 2528/ 3200]\n",
      "loss: 0.812769  [ 2544/ 3200]\n",
      "loss: 0.914670  [ 2560/ 3200]\n",
      "loss: 0.774622  [ 2576/ 3200]\n",
      "loss: 0.717381  [ 2592/ 3200]\n",
      "loss: 0.942012  [ 2608/ 3200]\n",
      "loss: 1.059207  [ 2624/ 3200]\n",
      "loss: 0.795282  [ 2640/ 3200]\n",
      "loss: 1.118419  [ 2656/ 3200]\n",
      "loss: 0.932798  [ 2672/ 3200]\n",
      "loss: 0.798445  [ 2688/ 3200]\n",
      "loss: 0.670046  [ 2704/ 3200]\n",
      "loss: 1.015101  [ 2720/ 3200]\n",
      "loss: 1.016162  [ 2736/ 3200]\n",
      "loss: 0.842497  [ 2752/ 3200]\n",
      "loss: 1.018824  [ 2768/ 3200]\n",
      "loss: 0.834974  [ 2784/ 3200]\n",
      "loss: 0.696126  [ 2800/ 3200]\n",
      "loss: 0.880512  [ 2816/ 3200]\n",
      "loss: 1.030726  [ 2832/ 3200]\n",
      "loss: 0.754380  [ 2848/ 3200]\n",
      "loss: 0.795353  [ 2864/ 3200]\n",
      "loss: 0.702567  [ 2880/ 3200]\n",
      "loss: 0.876725  [ 2896/ 3200]\n",
      "loss: 1.083364  [ 2912/ 3200]\n",
      "loss: 0.886690  [ 2928/ 3200]\n",
      "loss: 0.789754  [ 2944/ 3200]\n",
      "loss: 0.600358  [ 2960/ 3200]\n",
      "loss: 0.933392  [ 2976/ 3200]\n",
      "loss: 1.067618  [ 2992/ 3200]\n",
      "loss: 1.491538  [ 3008/ 3200]\n",
      "loss: 1.153259  [ 3024/ 3200]\n",
      "loss: 0.924258  [ 3040/ 3200]\n",
      "loss: 0.623693  [ 3056/ 3200]\n",
      "loss: 0.929973  [ 3072/ 3200]\n",
      "loss: 1.108118  [ 3088/ 3200]\n",
      "loss: 1.057513  [ 3104/ 3200]\n",
      "loss: 0.735093  [ 3120/ 3200]\n",
      "loss: 0.871258  [ 3136/ 3200]\n",
      "loss: 1.155408  [ 3152/ 3200]\n",
      "loss: 1.084249  [ 3168/ 3200]\n",
      "loss: 0.796605  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.058930\n",
      "f1 macro averaged score: 0.574200\n",
      "Accuracy               : 57.5%\n",
      "Confusion matrix       :\n",
      "tensor([[155,  40,   3,   2],\n",
      "        [ 38,  81,  44,  37],\n",
      "        [  2,  28, 128,  42],\n",
      "        [  3,  54,  47,  96]], device='cuda:0')\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 0.765950  [    0/ 3200]\n",
      "loss: 1.044730  [   16/ 3200]\n",
      "loss: 1.128766  [   32/ 3200]\n",
      "loss: 1.113684  [   48/ 3200]\n",
      "loss: 1.012060  [   64/ 3200]\n",
      "loss: 1.087520  [   80/ 3200]\n",
      "loss: 1.107664  [   96/ 3200]\n",
      "loss: 0.794414  [  112/ 3200]\n",
      "loss: 0.784891  [  128/ 3200]\n",
      "loss: 0.859107  [  144/ 3200]\n",
      "loss: 0.806912  [  160/ 3200]\n",
      "loss: 0.785206  [  176/ 3200]\n",
      "loss: 0.813220  [  192/ 3200]\n",
      "loss: 0.710235  [  208/ 3200]\n",
      "loss: 0.949372  [  224/ 3200]\n",
      "loss: 0.763266  [  240/ 3200]\n",
      "loss: 0.571145  [  256/ 3200]\n",
      "loss: 0.917305  [  272/ 3200]\n",
      "loss: 0.869116  [  288/ 3200]\n",
      "loss: 0.895147  [  304/ 3200]\n",
      "loss: 1.008376  [  320/ 3200]\n",
      "loss: 1.035696  [  336/ 3200]\n",
      "loss: 0.870496  [  352/ 3200]\n",
      "loss: 1.033932  [  368/ 3200]\n",
      "loss: 0.730713  [  384/ 3200]\n",
      "loss: 1.005242  [  400/ 3200]\n",
      "loss: 0.857984  [  416/ 3200]\n",
      "loss: 1.036239  [  432/ 3200]\n",
      "loss: 0.796770  [  448/ 3200]\n",
      "loss: 0.831754  [  464/ 3200]\n",
      "loss: 0.862128  [  480/ 3200]\n",
      "loss: 0.612249  [  496/ 3200]\n",
      "loss: 1.103649  [  512/ 3200]\n",
      "loss: 0.867800  [  528/ 3200]\n",
      "loss: 0.905857  [  544/ 3200]\n",
      "loss: 0.554660  [  560/ 3200]\n",
      "loss: 0.700919  [  576/ 3200]\n",
      "loss: 1.100396  [  592/ 3200]\n",
      "loss: 1.046434  [  608/ 3200]\n",
      "loss: 0.902554  [  624/ 3200]\n",
      "loss: 1.114425  [  640/ 3200]\n",
      "loss: 0.839742  [  656/ 3200]\n",
      "loss: 0.971803  [  672/ 3200]\n",
      "loss: 0.851691  [  688/ 3200]\n",
      "loss: 0.939849  [  704/ 3200]\n",
      "loss: 1.074151  [  720/ 3200]\n",
      "loss: 1.095707  [  736/ 3200]\n",
      "loss: 1.236518  [  752/ 3200]\n",
      "loss: 1.075157  [  768/ 3200]\n",
      "loss: 0.975131  [  784/ 3200]\n",
      "loss: 0.733292  [  800/ 3200]\n",
      "loss: 0.867119  [  816/ 3200]\n",
      "loss: 0.853626  [  832/ 3200]\n",
      "loss: 0.571565  [  848/ 3200]\n",
      "loss: 0.866868  [  864/ 3200]\n",
      "loss: 1.111651  [  880/ 3200]\n",
      "loss: 1.036869  [  896/ 3200]\n",
      "loss: 1.259700  [  912/ 3200]\n",
      "loss: 0.968826  [  928/ 3200]\n",
      "loss: 0.947904  [  944/ 3200]\n",
      "loss: 1.078130  [  960/ 3200]\n",
      "loss: 1.005254  [  976/ 3200]\n",
      "loss: 0.849815  [  992/ 3200]\n",
      "loss: 1.056375  [ 1008/ 3200]\n",
      "loss: 0.828880  [ 1024/ 3200]\n",
      "loss: 0.671593  [ 1040/ 3200]\n",
      "loss: 0.851332  [ 1056/ 3200]\n",
      "loss: 0.839715  [ 1072/ 3200]\n",
      "loss: 0.938995  [ 1088/ 3200]\n",
      "loss: 0.903246  [ 1104/ 3200]\n",
      "loss: 0.810570  [ 1120/ 3200]\n",
      "loss: 1.058603  [ 1136/ 3200]\n",
      "loss: 0.855765  [ 1152/ 3200]\n",
      "loss: 0.907083  [ 1168/ 3200]\n",
      "loss: 1.011296  [ 1184/ 3200]\n",
      "loss: 0.745545  [ 1200/ 3200]\n",
      "loss: 1.383008  [ 1216/ 3200]\n",
      "loss: 0.830975  [ 1232/ 3200]\n",
      "loss: 1.056885  [ 1248/ 3200]\n",
      "loss: 1.013418  [ 1264/ 3200]\n",
      "loss: 0.668799  [ 1280/ 3200]\n",
      "loss: 0.683078  [ 1296/ 3200]\n",
      "loss: 0.945533  [ 1312/ 3200]\n",
      "loss: 0.784943  [ 1328/ 3200]\n",
      "loss: 1.214723  [ 1344/ 3200]\n",
      "loss: 0.595452  [ 1360/ 3200]\n",
      "loss: 0.894384  [ 1376/ 3200]\n",
      "loss: 0.556779  [ 1392/ 3200]\n",
      "loss: 1.082832  [ 1408/ 3200]\n",
      "loss: 0.938761  [ 1424/ 3200]\n",
      "loss: 0.960939  [ 1440/ 3200]\n",
      "loss: 0.736134  [ 1456/ 3200]\n",
      "loss: 0.555754  [ 1472/ 3200]\n",
      "loss: 0.763904  [ 1488/ 3200]\n",
      "loss: 0.787922  [ 1504/ 3200]\n",
      "loss: 1.064549  [ 1520/ 3200]\n",
      "loss: 1.080926  [ 1536/ 3200]\n",
      "loss: 1.126847  [ 1552/ 3200]\n",
      "loss: 0.693900  [ 1568/ 3200]\n",
      "loss: 0.662603  [ 1584/ 3200]\n",
      "loss: 0.765095  [ 1600/ 3200]\n",
      "loss: 0.814753  [ 1616/ 3200]\n",
      "loss: 0.863284  [ 1632/ 3200]\n",
      "loss: 1.107245  [ 1648/ 3200]\n",
      "loss: 1.012589  [ 1664/ 3200]\n",
      "loss: 0.732733  [ 1680/ 3200]\n",
      "loss: 1.194748  [ 1696/ 3200]\n",
      "loss: 0.676433  [ 1712/ 3200]\n",
      "loss: 0.549181  [ 1728/ 3200]\n",
      "loss: 0.681502  [ 1744/ 3200]\n",
      "loss: 0.888390  [ 1760/ 3200]\n",
      "loss: 0.703078  [ 1776/ 3200]\n",
      "loss: 0.692127  [ 1792/ 3200]\n",
      "loss: 0.940457  [ 1808/ 3200]\n",
      "loss: 1.126903  [ 1824/ 3200]\n",
      "loss: 0.854718  [ 1840/ 3200]\n",
      "loss: 0.952783  [ 1856/ 3200]\n",
      "loss: 1.085552  [ 1872/ 3200]\n",
      "loss: 1.127450  [ 1888/ 3200]\n",
      "loss: 1.161763  [ 1904/ 3200]\n",
      "loss: 0.959395  [ 1920/ 3200]\n",
      "loss: 0.884647  [ 1936/ 3200]\n",
      "loss: 0.796382  [ 1952/ 3200]\n",
      "loss: 1.029644  [ 1968/ 3200]\n",
      "loss: 0.849474  [ 1984/ 3200]\n",
      "loss: 0.584178  [ 2000/ 3200]\n",
      "loss: 0.436963  [ 2016/ 3200]\n",
      "loss: 0.986836  [ 2032/ 3200]\n",
      "loss: 0.804456  [ 2048/ 3200]\n",
      "loss: 0.752323  [ 2064/ 3200]\n",
      "loss: 0.904369  [ 2080/ 3200]\n",
      "loss: 1.241484  [ 2096/ 3200]\n",
      "loss: 1.242533  [ 2112/ 3200]\n",
      "loss: 0.697551  [ 2128/ 3200]\n",
      "loss: 0.536253  [ 2144/ 3200]\n",
      "loss: 0.681467  [ 2160/ 3200]\n",
      "loss: 0.735054  [ 2176/ 3200]\n",
      "loss: 0.739222  [ 2192/ 3200]\n",
      "loss: 0.619975  [ 2208/ 3200]\n",
      "loss: 0.796742  [ 2224/ 3200]\n",
      "loss: 0.772414  [ 2240/ 3200]\n",
      "loss: 0.965822  [ 2256/ 3200]\n",
      "loss: 0.650566  [ 2272/ 3200]\n",
      "loss: 1.484875  [ 2288/ 3200]\n",
      "loss: 0.837705  [ 2304/ 3200]\n",
      "loss: 0.829378  [ 2320/ 3200]\n",
      "loss: 0.996797  [ 2336/ 3200]\n",
      "loss: 0.885712  [ 2352/ 3200]\n",
      "loss: 1.030502  [ 2368/ 3200]\n",
      "loss: 0.754277  [ 2384/ 3200]\n",
      "loss: 0.702159  [ 2400/ 3200]\n",
      "loss: 0.910900  [ 2416/ 3200]\n",
      "loss: 0.760392  [ 2432/ 3200]\n",
      "loss: 0.601978  [ 2448/ 3200]\n",
      "loss: 0.896135  [ 2464/ 3200]\n",
      "loss: 1.139094  [ 2480/ 3200]\n",
      "loss: 0.931458  [ 2496/ 3200]\n",
      "loss: 1.034861  [ 2512/ 3200]\n",
      "loss: 0.745189  [ 2528/ 3200]\n",
      "loss: 0.930650  [ 2544/ 3200]\n",
      "loss: 1.054487  [ 2560/ 3200]\n",
      "loss: 0.708573  [ 2576/ 3200]\n",
      "loss: 0.771309  [ 2592/ 3200]\n",
      "loss: 1.105648  [ 2608/ 3200]\n",
      "loss: 1.002618  [ 2624/ 3200]\n",
      "loss: 0.821401  [ 2640/ 3200]\n",
      "loss: 1.035581  [ 2656/ 3200]\n",
      "loss: 0.954368  [ 2672/ 3200]\n",
      "loss: 0.642946  [ 2688/ 3200]\n",
      "loss: 0.632757  [ 2704/ 3200]\n",
      "loss: 0.820532  [ 2720/ 3200]\n",
      "loss: 0.676627  [ 2736/ 3200]\n",
      "loss: 0.888305  [ 2752/ 3200]\n",
      "loss: 0.752988  [ 2768/ 3200]\n",
      "loss: 1.385482  [ 2784/ 3200]\n",
      "loss: 1.023975  [ 2800/ 3200]\n",
      "loss: 1.130954  [ 2816/ 3200]\n",
      "loss: 1.210965  [ 2832/ 3200]\n",
      "loss: 0.737879  [ 2848/ 3200]\n",
      "loss: 0.810612  [ 2864/ 3200]\n",
      "loss: 0.694826  [ 2880/ 3200]\n",
      "loss: 0.942730  [ 2896/ 3200]\n",
      "loss: 0.813236  [ 2912/ 3200]\n",
      "loss: 0.592989  [ 2928/ 3200]\n",
      "loss: 0.663477  [ 2944/ 3200]\n",
      "loss: 1.174434  [ 2960/ 3200]\n",
      "loss: 0.859883  [ 2976/ 3200]\n",
      "loss: 0.976915  [ 2992/ 3200]\n",
      "loss: 0.929139  [ 3008/ 3200]\n",
      "loss: 0.846247  [ 3024/ 3200]\n",
      "loss: 0.637710  [ 3040/ 3200]\n",
      "loss: 1.047894  [ 3056/ 3200]\n",
      "loss: 0.846904  [ 3072/ 3200]\n",
      "loss: 0.756871  [ 3088/ 3200]\n",
      "loss: 0.875482  [ 3104/ 3200]\n",
      "loss: 0.874872  [ 3120/ 3200]\n",
      "loss: 0.870474  [ 3136/ 3200]\n",
      "loss: 0.897393  [ 3152/ 3200]\n",
      "loss: 1.030143  [ 3168/ 3200]\n",
      "loss: 0.707972  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.061926\n",
      "f1 macro averaged score: 0.553710\n",
      "Accuracy               : 57.2%\n",
      "Confusion matrix       :\n",
      "tensor([[141,  36,  19,   4],\n",
      "        [ 32,  46,  74,  48],\n",
      "        [  0,   6, 171,  23],\n",
      "        [  3,  23,  74, 100]], device='cuda:0')\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 0.853929  [    0/ 3200]\n",
      "loss: 0.883948  [   16/ 3200]\n",
      "loss: 0.927207  [   32/ 3200]\n",
      "loss: 0.969268  [   48/ 3200]\n",
      "loss: 0.603044  [   64/ 3200]\n",
      "loss: 1.279354  [   80/ 3200]\n",
      "loss: 1.203259  [   96/ 3200]\n",
      "loss: 1.136765  [  112/ 3200]\n",
      "loss: 0.812287  [  128/ 3200]\n",
      "loss: 0.808076  [  144/ 3200]\n",
      "loss: 0.930643  [  160/ 3200]\n",
      "loss: 0.638414  [  176/ 3200]\n",
      "loss: 0.930198  [  192/ 3200]\n",
      "loss: 1.092094  [  208/ 3200]\n",
      "loss: 1.089166  [  224/ 3200]\n",
      "loss: 0.942631  [  240/ 3200]\n",
      "loss: 0.834863  [  256/ 3200]\n",
      "loss: 0.739246  [  272/ 3200]\n",
      "loss: 0.731922  [  288/ 3200]\n",
      "loss: 0.879912  [  304/ 3200]\n",
      "loss: 1.340440  [  320/ 3200]\n",
      "loss: 0.950049  [  336/ 3200]\n",
      "loss: 0.946347  [  352/ 3200]\n",
      "loss: 0.916180  [  368/ 3200]\n",
      "loss: 0.813154  [  384/ 3200]\n",
      "loss: 0.739654  [  400/ 3200]\n",
      "loss: 0.660843  [  416/ 3200]\n",
      "loss: 0.880977  [  432/ 3200]\n",
      "loss: 0.655677  [  448/ 3200]\n",
      "loss: 0.784845  [  464/ 3200]\n",
      "loss: 0.950564  [  480/ 3200]\n",
      "loss: 1.129680  [  496/ 3200]\n",
      "loss: 1.101891  [  512/ 3200]\n",
      "loss: 0.590243  [  528/ 3200]\n",
      "loss: 0.698098  [  544/ 3200]\n",
      "loss: 0.843887  [  560/ 3200]\n",
      "loss: 0.993088  [  576/ 3200]\n",
      "loss: 0.724282  [  592/ 3200]\n",
      "loss: 0.382705  [  608/ 3200]\n",
      "loss: 0.767923  [  624/ 3200]\n",
      "loss: 0.770895  [  640/ 3200]\n",
      "loss: 0.772865  [  656/ 3200]\n",
      "loss: 0.683942  [  672/ 3200]\n",
      "loss: 1.005135  [  688/ 3200]\n",
      "loss: 0.850143  [  704/ 3200]\n",
      "loss: 0.787800  [  720/ 3200]\n",
      "loss: 0.921870  [  736/ 3200]\n",
      "loss: 0.868578  [  752/ 3200]\n",
      "loss: 0.939667  [  768/ 3200]\n",
      "loss: 0.560119  [  784/ 3200]\n",
      "loss: 0.653989  [  800/ 3200]\n",
      "loss: 0.608722  [  816/ 3200]\n",
      "loss: 0.955228  [  832/ 3200]\n",
      "loss: 0.977088  [  848/ 3200]\n",
      "loss: 1.109411  [  864/ 3200]\n",
      "loss: 0.986876  [  880/ 3200]\n",
      "loss: 0.924784  [  896/ 3200]\n",
      "loss: 0.955706  [  912/ 3200]\n",
      "loss: 0.890441  [  928/ 3200]\n",
      "loss: 1.101857  [  944/ 3200]\n",
      "loss: 1.090132  [  960/ 3200]\n",
      "loss: 0.936102  [  976/ 3200]\n",
      "loss: 1.061403  [  992/ 3200]\n",
      "loss: 0.886958  [ 1008/ 3200]\n",
      "loss: 0.816635  [ 1024/ 3200]\n",
      "loss: 0.842064  [ 1040/ 3200]\n",
      "loss: 0.969504  [ 1056/ 3200]\n",
      "loss: 0.581233  [ 1072/ 3200]\n",
      "loss: 0.633193  [ 1088/ 3200]\n",
      "loss: 0.728251  [ 1104/ 3200]\n",
      "loss: 0.903113  [ 1120/ 3200]\n",
      "loss: 0.604951  [ 1136/ 3200]\n",
      "loss: 0.648766  [ 1152/ 3200]\n",
      "loss: 0.790165  [ 1168/ 3200]\n",
      "loss: 0.793586  [ 1184/ 3200]\n",
      "loss: 0.751093  [ 1200/ 3200]\n",
      "loss: 0.646225  [ 1216/ 3200]\n",
      "loss: 0.738183  [ 1232/ 3200]\n",
      "loss: 0.769887  [ 1248/ 3200]\n",
      "loss: 0.585265  [ 1264/ 3200]\n",
      "loss: 0.841212  [ 1280/ 3200]\n",
      "loss: 0.906569  [ 1296/ 3200]\n",
      "loss: 0.755000  [ 1312/ 3200]\n",
      "loss: 0.792532  [ 1328/ 3200]\n",
      "loss: 1.026010  [ 1344/ 3200]\n",
      "loss: 1.084187  [ 1360/ 3200]\n",
      "loss: 0.921365  [ 1376/ 3200]\n",
      "loss: 0.843694  [ 1392/ 3200]\n",
      "loss: 0.906665  [ 1408/ 3200]\n",
      "loss: 0.674825  [ 1424/ 3200]\n",
      "loss: 0.908934  [ 1440/ 3200]\n",
      "loss: 0.763245  [ 1456/ 3200]\n",
      "loss: 1.318703  [ 1472/ 3200]\n",
      "loss: 1.062668  [ 1488/ 3200]\n",
      "loss: 0.925524  [ 1504/ 3200]\n",
      "loss: 0.762308  [ 1520/ 3200]\n",
      "loss: 0.767578  [ 1536/ 3200]\n",
      "loss: 0.697093  [ 1552/ 3200]\n",
      "loss: 0.894640  [ 1568/ 3200]\n",
      "loss: 0.875630  [ 1584/ 3200]\n",
      "loss: 0.791490  [ 1600/ 3200]\n",
      "loss: 0.656977  [ 1616/ 3200]\n",
      "loss: 0.716801  [ 1632/ 3200]\n",
      "loss: 0.909074  [ 1648/ 3200]\n",
      "loss: 0.951814  [ 1664/ 3200]\n",
      "loss: 0.838280  [ 1680/ 3200]\n",
      "loss: 0.951404  [ 1696/ 3200]\n",
      "loss: 0.986896  [ 1712/ 3200]\n",
      "loss: 0.632343  [ 1728/ 3200]\n",
      "loss: 0.940383  [ 1744/ 3200]\n",
      "loss: 0.596931  [ 1760/ 3200]\n",
      "loss: 1.086774  [ 1776/ 3200]\n",
      "loss: 0.820427  [ 1792/ 3200]\n",
      "loss: 0.647633  [ 1808/ 3200]\n",
      "loss: 1.080876  [ 1824/ 3200]\n",
      "loss: 0.601950  [ 1840/ 3200]\n",
      "loss: 0.597571  [ 1856/ 3200]\n",
      "loss: 0.779528  [ 1872/ 3200]\n",
      "loss: 0.949731  [ 1888/ 3200]\n",
      "loss: 0.895914  [ 1904/ 3200]\n",
      "loss: 0.761880  [ 1920/ 3200]\n",
      "loss: 0.796503  [ 1936/ 3200]\n",
      "loss: 0.795375  [ 1952/ 3200]\n",
      "loss: 0.853521  [ 1968/ 3200]\n",
      "loss: 0.783666  [ 1984/ 3200]\n",
      "loss: 0.947388  [ 2000/ 3200]\n",
      "loss: 0.693208  [ 2016/ 3200]\n",
      "loss: 0.938444  [ 2032/ 3200]\n",
      "loss: 0.976751  [ 2048/ 3200]\n",
      "loss: 0.878943  [ 2064/ 3200]\n",
      "loss: 1.080256  [ 2080/ 3200]\n",
      "loss: 1.311710  [ 2096/ 3200]\n",
      "loss: 1.176700  [ 2112/ 3200]\n",
      "loss: 0.846058  [ 2128/ 3200]\n",
      "loss: 1.024629  [ 2144/ 3200]\n",
      "loss: 0.756669  [ 2160/ 3200]\n",
      "loss: 1.067862  [ 2176/ 3200]\n",
      "loss: 0.556749  [ 2192/ 3200]\n",
      "loss: 0.822689  [ 2208/ 3200]\n",
      "loss: 0.778761  [ 2224/ 3200]\n",
      "loss: 0.694320  [ 2240/ 3200]\n",
      "loss: 1.007580  [ 2256/ 3200]\n",
      "loss: 1.030826  [ 2272/ 3200]\n",
      "loss: 0.888350  [ 2288/ 3200]\n",
      "loss: 0.983350  [ 2304/ 3200]\n",
      "loss: 0.747161  [ 2320/ 3200]\n",
      "loss: 0.784581  [ 2336/ 3200]\n",
      "loss: 0.906932  [ 2352/ 3200]\n",
      "loss: 0.834250  [ 2368/ 3200]\n",
      "loss: 0.958773  [ 2384/ 3200]\n",
      "loss: 0.986858  [ 2400/ 3200]\n",
      "loss: 0.896604  [ 2416/ 3200]\n",
      "loss: 1.085990  [ 2432/ 3200]\n",
      "loss: 0.967668  [ 2448/ 3200]\n",
      "loss: 0.817413  [ 2464/ 3200]\n",
      "loss: 1.199256  [ 2480/ 3200]\n",
      "loss: 1.112810  [ 2496/ 3200]\n",
      "loss: 0.789527  [ 2512/ 3200]\n",
      "loss: 1.108344  [ 2528/ 3200]\n",
      "loss: 1.187354  [ 2544/ 3200]\n",
      "loss: 0.924193  [ 2560/ 3200]\n",
      "loss: 0.924436  [ 2576/ 3200]\n",
      "loss: 0.819542  [ 2592/ 3200]\n",
      "loss: 0.903797  [ 2608/ 3200]\n",
      "loss: 0.701268  [ 2624/ 3200]\n",
      "loss: 0.808777  [ 2640/ 3200]\n",
      "loss: 0.936634  [ 2656/ 3200]\n",
      "loss: 0.762621  [ 2672/ 3200]\n",
      "loss: 0.966515  [ 2688/ 3200]\n",
      "loss: 0.680016  [ 2704/ 3200]\n",
      "loss: 1.036658  [ 2720/ 3200]\n",
      "loss: 0.795411  [ 2736/ 3200]\n",
      "loss: 0.644132  [ 2752/ 3200]\n",
      "loss: 0.762791  [ 2768/ 3200]\n",
      "loss: 0.986466  [ 2784/ 3200]\n",
      "loss: 1.117711  [ 2800/ 3200]\n",
      "loss: 0.981817  [ 2816/ 3200]\n",
      "loss: 0.843527  [ 2832/ 3200]\n",
      "loss: 1.018754  [ 2848/ 3200]\n",
      "loss: 0.965446  [ 2864/ 3200]\n",
      "loss: 0.860578  [ 2880/ 3200]\n",
      "loss: 0.875513  [ 2896/ 3200]\n",
      "loss: 0.888890  [ 2912/ 3200]\n",
      "loss: 0.629649  [ 2928/ 3200]\n",
      "loss: 0.754241  [ 2944/ 3200]\n",
      "loss: 0.730205  [ 2960/ 3200]\n",
      "loss: 1.142874  [ 2976/ 3200]\n",
      "loss: 0.814561  [ 2992/ 3200]\n",
      "loss: 0.932264  [ 3008/ 3200]\n",
      "loss: 0.756660  [ 3024/ 3200]\n",
      "loss: 0.829470  [ 3040/ 3200]\n",
      "loss: 0.831712  [ 3056/ 3200]\n",
      "loss: 0.786405  [ 3072/ 3200]\n",
      "loss: 0.657402  [ 3088/ 3200]\n",
      "loss: 0.966124  [ 3104/ 3200]\n",
      "loss: 0.715817  [ 3120/ 3200]\n",
      "loss: 1.014491  [ 3136/ 3200]\n",
      "loss: 1.012222  [ 3152/ 3200]\n",
      "loss: 1.159357  [ 3168/ 3200]\n",
      "loss: 1.023828  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.066093\n",
      "f1 macro averaged score: 0.501297\n",
      "Accuracy               : 51.4%\n",
      "Confusion matrix       :\n",
      "tensor([[103,  71,  24,   2],\n",
      "        [ 18,  63, 100,  19],\n",
      "        [  0,   7, 181,  12],\n",
      "        [  0,  35, 101,  64]], device='cuda:0')\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 0.836363  [    0/ 3200]\n",
      "loss: 0.835287  [   16/ 3200]\n",
      "loss: 0.908506  [   32/ 3200]\n",
      "loss: 0.647309  [   48/ 3200]\n",
      "loss: 1.094202  [   64/ 3200]\n",
      "loss: 0.668084  [   80/ 3200]\n",
      "loss: 0.834063  [   96/ 3200]\n",
      "loss: 0.870128  [  112/ 3200]\n",
      "loss: 0.646499  [  128/ 3200]\n",
      "loss: 0.996260  [  144/ 3200]\n",
      "loss: 0.769644  [  160/ 3200]\n",
      "loss: 0.919306  [  176/ 3200]\n",
      "loss: 1.301639  [  192/ 3200]\n",
      "loss: 0.672895  [  208/ 3200]\n",
      "loss: 0.956546  [  224/ 3200]\n",
      "loss: 0.786305  [  240/ 3200]\n",
      "loss: 0.648580  [  256/ 3200]\n",
      "loss: 0.778354  [  272/ 3200]\n",
      "loss: 0.800885  [  288/ 3200]\n",
      "loss: 0.656404  [  304/ 3200]\n",
      "loss: 0.814224  [  320/ 3200]\n",
      "loss: 0.818610  [  336/ 3200]\n",
      "loss: 0.829155  [  352/ 3200]\n",
      "loss: 0.819959  [  368/ 3200]\n",
      "loss: 0.420728  [  384/ 3200]\n",
      "loss: 0.823748  [  400/ 3200]\n",
      "loss: 1.090018  [  416/ 3200]\n",
      "loss: 0.971028  [  432/ 3200]\n",
      "loss: 1.148315  [  448/ 3200]\n",
      "loss: 0.698833  [  464/ 3200]\n",
      "loss: 1.049589  [  480/ 3200]\n",
      "loss: 0.691607  [  496/ 3200]\n",
      "loss: 1.241920  [  512/ 3200]\n",
      "loss: 0.768119  [  528/ 3200]\n",
      "loss: 0.669956  [  544/ 3200]\n",
      "loss: 0.685460  [  560/ 3200]\n",
      "loss: 0.820048  [  576/ 3200]\n",
      "loss: 0.878108  [  592/ 3200]\n",
      "loss: 0.782424  [  608/ 3200]\n",
      "loss: 0.738813  [  624/ 3200]\n",
      "loss: 1.284580  [  640/ 3200]\n",
      "loss: 0.542903  [  656/ 3200]\n",
      "loss: 0.959945  [  672/ 3200]\n",
      "loss: 0.793251  [  688/ 3200]\n",
      "loss: 0.665690  [  704/ 3200]\n",
      "loss: 0.659675  [  720/ 3200]\n",
      "loss: 1.085652  [  736/ 3200]\n",
      "loss: 0.743859  [  752/ 3200]\n",
      "loss: 0.851635  [  768/ 3200]\n",
      "loss: 0.902023  [  784/ 3200]\n",
      "loss: 0.573611  [  800/ 3200]\n",
      "loss: 0.692194  [  816/ 3200]\n",
      "loss: 0.851517  [  832/ 3200]\n",
      "loss: 0.695462  [  848/ 3200]\n",
      "loss: 1.095392  [  864/ 3200]\n",
      "loss: 0.873637  [  880/ 3200]\n",
      "loss: 0.732127  [  896/ 3200]\n",
      "loss: 0.670392  [  912/ 3200]\n",
      "loss: 0.927231  [  928/ 3200]\n",
      "loss: 1.082772  [  944/ 3200]\n",
      "loss: 1.158432  [  960/ 3200]\n",
      "loss: 0.717418  [  976/ 3200]\n",
      "loss: 0.857299  [  992/ 3200]\n",
      "loss: 0.718999  [ 1008/ 3200]\n",
      "loss: 1.157285  [ 1024/ 3200]\n",
      "loss: 0.840558  [ 1040/ 3200]\n",
      "loss: 0.722813  [ 1056/ 3200]\n",
      "loss: 0.848210  [ 1072/ 3200]\n",
      "loss: 0.804386  [ 1088/ 3200]\n",
      "loss: 0.782412  [ 1104/ 3200]\n",
      "loss: 1.304309  [ 1120/ 3200]\n",
      "loss: 0.904189  [ 1136/ 3200]\n",
      "loss: 0.953959  [ 1152/ 3200]\n",
      "loss: 0.973058  [ 1168/ 3200]\n",
      "loss: 0.627251  [ 1184/ 3200]\n",
      "loss: 0.752677  [ 1200/ 3200]\n",
      "loss: 0.954619  [ 1216/ 3200]\n",
      "loss: 1.028589  [ 1232/ 3200]\n",
      "loss: 0.909362  [ 1248/ 3200]\n",
      "loss: 0.847491  [ 1264/ 3200]\n",
      "loss: 0.769350  [ 1280/ 3200]\n",
      "loss: 0.992540  [ 1296/ 3200]\n",
      "loss: 0.751641  [ 1312/ 3200]\n",
      "loss: 0.910816  [ 1328/ 3200]\n",
      "loss: 0.982825  [ 1344/ 3200]\n",
      "loss: 0.956759  [ 1360/ 3200]\n",
      "loss: 0.716562  [ 1376/ 3200]\n",
      "loss: 0.894095  [ 1392/ 3200]\n",
      "loss: 0.911342  [ 1408/ 3200]\n",
      "loss: 0.552351  [ 1424/ 3200]\n",
      "loss: 0.798812  [ 1440/ 3200]\n",
      "loss: 0.739417  [ 1456/ 3200]\n",
      "loss: 0.711052  [ 1472/ 3200]\n",
      "loss: 0.732027  [ 1488/ 3200]\n",
      "loss: 0.664756  [ 1504/ 3200]\n",
      "loss: 1.223964  [ 1520/ 3200]\n",
      "loss: 0.931562  [ 1536/ 3200]\n",
      "loss: 0.755538  [ 1552/ 3200]\n",
      "loss: 0.706221  [ 1568/ 3200]\n",
      "loss: 0.788570  [ 1584/ 3200]\n",
      "loss: 0.522617  [ 1600/ 3200]\n",
      "loss: 1.149765  [ 1616/ 3200]\n",
      "loss: 1.058168  [ 1632/ 3200]\n",
      "loss: 0.898961  [ 1648/ 3200]\n",
      "loss: 0.883629  [ 1664/ 3200]\n",
      "loss: 0.755777  [ 1680/ 3200]\n",
      "loss: 1.370203  [ 1696/ 3200]\n",
      "loss: 1.187567  [ 1712/ 3200]\n",
      "loss: 0.690769  [ 1728/ 3200]\n",
      "loss: 0.751278  [ 1744/ 3200]\n",
      "loss: 1.167909  [ 1760/ 3200]\n",
      "loss: 0.911097  [ 1776/ 3200]\n",
      "loss: 0.890125  [ 1792/ 3200]\n",
      "loss: 0.745543  [ 1808/ 3200]\n",
      "loss: 0.902546  [ 1824/ 3200]\n",
      "loss: 0.766789  [ 1840/ 3200]\n",
      "loss: 0.816499  [ 1856/ 3200]\n",
      "loss: 0.734555  [ 1872/ 3200]\n",
      "loss: 0.617052  [ 1888/ 3200]\n",
      "loss: 0.725881  [ 1904/ 3200]\n",
      "loss: 0.527467  [ 1920/ 3200]\n",
      "loss: 0.700103  [ 1936/ 3200]\n",
      "loss: 0.533838  [ 1952/ 3200]\n",
      "loss: 0.818877  [ 1968/ 3200]\n",
      "loss: 0.568848  [ 1984/ 3200]\n",
      "loss: 1.017152  [ 2000/ 3200]\n",
      "loss: 0.703387  [ 2016/ 3200]\n",
      "loss: 0.717218  [ 2032/ 3200]\n",
      "loss: 0.978170  [ 2048/ 3200]\n",
      "loss: 1.123180  [ 2064/ 3200]\n",
      "loss: 0.696356  [ 2080/ 3200]\n",
      "loss: 0.968108  [ 2096/ 3200]\n",
      "loss: 0.924420  [ 2112/ 3200]\n",
      "loss: 0.964912  [ 2128/ 3200]\n",
      "loss: 0.713526  [ 2144/ 3200]\n",
      "loss: 0.947264  [ 2160/ 3200]\n",
      "loss: 0.926105  [ 2176/ 3200]\n",
      "loss: 0.743393  [ 2192/ 3200]\n",
      "loss: 0.868985  [ 2208/ 3200]\n",
      "loss: 0.702624  [ 2224/ 3200]\n",
      "loss: 0.900717  [ 2240/ 3200]\n",
      "loss: 0.755004  [ 2256/ 3200]\n",
      "loss: 0.732447  [ 2272/ 3200]\n",
      "loss: 0.641133  [ 2288/ 3200]\n",
      "loss: 0.959634  [ 2304/ 3200]\n",
      "loss: 0.699725  [ 2320/ 3200]\n",
      "loss: 1.264318  [ 2336/ 3200]\n",
      "loss: 0.943701  [ 2352/ 3200]\n",
      "loss: 0.838713  [ 2368/ 3200]\n",
      "loss: 0.922832  [ 2384/ 3200]\n",
      "loss: 0.696920  [ 2400/ 3200]\n",
      "loss: 0.855084  [ 2416/ 3200]\n",
      "loss: 0.890414  [ 2432/ 3200]\n",
      "loss: 0.945496  [ 2448/ 3200]\n",
      "loss: 0.712076  [ 2464/ 3200]\n",
      "loss: 0.781663  [ 2480/ 3200]\n",
      "loss: 0.765679  [ 2496/ 3200]\n",
      "loss: 1.168002  [ 2512/ 3200]\n",
      "loss: 0.829509  [ 2528/ 3200]\n",
      "loss: 0.720851  [ 2544/ 3200]\n",
      "loss: 0.881045  [ 2560/ 3200]\n",
      "loss: 0.970457  [ 2576/ 3200]\n",
      "loss: 0.779890  [ 2592/ 3200]\n",
      "loss: 0.862502  [ 2608/ 3200]\n",
      "loss: 0.957268  [ 2624/ 3200]\n",
      "loss: 0.715297  [ 2640/ 3200]\n",
      "loss: 0.673749  [ 2656/ 3200]\n",
      "loss: 0.884585  [ 2672/ 3200]\n",
      "loss: 1.011659  [ 2688/ 3200]\n",
      "loss: 0.637384  [ 2704/ 3200]\n",
      "loss: 0.741175  [ 2720/ 3200]\n",
      "loss: 0.733780  [ 2736/ 3200]\n",
      "loss: 0.697328  [ 2752/ 3200]\n",
      "loss: 1.026244  [ 2768/ 3200]\n",
      "loss: 0.973380  [ 2784/ 3200]\n",
      "loss: 0.959080  [ 2800/ 3200]\n",
      "loss: 0.873850  [ 2816/ 3200]\n",
      "loss: 0.777885  [ 2832/ 3200]\n",
      "loss: 0.621629  [ 2848/ 3200]\n",
      "loss: 0.737762  [ 2864/ 3200]\n",
      "loss: 0.803037  [ 2880/ 3200]\n",
      "loss: 0.913036  [ 2896/ 3200]\n",
      "loss: 0.947824  [ 2912/ 3200]\n",
      "loss: 0.792403  [ 2928/ 3200]\n",
      "loss: 1.423609  [ 2944/ 3200]\n",
      "loss: 0.875230  [ 2960/ 3200]\n",
      "loss: 0.796801  [ 2976/ 3200]\n",
      "loss: 0.649984  [ 2992/ 3200]\n",
      "loss: 0.767883  [ 3008/ 3200]\n",
      "loss: 0.738024  [ 3024/ 3200]\n",
      "loss: 0.628071  [ 3040/ 3200]\n",
      "loss: 0.897323  [ 3056/ 3200]\n",
      "loss: 0.823633  [ 3072/ 3200]\n",
      "loss: 1.249790  [ 3088/ 3200]\n",
      "loss: 0.955041  [ 3104/ 3200]\n",
      "loss: 0.924857  [ 3120/ 3200]\n",
      "loss: 0.795154  [ 3136/ 3200]\n",
      "loss: 0.741683  [ 3152/ 3200]\n",
      "loss: 1.051182  [ 3168/ 3200]\n",
      "loss: 0.749698  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.059776\n",
      "f1 macro averaged score: 0.590094\n",
      "Accuracy               : 59.2%\n",
      "Confusion matrix       :\n",
      "tensor([[165,  34,   0,   1],\n",
      "        [ 45, 114,   6,  35],\n",
      "        [  4,  52,  79,  65],\n",
      "        [  7,  62,  15, 116]], device='cuda:0')\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 0.895089  [    0/ 3200]\n",
      "loss: 0.657763  [   16/ 3200]\n",
      "loss: 0.593034  [   32/ 3200]\n",
      "loss: 1.127773  [   48/ 3200]\n",
      "loss: 0.743860  [   64/ 3200]\n",
      "loss: 0.511225  [   80/ 3200]\n",
      "loss: 0.746842  [   96/ 3200]\n",
      "loss: 1.163927  [  112/ 3200]\n",
      "loss: 0.770120  [  128/ 3200]\n",
      "loss: 0.736996  [  144/ 3200]\n",
      "loss: 0.625100  [  160/ 3200]\n",
      "loss: 0.754675  [  176/ 3200]\n",
      "loss: 0.753031  [  192/ 3200]\n",
      "loss: 0.755395  [  208/ 3200]\n",
      "loss: 0.750646  [  224/ 3200]\n",
      "loss: 0.746470  [  240/ 3200]\n",
      "loss: 0.838217  [  256/ 3200]\n",
      "loss: 0.705565  [  272/ 3200]\n",
      "loss: 1.338564  [  288/ 3200]\n",
      "loss: 0.788168  [  304/ 3200]\n",
      "loss: 0.692427  [  320/ 3200]\n",
      "loss: 0.670148  [  336/ 3200]\n",
      "loss: 1.066108  [  352/ 3200]\n",
      "loss: 1.130572  [  368/ 3200]\n",
      "loss: 0.955741  [  384/ 3200]\n",
      "loss: 0.803278  [  400/ 3200]\n",
      "loss: 1.191510  [  416/ 3200]\n",
      "loss: 0.779824  [  432/ 3200]\n",
      "loss: 0.697781  [  448/ 3200]\n",
      "loss: 0.638780  [  464/ 3200]\n",
      "loss: 0.836135  [  480/ 3200]\n",
      "loss: 0.809223  [  496/ 3200]\n",
      "loss: 0.563482  [  512/ 3200]\n",
      "loss: 1.070139  [  528/ 3200]\n",
      "loss: 0.649037  [  544/ 3200]\n",
      "loss: 0.818673  [  560/ 3200]\n",
      "loss: 0.838255  [  576/ 3200]\n",
      "loss: 0.795194  [  592/ 3200]\n",
      "loss: 0.696489  [  608/ 3200]\n",
      "loss: 1.239509  [  624/ 3200]\n",
      "loss: 0.852282  [  640/ 3200]\n",
      "loss: 0.975688  [  656/ 3200]\n",
      "loss: 0.487558  [  672/ 3200]\n",
      "loss: 0.822048  [  688/ 3200]\n",
      "loss: 0.936781  [  704/ 3200]\n",
      "loss: 0.693456  [  720/ 3200]\n",
      "loss: 0.804050  [  736/ 3200]\n",
      "loss: 1.002907  [  752/ 3200]\n",
      "loss: 0.710212  [  768/ 3200]\n",
      "loss: 0.725747  [  784/ 3200]\n",
      "loss: 0.749388  [  800/ 3200]\n",
      "loss: 0.515515  [  816/ 3200]\n",
      "loss: 1.459846  [  832/ 3200]\n",
      "loss: 0.713111  [  848/ 3200]\n",
      "loss: 0.533622  [  864/ 3200]\n",
      "loss: 0.679092  [  880/ 3200]\n",
      "loss: 1.063412  [  896/ 3200]\n",
      "loss: 0.878862  [  912/ 3200]\n",
      "loss: 1.135048  [  928/ 3200]\n",
      "loss: 0.802511  [  944/ 3200]\n",
      "loss: 0.720307  [  960/ 3200]\n",
      "loss: 0.916061  [  976/ 3200]\n",
      "loss: 1.036954  [  992/ 3200]\n",
      "loss: 0.646077  [ 1008/ 3200]\n",
      "loss: 0.873476  [ 1024/ 3200]\n",
      "loss: 0.890504  [ 1040/ 3200]\n",
      "loss: 0.993909  [ 1056/ 3200]\n",
      "loss: 0.883475  [ 1072/ 3200]\n",
      "loss: 0.679835  [ 1088/ 3200]\n",
      "loss: 0.683849  [ 1104/ 3200]\n",
      "loss: 0.473912  [ 1120/ 3200]\n",
      "loss: 0.831930  [ 1136/ 3200]\n",
      "loss: 1.000211  [ 1152/ 3200]\n",
      "loss: 1.041353  [ 1168/ 3200]\n",
      "loss: 0.749603  [ 1184/ 3200]\n",
      "loss: 0.765731  [ 1200/ 3200]\n",
      "loss: 0.955884  [ 1216/ 3200]\n",
      "loss: 0.988848  [ 1232/ 3200]\n",
      "loss: 0.802096  [ 1248/ 3200]\n",
      "loss: 0.856051  [ 1264/ 3200]\n",
      "loss: 0.700881  [ 1280/ 3200]\n",
      "loss: 0.737427  [ 1296/ 3200]\n",
      "loss: 0.649299  [ 1312/ 3200]\n",
      "loss: 0.891860  [ 1328/ 3200]\n",
      "loss: 0.813394  [ 1344/ 3200]\n",
      "loss: 1.059211  [ 1360/ 3200]\n",
      "loss: 0.921578  [ 1376/ 3200]\n",
      "loss: 0.944949  [ 1392/ 3200]\n",
      "loss: 0.999847  [ 1408/ 3200]\n",
      "loss: 0.664872  [ 1424/ 3200]\n",
      "loss: 0.869832  [ 1440/ 3200]\n",
      "loss: 0.731022  [ 1456/ 3200]\n",
      "loss: 0.914999  [ 1472/ 3200]\n",
      "loss: 0.850653  [ 1488/ 3200]\n",
      "loss: 0.542365  [ 1504/ 3200]\n",
      "loss: 0.939719  [ 1520/ 3200]\n",
      "loss: 0.669357  [ 1536/ 3200]\n",
      "loss: 0.824722  [ 1552/ 3200]\n",
      "loss: 0.607674  [ 1568/ 3200]\n",
      "loss: 0.787614  [ 1584/ 3200]\n",
      "loss: 1.168361  [ 1600/ 3200]\n",
      "loss: 1.015086  [ 1616/ 3200]\n",
      "loss: 0.526899  [ 1632/ 3200]\n",
      "loss: 0.853204  [ 1648/ 3200]\n",
      "loss: 0.659210  [ 1664/ 3200]\n",
      "loss: 1.150333  [ 1680/ 3200]\n",
      "loss: 0.808023  [ 1696/ 3200]\n",
      "loss: 0.580943  [ 1712/ 3200]\n",
      "loss: 0.547958  [ 1728/ 3200]\n",
      "loss: 0.860628  [ 1744/ 3200]\n",
      "loss: 1.064447  [ 1760/ 3200]\n",
      "loss: 0.603971  [ 1776/ 3200]\n",
      "loss: 1.025927  [ 1792/ 3200]\n",
      "loss: 0.677036  [ 1808/ 3200]\n",
      "loss: 0.895273  [ 1824/ 3200]\n",
      "loss: 0.798327  [ 1840/ 3200]\n",
      "loss: 0.823352  [ 1856/ 3200]\n",
      "loss: 0.966045  [ 1872/ 3200]\n",
      "loss: 0.934758  [ 1888/ 3200]\n",
      "loss: 0.725491  [ 1904/ 3200]\n",
      "loss: 1.099439  [ 1920/ 3200]\n",
      "loss: 0.877219  [ 1936/ 3200]\n",
      "loss: 0.928336  [ 1952/ 3200]\n",
      "loss: 0.854974  [ 1968/ 3200]\n",
      "loss: 0.615843  [ 1984/ 3200]\n",
      "loss: 0.586876  [ 2000/ 3200]\n",
      "loss: 0.668512  [ 2016/ 3200]\n",
      "loss: 1.145525  [ 2032/ 3200]\n",
      "loss: 1.053835  [ 2048/ 3200]\n",
      "loss: 0.663777  [ 2064/ 3200]\n",
      "loss: 0.701357  [ 2080/ 3200]\n",
      "loss: 0.860356  [ 2096/ 3200]\n",
      "loss: 0.862390  [ 2112/ 3200]\n",
      "loss: 1.041429  [ 2128/ 3200]\n",
      "loss: 0.493445  [ 2144/ 3200]\n",
      "loss: 0.627116  [ 2160/ 3200]\n",
      "loss: 0.942101  [ 2176/ 3200]\n",
      "loss: 0.701607  [ 2192/ 3200]\n",
      "loss: 0.889710  [ 2208/ 3200]\n",
      "loss: 0.570902  [ 2224/ 3200]\n",
      "loss: 0.973021  [ 2240/ 3200]\n",
      "loss: 0.654966  [ 2256/ 3200]\n",
      "loss: 0.760293  [ 2272/ 3200]\n",
      "loss: 0.895734  [ 2288/ 3200]\n",
      "loss: 0.635249  [ 2304/ 3200]\n",
      "loss: 0.787943  [ 2320/ 3200]\n",
      "loss: 0.567878  [ 2336/ 3200]\n",
      "loss: 0.748235  [ 2352/ 3200]\n",
      "loss: 0.966302  [ 2368/ 3200]\n",
      "loss: 0.969069  [ 2384/ 3200]\n",
      "loss: 0.691459  [ 2400/ 3200]\n",
      "loss: 0.854743  [ 2416/ 3200]\n",
      "loss: 0.730200  [ 2432/ 3200]\n",
      "loss: 0.745568  [ 2448/ 3200]\n",
      "loss: 0.688152  [ 2464/ 3200]\n",
      "loss: 1.121194  [ 2480/ 3200]\n",
      "loss: 0.756803  [ 2496/ 3200]\n",
      "loss: 0.776029  [ 2512/ 3200]\n",
      "loss: 1.038892  [ 2528/ 3200]\n",
      "loss: 1.159871  [ 2544/ 3200]\n",
      "loss: 1.047672  [ 2560/ 3200]\n",
      "loss: 0.571402  [ 2576/ 3200]\n",
      "loss: 0.799061  [ 2592/ 3200]\n",
      "loss: 0.853469  [ 2608/ 3200]\n",
      "loss: 0.756291  [ 2624/ 3200]\n",
      "loss: 0.744459  [ 2640/ 3200]\n",
      "loss: 0.997766  [ 2656/ 3200]\n",
      "loss: 0.633151  [ 2672/ 3200]\n",
      "loss: 0.692645  [ 2688/ 3200]\n",
      "loss: 1.098616  [ 2704/ 3200]\n",
      "loss: 1.011690  [ 2720/ 3200]\n",
      "loss: 0.725370  [ 2736/ 3200]\n",
      "loss: 0.792741  [ 2752/ 3200]\n",
      "loss: 0.677455  [ 2768/ 3200]\n",
      "loss: 0.729658  [ 2784/ 3200]\n",
      "loss: 0.901179  [ 2800/ 3200]\n",
      "loss: 0.596748  [ 2816/ 3200]\n",
      "loss: 0.862762  [ 2832/ 3200]\n",
      "loss: 0.940879  [ 2848/ 3200]\n",
      "loss: 0.784916  [ 2864/ 3200]\n",
      "loss: 0.947237  [ 2880/ 3200]\n",
      "loss: 0.781241  [ 2896/ 3200]\n",
      "loss: 0.901604  [ 2912/ 3200]\n",
      "loss: 0.881686  [ 2928/ 3200]\n",
      "loss: 0.785506  [ 2944/ 3200]\n",
      "loss: 0.763045  [ 2960/ 3200]\n",
      "loss: 0.576785  [ 2976/ 3200]\n",
      "loss: 0.654538  [ 2992/ 3200]\n",
      "loss: 0.454872  [ 3008/ 3200]\n",
      "loss: 0.642082  [ 3024/ 3200]\n",
      "loss: 0.681159  [ 3040/ 3200]\n",
      "loss: 0.889927  [ 3056/ 3200]\n",
      "loss: 0.932191  [ 3072/ 3200]\n",
      "loss: 1.173671  [ 3088/ 3200]\n",
      "loss: 0.826619  [ 3104/ 3200]\n",
      "loss: 0.409844  [ 3120/ 3200]\n",
      "loss: 0.975460  [ 3136/ 3200]\n",
      "loss: 0.673415  [ 3152/ 3200]\n",
      "loss: 0.715246  [ 3168/ 3200]\n",
      "loss: 0.784482  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.058912\n",
      "f1 macro averaged score: 0.576817\n",
      "Accuracy               : 58.6%\n",
      "Confusion matrix       :\n",
      "tensor([[140,  40,   6,  14],\n",
      "        [ 28,  54,  41,  77],\n",
      "        [  0,  11, 132,  57],\n",
      "        [  2,  16,  39, 143]], device='cuda:0')\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 0.936372  [    0/ 3200]\n",
      "loss: 0.971436  [   16/ 3200]\n",
      "loss: 0.913529  [   32/ 3200]\n",
      "loss: 0.907924  [   48/ 3200]\n",
      "loss: 0.762111  [   64/ 3200]\n",
      "loss: 0.706413  [   80/ 3200]\n",
      "loss: 1.163617  [   96/ 3200]\n",
      "loss: 0.615501  [  112/ 3200]\n",
      "loss: 0.900774  [  128/ 3200]\n",
      "loss: 0.763062  [  144/ 3200]\n",
      "loss: 0.762040  [  160/ 3200]\n",
      "loss: 0.649086  [  176/ 3200]\n",
      "loss: 1.144356  [  192/ 3200]\n",
      "loss: 1.108578  [  208/ 3200]\n",
      "loss: 0.877867  [  224/ 3200]\n",
      "loss: 0.569093  [  240/ 3200]\n",
      "loss: 0.969555  [  256/ 3200]\n",
      "loss: 0.912187  [  272/ 3200]\n",
      "loss: 0.697906  [  288/ 3200]\n",
      "loss: 1.021043  [  304/ 3200]\n",
      "loss: 0.686076  [  320/ 3200]\n",
      "loss: 0.631496  [  336/ 3200]\n",
      "loss: 1.034055  [  352/ 3200]\n",
      "loss: 0.726675  [  368/ 3200]\n",
      "loss: 0.653294  [  384/ 3200]\n",
      "loss: 0.585671  [  400/ 3200]\n",
      "loss: 0.548028  [  416/ 3200]\n",
      "loss: 0.557195  [  432/ 3200]\n",
      "loss: 0.614851  [  448/ 3200]\n",
      "loss: 1.014125  [  464/ 3200]\n",
      "loss: 0.772830  [  480/ 3200]\n",
      "loss: 0.939731  [  496/ 3200]\n",
      "loss: 0.727763  [  512/ 3200]\n",
      "loss: 1.017920  [  528/ 3200]\n",
      "loss: 0.986701  [  544/ 3200]\n",
      "loss: 0.830567  [  560/ 3200]\n",
      "loss: 0.964332  [  576/ 3200]\n",
      "loss: 0.867274  [  592/ 3200]\n",
      "loss: 0.794692  [  608/ 3200]\n",
      "loss: 0.641871  [  624/ 3200]\n",
      "loss: 0.813081  [  640/ 3200]\n",
      "loss: 0.895002  [  656/ 3200]\n",
      "loss: 1.220231  [  672/ 3200]\n",
      "loss: 0.797427  [  688/ 3200]\n",
      "loss: 0.775610  [  704/ 3200]\n",
      "loss: 0.649040  [  720/ 3200]\n",
      "loss: 0.836121  [  736/ 3200]\n",
      "loss: 1.104589  [  752/ 3200]\n",
      "loss: 0.554056  [  768/ 3200]\n",
      "loss: 0.984347  [  784/ 3200]\n",
      "loss: 1.013520  [  800/ 3200]\n",
      "loss: 0.821241  [  816/ 3200]\n",
      "loss: 0.510207  [  832/ 3200]\n",
      "loss: 1.198365  [  848/ 3200]\n",
      "loss: 0.744898  [  864/ 3200]\n",
      "loss: 0.918121  [  880/ 3200]\n",
      "loss: 0.915067  [  896/ 3200]\n",
      "loss: 0.700580  [  912/ 3200]\n",
      "loss: 0.842384  [  928/ 3200]\n",
      "loss: 0.831622  [  944/ 3200]\n",
      "loss: 0.622018  [  960/ 3200]\n",
      "loss: 0.952935  [  976/ 3200]\n",
      "loss: 0.980858  [  992/ 3200]\n",
      "loss: 1.316740  [ 1008/ 3200]\n",
      "loss: 0.922517  [ 1024/ 3200]\n",
      "loss: 0.650368  [ 1040/ 3200]\n",
      "loss: 0.617168  [ 1056/ 3200]\n",
      "loss: 0.766127  [ 1072/ 3200]\n",
      "loss: 1.034100  [ 1088/ 3200]\n",
      "loss: 0.557692  [ 1104/ 3200]\n",
      "loss: 0.897268  [ 1120/ 3200]\n",
      "loss: 0.621864  [ 1136/ 3200]\n",
      "loss: 0.700553  [ 1152/ 3200]\n",
      "loss: 0.934157  [ 1168/ 3200]\n",
      "loss: 0.804625  [ 1184/ 3200]\n",
      "loss: 0.884535  [ 1200/ 3200]\n",
      "loss: 1.228726  [ 1216/ 3200]\n",
      "loss: 0.545836  [ 1232/ 3200]\n",
      "loss: 1.162361  [ 1248/ 3200]\n",
      "loss: 0.992613  [ 1264/ 3200]\n",
      "loss: 0.716810  [ 1280/ 3200]\n",
      "loss: 0.755576  [ 1296/ 3200]\n",
      "loss: 0.673254  [ 1312/ 3200]\n",
      "loss: 0.661985  [ 1328/ 3200]\n",
      "loss: 0.963490  [ 1344/ 3200]\n",
      "loss: 0.624765  [ 1360/ 3200]\n",
      "loss: 0.730458  [ 1376/ 3200]\n",
      "loss: 0.723211  [ 1392/ 3200]\n",
      "loss: 0.725436  [ 1408/ 3200]\n",
      "loss: 0.813412  [ 1424/ 3200]\n",
      "loss: 0.765963  [ 1440/ 3200]\n",
      "loss: 0.770621  [ 1456/ 3200]\n",
      "loss: 0.784210  [ 1472/ 3200]\n",
      "loss: 0.942448  [ 1488/ 3200]\n",
      "loss: 0.749256  [ 1504/ 3200]\n",
      "loss: 0.954008  [ 1520/ 3200]\n",
      "loss: 0.963622  [ 1536/ 3200]\n",
      "loss: 0.902861  [ 1552/ 3200]\n",
      "loss: 0.508731  [ 1568/ 3200]\n",
      "loss: 0.858658  [ 1584/ 3200]\n",
      "loss: 1.083078  [ 1600/ 3200]\n",
      "loss: 0.796083  [ 1616/ 3200]\n",
      "loss: 0.932108  [ 1632/ 3200]\n",
      "loss: 1.045530  [ 1648/ 3200]\n",
      "loss: 0.792843  [ 1664/ 3200]\n",
      "loss: 1.000913  [ 1680/ 3200]\n",
      "loss: 0.744064  [ 1696/ 3200]\n",
      "loss: 0.524506  [ 1712/ 3200]\n",
      "loss: 0.759121  [ 1728/ 3200]\n",
      "loss: 0.753102  [ 1744/ 3200]\n",
      "loss: 0.921526  [ 1760/ 3200]\n",
      "loss: 0.792862  [ 1776/ 3200]\n",
      "loss: 0.654182  [ 1792/ 3200]\n",
      "loss: 0.737268  [ 1808/ 3200]\n",
      "loss: 0.586875  [ 1824/ 3200]\n",
      "loss: 0.742607  [ 1840/ 3200]\n",
      "loss: 0.991435  [ 1856/ 3200]\n",
      "loss: 0.805700  [ 1872/ 3200]\n",
      "loss: 0.479627  [ 1888/ 3200]\n",
      "loss: 0.606799  [ 1904/ 3200]\n",
      "loss: 0.532348  [ 1920/ 3200]\n",
      "loss: 1.017552  [ 1936/ 3200]\n",
      "loss: 0.870513  [ 1952/ 3200]\n",
      "loss: 0.544120  [ 1968/ 3200]\n",
      "loss: 0.891498  [ 1984/ 3200]\n",
      "loss: 0.706385  [ 2000/ 3200]\n",
      "loss: 0.490517  [ 2016/ 3200]\n",
      "loss: 0.922907  [ 2032/ 3200]\n",
      "loss: 0.675313  [ 2048/ 3200]\n",
      "loss: 0.639677  [ 2064/ 3200]\n",
      "loss: 0.814746  [ 2080/ 3200]\n",
      "loss: 0.785955  [ 2096/ 3200]\n",
      "loss: 1.175107  [ 2112/ 3200]\n",
      "loss: 0.671603  [ 2128/ 3200]\n",
      "loss: 0.774447  [ 2144/ 3200]\n",
      "loss: 0.806243  [ 2160/ 3200]\n",
      "loss: 0.746446  [ 2176/ 3200]\n",
      "loss: 0.995105  [ 2192/ 3200]\n",
      "loss: 1.067396  [ 2208/ 3200]\n",
      "loss: 0.702264  [ 2224/ 3200]\n",
      "loss: 0.822792  [ 2240/ 3200]\n",
      "loss: 0.852767  [ 2256/ 3200]\n",
      "loss: 0.629512  [ 2272/ 3200]\n",
      "loss: 0.643243  [ 2288/ 3200]\n",
      "loss: 0.494835  [ 2304/ 3200]\n",
      "loss: 0.993260  [ 2320/ 3200]\n",
      "loss: 0.579503  [ 2336/ 3200]\n",
      "loss: 0.595448  [ 2352/ 3200]\n",
      "loss: 0.715389  [ 2368/ 3200]\n",
      "loss: 0.741816  [ 2384/ 3200]\n",
      "loss: 0.740324  [ 2400/ 3200]\n",
      "loss: 1.190080  [ 2416/ 3200]\n",
      "loss: 0.916264  [ 2432/ 3200]\n",
      "loss: 0.984969  [ 2448/ 3200]\n",
      "loss: 0.790023  [ 2464/ 3200]\n",
      "loss: 0.712248  [ 2480/ 3200]\n",
      "loss: 0.812916  [ 2496/ 3200]\n",
      "loss: 1.065955  [ 2512/ 3200]\n",
      "loss: 0.689381  [ 2528/ 3200]\n",
      "loss: 0.775964  [ 2544/ 3200]\n",
      "loss: 0.886366  [ 2560/ 3200]\n",
      "loss: 0.756369  [ 2576/ 3200]\n",
      "loss: 0.825278  [ 2592/ 3200]\n",
      "loss: 0.664439  [ 2608/ 3200]\n",
      "loss: 1.025286  [ 2624/ 3200]\n",
      "loss: 1.103750  [ 2640/ 3200]\n",
      "loss: 0.782115  [ 2656/ 3200]\n",
      "loss: 0.806055  [ 2672/ 3200]\n",
      "loss: 0.684779  [ 2688/ 3200]\n",
      "loss: 1.065903  [ 2704/ 3200]\n",
      "loss: 0.808720  [ 2720/ 3200]\n",
      "loss: 0.795871  [ 2736/ 3200]\n",
      "loss: 0.717644  [ 2752/ 3200]\n",
      "loss: 0.578433  [ 2768/ 3200]\n",
      "loss: 0.689586  [ 2784/ 3200]\n",
      "loss: 0.882518  [ 2800/ 3200]\n",
      "loss: 0.840307  [ 2816/ 3200]\n",
      "loss: 0.766689  [ 2832/ 3200]\n",
      "loss: 0.515698  [ 2848/ 3200]\n",
      "loss: 0.611036  [ 2864/ 3200]\n",
      "loss: 0.840174  [ 2880/ 3200]\n",
      "loss: 1.155105  [ 2896/ 3200]\n",
      "loss: 0.762394  [ 2912/ 3200]\n",
      "loss: 0.906707  [ 2928/ 3200]\n",
      "loss: 1.283267  [ 2944/ 3200]\n",
      "loss: 1.008480  [ 2960/ 3200]\n",
      "loss: 0.597764  [ 2976/ 3200]\n",
      "loss: 0.756823  [ 2992/ 3200]\n",
      "loss: 0.927091  [ 3008/ 3200]\n",
      "loss: 0.757059  [ 3024/ 3200]\n",
      "loss: 0.626402  [ 3040/ 3200]\n",
      "loss: 0.493296  [ 3056/ 3200]\n",
      "loss: 0.522838  [ 3072/ 3200]\n",
      "loss: 0.744898  [ 3088/ 3200]\n",
      "loss: 0.782560  [ 3104/ 3200]\n",
      "loss: 0.504163  [ 3120/ 3200]\n",
      "loss: 1.078092  [ 3136/ 3200]\n",
      "loss: 0.257430  [ 3152/ 3200]\n",
      "loss: 0.547427  [ 3168/ 3200]\n",
      "loss: 0.930636  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.065446\n",
      "f1 macro averaged score: 0.576349\n",
      "Accuracy               : 58.8%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  24,   0,   1],\n",
      "        [ 57, 108,   3,  32],\n",
      "        [  6,  62,  66,  66],\n",
      "        [ 12,  56,  11, 121]], device='cuda:0')\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.683599  [    0/ 3200]\n",
      "loss: 0.797798  [   16/ 3200]\n",
      "loss: 1.018046  [   32/ 3200]\n",
      "loss: 0.539858  [   48/ 3200]\n",
      "loss: 0.706139  [   64/ 3200]\n",
      "loss: 0.682366  [   80/ 3200]\n",
      "loss: 0.860093  [   96/ 3200]\n",
      "loss: 0.701490  [  112/ 3200]\n",
      "loss: 0.908988  [  128/ 3200]\n",
      "loss: 0.650447  [  144/ 3200]\n",
      "loss: 0.658072  [  160/ 3200]\n",
      "loss: 0.836877  [  176/ 3200]\n",
      "loss: 1.165553  [  192/ 3200]\n",
      "loss: 1.175067  [  208/ 3200]\n",
      "loss: 0.893622  [  224/ 3200]\n",
      "loss: 0.870415  [  240/ 3200]\n",
      "loss: 1.022962  [  256/ 3200]\n",
      "loss: 0.774971  [  272/ 3200]\n",
      "loss: 0.947353  [  288/ 3200]\n",
      "loss: 1.004895  [  304/ 3200]\n",
      "loss: 0.888272  [  320/ 3200]\n",
      "loss: 1.087422  [  336/ 3200]\n",
      "loss: 0.704851  [  352/ 3200]\n",
      "loss: 0.530151  [  368/ 3200]\n",
      "loss: 0.791521  [  384/ 3200]\n",
      "loss: 0.754257  [  400/ 3200]\n",
      "loss: 1.269456  [  416/ 3200]\n",
      "loss: 1.138676  [  432/ 3200]\n",
      "loss: 0.812675  [  448/ 3200]\n",
      "loss: 0.621154  [  464/ 3200]\n",
      "loss: 0.621516  [  480/ 3200]\n",
      "loss: 0.805587  [  496/ 3200]\n",
      "loss: 0.636854  [  512/ 3200]\n",
      "loss: 0.649012  [  528/ 3200]\n",
      "loss: 0.676010  [  544/ 3200]\n",
      "loss: 0.409174  [  560/ 3200]\n",
      "loss: 0.487121  [  576/ 3200]\n",
      "loss: 0.846229  [  592/ 3200]\n",
      "loss: 0.905710  [  608/ 3200]\n",
      "loss: 0.842047  [  624/ 3200]\n",
      "loss: 0.722798  [  640/ 3200]\n",
      "loss: 0.940750  [  656/ 3200]\n",
      "loss: 0.905125  [  672/ 3200]\n",
      "loss: 1.242702  [  688/ 3200]\n",
      "loss: 0.554693  [  704/ 3200]\n",
      "loss: 0.895095  [  720/ 3200]\n",
      "loss: 0.535676  [  736/ 3200]\n",
      "loss: 1.022645  [  752/ 3200]\n",
      "loss: 0.854658  [  768/ 3200]\n",
      "loss: 0.626645  [  784/ 3200]\n",
      "loss: 0.750663  [  800/ 3200]\n",
      "loss: 0.741479  [  816/ 3200]\n",
      "loss: 0.543392  [  832/ 3200]\n",
      "loss: 0.543901  [  848/ 3200]\n",
      "loss: 0.816671  [  864/ 3200]\n",
      "loss: 1.133818  [  880/ 3200]\n",
      "loss: 0.831673  [  896/ 3200]\n",
      "loss: 0.877295  [  912/ 3200]\n",
      "loss: 1.020979  [  928/ 3200]\n",
      "loss: 0.585642  [  944/ 3200]\n",
      "loss: 1.032719  [  960/ 3200]\n",
      "loss: 0.870284  [  976/ 3200]\n",
      "loss: 0.635514  [  992/ 3200]\n",
      "loss: 0.893015  [ 1008/ 3200]\n",
      "loss: 0.789642  [ 1024/ 3200]\n",
      "loss: 0.687995  [ 1040/ 3200]\n",
      "loss: 1.000945  [ 1056/ 3200]\n",
      "loss: 0.667389  [ 1072/ 3200]\n",
      "loss: 0.623641  [ 1088/ 3200]\n",
      "loss: 0.834826  [ 1104/ 3200]\n",
      "loss: 1.531165  [ 1120/ 3200]\n",
      "loss: 1.069151  [ 1136/ 3200]\n",
      "loss: 0.705636  [ 1152/ 3200]\n",
      "loss: 0.811624  [ 1168/ 3200]\n",
      "loss: 0.800686  [ 1184/ 3200]\n",
      "loss: 0.420543  [ 1200/ 3200]\n",
      "loss: 0.625253  [ 1216/ 3200]\n",
      "loss: 0.654951  [ 1232/ 3200]\n",
      "loss: 0.618018  [ 1248/ 3200]\n",
      "loss: 0.729847  [ 1264/ 3200]\n",
      "loss: 0.600998  [ 1280/ 3200]\n",
      "loss: 1.018551  [ 1296/ 3200]\n",
      "loss: 0.622581  [ 1312/ 3200]\n",
      "loss: 1.139655  [ 1328/ 3200]\n",
      "loss: 0.712094  [ 1344/ 3200]\n",
      "loss: 0.635795  [ 1360/ 3200]\n",
      "loss: 1.128612  [ 1376/ 3200]\n",
      "loss: 0.872871  [ 1392/ 3200]\n",
      "loss: 0.787862  [ 1408/ 3200]\n",
      "loss: 0.550515  [ 1424/ 3200]\n",
      "loss: 0.658548  [ 1440/ 3200]\n",
      "loss: 0.703032  [ 1456/ 3200]\n",
      "loss: 0.764631  [ 1472/ 3200]\n",
      "loss: 1.077426  [ 1488/ 3200]\n",
      "loss: 0.990362  [ 1504/ 3200]\n",
      "loss: 0.821758  [ 1520/ 3200]\n",
      "loss: 0.884842  [ 1536/ 3200]\n",
      "loss: 0.757028  [ 1552/ 3200]\n",
      "loss: 1.312010  [ 1568/ 3200]\n",
      "loss: 0.860868  [ 1584/ 3200]\n",
      "loss: 0.631418  [ 1600/ 3200]\n",
      "loss: 0.550460  [ 1616/ 3200]\n",
      "loss: 0.782624  [ 1632/ 3200]\n",
      "loss: 0.561993  [ 1648/ 3200]\n",
      "loss: 0.865269  [ 1664/ 3200]\n",
      "loss: 0.893801  [ 1680/ 3200]\n",
      "loss: 0.602219  [ 1696/ 3200]\n",
      "loss: 0.681469  [ 1712/ 3200]\n",
      "loss: 0.937739  [ 1728/ 3200]\n",
      "loss: 1.101290  [ 1744/ 3200]\n",
      "loss: 0.506931  [ 1760/ 3200]\n",
      "loss: 0.621568  [ 1776/ 3200]\n",
      "loss: 0.832992  [ 1792/ 3200]\n",
      "loss: 1.233301  [ 1808/ 3200]\n",
      "loss: 1.140302  [ 1824/ 3200]\n",
      "loss: 0.610189  [ 1840/ 3200]\n",
      "loss: 0.931553  [ 1856/ 3200]\n",
      "loss: 0.654930  [ 1872/ 3200]\n",
      "loss: 0.895799  [ 1888/ 3200]\n",
      "loss: 0.721310  [ 1904/ 3200]\n",
      "loss: 1.182218  [ 1920/ 3200]\n",
      "loss: 1.033245  [ 1936/ 3200]\n",
      "loss: 0.995803  [ 1952/ 3200]\n",
      "loss: 0.942278  [ 1968/ 3200]\n",
      "loss: 0.763238  [ 1984/ 3200]\n",
      "loss: 0.848791  [ 2000/ 3200]\n",
      "loss: 0.737643  [ 2016/ 3200]\n",
      "loss: 0.623801  [ 2032/ 3200]\n",
      "loss: 0.838266  [ 2048/ 3200]\n",
      "loss: 0.547673  [ 2064/ 3200]\n",
      "loss: 0.680630  [ 2080/ 3200]\n",
      "loss: 0.716335  [ 2096/ 3200]\n",
      "loss: 0.533622  [ 2112/ 3200]\n",
      "loss: 0.862561  [ 2128/ 3200]\n",
      "loss: 0.857221  [ 2144/ 3200]\n",
      "loss: 0.467794  [ 2160/ 3200]\n",
      "loss: 1.246109  [ 2176/ 3200]\n",
      "loss: 0.657615  [ 2192/ 3200]\n",
      "loss: 1.264075  [ 2208/ 3200]\n",
      "loss: 1.013104  [ 2224/ 3200]\n",
      "loss: 0.599241  [ 2240/ 3200]\n",
      "loss: 0.805609  [ 2256/ 3200]\n",
      "loss: 0.597529  [ 2272/ 3200]\n",
      "loss: 0.945967  [ 2288/ 3200]\n",
      "loss: 0.962169  [ 2304/ 3200]\n",
      "loss: 0.774906  [ 2320/ 3200]\n",
      "loss: 0.580841  [ 2336/ 3200]\n",
      "loss: 0.706693  [ 2352/ 3200]\n",
      "loss: 0.750574  [ 2368/ 3200]\n",
      "loss: 0.412816  [ 2384/ 3200]\n",
      "loss: 0.624661  [ 2400/ 3200]\n",
      "loss: 0.453245  [ 2416/ 3200]\n",
      "loss: 0.447730  [ 2432/ 3200]\n",
      "loss: 1.543103  [ 2448/ 3200]\n",
      "loss: 0.780314  [ 2464/ 3200]\n",
      "loss: 0.629006  [ 2480/ 3200]\n",
      "loss: 1.000565  [ 2496/ 3200]\n",
      "loss: 0.892137  [ 2512/ 3200]\n",
      "loss: 0.757522  [ 2528/ 3200]\n",
      "loss: 0.918841  [ 2544/ 3200]\n",
      "loss: 0.612781  [ 2560/ 3200]\n",
      "loss: 0.683053  [ 2576/ 3200]\n",
      "loss: 0.594115  [ 2592/ 3200]\n",
      "loss: 0.972486  [ 2608/ 3200]\n",
      "loss: 0.748356  [ 2624/ 3200]\n",
      "loss: 0.791658  [ 2640/ 3200]\n",
      "loss: 0.444307  [ 2656/ 3200]\n",
      "loss: 0.626990  [ 2672/ 3200]\n",
      "loss: 0.669592  [ 2688/ 3200]\n",
      "loss: 0.661250  [ 2704/ 3200]\n",
      "loss: 0.719172  [ 2720/ 3200]\n",
      "loss: 0.589626  [ 2736/ 3200]\n",
      "loss: 0.637818  [ 2752/ 3200]\n",
      "loss: 0.768369  [ 2768/ 3200]\n",
      "loss: 0.878398  [ 2784/ 3200]\n",
      "loss: 0.758370  [ 2800/ 3200]\n",
      "loss: 1.153941  [ 2816/ 3200]\n",
      "loss: 0.731310  [ 2832/ 3200]\n",
      "loss: 0.647510  [ 2848/ 3200]\n",
      "loss: 0.656015  [ 2864/ 3200]\n",
      "loss: 0.821709  [ 2880/ 3200]\n",
      "loss: 1.087514  [ 2896/ 3200]\n",
      "loss: 0.959550  [ 2912/ 3200]\n",
      "loss: 0.873497  [ 2928/ 3200]\n",
      "loss: 0.561592  [ 2944/ 3200]\n",
      "loss: 0.665188  [ 2960/ 3200]\n",
      "loss: 0.883010  [ 2976/ 3200]\n",
      "loss: 1.252086  [ 2992/ 3200]\n",
      "loss: 0.656466  [ 3008/ 3200]\n",
      "loss: 0.739438  [ 3024/ 3200]\n",
      "loss: 1.166779  [ 3040/ 3200]\n",
      "loss: 0.711965  [ 3056/ 3200]\n",
      "loss: 0.781370  [ 3072/ 3200]\n",
      "loss: 0.899225  [ 3088/ 3200]\n",
      "loss: 0.801586  [ 3104/ 3200]\n",
      "loss: 0.687360  [ 3120/ 3200]\n",
      "loss: 0.764641  [ 3136/ 3200]\n",
      "loss: 0.575955  [ 3152/ 3200]\n",
      "loss: 0.791014  [ 3168/ 3200]\n",
      "loss: 0.930238  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.056238\n",
      "f1 macro averaged score: 0.635066\n",
      "Accuracy               : 63.5%\n",
      "Confusion matrix       :\n",
      "tensor([[169,  27,   2,   2],\n",
      "        [ 41, 114,  11,  34],\n",
      "        [  2,  36, 113,  49],\n",
      "        [  6,  54,  28, 112]], device='cuda:0')\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 0.864601  [    0/ 3200]\n",
      "loss: 0.587364  [   16/ 3200]\n",
      "loss: 0.975258  [   32/ 3200]\n",
      "loss: 1.015894  [   48/ 3200]\n",
      "loss: 0.878735  [   64/ 3200]\n",
      "loss: 0.774334  [   80/ 3200]\n",
      "loss: 0.814419  [   96/ 3200]\n",
      "loss: 0.329562  [  112/ 3200]\n",
      "loss: 0.521576  [  128/ 3200]\n",
      "loss: 0.825660  [  144/ 3200]\n",
      "loss: 0.789091  [  160/ 3200]\n",
      "loss: 0.799755  [  176/ 3200]\n",
      "loss: 0.424377  [  192/ 3200]\n",
      "loss: 0.551502  [  208/ 3200]\n",
      "loss: 0.859237  [  224/ 3200]\n",
      "loss: 0.739436  [  240/ 3200]\n",
      "loss: 1.159400  [  256/ 3200]\n",
      "loss: 0.713169  [  272/ 3200]\n",
      "loss: 0.726501  [  288/ 3200]\n",
      "loss: 0.814482  [  304/ 3200]\n",
      "loss: 0.743018  [  320/ 3200]\n",
      "loss: 1.010072  [  336/ 3200]\n",
      "loss: 0.791687  [  352/ 3200]\n",
      "loss: 0.578912  [  368/ 3200]\n",
      "loss: 0.884377  [  384/ 3200]\n",
      "loss: 0.500469  [  400/ 3200]\n",
      "loss: 0.767792  [  416/ 3200]\n",
      "loss: 0.658028  [  432/ 3200]\n",
      "loss: 1.170812  [  448/ 3200]\n",
      "loss: 0.763649  [  464/ 3200]\n",
      "loss: 0.487686  [  480/ 3200]\n",
      "loss: 0.809200  [  496/ 3200]\n",
      "loss: 0.489894  [  512/ 3200]\n",
      "loss: 0.857131  [  528/ 3200]\n",
      "loss: 1.077114  [  544/ 3200]\n",
      "loss: 0.355201  [  560/ 3200]\n",
      "loss: 0.898476  [  576/ 3200]\n",
      "loss: 0.856900  [  592/ 3200]\n",
      "loss: 0.950134  [  608/ 3200]\n",
      "loss: 0.607746  [  624/ 3200]\n",
      "loss: 0.840743  [  640/ 3200]\n",
      "loss: 0.613929  [  656/ 3200]\n",
      "loss: 0.635028  [  672/ 3200]\n",
      "loss: 0.782686  [  688/ 3200]\n",
      "loss: 0.856145  [  704/ 3200]\n",
      "loss: 0.536106  [  720/ 3200]\n",
      "loss: 0.559459  [  736/ 3200]\n",
      "loss: 0.905903  [  752/ 3200]\n",
      "loss: 0.500514  [  768/ 3200]\n",
      "loss: 1.349313  [  784/ 3200]\n",
      "loss: 0.885585  [  800/ 3200]\n",
      "loss: 0.652225  [  816/ 3200]\n",
      "loss: 0.817780  [  832/ 3200]\n",
      "loss: 0.680629  [  848/ 3200]\n",
      "loss: 1.004733  [  864/ 3200]\n",
      "loss: 0.721926  [  880/ 3200]\n",
      "loss: 0.516787  [  896/ 3200]\n",
      "loss: 0.574962  [  912/ 3200]\n",
      "loss: 0.700177  [  928/ 3200]\n",
      "loss: 0.996187  [  944/ 3200]\n",
      "loss: 0.491579  [  960/ 3200]\n",
      "loss: 0.762301  [  976/ 3200]\n",
      "loss: 0.404631  [  992/ 3200]\n",
      "loss: 0.761261  [ 1008/ 3200]\n",
      "loss: 0.896370  [ 1024/ 3200]\n",
      "loss: 0.825335  [ 1040/ 3200]\n",
      "loss: 0.701518  [ 1056/ 3200]\n",
      "loss: 1.429896  [ 1072/ 3200]\n",
      "loss: 0.716581  [ 1088/ 3200]\n",
      "loss: 0.782135  [ 1104/ 3200]\n",
      "loss: 0.816519  [ 1120/ 3200]\n",
      "loss: 0.959264  [ 1136/ 3200]\n",
      "loss: 0.790472  [ 1152/ 3200]\n",
      "loss: 0.629263  [ 1168/ 3200]\n",
      "loss: 1.029258  [ 1184/ 3200]\n",
      "loss: 0.939106  [ 1200/ 3200]\n",
      "loss: 0.571225  [ 1216/ 3200]\n",
      "loss: 0.581440  [ 1232/ 3200]\n",
      "loss: 0.726647  [ 1248/ 3200]\n",
      "loss: 0.620989  [ 1264/ 3200]\n",
      "loss: 0.816330  [ 1280/ 3200]\n",
      "loss: 0.983716  [ 1296/ 3200]\n",
      "loss: 0.668967  [ 1312/ 3200]\n",
      "loss: 0.707976  [ 1328/ 3200]\n",
      "loss: 0.736173  [ 1344/ 3200]\n",
      "loss: 0.779041  [ 1360/ 3200]\n",
      "loss: 0.977676  [ 1376/ 3200]\n",
      "loss: 0.510451  [ 1392/ 3200]\n",
      "loss: 0.628314  [ 1408/ 3200]\n",
      "loss: 0.606130  [ 1424/ 3200]\n",
      "loss: 1.055788  [ 1440/ 3200]\n",
      "loss: 0.519313  [ 1456/ 3200]\n",
      "loss: 0.581190  [ 1472/ 3200]\n",
      "loss: 0.864739  [ 1488/ 3200]\n",
      "loss: 0.882480  [ 1504/ 3200]\n",
      "loss: 0.546882  [ 1520/ 3200]\n",
      "loss: 0.538289  [ 1536/ 3200]\n",
      "loss: 0.569022  [ 1552/ 3200]\n",
      "loss: 0.766438  [ 1568/ 3200]\n",
      "loss: 0.935276  [ 1584/ 3200]\n",
      "loss: 0.753436  [ 1600/ 3200]\n",
      "loss: 0.637535  [ 1616/ 3200]\n",
      "loss: 0.806230  [ 1632/ 3200]\n",
      "loss: 0.880544  [ 1648/ 3200]\n",
      "loss: 0.694418  [ 1664/ 3200]\n",
      "loss: 1.157905  [ 1680/ 3200]\n",
      "loss: 0.709990  [ 1696/ 3200]\n",
      "loss: 1.045958  [ 1712/ 3200]\n",
      "loss: 0.798970  [ 1728/ 3200]\n",
      "loss: 0.495912  [ 1744/ 3200]\n",
      "loss: 0.797116  [ 1760/ 3200]\n",
      "loss: 0.868788  [ 1776/ 3200]\n",
      "loss: 0.727200  [ 1792/ 3200]\n",
      "loss: 0.612495  [ 1808/ 3200]\n",
      "loss: 0.717031  [ 1824/ 3200]\n",
      "loss: 0.540040  [ 1840/ 3200]\n",
      "loss: 0.919210  [ 1856/ 3200]\n",
      "loss: 1.208736  [ 1872/ 3200]\n",
      "loss: 0.860257  [ 1888/ 3200]\n",
      "loss: 0.865751  [ 1904/ 3200]\n",
      "loss: 0.855190  [ 1920/ 3200]\n",
      "loss: 0.785287  [ 1936/ 3200]\n",
      "loss: 0.609119  [ 1952/ 3200]\n",
      "loss: 0.696706  [ 1968/ 3200]\n",
      "loss: 0.703900  [ 1984/ 3200]\n",
      "loss: 0.948352  [ 2000/ 3200]\n",
      "loss: 0.763293  [ 2016/ 3200]\n",
      "loss: 0.490131  [ 2032/ 3200]\n",
      "loss: 0.685045  [ 2048/ 3200]\n",
      "loss: 0.978660  [ 2064/ 3200]\n",
      "loss: 0.561135  [ 2080/ 3200]\n",
      "loss: 0.667976  [ 2096/ 3200]\n",
      "loss: 0.761146  [ 2112/ 3200]\n",
      "loss: 1.138156  [ 2128/ 3200]\n",
      "loss: 0.591765  [ 2144/ 3200]\n",
      "loss: 0.751519  [ 2160/ 3200]\n",
      "loss: 0.524380  [ 2176/ 3200]\n",
      "loss: 0.957448  [ 2192/ 3200]\n",
      "loss: 0.742042  [ 2208/ 3200]\n",
      "loss: 0.801238  [ 2224/ 3200]\n",
      "loss: 0.892086  [ 2240/ 3200]\n",
      "loss: 0.828503  [ 2256/ 3200]\n",
      "loss: 0.576902  [ 2272/ 3200]\n",
      "loss: 0.624610  [ 2288/ 3200]\n",
      "loss: 0.686559  [ 2304/ 3200]\n",
      "loss: 0.919419  [ 2320/ 3200]\n",
      "loss: 0.727405  [ 2336/ 3200]\n",
      "loss: 0.692689  [ 2352/ 3200]\n",
      "loss: 1.123286  [ 2368/ 3200]\n",
      "loss: 0.940026  [ 2384/ 3200]\n",
      "loss: 0.925567  [ 2400/ 3200]\n",
      "loss: 1.084379  [ 2416/ 3200]\n",
      "loss: 0.857635  [ 2432/ 3200]\n",
      "loss: 0.561579  [ 2448/ 3200]\n",
      "loss: 0.740476  [ 2464/ 3200]\n",
      "loss: 0.612404  [ 2480/ 3200]\n",
      "loss: 0.767530  [ 2496/ 3200]\n",
      "loss: 0.595094  [ 2512/ 3200]\n",
      "loss: 0.855909  [ 2528/ 3200]\n",
      "loss: 0.780273  [ 2544/ 3200]\n",
      "loss: 0.929591  [ 2560/ 3200]\n",
      "loss: 0.813341  [ 2576/ 3200]\n",
      "loss: 0.766923  [ 2592/ 3200]\n",
      "loss: 1.101394  [ 2608/ 3200]\n",
      "loss: 0.835102  [ 2624/ 3200]\n",
      "loss: 1.064494  [ 2640/ 3200]\n",
      "loss: 1.068460  [ 2656/ 3200]\n",
      "loss: 0.422971  [ 2672/ 3200]\n",
      "loss: 0.657333  [ 2688/ 3200]\n",
      "loss: 0.853135  [ 2704/ 3200]\n",
      "loss: 0.790024  [ 2720/ 3200]\n",
      "loss: 0.628859  [ 2736/ 3200]\n",
      "loss: 0.748423  [ 2752/ 3200]\n",
      "loss: 0.779409  [ 2768/ 3200]\n",
      "loss: 0.695881  [ 2784/ 3200]\n",
      "loss: 0.569938  [ 2800/ 3200]\n",
      "loss: 0.844046  [ 2816/ 3200]\n",
      "loss: 0.691085  [ 2832/ 3200]\n",
      "loss: 0.729404  [ 2848/ 3200]\n",
      "loss: 1.177138  [ 2864/ 3200]\n",
      "loss: 0.720769  [ 2880/ 3200]\n",
      "loss: 0.991438  [ 2896/ 3200]\n",
      "loss: 0.576464  [ 2912/ 3200]\n",
      "loss: 0.845120  [ 2928/ 3200]\n",
      "loss: 0.814787  [ 2944/ 3200]\n",
      "loss: 0.892877  [ 2960/ 3200]\n",
      "loss: 0.676761  [ 2976/ 3200]\n",
      "loss: 0.820491  [ 2992/ 3200]\n",
      "loss: 1.120343  [ 3008/ 3200]\n",
      "loss: 0.672403  [ 3024/ 3200]\n",
      "loss: 0.858086  [ 3040/ 3200]\n",
      "loss: 0.635775  [ 3056/ 3200]\n",
      "loss: 0.699583  [ 3072/ 3200]\n",
      "loss: 0.692666  [ 3088/ 3200]\n",
      "loss: 0.581966  [ 3104/ 3200]\n",
      "loss: 0.811805  [ 3120/ 3200]\n",
      "loss: 0.373179  [ 3136/ 3200]\n",
      "loss: 0.709785  [ 3152/ 3200]\n",
      "loss: 0.817406  [ 3168/ 3200]\n",
      "loss: 0.734514  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.055596\n",
      "f1 macro averaged score: 0.598818\n",
      "Accuracy               : 61.1%\n",
      "Confusion matrix       :\n",
      "tensor([[147,  41,   6,   6],\n",
      "        [ 33,  58,  56,  53],\n",
      "        [  1,   9, 161,  29],\n",
      "        [  2,  25,  50, 123]], device='cuda:0')\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.814376  [    0/ 3200]\n",
      "loss: 0.813570  [   16/ 3200]\n",
      "loss: 0.665218  [   32/ 3200]\n",
      "loss: 0.572066  [   48/ 3200]\n",
      "loss: 0.850500  [   64/ 3200]\n",
      "loss: 0.766956  [   80/ 3200]\n",
      "loss: 0.665849  [   96/ 3200]\n",
      "loss: 0.957337  [  112/ 3200]\n",
      "loss: 0.566342  [  128/ 3200]\n",
      "loss: 0.773970  [  144/ 3200]\n",
      "loss: 0.538469  [  160/ 3200]\n",
      "loss: 0.604877  [  176/ 3200]\n",
      "loss: 0.927728  [  192/ 3200]\n",
      "loss: 0.665211  [  208/ 3200]\n",
      "loss: 0.945109  [  224/ 3200]\n",
      "loss: 1.192331  [  240/ 3200]\n",
      "loss: 0.743387  [  256/ 3200]\n",
      "loss: 1.108510  [  272/ 3200]\n",
      "loss: 0.597300  [  288/ 3200]\n",
      "loss: 0.805644  [  304/ 3200]\n",
      "loss: 1.110727  [  320/ 3200]\n",
      "loss: 0.633897  [  336/ 3200]\n",
      "loss: 0.860783  [  352/ 3200]\n",
      "loss: 0.697461  [  368/ 3200]\n",
      "loss: 0.813999  [  384/ 3200]\n",
      "loss: 0.568805  [  400/ 3200]\n",
      "loss: 0.717389  [  416/ 3200]\n",
      "loss: 1.002879  [  432/ 3200]\n",
      "loss: 1.049637  [  448/ 3200]\n",
      "loss: 0.832856  [  464/ 3200]\n",
      "loss: 0.539701  [  480/ 3200]\n",
      "loss: 1.157409  [  496/ 3200]\n",
      "loss: 0.933402  [  512/ 3200]\n",
      "loss: 0.655477  [  528/ 3200]\n",
      "loss: 0.791474  [  544/ 3200]\n",
      "loss: 0.580475  [  560/ 3200]\n",
      "loss: 0.821706  [  576/ 3200]\n",
      "loss: 0.738142  [  592/ 3200]\n",
      "loss: 0.930460  [  608/ 3200]\n",
      "loss: 0.627431  [  624/ 3200]\n",
      "loss: 0.809253  [  640/ 3200]\n",
      "loss: 0.526253  [  656/ 3200]\n",
      "loss: 0.955708  [  672/ 3200]\n",
      "loss: 0.560448  [  688/ 3200]\n",
      "loss: 0.605669  [  704/ 3200]\n",
      "loss: 0.667075  [  720/ 3200]\n",
      "loss: 0.820245  [  736/ 3200]\n",
      "loss: 0.670638  [  752/ 3200]\n",
      "loss: 0.749874  [  768/ 3200]\n",
      "loss: 0.765185  [  784/ 3200]\n",
      "loss: 0.739633  [  800/ 3200]\n",
      "loss: 1.234153  [  816/ 3200]\n",
      "loss: 0.822611  [  832/ 3200]\n",
      "loss: 0.729413  [  848/ 3200]\n",
      "loss: 0.731279  [  864/ 3200]\n",
      "loss: 1.230469  [  880/ 3200]\n",
      "loss: 1.447006  [  896/ 3200]\n",
      "loss: 0.519723  [  912/ 3200]\n",
      "loss: 0.592106  [  928/ 3200]\n",
      "loss: 0.790927  [  944/ 3200]\n",
      "loss: 0.843022  [  960/ 3200]\n",
      "loss: 0.606930  [  976/ 3200]\n",
      "loss: 0.621582  [  992/ 3200]\n",
      "loss: 0.757379  [ 1008/ 3200]\n",
      "loss: 0.627897  [ 1024/ 3200]\n",
      "loss: 0.533709  [ 1040/ 3200]\n",
      "loss: 0.495390  [ 1056/ 3200]\n",
      "loss: 0.679194  [ 1072/ 3200]\n",
      "loss: 0.883773  [ 1088/ 3200]\n",
      "loss: 0.695754  [ 1104/ 3200]\n",
      "loss: 0.588489  [ 1120/ 3200]\n",
      "loss: 0.953575  [ 1136/ 3200]\n",
      "loss: 0.560611  [ 1152/ 3200]\n",
      "loss: 0.704406  [ 1168/ 3200]\n",
      "loss: 0.925147  [ 1184/ 3200]\n",
      "loss: 0.863508  [ 1200/ 3200]\n",
      "loss: 0.854731  [ 1216/ 3200]\n",
      "loss: 1.132386  [ 1232/ 3200]\n",
      "loss: 0.747724  [ 1248/ 3200]\n",
      "loss: 0.909855  [ 1264/ 3200]\n",
      "loss: 0.430709  [ 1280/ 3200]\n",
      "loss: 0.670820  [ 1296/ 3200]\n",
      "loss: 0.628443  [ 1312/ 3200]\n",
      "loss: 0.639372  [ 1328/ 3200]\n",
      "loss: 0.783744  [ 1344/ 3200]\n",
      "loss: 0.742006  [ 1360/ 3200]\n",
      "loss: 0.679684  [ 1376/ 3200]\n",
      "loss: 0.756617  [ 1392/ 3200]\n",
      "loss: 0.794061  [ 1408/ 3200]\n",
      "loss: 0.626143  [ 1424/ 3200]\n",
      "loss: 0.903180  [ 1440/ 3200]\n",
      "loss: 0.678472  [ 1456/ 3200]\n",
      "loss: 0.666026  [ 1472/ 3200]\n",
      "loss: 0.652322  [ 1488/ 3200]\n",
      "loss: 0.740239  [ 1504/ 3200]\n",
      "loss: 0.601242  [ 1520/ 3200]\n",
      "loss: 0.989209  [ 1536/ 3200]\n",
      "loss: 0.887611  [ 1552/ 3200]\n",
      "loss: 0.634248  [ 1568/ 3200]\n",
      "loss: 0.937971  [ 1584/ 3200]\n",
      "loss: 0.612671  [ 1600/ 3200]\n",
      "loss: 0.690327  [ 1616/ 3200]\n",
      "loss: 0.830291  [ 1632/ 3200]\n",
      "loss: 0.625802  [ 1648/ 3200]\n",
      "loss: 0.450906  [ 1664/ 3200]\n",
      "loss: 0.737086  [ 1680/ 3200]\n",
      "loss: 0.967226  [ 1696/ 3200]\n",
      "loss: 0.495510  [ 1712/ 3200]\n",
      "loss: 0.648335  [ 1728/ 3200]\n",
      "loss: 0.659002  [ 1744/ 3200]\n",
      "loss: 0.619394  [ 1760/ 3200]\n",
      "loss: 0.646957  [ 1776/ 3200]\n",
      "loss: 0.640510  [ 1792/ 3200]\n",
      "loss: 0.563243  [ 1808/ 3200]\n",
      "loss: 0.626219  [ 1824/ 3200]\n",
      "loss: 1.082875  [ 1840/ 3200]\n",
      "loss: 0.950143  [ 1856/ 3200]\n",
      "loss: 0.951847  [ 1872/ 3200]\n",
      "loss: 0.664315  [ 1888/ 3200]\n",
      "loss: 0.677560  [ 1904/ 3200]\n",
      "loss: 0.634656  [ 1920/ 3200]\n",
      "loss: 0.503504  [ 1936/ 3200]\n",
      "loss: 0.857564  [ 1952/ 3200]\n",
      "loss: 0.505431  [ 1968/ 3200]\n",
      "loss: 1.047073  [ 1984/ 3200]\n",
      "loss: 0.678666  [ 2000/ 3200]\n",
      "loss: 0.685487  [ 2016/ 3200]\n",
      "loss: 0.893981  [ 2032/ 3200]\n",
      "loss: 0.629154  [ 2048/ 3200]\n",
      "loss: 0.720888  [ 2064/ 3200]\n",
      "loss: 0.867194  [ 2080/ 3200]\n",
      "loss: 1.259276  [ 2096/ 3200]\n",
      "loss: 1.113361  [ 2112/ 3200]\n",
      "loss: 1.029699  [ 2128/ 3200]\n",
      "loss: 0.808581  [ 2144/ 3200]\n",
      "loss: 0.645750  [ 2160/ 3200]\n",
      "loss: 0.934043  [ 2176/ 3200]\n",
      "loss: 0.680346  [ 2192/ 3200]\n",
      "loss: 0.506799  [ 2208/ 3200]\n",
      "loss: 0.698708  [ 2224/ 3200]\n",
      "loss: 0.648676  [ 2240/ 3200]\n",
      "loss: 0.587639  [ 2256/ 3200]\n",
      "loss: 0.798435  [ 2272/ 3200]\n",
      "loss: 0.819084  [ 2288/ 3200]\n",
      "loss: 0.647398  [ 2304/ 3200]\n",
      "loss: 0.746010  [ 2320/ 3200]\n",
      "loss: 0.595559  [ 2336/ 3200]\n",
      "loss: 1.085704  [ 2352/ 3200]\n",
      "loss: 0.808702  [ 2368/ 3200]\n",
      "loss: 0.877664  [ 2384/ 3200]\n",
      "loss: 0.642151  [ 2400/ 3200]\n",
      "loss: 0.700793  [ 2416/ 3200]\n",
      "loss: 1.101948  [ 2432/ 3200]\n",
      "loss: 0.554180  [ 2448/ 3200]\n",
      "loss: 0.975069  [ 2464/ 3200]\n",
      "loss: 0.798144  [ 2480/ 3200]\n",
      "loss: 0.717526  [ 2496/ 3200]\n",
      "loss: 0.799775  [ 2512/ 3200]\n",
      "loss: 0.609023  [ 2528/ 3200]\n",
      "loss: 0.639373  [ 2544/ 3200]\n",
      "loss: 0.604458  [ 2560/ 3200]\n",
      "loss: 0.674822  [ 2576/ 3200]\n",
      "loss: 1.050956  [ 2592/ 3200]\n",
      "loss: 0.726732  [ 2608/ 3200]\n",
      "loss: 0.450490  [ 2624/ 3200]\n",
      "loss: 0.960655  [ 2640/ 3200]\n",
      "loss: 0.682209  [ 2656/ 3200]\n",
      "loss: 0.789081  [ 2672/ 3200]\n",
      "loss: 0.461414  [ 2688/ 3200]\n",
      "loss: 0.719480  [ 2704/ 3200]\n",
      "loss: 0.916176  [ 2720/ 3200]\n",
      "loss: 0.734880  [ 2736/ 3200]\n",
      "loss: 0.867923  [ 2752/ 3200]\n",
      "loss: 0.718034  [ 2768/ 3200]\n",
      "loss: 1.182894  [ 2784/ 3200]\n",
      "loss: 1.025506  [ 2800/ 3200]\n",
      "loss: 0.844605  [ 2816/ 3200]\n",
      "loss: 1.029341  [ 2832/ 3200]\n",
      "loss: 0.632318  [ 2848/ 3200]\n",
      "loss: 1.031156  [ 2864/ 3200]\n",
      "loss: 0.531666  [ 2880/ 3200]\n",
      "loss: 0.603918  [ 2896/ 3200]\n",
      "loss: 0.492764  [ 2912/ 3200]\n",
      "loss: 0.969039  [ 2928/ 3200]\n",
      "loss: 0.824786  [ 2944/ 3200]\n",
      "loss: 0.786150  [ 2960/ 3200]\n",
      "loss: 0.743881  [ 2976/ 3200]\n",
      "loss: 0.957246  [ 2992/ 3200]\n",
      "loss: 0.646790  [ 3008/ 3200]\n",
      "loss: 0.815318  [ 3024/ 3200]\n",
      "loss: 0.780484  [ 3040/ 3200]\n",
      "loss: 1.075470  [ 3056/ 3200]\n",
      "loss: 0.716263  [ 3072/ 3200]\n",
      "loss: 0.856265  [ 3088/ 3200]\n",
      "loss: 0.951129  [ 3104/ 3200]\n",
      "loss: 0.700467  [ 3120/ 3200]\n",
      "loss: 0.562214  [ 3136/ 3200]\n",
      "loss: 0.875331  [ 3152/ 3200]\n",
      "loss: 0.641849  [ 3168/ 3200]\n",
      "loss: 0.826565  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.054905\n",
      "f1 macro averaged score: 0.605496\n",
      "Accuracy               : 61.8%\n",
      "Confusion matrix       :\n",
      "tensor([[181,  14,   2,   3],\n",
      "        [ 64,  74,  21,  41],\n",
      "        [  6,  18, 119,  57],\n",
      "        [ 13,  38,  29, 120]], device='cuda:0')\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.694720  [    0/ 3200]\n",
      "loss: 0.742062  [   16/ 3200]\n",
      "loss: 0.636975  [   32/ 3200]\n",
      "loss: 0.815543  [   48/ 3200]\n",
      "loss: 0.981347  [   64/ 3200]\n",
      "loss: 0.934371  [   80/ 3200]\n",
      "loss: 0.753520  [   96/ 3200]\n",
      "loss: 0.777785  [  112/ 3200]\n",
      "loss: 0.740949  [  128/ 3200]\n",
      "loss: 0.765519  [  144/ 3200]\n",
      "loss: 0.936492  [  160/ 3200]\n",
      "loss: 0.565594  [  176/ 3200]\n",
      "loss: 0.430687  [  192/ 3200]\n",
      "loss: 1.024872  [  208/ 3200]\n",
      "loss: 0.768235  [  224/ 3200]\n",
      "loss: 0.433005  [  240/ 3200]\n",
      "loss: 0.700189  [  256/ 3200]\n",
      "loss: 1.001604  [  272/ 3200]\n",
      "loss: 0.773099  [  288/ 3200]\n",
      "loss: 0.811275  [  304/ 3200]\n",
      "loss: 0.716256  [  320/ 3200]\n",
      "loss: 1.042971  [  336/ 3200]\n",
      "loss: 0.645022  [  352/ 3200]\n",
      "loss: 0.822517  [  368/ 3200]\n",
      "loss: 0.853619  [  384/ 3200]\n",
      "loss: 0.523119  [  400/ 3200]\n",
      "loss: 0.651876  [  416/ 3200]\n",
      "loss: 0.635925  [  432/ 3200]\n",
      "loss: 0.569345  [  448/ 3200]\n",
      "loss: 0.834812  [  464/ 3200]\n",
      "loss: 0.926188  [  480/ 3200]\n",
      "loss: 1.027219  [  496/ 3200]\n",
      "loss: 0.738685  [  512/ 3200]\n",
      "loss: 0.783656  [  528/ 3200]\n",
      "loss: 0.618672  [  544/ 3200]\n",
      "loss: 0.645543  [  560/ 3200]\n",
      "loss: 0.575128  [  576/ 3200]\n",
      "loss: 0.717442  [  592/ 3200]\n",
      "loss: 0.615772  [  608/ 3200]\n",
      "loss: 0.803101  [  624/ 3200]\n",
      "loss: 0.651042  [  640/ 3200]\n",
      "loss: 0.792124  [  656/ 3200]\n",
      "loss: 0.592904  [  672/ 3200]\n",
      "loss: 1.181224  [  688/ 3200]\n",
      "loss: 0.827664  [  704/ 3200]\n",
      "loss: 0.849114  [  720/ 3200]\n",
      "loss: 0.716879  [  736/ 3200]\n",
      "loss: 0.713714  [  752/ 3200]\n",
      "loss: 0.528769  [  768/ 3200]\n",
      "loss: 0.526200  [  784/ 3200]\n",
      "loss: 0.716683  [  800/ 3200]\n",
      "loss: 1.021048  [  816/ 3200]\n",
      "loss: 0.897158  [  832/ 3200]\n",
      "loss: 0.699025  [  848/ 3200]\n",
      "loss: 0.618943  [  864/ 3200]\n",
      "loss: 0.394893  [  880/ 3200]\n",
      "loss: 0.537935  [  896/ 3200]\n",
      "loss: 0.480777  [  912/ 3200]\n",
      "loss: 0.620192  [  928/ 3200]\n",
      "loss: 0.699433  [  944/ 3200]\n",
      "loss: 0.447807  [  960/ 3200]\n",
      "loss: 0.753538  [  976/ 3200]\n",
      "loss: 0.796371  [  992/ 3200]\n",
      "loss: 0.869495  [ 1008/ 3200]\n",
      "loss: 0.911426  [ 1024/ 3200]\n",
      "loss: 0.586610  [ 1040/ 3200]\n",
      "loss: 0.694107  [ 1056/ 3200]\n",
      "loss: 0.596763  [ 1072/ 3200]\n",
      "loss: 0.742038  [ 1088/ 3200]\n",
      "loss: 0.643553  [ 1104/ 3200]\n",
      "loss: 1.032387  [ 1120/ 3200]\n",
      "loss: 0.663648  [ 1136/ 3200]\n",
      "loss: 0.927017  [ 1152/ 3200]\n",
      "loss: 0.949498  [ 1168/ 3200]\n",
      "loss: 1.010969  [ 1184/ 3200]\n",
      "loss: 0.713792  [ 1200/ 3200]\n",
      "loss: 0.804951  [ 1216/ 3200]\n",
      "loss: 0.633940  [ 1232/ 3200]\n",
      "loss: 0.883749  [ 1248/ 3200]\n",
      "loss: 0.778845  [ 1264/ 3200]\n",
      "loss: 0.740413  [ 1280/ 3200]\n",
      "loss: 0.908925  [ 1296/ 3200]\n",
      "loss: 0.730977  [ 1312/ 3200]\n",
      "loss: 0.684294  [ 1328/ 3200]\n",
      "loss: 0.597576  [ 1344/ 3200]\n",
      "loss: 0.872745  [ 1360/ 3200]\n",
      "loss: 0.736898  [ 1376/ 3200]\n",
      "loss: 1.002071  [ 1392/ 3200]\n",
      "loss: 0.511125  [ 1408/ 3200]\n",
      "loss: 0.553461  [ 1424/ 3200]\n",
      "loss: 0.794016  [ 1440/ 3200]\n",
      "loss: 1.122301  [ 1456/ 3200]\n",
      "loss: 0.851969  [ 1472/ 3200]\n",
      "loss: 1.041222  [ 1488/ 3200]\n",
      "loss: 0.574584  [ 1504/ 3200]\n",
      "loss: 0.665117  [ 1520/ 3200]\n",
      "loss: 0.886618  [ 1536/ 3200]\n",
      "loss: 0.886738  [ 1552/ 3200]\n",
      "loss: 0.843738  [ 1568/ 3200]\n",
      "loss: 0.413802  [ 1584/ 3200]\n",
      "loss: 0.786065  [ 1600/ 3200]\n",
      "loss: 1.107172  [ 1616/ 3200]\n",
      "loss: 0.942233  [ 1632/ 3200]\n",
      "loss: 0.972043  [ 1648/ 3200]\n",
      "loss: 0.758441  [ 1664/ 3200]\n",
      "loss: 0.648939  [ 1680/ 3200]\n",
      "loss: 0.643050  [ 1696/ 3200]\n",
      "loss: 1.077723  [ 1712/ 3200]\n",
      "loss: 0.852408  [ 1728/ 3200]\n",
      "loss: 0.349076  [ 1744/ 3200]\n",
      "loss: 0.748585  [ 1760/ 3200]\n",
      "loss: 0.654007  [ 1776/ 3200]\n",
      "loss: 0.816277  [ 1792/ 3200]\n",
      "loss: 0.681093  [ 1808/ 3200]\n",
      "loss: 0.526637  [ 1824/ 3200]\n",
      "loss: 0.600895  [ 1840/ 3200]\n",
      "loss: 0.607289  [ 1856/ 3200]\n",
      "loss: 0.988717  [ 1872/ 3200]\n",
      "loss: 0.559191  [ 1888/ 3200]\n",
      "loss: 0.841341  [ 1904/ 3200]\n",
      "loss: 0.619288  [ 1920/ 3200]\n",
      "loss: 0.511721  [ 1936/ 3200]\n",
      "loss: 0.716197  [ 1952/ 3200]\n",
      "loss: 0.825609  [ 1968/ 3200]\n",
      "loss: 0.661418  [ 1984/ 3200]\n",
      "loss: 0.346461  [ 2000/ 3200]\n",
      "loss: 0.451150  [ 2016/ 3200]\n",
      "loss: 0.631164  [ 2032/ 3200]\n",
      "loss: 0.529987  [ 2048/ 3200]\n",
      "loss: 0.824019  [ 2064/ 3200]\n",
      "loss: 0.731826  [ 2080/ 3200]\n",
      "loss: 0.470896  [ 2096/ 3200]\n",
      "loss: 0.987938  [ 2112/ 3200]\n",
      "loss: 0.886129  [ 2128/ 3200]\n",
      "loss: 0.791433  [ 2144/ 3200]\n",
      "loss: 0.713901  [ 2160/ 3200]\n",
      "loss: 0.585009  [ 2176/ 3200]\n",
      "loss: 0.773816  [ 2192/ 3200]\n",
      "loss: 0.756400  [ 2208/ 3200]\n",
      "loss: 0.612527  [ 2224/ 3200]\n",
      "loss: 0.797074  [ 2240/ 3200]\n",
      "loss: 0.610214  [ 2256/ 3200]\n",
      "loss: 0.608898  [ 2272/ 3200]\n",
      "loss: 0.770284  [ 2288/ 3200]\n",
      "loss: 0.769454  [ 2304/ 3200]\n",
      "loss: 0.625235  [ 2320/ 3200]\n",
      "loss: 0.724341  [ 2336/ 3200]\n",
      "loss: 0.578121  [ 2352/ 3200]\n",
      "loss: 0.603733  [ 2368/ 3200]\n",
      "loss: 1.150130  [ 2384/ 3200]\n",
      "loss: 0.828353  [ 2400/ 3200]\n",
      "loss: 0.730339  [ 2416/ 3200]\n",
      "loss: 1.374136  [ 2432/ 3200]\n",
      "loss: 0.778494  [ 2448/ 3200]\n",
      "loss: 0.993212  [ 2464/ 3200]\n",
      "loss: 0.948854  [ 2480/ 3200]\n",
      "loss: 0.737573  [ 2496/ 3200]\n",
      "loss: 0.570751  [ 2512/ 3200]\n",
      "loss: 0.816517  [ 2528/ 3200]\n",
      "loss: 0.701902  [ 2544/ 3200]\n",
      "loss: 0.664351  [ 2560/ 3200]\n",
      "loss: 0.853573  [ 2576/ 3200]\n",
      "loss: 0.558748  [ 2592/ 3200]\n",
      "loss: 1.073534  [ 2608/ 3200]\n",
      "loss: 0.680778  [ 2624/ 3200]\n",
      "loss: 0.739766  [ 2640/ 3200]\n",
      "loss: 0.758592  [ 2656/ 3200]\n",
      "loss: 0.590504  [ 2672/ 3200]\n",
      "loss: 0.717696  [ 2688/ 3200]\n",
      "loss: 0.634505  [ 2704/ 3200]\n",
      "loss: 0.801922  [ 2720/ 3200]\n",
      "loss: 0.752214  [ 2736/ 3200]\n",
      "loss: 1.018216  [ 2752/ 3200]\n",
      "loss: 0.673731  [ 2768/ 3200]\n",
      "loss: 0.497499  [ 2784/ 3200]\n",
      "loss: 0.527999  [ 2800/ 3200]\n",
      "loss: 0.761291  [ 2816/ 3200]\n",
      "loss: 0.926048  [ 2832/ 3200]\n",
      "loss: 0.452185  [ 2848/ 3200]\n",
      "loss: 0.426285  [ 2864/ 3200]\n",
      "loss: 0.577694  [ 2880/ 3200]\n",
      "loss: 0.662066  [ 2896/ 3200]\n",
      "loss: 0.867814  [ 2912/ 3200]\n",
      "loss: 0.506789  [ 2928/ 3200]\n",
      "loss: 1.028576  [ 2944/ 3200]\n",
      "loss: 1.017053  [ 2960/ 3200]\n",
      "loss: 0.706261  [ 2976/ 3200]\n",
      "loss: 1.045217  [ 2992/ 3200]\n",
      "loss: 0.570653  [ 3008/ 3200]\n",
      "loss: 1.106976  [ 3024/ 3200]\n",
      "loss: 0.904994  [ 3040/ 3200]\n",
      "loss: 0.882121  [ 3056/ 3200]\n",
      "loss: 0.596495  [ 3072/ 3200]\n",
      "loss: 0.639054  [ 3088/ 3200]\n",
      "loss: 0.689497  [ 3104/ 3200]\n",
      "loss: 0.523552  [ 3120/ 3200]\n",
      "loss: 0.647867  [ 3136/ 3200]\n",
      "loss: 0.833445  [ 3152/ 3200]\n",
      "loss: 0.990568  [ 3168/ 3200]\n",
      "loss: 1.169970  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.053214\n",
      "f1 macro averaged score: 0.636967\n",
      "Accuracy               : 64.1%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  21,   2,   2],\n",
      "        [ 53, 100,  19,  28],\n",
      "        [  3,  32, 130,  35],\n",
      "        [ 10,  50,  32, 108]], device='cuda:0')\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.782881  [    0/ 3200]\n",
      "loss: 0.818343  [   16/ 3200]\n",
      "loss: 0.858454  [   32/ 3200]\n",
      "loss: 0.949769  [   48/ 3200]\n",
      "loss: 0.505766  [   64/ 3200]\n",
      "loss: 0.579435  [   80/ 3200]\n",
      "loss: 0.605889  [   96/ 3200]\n",
      "loss: 0.495656  [  112/ 3200]\n",
      "loss: 0.696047  [  128/ 3200]\n",
      "loss: 0.688621  [  144/ 3200]\n",
      "loss: 0.652465  [  160/ 3200]\n",
      "loss: 0.748690  [  176/ 3200]\n",
      "loss: 0.549889  [  192/ 3200]\n",
      "loss: 0.597822  [  208/ 3200]\n",
      "loss: 0.679147  [  224/ 3200]\n",
      "loss: 0.508268  [  240/ 3200]\n",
      "loss: 0.409998  [  256/ 3200]\n",
      "loss: 0.742988  [  272/ 3200]\n",
      "loss: 0.552569  [  288/ 3200]\n",
      "loss: 0.444959  [  304/ 3200]\n",
      "loss: 0.654963  [  320/ 3200]\n",
      "loss: 0.537887  [  336/ 3200]\n",
      "loss: 0.645384  [  352/ 3200]\n",
      "loss: 0.729157  [  368/ 3200]\n",
      "loss: 1.127389  [  384/ 3200]\n",
      "loss: 0.701189  [  400/ 3200]\n",
      "loss: 0.454414  [  416/ 3200]\n",
      "loss: 1.116131  [  432/ 3200]\n",
      "loss: 0.933003  [  448/ 3200]\n",
      "loss: 0.742168  [  464/ 3200]\n",
      "loss: 0.625304  [  480/ 3200]\n",
      "loss: 0.977363  [  496/ 3200]\n",
      "loss: 0.477158  [  512/ 3200]\n",
      "loss: 0.774348  [  528/ 3200]\n",
      "loss: 0.771716  [  544/ 3200]\n",
      "loss: 0.469909  [  560/ 3200]\n",
      "loss: 0.933005  [  576/ 3200]\n",
      "loss: 0.673058  [  592/ 3200]\n",
      "loss: 1.116226  [  608/ 3200]\n",
      "loss: 1.454626  [  624/ 3200]\n",
      "loss: 0.629660  [  640/ 3200]\n",
      "loss: 0.823384  [  656/ 3200]\n",
      "loss: 0.474275  [  672/ 3200]\n",
      "loss: 0.752507  [  688/ 3200]\n",
      "loss: 0.638408  [  704/ 3200]\n",
      "loss: 0.908009  [  720/ 3200]\n",
      "loss: 0.520677  [  736/ 3200]\n",
      "loss: 0.571916  [  752/ 3200]\n",
      "loss: 0.716892  [  768/ 3200]\n",
      "loss: 0.507996  [  784/ 3200]\n",
      "loss: 0.872820  [  800/ 3200]\n",
      "loss: 0.441988  [  816/ 3200]\n",
      "loss: 0.774213  [  832/ 3200]\n",
      "loss: 1.037870  [  848/ 3200]\n",
      "loss: 0.634932  [  864/ 3200]\n",
      "loss: 1.012989  [  880/ 3200]\n",
      "loss: 0.733841  [  896/ 3200]\n",
      "loss: 0.698973  [  912/ 3200]\n",
      "loss: 0.402590  [  928/ 3200]\n",
      "loss: 0.726993  [  944/ 3200]\n",
      "loss: 0.766612  [  960/ 3200]\n",
      "loss: 0.770128  [  976/ 3200]\n",
      "loss: 0.464568  [  992/ 3200]\n",
      "loss: 0.778721  [ 1008/ 3200]\n",
      "loss: 0.629672  [ 1024/ 3200]\n",
      "loss: 0.493354  [ 1040/ 3200]\n",
      "loss: 0.830453  [ 1056/ 3200]\n",
      "loss: 0.975372  [ 1072/ 3200]\n",
      "loss: 1.009922  [ 1088/ 3200]\n",
      "loss: 0.853757  [ 1104/ 3200]\n",
      "loss: 0.898006  [ 1120/ 3200]\n",
      "loss: 0.513333  [ 1136/ 3200]\n",
      "loss: 1.114532  [ 1152/ 3200]\n",
      "loss: 0.897696  [ 1168/ 3200]\n",
      "loss: 0.575005  [ 1184/ 3200]\n",
      "loss: 0.544484  [ 1200/ 3200]\n",
      "loss: 0.806022  [ 1216/ 3200]\n",
      "loss: 0.368976  [ 1232/ 3200]\n",
      "loss: 0.733681  [ 1248/ 3200]\n",
      "loss: 0.655215  [ 1264/ 3200]\n",
      "loss: 0.688339  [ 1280/ 3200]\n",
      "loss: 0.735068  [ 1296/ 3200]\n",
      "loss: 0.513936  [ 1312/ 3200]\n",
      "loss: 0.549801  [ 1328/ 3200]\n",
      "loss: 0.650016  [ 1344/ 3200]\n",
      "loss: 0.919138  [ 1360/ 3200]\n",
      "loss: 0.592591  [ 1376/ 3200]\n",
      "loss: 1.028193  [ 1392/ 3200]\n",
      "loss: 0.738426  [ 1408/ 3200]\n",
      "loss: 0.518696  [ 1424/ 3200]\n",
      "loss: 0.570326  [ 1440/ 3200]\n",
      "loss: 0.727641  [ 1456/ 3200]\n",
      "loss: 0.879904  [ 1472/ 3200]\n",
      "loss: 0.772181  [ 1488/ 3200]\n",
      "loss: 0.789914  [ 1504/ 3200]\n",
      "loss: 0.659970  [ 1520/ 3200]\n",
      "loss: 0.576717  [ 1536/ 3200]\n",
      "loss: 0.659795  [ 1552/ 3200]\n",
      "loss: 0.548756  [ 1568/ 3200]\n",
      "loss: 0.522326  [ 1584/ 3200]\n",
      "loss: 1.073031  [ 1600/ 3200]\n",
      "loss: 1.869331  [ 1616/ 3200]\n",
      "loss: 0.883265  [ 1632/ 3200]\n",
      "loss: 1.065303  [ 1648/ 3200]\n",
      "loss: 1.102593  [ 1664/ 3200]\n",
      "loss: 0.588438  [ 1680/ 3200]\n",
      "loss: 1.187050  [ 1696/ 3200]\n",
      "loss: 0.834246  [ 1712/ 3200]\n",
      "loss: 1.032093  [ 1728/ 3200]\n",
      "loss: 0.546356  [ 1744/ 3200]\n",
      "loss: 0.656484  [ 1760/ 3200]\n",
      "loss: 0.635238  [ 1776/ 3200]\n",
      "loss: 0.590362  [ 1792/ 3200]\n",
      "loss: 0.381366  [ 1808/ 3200]\n",
      "loss: 0.625550  [ 1824/ 3200]\n",
      "loss: 1.144310  [ 1840/ 3200]\n",
      "loss: 0.450202  [ 1856/ 3200]\n",
      "loss: 0.650656  [ 1872/ 3200]\n",
      "loss: 0.543099  [ 1888/ 3200]\n",
      "loss: 0.712860  [ 1904/ 3200]\n",
      "loss: 0.563181  [ 1920/ 3200]\n",
      "loss: 0.870308  [ 1936/ 3200]\n",
      "loss: 0.622263  [ 1952/ 3200]\n",
      "loss: 0.553469  [ 1968/ 3200]\n",
      "loss: 0.528241  [ 1984/ 3200]\n",
      "loss: 0.852964  [ 2000/ 3200]\n",
      "loss: 0.593822  [ 2016/ 3200]\n",
      "loss: 0.945009  [ 2032/ 3200]\n",
      "loss: 0.428898  [ 2048/ 3200]\n",
      "loss: 0.505635  [ 2064/ 3200]\n",
      "loss: 0.624089  [ 2080/ 3200]\n",
      "loss: 0.628032  [ 2096/ 3200]\n",
      "loss: 0.609064  [ 2112/ 3200]\n",
      "loss: 0.638125  [ 2128/ 3200]\n",
      "loss: 0.822297  [ 2144/ 3200]\n",
      "loss: 0.311180  [ 2160/ 3200]\n",
      "loss: 0.734625  [ 2176/ 3200]\n",
      "loss: 1.245777  [ 2192/ 3200]\n",
      "loss: 0.697952  [ 2208/ 3200]\n",
      "loss: 0.640787  [ 2224/ 3200]\n",
      "loss: 0.862340  [ 2240/ 3200]\n",
      "loss: 0.690415  [ 2256/ 3200]\n",
      "loss: 1.280199  [ 2272/ 3200]\n",
      "loss: 0.498689  [ 2288/ 3200]\n",
      "loss: 0.680162  [ 2304/ 3200]\n",
      "loss: 0.412677  [ 2320/ 3200]\n",
      "loss: 0.870969  [ 2336/ 3200]\n",
      "loss: 0.653272  [ 2352/ 3200]\n",
      "loss: 1.055749  [ 2368/ 3200]\n",
      "loss: 0.585332  [ 2384/ 3200]\n",
      "loss: 0.752913  [ 2400/ 3200]\n",
      "loss: 0.608218  [ 2416/ 3200]\n",
      "loss: 0.712484  [ 2432/ 3200]\n",
      "loss: 1.406519  [ 2448/ 3200]\n",
      "loss: 0.703086  [ 2464/ 3200]\n",
      "loss: 1.189305  [ 2480/ 3200]\n",
      "loss: 0.668254  [ 2496/ 3200]\n",
      "loss: 0.593173  [ 2512/ 3200]\n",
      "loss: 0.834185  [ 2528/ 3200]\n",
      "loss: 0.624496  [ 2544/ 3200]\n",
      "loss: 0.409247  [ 2560/ 3200]\n",
      "loss: 0.969945  [ 2576/ 3200]\n",
      "loss: 0.780277  [ 2592/ 3200]\n",
      "loss: 0.918908  [ 2608/ 3200]\n",
      "loss: 1.223043  [ 2624/ 3200]\n",
      "loss: 0.752105  [ 2640/ 3200]\n",
      "loss: 0.851275  [ 2656/ 3200]\n",
      "loss: 0.697006  [ 2672/ 3200]\n",
      "loss: 0.849275  [ 2688/ 3200]\n",
      "loss: 0.883448  [ 2704/ 3200]\n",
      "loss: 0.714700  [ 2720/ 3200]\n",
      "loss: 0.535105  [ 2736/ 3200]\n",
      "loss: 1.036646  [ 2752/ 3200]\n",
      "loss: 0.790208  [ 2768/ 3200]\n",
      "loss: 0.783151  [ 2784/ 3200]\n",
      "loss: 0.419606  [ 2800/ 3200]\n",
      "loss: 0.786375  [ 2816/ 3200]\n",
      "loss: 0.886597  [ 2832/ 3200]\n",
      "loss: 0.848207  [ 2848/ 3200]\n",
      "loss: 0.442156  [ 2864/ 3200]\n",
      "loss: 0.978210  [ 2880/ 3200]\n",
      "loss: 0.727640  [ 2896/ 3200]\n",
      "loss: 0.675290  [ 2912/ 3200]\n",
      "loss: 0.535677  [ 2928/ 3200]\n",
      "loss: 0.793539  [ 2944/ 3200]\n",
      "loss: 0.712962  [ 2960/ 3200]\n",
      "loss: 0.858279  [ 2976/ 3200]\n",
      "loss: 0.801490  [ 2992/ 3200]\n",
      "loss: 0.818559  [ 3008/ 3200]\n",
      "loss: 0.672660  [ 3024/ 3200]\n",
      "loss: 0.575742  [ 3040/ 3200]\n",
      "loss: 0.837933  [ 3056/ 3200]\n",
      "loss: 0.895462  [ 3072/ 3200]\n",
      "loss: 0.760620  [ 3088/ 3200]\n",
      "loss: 0.973746  [ 3104/ 3200]\n",
      "loss: 0.653945  [ 3120/ 3200]\n",
      "loss: 0.677037  [ 3136/ 3200]\n",
      "loss: 0.570336  [ 3152/ 3200]\n",
      "loss: 0.698300  [ 3168/ 3200]\n",
      "loss: 0.752892  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.056325\n",
      "f1 macro averaged score: 0.589871\n",
      "Accuracy               : 60.1%\n",
      "Confusion matrix       :\n",
      "tensor([[140,  46,   8,   6],\n",
      "        [ 23,  55,  76,  46],\n",
      "        [  0,   7, 166,  27],\n",
      "        [  1,  24,  55, 120]], device='cuda:0')\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.733598  [    0/ 3200]\n",
      "loss: 0.621742  [   16/ 3200]\n",
      "loss: 0.769792  [   32/ 3200]\n",
      "loss: 0.557587  [   48/ 3200]\n",
      "loss: 0.576257  [   64/ 3200]\n",
      "loss: 0.885039  [   80/ 3200]\n",
      "loss: 0.905095  [   96/ 3200]\n",
      "loss: 0.998250  [  112/ 3200]\n",
      "loss: 0.728581  [  128/ 3200]\n",
      "loss: 0.688167  [  144/ 3200]\n",
      "loss: 0.714208  [  160/ 3200]\n",
      "loss: 1.053440  [  176/ 3200]\n",
      "loss: 1.150674  [  192/ 3200]\n",
      "loss: 0.651948  [  208/ 3200]\n",
      "loss: 1.027543  [  224/ 3200]\n",
      "loss: 0.743610  [  240/ 3200]\n",
      "loss: 0.495666  [  256/ 3200]\n",
      "loss: 0.454206  [  272/ 3200]\n",
      "loss: 0.559969  [  288/ 3200]\n",
      "loss: 0.662592  [  304/ 3200]\n",
      "loss: 0.523455  [  320/ 3200]\n",
      "loss: 0.618415  [  336/ 3200]\n",
      "loss: 0.668838  [  352/ 3200]\n",
      "loss: 0.726922  [  368/ 3200]\n",
      "loss: 0.749841  [  384/ 3200]\n",
      "loss: 0.994142  [  400/ 3200]\n",
      "loss: 0.809373  [  416/ 3200]\n",
      "loss: 0.783406  [  432/ 3200]\n",
      "loss: 0.568405  [  448/ 3200]\n",
      "loss: 0.357357  [  464/ 3200]\n",
      "loss: 0.913105  [  480/ 3200]\n",
      "loss: 0.968195  [  496/ 3200]\n",
      "loss: 0.635431  [  512/ 3200]\n",
      "loss: 0.449453  [  528/ 3200]\n",
      "loss: 0.613360  [  544/ 3200]\n",
      "loss: 0.579489  [  560/ 3200]\n",
      "loss: 0.746990  [  576/ 3200]\n",
      "loss: 0.644492  [  592/ 3200]\n",
      "loss: 0.506273  [  608/ 3200]\n",
      "loss: 0.700058  [  624/ 3200]\n",
      "loss: 0.646560  [  640/ 3200]\n",
      "loss: 0.889440  [  656/ 3200]\n",
      "loss: 0.534589  [  672/ 3200]\n",
      "loss: 0.747177  [  688/ 3200]\n",
      "loss: 0.848988  [  704/ 3200]\n",
      "loss: 0.827069  [  720/ 3200]\n",
      "loss: 0.489669  [  736/ 3200]\n",
      "loss: 0.512308  [  752/ 3200]\n",
      "loss: 0.632214  [  768/ 3200]\n",
      "loss: 0.860712  [  784/ 3200]\n",
      "loss: 0.404152  [  800/ 3200]\n",
      "loss: 0.824848  [  816/ 3200]\n",
      "loss: 0.840208  [  832/ 3200]\n",
      "loss: 0.671479  [  848/ 3200]\n",
      "loss: 0.832760  [  864/ 3200]\n",
      "loss: 0.845161  [  880/ 3200]\n",
      "loss: 0.949901  [  896/ 3200]\n",
      "loss: 0.773922  [  912/ 3200]\n",
      "loss: 1.106792  [  928/ 3200]\n",
      "loss: 0.632163  [  944/ 3200]\n",
      "loss: 0.687115  [  960/ 3200]\n",
      "loss: 0.689899  [  976/ 3200]\n",
      "loss: 0.609330  [  992/ 3200]\n",
      "loss: 0.958701  [ 1008/ 3200]\n",
      "loss: 0.606171  [ 1024/ 3200]\n",
      "loss: 0.669446  [ 1040/ 3200]\n",
      "loss: 0.503880  [ 1056/ 3200]\n",
      "loss: 0.944322  [ 1072/ 3200]\n",
      "loss: 1.067894  [ 1088/ 3200]\n",
      "loss: 0.482908  [ 1104/ 3200]\n",
      "loss: 0.511779  [ 1120/ 3200]\n",
      "loss: 0.697883  [ 1136/ 3200]\n",
      "loss: 1.079599  [ 1152/ 3200]\n",
      "loss: 0.606927  [ 1168/ 3200]\n",
      "loss: 0.612707  [ 1184/ 3200]\n",
      "loss: 0.651404  [ 1200/ 3200]\n",
      "loss: 0.674451  [ 1216/ 3200]\n",
      "loss: 1.002243  [ 1232/ 3200]\n",
      "loss: 0.524821  [ 1248/ 3200]\n",
      "loss: 0.976563  [ 1264/ 3200]\n",
      "loss: 0.576656  [ 1280/ 3200]\n",
      "loss: 0.663636  [ 1296/ 3200]\n",
      "loss: 1.015446  [ 1312/ 3200]\n",
      "loss: 0.853811  [ 1328/ 3200]\n",
      "loss: 0.770284  [ 1344/ 3200]\n",
      "loss: 0.798233  [ 1360/ 3200]\n",
      "loss: 1.142154  [ 1376/ 3200]\n",
      "loss: 0.444503  [ 1392/ 3200]\n",
      "loss: 0.716982  [ 1408/ 3200]\n",
      "loss: 1.277067  [ 1424/ 3200]\n",
      "loss: 0.823934  [ 1440/ 3200]\n",
      "loss: 0.583123  [ 1456/ 3200]\n",
      "loss: 0.720500  [ 1472/ 3200]\n",
      "loss: 1.029956  [ 1488/ 3200]\n",
      "loss: 0.850698  [ 1504/ 3200]\n",
      "loss: 1.050883  [ 1520/ 3200]\n",
      "loss: 0.521479  [ 1536/ 3200]\n",
      "loss: 0.734589  [ 1552/ 3200]\n",
      "loss: 0.705390  [ 1568/ 3200]\n",
      "loss: 0.859147  [ 1584/ 3200]\n",
      "loss: 0.837318  [ 1600/ 3200]\n",
      "loss: 0.803173  [ 1616/ 3200]\n",
      "loss: 0.725156  [ 1632/ 3200]\n",
      "loss: 0.687143  [ 1648/ 3200]\n",
      "loss: 0.781365  [ 1664/ 3200]\n",
      "loss: 0.749294  [ 1680/ 3200]\n",
      "loss: 0.666904  [ 1696/ 3200]\n",
      "loss: 0.357078  [ 1712/ 3200]\n",
      "loss: 0.367000  [ 1728/ 3200]\n",
      "loss: 0.549519  [ 1744/ 3200]\n",
      "loss: 0.497174  [ 1760/ 3200]\n",
      "loss: 0.425340  [ 1776/ 3200]\n",
      "loss: 0.905373  [ 1792/ 3200]\n",
      "loss: 0.734707  [ 1808/ 3200]\n",
      "loss: 0.696246  [ 1824/ 3200]\n",
      "loss: 0.788275  [ 1840/ 3200]\n",
      "loss: 1.127331  [ 1856/ 3200]\n",
      "loss: 0.588497  [ 1872/ 3200]\n",
      "loss: 0.837407  [ 1888/ 3200]\n",
      "loss: 0.655465  [ 1904/ 3200]\n",
      "loss: 0.527568  [ 1920/ 3200]\n",
      "loss: 0.539097  [ 1936/ 3200]\n",
      "loss: 0.965748  [ 1952/ 3200]\n",
      "loss: 0.652823  [ 1968/ 3200]\n",
      "loss: 0.634353  [ 1984/ 3200]\n",
      "loss: 0.596354  [ 2000/ 3200]\n",
      "loss: 0.716676  [ 2016/ 3200]\n",
      "loss: 0.643364  [ 2032/ 3200]\n",
      "loss: 0.650170  [ 2048/ 3200]\n",
      "loss: 0.872405  [ 2064/ 3200]\n",
      "loss: 0.574171  [ 2080/ 3200]\n",
      "loss: 0.561376  [ 2096/ 3200]\n",
      "loss: 0.645960  [ 2112/ 3200]\n",
      "loss: 0.626818  [ 2128/ 3200]\n",
      "loss: 0.908143  [ 2144/ 3200]\n",
      "loss: 0.855102  [ 2160/ 3200]\n",
      "loss: 0.759354  [ 2176/ 3200]\n",
      "loss: 0.725915  [ 2192/ 3200]\n",
      "loss: 0.727224  [ 2208/ 3200]\n",
      "loss: 0.860341  [ 2224/ 3200]\n",
      "loss: 0.789855  [ 2240/ 3200]\n",
      "loss: 0.372663  [ 2256/ 3200]\n",
      "loss: 0.562687  [ 2272/ 3200]\n",
      "loss: 0.733251  [ 2288/ 3200]\n",
      "loss: 0.797732  [ 2304/ 3200]\n",
      "loss: 0.582970  [ 2320/ 3200]\n",
      "loss: 0.611330  [ 2336/ 3200]\n",
      "loss: 0.525855  [ 2352/ 3200]\n",
      "loss: 0.965089  [ 2368/ 3200]\n",
      "loss: 0.755540  [ 2384/ 3200]\n",
      "loss: 0.866731  [ 2400/ 3200]\n",
      "loss: 0.505138  [ 2416/ 3200]\n",
      "loss: 0.728283  [ 2432/ 3200]\n",
      "loss: 0.892562  [ 2448/ 3200]\n",
      "loss: 0.419297  [ 2464/ 3200]\n",
      "loss: 0.570336  [ 2480/ 3200]\n",
      "loss: 0.634109  [ 2496/ 3200]\n",
      "loss: 0.777972  [ 2512/ 3200]\n",
      "loss: 0.988370  [ 2528/ 3200]\n",
      "loss: 0.415193  [ 2544/ 3200]\n",
      "loss: 0.680114  [ 2560/ 3200]\n",
      "loss: 0.518156  [ 2576/ 3200]\n",
      "loss: 0.502283  [ 2592/ 3200]\n",
      "loss: 0.774770  [ 2608/ 3200]\n",
      "loss: 0.954133  [ 2624/ 3200]\n",
      "loss: 0.432203  [ 2640/ 3200]\n",
      "loss: 0.626560  [ 2656/ 3200]\n",
      "loss: 0.911962  [ 2672/ 3200]\n",
      "loss: 0.482399  [ 2688/ 3200]\n",
      "loss: 0.745272  [ 2704/ 3200]\n",
      "loss: 0.870193  [ 2720/ 3200]\n",
      "loss: 0.808150  [ 2736/ 3200]\n",
      "loss: 1.114770  [ 2752/ 3200]\n",
      "loss: 0.835254  [ 2768/ 3200]\n",
      "loss: 0.824324  [ 2784/ 3200]\n",
      "loss: 0.677390  [ 2800/ 3200]\n",
      "loss: 0.734293  [ 2816/ 3200]\n",
      "loss: 0.815479  [ 2832/ 3200]\n",
      "loss: 0.538794  [ 2848/ 3200]\n",
      "loss: 0.558582  [ 2864/ 3200]\n",
      "loss: 0.613513  [ 2880/ 3200]\n",
      "loss: 0.798598  [ 2896/ 3200]\n",
      "loss: 0.629887  [ 2912/ 3200]\n",
      "loss: 0.528525  [ 2928/ 3200]\n",
      "loss: 0.915983  [ 2944/ 3200]\n",
      "loss: 1.260321  [ 2960/ 3200]\n",
      "loss: 0.799284  [ 2976/ 3200]\n",
      "loss: 0.771228  [ 2992/ 3200]\n",
      "loss: 0.743461  [ 3008/ 3200]\n",
      "loss: 0.649562  [ 3024/ 3200]\n",
      "loss: 0.706937  [ 3040/ 3200]\n",
      "loss: 0.607584  [ 3056/ 3200]\n",
      "loss: 0.640505  [ 3072/ 3200]\n",
      "loss: 0.532904  [ 3088/ 3200]\n",
      "loss: 0.476439  [ 3104/ 3200]\n",
      "loss: 0.827982  [ 3120/ 3200]\n",
      "loss: 0.600443  [ 3136/ 3200]\n",
      "loss: 0.872250  [ 3152/ 3200]\n",
      "loss: 0.714550  [ 3168/ 3200]\n",
      "loss: 0.728185  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.055484\n",
      "f1 macro averaged score: 0.647118\n",
      "Accuracy               : 65.1%\n",
      "Confusion matrix       :\n",
      "tensor([[183,  17,   0,   0],\n",
      "        [ 49, 129,   7,  15],\n",
      "        [  1,  54, 110,  35],\n",
      "        [ 13,  63,  25,  99]], device='cuda:0')\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 0.845150  [    0/ 3200]\n",
      "loss: 0.523465  [   16/ 3200]\n",
      "loss: 0.685305  [   32/ 3200]\n",
      "loss: 0.584210  [   48/ 3200]\n",
      "loss: 0.632366  [   64/ 3200]\n",
      "loss: 0.685723  [   80/ 3200]\n",
      "loss: 0.906801  [   96/ 3200]\n",
      "loss: 0.796257  [  112/ 3200]\n",
      "loss: 0.579503  [  128/ 3200]\n",
      "loss: 0.537240  [  144/ 3200]\n",
      "loss: 0.781867  [  160/ 3200]\n",
      "loss: 0.662705  [  176/ 3200]\n",
      "loss: 0.512825  [  192/ 3200]\n",
      "loss: 1.012501  [  208/ 3200]\n",
      "loss: 0.689967  [  224/ 3200]\n",
      "loss: 0.664133  [  240/ 3200]\n",
      "loss: 0.789287  [  256/ 3200]\n",
      "loss: 0.538076  [  272/ 3200]\n",
      "loss: 0.434724  [  288/ 3200]\n",
      "loss: 0.614696  [  304/ 3200]\n",
      "loss: 0.550260  [  320/ 3200]\n",
      "loss: 0.497451  [  336/ 3200]\n",
      "loss: 0.779145  [  352/ 3200]\n",
      "loss: 0.966574  [  368/ 3200]\n",
      "loss: 0.728831  [  384/ 3200]\n",
      "loss: 0.526819  [  400/ 3200]\n",
      "loss: 0.546599  [  416/ 3200]\n",
      "loss: 0.749924  [  432/ 3200]\n",
      "loss: 0.636782  [  448/ 3200]\n",
      "loss: 0.351740  [  464/ 3200]\n",
      "loss: 0.684830  [  480/ 3200]\n",
      "loss: 1.007706  [  496/ 3200]\n",
      "loss: 0.711947  [  512/ 3200]\n",
      "loss: 0.658209  [  528/ 3200]\n",
      "loss: 0.631322  [  544/ 3200]\n",
      "loss: 0.723099  [  560/ 3200]\n",
      "loss: 0.648956  [  576/ 3200]\n",
      "loss: 0.800951  [  592/ 3200]\n",
      "loss: 0.717549  [  608/ 3200]\n",
      "loss: 0.665738  [  624/ 3200]\n",
      "loss: 0.701481  [  640/ 3200]\n",
      "loss: 0.961511  [  656/ 3200]\n",
      "loss: 0.704565  [  672/ 3200]\n",
      "loss: 0.615713  [  688/ 3200]\n",
      "loss: 0.998252  [  704/ 3200]\n",
      "loss: 0.870988  [  720/ 3200]\n",
      "loss: 0.667847  [  736/ 3200]\n",
      "loss: 0.750154  [  752/ 3200]\n",
      "loss: 0.643947  [  768/ 3200]\n",
      "loss: 0.617224  [  784/ 3200]\n",
      "loss: 0.753787  [  800/ 3200]\n",
      "loss: 0.718388  [  816/ 3200]\n",
      "loss: 1.193275  [  832/ 3200]\n",
      "loss: 0.715379  [  848/ 3200]\n",
      "loss: 0.261481  [  864/ 3200]\n",
      "loss: 0.767213  [  880/ 3200]\n",
      "loss: 0.558066  [  896/ 3200]\n",
      "loss: 0.728382  [  912/ 3200]\n",
      "loss: 0.778512  [  928/ 3200]\n",
      "loss: 0.547459  [  944/ 3200]\n",
      "loss: 1.110559  [  960/ 3200]\n",
      "loss: 0.477983  [  976/ 3200]\n",
      "loss: 0.823599  [  992/ 3200]\n",
      "loss: 0.585519  [ 1008/ 3200]\n",
      "loss: 0.376297  [ 1024/ 3200]\n",
      "loss: 0.556289  [ 1040/ 3200]\n",
      "loss: 0.783088  [ 1056/ 3200]\n",
      "loss: 0.742238  [ 1072/ 3200]\n",
      "loss: 0.946460  [ 1088/ 3200]\n",
      "loss: 1.084370  [ 1104/ 3200]\n",
      "loss: 0.547665  [ 1120/ 3200]\n",
      "loss: 0.712238  [ 1136/ 3200]\n",
      "loss: 0.695083  [ 1152/ 3200]\n",
      "loss: 0.472211  [ 1168/ 3200]\n",
      "loss: 0.436335  [ 1184/ 3200]\n",
      "loss: 0.802159  [ 1200/ 3200]\n",
      "loss: 0.708264  [ 1216/ 3200]\n",
      "loss: 0.511134  [ 1232/ 3200]\n",
      "loss: 0.650970  [ 1248/ 3200]\n",
      "loss: 1.225907  [ 1264/ 3200]\n",
      "loss: 0.841725  [ 1280/ 3200]\n",
      "loss: 0.962629  [ 1296/ 3200]\n",
      "loss: 0.871246  [ 1312/ 3200]\n",
      "loss: 1.006454  [ 1328/ 3200]\n",
      "loss: 0.552597  [ 1344/ 3200]\n",
      "loss: 0.852412  [ 1360/ 3200]\n",
      "loss: 0.558073  [ 1376/ 3200]\n",
      "loss: 0.418708  [ 1392/ 3200]\n",
      "loss: 0.315674  [ 1408/ 3200]\n",
      "loss: 0.743485  [ 1424/ 3200]\n",
      "loss: 0.818328  [ 1440/ 3200]\n",
      "loss: 0.757051  [ 1456/ 3200]\n",
      "loss: 0.855351  [ 1472/ 3200]\n",
      "loss: 0.446564  [ 1488/ 3200]\n",
      "loss: 0.515374  [ 1504/ 3200]\n",
      "loss: 0.688373  [ 1520/ 3200]\n",
      "loss: 0.690715  [ 1536/ 3200]\n",
      "loss: 0.646777  [ 1552/ 3200]\n",
      "loss: 1.177738  [ 1568/ 3200]\n",
      "loss: 0.715309  [ 1584/ 3200]\n",
      "loss: 0.759508  [ 1600/ 3200]\n",
      "loss: 0.627235  [ 1616/ 3200]\n",
      "loss: 0.862736  [ 1632/ 3200]\n",
      "loss: 0.558664  [ 1648/ 3200]\n",
      "loss: 0.695484  [ 1664/ 3200]\n",
      "loss: 0.625076  [ 1680/ 3200]\n",
      "loss: 0.490922  [ 1696/ 3200]\n",
      "loss: 0.800992  [ 1712/ 3200]\n",
      "loss: 1.249905  [ 1728/ 3200]\n",
      "loss: 0.894561  [ 1744/ 3200]\n",
      "loss: 0.684757  [ 1760/ 3200]\n",
      "loss: 0.670699  [ 1776/ 3200]\n",
      "loss: 0.927860  [ 1792/ 3200]\n",
      "loss: 0.946974  [ 1808/ 3200]\n",
      "loss: 0.669752  [ 1824/ 3200]\n",
      "loss: 0.654956  [ 1840/ 3200]\n",
      "loss: 1.077200  [ 1856/ 3200]\n",
      "loss: 0.546020  [ 1872/ 3200]\n",
      "loss: 0.578602  [ 1888/ 3200]\n",
      "loss: 0.645806  [ 1904/ 3200]\n",
      "loss: 0.851215  [ 1920/ 3200]\n",
      "loss: 0.769235  [ 1936/ 3200]\n",
      "loss: 0.606707  [ 1952/ 3200]\n",
      "loss: 0.596413  [ 1968/ 3200]\n",
      "loss: 0.962444  [ 1984/ 3200]\n",
      "loss: 0.533376  [ 2000/ 3200]\n",
      "loss: 0.767035  [ 2016/ 3200]\n",
      "loss: 0.339852  [ 2032/ 3200]\n",
      "loss: 0.867561  [ 2048/ 3200]\n",
      "loss: 0.961650  [ 2064/ 3200]\n",
      "loss: 0.690213  [ 2080/ 3200]\n",
      "loss: 0.728911  [ 2096/ 3200]\n",
      "loss: 0.802309  [ 2112/ 3200]\n",
      "loss: 0.698346  [ 2128/ 3200]\n",
      "loss: 0.769193  [ 2144/ 3200]\n",
      "loss: 0.634870  [ 2160/ 3200]\n",
      "loss: 0.373517  [ 2176/ 3200]\n",
      "loss: 0.943104  [ 2192/ 3200]\n",
      "loss: 0.616053  [ 2208/ 3200]\n",
      "loss: 0.486150  [ 2224/ 3200]\n",
      "loss: 0.611674  [ 2240/ 3200]\n",
      "loss: 0.545088  [ 2256/ 3200]\n",
      "loss: 0.588432  [ 2272/ 3200]\n",
      "loss: 0.612374  [ 2288/ 3200]\n",
      "loss: 0.855005  [ 2304/ 3200]\n",
      "loss: 0.833369  [ 2320/ 3200]\n",
      "loss: 0.764670  [ 2336/ 3200]\n",
      "loss: 0.502355  [ 2352/ 3200]\n",
      "loss: 0.532304  [ 2368/ 3200]\n",
      "loss: 0.493102  [ 2384/ 3200]\n",
      "loss: 1.024164  [ 2400/ 3200]\n",
      "loss: 0.678828  [ 2416/ 3200]\n",
      "loss: 0.618333  [ 2432/ 3200]\n",
      "loss: 0.387488  [ 2448/ 3200]\n",
      "loss: 0.755238  [ 2464/ 3200]\n",
      "loss: 0.653888  [ 2480/ 3200]\n",
      "loss: 0.676226  [ 2496/ 3200]\n",
      "loss: 0.909692  [ 2512/ 3200]\n",
      "loss: 0.784421  [ 2528/ 3200]\n",
      "loss: 0.564082  [ 2544/ 3200]\n",
      "loss: 0.363561  [ 2560/ 3200]\n",
      "loss: 0.878876  [ 2576/ 3200]\n",
      "loss: 0.463807  [ 2592/ 3200]\n",
      "loss: 0.798594  [ 2608/ 3200]\n",
      "loss: 0.578433  [ 2624/ 3200]\n",
      "loss: 0.597336  [ 2640/ 3200]\n",
      "loss: 0.849085  [ 2656/ 3200]\n",
      "loss: 0.566642  [ 2672/ 3200]\n",
      "loss: 0.810870  [ 2688/ 3200]\n",
      "loss: 0.570379  [ 2704/ 3200]\n",
      "loss: 1.053371  [ 2720/ 3200]\n",
      "loss: 0.796036  [ 2736/ 3200]\n",
      "loss: 0.630639  [ 2752/ 3200]\n",
      "loss: 0.720332  [ 2768/ 3200]\n",
      "loss: 0.558464  [ 2784/ 3200]\n",
      "loss: 0.750276  [ 2800/ 3200]\n",
      "loss: 0.861550  [ 2816/ 3200]\n",
      "loss: 0.610985  [ 2832/ 3200]\n",
      "loss: 0.441757  [ 2848/ 3200]\n",
      "loss: 0.897428  [ 2864/ 3200]\n",
      "loss: 0.753569  [ 2880/ 3200]\n",
      "loss: 0.750908  [ 2896/ 3200]\n",
      "loss: 0.771438  [ 2912/ 3200]\n",
      "loss: 1.046842  [ 2928/ 3200]\n",
      "loss: 0.506607  [ 2944/ 3200]\n",
      "loss: 0.808138  [ 2960/ 3200]\n",
      "loss: 0.555894  [ 2976/ 3200]\n",
      "loss: 0.762585  [ 2992/ 3200]\n",
      "loss: 0.571542  [ 3008/ 3200]\n",
      "loss: 0.769295  [ 3024/ 3200]\n",
      "loss: 0.763962  [ 3040/ 3200]\n",
      "loss: 0.769350  [ 3056/ 3200]\n",
      "loss: 0.536268  [ 3072/ 3200]\n",
      "loss: 0.505089  [ 3088/ 3200]\n",
      "loss: 1.115933  [ 3104/ 3200]\n",
      "loss: 0.556104  [ 3120/ 3200]\n",
      "loss: 0.863321  [ 3136/ 3200]\n",
      "loss: 0.961544  [ 3152/ 3200]\n",
      "loss: 0.647518  [ 3168/ 3200]\n",
      "loss: 0.573392  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.051559\n",
      "f1 macro averaged score: 0.677259\n",
      "Accuracy               : 67.6%\n",
      "Confusion matrix       :\n",
      "tensor([[167,  32,   1,   0],\n",
      "        [ 29, 144,  13,  14],\n",
      "        [  0,  44, 127,  29],\n",
      "        [  9,  61,  27, 103]], device='cuda:0')\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.517155  [    0/ 3200]\n",
      "loss: 0.500213  [   16/ 3200]\n",
      "loss: 0.302422  [   32/ 3200]\n",
      "loss: 0.831553  [   48/ 3200]\n",
      "loss: 0.525808  [   64/ 3200]\n",
      "loss: 0.937957  [   80/ 3200]\n",
      "loss: 0.720737  [   96/ 3200]\n",
      "loss: 0.387149  [  112/ 3200]\n",
      "loss: 0.760018  [  128/ 3200]\n",
      "loss: 0.687847  [  144/ 3200]\n",
      "loss: 0.538788  [  160/ 3200]\n",
      "loss: 0.767896  [  176/ 3200]\n",
      "loss: 0.751225  [  192/ 3200]\n",
      "loss: 0.442155  [  208/ 3200]\n",
      "loss: 0.922200  [  224/ 3200]\n",
      "loss: 0.667657  [  240/ 3200]\n",
      "loss: 0.728021  [  256/ 3200]\n",
      "loss: 0.723465  [  272/ 3200]\n",
      "loss: 0.895274  [  288/ 3200]\n",
      "loss: 0.888575  [  304/ 3200]\n",
      "loss: 0.736904  [  320/ 3200]\n",
      "loss: 0.832495  [  336/ 3200]\n",
      "loss: 0.762642  [  352/ 3200]\n",
      "loss: 0.672040  [  368/ 3200]\n",
      "loss: 0.658112  [  384/ 3200]\n",
      "loss: 0.472821  [  400/ 3200]\n",
      "loss: 0.743343  [  416/ 3200]\n",
      "loss: 1.010836  [  432/ 3200]\n",
      "loss: 0.448600  [  448/ 3200]\n",
      "loss: 0.798664  [  464/ 3200]\n",
      "loss: 0.615558  [  480/ 3200]\n",
      "loss: 0.693436  [  496/ 3200]\n",
      "loss: 0.393528  [  512/ 3200]\n",
      "loss: 0.802632  [  528/ 3200]\n",
      "loss: 0.483926  [  544/ 3200]\n",
      "loss: 0.803142  [  560/ 3200]\n",
      "loss: 0.702939  [  576/ 3200]\n",
      "loss: 0.472522  [  592/ 3200]\n",
      "loss: 0.457725  [  608/ 3200]\n",
      "loss: 0.596416  [  624/ 3200]\n",
      "loss: 0.494156  [  640/ 3200]\n",
      "loss: 0.513069  [  656/ 3200]\n",
      "loss: 1.001623  [  672/ 3200]\n",
      "loss: 0.662585  [  688/ 3200]\n",
      "loss: 0.555795  [  704/ 3200]\n",
      "loss: 0.645635  [  720/ 3200]\n",
      "loss: 0.785240  [  736/ 3200]\n",
      "loss: 0.631379  [  752/ 3200]\n",
      "loss: 0.475440  [  768/ 3200]\n",
      "loss: 0.574600  [  784/ 3200]\n",
      "loss: 0.732811  [  800/ 3200]\n",
      "loss: 0.611768  [  816/ 3200]\n",
      "loss: 0.419898  [  832/ 3200]\n",
      "loss: 0.594130  [  848/ 3200]\n",
      "loss: 0.596013  [  864/ 3200]\n",
      "loss: 0.676972  [  880/ 3200]\n",
      "loss: 0.607769  [  896/ 3200]\n",
      "loss: 0.823132  [  912/ 3200]\n",
      "loss: 0.729411  [  928/ 3200]\n",
      "loss: 0.888140  [  944/ 3200]\n",
      "loss: 0.531198  [  960/ 3200]\n",
      "loss: 0.836792  [  976/ 3200]\n",
      "loss: 0.604948  [  992/ 3200]\n",
      "loss: 0.818883  [ 1008/ 3200]\n",
      "loss: 0.678976  [ 1024/ 3200]\n",
      "loss: 1.079635  [ 1040/ 3200]\n",
      "loss: 1.054677  [ 1056/ 3200]\n",
      "loss: 0.512122  [ 1072/ 3200]\n",
      "loss: 0.779455  [ 1088/ 3200]\n",
      "loss: 0.387118  [ 1104/ 3200]\n",
      "loss: 0.591564  [ 1120/ 3200]\n",
      "loss: 0.705029  [ 1136/ 3200]\n",
      "loss: 0.616612  [ 1152/ 3200]\n",
      "loss: 0.599688  [ 1168/ 3200]\n",
      "loss: 0.611685  [ 1184/ 3200]\n",
      "loss: 0.542799  [ 1200/ 3200]\n",
      "loss: 0.779147  [ 1216/ 3200]\n",
      "loss: 0.619352  [ 1232/ 3200]\n",
      "loss: 0.633363  [ 1248/ 3200]\n",
      "loss: 0.914747  [ 1264/ 3200]\n",
      "loss: 0.763614  [ 1280/ 3200]\n",
      "loss: 0.638734  [ 1296/ 3200]\n",
      "loss: 0.802377  [ 1312/ 3200]\n",
      "loss: 0.957056  [ 1328/ 3200]\n",
      "loss: 0.486834  [ 1344/ 3200]\n",
      "loss: 0.543768  [ 1360/ 3200]\n",
      "loss: 0.541805  [ 1376/ 3200]\n",
      "loss: 0.576370  [ 1392/ 3200]\n",
      "loss: 1.061764  [ 1408/ 3200]\n",
      "loss: 0.538009  [ 1424/ 3200]\n",
      "loss: 0.419896  [ 1440/ 3200]\n",
      "loss: 0.667901  [ 1456/ 3200]\n",
      "loss: 0.566530  [ 1472/ 3200]\n",
      "loss: 0.503540  [ 1488/ 3200]\n",
      "loss: 0.840371  [ 1504/ 3200]\n",
      "loss: 0.611076  [ 1520/ 3200]\n",
      "loss: 0.415640  [ 1536/ 3200]\n",
      "loss: 0.452189  [ 1552/ 3200]\n",
      "loss: 0.534046  [ 1568/ 3200]\n",
      "loss: 0.716126  [ 1584/ 3200]\n",
      "loss: 0.648890  [ 1600/ 3200]\n",
      "loss: 0.922832  [ 1616/ 3200]\n",
      "loss: 0.563110  [ 1632/ 3200]\n",
      "loss: 0.733766  [ 1648/ 3200]\n",
      "loss: 0.968932  [ 1664/ 3200]\n",
      "loss: 0.575096  [ 1680/ 3200]\n",
      "loss: 0.629023  [ 1696/ 3200]\n",
      "loss: 0.312790  [ 1712/ 3200]\n",
      "loss: 0.941671  [ 1728/ 3200]\n",
      "loss: 0.690064  [ 1744/ 3200]\n",
      "loss: 0.505410  [ 1760/ 3200]\n",
      "loss: 0.583586  [ 1776/ 3200]\n",
      "loss: 1.402594  [ 1792/ 3200]\n",
      "loss: 0.372702  [ 1808/ 3200]\n",
      "loss: 0.579813  [ 1824/ 3200]\n",
      "loss: 0.641351  [ 1840/ 3200]\n",
      "loss: 0.905646  [ 1856/ 3200]\n",
      "loss: 0.695564  [ 1872/ 3200]\n",
      "loss: 0.452133  [ 1888/ 3200]\n",
      "loss: 0.507069  [ 1904/ 3200]\n",
      "loss: 0.885275  [ 1920/ 3200]\n",
      "loss: 0.964269  [ 1936/ 3200]\n",
      "loss: 0.626778  [ 1952/ 3200]\n",
      "loss: 0.589162  [ 1968/ 3200]\n",
      "loss: 0.519674  [ 1984/ 3200]\n",
      "loss: 0.592499  [ 2000/ 3200]\n",
      "loss: 0.722512  [ 2016/ 3200]\n",
      "loss: 1.036264  [ 2032/ 3200]\n",
      "loss: 0.506884  [ 2048/ 3200]\n",
      "loss: 0.459849  [ 2064/ 3200]\n",
      "loss: 0.677464  [ 2080/ 3200]\n",
      "loss: 0.723325  [ 2096/ 3200]\n",
      "loss: 0.597008  [ 2112/ 3200]\n",
      "loss: 0.627664  [ 2128/ 3200]\n",
      "loss: 0.637539  [ 2144/ 3200]\n",
      "loss: 0.951901  [ 2160/ 3200]\n",
      "loss: 0.883334  [ 2176/ 3200]\n",
      "loss: 0.792885  [ 2192/ 3200]\n",
      "loss: 0.561913  [ 2208/ 3200]\n",
      "loss: 0.709689  [ 2224/ 3200]\n",
      "loss: 0.710526  [ 2240/ 3200]\n",
      "loss: 0.478426  [ 2256/ 3200]\n",
      "loss: 0.736220  [ 2272/ 3200]\n",
      "loss: 0.882161  [ 2288/ 3200]\n",
      "loss: 0.681510  [ 2304/ 3200]\n",
      "loss: 0.582500  [ 2320/ 3200]\n",
      "loss: 0.513204  [ 2336/ 3200]\n",
      "loss: 0.507503  [ 2352/ 3200]\n",
      "loss: 0.836882  [ 2368/ 3200]\n",
      "loss: 0.481151  [ 2384/ 3200]\n",
      "loss: 0.550920  [ 2400/ 3200]\n",
      "loss: 0.734778  [ 2416/ 3200]\n",
      "loss: 0.717351  [ 2432/ 3200]\n",
      "loss: 1.011118  [ 2448/ 3200]\n",
      "loss: 0.369538  [ 2464/ 3200]\n",
      "loss: 0.425633  [ 2480/ 3200]\n",
      "loss: 0.533528  [ 2496/ 3200]\n",
      "loss: 0.465633  [ 2512/ 3200]\n",
      "loss: 0.824426  [ 2528/ 3200]\n",
      "loss: 0.313471  [ 2544/ 3200]\n",
      "loss: 0.751834  [ 2560/ 3200]\n",
      "loss: 0.802739  [ 2576/ 3200]\n",
      "loss: 1.013756  [ 2592/ 3200]\n",
      "loss: 1.012449  [ 2608/ 3200]\n",
      "loss: 0.669645  [ 2624/ 3200]\n",
      "loss: 0.707563  [ 2640/ 3200]\n",
      "loss: 0.253853  [ 2656/ 3200]\n",
      "loss: 0.587845  [ 2672/ 3200]\n",
      "loss: 0.541361  [ 2688/ 3200]\n",
      "loss: 0.656631  [ 2704/ 3200]\n",
      "loss: 0.696669  [ 2720/ 3200]\n",
      "loss: 0.835848  [ 2736/ 3200]\n",
      "loss: 0.623374  [ 2752/ 3200]\n",
      "loss: 0.621768  [ 2768/ 3200]\n",
      "loss: 1.195331  [ 2784/ 3200]\n",
      "loss: 1.043553  [ 2800/ 3200]\n",
      "loss: 0.760121  [ 2816/ 3200]\n",
      "loss: 1.000892  [ 2832/ 3200]\n",
      "loss: 0.985652  [ 2848/ 3200]\n",
      "loss: 0.814151  [ 2864/ 3200]\n",
      "loss: 0.689339  [ 2880/ 3200]\n",
      "loss: 0.665751  [ 2896/ 3200]\n",
      "loss: 0.824351  [ 2912/ 3200]\n",
      "loss: 0.915640  [ 2928/ 3200]\n",
      "loss: 0.843524  [ 2944/ 3200]\n",
      "loss: 0.729972  [ 2960/ 3200]\n",
      "loss: 0.568463  [ 2976/ 3200]\n",
      "loss: 0.923839  [ 2992/ 3200]\n",
      "loss: 0.780798  [ 3008/ 3200]\n",
      "loss: 0.661112  [ 3024/ 3200]\n",
      "loss: 0.544776  [ 3040/ 3200]\n",
      "loss: 0.727158  [ 3056/ 3200]\n",
      "loss: 0.661812  [ 3072/ 3200]\n",
      "loss: 0.731257  [ 3088/ 3200]\n",
      "loss: 0.886699  [ 3104/ 3200]\n",
      "loss: 0.625392  [ 3120/ 3200]\n",
      "loss: 0.515387  [ 3136/ 3200]\n",
      "loss: 0.986887  [ 3152/ 3200]\n",
      "loss: 0.576536  [ 3168/ 3200]\n",
      "loss: 0.954855  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.067554\n",
      "f1 macro averaged score: 0.533827\n",
      "Accuracy               : 55.9%\n",
      "Confusion matrix       :\n",
      "tensor([[ 93,  77,  12,  18],\n",
      "        [ 11,  35,  86,  68],\n",
      "        [  0,   1, 169,  30],\n",
      "        [  0,   4,  46, 150]], device='cuda:0')\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 1.182756  [    0/ 3200]\n",
      "loss: 0.773077  [   16/ 3200]\n",
      "loss: 0.547974  [   32/ 3200]\n",
      "loss: 0.534450  [   48/ 3200]\n",
      "loss: 0.868750  [   64/ 3200]\n",
      "loss: 0.820307  [   80/ 3200]\n",
      "loss: 0.559850  [   96/ 3200]\n",
      "loss: 0.417676  [  112/ 3200]\n",
      "loss: 0.741832  [  128/ 3200]\n",
      "loss: 0.452831  [  144/ 3200]\n",
      "loss: 0.568991  [  160/ 3200]\n",
      "loss: 0.655808  [  176/ 3200]\n",
      "loss: 0.937666  [  192/ 3200]\n",
      "loss: 0.751218  [  208/ 3200]\n",
      "loss: 0.866656  [  224/ 3200]\n",
      "loss: 0.476914  [  240/ 3200]\n",
      "loss: 0.501125  [  256/ 3200]\n",
      "loss: 0.728374  [  272/ 3200]\n",
      "loss: 0.529123  [  288/ 3200]\n",
      "loss: 0.660148  [  304/ 3200]\n",
      "loss: 0.782521  [  320/ 3200]\n",
      "loss: 0.784502  [  336/ 3200]\n",
      "loss: 0.678817  [  352/ 3200]\n",
      "loss: 0.639870  [  368/ 3200]\n",
      "loss: 0.961012  [  384/ 3200]\n",
      "loss: 0.653044  [  400/ 3200]\n",
      "loss: 0.638471  [  416/ 3200]\n",
      "loss: 0.970070  [  432/ 3200]\n",
      "loss: 0.726282  [  448/ 3200]\n",
      "loss: 0.649398  [  464/ 3200]\n",
      "loss: 0.423301  [  480/ 3200]\n",
      "loss: 0.900779  [  496/ 3200]\n",
      "loss: 0.644833  [  512/ 3200]\n",
      "loss: 0.629283  [  528/ 3200]\n",
      "loss: 0.551326  [  544/ 3200]\n",
      "loss: 0.580628  [  560/ 3200]\n",
      "loss: 0.600747  [  576/ 3200]\n",
      "loss: 0.535278  [  592/ 3200]\n",
      "loss: 0.882100  [  608/ 3200]\n",
      "loss: 0.584361  [  624/ 3200]\n",
      "loss: 0.636984  [  640/ 3200]\n",
      "loss: 0.494642  [  656/ 3200]\n",
      "loss: 0.581899  [  672/ 3200]\n",
      "loss: 0.718951  [  688/ 3200]\n",
      "loss: 0.403940  [  704/ 3200]\n",
      "loss: 0.638896  [  720/ 3200]\n",
      "loss: 0.425484  [  736/ 3200]\n",
      "loss: 0.701681  [  752/ 3200]\n",
      "loss: 0.845044  [  768/ 3200]\n",
      "loss: 0.524337  [  784/ 3200]\n",
      "loss: 0.807574  [  800/ 3200]\n",
      "loss: 0.405465  [  816/ 3200]\n",
      "loss: 0.495321  [  832/ 3200]\n",
      "loss: 1.083686  [  848/ 3200]\n",
      "loss: 1.018557  [  864/ 3200]\n",
      "loss: 0.774775  [  880/ 3200]\n",
      "loss: 0.865152  [  896/ 3200]\n",
      "loss: 0.532721  [  912/ 3200]\n",
      "loss: 0.332272  [  928/ 3200]\n",
      "loss: 0.728162  [  944/ 3200]\n",
      "loss: 0.671964  [  960/ 3200]\n",
      "loss: 0.897028  [  976/ 3200]\n",
      "loss: 0.384014  [  992/ 3200]\n",
      "loss: 0.732220  [ 1008/ 3200]\n",
      "loss: 0.597659  [ 1024/ 3200]\n",
      "loss: 0.600270  [ 1040/ 3200]\n",
      "loss: 0.406282  [ 1056/ 3200]\n",
      "loss: 0.815452  [ 1072/ 3200]\n",
      "loss: 1.243665  [ 1088/ 3200]\n",
      "loss: 0.964761  [ 1104/ 3200]\n",
      "loss: 0.651690  [ 1120/ 3200]\n",
      "loss: 1.198798  [ 1136/ 3200]\n",
      "loss: 0.658148  [ 1152/ 3200]\n",
      "loss: 0.573307  [ 1168/ 3200]\n",
      "loss: 0.864876  [ 1184/ 3200]\n",
      "loss: 0.780314  [ 1200/ 3200]\n",
      "loss: 0.807613  [ 1216/ 3200]\n",
      "loss: 0.791243  [ 1232/ 3200]\n",
      "loss: 0.570041  [ 1248/ 3200]\n",
      "loss: 0.770648  [ 1264/ 3200]\n",
      "loss: 0.876499  [ 1280/ 3200]\n",
      "loss: 0.692323  [ 1296/ 3200]\n",
      "loss: 0.807132  [ 1312/ 3200]\n",
      "loss: 0.619657  [ 1328/ 3200]\n",
      "loss: 0.849145  [ 1344/ 3200]\n",
      "loss: 0.451203  [ 1360/ 3200]\n",
      "loss: 0.849538  [ 1376/ 3200]\n",
      "loss: 0.970458  [ 1392/ 3200]\n",
      "loss: 0.482846  [ 1408/ 3200]\n",
      "loss: 0.513505  [ 1424/ 3200]\n",
      "loss: 0.764066  [ 1440/ 3200]\n",
      "loss: 0.573101  [ 1456/ 3200]\n",
      "loss: 0.669365  [ 1472/ 3200]\n",
      "loss: 0.650170  [ 1488/ 3200]\n",
      "loss: 0.748549  [ 1504/ 3200]\n",
      "loss: 0.841952  [ 1520/ 3200]\n",
      "loss: 0.907089  [ 1536/ 3200]\n",
      "loss: 0.820041  [ 1552/ 3200]\n",
      "loss: 0.791943  [ 1568/ 3200]\n",
      "loss: 0.428448  [ 1584/ 3200]\n",
      "loss: 0.514239  [ 1600/ 3200]\n",
      "loss: 0.541666  [ 1616/ 3200]\n",
      "loss: 0.381213  [ 1632/ 3200]\n",
      "loss: 0.686865  [ 1648/ 3200]\n",
      "loss: 0.539789  [ 1664/ 3200]\n",
      "loss: 0.524921  [ 1680/ 3200]\n",
      "loss: 0.725001  [ 1696/ 3200]\n",
      "loss: 1.147873  [ 1712/ 3200]\n",
      "loss: 0.618170  [ 1728/ 3200]\n",
      "loss: 0.539664  [ 1744/ 3200]\n",
      "loss: 0.601282  [ 1760/ 3200]\n",
      "loss: 0.488034  [ 1776/ 3200]\n",
      "loss: 0.544998  [ 1792/ 3200]\n",
      "loss: 0.763615  [ 1808/ 3200]\n",
      "loss: 0.793361  [ 1824/ 3200]\n",
      "loss: 0.601525  [ 1840/ 3200]\n",
      "loss: 0.701470  [ 1856/ 3200]\n",
      "loss: 0.657359  [ 1872/ 3200]\n",
      "loss: 0.683950  [ 1888/ 3200]\n",
      "loss: 0.819582  [ 1904/ 3200]\n",
      "loss: 0.512191  [ 1920/ 3200]\n",
      "loss: 0.766553  [ 1936/ 3200]\n",
      "loss: 0.523703  [ 1952/ 3200]\n",
      "loss: 0.645781  [ 1968/ 3200]\n",
      "loss: 0.636902  [ 1984/ 3200]\n",
      "loss: 0.705882  [ 2000/ 3200]\n",
      "loss: 0.537099  [ 2016/ 3200]\n",
      "loss: 0.611053  [ 2032/ 3200]\n",
      "loss: 0.959796  [ 2048/ 3200]\n",
      "loss: 0.584438  [ 2064/ 3200]\n",
      "loss: 0.540350  [ 2080/ 3200]\n",
      "loss: 0.690813  [ 2096/ 3200]\n",
      "loss: 0.612780  [ 2112/ 3200]\n",
      "loss: 0.658593  [ 2128/ 3200]\n",
      "loss: 0.831668  [ 2144/ 3200]\n",
      "loss: 0.653613  [ 2160/ 3200]\n",
      "loss: 0.732637  [ 2176/ 3200]\n",
      "loss: 0.607115  [ 2192/ 3200]\n",
      "loss: 0.637322  [ 2208/ 3200]\n",
      "loss: 0.774400  [ 2224/ 3200]\n",
      "loss: 0.425424  [ 2240/ 3200]\n",
      "loss: 0.526798  [ 2256/ 3200]\n",
      "loss: 0.596133  [ 2272/ 3200]\n",
      "loss: 0.677069  [ 2288/ 3200]\n",
      "loss: 0.586817  [ 2304/ 3200]\n",
      "loss: 0.519271  [ 2320/ 3200]\n",
      "loss: 0.524057  [ 2336/ 3200]\n",
      "loss: 0.622789  [ 2352/ 3200]\n",
      "loss: 0.557119  [ 2368/ 3200]\n",
      "loss: 0.692917  [ 2384/ 3200]\n",
      "loss: 0.397392  [ 2400/ 3200]\n",
      "loss: 0.393661  [ 2416/ 3200]\n",
      "loss: 0.586119  [ 2432/ 3200]\n",
      "loss: 0.555201  [ 2448/ 3200]\n",
      "loss: 0.612920  [ 2464/ 3200]\n",
      "loss: 0.580812  [ 2480/ 3200]\n",
      "loss: 0.441218  [ 2496/ 3200]\n",
      "loss: 0.600161  [ 2512/ 3200]\n",
      "loss: 0.551441  [ 2528/ 3200]\n",
      "loss: 0.710430  [ 2544/ 3200]\n",
      "loss: 0.555048  [ 2560/ 3200]\n",
      "loss: 0.275001  [ 2576/ 3200]\n",
      "loss: 0.735501  [ 2592/ 3200]\n",
      "loss: 1.018853  [ 2608/ 3200]\n",
      "loss: 0.595467  [ 2624/ 3200]\n",
      "loss: 1.035576  [ 2640/ 3200]\n",
      "loss: 1.075406  [ 2656/ 3200]\n",
      "loss: 1.092464  [ 2672/ 3200]\n",
      "loss: 0.644445  [ 2688/ 3200]\n",
      "loss: 0.756077  [ 2704/ 3200]\n",
      "loss: 0.555307  [ 2720/ 3200]\n",
      "loss: 0.411180  [ 2736/ 3200]\n",
      "loss: 0.621667  [ 2752/ 3200]\n",
      "loss: 0.881833  [ 2768/ 3200]\n",
      "loss: 1.181527  [ 2784/ 3200]\n",
      "loss: 0.753035  [ 2800/ 3200]\n",
      "loss: 0.473265  [ 2816/ 3200]\n",
      "loss: 0.596572  [ 2832/ 3200]\n",
      "loss: 0.733343  [ 2848/ 3200]\n",
      "loss: 1.139204  [ 2864/ 3200]\n",
      "loss: 0.318239  [ 2880/ 3200]\n",
      "loss: 0.478075  [ 2896/ 3200]\n",
      "loss: 0.796016  [ 2912/ 3200]\n",
      "loss: 0.710673  [ 2928/ 3200]\n",
      "loss: 0.640387  [ 2944/ 3200]\n",
      "loss: 0.577956  [ 2960/ 3200]\n",
      "loss: 1.193395  [ 2976/ 3200]\n",
      "loss: 0.414729  [ 2992/ 3200]\n",
      "loss: 0.580834  [ 3008/ 3200]\n",
      "loss: 0.572276  [ 3024/ 3200]\n",
      "loss: 0.405568  [ 3040/ 3200]\n",
      "loss: 0.953469  [ 3056/ 3200]\n",
      "loss: 0.674908  [ 3072/ 3200]\n",
      "loss: 0.431989  [ 3088/ 3200]\n",
      "loss: 0.940905  [ 3104/ 3200]\n",
      "loss: 0.707266  [ 3120/ 3200]\n",
      "loss: 1.081283  [ 3136/ 3200]\n",
      "loss: 0.695025  [ 3152/ 3200]\n",
      "loss: 0.851960  [ 3168/ 3200]\n",
      "loss: 0.678204  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.050195\n",
      "f1 macro averaged score: 0.663962\n",
      "Accuracy               : 66.0%\n",
      "Confusion matrix       :\n",
      "tensor([[144,  53,   3,   0],\n",
      "        [ 19, 122,  43,  16],\n",
      "        [  0,  22, 156,  22],\n",
      "        [  2,  55,  37, 106]], device='cuda:0')\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.615186  [    0/ 3200]\n",
      "loss: 0.881220  [   16/ 3200]\n",
      "loss: 0.536159  [   32/ 3200]\n",
      "loss: 0.599783  [   48/ 3200]\n",
      "loss: 0.587009  [   64/ 3200]\n",
      "loss: 0.583038  [   80/ 3200]\n",
      "loss: 0.882935  [   96/ 3200]\n",
      "loss: 0.477559  [  112/ 3200]\n",
      "loss: 0.677279  [  128/ 3200]\n",
      "loss: 0.641968  [  144/ 3200]\n",
      "loss: 0.753298  [  160/ 3200]\n",
      "loss: 0.663240  [  176/ 3200]\n",
      "loss: 0.657068  [  192/ 3200]\n",
      "loss: 0.385415  [  208/ 3200]\n",
      "loss: 1.066261  [  224/ 3200]\n",
      "loss: 0.703436  [  240/ 3200]\n",
      "loss: 0.736542  [  256/ 3200]\n",
      "loss: 0.622269  [  272/ 3200]\n",
      "loss: 0.905487  [  288/ 3200]\n",
      "loss: 0.484155  [  304/ 3200]\n",
      "loss: 0.737213  [  320/ 3200]\n",
      "loss: 0.528529  [  336/ 3200]\n",
      "loss: 0.550082  [  352/ 3200]\n",
      "loss: 0.412071  [  368/ 3200]\n",
      "loss: 0.510062  [  384/ 3200]\n",
      "loss: 0.507554  [  400/ 3200]\n",
      "loss: 0.430709  [  416/ 3200]\n",
      "loss: 0.582307  [  432/ 3200]\n",
      "loss: 0.776032  [  448/ 3200]\n",
      "loss: 1.028840  [  464/ 3200]\n",
      "loss: 1.162653  [  480/ 3200]\n",
      "loss: 0.646881  [  496/ 3200]\n",
      "loss: 0.537907  [  512/ 3200]\n",
      "loss: 0.605440  [  528/ 3200]\n",
      "loss: 0.493418  [  544/ 3200]\n",
      "loss: 0.370072  [  560/ 3200]\n",
      "loss: 0.602102  [  576/ 3200]\n",
      "loss: 0.548915  [  592/ 3200]\n",
      "loss: 0.579240  [  608/ 3200]\n",
      "loss: 0.353023  [  624/ 3200]\n",
      "loss: 0.889921  [  640/ 3200]\n",
      "loss: 0.683796  [  656/ 3200]\n",
      "loss: 0.899722  [  672/ 3200]\n",
      "loss: 0.529056  [  688/ 3200]\n",
      "loss: 0.559724  [  704/ 3200]\n",
      "loss: 0.390416  [  720/ 3200]\n",
      "loss: 0.683584  [  736/ 3200]\n",
      "loss: 0.598661  [  752/ 3200]\n",
      "loss: 1.050607  [  768/ 3200]\n",
      "loss: 0.755839  [  784/ 3200]\n",
      "loss: 0.478193  [  800/ 3200]\n",
      "loss: 0.467232  [  816/ 3200]\n",
      "loss: 0.649963  [  832/ 3200]\n",
      "loss: 0.558537  [  848/ 3200]\n",
      "loss: 0.616386  [  864/ 3200]\n",
      "loss: 0.722878  [  880/ 3200]\n",
      "loss: 0.486807  [  896/ 3200]\n",
      "loss: 0.604745  [  912/ 3200]\n",
      "loss: 0.735923  [  928/ 3200]\n",
      "loss: 0.735182  [  944/ 3200]\n",
      "loss: 0.537986  [  960/ 3200]\n",
      "loss: 0.517981  [  976/ 3200]\n",
      "loss: 1.124619  [  992/ 3200]\n",
      "loss: 0.576119  [ 1008/ 3200]\n",
      "loss: 0.948117  [ 1024/ 3200]\n",
      "loss: 0.857829  [ 1040/ 3200]\n",
      "loss: 0.495344  [ 1056/ 3200]\n",
      "loss: 0.661197  [ 1072/ 3200]\n",
      "loss: 0.524158  [ 1088/ 3200]\n",
      "loss: 0.578224  [ 1104/ 3200]\n",
      "loss: 0.802141  [ 1120/ 3200]\n",
      "loss: 0.912389  [ 1136/ 3200]\n",
      "loss: 0.651738  [ 1152/ 3200]\n",
      "loss: 0.517593  [ 1168/ 3200]\n",
      "loss: 0.678919  [ 1184/ 3200]\n",
      "loss: 0.474329  [ 1200/ 3200]\n",
      "loss: 0.552927  [ 1216/ 3200]\n",
      "loss: 1.000120  [ 1232/ 3200]\n",
      "loss: 0.557299  [ 1248/ 3200]\n",
      "loss: 0.674302  [ 1264/ 3200]\n",
      "loss: 0.638022  [ 1280/ 3200]\n",
      "loss: 0.663546  [ 1296/ 3200]\n",
      "loss: 0.673471  [ 1312/ 3200]\n",
      "loss: 0.394546  [ 1328/ 3200]\n",
      "loss: 0.506129  [ 1344/ 3200]\n",
      "loss: 0.490787  [ 1360/ 3200]\n",
      "loss: 0.714155  [ 1376/ 3200]\n",
      "loss: 0.766026  [ 1392/ 3200]\n",
      "loss: 0.681848  [ 1408/ 3200]\n",
      "loss: 0.685021  [ 1424/ 3200]\n",
      "loss: 0.587476  [ 1440/ 3200]\n",
      "loss: 0.427086  [ 1456/ 3200]\n",
      "loss: 0.733443  [ 1472/ 3200]\n",
      "loss: 0.829204  [ 1488/ 3200]\n",
      "loss: 0.496437  [ 1504/ 3200]\n",
      "loss: 0.723007  [ 1520/ 3200]\n",
      "loss: 0.566223  [ 1536/ 3200]\n",
      "loss: 0.679260  [ 1552/ 3200]\n",
      "loss: 1.009399  [ 1568/ 3200]\n",
      "loss: 0.327316  [ 1584/ 3200]\n",
      "loss: 0.734410  [ 1600/ 3200]\n",
      "loss: 0.489250  [ 1616/ 3200]\n",
      "loss: 0.726522  [ 1632/ 3200]\n",
      "loss: 0.612947  [ 1648/ 3200]\n",
      "loss: 1.109156  [ 1664/ 3200]\n",
      "loss: 0.936218  [ 1680/ 3200]\n",
      "loss: 0.674132  [ 1696/ 3200]\n",
      "loss: 0.398433  [ 1712/ 3200]\n",
      "loss: 0.744989  [ 1728/ 3200]\n",
      "loss: 0.634987  [ 1744/ 3200]\n",
      "loss: 1.016148  [ 1760/ 3200]\n",
      "loss: 0.752218  [ 1776/ 3200]\n",
      "loss: 0.646020  [ 1792/ 3200]\n",
      "loss: 0.495280  [ 1808/ 3200]\n",
      "loss: 0.551417  [ 1824/ 3200]\n",
      "loss: 0.662188  [ 1840/ 3200]\n",
      "loss: 0.715006  [ 1856/ 3200]\n",
      "loss: 0.668967  [ 1872/ 3200]\n",
      "loss: 0.632775  [ 1888/ 3200]\n",
      "loss: 0.469559  [ 1904/ 3200]\n",
      "loss: 0.697734  [ 1920/ 3200]\n",
      "loss: 0.635984  [ 1936/ 3200]\n",
      "loss: 0.496389  [ 1952/ 3200]\n",
      "loss: 0.754927  [ 1968/ 3200]\n",
      "loss: 0.751213  [ 1984/ 3200]\n",
      "loss: 0.567055  [ 2000/ 3200]\n",
      "loss: 0.360214  [ 2016/ 3200]\n",
      "loss: 0.685149  [ 2032/ 3200]\n",
      "loss: 0.506972  [ 2048/ 3200]\n",
      "loss: 0.392191  [ 2064/ 3200]\n",
      "loss: 0.599264  [ 2080/ 3200]\n",
      "loss: 0.760900  [ 2096/ 3200]\n",
      "loss: 0.671027  [ 2112/ 3200]\n",
      "loss: 0.391931  [ 2128/ 3200]\n",
      "loss: 0.539246  [ 2144/ 3200]\n",
      "loss: 0.728062  [ 2160/ 3200]\n",
      "loss: 0.689502  [ 2176/ 3200]\n",
      "loss: 0.480075  [ 2192/ 3200]\n",
      "loss: 0.502674  [ 2208/ 3200]\n",
      "loss: 1.198916  [ 2224/ 3200]\n",
      "loss: 1.046535  [ 2240/ 3200]\n",
      "loss: 0.980647  [ 2256/ 3200]\n",
      "loss: 0.768869  [ 2272/ 3200]\n",
      "loss: 0.553856  [ 2288/ 3200]\n",
      "loss: 1.101197  [ 2304/ 3200]\n",
      "loss: 0.396153  [ 2320/ 3200]\n",
      "loss: 0.545587  [ 2336/ 3200]\n",
      "loss: 0.776936  [ 2352/ 3200]\n",
      "loss: 0.595971  [ 2368/ 3200]\n",
      "loss: 0.651398  [ 2384/ 3200]\n",
      "loss: 0.773299  [ 2400/ 3200]\n",
      "loss: 0.429345  [ 2416/ 3200]\n",
      "loss: 0.477341  [ 2432/ 3200]\n",
      "loss: 0.469134  [ 2448/ 3200]\n",
      "loss: 0.796341  [ 2464/ 3200]\n",
      "loss: 0.425371  [ 2480/ 3200]\n",
      "loss: 1.151350  [ 2496/ 3200]\n",
      "loss: 0.487923  [ 2512/ 3200]\n",
      "loss: 0.918058  [ 2528/ 3200]\n",
      "loss: 0.522466  [ 2544/ 3200]\n",
      "loss: 0.657185  [ 2560/ 3200]\n",
      "loss: 0.596496  [ 2576/ 3200]\n",
      "loss: 0.673121  [ 2592/ 3200]\n",
      "loss: 0.630322  [ 2608/ 3200]\n",
      "loss: 0.538973  [ 2624/ 3200]\n",
      "loss: 0.792760  [ 2640/ 3200]\n",
      "loss: 0.836248  [ 2656/ 3200]\n",
      "loss: 0.704529  [ 2672/ 3200]\n",
      "loss: 0.531079  [ 2688/ 3200]\n",
      "loss: 0.744130  [ 2704/ 3200]\n",
      "loss: 0.877529  [ 2720/ 3200]\n",
      "loss: 0.315397  [ 2736/ 3200]\n",
      "loss: 0.859863  [ 2752/ 3200]\n",
      "loss: 0.900863  [ 2768/ 3200]\n",
      "loss: 1.123783  [ 2784/ 3200]\n",
      "loss: 0.811567  [ 2800/ 3200]\n",
      "loss: 0.708286  [ 2816/ 3200]\n",
      "loss: 0.890710  [ 2832/ 3200]\n",
      "loss: 0.525881  [ 2848/ 3200]\n",
      "loss: 1.078373  [ 2864/ 3200]\n",
      "loss: 1.044225  [ 2880/ 3200]\n",
      "loss: 0.500175  [ 2896/ 3200]\n",
      "loss: 0.394555  [ 2912/ 3200]\n",
      "loss: 0.476774  [ 2928/ 3200]\n",
      "loss: 0.851267  [ 2944/ 3200]\n",
      "loss: 0.643446  [ 2960/ 3200]\n",
      "loss: 0.691577  [ 2976/ 3200]\n",
      "loss: 0.813333  [ 2992/ 3200]\n",
      "loss: 0.625374  [ 3008/ 3200]\n",
      "loss: 0.783282  [ 3024/ 3200]\n",
      "loss: 0.773809  [ 3040/ 3200]\n",
      "loss: 1.028778  [ 3056/ 3200]\n",
      "loss: 0.658487  [ 3072/ 3200]\n",
      "loss: 0.245967  [ 3088/ 3200]\n",
      "loss: 0.680003  [ 3104/ 3200]\n",
      "loss: 0.528143  [ 3120/ 3200]\n",
      "loss: 0.306889  [ 3136/ 3200]\n",
      "loss: 0.672706  [ 3152/ 3200]\n",
      "loss: 0.591486  [ 3168/ 3200]\n",
      "loss: 0.564284  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.049731\n",
      "f1 macro averaged score: 0.658172\n",
      "Accuracy               : 66.5%\n",
      "Confusion matrix       :\n",
      "tensor([[161,  31,   4,   4],\n",
      "        [ 28,  82,  51,  39],\n",
      "        [  0,  10, 160,  30],\n",
      "        [  5,  28,  38, 129]], device='cuda:0')\n",
      "\n",
      "Best epoch: 27 with f1 macro averaged score: 0.6772592663764954\n",
      "Test Error:\n",
      "Avg loss               : 0.050933\n",
      "f1 macro averaged score: 0.658436\n",
      "Accuracy               : 66.4%\n",
      "Confusion matrix       :\n",
      "tensor([[255,  28,   9,   5],\n",
      "        [ 27, 130,  70,  97],\n",
      "        [  4,  18, 301,  33],\n",
      "        [ 20,  61,  91, 227]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizers = {\"Adadelta\": Adadelta, \"Adagrad\": Adagrad, \"Adam\": Adam, \"AdamW\": AdamW, \"Adamax\": Adamax,\n",
    "              \"ASGD\": ASGD, \"NAdam\": NAdam, \"RAdam\": RAdam, \"RMSprop\": RMSprop, \"Rprop\": Rprop, \"SGD\": SGD}\n",
    "\n",
    "learning_rate = 0.002\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "f1_accuracy = []\n",
    "for opt, optimizer in optimizers.items():\n",
    "  print(\"  Optimizer:\", opt)\n",
    "  cnn_model = Net().to(device)\n",
    "  optimizer = optimizer(params=cnn_model.parameters(), lr=learning_rate)\n",
    "  best_model, f1_per_epoch = validate_convolutional_neural_network(epochs, optimizer, train_dataloader, val_dataloader, loss_function, cnn_model)\n",
    "  results = test_convolutional_neural_network(test_dataloader, loss_function, best_model)\n",
    "  f1_accuracy.append((opt, results[1], results[2]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPXgjELzxnsW"
   },
   "source": [
    "Execution time: $~5$ minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aszH5XgTpnO"
   },
   "source": [
    "Sort test results with the best Convolutional Model for each optimizer in decreasing order according to the f1 macro average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bA-wKHnqQbj1",
    "outputId": "891e4266-980c-41b9-fc50-6a85d82aa2d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer\tf1_macro_avg\tAccuracy\n",
      "Adagrad\t\t0.750259\t74.273256\n",
      "Adamax\t\t0.730673\t73.037791\n",
      "AdamW\t\t0.688723\t69.404070\n",
      "RMSprop\t\t0.676906\t68.168605\n",
      "Adam\t\t0.670797\t66.279070\n",
      "NAdam\t\t0.669163\t66.061047\n",
      "SGD\t\t0.658436\t66.351744\n",
      "RAdam\t\t0.636210\t66.133721\n",
      "Rprop\t\t0.620761\t61.627907\n",
      "ASGD\t\t0.612188\t61.845930\n",
      "Adadelta\t0.551784\t53.924419\n"
     ]
    }
   ],
   "source": [
    "f1_accuracy = sorted(f1_accuracy, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Optimizer\\tf1_macro_avg\\tAccuracy\")\n",
    "for (opt, f1_macro_avg, accuracy) in f1_accuracy:\n",
    "  if opt == 'Adadelta': # for better visualization\n",
    "    print(f\"{opt}\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")\n",
    "    continue\n",
    "  print(f\"{opt}\\t\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf1iZaLpTpDl"
   },
   "source": [
    "The table above shows that the optimizer plays a big role in the development of the model, since it can really change the results during testing.\n",
    "\n",
    "The best optimizer for our Convolutional Neural Network was Adagrad with $74\\%$ accuracy and the worst was Adadelta with $53\\%$ accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwiW96f6Ul6C"
   },
   "source": [
    "# Question 3 - Improving Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGI9UyJbU1wk"
   },
   "source": [
    "### Step 1 - Reproducibility\n",
    "\n",
    "Seed all libraries and algorithms, so that the model is deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "MV15pGjPUo7h"
   },
   "outputs": [],
   "source": [
    "def torch_seed(seed=0):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  os.environ['PYTHONASHSEED'] = str(seed)\n",
    "\n",
    "def set_epoch(self, epoch):\n",
    "  self.epoch = epoch\n",
    "  self.generator.manual_seed(0 + self.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWvOoo0XvDoR"
   },
   "source": [
    "Repeat **Step 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LUPXEEc5vGDY"
   },
   "outputs": [],
   "source": [
    "melgram_x_train = np.load('/content/music_genre_data_di/train/melgrams/X.npy')\n",
    "melgram_y_train = np.load('/content/music_genre_data_di/train/melgrams/labels.npy')\n",
    "\n",
    "melgram_x_test  = np.load('/content/music_genre_data_di/test/melgrams/X.npy')\n",
    "melgram_y_test  = np.load('/content/music_genre_data_di/test/melgrams/labels.npy')\n",
    "\n",
    "melgram_x_val   = np.load('/content/music_genre_data_di/val/melgrams/X.npy')\n",
    "melgram_y_val   = np.load('/content/music_genre_data_di/val/melgrams/labels.npy')\n",
    "\n",
    "melgram_y_train = convert_labels(melgram_y_train)\n",
    "melgram_y_test  = convert_labels(melgram_y_test)\n",
    "melgram_y_val   = convert_labels(melgram_y_val)\n",
    "\n",
    "train_dataloader = DataLoader(list(zip(melgram_x_train, melgram_y_train)), batch_size=16, shuffle=True, generator=torch.Generator(device='cpu'))\n",
    "train_dataloader.set_epoch = set_epoch\n",
    "\n",
    "test_dataloader  = DataLoader(list(zip(melgram_x_test, melgram_y_test)), batch_size=16, shuffle=True, generator=torch.Generator(device='cpu'))\n",
    "test_dataloader.set_epoch = set_epoch\n",
    "\n",
    "val_dataloader   = DataLoader(list(zip(melgram_x_val, melgram_y_val)), batch_size=16, shuffle=True, generator=torch.Generator(device='cpu'))\n",
    "val_dataloader.set_epoch = set_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffamFg54slha"
   },
   "source": [
    "Redefine the Convolutional Neural Network (same code as in **Step 4**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wE17yx79vUfY"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # convolutional layers\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, padding=2)   # 1 channel   -> 16 channels\n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2)  # 16 channels -> 32 channels\n",
    "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)  # 32 channels -> 64 channels\n",
    "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2) # 64 channels -> 128 channels\n",
    "\n",
    "    # fully connected (dense) layers\n",
    "    self.dense1 = nn.Linear(1024, 1024) # input size = 1024 -> number of perceptrons in 1st hidden layer = 1024\n",
    "    self.dense2 = nn.Linear(1024, 256)  # number of perceptrons in 1st hidden layer = 1024 ->\n",
    "                                        # number of perceptrons in 2nd hidden layer = 256\n",
    "    self.dense3 = nn.Linear(256, 32)    # number of perceptrons in 2nd hidden layer = 256 ->\n",
    "                                        # number of perceptrons in 3rd hidden layer = 32\n",
    "    self.dense4 = nn.Linear(32, 4)      # number of perceptrons in 3rd hidden layer = 32 ->\n",
    "                                        # number of different labels = 4\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv3(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv4(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    x = torch.flatten(x, 1)\n",
    "\n",
    "    x = self.dense1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dense2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dense3(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dense4(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ7ddzsUwO3I"
   },
   "source": [
    "Redefine the training, testing and validation procedure in order to make the model reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr066yNDviyC"
   },
   "outputs": [],
   "source": [
    "def train_convolutional_neural_network(epochs, optimizer, dataloader, loss_function, model, reproducibility=False):\n",
    "  size = len(dataloader.dataset)\n",
    "\n",
    "  for epoch in range(0, epochs):\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"-----------------------------\")\n",
    "    if reproducibility: # added\n",
    "      dataloader.set_epoch(dataloader, epoch)\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(torch.unsqueeze(x, 1).type(torch.float))\n",
    "      loss = loss_function(prediction, y)\n",
    "\n",
    "      # backpropagation\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss, current = loss.item(), batch * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    print()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "YYilnid0v_uB"
   },
   "outputs": [],
   "source": [
    "def test_convolutional_neural_network(dataloader, loss_function, model):\n",
    "  size = len(dataloader.dataset)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  predictions = []\n",
    "  labels = []\n",
    "  with torch.no_grad():\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(torch.unsqueeze(x, 1))\n",
    "      predictions.append(prediction.argmax(1))\n",
    "      test_loss += loss_function(prediction, y)\n",
    "      correct += (prediction.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "      # keep labels for later\n",
    "      labels.append(y)\n",
    "\n",
    "  test_loss /= size\n",
    "  accuracy = 100 * (correct / size)\n",
    "  predictions = torch.cat(predictions)\n",
    "  labels = torch.cat(labels)\n",
    "  f1_macro_avg = f1_score(predictions, labels, task='multiclass', num_classes=4, average='macro')\n",
    "\n",
    "  # move confusion matrix to GPU if GPU is in use\n",
    "  confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=4).to(device)\n",
    "  print(\"Test Error:\")\n",
    "  print(f\"Avg loss               : {test_loss:>8f}\")\n",
    "  print(f\"f1 macro averaged score: {f1_macro_avg:>8f}\")\n",
    "  print(f\"Accuracy               : {accuracy:>0.1f}%\")\n",
    "  print(f\"Confusion matrix       :\")\n",
    "  print(confusion_matrix(predictions, labels))\n",
    "\n",
    "  return test_loss, f1_macro_avg, accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_nT-5fQwIHI"
   },
   "outputs": [],
   "source": [
    "def validate_convolutional_neural_network(epochs, optimizer, train_dataloader, val_dataloader, loss_function, model, reproducibility):\n",
    "  size = len(train_dataloader.dataset)\n",
    "\n",
    "  best = (0, 0, 0) # best model, best epoch, best f1 score\n",
    "  f1_list = []\n",
    "\n",
    "  for epoch in range(0, epochs):\n",
    "    # train model with train data\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"-----------------------------\")\n",
    "    if reproducibility: # added\n",
    "      train_dataloader.set_epoch(train_dataloader, epoch)\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(torch.unsqueeze(x, 1))\n",
    "      loss = loss_function(prediction, y)\n",
    "\n",
    "      # backpropagation\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss, current = loss.item(), batch * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # test model on validation set\n",
    "    results = test_convolutional_neural_network(val_dataloader, loss_function, model)\n",
    "    f1_macro_avg = results[1]\n",
    "    f1_list.append(f1_macro_avg)\n",
    "    if f1_macro_avg > best[2]:\n",
    "      best = (model, epoch, f1_macro_avg)\n",
    "    print()\n",
    "\n",
    "  print(f\"Best epoch: {(best[1] + 1)} with f1 macro averaged score: {best[2]}\")\n",
    "  return best[0], f1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_gnfDzskjP7"
   },
   "source": [
    "Test the seeding done above, by training the model twice.\n",
    "\n",
    "Results should now be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2QthUnudlInj",
    "outputId": "9f4af01f-7d8a-4270-cdbf-60cb3e300420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 0.772601  [ 1312/ 3200]\n",
      "loss: 0.603267  [ 1328/ 3200]\n",
      "loss: 0.389988  [ 1344/ 3200]\n",
      "loss: 0.312300  [ 1360/ 3200]\n",
      "loss: 0.649883  [ 1376/ 3200]\n",
      "loss: 0.402415  [ 1392/ 3200]\n",
      "loss: 0.379302  [ 1408/ 3200]\n",
      "loss: 0.487795  [ 1424/ 3200]\n",
      "loss: 0.817650  [ 1440/ 3200]\n",
      "loss: 0.879129  [ 1456/ 3200]\n",
      "loss: 0.573630  [ 1472/ 3200]\n",
      "loss: 0.795514  [ 1488/ 3200]\n",
      "loss: 0.538295  [ 1504/ 3200]\n",
      "loss: 0.242006  [ 1520/ 3200]\n",
      "loss: 0.562402  [ 1536/ 3200]\n",
      "loss: 0.397960  [ 1552/ 3200]\n",
      "loss: 0.680610  [ 1568/ 3200]\n",
      "loss: 0.461949  [ 1584/ 3200]\n",
      "loss: 0.516605  [ 1600/ 3200]\n",
      "loss: 0.414831  [ 1616/ 3200]\n",
      "loss: 0.348879  [ 1632/ 3200]\n",
      "loss: 0.289238  [ 1648/ 3200]\n",
      "loss: 0.464025  [ 1664/ 3200]\n",
      "loss: 0.299047  [ 1680/ 3200]\n",
      "loss: 0.517515  [ 1696/ 3200]\n",
      "loss: 0.367538  [ 1712/ 3200]\n",
      "loss: 0.354482  [ 1728/ 3200]\n",
      "loss: 0.381026  [ 1744/ 3200]\n",
      "loss: 0.514871  [ 1760/ 3200]\n",
      "loss: 0.283617  [ 1776/ 3200]\n",
      "loss: 0.847765  [ 1792/ 3200]\n",
      "loss: 0.800440  [ 1808/ 3200]\n",
      "loss: 0.441479  [ 1824/ 3200]\n",
      "loss: 0.393025  [ 1840/ 3200]\n",
      "loss: 0.506551  [ 1856/ 3200]\n",
      "loss: 0.707691  [ 1872/ 3200]\n",
      "loss: 0.276830  [ 1888/ 3200]\n",
      "loss: 0.212400  [ 1904/ 3200]\n",
      "loss: 0.353439  [ 1920/ 3200]\n",
      "loss: 0.342095  [ 1936/ 3200]\n",
      "loss: 0.416285  [ 1952/ 3200]\n",
      "loss: 1.338422  [ 1968/ 3200]\n",
      "loss: 0.333762  [ 1984/ 3200]\n",
      "loss: 0.399717  [ 2000/ 3200]\n",
      "loss: 0.569322  [ 2016/ 3200]\n",
      "loss: 0.518103  [ 2032/ 3200]\n",
      "loss: 0.437927  [ 2048/ 3200]\n",
      "loss: 0.287604  [ 2064/ 3200]\n",
      "loss: 0.506500  [ 2080/ 3200]\n",
      "loss: 0.696222  [ 2096/ 3200]\n",
      "loss: 0.314076  [ 2112/ 3200]\n",
      "loss: 0.357327  [ 2128/ 3200]\n",
      "loss: 0.705183  [ 2144/ 3200]\n",
      "loss: 0.912338  [ 2160/ 3200]\n",
      "loss: 0.869701  [ 2176/ 3200]\n",
      "loss: 0.681277  [ 2192/ 3200]\n",
      "loss: 0.614283  [ 2208/ 3200]\n",
      "loss: 0.846037  [ 2224/ 3200]\n",
      "loss: 0.747912  [ 2240/ 3200]\n",
      "loss: 0.622996  [ 2256/ 3200]\n",
      "loss: 0.959305  [ 2272/ 3200]\n",
      "loss: 0.554042  [ 2288/ 3200]\n",
      "loss: 0.866651  [ 2304/ 3200]\n",
      "loss: 0.481421  [ 2320/ 3200]\n",
      "loss: 0.481797  [ 2336/ 3200]\n",
      "loss: 1.029407  [ 2352/ 3200]\n",
      "loss: 0.548087  [ 2368/ 3200]\n",
      "loss: 0.532413  [ 2384/ 3200]\n",
      "loss: 0.702358  [ 2400/ 3200]\n",
      "loss: 0.701010  [ 2416/ 3200]\n",
      "loss: 0.401129  [ 2432/ 3200]\n",
      "loss: 0.512014  [ 2448/ 3200]\n",
      "loss: 0.221664  [ 2464/ 3200]\n",
      "loss: 0.596591  [ 2480/ 3200]\n",
      "loss: 0.528545  [ 2496/ 3200]\n",
      "loss: 0.639882  [ 2512/ 3200]\n",
      "loss: 0.345012  [ 2528/ 3200]\n",
      "loss: 0.633696  [ 2544/ 3200]\n",
      "loss: 0.292065  [ 2560/ 3200]\n",
      "loss: 0.370461  [ 2576/ 3200]\n",
      "loss: 0.365508  [ 2592/ 3200]\n",
      "loss: 0.388842  [ 2608/ 3200]\n",
      "loss: 0.803300  [ 2624/ 3200]\n",
      "loss: 0.479068  [ 2640/ 3200]\n",
      "loss: 0.654812  [ 2656/ 3200]\n",
      "loss: 0.268814  [ 2672/ 3200]\n",
      "loss: 0.477607  [ 2688/ 3200]\n",
      "loss: 0.444235  [ 2704/ 3200]\n",
      "loss: 0.724645  [ 2720/ 3200]\n",
      "loss: 0.404299  [ 2736/ 3200]\n",
      "loss: 0.569152  [ 2752/ 3200]\n",
      "loss: 0.362218  [ 2768/ 3200]\n",
      "loss: 0.408683  [ 2784/ 3200]\n",
      "loss: 0.188690  [ 2800/ 3200]\n",
      "loss: 0.628834  [ 2816/ 3200]\n",
      "loss: 0.208608  [ 2832/ 3200]\n",
      "loss: 0.540076  [ 2848/ 3200]\n",
      "loss: 0.244259  [ 2864/ 3200]\n",
      "loss: 0.239665  [ 2880/ 3200]\n",
      "loss: 0.798999  [ 2896/ 3200]\n",
      "loss: 0.672903  [ 2912/ 3200]\n",
      "loss: 0.272219  [ 2928/ 3200]\n",
      "loss: 0.578040  [ 2944/ 3200]\n",
      "loss: 0.461077  [ 2960/ 3200]\n",
      "loss: 0.346330  [ 2976/ 3200]\n",
      "loss: 0.389838  [ 2992/ 3200]\n",
      "loss: 0.532002  [ 3008/ 3200]\n",
      "loss: 0.417661  [ 3024/ 3200]\n",
      "loss: 0.563619  [ 3040/ 3200]\n",
      "loss: 0.279548  [ 3056/ 3200]\n",
      "loss: 0.443133  [ 3072/ 3200]\n",
      "loss: 0.758234  [ 3088/ 3200]\n",
      "loss: 0.442722  [ 3104/ 3200]\n",
      "loss: 0.420132  [ 3120/ 3200]\n",
      "loss: 0.514042  [ 3136/ 3200]\n",
      "loss: 0.825135  [ 3152/ 3200]\n",
      "loss: 0.427677  [ 3168/ 3200]\n",
      "loss: 1.111708  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 7\n",
      "-----------------------------\n",
      "loss: 0.604259  [    0/ 3200]\n",
      "loss: 0.545092  [   16/ 3200]\n",
      "loss: 0.385484  [   32/ 3200]\n",
      "loss: 0.553044  [   48/ 3200]\n",
      "loss: 0.483814  [   64/ 3200]\n",
      "loss: 0.418348  [   80/ 3200]\n",
      "loss: 0.608680  [   96/ 3200]\n",
      "loss: 0.508639  [  112/ 3200]\n",
      "loss: 0.642649  [  128/ 3200]\n",
      "loss: 0.849038  [  144/ 3200]\n",
      "loss: 0.729633  [  160/ 3200]\n",
      "loss: 0.270022  [  176/ 3200]\n",
      "loss: 0.480275  [  192/ 3200]\n",
      "loss: 0.368840  [  208/ 3200]\n",
      "loss: 0.475082  [  224/ 3200]\n",
      "loss: 0.777873  [  240/ 3200]\n",
      "loss: 0.519397  [  256/ 3200]\n",
      "loss: 0.436087  [  272/ 3200]\n",
      "loss: 0.345529  [  288/ 3200]\n",
      "loss: 0.423247  [  304/ 3200]\n",
      "loss: 0.454200  [  320/ 3200]\n",
      "loss: 0.698564  [  336/ 3200]\n",
      "loss: 0.410207  [  352/ 3200]\n",
      "loss: 0.499379  [  368/ 3200]\n",
      "loss: 0.386979  [  384/ 3200]\n",
      "loss: 0.569373  [  400/ 3200]\n",
      "loss: 0.624481  [  416/ 3200]\n",
      "loss: 0.821198  [  432/ 3200]\n",
      "loss: 0.543597  [  448/ 3200]\n",
      "loss: 0.428619  [  464/ 3200]\n",
      "loss: 0.266225  [  480/ 3200]\n",
      "loss: 0.595347  [  496/ 3200]\n",
      "loss: 0.646592  [  512/ 3200]\n",
      "loss: 0.451592  [  528/ 3200]\n",
      "loss: 0.422989  [  544/ 3200]\n",
      "loss: 0.241556  [  560/ 3200]\n",
      "loss: 0.617520  [  576/ 3200]\n",
      "loss: 0.510072  [  592/ 3200]\n",
      "loss: 0.218113  [  608/ 3200]\n",
      "loss: 0.532976  [  624/ 3200]\n",
      "loss: 0.299912  [  640/ 3200]\n",
      "loss: 0.568298  [  656/ 3200]\n",
      "loss: 0.711312  [  672/ 3200]\n",
      "loss: 0.545256  [  688/ 3200]\n",
      "loss: 0.254038  [  704/ 3200]\n",
      "loss: 0.548665  [  720/ 3200]\n",
      "loss: 0.256588  [  736/ 3200]\n",
      "loss: 0.459878  [  752/ 3200]\n",
      "loss: 0.434090  [  768/ 3200]\n",
      "loss: 0.293704  [  784/ 3200]\n",
      "loss: 0.407704  [  800/ 3200]\n",
      "loss: 0.615826  [  816/ 3200]\n",
      "loss: 0.466815  [  832/ 3200]\n",
      "loss: 0.356685  [  848/ 3200]\n",
      "loss: 0.639793  [  864/ 3200]\n",
      "loss: 0.324015  [  880/ 3200]\n",
      "loss: 0.332584  [  896/ 3200]\n",
      "loss: 0.414069  [  912/ 3200]\n",
      "loss: 0.795717  [  928/ 3200]\n",
      "loss: 0.315515  [  944/ 3200]\n",
      "loss: 0.443512  [  960/ 3200]\n",
      "loss: 0.519152  [  976/ 3200]\n",
      "loss: 0.147084  [  992/ 3200]\n",
      "loss: 0.294967  [ 1008/ 3200]\n",
      "loss: 0.510366  [ 1024/ 3200]\n",
      "loss: 0.482535  [ 1040/ 3200]\n",
      "loss: 0.357495  [ 1056/ 3200]\n",
      "loss: 0.313194  [ 1072/ 3200]\n",
      "loss: 0.540125  [ 1088/ 3200]\n",
      "loss: 0.336626  [ 1104/ 3200]\n",
      "loss: 0.213910  [ 1120/ 3200]\n",
      "loss: 0.658889  [ 1136/ 3200]\n",
      "loss: 0.584139  [ 1152/ 3200]\n",
      "loss: 0.521574  [ 1168/ 3200]\n",
      "loss: 0.483902  [ 1184/ 3200]\n",
      "loss: 0.853015  [ 1200/ 3200]\n",
      "loss: 0.662796  [ 1216/ 3200]\n",
      "loss: 0.395862  [ 1232/ 3200]\n",
      "loss: 0.368869  [ 1248/ 3200]\n",
      "loss: 0.228648  [ 1264/ 3200]\n",
      "loss: 0.449919  [ 1280/ 3200]\n",
      "loss: 0.129332  [ 1296/ 3200]\n",
      "loss: 0.628475  [ 1312/ 3200]\n",
      "loss: 0.288767  [ 1328/ 3200]\n",
      "loss: 0.598982  [ 1344/ 3200]\n",
      "loss: 0.456879  [ 1360/ 3200]\n",
      "loss: 0.975621  [ 1376/ 3200]\n",
      "loss: 0.977767  [ 1392/ 3200]\n",
      "loss: 0.381505  [ 1408/ 3200]\n",
      "loss: 0.295749  [ 1424/ 3200]\n",
      "loss: 0.538856  [ 1440/ 3200]\n",
      "loss: 0.447439  [ 1456/ 3200]\n",
      "loss: 0.537860  [ 1472/ 3200]\n",
      "loss: 0.540695  [ 1488/ 3200]\n",
      "loss: 0.625507  [ 1504/ 3200]\n",
      "loss: 0.292194  [ 1520/ 3200]\n",
      "loss: 0.715329  [ 1536/ 3200]\n",
      "loss: 0.501448  [ 1552/ 3200]\n",
      "loss: 0.509650  [ 1568/ 3200]\n",
      "loss: 0.560599  [ 1584/ 3200]\n",
      "loss: 0.188931  [ 1600/ 3200]\n",
      "loss: 0.430674  [ 1616/ 3200]\n",
      "loss: 0.269021  [ 1632/ 3200]\n",
      "loss: 1.210183  [ 1648/ 3200]\n",
      "loss: 0.602831  [ 1664/ 3200]\n",
      "loss: 0.576143  [ 1680/ 3200]\n",
      "loss: 0.345341  [ 1696/ 3200]\n",
      "loss: 0.365588  [ 1712/ 3200]\n",
      "loss: 0.384479  [ 1728/ 3200]\n",
      "loss: 0.551074  [ 1744/ 3200]\n",
      "loss: 0.299750  [ 1760/ 3200]\n",
      "loss: 0.416040  [ 1776/ 3200]\n",
      "loss: 0.330834  [ 1792/ 3200]\n",
      "loss: 0.336526  [ 1808/ 3200]\n",
      "loss: 0.820294  [ 1824/ 3200]\n",
      "loss: 0.172824  [ 1840/ 3200]\n",
      "loss: 0.426919  [ 1856/ 3200]\n",
      "loss: 0.726767  [ 1872/ 3200]\n",
      "loss: 0.420032  [ 1888/ 3200]\n",
      "loss: 0.449523  [ 1904/ 3200]\n",
      "loss: 0.231814  [ 1920/ 3200]\n",
      "loss: 0.208110  [ 1936/ 3200]\n",
      "loss: 0.266265  [ 1952/ 3200]\n",
      "loss: 0.442198  [ 1968/ 3200]\n",
      "loss: 0.220951  [ 1984/ 3200]\n",
      "loss: 0.234713  [ 2000/ 3200]\n",
      "loss: 0.301117  [ 2016/ 3200]\n",
      "loss: 0.623503  [ 2032/ 3200]\n",
      "loss: 0.302487  [ 2048/ 3200]\n",
      "loss: 0.282601  [ 2064/ 3200]\n",
      "loss: 0.161252  [ 2080/ 3200]\n",
      "loss: 0.653037  [ 2096/ 3200]\n",
      "loss: 0.404168  [ 2112/ 3200]\n",
      "loss: 0.499070  [ 2128/ 3200]\n",
      "loss: 0.646386  [ 2144/ 3200]\n",
      "loss: 0.412843  [ 2160/ 3200]\n",
      "loss: 0.620725  [ 2176/ 3200]\n",
      "loss: 0.628862  [ 2192/ 3200]\n",
      "loss: 0.413971  [ 2208/ 3200]\n",
      "loss: 0.444264  [ 2224/ 3200]\n",
      "loss: 0.337591  [ 2240/ 3200]\n",
      "loss: 0.724134  [ 2256/ 3200]\n",
      "loss: 1.307406  [ 2272/ 3200]\n",
      "loss: 0.652496  [ 2288/ 3200]\n",
      "loss: 0.449997  [ 2304/ 3200]\n",
      "loss: 0.704740  [ 2320/ 3200]\n",
      "loss: 0.502809  [ 2336/ 3200]\n",
      "loss: 0.573333  [ 2352/ 3200]\n",
      "loss: 0.554100  [ 2368/ 3200]\n",
      "loss: 0.318289  [ 2384/ 3200]\n",
      "loss: 0.722339  [ 2400/ 3200]\n",
      "loss: 0.582307  [ 2416/ 3200]\n",
      "loss: 0.402946  [ 2432/ 3200]\n",
      "loss: 0.441908  [ 2448/ 3200]\n",
      "loss: 0.301799  [ 2464/ 3200]\n",
      "loss: 0.458222  [ 2480/ 3200]\n",
      "loss: 0.594831  [ 2496/ 3200]\n",
      "loss: 0.633888  [ 2512/ 3200]\n",
      "loss: 0.421141  [ 2528/ 3200]\n",
      "loss: 0.266656  [ 2544/ 3200]\n",
      "loss: 0.390858  [ 2560/ 3200]\n",
      "loss: 0.906136  [ 2576/ 3200]\n",
      "loss: 0.497526  [ 2592/ 3200]\n",
      "loss: 0.241408  [ 2608/ 3200]\n",
      "loss: 0.365770  [ 2624/ 3200]\n",
      "loss: 0.140950  [ 2640/ 3200]\n",
      "loss: 0.451416  [ 2656/ 3200]\n",
      "loss: 0.350097  [ 2672/ 3200]\n",
      "loss: 0.479567  [ 2688/ 3200]\n",
      "loss: 0.265332  [ 2704/ 3200]\n",
      "loss: 0.202554  [ 2720/ 3200]\n",
      "loss: 0.317996  [ 2736/ 3200]\n",
      "loss: 0.496376  [ 2752/ 3200]\n",
      "loss: 0.337477  [ 2768/ 3200]\n",
      "loss: 0.204005  [ 2784/ 3200]\n",
      "loss: 0.468665  [ 2800/ 3200]\n",
      "loss: 0.295616  [ 2816/ 3200]\n",
      "loss: 0.147537  [ 2832/ 3200]\n",
      "loss: 0.238621  [ 2848/ 3200]\n",
      "loss: 0.902326  [ 2864/ 3200]\n",
      "loss: 0.651899  [ 2880/ 3200]\n",
      "loss: 0.282238  [ 2896/ 3200]\n",
      "loss: 0.389708  [ 2912/ 3200]\n",
      "loss: 0.465015  [ 2928/ 3200]\n",
      "loss: 0.217147  [ 2944/ 3200]\n",
      "loss: 0.236606  [ 2960/ 3200]\n",
      "loss: 0.377602  [ 2976/ 3200]\n",
      "loss: 0.259272  [ 2992/ 3200]\n",
      "loss: 0.655753  [ 3008/ 3200]\n",
      "loss: 0.368078  [ 3024/ 3200]\n",
      "loss: 0.432882  [ 3040/ 3200]\n",
      "loss: 0.685873  [ 3056/ 3200]\n",
      "loss: 0.541975  [ 3072/ 3200]\n",
      "loss: 0.263464  [ 3088/ 3200]\n",
      "loss: 0.085639  [ 3104/ 3200]\n",
      "loss: 0.600752  [ 3120/ 3200]\n",
      "loss: 0.311122  [ 3136/ 3200]\n",
      "loss: 0.452313  [ 3152/ 3200]\n",
      "loss: 0.294717  [ 3168/ 3200]\n",
      "loss: 0.449333  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 0.730692  [    0/ 3200]\n",
      "loss: 0.418290  [   16/ 3200]\n",
      "loss: 0.415663  [   32/ 3200]\n",
      "loss: 0.273909  [   48/ 3200]\n",
      "loss: 0.385639  [   64/ 3200]\n",
      "loss: 0.660233  [   80/ 3200]\n",
      "loss: 0.343834  [   96/ 3200]\n",
      "loss: 0.290912  [  112/ 3200]\n",
      "loss: 0.342789  [  128/ 3200]\n",
      "loss: 0.519524  [  144/ 3200]\n",
      "loss: 0.401969  [  160/ 3200]\n",
      "loss: 0.543492  [  176/ 3200]\n",
      "loss: 0.686928  [  192/ 3200]\n",
      "loss: 0.440355  [  208/ 3200]\n",
      "loss: 0.300967  [  224/ 3200]\n",
      "loss: 0.391135  [  240/ 3200]\n",
      "loss: 0.572804  [  256/ 3200]\n",
      "loss: 0.405310  [  272/ 3200]\n",
      "loss: 0.362865  [  288/ 3200]\n",
      "loss: 0.300015  [  304/ 3200]\n",
      "loss: 0.691227  [  320/ 3200]\n",
      "loss: 0.457319  [  336/ 3200]\n",
      "loss: 0.398999  [  352/ 3200]\n",
      "loss: 0.609491  [  368/ 3200]\n",
      "loss: 0.281662  [  384/ 3200]\n",
      "loss: 0.461720  [  400/ 3200]\n",
      "loss: 0.479036  [  416/ 3200]\n",
      "loss: 0.802609  [  432/ 3200]\n",
      "loss: 0.228982  [  448/ 3200]\n",
      "loss: 0.576552  [  464/ 3200]\n",
      "loss: 0.196686  [  480/ 3200]\n",
      "loss: 0.254992  [  496/ 3200]\n",
      "loss: 0.515286  [  512/ 3200]\n",
      "loss: 0.365727  [  528/ 3200]\n",
      "loss: 0.707812  [  544/ 3200]\n",
      "loss: 0.296174  [  560/ 3200]\n",
      "loss: 0.406022  [  576/ 3200]\n",
      "loss: 0.401028  [  592/ 3200]\n",
      "loss: 0.335310  [  608/ 3200]\n",
      "loss: 0.365531  [  624/ 3200]\n",
      "loss: 0.305278  [  640/ 3200]\n",
      "loss: 0.732411  [  656/ 3200]\n",
      "loss: 0.493632  [  672/ 3200]\n",
      "loss: 0.536010  [  688/ 3200]\n",
      "loss: 0.283907  [  704/ 3200]\n",
      "loss: 0.760826  [  720/ 3200]\n",
      "loss: 0.221942  [  736/ 3200]\n",
      "loss: 0.302358  [  752/ 3200]\n",
      "loss: 0.566054  [  768/ 3200]\n",
      "loss: 0.303111  [  784/ 3200]\n",
      "loss: 0.180629  [  800/ 3200]\n",
      "loss: 0.512565  [  816/ 3200]\n",
      "loss: 0.248846  [  832/ 3200]\n",
      "loss: 0.277159  [  848/ 3200]\n",
      "loss: 0.739590  [  864/ 3200]\n",
      "loss: 0.487173  [  880/ 3200]\n",
      "loss: 0.270477  [  896/ 3200]\n",
      "loss: 0.279367  [  912/ 3200]\n",
      "loss: 0.688743  [  928/ 3200]\n",
      "loss: 0.384696  [  944/ 3200]\n",
      "loss: 0.383562  [  960/ 3200]\n",
      "loss: 0.104604  [  976/ 3200]\n",
      "loss: 0.233280  [  992/ 3200]\n",
      "loss: 0.552496  [ 1008/ 3200]\n",
      "loss: 0.418627  [ 1024/ 3200]\n",
      "loss: 0.513200  [ 1040/ 3200]\n",
      "loss: 0.234934  [ 1056/ 3200]\n",
      "loss: 0.342996  [ 1072/ 3200]\n",
      "loss: 0.346557  [ 1088/ 3200]\n",
      "loss: 0.513984  [ 1104/ 3200]\n",
      "loss: 0.170752  [ 1120/ 3200]\n",
      "loss: 0.238275  [ 1136/ 3200]\n",
      "loss: 0.262568  [ 1152/ 3200]\n",
      "loss: 0.774727  [ 1168/ 3200]\n",
      "loss: 0.525797  [ 1184/ 3200]\n",
      "loss: 0.738121  [ 1200/ 3200]\n",
      "loss: 0.355354  [ 1216/ 3200]\n",
      "loss: 0.268315  [ 1232/ 3200]\n",
      "loss: 0.382326  [ 1248/ 3200]\n",
      "loss: 0.210766  [ 1264/ 3200]\n",
      "loss: 0.677180  [ 1280/ 3200]\n",
      "loss: 0.409166  [ 1296/ 3200]\n",
      "loss: 0.343075  [ 1312/ 3200]\n",
      "loss: 0.345202  [ 1328/ 3200]\n",
      "loss: 0.293614  [ 1344/ 3200]\n",
      "loss: 0.247206  [ 1360/ 3200]\n",
      "loss: 0.253629  [ 1376/ 3200]\n",
      "loss: 0.224049  [ 1392/ 3200]\n",
      "loss: 0.348924  [ 1408/ 3200]\n",
      "loss: 0.352133  [ 1424/ 3200]\n",
      "loss: 0.734014  [ 1440/ 3200]\n",
      "loss: 0.993742  [ 1456/ 3200]\n",
      "loss: 0.155026  [ 1472/ 3200]\n",
      "loss: 0.388245  [ 1488/ 3200]\n",
      "loss: 0.216691  [ 1504/ 3200]\n",
      "loss: 0.630468  [ 1520/ 3200]\n",
      "loss: 0.469183  [ 1536/ 3200]\n",
      "loss: 0.408424  [ 1552/ 3200]\n",
      "loss: 0.249624  [ 1568/ 3200]\n",
      "loss: 0.335143  [ 1584/ 3200]\n",
      "loss: 0.358867  [ 1600/ 3200]\n",
      "loss: 0.139657  [ 1616/ 3200]\n",
      "loss: 0.170215  [ 1632/ 3200]\n",
      "loss: 0.250187  [ 1648/ 3200]\n",
      "loss: 0.791692  [ 1664/ 3200]\n",
      "loss: 0.856992  [ 1680/ 3200]\n",
      "loss: 0.822764  [ 1696/ 3200]\n",
      "loss: 0.438748  [ 1712/ 3200]\n",
      "loss: 0.298408  [ 1728/ 3200]\n",
      "loss: 0.120345  [ 1744/ 3200]\n",
      "loss: 0.260800  [ 1760/ 3200]\n",
      "loss: 0.622354  [ 1776/ 3200]\n",
      "loss: 0.224949  [ 1792/ 3200]\n",
      "loss: 0.342468  [ 1808/ 3200]\n",
      "loss: 0.525311  [ 1824/ 3200]\n",
      "loss: 0.330075  [ 1840/ 3200]\n",
      "loss: 0.341129  [ 1856/ 3200]\n",
      "loss: 0.340783  [ 1872/ 3200]\n",
      "loss: 0.240855  [ 1888/ 3200]\n",
      "loss: 0.339070  [ 1904/ 3200]\n",
      "loss: 0.384591  [ 1920/ 3200]\n",
      "loss: 0.635462  [ 1936/ 3200]\n",
      "loss: 0.216663  [ 1952/ 3200]\n",
      "loss: 0.424211  [ 1968/ 3200]\n",
      "loss: 0.362593  [ 1984/ 3200]\n",
      "loss: 0.191772  [ 2000/ 3200]\n",
      "loss: 0.387619  [ 2016/ 3200]\n",
      "loss: 0.393991  [ 2032/ 3200]\n",
      "loss: 0.484420  [ 2048/ 3200]\n",
      "loss: 0.289537  [ 2064/ 3200]\n",
      "loss: 0.467406  [ 2080/ 3200]\n",
      "loss: 0.689857  [ 2096/ 3200]\n",
      "loss: 0.372905  [ 2112/ 3200]\n",
      "loss: 0.485377  [ 2128/ 3200]\n",
      "loss: 0.465392  [ 2144/ 3200]\n",
      "loss: 0.594780  [ 2160/ 3200]\n",
      "loss: 0.258261  [ 2176/ 3200]\n",
      "loss: 0.621469  [ 2192/ 3200]\n",
      "loss: 0.209397  [ 2208/ 3200]\n",
      "loss: 0.398597  [ 2224/ 3200]\n",
      "loss: 0.370424  [ 2240/ 3200]\n",
      "loss: 0.418812  [ 2256/ 3200]\n",
      "loss: 0.326631  [ 2272/ 3200]\n",
      "loss: 0.670571  [ 2288/ 3200]\n",
      "loss: 0.518925  [ 2304/ 3200]\n",
      "loss: 0.601585  [ 2320/ 3200]\n",
      "loss: 0.391033  [ 2336/ 3200]\n",
      "loss: 0.389838  [ 2352/ 3200]\n",
      "loss: 0.480824  [ 2368/ 3200]\n",
      "loss: 0.508403  [ 2384/ 3200]\n",
      "loss: 0.411191  [ 2400/ 3200]\n",
      "loss: 0.308113  [ 2416/ 3200]\n",
      "loss: 0.633500  [ 2432/ 3200]\n",
      "loss: 0.293617  [ 2448/ 3200]\n",
      "loss: 0.369657  [ 2464/ 3200]\n",
      "loss: 0.243679  [ 2480/ 3200]\n",
      "loss: 0.177325  [ 2496/ 3200]\n",
      "loss: 0.461775  [ 2512/ 3200]\n",
      "loss: 0.207772  [ 2528/ 3200]\n",
      "loss: 0.278778  [ 2544/ 3200]\n",
      "loss: 0.374944  [ 2560/ 3200]\n",
      "loss: 0.565897  [ 2576/ 3200]\n",
      "loss: 0.477087  [ 2592/ 3200]\n",
      "loss: 0.679063  [ 2608/ 3200]\n",
      "loss: 0.311628  [ 2624/ 3200]\n",
      "loss: 0.346700  [ 2640/ 3200]\n",
      "loss: 0.136461  [ 2656/ 3200]\n",
      "loss: 0.240950  [ 2672/ 3200]\n",
      "loss: 0.625037  [ 2688/ 3200]\n",
      "loss: 0.843279  [ 2704/ 3200]\n",
      "loss: 0.550582  [ 2720/ 3200]\n",
      "loss: 0.213174  [ 2736/ 3200]\n",
      "loss: 0.697197  [ 2752/ 3200]\n",
      "loss: 0.273543  [ 2768/ 3200]\n",
      "loss: 0.524656  [ 2784/ 3200]\n",
      "loss: 0.166599  [ 2800/ 3200]\n",
      "loss: 0.370341  [ 2816/ 3200]\n",
      "loss: 0.240677  [ 2832/ 3200]\n",
      "loss: 0.772440  [ 2848/ 3200]\n",
      "loss: 0.322903  [ 2864/ 3200]\n",
      "loss: 0.486742  [ 2880/ 3200]\n",
      "loss: 0.411417  [ 2896/ 3200]\n",
      "loss: 0.796415  [ 2912/ 3200]\n",
      "loss: 0.533057  [ 2928/ 3200]\n",
      "loss: 0.354326  [ 2944/ 3200]\n",
      "loss: 0.504860  [ 2960/ 3200]\n",
      "loss: 0.220557  [ 2976/ 3200]\n",
      "loss: 0.277409  [ 2992/ 3200]\n",
      "loss: 0.268525  [ 3008/ 3200]\n",
      "loss: 0.143743  [ 3024/ 3200]\n",
      "loss: 1.023033  [ 3040/ 3200]\n",
      "loss: 0.432756  [ 3056/ 3200]\n",
      "loss: 0.191012  [ 3072/ 3200]\n",
      "loss: 0.237393  [ 3088/ 3200]\n",
      "loss: 0.583804  [ 3104/ 3200]\n",
      "loss: 0.576465  [ 3120/ 3200]\n",
      "loss: 0.227227  [ 3136/ 3200]\n",
      "loss: 0.461343  [ 3152/ 3200]\n",
      "loss: 0.439558  [ 3168/ 3200]\n",
      "loss: 0.408311  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 0.352474  [    0/ 3200]\n",
      "loss: 0.482466  [   16/ 3200]\n",
      "loss: 0.292362  [   32/ 3200]\n",
      "loss: 0.299553  [   48/ 3200]\n",
      "loss: 0.455160  [   64/ 3200]\n",
      "loss: 0.394731  [   80/ 3200]\n",
      "loss: 0.656233  [   96/ 3200]\n",
      "loss: 0.209592  [  112/ 3200]\n",
      "loss: 0.227665  [  128/ 3200]\n",
      "loss: 0.583265  [  144/ 3200]\n",
      "loss: 0.720729  [  160/ 3200]\n",
      "loss: 0.419220  [  176/ 3200]\n",
      "loss: 0.486325  [  192/ 3200]\n",
      "loss: 0.261213  [  208/ 3200]\n",
      "loss: 0.392611  [  224/ 3200]\n",
      "loss: 0.343539  [  240/ 3200]\n",
      "loss: 0.293057  [  256/ 3200]\n",
      "loss: 0.217942  [  272/ 3200]\n",
      "loss: 0.535831  [  288/ 3200]\n",
      "loss: 0.508815  [  304/ 3200]\n",
      "loss: 0.685155  [  320/ 3200]\n",
      "loss: 0.421498  [  336/ 3200]\n",
      "loss: 0.525210  [  352/ 3200]\n",
      "loss: 0.388281  [  368/ 3200]\n",
      "loss: 0.135146  [  384/ 3200]\n",
      "loss: 0.495417  [  400/ 3200]\n",
      "loss: 0.474268  [  416/ 3200]\n",
      "loss: 0.100528  [  432/ 3200]\n",
      "loss: 0.306569  [  448/ 3200]\n",
      "loss: 0.259598  [  464/ 3200]\n",
      "loss: 0.418667  [  480/ 3200]\n",
      "loss: 0.179063  [  496/ 3200]\n",
      "loss: 0.308162  [  512/ 3200]\n",
      "loss: 0.293533  [  528/ 3200]\n",
      "loss: 0.390858  [  544/ 3200]\n",
      "loss: 0.348075  [  560/ 3200]\n",
      "loss: 0.369261  [  576/ 3200]\n",
      "loss: 0.459293  [  592/ 3200]\n",
      "loss: 0.578596  [  608/ 3200]\n",
      "loss: 0.589280  [  624/ 3200]\n",
      "loss: 0.451558  [  640/ 3200]\n",
      "loss: 0.328132  [  656/ 3200]\n",
      "loss: 0.434749  [  672/ 3200]\n",
      "loss: 0.228976  [  688/ 3200]\n",
      "loss: 0.516271  [  704/ 3200]\n",
      "loss: 0.693045  [  720/ 3200]\n",
      "loss: 0.470353  [  736/ 3200]\n",
      "loss: 0.317465  [  752/ 3200]\n",
      "loss: 0.242111  [  768/ 3200]\n",
      "loss: 0.356654  [  784/ 3200]\n",
      "loss: 0.322670  [  800/ 3200]\n",
      "loss: 0.205844  [  816/ 3200]\n",
      "loss: 0.129291  [  832/ 3200]\n",
      "loss: 0.159356  [  848/ 3200]\n",
      "loss: 0.544670  [  864/ 3200]\n",
      "loss: 0.366054  [  880/ 3200]\n",
      "loss: 0.185607  [  896/ 3200]\n",
      "loss: 0.480234  [  912/ 3200]\n",
      "loss: 0.193998  [  928/ 3200]\n",
      "loss: 0.286423  [  944/ 3200]\n",
      "loss: 0.407200  [  960/ 3200]\n",
      "loss: 0.413665  [  976/ 3200]\n",
      "loss: 0.495216  [  992/ 3200]\n",
      "loss: 0.442699  [ 1008/ 3200]\n",
      "loss: 0.381260  [ 1024/ 3200]\n",
      "loss: 0.275969  [ 1040/ 3200]\n",
      "loss: 0.420597  [ 1056/ 3200]\n",
      "loss: 0.624237  [ 1072/ 3200]\n",
      "loss: 1.030302  [ 1088/ 3200]\n",
      "loss: 0.306345  [ 1104/ 3200]\n",
      "loss: 0.352170  [ 1120/ 3200]\n",
      "loss: 0.516877  [ 1136/ 3200]\n",
      "loss: 0.290014  [ 1152/ 3200]\n",
      "loss: 0.472800  [ 1168/ 3200]\n",
      "loss: 0.305165  [ 1184/ 3200]\n",
      "loss: 0.290005  [ 1200/ 3200]\n",
      "loss: 0.522375  [ 1216/ 3200]\n",
      "loss: 0.365505  [ 1232/ 3200]\n",
      "loss: 0.354025  [ 1248/ 3200]\n",
      "loss: 0.489176  [ 1264/ 3200]\n",
      "loss: 0.260482  [ 1280/ 3200]\n",
      "loss: 0.329449  [ 1296/ 3200]\n",
      "loss: 0.419929  [ 1312/ 3200]\n",
      "loss: 0.232402  [ 1328/ 3200]\n",
      "loss: 0.577552  [ 1344/ 3200]\n",
      "loss: 0.312930  [ 1360/ 3200]\n",
      "loss: 0.244712  [ 1376/ 3200]\n",
      "loss: 0.187046  [ 1392/ 3200]\n",
      "loss: 0.900516  [ 1408/ 3200]\n",
      "loss: 0.359006  [ 1424/ 3200]\n",
      "loss: 0.760821  [ 1440/ 3200]\n",
      "loss: 0.565898  [ 1456/ 3200]\n",
      "loss: 0.231125  [ 1472/ 3200]\n",
      "loss: 0.650490  [ 1488/ 3200]\n",
      "loss: 0.218437  [ 1504/ 3200]\n",
      "loss: 0.126314  [ 1520/ 3200]\n",
      "loss: 0.368365  [ 1536/ 3200]\n",
      "loss: 0.417757  [ 1552/ 3200]\n",
      "loss: 0.290863  [ 1568/ 3200]\n",
      "loss: 0.089162  [ 1584/ 3200]\n",
      "loss: 0.331368  [ 1600/ 3200]\n",
      "loss: 0.432760  [ 1616/ 3200]\n",
      "loss: 0.360127  [ 1632/ 3200]\n",
      "loss: 0.258919  [ 1648/ 3200]\n",
      "loss: 0.420370  [ 1664/ 3200]\n",
      "loss: 0.367342  [ 1680/ 3200]\n",
      "loss: 0.330813  [ 1696/ 3200]\n",
      "loss: 0.238866  [ 1712/ 3200]\n",
      "loss: 0.274211  [ 1728/ 3200]\n",
      "loss: 0.411950  [ 1744/ 3200]\n",
      "loss: 0.249540  [ 1760/ 3200]\n",
      "loss: 0.306303  [ 1776/ 3200]\n",
      "loss: 0.588898  [ 1792/ 3200]\n",
      "loss: 0.141905  [ 1808/ 3200]\n",
      "loss: 0.190042  [ 1824/ 3200]\n",
      "loss: 0.364907  [ 1840/ 3200]\n",
      "loss: 0.423780  [ 1856/ 3200]\n",
      "loss: 0.323354  [ 1872/ 3200]\n",
      "loss: 0.212603  [ 1888/ 3200]\n",
      "loss: 0.310300  [ 1904/ 3200]\n",
      "loss: 0.445320  [ 1920/ 3200]\n",
      "loss: 0.184185  [ 1936/ 3200]\n",
      "loss: 0.639067  [ 1952/ 3200]\n",
      "loss: 0.197349  [ 1968/ 3200]\n",
      "loss: 0.263949  [ 1984/ 3200]\n",
      "loss: 0.223282  [ 2000/ 3200]\n",
      "loss: 0.294567  [ 2016/ 3200]\n",
      "loss: 0.262368  [ 2032/ 3200]\n",
      "loss: 0.287744  [ 2048/ 3200]\n",
      "loss: 0.221915  [ 2064/ 3200]\n",
      "loss: 0.281776  [ 2080/ 3200]\n",
      "loss: 0.207817  [ 2096/ 3200]\n",
      "loss: 0.320160  [ 2112/ 3200]\n",
      "loss: 0.223086  [ 2128/ 3200]\n",
      "loss: 0.164719  [ 2144/ 3200]\n",
      "loss: 1.032180  [ 2160/ 3200]\n",
      "loss: 0.261999  [ 2176/ 3200]\n",
      "loss: 0.144863  [ 2192/ 3200]\n",
      "loss: 0.234314  [ 2208/ 3200]\n",
      "loss: 0.448250  [ 2224/ 3200]\n",
      "loss: 0.244032  [ 2240/ 3200]\n",
      "loss: 0.237839  [ 2256/ 3200]\n",
      "loss: 0.261782  [ 2272/ 3200]\n",
      "loss: 0.520772  [ 2288/ 3200]\n",
      "loss: 0.535821  [ 2304/ 3200]\n",
      "loss: 0.246485  [ 2320/ 3200]\n",
      "loss: 0.350823  [ 2336/ 3200]\n",
      "loss: 0.550038  [ 2352/ 3200]\n",
      "loss: 0.668828  [ 2368/ 3200]\n",
      "loss: 0.243535  [ 2384/ 3200]\n",
      "loss: 0.217419  [ 2400/ 3200]\n",
      "loss: 0.502667  [ 2416/ 3200]\n",
      "loss: 0.337987  [ 2432/ 3200]\n",
      "loss: 0.159305  [ 2448/ 3200]\n",
      "loss: 0.185072  [ 2464/ 3200]\n",
      "loss: 0.262569  [ 2480/ 3200]\n",
      "loss: 0.188506  [ 2496/ 3200]\n",
      "loss: 0.256996  [ 2512/ 3200]\n",
      "loss: 0.201943  [ 2528/ 3200]\n",
      "loss: 0.384982  [ 2544/ 3200]\n",
      "loss: 0.337615  [ 2560/ 3200]\n",
      "loss: 0.175848  [ 2576/ 3200]\n",
      "loss: 0.326636  [ 2592/ 3200]\n",
      "loss: 0.382903  [ 2608/ 3200]\n",
      "loss: 0.494001  [ 2624/ 3200]\n",
      "loss: 0.309304  [ 2640/ 3200]\n",
      "loss: 0.241104  [ 2656/ 3200]\n",
      "loss: 0.228763  [ 2672/ 3200]\n",
      "loss: 0.225805  [ 2688/ 3200]\n",
      "loss: 0.304861  [ 2704/ 3200]\n",
      "loss: 0.701351  [ 2720/ 3200]\n",
      "loss: 0.428564  [ 2736/ 3200]\n",
      "loss: 0.213034  [ 2752/ 3200]\n",
      "loss: 0.731185  [ 2768/ 3200]\n",
      "loss: 0.473284  [ 2784/ 3200]\n",
      "loss: 0.149231  [ 2800/ 3200]\n",
      "loss: 0.321123  [ 2816/ 3200]\n",
      "loss: 0.409500  [ 2832/ 3200]\n",
      "loss: 0.739504  [ 2848/ 3200]\n",
      "loss: 0.313332  [ 2864/ 3200]\n",
      "loss: 0.407674  [ 2880/ 3200]\n",
      "loss: 0.374427  [ 2896/ 3200]\n",
      "loss: 0.400779  [ 2912/ 3200]\n",
      "loss: 0.320001  [ 2928/ 3200]\n",
      "loss: 0.348034  [ 2944/ 3200]\n",
      "loss: 0.233815  [ 2960/ 3200]\n",
      "loss: 0.811570  [ 2976/ 3200]\n",
      "loss: 0.477371  [ 2992/ 3200]\n",
      "loss: 0.459477  [ 3008/ 3200]\n",
      "loss: 0.209596  [ 3024/ 3200]\n",
      "loss: 0.251035  [ 3040/ 3200]\n",
      "loss: 0.445571  [ 3056/ 3200]\n",
      "loss: 0.485714  [ 3072/ 3200]\n",
      "loss: 0.327037  [ 3088/ 3200]\n",
      "loss: 0.240983  [ 3104/ 3200]\n",
      "loss: 0.307121  [ 3120/ 3200]\n",
      "loss: 0.566369  [ 3136/ 3200]\n",
      "loss: 0.454112  [ 3152/ 3200]\n",
      "loss: 0.222257  [ 3168/ 3200]\n",
      "loss: 0.527430  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 0.287191  [    0/ 3200]\n",
      "loss: 0.683948  [   16/ 3200]\n",
      "loss: 0.217894  [   32/ 3200]\n",
      "loss: 0.658478  [   48/ 3200]\n",
      "loss: 0.690379  [   64/ 3200]\n",
      "loss: 0.273541  [   80/ 3200]\n",
      "loss: 0.155088  [   96/ 3200]\n",
      "loss: 0.319267  [  112/ 3200]\n",
      "loss: 0.284800  [  128/ 3200]\n",
      "loss: 0.260800  [  144/ 3200]\n",
      "loss: 0.148191  [  160/ 3200]\n",
      "loss: 0.314890  [  176/ 3200]\n",
      "loss: 0.291564  [  192/ 3200]\n",
      "loss: 0.207966  [  208/ 3200]\n",
      "loss: 0.519707  [  224/ 3200]\n",
      "loss: 0.335158  [  240/ 3200]\n",
      "loss: 0.261011  [  256/ 3200]\n",
      "loss: 0.677687  [  272/ 3200]\n",
      "loss: 0.225963  [  288/ 3200]\n",
      "loss: 0.348700  [  304/ 3200]\n",
      "loss: 0.381369  [  320/ 3200]\n",
      "loss: 0.424367  [  336/ 3200]\n",
      "loss: 0.278411  [  352/ 3200]\n",
      "loss: 0.180309  [  368/ 3200]\n",
      "loss: 0.270671  [  384/ 3200]\n",
      "loss: 0.293626  [  400/ 3200]\n",
      "loss: 0.218324  [  416/ 3200]\n",
      "loss: 0.295042  [  432/ 3200]\n",
      "loss: 0.155243  [  448/ 3200]\n",
      "loss: 0.384243  [  464/ 3200]\n",
      "loss: 0.180045  [  480/ 3200]\n",
      "loss: 0.467331  [  496/ 3200]\n",
      "loss: 0.245929  [  512/ 3200]\n",
      "loss: 0.572783  [  528/ 3200]\n",
      "loss: 0.276430  [  544/ 3200]\n",
      "loss: 0.429238  [  560/ 3200]\n",
      "loss: 0.672792  [  576/ 3200]\n",
      "loss: 0.227365  [  592/ 3200]\n",
      "loss: 0.256323  [  608/ 3200]\n",
      "loss: 0.180688  [  624/ 3200]\n",
      "loss: 0.254679  [  640/ 3200]\n",
      "loss: 0.201571  [  656/ 3200]\n",
      "loss: 0.422254  [  672/ 3200]\n",
      "loss: 0.334365  [  688/ 3200]\n",
      "loss: 0.502549  [  704/ 3200]\n",
      "loss: 0.405273  [  720/ 3200]\n",
      "loss: 0.391604  [  736/ 3200]\n",
      "loss: 0.586898  [  752/ 3200]\n",
      "loss: 0.532773  [  768/ 3200]\n",
      "loss: 0.298303  [  784/ 3200]\n",
      "loss: 0.475791  [  800/ 3200]\n",
      "loss: 0.678943  [  816/ 3200]\n",
      "loss: 0.484709  [  832/ 3200]\n",
      "loss: 0.581463  [  848/ 3200]\n",
      "loss: 0.410080  [  864/ 3200]\n",
      "loss: 0.488966  [  880/ 3200]\n",
      "loss: 0.321901  [  896/ 3200]\n",
      "loss: 0.354493  [  912/ 3200]\n",
      "loss: 0.394313  [  928/ 3200]\n",
      "loss: 0.334725  [  944/ 3200]\n",
      "loss: 0.171392  [  960/ 3200]\n",
      "loss: 0.236032  [  976/ 3200]\n",
      "loss: 0.871763  [  992/ 3200]\n",
      "loss: 0.237737  [ 1008/ 3200]\n",
      "loss: 0.152585  [ 1024/ 3200]\n",
      "loss: 0.698753  [ 1040/ 3200]\n",
      "loss: 0.068522  [ 1056/ 3200]\n",
      "loss: 0.242860  [ 1072/ 3200]\n",
      "loss: 0.609023  [ 1088/ 3200]\n",
      "loss: 0.186277  [ 1104/ 3200]\n",
      "loss: 0.294215  [ 1120/ 3200]\n",
      "loss: 0.300683  [ 1136/ 3200]\n",
      "loss: 0.466896  [ 1152/ 3200]\n",
      "loss: 0.187502  [ 1168/ 3200]\n",
      "loss: 0.194845  [ 1184/ 3200]\n",
      "loss: 0.179395  [ 1200/ 3200]\n",
      "loss: 0.200438  [ 1216/ 3200]\n",
      "loss: 0.313871  [ 1232/ 3200]\n",
      "loss: 0.281547  [ 1248/ 3200]\n",
      "loss: 0.247120  [ 1264/ 3200]\n",
      "loss: 0.486139  [ 1280/ 3200]\n",
      "loss: 0.300211  [ 1296/ 3200]\n",
      "loss: 0.300315  [ 1312/ 3200]\n",
      "loss: 0.235281  [ 1328/ 3200]\n",
      "loss: 0.233784  [ 1344/ 3200]\n",
      "loss: 0.161728  [ 1360/ 3200]\n",
      "loss: 0.163723  [ 1376/ 3200]\n",
      "loss: 0.343878  [ 1392/ 3200]\n",
      "loss: 0.244848  [ 1408/ 3200]\n",
      "loss: 0.346365  [ 1424/ 3200]\n",
      "loss: 0.118630  [ 1440/ 3200]\n",
      "loss: 0.152721  [ 1456/ 3200]\n",
      "loss: 0.398806  [ 1472/ 3200]\n",
      "loss: 0.463115  [ 1488/ 3200]\n",
      "loss: 0.075111  [ 1504/ 3200]\n",
      "loss: 0.187534  [ 1520/ 3200]\n",
      "loss: 0.641167  [ 1536/ 3200]\n",
      "loss: 0.392988  [ 1552/ 3200]\n",
      "loss: 0.237658  [ 1568/ 3200]\n",
      "loss: 0.294059  [ 1584/ 3200]\n",
      "loss: 0.244064  [ 1600/ 3200]\n",
      "loss: 0.258816  [ 1616/ 3200]\n",
      "loss: 0.104372  [ 1632/ 3200]\n",
      "loss: 0.210797  [ 1648/ 3200]\n",
      "loss: 0.282870  [ 1664/ 3200]\n",
      "loss: 0.424540  [ 1680/ 3200]\n",
      "loss: 0.243829  [ 1696/ 3200]\n",
      "loss: 0.183747  [ 1712/ 3200]\n",
      "loss: 0.435862  [ 1728/ 3200]\n",
      "loss: 0.513737  [ 1744/ 3200]\n",
      "loss: 0.441873  [ 1760/ 3200]\n",
      "loss: 0.516762  [ 1776/ 3200]\n",
      "loss: 0.075084  [ 1792/ 3200]\n",
      "loss: 0.342063  [ 1808/ 3200]\n",
      "loss: 0.448302  [ 1824/ 3200]\n",
      "loss: 0.142083  [ 1840/ 3200]\n",
      "loss: 0.063183  [ 1856/ 3200]\n",
      "loss: 0.267839  [ 1872/ 3200]\n",
      "loss: 0.191335  [ 1888/ 3200]\n",
      "loss: 0.325181  [ 1904/ 3200]\n",
      "loss: 0.312878  [ 1920/ 3200]\n",
      "loss: 0.344586  [ 1936/ 3200]\n",
      "loss: 0.441058  [ 1952/ 3200]\n",
      "loss: 0.323008  [ 1968/ 3200]\n",
      "loss: 0.690827  [ 1984/ 3200]\n",
      "loss: 0.207732  [ 2000/ 3200]\n",
      "loss: 0.307468  [ 2016/ 3200]\n",
      "loss: 0.302497  [ 2032/ 3200]\n",
      "loss: 0.209366  [ 2048/ 3200]\n",
      "loss: 0.117279  [ 2064/ 3200]\n",
      "loss: 0.263322  [ 2080/ 3200]\n",
      "loss: 0.107777  [ 2096/ 3200]\n",
      "loss: 0.318539  [ 2112/ 3200]\n",
      "loss: 0.414685  [ 2128/ 3200]\n",
      "loss: 0.421492  [ 2144/ 3200]\n",
      "loss: 0.421325  [ 2160/ 3200]\n",
      "loss: 0.289335  [ 2176/ 3200]\n",
      "loss: 0.136132  [ 2192/ 3200]\n",
      "loss: 0.207939  [ 2208/ 3200]\n",
      "loss: 0.608390  [ 2224/ 3200]\n",
      "loss: 0.179791  [ 2240/ 3200]\n",
      "loss: 0.595113  [ 2256/ 3200]\n",
      "loss: 0.075013  [ 2272/ 3200]\n",
      "loss: 0.361463  [ 2288/ 3200]\n",
      "loss: 0.220481  [ 2304/ 3200]\n",
      "loss: 0.453914  [ 2320/ 3200]\n",
      "loss: 0.218887  [ 2336/ 3200]\n",
      "loss: 0.204244  [ 2352/ 3200]\n",
      "loss: 0.402498  [ 2368/ 3200]\n",
      "loss: 0.100886  [ 2384/ 3200]\n",
      "loss: 0.172182  [ 2400/ 3200]\n",
      "loss: 0.148796  [ 2416/ 3200]\n",
      "loss: 0.324155  [ 2432/ 3200]\n",
      "loss: 0.478427  [ 2448/ 3200]\n",
      "loss: 0.486732  [ 2464/ 3200]\n",
      "loss: 0.257645  [ 2480/ 3200]\n",
      "loss: 0.223602  [ 2496/ 3200]\n",
      "loss: 0.536086  [ 2512/ 3200]\n",
      "loss: 0.239744  [ 2528/ 3200]\n",
      "loss: 0.438968  [ 2544/ 3200]\n",
      "loss: 0.412167  [ 2560/ 3200]\n",
      "loss: 0.241562  [ 2576/ 3200]\n",
      "loss: 0.179341  [ 2592/ 3200]\n",
      "loss: 0.188908  [ 2608/ 3200]\n",
      "loss: 0.242166  [ 2624/ 3200]\n",
      "loss: 0.199064  [ 2640/ 3200]\n",
      "loss: 0.304343  [ 2656/ 3200]\n",
      "loss: 0.240503  [ 2672/ 3200]\n",
      "loss: 0.358993  [ 2688/ 3200]\n",
      "loss: 0.381657  [ 2704/ 3200]\n",
      "loss: 0.115859  [ 2720/ 3200]\n",
      "loss: 0.581762  [ 2736/ 3200]\n",
      "loss: 0.165330  [ 2752/ 3200]\n",
      "loss: 0.305128  [ 2768/ 3200]\n",
      "loss: 0.385540  [ 2784/ 3200]\n",
      "loss: 0.165747  [ 2800/ 3200]\n",
      "loss: 0.442038  [ 2816/ 3200]\n",
      "loss: 0.210208  [ 2832/ 3200]\n",
      "loss: 0.214103  [ 2848/ 3200]\n",
      "loss: 0.177034  [ 2864/ 3200]\n",
      "loss: 0.302220  [ 2880/ 3200]\n",
      "loss: 0.214283  [ 2896/ 3200]\n",
      "loss: 0.469159  [ 2912/ 3200]\n",
      "loss: 0.406761  [ 2928/ 3200]\n",
      "loss: 0.443470  [ 2944/ 3200]\n",
      "loss: 0.461343  [ 2960/ 3200]\n",
      "loss: 0.183529  [ 2976/ 3200]\n",
      "loss: 0.235056  [ 2992/ 3200]\n",
      "loss: 0.156285  [ 3008/ 3200]\n",
      "loss: 0.150504  [ 3024/ 3200]\n",
      "loss: 0.276987  [ 3040/ 3200]\n",
      "loss: 0.112157  [ 3056/ 3200]\n",
      "loss: 0.165636  [ 3072/ 3200]\n",
      "loss: 0.450679  [ 3088/ 3200]\n",
      "loss: 0.487291  [ 3104/ 3200]\n",
      "loss: 0.340651  [ 3120/ 3200]\n",
      "loss: 0.202785  [ 3136/ 3200]\n",
      "loss: 0.187127  [ 3152/ 3200]\n",
      "loss: 0.281919  [ 3168/ 3200]\n",
      "loss: 0.274674  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 0.249903  [    0/ 3200]\n",
      "loss: 0.307000  [   16/ 3200]\n",
      "loss: 0.144598  [   32/ 3200]\n",
      "loss: 0.527576  [   48/ 3200]\n",
      "loss: 0.610687  [   64/ 3200]\n",
      "loss: 0.270759  [   80/ 3200]\n",
      "loss: 0.372941  [   96/ 3200]\n",
      "loss: 0.308741  [  112/ 3200]\n",
      "loss: 0.227586  [  128/ 3200]\n",
      "loss: 0.133563  [  144/ 3200]\n",
      "loss: 0.353736  [  160/ 3200]\n",
      "loss: 0.260229  [  176/ 3200]\n",
      "loss: 0.165997  [  192/ 3200]\n",
      "loss: 0.087985  [  208/ 3200]\n",
      "loss: 0.163789  [  224/ 3200]\n",
      "loss: 0.436389  [  240/ 3200]\n",
      "loss: 0.126307  [  256/ 3200]\n",
      "loss: 0.204954  [  272/ 3200]\n",
      "loss: 0.137655  [  288/ 3200]\n",
      "loss: 0.311765  [  304/ 3200]\n",
      "loss: 0.679534  [  320/ 3200]\n",
      "loss: 0.498149  [  336/ 3200]\n",
      "loss: 0.438715  [  352/ 3200]\n",
      "loss: 0.500468  [  368/ 3200]\n",
      "loss: 0.171584  [  384/ 3200]\n",
      "loss: 0.166724  [  400/ 3200]\n",
      "loss: 0.652832  [  416/ 3200]\n",
      "loss: 0.207642  [  432/ 3200]\n",
      "loss: 0.353297  [  448/ 3200]\n",
      "loss: 0.312872  [  464/ 3200]\n",
      "loss: 0.100082  [  480/ 3200]\n",
      "loss: 0.297118  [  496/ 3200]\n",
      "loss: 0.273326  [  512/ 3200]\n",
      "loss: 0.312748  [  528/ 3200]\n",
      "loss: 0.388921  [  544/ 3200]\n",
      "loss: 0.391598  [  560/ 3200]\n",
      "loss: 0.102304  [  576/ 3200]\n",
      "loss: 0.114555  [  592/ 3200]\n",
      "loss: 0.284025  [  608/ 3200]\n",
      "loss: 0.604800  [  624/ 3200]\n",
      "loss: 0.926432  [  640/ 3200]\n",
      "loss: 0.274239  [  656/ 3200]\n",
      "loss: 0.227268  [  672/ 3200]\n",
      "loss: 0.766162  [  688/ 3200]\n",
      "loss: 0.279016  [  704/ 3200]\n",
      "loss: 0.216610  [  720/ 3200]\n",
      "loss: 0.274591  [  736/ 3200]\n",
      "loss: 0.098057  [  752/ 3200]\n",
      "loss: 0.099441  [  768/ 3200]\n",
      "loss: 0.099577  [  784/ 3200]\n",
      "loss: 0.153883  [  800/ 3200]\n",
      "loss: 0.233447  [  816/ 3200]\n",
      "loss: 0.269695  [  832/ 3200]\n",
      "loss: 0.310641  [  848/ 3200]\n",
      "loss: 0.346967  [  864/ 3200]\n",
      "loss: 0.382943  [  880/ 3200]\n",
      "loss: 0.208134  [  896/ 3200]\n",
      "loss: 0.534665  [  912/ 3200]\n",
      "loss: 0.368593  [  928/ 3200]\n",
      "loss: 0.374133  [  944/ 3200]\n",
      "loss: 0.128719  [  960/ 3200]\n",
      "loss: 0.138876  [  976/ 3200]\n",
      "loss: 0.176794  [  992/ 3200]\n",
      "loss: 0.201210  [ 1008/ 3200]\n",
      "loss: 0.410254  [ 1024/ 3200]\n",
      "loss: 0.258115  [ 1040/ 3200]\n",
      "loss: 0.257547  [ 1056/ 3200]\n",
      "loss: 0.196443  [ 1072/ 3200]\n",
      "loss: 0.148122  [ 1088/ 3200]\n",
      "loss: 0.342739  [ 1104/ 3200]\n",
      "loss: 0.352241  [ 1120/ 3200]\n",
      "loss: 0.064324  [ 1136/ 3200]\n",
      "loss: 0.223495  [ 1152/ 3200]\n",
      "loss: 0.121053  [ 1168/ 3200]\n",
      "loss: 0.272241  [ 1184/ 3200]\n",
      "loss: 0.257926  [ 1200/ 3200]\n",
      "loss: 0.160785  [ 1216/ 3200]\n",
      "loss: 0.274495  [ 1232/ 3200]\n",
      "loss: 0.134343  [ 1248/ 3200]\n",
      "loss: 0.318398  [ 1264/ 3200]\n",
      "loss: 0.236553  [ 1280/ 3200]\n",
      "loss: 0.106991  [ 1296/ 3200]\n",
      "loss: 0.134280  [ 1312/ 3200]\n",
      "loss: 0.316604  [ 1328/ 3200]\n",
      "loss: 0.041641  [ 1344/ 3200]\n",
      "loss: 0.473174  [ 1360/ 3200]\n",
      "loss: 0.279663  [ 1376/ 3200]\n",
      "loss: 0.510393  [ 1392/ 3200]\n",
      "loss: 0.711179  [ 1408/ 3200]\n",
      "loss: 0.232502  [ 1424/ 3200]\n",
      "loss: 0.186273  [ 1440/ 3200]\n",
      "loss: 0.571637  [ 1456/ 3200]\n",
      "loss: 0.074738  [ 1472/ 3200]\n",
      "loss: 0.283859  [ 1488/ 3200]\n",
      "loss: 0.176083  [ 1504/ 3200]\n",
      "loss: 0.271113  [ 1520/ 3200]\n",
      "loss: 0.295914  [ 1536/ 3200]\n",
      "loss: 0.172082  [ 1552/ 3200]\n",
      "loss: 0.097469  [ 1568/ 3200]\n",
      "loss: 0.176800  [ 1584/ 3200]\n",
      "loss: 0.412417  [ 1600/ 3200]\n",
      "loss: 0.595820  [ 1616/ 3200]\n",
      "loss: 0.374982  [ 1632/ 3200]\n",
      "loss: 0.410378  [ 1648/ 3200]\n",
      "loss: 0.431132  [ 1664/ 3200]\n",
      "loss: 0.340342  [ 1680/ 3200]\n",
      "loss: 0.166628  [ 1696/ 3200]\n",
      "loss: 0.166795  [ 1712/ 3200]\n",
      "loss: 0.301374  [ 1728/ 3200]\n",
      "loss: 0.247931  [ 1744/ 3200]\n",
      "loss: 0.069985  [ 1760/ 3200]\n",
      "loss: 0.359606  [ 1776/ 3200]\n",
      "loss: 0.151699  [ 1792/ 3200]\n",
      "loss: 0.108716  [ 1808/ 3200]\n",
      "loss: 0.144944  [ 1824/ 3200]\n",
      "loss: 0.298948  [ 1840/ 3200]\n",
      "loss: 0.031838  [ 1856/ 3200]\n",
      "loss: 0.123839  [ 1872/ 3200]\n",
      "loss: 0.437702  [ 1888/ 3200]\n",
      "loss: 0.557889  [ 1904/ 3200]\n",
      "loss: 0.126815  [ 1920/ 3200]\n",
      "loss: 0.399974  [ 1936/ 3200]\n",
      "loss: 0.195881  [ 1952/ 3200]\n",
      "loss: 0.100589  [ 1968/ 3200]\n",
      "loss: 0.093019  [ 1984/ 3200]\n",
      "loss: 0.269878  [ 2000/ 3200]\n",
      "loss: 0.045936  [ 2016/ 3200]\n",
      "loss: 0.401013  [ 2032/ 3200]\n",
      "loss: 0.430579  [ 2048/ 3200]\n",
      "loss: 0.451212  [ 2064/ 3200]\n",
      "loss: 0.431392  [ 2080/ 3200]\n",
      "loss: 0.462083  [ 2096/ 3200]\n",
      "loss: 0.452138  [ 2112/ 3200]\n",
      "loss: 0.112227  [ 2128/ 3200]\n",
      "loss: 0.151090  [ 2144/ 3200]\n",
      "loss: 0.521681  [ 2160/ 3200]\n",
      "loss: 0.604232  [ 2176/ 3200]\n",
      "loss: 0.402108  [ 2192/ 3200]\n",
      "loss: 0.227075  [ 2208/ 3200]\n",
      "loss: 0.139731  [ 2224/ 3200]\n",
      "loss: 0.110685  [ 2240/ 3200]\n",
      "loss: 0.281039  [ 2256/ 3200]\n",
      "loss: 0.406816  [ 2272/ 3200]\n",
      "loss: 0.114560  [ 2288/ 3200]\n",
      "loss: 0.178617  [ 2304/ 3200]\n",
      "loss: 0.171591  [ 2320/ 3200]\n",
      "loss: 0.266180  [ 2336/ 3200]\n",
      "loss: 0.305192  [ 2352/ 3200]\n",
      "loss: 0.130021  [ 2368/ 3200]\n",
      "loss: 0.307273  [ 2384/ 3200]\n",
      "loss: 0.228376  [ 2400/ 3200]\n",
      "loss: 0.297778  [ 2416/ 3200]\n",
      "loss: 0.267373  [ 2432/ 3200]\n",
      "loss: 0.187539  [ 2448/ 3200]\n",
      "loss: 0.255951  [ 2464/ 3200]\n",
      "loss: 0.106379  [ 2480/ 3200]\n",
      "loss: 0.255488  [ 2496/ 3200]\n",
      "loss: 0.382737  [ 2512/ 3200]\n",
      "loss: 0.239805  [ 2528/ 3200]\n",
      "loss: 0.095389  [ 2544/ 3200]\n",
      "loss: 0.183076  [ 2560/ 3200]\n",
      "loss: 0.282286  [ 2576/ 3200]\n",
      "loss: 0.528817  [ 2592/ 3200]\n",
      "loss: 0.183282  [ 2608/ 3200]\n",
      "loss: 0.240512  [ 2624/ 3200]\n",
      "loss: 0.269701  [ 2640/ 3200]\n",
      "loss: 0.436902  [ 2656/ 3200]\n",
      "loss: 0.571600  [ 2672/ 3200]\n",
      "loss: 0.192037  [ 2688/ 3200]\n",
      "loss: 0.209472  [ 2704/ 3200]\n",
      "loss: 0.069460  [ 2720/ 3200]\n",
      "loss: 0.202721  [ 2736/ 3200]\n",
      "loss: 0.316855  [ 2752/ 3200]\n",
      "loss: 0.246616  [ 2768/ 3200]\n",
      "loss: 0.082596  [ 2784/ 3200]\n",
      "loss: 0.210453  [ 2800/ 3200]\n",
      "loss: 0.125245  [ 2816/ 3200]\n",
      "loss: 0.326030  [ 2832/ 3200]\n",
      "loss: 0.310298  [ 2848/ 3200]\n",
      "loss: 0.338007  [ 2864/ 3200]\n",
      "loss: 0.342090  [ 2880/ 3200]\n",
      "loss: 0.469588  [ 2896/ 3200]\n",
      "loss: 0.223940  [ 2912/ 3200]\n",
      "loss: 0.214868  [ 2928/ 3200]\n",
      "loss: 0.094407  [ 2944/ 3200]\n",
      "loss: 0.463811  [ 2960/ 3200]\n",
      "loss: 0.299723  [ 2976/ 3200]\n",
      "loss: 0.216361  [ 2992/ 3200]\n",
      "loss: 0.180542  [ 3008/ 3200]\n",
      "loss: 0.253895  [ 3024/ 3200]\n",
      "loss: 0.078628  [ 3040/ 3200]\n",
      "loss: 0.142063  [ 3056/ 3200]\n",
      "loss: 0.394575  [ 3072/ 3200]\n",
      "loss: 0.487168  [ 3088/ 3200]\n",
      "loss: 0.380999  [ 3104/ 3200]\n",
      "loss: 0.232169  [ 3120/ 3200]\n",
      "loss: 0.200370  [ 3136/ 3200]\n",
      "loss: 0.514139  [ 3152/ 3200]\n",
      "loss: 0.803292  [ 3168/ 3200]\n",
      "loss: 0.734906  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 0.292623  [    0/ 3200]\n",
      "loss: 0.406979  [   16/ 3200]\n",
      "loss: 0.358834  [   32/ 3200]\n",
      "loss: 0.335738  [   48/ 3200]\n",
      "loss: 0.169467  [   64/ 3200]\n",
      "loss: 0.449638  [   80/ 3200]\n",
      "loss: 0.266904  [   96/ 3200]\n",
      "loss: 0.169398  [  112/ 3200]\n",
      "loss: 0.228658  [  128/ 3200]\n",
      "loss: 0.286757  [  144/ 3200]\n",
      "loss: 0.256627  [  160/ 3200]\n",
      "loss: 0.122832  [  176/ 3200]\n",
      "loss: 0.179433  [  192/ 3200]\n",
      "loss: 0.439581  [  208/ 3200]\n",
      "loss: 0.048975  [  224/ 3200]\n",
      "loss: 0.255731  [  240/ 3200]\n",
      "loss: 0.574495  [  256/ 3200]\n",
      "loss: 0.197250  [  272/ 3200]\n",
      "loss: 0.287696  [  288/ 3200]\n",
      "loss: 0.290355  [  304/ 3200]\n",
      "loss: 0.347311  [  320/ 3200]\n",
      "loss: 0.238851  [  336/ 3200]\n",
      "loss: 0.159946  [  352/ 3200]\n",
      "loss: 0.285486  [  368/ 3200]\n",
      "loss: 0.163545  [  384/ 3200]\n",
      "loss: 0.249375  [  400/ 3200]\n",
      "loss: 0.074622  [  416/ 3200]\n",
      "loss: 0.056570  [  432/ 3200]\n",
      "loss: 0.119192  [  448/ 3200]\n",
      "loss: 0.174211  [  464/ 3200]\n",
      "loss: 0.555494  [  480/ 3200]\n",
      "loss: 0.068769  [  496/ 3200]\n",
      "loss: 0.620641  [  512/ 3200]\n",
      "loss: 0.253405  [  528/ 3200]\n",
      "loss: 0.091956  [  544/ 3200]\n",
      "loss: 0.113145  [  560/ 3200]\n",
      "loss: 0.198666  [  576/ 3200]\n",
      "loss: 0.267315  [  592/ 3200]\n",
      "loss: 0.163451  [  608/ 3200]\n",
      "loss: 0.255617  [  624/ 3200]\n",
      "loss: 0.080499  [  640/ 3200]\n",
      "loss: 0.512140  [  656/ 3200]\n",
      "loss: 0.204833  [  672/ 3200]\n",
      "loss: 0.332368  [  688/ 3200]\n",
      "loss: 0.152683  [  704/ 3200]\n",
      "loss: 0.219662  [  720/ 3200]\n",
      "loss: 0.171292  [  736/ 3200]\n",
      "loss: 0.488440  [  752/ 3200]\n",
      "loss: 0.344472  [  768/ 3200]\n",
      "loss: 0.260130  [  784/ 3200]\n",
      "loss: 0.159541  [  800/ 3200]\n",
      "loss: 0.148200  [  816/ 3200]\n",
      "loss: 0.191896  [  832/ 3200]\n",
      "loss: 0.126851  [  848/ 3200]\n",
      "loss: 0.284942  [  864/ 3200]\n",
      "loss: 0.312164  [  880/ 3200]\n",
      "loss: 0.081266  [  896/ 3200]\n",
      "loss: 0.169816  [  912/ 3200]\n",
      "loss: 0.116250  [  928/ 3200]\n",
      "loss: 0.101115  [  944/ 3200]\n",
      "loss: 0.216660  [  960/ 3200]\n",
      "loss: 0.353590  [  976/ 3200]\n",
      "loss: 0.151850  [  992/ 3200]\n",
      "loss: 0.273424  [ 1008/ 3200]\n",
      "loss: 0.222865  [ 1024/ 3200]\n",
      "loss: 0.166232  [ 1040/ 3200]\n",
      "loss: 0.459844  [ 1056/ 3200]\n",
      "loss: 0.182630  [ 1072/ 3200]\n",
      "loss: 0.107528  [ 1088/ 3200]\n",
      "loss: 0.472178  [ 1104/ 3200]\n",
      "loss: 0.160370  [ 1120/ 3200]\n",
      "loss: 0.218270  [ 1136/ 3200]\n",
      "loss: 0.154762  [ 1152/ 3200]\n",
      "loss: 0.103794  [ 1168/ 3200]\n",
      "loss: 0.436209  [ 1184/ 3200]\n",
      "loss: 0.075257  [ 1200/ 3200]\n",
      "loss: 0.658881  [ 1216/ 3200]\n",
      "loss: 0.316116  [ 1232/ 3200]\n",
      "loss: 0.290141  [ 1248/ 3200]\n",
      "loss: 0.085859  [ 1264/ 3200]\n",
      "loss: 0.115936  [ 1280/ 3200]\n",
      "loss: 0.100439  [ 1296/ 3200]\n",
      "loss: 0.250926  [ 1312/ 3200]\n",
      "loss: 0.118144  [ 1328/ 3200]\n",
      "loss: 0.318618  [ 1344/ 3200]\n",
      "loss: 0.100371  [ 1360/ 3200]\n",
      "loss: 0.175848  [ 1376/ 3200]\n",
      "loss: 0.338578  [ 1392/ 3200]\n",
      "loss: 0.330610  [ 1408/ 3200]\n",
      "loss: 0.318036  [ 1424/ 3200]\n",
      "loss: 0.248452  [ 1440/ 3200]\n",
      "loss: 0.264111  [ 1456/ 3200]\n",
      "loss: 0.246648  [ 1472/ 3200]\n",
      "loss: 0.226554  [ 1488/ 3200]\n",
      "loss: 0.282739  [ 1504/ 3200]\n",
      "loss: 0.079471  [ 1520/ 3200]\n",
      "loss: 0.233728  [ 1536/ 3200]\n",
      "loss: 0.403320  [ 1552/ 3200]\n",
      "loss: 1.162694  [ 1568/ 3200]\n",
      "loss: 0.473164  [ 1584/ 3200]\n",
      "loss: 0.398760  [ 1600/ 3200]\n",
      "loss: 0.258985  [ 1616/ 3200]\n",
      "loss: 0.107464  [ 1632/ 3200]\n",
      "loss: 0.240649  [ 1648/ 3200]\n",
      "loss: 0.136468  [ 1664/ 3200]\n",
      "loss: 0.506827  [ 1680/ 3200]\n",
      "loss: 0.278076  [ 1696/ 3200]\n",
      "loss: 0.269658  [ 1712/ 3200]\n",
      "loss: 0.464467  [ 1728/ 3200]\n",
      "loss: 0.143694  [ 1744/ 3200]\n",
      "loss: 0.096692  [ 1760/ 3200]\n",
      "loss: 0.117766  [ 1776/ 3200]\n",
      "loss: 0.249052  [ 1792/ 3200]\n",
      "loss: 0.118492  [ 1808/ 3200]\n",
      "loss: 0.203934  [ 1824/ 3200]\n",
      "loss: 0.120956  [ 1840/ 3200]\n",
      "loss: 0.158528  [ 1856/ 3200]\n",
      "loss: 0.096966  [ 1872/ 3200]\n",
      "loss: 0.176963  [ 1888/ 3200]\n",
      "loss: 0.361911  [ 1904/ 3200]\n",
      "loss: 0.095673  [ 1920/ 3200]\n",
      "loss: 0.107059  [ 1936/ 3200]\n",
      "loss: 0.195204  [ 1952/ 3200]\n",
      "loss: 0.168515  [ 1968/ 3200]\n",
      "loss: 0.080274  [ 1984/ 3200]\n",
      "loss: 0.199721  [ 2000/ 3200]\n",
      "loss: 0.293729  [ 2016/ 3200]\n",
      "loss: 0.283483  [ 2032/ 3200]\n",
      "loss: 0.235061  [ 2048/ 3200]\n",
      "loss: 0.163296  [ 2064/ 3200]\n",
      "loss: 0.148402  [ 2080/ 3200]\n",
      "loss: 0.146203  [ 2096/ 3200]\n",
      "loss: 0.457939  [ 2112/ 3200]\n",
      "loss: 0.590315  [ 2128/ 3200]\n",
      "loss: 0.498633  [ 2144/ 3200]\n",
      "loss: 0.280378  [ 2160/ 3200]\n",
      "loss: 0.106291  [ 2176/ 3200]\n",
      "loss: 0.173270  [ 2192/ 3200]\n",
      "loss: 0.105194  [ 2208/ 3200]\n",
      "loss: 0.374734  [ 2224/ 3200]\n",
      "loss: 0.170761  [ 2240/ 3200]\n",
      "loss: 0.591683  [ 2256/ 3200]\n",
      "loss: 0.174592  [ 2272/ 3200]\n",
      "loss: 0.122112  [ 2288/ 3200]\n",
      "loss: 0.475947  [ 2304/ 3200]\n",
      "loss: 0.216674  [ 2320/ 3200]\n",
      "loss: 0.173434  [ 2336/ 3200]\n",
      "loss: 0.055297  [ 2352/ 3200]\n",
      "loss: 0.346908  [ 2368/ 3200]\n",
      "loss: 0.223798  [ 2384/ 3200]\n",
      "loss: 0.221738  [ 2400/ 3200]\n",
      "loss: 0.265975  [ 2416/ 3200]\n",
      "loss: 0.354075  [ 2432/ 3200]\n",
      "loss: 0.516222  [ 2448/ 3200]\n",
      "loss: 0.087547  [ 2464/ 3200]\n",
      "loss: 0.102482  [ 2480/ 3200]\n",
      "loss: 0.286134  [ 2496/ 3200]\n",
      "loss: 0.317240  [ 2512/ 3200]\n",
      "loss: 0.120659  [ 2528/ 3200]\n",
      "loss: 0.271481  [ 2544/ 3200]\n",
      "loss: 0.092084  [ 2560/ 3200]\n",
      "loss: 0.249286  [ 2576/ 3200]\n",
      "loss: 0.165609  [ 2592/ 3200]\n",
      "loss: 0.351385  [ 2608/ 3200]\n",
      "loss: 0.198879  [ 2624/ 3200]\n",
      "loss: 0.193656  [ 2640/ 3200]\n",
      "loss: 0.085470  [ 2656/ 3200]\n",
      "loss: 0.131287  [ 2672/ 3200]\n",
      "loss: 0.083847  [ 2688/ 3200]\n",
      "loss: 0.687289  [ 2704/ 3200]\n",
      "loss: 0.388753  [ 2720/ 3200]\n",
      "loss: 0.161886  [ 2736/ 3200]\n",
      "loss: 0.508521  [ 2752/ 3200]\n",
      "loss: 0.039274  [ 2768/ 3200]\n",
      "loss: 0.235426  [ 2784/ 3200]\n",
      "loss: 0.278260  [ 2800/ 3200]\n",
      "loss: 0.403667  [ 2816/ 3200]\n",
      "loss: 0.264380  [ 2832/ 3200]\n",
      "loss: 0.282320  [ 2848/ 3200]\n",
      "loss: 0.276968  [ 2864/ 3200]\n",
      "loss: 0.234357  [ 2880/ 3200]\n",
      "loss: 0.183813  [ 2896/ 3200]\n",
      "loss: 0.107435  [ 2912/ 3200]\n",
      "loss: 0.312553  [ 2928/ 3200]\n",
      "loss: 0.274580  [ 2944/ 3200]\n",
      "loss: 0.495862  [ 2960/ 3200]\n",
      "loss: 0.114776  [ 2976/ 3200]\n",
      "loss: 0.394422  [ 2992/ 3200]\n",
      "loss: 0.558025  [ 3008/ 3200]\n",
      "loss: 0.177135  [ 3024/ 3200]\n",
      "loss: 0.187550  [ 3040/ 3200]\n",
      "loss: 0.119677  [ 3056/ 3200]\n",
      "loss: 0.156818  [ 3072/ 3200]\n",
      "loss: 0.258865  [ 3088/ 3200]\n",
      "loss: 0.182681  [ 3104/ 3200]\n",
      "loss: 0.330140  [ 3120/ 3200]\n",
      "loss: 0.304983  [ 3136/ 3200]\n",
      "loss: 0.057285  [ 3152/ 3200]\n",
      "loss: 0.308571  [ 3168/ 3200]\n",
      "loss: 0.269586  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 0.304080  [    0/ 3200]\n",
      "loss: 0.245756  [   16/ 3200]\n",
      "loss: 0.119023  [   32/ 3200]\n",
      "loss: 0.245677  [   48/ 3200]\n",
      "loss: 0.238649  [   64/ 3200]\n",
      "loss: 0.206982  [   80/ 3200]\n",
      "loss: 0.615420  [   96/ 3200]\n",
      "loss: 0.384317  [  112/ 3200]\n",
      "loss: 0.193371  [  128/ 3200]\n",
      "loss: 0.179480  [  144/ 3200]\n",
      "loss: 0.238415  [  160/ 3200]\n",
      "loss: 0.357030  [  176/ 3200]\n",
      "loss: 0.178324  [  192/ 3200]\n",
      "loss: 0.202162  [  208/ 3200]\n",
      "loss: 0.156883  [  224/ 3200]\n",
      "loss: 0.080277  [  240/ 3200]\n",
      "loss: 0.474885  [  256/ 3200]\n",
      "loss: 0.050518  [  272/ 3200]\n",
      "loss: 0.089359  [  288/ 3200]\n",
      "loss: 0.189572  [  304/ 3200]\n",
      "loss: 0.123407  [  320/ 3200]\n",
      "loss: 0.224024  [  336/ 3200]\n",
      "loss: 0.328526  [  352/ 3200]\n",
      "loss: 0.154773  [  368/ 3200]\n",
      "loss: 0.192841  [  384/ 3200]\n",
      "loss: 0.352032  [  400/ 3200]\n",
      "loss: 0.160784  [  416/ 3200]\n",
      "loss: 0.251031  [  432/ 3200]\n",
      "loss: 0.146518  [  448/ 3200]\n",
      "loss: 0.207507  [  464/ 3200]\n",
      "loss: 0.128079  [  480/ 3200]\n",
      "loss: 0.228537  [  496/ 3200]\n",
      "loss: 0.198021  [  512/ 3200]\n",
      "loss: 0.152934  [  528/ 3200]\n",
      "loss: 0.114891  [  544/ 3200]\n",
      "loss: 0.316470  [  560/ 3200]\n",
      "loss: 0.087967  [  576/ 3200]\n",
      "loss: 0.118729  [  592/ 3200]\n",
      "loss: 0.137344  [  608/ 3200]\n",
      "loss: 0.269468  [  624/ 3200]\n",
      "loss: 0.253472  [  640/ 3200]\n",
      "loss: 0.245339  [  656/ 3200]\n",
      "loss: 0.104862  [  672/ 3200]\n",
      "loss: 0.469301  [  688/ 3200]\n",
      "loss: 0.207909  [  704/ 3200]\n",
      "loss: 0.227344  [  720/ 3200]\n",
      "loss: 0.157177  [  736/ 3200]\n",
      "loss: 0.121746  [  752/ 3200]\n",
      "loss: 0.166872  [  768/ 3200]\n",
      "loss: 0.136249  [  784/ 3200]\n",
      "loss: 0.125409  [  800/ 3200]\n",
      "loss: 0.253787  [  816/ 3200]\n",
      "loss: 0.254322  [  832/ 3200]\n",
      "loss: 0.186016  [  848/ 3200]\n",
      "loss: 0.097247  [  864/ 3200]\n",
      "loss: 0.251608  [  880/ 3200]\n",
      "loss: 0.072673  [  896/ 3200]\n",
      "loss: 0.052116  [  912/ 3200]\n",
      "loss: 0.203692  [  928/ 3200]\n",
      "loss: 0.195837  [  944/ 3200]\n",
      "loss: 0.201655  [  960/ 3200]\n",
      "loss: 0.132731  [  976/ 3200]\n",
      "loss: 0.166559  [  992/ 3200]\n",
      "loss: 0.118664  [ 1008/ 3200]\n",
      "loss: 0.174400  [ 1024/ 3200]\n",
      "loss: 0.127050  [ 1040/ 3200]\n",
      "loss: 0.158788  [ 1056/ 3200]\n",
      "loss: 0.153750  [ 1072/ 3200]\n",
      "loss: 0.290683  [ 1088/ 3200]\n",
      "loss: 0.264895  [ 1104/ 3200]\n",
      "loss: 0.144619  [ 1120/ 3200]\n",
      "loss: 0.232488  [ 1136/ 3200]\n",
      "loss: 0.255680  [ 1152/ 3200]\n",
      "loss: 0.298818  [ 1168/ 3200]\n",
      "loss: 0.353283  [ 1184/ 3200]\n",
      "loss: 0.316568  [ 1200/ 3200]\n",
      "loss: 0.294527  [ 1216/ 3200]\n",
      "loss: 0.183519  [ 1232/ 3200]\n",
      "loss: 0.137303  [ 1248/ 3200]\n",
      "loss: 0.109904  [ 1264/ 3200]\n",
      "loss: 0.237155  [ 1280/ 3200]\n",
      "loss: 0.169357  [ 1296/ 3200]\n",
      "loss: 0.321740  [ 1312/ 3200]\n",
      "loss: 0.219450  [ 1328/ 3200]\n",
      "loss: 0.086309  [ 1344/ 3200]\n",
      "loss: 0.097985  [ 1360/ 3200]\n",
      "loss: 0.254722  [ 1376/ 3200]\n",
      "loss: 0.170615  [ 1392/ 3200]\n",
      "loss: 0.194809  [ 1408/ 3200]\n",
      "loss: 0.163653  [ 1424/ 3200]\n",
      "loss: 0.086026  [ 1440/ 3200]\n",
      "loss: 0.055929  [ 1456/ 3200]\n",
      "loss: 0.069308  [ 1472/ 3200]\n",
      "loss: 0.361170  [ 1488/ 3200]\n",
      "loss: 0.201313  [ 1504/ 3200]\n",
      "loss: 0.042258  [ 1520/ 3200]\n",
      "loss: 0.095631  [ 1536/ 3200]\n",
      "loss: 0.143329  [ 1552/ 3200]\n",
      "loss: 0.377056  [ 1568/ 3200]\n",
      "loss: 0.109310  [ 1584/ 3200]\n",
      "loss: 0.053500  [ 1600/ 3200]\n",
      "loss: 0.183973  [ 1616/ 3200]\n",
      "loss: 0.137480  [ 1632/ 3200]\n",
      "loss: 0.112579  [ 1648/ 3200]\n",
      "loss: 0.106038  [ 1664/ 3200]\n",
      "loss: 0.292631  [ 1680/ 3200]\n",
      "loss: 0.260847  [ 1696/ 3200]\n",
      "loss: 0.023665  [ 1712/ 3200]\n",
      "loss: 0.130683  [ 1728/ 3200]\n",
      "loss: 0.330421  [ 1744/ 3200]\n",
      "loss: 0.203572  [ 1760/ 3200]\n",
      "loss: 0.249471  [ 1776/ 3200]\n",
      "loss: 0.104575  [ 1792/ 3200]\n",
      "loss: 0.106790  [ 1808/ 3200]\n",
      "loss: 0.383690  [ 1824/ 3200]\n",
      "loss: 0.164959  [ 1840/ 3200]\n",
      "loss: 0.257568  [ 1856/ 3200]\n",
      "loss: 0.104768  [ 1872/ 3200]\n",
      "loss: 0.090992  [ 1888/ 3200]\n",
      "loss: 0.077534  [ 1904/ 3200]\n",
      "loss: 0.087850  [ 1920/ 3200]\n",
      "loss: 0.023372  [ 1936/ 3200]\n",
      "loss: 0.177871  [ 1952/ 3200]\n",
      "loss: 0.171639  [ 1968/ 3200]\n",
      "loss: 0.253491  [ 1984/ 3200]\n",
      "loss: 0.229839  [ 2000/ 3200]\n",
      "loss: 0.101895  [ 2016/ 3200]\n",
      "loss: 0.112392  [ 2032/ 3200]\n",
      "loss: 0.113996  [ 2048/ 3200]\n",
      "loss: 0.110446  [ 2064/ 3200]\n",
      "loss: 0.300822  [ 2080/ 3200]\n",
      "loss: 0.540397  [ 2096/ 3200]\n",
      "loss: 0.189335  [ 2112/ 3200]\n",
      "loss: 0.132775  [ 2128/ 3200]\n",
      "loss: 0.077475  [ 2144/ 3200]\n",
      "loss: 0.266563  [ 2160/ 3200]\n",
      "loss: 0.154970  [ 2176/ 3200]\n",
      "loss: 0.052214  [ 2192/ 3200]\n",
      "loss: 0.108300  [ 2208/ 3200]\n",
      "loss: 0.258678  [ 2224/ 3200]\n",
      "loss: 0.221113  [ 2240/ 3200]\n",
      "loss: 0.219521  [ 2256/ 3200]\n",
      "loss: 0.118277  [ 2272/ 3200]\n",
      "loss: 0.137508  [ 2288/ 3200]\n",
      "loss: 0.121596  [ 2304/ 3200]\n",
      "loss: 0.320969  [ 2320/ 3200]\n",
      "loss: 0.082718  [ 2336/ 3200]\n",
      "loss: 0.140368  [ 2352/ 3200]\n",
      "loss: 0.142128  [ 2368/ 3200]\n",
      "loss: 0.386815  [ 2384/ 3200]\n",
      "loss: 0.326071  [ 2400/ 3200]\n",
      "loss: 0.204786  [ 2416/ 3200]\n",
      "loss: 0.195876  [ 2432/ 3200]\n",
      "loss: 0.316680  [ 2448/ 3200]\n",
      "loss: 0.092016  [ 2464/ 3200]\n",
      "loss: 0.152385  [ 2480/ 3200]\n",
      "loss: 0.084942  [ 2496/ 3200]\n",
      "loss: 0.084992  [ 2512/ 3200]\n",
      "loss: 0.054299  [ 2528/ 3200]\n",
      "loss: 0.192631  [ 2544/ 3200]\n",
      "loss: 0.181399  [ 2560/ 3200]\n",
      "loss: 0.095036  [ 2576/ 3200]\n",
      "loss: 0.218045  [ 2592/ 3200]\n",
      "loss: 0.218050  [ 2608/ 3200]\n",
      "loss: 0.352194  [ 2624/ 3200]\n",
      "loss: 0.792093  [ 2640/ 3200]\n",
      "loss: 0.126880  [ 2656/ 3200]\n",
      "loss: 0.482599  [ 2672/ 3200]\n",
      "loss: 0.624259  [ 2688/ 3200]\n",
      "loss: 0.210263  [ 2704/ 3200]\n",
      "loss: 0.126782  [ 2720/ 3200]\n",
      "loss: 0.494236  [ 2736/ 3200]\n",
      "loss: 0.360218  [ 2752/ 3200]\n",
      "loss: 0.554583  [ 2768/ 3200]\n",
      "loss: 0.159129  [ 2784/ 3200]\n",
      "loss: 0.171543  [ 2800/ 3200]\n",
      "loss: 0.206655  [ 2816/ 3200]\n",
      "loss: 0.269572  [ 2832/ 3200]\n",
      "loss: 0.104270  [ 2848/ 3200]\n",
      "loss: 0.159019  [ 2864/ 3200]\n",
      "loss: 0.294549  [ 2880/ 3200]\n",
      "loss: 0.390765  [ 2896/ 3200]\n",
      "loss: 0.062909  [ 2912/ 3200]\n",
      "loss: 0.171204  [ 2928/ 3200]\n",
      "loss: 0.483754  [ 2944/ 3200]\n",
      "loss: 0.393741  [ 2960/ 3200]\n",
      "loss: 0.075792  [ 2976/ 3200]\n",
      "loss: 0.083205  [ 2992/ 3200]\n",
      "loss: 0.396703  [ 3008/ 3200]\n",
      "loss: 0.304154  [ 3024/ 3200]\n",
      "loss: 0.161616  [ 3040/ 3200]\n",
      "loss: 0.151004  [ 3056/ 3200]\n",
      "loss: 0.301039  [ 3072/ 3200]\n",
      "loss: 0.953742  [ 3088/ 3200]\n",
      "loss: 0.473454  [ 3104/ 3200]\n",
      "loss: 0.123389  [ 3120/ 3200]\n",
      "loss: 0.194805  [ 3136/ 3200]\n",
      "loss: 0.092066  [ 3152/ 3200]\n",
      "loss: 0.267164  [ 3168/ 3200]\n",
      "loss: 0.603024  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 0.120776  [    0/ 3200]\n",
      "loss: 0.141454  [   16/ 3200]\n",
      "loss: 0.136535  [   32/ 3200]\n",
      "loss: 0.147934  [   48/ 3200]\n",
      "loss: 0.127419  [   64/ 3200]\n",
      "loss: 0.123107  [   80/ 3200]\n",
      "loss: 0.231794  [   96/ 3200]\n",
      "loss: 0.127788  [  112/ 3200]\n",
      "loss: 0.035800  [  128/ 3200]\n",
      "loss: 0.174828  [  144/ 3200]\n",
      "loss: 0.074892  [  160/ 3200]\n",
      "loss: 0.056642  [  176/ 3200]\n",
      "loss: 0.058069  [  192/ 3200]\n",
      "loss: 0.083072  [  208/ 3200]\n",
      "loss: 0.464602  [  224/ 3200]\n",
      "loss: 0.276451  [  240/ 3200]\n",
      "loss: 0.117616  [  256/ 3200]\n",
      "loss: 0.084704  [  272/ 3200]\n",
      "loss: 0.195044  [  288/ 3200]\n",
      "loss: 0.201495  [  304/ 3200]\n",
      "loss: 0.333514  [  320/ 3200]\n",
      "loss: 0.231651  [  336/ 3200]\n",
      "loss: 0.477073  [  352/ 3200]\n",
      "loss: 0.150384  [  368/ 3200]\n",
      "loss: 0.168456  [  384/ 3200]\n",
      "loss: 0.178952  [  400/ 3200]\n",
      "loss: 0.426045  [  416/ 3200]\n",
      "loss: 0.171075  [  432/ 3200]\n",
      "loss: 0.052026  [  448/ 3200]\n",
      "loss: 0.137152  [  464/ 3200]\n",
      "loss: 0.107451  [  480/ 3200]\n",
      "loss: 0.045339  [  496/ 3200]\n",
      "loss: 0.225757  [  512/ 3200]\n",
      "loss: 0.061422  [  528/ 3200]\n",
      "loss: 0.027880  [  544/ 3200]\n",
      "loss: 0.386639  [  560/ 3200]\n",
      "loss: 0.106231  [  576/ 3200]\n",
      "loss: 0.193230  [  592/ 3200]\n",
      "loss: 0.198433  [  608/ 3200]\n",
      "loss: 0.070619  [  624/ 3200]\n",
      "loss: 0.198333  [  640/ 3200]\n",
      "loss: 0.118278  [  656/ 3200]\n",
      "loss: 0.347631  [  672/ 3200]\n",
      "loss: 0.096088  [  688/ 3200]\n",
      "loss: 0.205604  [  704/ 3200]\n",
      "loss: 0.097820  [  720/ 3200]\n",
      "loss: 0.148421  [  736/ 3200]\n",
      "loss: 0.224173  [  752/ 3200]\n",
      "loss: 0.080591  [  768/ 3200]\n",
      "loss: 0.043783  [  784/ 3200]\n",
      "loss: 0.089175  [  800/ 3200]\n",
      "loss: 0.337870  [  816/ 3200]\n",
      "loss: 0.176500  [  832/ 3200]\n",
      "loss: 0.096807  [  848/ 3200]\n",
      "loss: 0.179393  [  864/ 3200]\n",
      "loss: 0.134291  [  880/ 3200]\n",
      "loss: 0.071066  [  896/ 3200]\n",
      "loss: 0.382487  [  912/ 3200]\n",
      "loss: 0.180037  [  928/ 3200]\n",
      "loss: 0.347085  [  944/ 3200]\n",
      "loss: 0.166786  [  960/ 3200]\n",
      "loss: 0.040573  [  976/ 3200]\n",
      "loss: 0.121055  [  992/ 3200]\n",
      "loss: 0.187485  [ 1008/ 3200]\n",
      "loss: 0.236032  [ 1024/ 3200]\n",
      "loss: 0.214521  [ 1040/ 3200]\n",
      "loss: 0.287516  [ 1056/ 3200]\n",
      "loss: 0.063409  [ 1072/ 3200]\n",
      "loss: 0.132167  [ 1088/ 3200]\n",
      "loss: 0.061793  [ 1104/ 3200]\n",
      "loss: 0.156824  [ 1120/ 3200]\n",
      "loss: 0.114531  [ 1136/ 3200]\n",
      "loss: 0.092112  [ 1152/ 3200]\n",
      "loss: 0.148958  [ 1168/ 3200]\n",
      "loss: 0.378908  [ 1184/ 3200]\n",
      "loss: 0.159349  [ 1200/ 3200]\n",
      "loss: 0.072390  [ 1216/ 3200]\n",
      "loss: 0.145314  [ 1232/ 3200]\n",
      "loss: 0.515159  [ 1248/ 3200]\n",
      "loss: 0.101245  [ 1264/ 3200]\n",
      "loss: 0.071255  [ 1280/ 3200]\n",
      "loss: 0.145199  [ 1296/ 3200]\n",
      "loss: 0.086837  [ 1312/ 3200]\n",
      "loss: 0.362199  [ 1328/ 3200]\n",
      "loss: 0.199400  [ 1344/ 3200]\n",
      "loss: 0.064312  [ 1360/ 3200]\n",
      "loss: 0.023323  [ 1376/ 3200]\n",
      "loss: 0.141927  [ 1392/ 3200]\n",
      "loss: 0.135884  [ 1408/ 3200]\n",
      "loss: 0.171362  [ 1424/ 3200]\n",
      "loss: 0.161360  [ 1440/ 3200]\n",
      "loss: 0.025661  [ 1456/ 3200]\n",
      "loss: 0.300318  [ 1472/ 3200]\n",
      "loss: 0.242658  [ 1488/ 3200]\n",
      "loss: 0.191415  [ 1504/ 3200]\n",
      "loss: 0.180138  [ 1520/ 3200]\n",
      "loss: 0.241626  [ 1536/ 3200]\n",
      "loss: 0.202218  [ 1552/ 3200]\n",
      "loss: 0.216950  [ 1568/ 3200]\n",
      "loss: 0.087841  [ 1584/ 3200]\n",
      "loss: 0.109417  [ 1600/ 3200]\n",
      "loss: 0.239682  [ 1616/ 3200]\n",
      "loss: 0.587279  [ 1632/ 3200]\n",
      "loss: 0.070172  [ 1648/ 3200]\n",
      "loss: 0.130847  [ 1664/ 3200]\n",
      "loss: 0.041510  [ 1680/ 3200]\n",
      "loss: 0.125722  [ 1696/ 3200]\n",
      "loss: 0.231825  [ 1712/ 3200]\n",
      "loss: 0.186868  [ 1728/ 3200]\n",
      "loss: 0.063779  [ 1744/ 3200]\n",
      "loss: 0.156394  [ 1760/ 3200]\n",
      "loss: 0.132585  [ 1776/ 3200]\n",
      "loss: 0.052195  [ 1792/ 3200]\n",
      "loss: 0.261221  [ 1808/ 3200]\n",
      "loss: 0.046608  [ 1824/ 3200]\n",
      "loss: 0.104904  [ 1840/ 3200]\n",
      "loss: 0.125781  [ 1856/ 3200]\n",
      "loss: 0.099487  [ 1872/ 3200]\n",
      "loss: 0.049138  [ 1888/ 3200]\n",
      "loss: 0.091723  [ 1904/ 3200]\n",
      "loss: 0.110052  [ 1920/ 3200]\n",
      "loss: 0.071101  [ 1936/ 3200]\n",
      "loss: 0.279638  [ 1952/ 3200]\n",
      "loss: 0.176704  [ 1968/ 3200]\n",
      "loss: 0.018908  [ 1984/ 3200]\n",
      "loss: 0.092632  [ 2000/ 3200]\n",
      "loss: 0.037623  [ 2016/ 3200]\n",
      "loss: 0.373457  [ 2032/ 3200]\n",
      "loss: 0.221932  [ 2048/ 3200]\n",
      "loss: 0.236208  [ 2064/ 3200]\n",
      "loss: 0.226939  [ 2080/ 3200]\n",
      "loss: 0.269723  [ 2096/ 3200]\n",
      "loss: 0.284927  [ 2112/ 3200]\n",
      "loss: 0.271318  [ 2128/ 3200]\n",
      "loss: 0.154054  [ 2144/ 3200]\n",
      "loss: 0.159139  [ 2160/ 3200]\n",
      "loss: 0.050805  [ 2176/ 3200]\n",
      "loss: 0.144230  [ 2192/ 3200]\n",
      "loss: 0.137221  [ 2208/ 3200]\n",
      "loss: 0.184365  [ 2224/ 3200]\n",
      "loss: 0.093425  [ 2240/ 3200]\n",
      "loss: 0.111041  [ 2256/ 3200]\n",
      "loss: 0.276361  [ 2272/ 3200]\n",
      "loss: 0.141816  [ 2288/ 3200]\n",
      "loss: 0.040444  [ 2304/ 3200]\n",
      "loss: 0.055567  [ 2320/ 3200]\n",
      "loss: 0.137862  [ 2336/ 3200]\n",
      "loss: 0.071825  [ 2352/ 3200]\n",
      "loss: 0.087897  [ 2368/ 3200]\n",
      "loss: 0.247564  [ 2384/ 3200]\n",
      "loss: 0.211884  [ 2400/ 3200]\n",
      "loss: 0.380198  [ 2416/ 3200]\n",
      "loss: 0.117033  [ 2432/ 3200]\n",
      "loss: 0.320521  [ 2448/ 3200]\n",
      "loss: 0.148856  [ 2464/ 3200]\n",
      "loss: 0.120437  [ 2480/ 3200]\n",
      "loss: 0.106989  [ 2496/ 3200]\n",
      "loss: 0.274590  [ 2512/ 3200]\n",
      "loss: 0.078636  [ 2528/ 3200]\n",
      "loss: 0.143941  [ 2544/ 3200]\n",
      "loss: 0.051974  [ 2560/ 3200]\n",
      "loss: 0.081038  [ 2576/ 3200]\n",
      "loss: 0.046431  [ 2592/ 3200]\n",
      "loss: 0.164565  [ 2608/ 3200]\n",
      "loss: 0.075429  [ 2624/ 3200]\n",
      "loss: 0.058451  [ 2640/ 3200]\n",
      "loss: 0.077170  [ 2656/ 3200]\n",
      "loss: 0.211938  [ 2672/ 3200]\n",
      "loss: 0.106518  [ 2688/ 3200]\n",
      "loss: 0.158120  [ 2704/ 3200]\n",
      "loss: 0.045926  [ 2720/ 3200]\n",
      "loss: 0.194350  [ 2736/ 3200]\n",
      "loss: 0.250136  [ 2752/ 3200]\n",
      "loss: 0.137389  [ 2768/ 3200]\n",
      "loss: 0.143266  [ 2784/ 3200]\n",
      "loss: 0.192165  [ 2800/ 3200]\n",
      "loss: 0.089046  [ 2816/ 3200]\n",
      "loss: 0.287471  [ 2832/ 3200]\n",
      "loss: 0.313270  [ 2848/ 3200]\n",
      "loss: 0.196454  [ 2864/ 3200]\n",
      "loss: 0.099722  [ 2880/ 3200]\n",
      "loss: 0.058918  [ 2896/ 3200]\n",
      "loss: 0.263263  [ 2912/ 3200]\n",
      "loss: 0.172645  [ 2928/ 3200]\n",
      "loss: 0.026668  [ 2944/ 3200]\n",
      "loss: 0.105403  [ 2960/ 3200]\n",
      "loss: 0.056259  [ 2976/ 3200]\n",
      "loss: 0.043417  [ 2992/ 3200]\n",
      "loss: 0.084761  [ 3008/ 3200]\n",
      "loss: 0.188909  [ 3024/ 3200]\n",
      "loss: 0.586762  [ 3040/ 3200]\n",
      "loss: 0.207215  [ 3056/ 3200]\n",
      "loss: 0.121052  [ 3072/ 3200]\n",
      "loss: 0.103005  [ 3088/ 3200]\n",
      "loss: 0.101594  [ 3104/ 3200]\n",
      "loss: 0.245875  [ 3120/ 3200]\n",
      "loss: 0.101542  [ 3136/ 3200]\n",
      "loss: 0.307414  [ 3152/ 3200]\n",
      "loss: 0.049929  [ 3168/ 3200]\n",
      "loss: 0.090821  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.135671  [    0/ 3200]\n",
      "loss: 0.118241  [   16/ 3200]\n",
      "loss: 0.105727  [   32/ 3200]\n",
      "loss: 0.064325  [   48/ 3200]\n",
      "loss: 0.059989  [   64/ 3200]\n",
      "loss: 0.200881  [   80/ 3200]\n",
      "loss: 0.127376  [   96/ 3200]\n",
      "loss: 0.276490  [  112/ 3200]\n",
      "loss: 0.049415  [  128/ 3200]\n",
      "loss: 0.047575  [  144/ 3200]\n",
      "loss: 0.058500  [  160/ 3200]\n",
      "loss: 0.093423  [  176/ 3200]\n",
      "loss: 0.169994  [  192/ 3200]\n",
      "loss: 0.154884  [  208/ 3200]\n",
      "loss: 0.055508  [  224/ 3200]\n",
      "loss: 0.114011  [  240/ 3200]\n",
      "loss: 0.257887  [  256/ 3200]\n",
      "loss: 0.050585  [  272/ 3200]\n",
      "loss: 0.080077  [  288/ 3200]\n",
      "loss: 0.036676  [  304/ 3200]\n",
      "loss: 0.200636  [  320/ 3200]\n",
      "loss: 0.084640  [  336/ 3200]\n",
      "loss: 0.079707  [  352/ 3200]\n",
      "loss: 0.046836  [  368/ 3200]\n",
      "loss: 0.138182  [  384/ 3200]\n",
      "loss: 0.168622  [  400/ 3200]\n",
      "loss: 0.119513  [  416/ 3200]\n",
      "loss: 0.102789  [  432/ 3200]\n",
      "loss: 0.157347  [  448/ 3200]\n",
      "loss: 0.086343  [  464/ 3200]\n",
      "loss: 0.164075  [  480/ 3200]\n",
      "loss: 0.279596  [  496/ 3200]\n",
      "loss: 0.082869  [  512/ 3200]\n",
      "loss: 0.178867  [  528/ 3200]\n",
      "loss: 0.076523  [  544/ 3200]\n",
      "loss: 0.052207  [  560/ 3200]\n",
      "loss: 0.112802  [  576/ 3200]\n",
      "loss: 0.263722  [  592/ 3200]\n",
      "loss: 0.267604  [  608/ 3200]\n",
      "loss: 0.144181  [  624/ 3200]\n",
      "loss: 0.071776  [  640/ 3200]\n",
      "loss: 0.047762  [  656/ 3200]\n",
      "loss: 0.100362  [  672/ 3200]\n",
      "loss: 0.172637  [  688/ 3200]\n",
      "loss: 0.136621  [  704/ 3200]\n",
      "loss: 0.090770  [  720/ 3200]\n",
      "loss: 0.041386  [  736/ 3200]\n",
      "loss: 0.080202  [  752/ 3200]\n",
      "loss: 0.073683  [  768/ 3200]\n",
      "loss: 0.034631  [  784/ 3200]\n",
      "loss: 0.185930  [  800/ 3200]\n",
      "loss: 0.086798  [  816/ 3200]\n",
      "loss: 0.176437  [  832/ 3200]\n",
      "loss: 0.074558  [  848/ 3200]\n",
      "loss: 0.067525  [  864/ 3200]\n",
      "loss: 0.075732  [  880/ 3200]\n",
      "loss: 0.100532  [  896/ 3200]\n",
      "loss: 0.054227  [  912/ 3200]\n",
      "loss: 0.196476  [  928/ 3200]\n",
      "loss: 0.039548  [  944/ 3200]\n",
      "loss: 0.132056  [  960/ 3200]\n",
      "loss: 0.079176  [  976/ 3200]\n",
      "loss: 0.024156  [  992/ 3200]\n",
      "loss: 0.079915  [ 1008/ 3200]\n",
      "loss: 0.124419  [ 1024/ 3200]\n",
      "loss: 0.055543  [ 1040/ 3200]\n",
      "loss: 0.153249  [ 1056/ 3200]\n",
      "loss: 0.102421  [ 1072/ 3200]\n",
      "loss: 0.174962  [ 1088/ 3200]\n",
      "loss: 0.062811  [ 1104/ 3200]\n",
      "loss: 0.050676  [ 1120/ 3200]\n",
      "loss: 0.026014  [ 1136/ 3200]\n",
      "loss: 0.028147  [ 1152/ 3200]\n",
      "loss: 0.057184  [ 1168/ 3200]\n",
      "loss: 0.133951  [ 1184/ 3200]\n",
      "loss: 0.053036  [ 1200/ 3200]\n",
      "loss: 0.017459  [ 1216/ 3200]\n",
      "loss: 0.070167  [ 1232/ 3200]\n",
      "loss: 0.044995  [ 1248/ 3200]\n",
      "loss: 0.113306  [ 1264/ 3200]\n",
      "loss: 0.069915  [ 1280/ 3200]\n",
      "loss: 0.043978  [ 1296/ 3200]\n",
      "loss: 0.059385  [ 1312/ 3200]\n",
      "loss: 0.180986  [ 1328/ 3200]\n",
      "loss: 0.633129  [ 1344/ 3200]\n",
      "loss: 0.121636  [ 1360/ 3200]\n",
      "loss: 0.055659  [ 1376/ 3200]\n",
      "loss: 0.046020  [ 1392/ 3200]\n",
      "loss: 0.122053  [ 1408/ 3200]\n",
      "loss: 0.072031  [ 1424/ 3200]\n",
      "loss: 0.143866  [ 1440/ 3200]\n",
      "loss: 0.041498  [ 1456/ 3200]\n",
      "loss: 0.065443  [ 1472/ 3200]\n",
      "loss: 0.218776  [ 1488/ 3200]\n",
      "loss: 0.551848  [ 1504/ 3200]\n",
      "loss: 0.116473  [ 1520/ 3200]\n",
      "loss: 0.116616  [ 1536/ 3200]\n",
      "loss: 0.062842  [ 1552/ 3200]\n",
      "loss: 0.106157  [ 1568/ 3200]\n",
      "loss: 0.123579  [ 1584/ 3200]\n",
      "loss: 0.057621  [ 1600/ 3200]\n",
      "loss: 0.119808  [ 1616/ 3200]\n",
      "loss: 0.043764  [ 1632/ 3200]\n",
      "loss: 0.052657  [ 1648/ 3200]\n",
      "loss: 0.115439  [ 1664/ 3200]\n",
      "loss: 0.324350  [ 1680/ 3200]\n",
      "loss: 0.195219  [ 1696/ 3200]\n",
      "loss: 0.106974  [ 1712/ 3200]\n",
      "loss: 0.090624  [ 1728/ 3200]\n",
      "loss: 0.464882  [ 1744/ 3200]\n",
      "loss: 0.087472  [ 1760/ 3200]\n",
      "loss: 0.113864  [ 1776/ 3200]\n",
      "loss: 0.187992  [ 1792/ 3200]\n",
      "loss: 0.180744  [ 1808/ 3200]\n",
      "loss: 0.140640  [ 1824/ 3200]\n",
      "loss: 0.021317  [ 1840/ 3200]\n",
      "loss: 0.302328  [ 1856/ 3200]\n",
      "loss: 0.191620  [ 1872/ 3200]\n",
      "loss: 0.117946  [ 1888/ 3200]\n",
      "loss: 0.670738  [ 1904/ 3200]\n",
      "loss: 0.212711  [ 1920/ 3200]\n",
      "loss: 0.339858  [ 1936/ 3200]\n",
      "loss: 0.043939  [ 1952/ 3200]\n",
      "loss: 0.067220  [ 1968/ 3200]\n",
      "loss: 0.203529  [ 1984/ 3200]\n",
      "loss: 0.112589  [ 2000/ 3200]\n",
      "loss: 0.061285  [ 2016/ 3200]\n",
      "loss: 0.038143  [ 2032/ 3200]\n",
      "loss: 0.133339  [ 2048/ 3200]\n",
      "loss: 0.112955  [ 2064/ 3200]\n",
      "loss: 0.362751  [ 2080/ 3200]\n",
      "loss: 0.051718  [ 2096/ 3200]\n",
      "loss: 0.175949  [ 2112/ 3200]\n",
      "loss: 0.157268  [ 2128/ 3200]\n",
      "loss: 0.152741  [ 2144/ 3200]\n",
      "loss: 0.126256  [ 2160/ 3200]\n",
      "loss: 0.149036  [ 2176/ 3200]\n",
      "loss: 0.061092  [ 2192/ 3200]\n",
      "loss: 0.237608  [ 2208/ 3200]\n",
      "loss: 0.194539  [ 2224/ 3200]\n",
      "loss: 0.290155  [ 2240/ 3200]\n",
      "loss: 0.143180  [ 2256/ 3200]\n",
      "loss: 0.157506  [ 2272/ 3200]\n",
      "loss: 0.067932  [ 2288/ 3200]\n",
      "loss: 0.075460  [ 2304/ 3200]\n",
      "loss: 0.029627  [ 2320/ 3200]\n",
      "loss: 0.104659  [ 2336/ 3200]\n",
      "loss: 0.262199  [ 2352/ 3200]\n",
      "loss: 0.104964  [ 2368/ 3200]\n",
      "loss: 0.123322  [ 2384/ 3200]\n",
      "loss: 0.132961  [ 2400/ 3200]\n",
      "loss: 0.259662  [ 2416/ 3200]\n",
      "loss: 0.191750  [ 2432/ 3200]\n",
      "loss: 0.156749  [ 2448/ 3200]\n",
      "loss: 0.181292  [ 2464/ 3200]\n",
      "loss: 0.312631  [ 2480/ 3200]\n",
      "loss: 0.020861  [ 2496/ 3200]\n",
      "loss: 0.063855  [ 2512/ 3200]\n",
      "loss: 0.110209  [ 2528/ 3200]\n",
      "loss: 0.073609  [ 2544/ 3200]\n",
      "loss: 0.052332  [ 2560/ 3200]\n",
      "loss: 0.091168  [ 2576/ 3200]\n",
      "loss: 0.086104  [ 2592/ 3200]\n",
      "loss: 0.089425  [ 2608/ 3200]\n",
      "loss: 0.068117  [ 2624/ 3200]\n",
      "loss: 0.065824  [ 2640/ 3200]\n",
      "loss: 0.135496  [ 2656/ 3200]\n",
      "loss: 0.452081  [ 2672/ 3200]\n",
      "loss: 0.063886  [ 2688/ 3200]\n",
      "loss: 0.086136  [ 2704/ 3200]\n",
      "loss: 0.189426  [ 2720/ 3200]\n",
      "loss: 0.149813  [ 2736/ 3200]\n",
      "loss: 0.083815  [ 2752/ 3200]\n",
      "loss: 0.151631  [ 2768/ 3200]\n",
      "loss: 0.094713  [ 2784/ 3200]\n",
      "loss: 0.197506  [ 2800/ 3200]\n",
      "loss: 0.200263  [ 2816/ 3200]\n",
      "loss: 0.064810  [ 2832/ 3200]\n",
      "loss: 0.201032  [ 2848/ 3200]\n",
      "loss: 0.072546  [ 2864/ 3200]\n",
      "loss: 0.123231  [ 2880/ 3200]\n",
      "loss: 0.099045  [ 2896/ 3200]\n",
      "loss: 0.054211  [ 2912/ 3200]\n",
      "loss: 0.078483  [ 2928/ 3200]\n",
      "loss: 0.057509  [ 2944/ 3200]\n",
      "loss: 0.028865  [ 2960/ 3200]\n",
      "loss: 0.100097  [ 2976/ 3200]\n",
      "loss: 0.348470  [ 2992/ 3200]\n",
      "loss: 0.260390  [ 3008/ 3200]\n",
      "loss: 0.058207  [ 3024/ 3200]\n",
      "loss: 0.047819  [ 3040/ 3200]\n",
      "loss: 0.208342  [ 3056/ 3200]\n",
      "loss: 0.196466  [ 3072/ 3200]\n",
      "loss: 0.175516  [ 3088/ 3200]\n",
      "loss: 0.377744  [ 3104/ 3200]\n",
      "loss: 0.059745  [ 3120/ 3200]\n",
      "loss: 0.061870  [ 3136/ 3200]\n",
      "loss: 0.186261  [ 3152/ 3200]\n",
      "loss: 0.060509  [ 3168/ 3200]\n",
      "loss: 0.154133  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 0.180974  [    0/ 3200]\n",
      "loss: 0.059962  [   16/ 3200]\n",
      "loss: 0.075393  [   32/ 3200]\n",
      "loss: 0.183416  [   48/ 3200]\n",
      "loss: 0.076314  [   64/ 3200]\n",
      "loss: 0.407551  [   80/ 3200]\n",
      "loss: 0.278830  [   96/ 3200]\n",
      "loss: 0.192761  [  112/ 3200]\n",
      "loss: 0.059410  [  128/ 3200]\n",
      "loss: 0.061840  [  144/ 3200]\n",
      "loss: 0.050131  [  160/ 3200]\n",
      "loss: 0.105444  [  176/ 3200]\n",
      "loss: 0.028244  [  192/ 3200]\n",
      "loss: 0.039817  [  208/ 3200]\n",
      "loss: 0.032980  [  224/ 3200]\n",
      "loss: 0.072330  [  240/ 3200]\n",
      "loss: 0.035878  [  256/ 3200]\n",
      "loss: 0.116102  [  272/ 3200]\n",
      "loss: 0.040010  [  288/ 3200]\n",
      "loss: 0.099136  [  304/ 3200]\n",
      "loss: 0.075950  [  320/ 3200]\n",
      "loss: 0.117979  [  336/ 3200]\n",
      "loss: 0.043683  [  352/ 3200]\n",
      "loss: 0.264848  [  368/ 3200]\n",
      "loss: 0.055063  [  384/ 3200]\n",
      "loss: 0.045570  [  400/ 3200]\n",
      "loss: 0.149007  [  416/ 3200]\n",
      "loss: 0.114745  [  432/ 3200]\n",
      "loss: 0.077352  [  448/ 3200]\n",
      "loss: 0.235595  [  464/ 3200]\n",
      "loss: 0.396136  [  480/ 3200]\n",
      "loss: 0.208925  [  496/ 3200]\n",
      "loss: 0.099133  [  512/ 3200]\n",
      "loss: 0.040888  [  528/ 3200]\n",
      "loss: 0.052848  [  544/ 3200]\n",
      "loss: 0.103845  [  560/ 3200]\n",
      "loss: 0.097934  [  576/ 3200]\n",
      "loss: 0.121793  [  592/ 3200]\n",
      "loss: 0.030149  [  608/ 3200]\n",
      "loss: 0.160635  [  624/ 3200]\n",
      "loss: 0.020232  [  640/ 3200]\n",
      "loss: 0.086726  [  656/ 3200]\n",
      "loss: 0.059732  [  672/ 3200]\n",
      "loss: 0.042157  [  688/ 3200]\n",
      "loss: 0.062507  [  704/ 3200]\n",
      "loss: 0.128861  [  720/ 3200]\n",
      "loss: 0.016690  [  736/ 3200]\n",
      "loss: 0.167153  [  752/ 3200]\n",
      "loss: 0.041412  [  768/ 3200]\n",
      "loss: 0.032190  [  784/ 3200]\n",
      "loss: 0.031791  [  800/ 3200]\n",
      "loss: 0.088901  [  816/ 3200]\n",
      "loss: 0.048944  [  832/ 3200]\n",
      "loss: 0.173815  [  848/ 3200]\n",
      "loss: 0.064715  [  864/ 3200]\n",
      "loss: 0.030988  [  880/ 3200]\n",
      "loss: 0.070539  [  896/ 3200]\n",
      "loss: 0.214095  [  912/ 3200]\n",
      "loss: 0.183527  [  928/ 3200]\n",
      "loss: 0.121963  [  944/ 3200]\n",
      "loss: 0.139887  [  960/ 3200]\n",
      "loss: 0.066476  [  976/ 3200]\n",
      "loss: 0.085774  [  992/ 3200]\n",
      "loss: 0.048926  [ 1008/ 3200]\n",
      "loss: 0.022304  [ 1024/ 3200]\n",
      "loss: 0.045033  [ 1040/ 3200]\n",
      "loss: 0.199737  [ 1056/ 3200]\n",
      "loss: 0.061946  [ 1072/ 3200]\n",
      "loss: 0.184579  [ 1088/ 3200]\n",
      "loss: 0.057773  [ 1104/ 3200]\n",
      "loss: 0.097785  [ 1120/ 3200]\n",
      "loss: 0.089650  [ 1136/ 3200]\n",
      "loss: 0.176632  [ 1152/ 3200]\n",
      "loss: 0.245210  [ 1168/ 3200]\n",
      "loss: 0.042999  [ 1184/ 3200]\n",
      "loss: 0.130451  [ 1200/ 3200]\n",
      "loss: 0.184558  [ 1216/ 3200]\n",
      "loss: 0.100354  [ 1232/ 3200]\n",
      "loss: 0.043516  [ 1248/ 3200]\n",
      "loss: 0.070959  [ 1264/ 3200]\n",
      "loss: 0.037365  [ 1280/ 3200]\n",
      "loss: 0.038736  [ 1296/ 3200]\n",
      "loss: 0.048565  [ 1312/ 3200]\n",
      "loss: 0.059758  [ 1328/ 3200]\n",
      "loss: 0.069317  [ 1344/ 3200]\n",
      "loss: 0.166543  [ 1360/ 3200]\n",
      "loss: 0.034360  [ 1376/ 3200]\n",
      "loss: 0.139512  [ 1392/ 3200]\n",
      "loss: 0.047906  [ 1408/ 3200]\n",
      "loss: 0.115390  [ 1424/ 3200]\n",
      "loss: 0.150849  [ 1440/ 3200]\n",
      "loss: 0.300316  [ 1456/ 3200]\n",
      "loss: 0.069456  [ 1472/ 3200]\n",
      "loss: 0.262419  [ 1488/ 3200]\n",
      "loss: 0.061138  [ 1504/ 3200]\n",
      "loss: 0.222305  [ 1520/ 3200]\n",
      "loss: 0.164106  [ 1536/ 3200]\n",
      "loss: 0.060166  [ 1552/ 3200]\n",
      "loss: 0.061732  [ 1568/ 3200]\n",
      "loss: 0.030535  [ 1584/ 3200]\n",
      "loss: 0.061654  [ 1600/ 3200]\n",
      "loss: 0.134447  [ 1616/ 3200]\n",
      "loss: 0.070911  [ 1632/ 3200]\n",
      "loss: 0.055683  [ 1648/ 3200]\n",
      "loss: 0.094809  [ 1664/ 3200]\n",
      "loss: 0.185423  [ 1680/ 3200]\n",
      "loss: 0.050311  [ 1696/ 3200]\n",
      "loss: 0.145138  [ 1712/ 3200]\n",
      "loss: 0.197719  [ 1728/ 3200]\n",
      "loss: 0.177606  [ 1744/ 3200]\n",
      "loss: 0.067284  [ 1760/ 3200]\n",
      "loss: 0.098686  [ 1776/ 3200]\n",
      "loss: 0.054753  [ 1792/ 3200]\n",
      "loss: 0.083297  [ 1808/ 3200]\n",
      "loss: 0.122485  [ 1824/ 3200]\n",
      "loss: 0.071697  [ 1840/ 3200]\n",
      "loss: 0.071955  [ 1856/ 3200]\n",
      "loss: 0.064184  [ 1872/ 3200]\n",
      "loss: 0.028186  [ 1888/ 3200]\n",
      "loss: 0.088128  [ 1904/ 3200]\n",
      "loss: 0.033157  [ 1920/ 3200]\n",
      "loss: 0.079963  [ 1936/ 3200]\n",
      "loss: 0.061955  [ 1952/ 3200]\n",
      "loss: 0.009765  [ 1968/ 3200]\n",
      "loss: 0.101257  [ 1984/ 3200]\n",
      "loss: 0.079302  [ 2000/ 3200]\n",
      "loss: 0.037188  [ 2016/ 3200]\n",
      "loss: 0.149394  [ 2032/ 3200]\n",
      "loss: 0.314059  [ 2048/ 3200]\n",
      "loss: 0.162394  [ 2064/ 3200]\n",
      "loss: 0.019911  [ 2080/ 3200]\n",
      "loss: 0.126742  [ 2096/ 3200]\n",
      "loss: 0.086467  [ 2112/ 3200]\n",
      "loss: 0.130177  [ 2128/ 3200]\n",
      "loss: 0.078998  [ 2144/ 3200]\n",
      "loss: 0.077248  [ 2160/ 3200]\n",
      "loss: 0.036516  [ 2176/ 3200]\n",
      "loss: 0.096909  [ 2192/ 3200]\n",
      "loss: 0.059443  [ 2208/ 3200]\n",
      "loss: 0.037474  [ 2224/ 3200]\n",
      "loss: 0.035008  [ 2240/ 3200]\n",
      "loss: 0.027006  [ 2256/ 3200]\n",
      "loss: 0.088452  [ 2272/ 3200]\n",
      "loss: 0.098273  [ 2288/ 3200]\n",
      "loss: 0.100302  [ 2304/ 3200]\n",
      "loss: 0.053205  [ 2320/ 3200]\n",
      "loss: 0.054543  [ 2336/ 3200]\n",
      "loss: 0.044255  [ 2352/ 3200]\n",
      "loss: 0.153378  [ 2368/ 3200]\n",
      "loss: 0.082533  [ 2384/ 3200]\n",
      "loss: 0.097717  [ 2400/ 3200]\n",
      "loss: 0.062479  [ 2416/ 3200]\n",
      "loss: 0.261617  [ 2432/ 3200]\n",
      "loss: 0.124952  [ 2448/ 3200]\n",
      "loss: 0.114588  [ 2464/ 3200]\n",
      "loss: 0.057012  [ 2480/ 3200]\n",
      "loss: 0.018776  [ 2496/ 3200]\n",
      "loss: 0.431881  [ 2512/ 3200]\n",
      "loss: 0.082127  [ 2528/ 3200]\n",
      "loss: 0.060501  [ 2544/ 3200]\n",
      "loss: 0.159018  [ 2560/ 3200]\n",
      "loss: 0.268037  [ 2576/ 3200]\n",
      "loss: 0.075931  [ 2592/ 3200]\n",
      "loss: 0.020142  [ 2608/ 3200]\n",
      "loss: 0.012811  [ 2624/ 3200]\n",
      "loss: 0.036080  [ 2640/ 3200]\n",
      "loss: 0.349371  [ 2656/ 3200]\n",
      "loss: 0.144423  [ 2672/ 3200]\n",
      "loss: 0.372819  [ 2688/ 3200]\n",
      "loss: 0.350409  [ 2704/ 3200]\n",
      "loss: 0.116432  [ 2720/ 3200]\n",
      "loss: 0.115757  [ 2736/ 3200]\n",
      "loss: 0.078207  [ 2752/ 3200]\n",
      "loss: 0.056960  [ 2768/ 3200]\n",
      "loss: 0.048075  [ 2784/ 3200]\n",
      "loss: 0.102131  [ 2800/ 3200]\n",
      "loss: 0.037725  [ 2816/ 3200]\n",
      "loss: 0.057836  [ 2832/ 3200]\n",
      "loss: 0.139851  [ 2848/ 3200]\n",
      "loss: 0.079245  [ 2864/ 3200]\n",
      "loss: 0.066562  [ 2880/ 3200]\n",
      "loss: 0.100691  [ 2896/ 3200]\n",
      "loss: 0.083252  [ 2912/ 3200]\n",
      "loss: 0.036534  [ 2928/ 3200]\n",
      "loss: 0.036580  [ 2944/ 3200]\n",
      "loss: 0.113945  [ 2960/ 3200]\n",
      "loss: 0.079508  [ 2976/ 3200]\n",
      "loss: 0.027922  [ 2992/ 3200]\n",
      "loss: 0.069072  [ 3008/ 3200]\n",
      "loss: 0.023347  [ 3024/ 3200]\n",
      "loss: 0.113049  [ 3040/ 3200]\n",
      "loss: 0.067458  [ 3056/ 3200]\n",
      "loss: 0.091299  [ 3072/ 3200]\n",
      "loss: 0.132375  [ 3088/ 3200]\n",
      "loss: 0.272681  [ 3104/ 3200]\n",
      "loss: 0.254609  [ 3120/ 3200]\n",
      "loss: 0.052850  [ 3136/ 3200]\n",
      "loss: 0.062197  [ 3152/ 3200]\n",
      "loss: 0.060560  [ 3168/ 3200]\n",
      "loss: 0.051903  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 0.065527  [    0/ 3200]\n",
      "loss: 0.047277  [   16/ 3200]\n",
      "loss: 0.024394  [   32/ 3200]\n",
      "loss: 0.050418  [   48/ 3200]\n",
      "loss: 0.296984  [   64/ 3200]\n",
      "loss: 0.142024  [   80/ 3200]\n",
      "loss: 0.092241  [   96/ 3200]\n",
      "loss: 0.098972  [  112/ 3200]\n",
      "loss: 0.030501  [  128/ 3200]\n",
      "loss: 0.017568  [  144/ 3200]\n",
      "loss: 0.049894  [  160/ 3200]\n",
      "loss: 0.244468  [  176/ 3200]\n",
      "loss: 0.045214  [  192/ 3200]\n",
      "loss: 0.146647  [  208/ 3200]\n",
      "loss: 0.049344  [  224/ 3200]\n",
      "loss: 0.080478  [  240/ 3200]\n",
      "loss: 0.089115  [  256/ 3200]\n",
      "loss: 0.019231  [  272/ 3200]\n",
      "loss: 0.070618  [  288/ 3200]\n",
      "loss: 0.060760  [  304/ 3200]\n",
      "loss: 0.042518  [  320/ 3200]\n",
      "loss: 0.013609  [  336/ 3200]\n",
      "loss: 0.057478  [  352/ 3200]\n",
      "loss: 0.308264  [  368/ 3200]\n",
      "loss: 0.053015  [  384/ 3200]\n",
      "loss: 0.055583  [  400/ 3200]\n",
      "loss: 0.182237  [  416/ 3200]\n",
      "loss: 0.138749  [  432/ 3200]\n",
      "loss: 0.085958  [  448/ 3200]\n",
      "loss: 0.044286  [  464/ 3200]\n",
      "loss: 0.046586  [  480/ 3200]\n",
      "loss: 0.031584  [  496/ 3200]\n",
      "loss: 0.027443  [  512/ 3200]\n",
      "loss: 0.022205  [  528/ 3200]\n",
      "loss: 0.023549  [  544/ 3200]\n",
      "loss: 0.058175  [  560/ 3200]\n",
      "loss: 0.065750  [  576/ 3200]\n",
      "loss: 0.108226  [  592/ 3200]\n",
      "loss: 0.195134  [  608/ 3200]\n",
      "loss: 0.025129  [  624/ 3200]\n",
      "loss: 0.016097  [  640/ 3200]\n",
      "loss: 0.084460  [  656/ 3200]\n",
      "loss: 0.078089  [  672/ 3200]\n",
      "loss: 0.041097  [  688/ 3200]\n",
      "loss: 0.124902  [  704/ 3200]\n",
      "loss: 0.119132  [  720/ 3200]\n",
      "loss: 0.129462  [  736/ 3200]\n",
      "loss: 0.093110  [  752/ 3200]\n",
      "loss: 0.015012  [  768/ 3200]\n",
      "loss: 0.030183  [  784/ 3200]\n",
      "loss: 0.018311  [  800/ 3200]\n",
      "loss: 0.145038  [  816/ 3200]\n",
      "loss: 0.189625  [  832/ 3200]\n",
      "loss: 0.184689  [  848/ 3200]\n",
      "loss: 0.118607  [  864/ 3200]\n",
      "loss: 0.026278  [  880/ 3200]\n",
      "loss: 0.169819  [  896/ 3200]\n",
      "loss: 0.112327  [  912/ 3200]\n",
      "loss: 0.102587  [  928/ 3200]\n",
      "loss: 0.072391  [  944/ 3200]\n",
      "loss: 0.080592  [  960/ 3200]\n",
      "loss: 0.032753  [  976/ 3200]\n",
      "loss: 0.101840  [  992/ 3200]\n",
      "loss: 0.060094  [ 1008/ 3200]\n",
      "loss: 0.037533  [ 1024/ 3200]\n",
      "loss: 0.033936  [ 1040/ 3200]\n",
      "loss: 0.025188  [ 1056/ 3200]\n",
      "loss: 0.017954  [ 1072/ 3200]\n",
      "loss: 0.068548  [ 1088/ 3200]\n",
      "loss: 0.079446  [ 1104/ 3200]\n",
      "loss: 0.012466  [ 1120/ 3200]\n",
      "loss: 0.118191  [ 1136/ 3200]\n",
      "loss: 0.038324  [ 1152/ 3200]\n",
      "loss: 0.062207  [ 1168/ 3200]\n",
      "loss: 0.122803  [ 1184/ 3200]\n",
      "loss: 0.061370  [ 1200/ 3200]\n",
      "loss: 0.213507  [ 1216/ 3200]\n",
      "loss: 0.131970  [ 1232/ 3200]\n",
      "loss: 0.221352  [ 1248/ 3200]\n",
      "loss: 0.073725  [ 1264/ 3200]\n",
      "loss: 0.065903  [ 1280/ 3200]\n",
      "loss: 0.045373  [ 1296/ 3200]\n",
      "loss: 0.086286  [ 1312/ 3200]\n",
      "loss: 0.065829  [ 1328/ 3200]\n",
      "loss: 0.080059  [ 1344/ 3200]\n",
      "loss: 0.057535  [ 1360/ 3200]\n",
      "loss: 0.090372  [ 1376/ 3200]\n",
      "loss: 0.266098  [ 1392/ 3200]\n",
      "loss: 0.043550  [ 1408/ 3200]\n",
      "loss: 0.065960  [ 1424/ 3200]\n",
      "loss: 0.036554  [ 1440/ 3200]\n",
      "loss: 0.069762  [ 1456/ 3200]\n",
      "loss: 0.107106  [ 1472/ 3200]\n",
      "loss: 0.169778  [ 1488/ 3200]\n",
      "loss: 0.147295  [ 1504/ 3200]\n",
      "loss: 0.083328  [ 1520/ 3200]\n",
      "loss: 0.053672  [ 1536/ 3200]\n",
      "loss: 0.410447  [ 1552/ 3200]\n",
      "loss: 0.027654  [ 1568/ 3200]\n",
      "loss: 0.048966  [ 1584/ 3200]\n",
      "loss: 0.020903  [ 1600/ 3200]\n",
      "loss: 0.129908  [ 1616/ 3200]\n",
      "loss: 0.030776  [ 1632/ 3200]\n",
      "loss: 0.044175  [ 1648/ 3200]\n",
      "loss: 0.059063  [ 1664/ 3200]\n",
      "loss: 0.019016  [ 1680/ 3200]\n",
      "loss: 0.141203  [ 1696/ 3200]\n",
      "loss: 0.026314  [ 1712/ 3200]\n",
      "loss: 0.029710  [ 1728/ 3200]\n",
      "loss: 0.093158  [ 1744/ 3200]\n",
      "loss: 0.059670  [ 1760/ 3200]\n",
      "loss: 0.035112  [ 1776/ 3200]\n",
      "loss: 0.095731  [ 1792/ 3200]\n",
      "loss: 0.123983  [ 1808/ 3200]\n",
      "loss: 0.036162  [ 1824/ 3200]\n",
      "loss: 0.035257  [ 1840/ 3200]\n",
      "loss: 0.075558  [ 1856/ 3200]\n",
      "loss: 0.041311  [ 1872/ 3200]\n",
      "loss: 0.080730  [ 1888/ 3200]\n",
      "loss: 0.038507  [ 1904/ 3200]\n",
      "loss: 0.041835  [ 1920/ 3200]\n",
      "loss: 0.088618  [ 1936/ 3200]\n",
      "loss: 0.045770  [ 1952/ 3200]\n",
      "loss: 0.068827  [ 1968/ 3200]\n",
      "loss: 0.031622  [ 1984/ 3200]\n",
      "loss: 0.083742  [ 2000/ 3200]\n",
      "loss: 0.020482  [ 2016/ 3200]\n",
      "loss: 0.079240  [ 2032/ 3200]\n",
      "loss: 0.048973  [ 2048/ 3200]\n",
      "loss: 0.366830  [ 2064/ 3200]\n",
      "loss: 0.031194  [ 2080/ 3200]\n",
      "loss: 0.027129  [ 2096/ 3200]\n",
      "loss: 0.122947  [ 2112/ 3200]\n",
      "loss: 0.261777  [ 2128/ 3200]\n",
      "loss: 0.062456  [ 2144/ 3200]\n",
      "loss: 0.047201  [ 2160/ 3200]\n",
      "loss: 0.078815  [ 2176/ 3200]\n",
      "loss: 0.023689  [ 2192/ 3200]\n",
      "loss: 0.022259  [ 2208/ 3200]\n",
      "loss: 0.082620  [ 2224/ 3200]\n",
      "loss: 0.023905  [ 2240/ 3200]\n",
      "loss: 0.134846  [ 2256/ 3200]\n",
      "loss: 0.065875  [ 2272/ 3200]\n",
      "loss: 0.025124  [ 2288/ 3200]\n",
      "loss: 0.009569  [ 2304/ 3200]\n",
      "loss: 0.009454  [ 2320/ 3200]\n",
      "loss: 0.038299  [ 2336/ 3200]\n",
      "loss: 0.016935  [ 2352/ 3200]\n",
      "loss: 0.076596  [ 2368/ 3200]\n",
      "loss: 0.007746  [ 2384/ 3200]\n",
      "loss: 0.106428  [ 2400/ 3200]\n",
      "loss: 0.056243  [ 2416/ 3200]\n",
      "loss: 0.063308  [ 2432/ 3200]\n",
      "loss: 0.070026  [ 2448/ 3200]\n",
      "loss: 0.037167  [ 2464/ 3200]\n",
      "loss: 0.022819  [ 2480/ 3200]\n",
      "loss: 0.033130  [ 2496/ 3200]\n",
      "loss: 0.022596  [ 2512/ 3200]\n",
      "loss: 0.019797  [ 2528/ 3200]\n",
      "loss: 0.044682  [ 2544/ 3200]\n",
      "loss: 0.029823  [ 2560/ 3200]\n",
      "loss: 0.052030  [ 2576/ 3200]\n",
      "loss: 0.033252  [ 2592/ 3200]\n",
      "loss: 0.045141  [ 2608/ 3200]\n",
      "loss: 0.044516  [ 2624/ 3200]\n",
      "loss: 0.089415  [ 2640/ 3200]\n",
      "loss: 0.142838  [ 2656/ 3200]\n",
      "loss: 0.052926  [ 2672/ 3200]\n",
      "loss: 0.047168  [ 2688/ 3200]\n",
      "loss: 0.169617  [ 2704/ 3200]\n",
      "loss: 0.028413  [ 2720/ 3200]\n",
      "loss: 0.072864  [ 2736/ 3200]\n",
      "loss: 0.109880  [ 2752/ 3200]\n",
      "loss: 0.131536  [ 2768/ 3200]\n",
      "loss: 0.040621  [ 2784/ 3200]\n",
      "loss: 0.090911  [ 2800/ 3200]\n",
      "loss: 0.018149  [ 2816/ 3200]\n",
      "loss: 0.021554  [ 2832/ 3200]\n",
      "loss: 0.089791  [ 2848/ 3200]\n",
      "loss: 0.029728  [ 2864/ 3200]\n",
      "loss: 0.096935  [ 2880/ 3200]\n",
      "loss: 0.015968  [ 2896/ 3200]\n",
      "loss: 0.026553  [ 2912/ 3200]\n",
      "loss: 0.091524  [ 2928/ 3200]\n",
      "loss: 0.079794  [ 2944/ 3200]\n",
      "loss: 0.426631  [ 2960/ 3200]\n",
      "loss: 0.021918  [ 2976/ 3200]\n",
      "loss: 0.014887  [ 2992/ 3200]\n",
      "loss: 0.035223  [ 3008/ 3200]\n",
      "loss: 0.043617  [ 3024/ 3200]\n",
      "loss: 0.157102  [ 3040/ 3200]\n",
      "loss: 0.120488  [ 3056/ 3200]\n",
      "loss: 0.019865  [ 3072/ 3200]\n",
      "loss: 0.543234  [ 3088/ 3200]\n",
      "loss: 0.251241  [ 3104/ 3200]\n",
      "loss: 0.089233  [ 3120/ 3200]\n",
      "loss: 0.091204  [ 3136/ 3200]\n",
      "loss: 0.025330  [ 3152/ 3200]\n",
      "loss: 0.179853  [ 3168/ 3200]\n",
      "loss: 0.083374  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 0.045975  [    0/ 3200]\n",
      "loss: 0.037675  [   16/ 3200]\n",
      "loss: 0.151916  [   32/ 3200]\n",
      "loss: 0.024625  [   48/ 3200]\n",
      "loss: 0.040769  [   64/ 3200]\n",
      "loss: 0.019729  [   80/ 3200]\n",
      "loss: 0.087637  [   96/ 3200]\n",
      "loss: 0.113423  [  112/ 3200]\n",
      "loss: 0.050346  [  128/ 3200]\n",
      "loss: 0.022839  [  144/ 3200]\n",
      "loss: 0.090698  [  160/ 3200]\n",
      "loss: 0.039791  [  176/ 3200]\n",
      "loss: 0.012061  [  192/ 3200]\n",
      "loss: 0.082432  [  208/ 3200]\n",
      "loss: 0.035895  [  224/ 3200]\n",
      "loss: 0.109476  [  240/ 3200]\n",
      "loss: 0.060343  [  256/ 3200]\n",
      "loss: 0.074168  [  272/ 3200]\n",
      "loss: 0.030066  [  288/ 3200]\n",
      "loss: 0.055978  [  304/ 3200]\n",
      "loss: 0.023454  [  320/ 3200]\n",
      "loss: 0.065890  [  336/ 3200]\n",
      "loss: 0.048818  [  352/ 3200]\n",
      "loss: 0.153140  [  368/ 3200]\n",
      "loss: 0.018521  [  384/ 3200]\n",
      "loss: 0.076913  [  400/ 3200]\n",
      "loss: 0.103900  [  416/ 3200]\n",
      "loss: 0.028937  [  432/ 3200]\n",
      "loss: 0.020647  [  448/ 3200]\n",
      "loss: 0.047495  [  464/ 3200]\n",
      "loss: 0.053899  [  480/ 3200]\n",
      "loss: 0.012582  [  496/ 3200]\n",
      "loss: 0.092963  [  512/ 3200]\n",
      "loss: 0.061337  [  528/ 3200]\n",
      "loss: 0.033619  [  544/ 3200]\n",
      "loss: 0.027785  [  560/ 3200]\n",
      "loss: 0.075282  [  576/ 3200]\n",
      "loss: 0.053199  [  592/ 3200]\n",
      "loss: 0.315454  [  608/ 3200]\n",
      "loss: 0.044228  [  624/ 3200]\n",
      "loss: 0.038143  [  640/ 3200]\n",
      "loss: 0.028690  [  656/ 3200]\n",
      "loss: 0.115444  [  672/ 3200]\n",
      "loss: 0.018507  [  688/ 3200]\n",
      "loss: 0.045969  [  704/ 3200]\n",
      "loss: 0.088219  [  720/ 3200]\n",
      "loss: 0.080745  [  736/ 3200]\n",
      "loss: 0.061722  [  752/ 3200]\n",
      "loss: 0.016456  [  768/ 3200]\n",
      "loss: 0.071514  [  784/ 3200]\n",
      "loss: 0.034592  [  800/ 3200]\n",
      "loss: 0.024762  [  816/ 3200]\n",
      "loss: 0.024709  [  832/ 3200]\n",
      "loss: 0.034373  [  848/ 3200]\n",
      "loss: 0.074197  [  864/ 3200]\n",
      "loss: 0.013816  [  880/ 3200]\n",
      "loss: 0.041160  [  896/ 3200]\n",
      "loss: 0.065358  [  912/ 3200]\n",
      "loss: 0.090576  [  928/ 3200]\n",
      "loss: 0.040314  [  944/ 3200]\n",
      "loss: 0.025664  [  960/ 3200]\n",
      "loss: 0.046119  [  976/ 3200]\n",
      "loss: 0.059052  [  992/ 3200]\n",
      "loss: 0.036395  [ 1008/ 3200]\n",
      "loss: 0.067240  [ 1024/ 3200]\n",
      "loss: 0.065430  [ 1040/ 3200]\n",
      "loss: 0.072952  [ 1056/ 3200]\n",
      "loss: 0.050415  [ 1072/ 3200]\n",
      "loss: 0.055885  [ 1088/ 3200]\n",
      "loss: 0.035342  [ 1104/ 3200]\n",
      "loss: 0.047435  [ 1120/ 3200]\n",
      "loss: 0.028853  [ 1136/ 3200]\n",
      "loss: 0.028433  [ 1152/ 3200]\n",
      "loss: 0.025175  [ 1168/ 3200]\n",
      "loss: 0.023787  [ 1184/ 3200]\n",
      "loss: 0.047463  [ 1200/ 3200]\n",
      "loss: 0.020774  [ 1216/ 3200]\n",
      "loss: 0.073715  [ 1232/ 3200]\n",
      "loss: 0.127401  [ 1248/ 3200]\n",
      "loss: 0.047201  [ 1264/ 3200]\n",
      "loss: 0.039870  [ 1280/ 3200]\n",
      "loss: 0.022313  [ 1296/ 3200]\n",
      "loss: 0.015104  [ 1312/ 3200]\n",
      "loss: 0.043103  [ 1328/ 3200]\n",
      "loss: 0.040680  [ 1344/ 3200]\n",
      "loss: 0.013188  [ 1360/ 3200]\n",
      "loss: 0.037931  [ 1376/ 3200]\n",
      "loss: 0.040635  [ 1392/ 3200]\n",
      "loss: 0.022380  [ 1408/ 3200]\n",
      "loss: 0.046597  [ 1424/ 3200]\n",
      "loss: 0.038769  [ 1440/ 3200]\n",
      "loss: 0.046515  [ 1456/ 3200]\n",
      "loss: 0.054600  [ 1472/ 3200]\n",
      "loss: 0.058352  [ 1488/ 3200]\n",
      "loss: 0.040475  [ 1504/ 3200]\n",
      "loss: 0.043226  [ 1520/ 3200]\n",
      "loss: 0.085970  [ 1536/ 3200]\n",
      "loss: 0.056404  [ 1552/ 3200]\n",
      "loss: 0.011411  [ 1568/ 3200]\n",
      "loss: 0.059234  [ 1584/ 3200]\n",
      "loss: 0.064217  [ 1600/ 3200]\n",
      "loss: 0.047876  [ 1616/ 3200]\n",
      "loss: 0.010096  [ 1632/ 3200]\n",
      "loss: 0.051251  [ 1648/ 3200]\n",
      "loss: 0.024069  [ 1664/ 3200]\n",
      "loss: 0.032503  [ 1680/ 3200]\n",
      "loss: 0.030485  [ 1696/ 3200]\n",
      "loss: 0.056321  [ 1712/ 3200]\n",
      "loss: 0.011293  [ 1728/ 3200]\n",
      "loss: 0.038301  [ 1744/ 3200]\n",
      "loss: 0.301323  [ 1760/ 3200]\n",
      "loss: 0.061287  [ 1776/ 3200]\n",
      "loss: 0.013527  [ 1792/ 3200]\n",
      "loss: 0.031599  [ 1808/ 3200]\n",
      "loss: 0.019226  [ 1824/ 3200]\n",
      "loss: 0.015356  [ 1840/ 3200]\n",
      "loss: 0.016225  [ 1856/ 3200]\n",
      "loss: 0.006246  [ 1872/ 3200]\n",
      "loss: 0.033919  [ 1888/ 3200]\n",
      "loss: 0.013447  [ 1904/ 3200]\n",
      "loss: 0.062242  [ 1920/ 3200]\n",
      "loss: 0.026076  [ 1936/ 3200]\n",
      "loss: 0.028998  [ 1952/ 3200]\n",
      "loss: 0.026200  [ 1968/ 3200]\n",
      "loss: 0.091784  [ 1984/ 3200]\n",
      "loss: 0.113929  [ 2000/ 3200]\n",
      "loss: 0.047074  [ 2016/ 3200]\n",
      "loss: 0.153972  [ 2032/ 3200]\n",
      "loss: 0.022250  [ 2048/ 3200]\n",
      "loss: 0.033186  [ 2064/ 3200]\n",
      "loss: 0.029450  [ 2080/ 3200]\n",
      "loss: 0.019662  [ 2096/ 3200]\n",
      "loss: 0.014478  [ 2112/ 3200]\n",
      "loss: 0.052497  [ 2128/ 3200]\n",
      "loss: 0.018705  [ 2144/ 3200]\n",
      "loss: 0.019878  [ 2160/ 3200]\n",
      "loss: 0.049116  [ 2176/ 3200]\n",
      "loss: 0.015509  [ 2192/ 3200]\n",
      "loss: 0.082295  [ 2208/ 3200]\n",
      "loss: 0.025423  [ 2224/ 3200]\n",
      "loss: 0.021438  [ 2240/ 3200]\n",
      "loss: 0.011819  [ 2256/ 3200]\n",
      "loss: 0.030056  [ 2272/ 3200]\n",
      "loss: 0.026216  [ 2288/ 3200]\n",
      "loss: 0.053641  [ 2304/ 3200]\n",
      "loss: 0.007802  [ 2320/ 3200]\n",
      "loss: 0.065625  [ 2336/ 3200]\n",
      "loss: 0.096965  [ 2352/ 3200]\n",
      "loss: 0.020624  [ 2368/ 3200]\n",
      "loss: 0.053749  [ 2384/ 3200]\n",
      "loss: 0.020511  [ 2400/ 3200]\n",
      "loss: 0.037808  [ 2416/ 3200]\n",
      "loss: 0.014216  [ 2432/ 3200]\n",
      "loss: 0.075995  [ 2448/ 3200]\n",
      "loss: 0.014754  [ 2464/ 3200]\n",
      "loss: 0.052056  [ 2480/ 3200]\n",
      "loss: 0.011227  [ 2496/ 3200]\n",
      "loss: 0.384519  [ 2512/ 3200]\n",
      "loss: 0.106538  [ 2528/ 3200]\n",
      "loss: 0.275003  [ 2544/ 3200]\n",
      "loss: 0.062377  [ 2560/ 3200]\n",
      "loss: 0.066213  [ 2576/ 3200]\n",
      "loss: 0.079694  [ 2592/ 3200]\n",
      "loss: 0.142051  [ 2608/ 3200]\n",
      "loss: 0.029152  [ 2624/ 3200]\n",
      "loss: 0.023902  [ 2640/ 3200]\n",
      "loss: 0.011778  [ 2656/ 3200]\n",
      "loss: 0.042075  [ 2672/ 3200]\n",
      "loss: 0.056560  [ 2688/ 3200]\n",
      "loss: 0.030229  [ 2704/ 3200]\n",
      "loss: 0.021426  [ 2720/ 3200]\n",
      "loss: 0.434699  [ 2736/ 3200]\n",
      "loss: 0.099030  [ 2752/ 3200]\n",
      "loss: 0.049169  [ 2768/ 3200]\n",
      "loss: 0.126214  [ 2784/ 3200]\n",
      "loss: 0.014564  [ 2800/ 3200]\n",
      "loss: 0.039588  [ 2816/ 3200]\n",
      "loss: 0.049373  [ 2832/ 3200]\n",
      "loss: 0.111663  [ 2848/ 3200]\n",
      "loss: 0.050209  [ 2864/ 3200]\n",
      "loss: 0.117372  [ 2880/ 3200]\n",
      "loss: 0.175323  [ 2896/ 3200]\n",
      "loss: 0.156144  [ 2912/ 3200]\n",
      "loss: 0.014060  [ 2928/ 3200]\n",
      "loss: 0.047980  [ 2944/ 3200]\n",
      "loss: 0.056015  [ 2960/ 3200]\n",
      "loss: 0.045952  [ 2976/ 3200]\n",
      "loss: 0.083070  [ 2992/ 3200]\n",
      "loss: 0.109142  [ 3008/ 3200]\n",
      "loss: 0.067667  [ 3024/ 3200]\n",
      "loss: 0.013567  [ 3040/ 3200]\n",
      "loss: 0.058219  [ 3056/ 3200]\n",
      "loss: 0.068183  [ 3072/ 3200]\n",
      "loss: 0.044545  [ 3088/ 3200]\n",
      "loss: 0.034987  [ 3104/ 3200]\n",
      "loss: 0.056437  [ 3120/ 3200]\n",
      "loss: 0.030274  [ 3136/ 3200]\n",
      "loss: 0.376952  [ 3152/ 3200]\n",
      "loss: 0.038438  [ 3168/ 3200]\n",
      "loss: 0.030489  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 0.050860  [    0/ 3200]\n",
      "loss: 0.007760  [   16/ 3200]\n",
      "loss: 0.014017  [   32/ 3200]\n",
      "loss: 0.025101  [   48/ 3200]\n",
      "loss: 0.012268  [   64/ 3200]\n",
      "loss: 0.064723  [   80/ 3200]\n",
      "loss: 0.068863  [   96/ 3200]\n",
      "loss: 0.026714  [  112/ 3200]\n",
      "loss: 0.049001  [  128/ 3200]\n",
      "loss: 0.011750  [  144/ 3200]\n",
      "loss: 0.028587  [  160/ 3200]\n",
      "loss: 0.033359  [  176/ 3200]\n",
      "loss: 0.009271  [  192/ 3200]\n",
      "loss: 0.034200  [  208/ 3200]\n",
      "loss: 0.023551  [  224/ 3200]\n",
      "loss: 0.011095  [  240/ 3200]\n",
      "loss: 0.025381  [  256/ 3200]\n",
      "loss: 0.045974  [  272/ 3200]\n",
      "loss: 0.012518  [  288/ 3200]\n",
      "loss: 0.017615  [  304/ 3200]\n",
      "loss: 0.047225  [  320/ 3200]\n",
      "loss: 0.087243  [  336/ 3200]\n",
      "loss: 0.048219  [  352/ 3200]\n",
      "loss: 0.036689  [  368/ 3200]\n",
      "loss: 0.034199  [  384/ 3200]\n",
      "loss: 0.033224  [  400/ 3200]\n",
      "loss: 0.007860  [  416/ 3200]\n",
      "loss: 0.027102  [  432/ 3200]\n",
      "loss: 0.040968  [  448/ 3200]\n",
      "loss: 0.031393  [  464/ 3200]\n",
      "loss: 0.047053  [  480/ 3200]\n",
      "loss: 0.030171  [  496/ 3200]\n",
      "loss: 0.007396  [  512/ 3200]\n",
      "loss: 0.009294  [  528/ 3200]\n",
      "loss: 0.015702  [  544/ 3200]\n",
      "loss: 0.078106  [  560/ 3200]\n",
      "loss: 0.025530  [  576/ 3200]\n",
      "loss: 0.010257  [  592/ 3200]\n",
      "loss: 0.021717  [  608/ 3200]\n",
      "loss: 0.025007  [  624/ 3200]\n",
      "loss: 0.023877  [  640/ 3200]\n",
      "loss: 0.037101  [  656/ 3200]\n",
      "loss: 0.037417  [  672/ 3200]\n",
      "loss: 0.029632  [  688/ 3200]\n",
      "loss: 0.013036  [  704/ 3200]\n",
      "loss: 0.014350  [  720/ 3200]\n",
      "loss: 0.044596  [  736/ 3200]\n",
      "loss: 0.022466  [  752/ 3200]\n",
      "loss: 0.039009  [  768/ 3200]\n",
      "loss: 0.034033  [  784/ 3200]\n",
      "loss: 0.045447  [  800/ 3200]\n",
      "loss: 0.067335  [  816/ 3200]\n",
      "loss: 0.031478  [  832/ 3200]\n",
      "loss: 0.026457  [  848/ 3200]\n",
      "loss: 0.021339  [  864/ 3200]\n",
      "loss: 0.020200  [  880/ 3200]\n",
      "loss: 0.035120  [  896/ 3200]\n",
      "loss: 0.022582  [  912/ 3200]\n",
      "loss: 0.019776  [  928/ 3200]\n",
      "loss: 0.011207  [  944/ 3200]\n",
      "loss: 0.024446  [  960/ 3200]\n",
      "loss: 0.014736  [  976/ 3200]\n",
      "loss: 0.013277  [  992/ 3200]\n",
      "loss: 0.056129  [ 1008/ 3200]\n",
      "loss: 0.029204  [ 1024/ 3200]\n",
      "loss: 0.021205  [ 1040/ 3200]\n",
      "loss: 0.020003  [ 1056/ 3200]\n",
      "loss: 0.072591  [ 1072/ 3200]\n",
      "loss: 0.025179  [ 1088/ 3200]\n",
      "loss: 0.040544  [ 1104/ 3200]\n",
      "loss: 0.043031  [ 1120/ 3200]\n",
      "loss: 0.040811  [ 1136/ 3200]\n",
      "loss: 0.076689  [ 1152/ 3200]\n",
      "loss: 0.005192  [ 1168/ 3200]\n",
      "loss: 0.008445  [ 1184/ 3200]\n",
      "loss: 0.362156  [ 1200/ 3200]\n",
      "loss: 0.032748  [ 1216/ 3200]\n",
      "loss: 0.028795  [ 1232/ 3200]\n",
      "loss: 0.038164  [ 1248/ 3200]\n",
      "loss: 0.113467  [ 1264/ 3200]\n",
      "loss: 0.060194  [ 1280/ 3200]\n",
      "loss: 0.021914  [ 1296/ 3200]\n",
      "loss: 0.020399  [ 1312/ 3200]\n",
      "loss: 0.024309  [ 1328/ 3200]\n",
      "loss: 0.044225  [ 1344/ 3200]\n",
      "loss: 0.026332  [ 1360/ 3200]\n",
      "loss: 0.049352  [ 1376/ 3200]\n",
      "loss: 0.040665  [ 1392/ 3200]\n",
      "loss: 0.029536  [ 1408/ 3200]\n",
      "loss: 0.030286  [ 1424/ 3200]\n",
      "loss: 0.071194  [ 1440/ 3200]\n",
      "loss: 0.034045  [ 1456/ 3200]\n",
      "loss: 0.049259  [ 1472/ 3200]\n",
      "loss: 0.014246  [ 1488/ 3200]\n",
      "loss: 0.016043  [ 1504/ 3200]\n",
      "loss: 0.018508  [ 1520/ 3200]\n",
      "loss: 0.041212  [ 1536/ 3200]\n",
      "loss: 0.011433  [ 1552/ 3200]\n",
      "loss: 0.019110  [ 1568/ 3200]\n",
      "loss: 0.035591  [ 1584/ 3200]\n",
      "loss: 0.013741  [ 1600/ 3200]\n",
      "loss: 0.031962  [ 1616/ 3200]\n",
      "loss: 0.029932  [ 1632/ 3200]\n",
      "loss: 0.010829  [ 1648/ 3200]\n",
      "loss: 0.049822  [ 1664/ 3200]\n",
      "loss: 0.043250  [ 1680/ 3200]\n",
      "loss: 0.020041  [ 1696/ 3200]\n",
      "loss: 0.018599  [ 1712/ 3200]\n",
      "loss: 0.035802  [ 1728/ 3200]\n",
      "loss: 0.038906  [ 1744/ 3200]\n",
      "loss: 0.020181  [ 1760/ 3200]\n",
      "loss: 0.016552  [ 1776/ 3200]\n",
      "loss: 0.013149  [ 1792/ 3200]\n",
      "loss: 0.020258  [ 1808/ 3200]\n",
      "loss: 0.066542  [ 1824/ 3200]\n",
      "loss: 0.032754  [ 1840/ 3200]\n",
      "loss: 0.017707  [ 1856/ 3200]\n",
      "loss: 0.059536  [ 1872/ 3200]\n",
      "loss: 0.030638  [ 1888/ 3200]\n",
      "loss: 0.018947  [ 1904/ 3200]\n",
      "loss: 0.008266  [ 1920/ 3200]\n",
      "loss: 0.129788  [ 1936/ 3200]\n",
      "loss: 0.098036  [ 1952/ 3200]\n",
      "loss: 0.023183  [ 1968/ 3200]\n",
      "loss: 0.008437  [ 1984/ 3200]\n",
      "loss: 0.073792  [ 2000/ 3200]\n",
      "loss: 0.034207  [ 2016/ 3200]\n",
      "loss: 0.023884  [ 2032/ 3200]\n",
      "loss: 0.032291  [ 2048/ 3200]\n",
      "loss: 0.043048  [ 2064/ 3200]\n",
      "loss: 0.012960  [ 2080/ 3200]\n",
      "loss: 0.042133  [ 2096/ 3200]\n",
      "loss: 0.015754  [ 2112/ 3200]\n",
      "loss: 0.006897  [ 2128/ 3200]\n",
      "loss: 0.031321  [ 2144/ 3200]\n",
      "loss: 0.024066  [ 2160/ 3200]\n",
      "loss: 0.060021  [ 2176/ 3200]\n",
      "loss: 0.032764  [ 2192/ 3200]\n",
      "loss: 0.013731  [ 2208/ 3200]\n",
      "loss: 0.014514  [ 2224/ 3200]\n",
      "loss: 0.017944  [ 2240/ 3200]\n",
      "loss: 0.010561  [ 2256/ 3200]\n",
      "loss: 0.020791  [ 2272/ 3200]\n",
      "loss: 0.021815  [ 2288/ 3200]\n",
      "loss: 0.043790  [ 2304/ 3200]\n",
      "loss: 0.012571  [ 2320/ 3200]\n",
      "loss: 0.036736  [ 2336/ 3200]\n",
      "loss: 0.015077  [ 2352/ 3200]\n",
      "loss: 0.066007  [ 2368/ 3200]\n",
      "loss: 0.437398  [ 2384/ 3200]\n",
      "loss: 0.079024  [ 2400/ 3200]\n",
      "loss: 0.020199  [ 2416/ 3200]\n",
      "loss: 0.009953  [ 2432/ 3200]\n",
      "loss: 0.198547  [ 2448/ 3200]\n",
      "loss: 0.018242  [ 2464/ 3200]\n",
      "loss: 0.352430  [ 2480/ 3200]\n",
      "loss: 0.023367  [ 2496/ 3200]\n",
      "loss: 0.008254  [ 2512/ 3200]\n",
      "loss: 0.004678  [ 2528/ 3200]\n",
      "loss: 0.010699  [ 2544/ 3200]\n",
      "loss: 0.048310  [ 2560/ 3200]\n",
      "loss: 0.008128  [ 2576/ 3200]\n",
      "loss: 0.005311  [ 2592/ 3200]\n",
      "loss: 0.052344  [ 2608/ 3200]\n",
      "loss: 0.034487  [ 2624/ 3200]\n",
      "loss: 0.040896  [ 2640/ 3200]\n",
      "loss: 0.031813  [ 2656/ 3200]\n",
      "loss: 0.022626  [ 2672/ 3200]\n",
      "loss: 0.010839  [ 2688/ 3200]\n",
      "loss: 0.036380  [ 2704/ 3200]\n",
      "loss: 0.057873  [ 2720/ 3200]\n",
      "loss: 0.045127  [ 2736/ 3200]\n",
      "loss: 0.036943  [ 2752/ 3200]\n",
      "loss: 0.018039  [ 2768/ 3200]\n",
      "loss: 0.046533  [ 2784/ 3200]\n",
      "loss: 0.054639  [ 2800/ 3200]\n",
      "loss: 0.016707  [ 2816/ 3200]\n",
      "loss: 0.097202  [ 2832/ 3200]\n",
      "loss: 0.016452  [ 2848/ 3200]\n",
      "loss: 0.009416  [ 2864/ 3200]\n",
      "loss: 0.037490  [ 2880/ 3200]\n",
      "loss: 0.009296  [ 2896/ 3200]\n",
      "loss: 0.039840  [ 2912/ 3200]\n",
      "loss: 0.332592  [ 2928/ 3200]\n",
      "loss: 0.014185  [ 2944/ 3200]\n",
      "loss: 0.030603  [ 2960/ 3200]\n",
      "loss: 0.037749  [ 2976/ 3200]\n",
      "loss: 0.051918  [ 2992/ 3200]\n",
      "loss: 0.025555  [ 3008/ 3200]\n",
      "loss: 0.027448  [ 3024/ 3200]\n",
      "loss: 0.012359  [ 3040/ 3200]\n",
      "loss: 0.042386  [ 3056/ 3200]\n",
      "loss: 0.084769  [ 3072/ 3200]\n",
      "loss: 0.051552  [ 3088/ 3200]\n",
      "loss: 0.009134  [ 3104/ 3200]\n",
      "loss: 0.025865  [ 3120/ 3200]\n",
      "loss: 0.026164  [ 3136/ 3200]\n",
      "loss: 0.020209  [ 3152/ 3200]\n",
      "loss: 0.320800  [ 3168/ 3200]\n",
      "loss: 0.083804  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 0.026698  [    0/ 3200]\n",
      "loss: 0.067529  [   16/ 3200]\n",
      "loss: 0.011121  [   32/ 3200]\n",
      "loss: 0.009590  [   48/ 3200]\n",
      "loss: 0.005325  [   64/ 3200]\n",
      "loss: 0.044535  [   80/ 3200]\n",
      "loss: 0.036508  [   96/ 3200]\n",
      "loss: 0.015621  [  112/ 3200]\n",
      "loss: 0.014402  [  128/ 3200]\n",
      "loss: 0.044267  [  144/ 3200]\n",
      "loss: 0.039591  [  160/ 3200]\n",
      "loss: 0.276368  [  176/ 3200]\n",
      "loss: 0.033170  [  192/ 3200]\n",
      "loss: 0.080420  [  208/ 3200]\n",
      "loss: 0.026935  [  224/ 3200]\n",
      "loss: 0.027706  [  240/ 3200]\n",
      "loss: 0.043972  [  256/ 3200]\n",
      "loss: 0.054220  [  272/ 3200]\n",
      "loss: 0.049961  [  288/ 3200]\n",
      "loss: 0.017926  [  304/ 3200]\n",
      "loss: 0.012406  [  320/ 3200]\n",
      "loss: 0.019655  [  336/ 3200]\n",
      "loss: 0.023097  [  352/ 3200]\n",
      "loss: 0.025643  [  368/ 3200]\n",
      "loss: 0.029318  [  384/ 3200]\n",
      "loss: 0.010269  [  400/ 3200]\n",
      "loss: 0.032718  [  416/ 3200]\n",
      "loss: 0.039086  [  432/ 3200]\n",
      "loss: 0.014520  [  448/ 3200]\n",
      "loss: 0.067270  [  464/ 3200]\n",
      "loss: 0.021739  [  480/ 3200]\n",
      "loss: 0.039424  [  496/ 3200]\n",
      "loss: 0.051803  [  512/ 3200]\n",
      "loss: 0.019505  [  528/ 3200]\n",
      "loss: 0.016081  [  544/ 3200]\n",
      "loss: 0.050749  [  560/ 3200]\n",
      "loss: 0.014416  [  576/ 3200]\n",
      "loss: 0.030766  [  592/ 3200]\n",
      "loss: 0.012710  [  608/ 3200]\n",
      "loss: 0.007017  [  624/ 3200]\n",
      "loss: 0.021656  [  640/ 3200]\n",
      "loss: 0.014854  [  656/ 3200]\n",
      "loss: 0.006789  [  672/ 3200]\n",
      "loss: 0.021989  [  688/ 3200]\n",
      "loss: 0.043225  [  704/ 3200]\n",
      "loss: 0.027276  [  720/ 3200]\n",
      "loss: 0.031012  [  736/ 3200]\n",
      "loss: 0.019748  [  752/ 3200]\n",
      "loss: 0.015646  [  768/ 3200]\n",
      "loss: 0.010620  [  784/ 3200]\n",
      "loss: 0.052725  [  800/ 3200]\n",
      "loss: 0.090665  [  816/ 3200]\n",
      "loss: 0.049182  [  832/ 3200]\n",
      "loss: 0.017456  [  848/ 3200]\n",
      "loss: 0.043946  [  864/ 3200]\n",
      "loss: 0.017589  [  880/ 3200]\n",
      "loss: 0.025887  [  896/ 3200]\n",
      "loss: 0.008989  [  912/ 3200]\n",
      "loss: 0.008435  [  928/ 3200]\n",
      "loss: 0.013013  [  944/ 3200]\n",
      "loss: 0.014191  [  960/ 3200]\n",
      "loss: 0.009531  [  976/ 3200]\n",
      "loss: 0.040591  [  992/ 3200]\n",
      "loss: 0.042080  [ 1008/ 3200]\n",
      "loss: 0.008070  [ 1024/ 3200]\n",
      "loss: 0.025496  [ 1040/ 3200]\n",
      "loss: 0.032303  [ 1056/ 3200]\n",
      "loss: 0.031677  [ 1072/ 3200]\n",
      "loss: 0.005268  [ 1088/ 3200]\n",
      "loss: 0.012676  [ 1104/ 3200]\n",
      "loss: 0.022220  [ 1120/ 3200]\n",
      "loss: 0.020094  [ 1136/ 3200]\n",
      "loss: 0.014361  [ 1152/ 3200]\n",
      "loss: 0.015741  [ 1168/ 3200]\n",
      "loss: 0.022263  [ 1184/ 3200]\n",
      "loss: 0.009272  [ 1200/ 3200]\n",
      "loss: 0.017069  [ 1216/ 3200]\n",
      "loss: 0.011645  [ 1232/ 3200]\n",
      "loss: 0.021363  [ 1248/ 3200]\n",
      "loss: 0.028339  [ 1264/ 3200]\n",
      "loss: 0.013000  [ 1280/ 3200]\n",
      "loss: 0.013339  [ 1296/ 3200]\n",
      "loss: 0.016285  [ 1312/ 3200]\n",
      "loss: 0.032433  [ 1328/ 3200]\n",
      "loss: 0.045747  [ 1344/ 3200]\n",
      "loss: 0.088875  [ 1360/ 3200]\n",
      "loss: 0.013817  [ 1376/ 3200]\n",
      "loss: 0.008362  [ 1392/ 3200]\n",
      "loss: 0.046964  [ 1408/ 3200]\n",
      "loss: 0.030367  [ 1424/ 3200]\n",
      "loss: 0.050103  [ 1440/ 3200]\n",
      "loss: 0.314213  [ 1456/ 3200]\n",
      "loss: 0.023919  [ 1472/ 3200]\n",
      "loss: 0.056263  [ 1488/ 3200]\n",
      "loss: 0.072656  [ 1504/ 3200]\n",
      "loss: 0.160765  [ 1520/ 3200]\n",
      "loss: 0.051285  [ 1536/ 3200]\n",
      "loss: 0.003264  [ 1552/ 3200]\n",
      "loss: 0.119795  [ 1568/ 3200]\n",
      "loss: 0.147771  [ 1584/ 3200]\n",
      "loss: 0.015453  [ 1600/ 3200]\n",
      "loss: 0.020271  [ 1616/ 3200]\n",
      "loss: 0.007556  [ 1632/ 3200]\n",
      "loss: 0.017552  [ 1648/ 3200]\n",
      "loss: 0.015042  [ 1664/ 3200]\n",
      "loss: 0.029886  [ 1680/ 3200]\n",
      "loss: 0.039508  [ 1696/ 3200]\n",
      "loss: 0.012710  [ 1712/ 3200]\n",
      "loss: 0.008030  [ 1728/ 3200]\n",
      "loss: 0.006526  [ 1744/ 3200]\n",
      "loss: 0.010885  [ 1760/ 3200]\n",
      "loss: 0.024210  [ 1776/ 3200]\n",
      "loss: 0.014418  [ 1792/ 3200]\n",
      "loss: 0.027812  [ 1808/ 3200]\n",
      "loss: 0.085443  [ 1824/ 3200]\n",
      "loss: 0.036263  [ 1840/ 3200]\n",
      "loss: 0.024685  [ 1856/ 3200]\n",
      "loss: 0.014820  [ 1872/ 3200]\n",
      "loss: 0.010637  [ 1888/ 3200]\n",
      "loss: 0.010811  [ 1904/ 3200]\n",
      "loss: 0.016310  [ 1920/ 3200]\n",
      "loss: 0.022307  [ 1936/ 3200]\n",
      "loss: 0.066225  [ 1952/ 3200]\n",
      "loss: 0.026842  [ 1968/ 3200]\n",
      "loss: 0.028143  [ 1984/ 3200]\n",
      "loss: 0.026250  [ 2000/ 3200]\n",
      "loss: 0.023103  [ 2016/ 3200]\n",
      "loss: 0.022314  [ 2032/ 3200]\n",
      "loss: 0.021967  [ 2048/ 3200]\n",
      "loss: 0.017608  [ 2064/ 3200]\n",
      "loss: 0.049132  [ 2080/ 3200]\n",
      "loss: 0.045027  [ 2096/ 3200]\n",
      "loss: 0.028894  [ 2112/ 3200]\n",
      "loss: 0.006250  [ 2128/ 3200]\n",
      "loss: 0.033770  [ 2144/ 3200]\n",
      "loss: 0.021141  [ 2160/ 3200]\n",
      "loss: 0.003563  [ 2176/ 3200]\n",
      "loss: 0.034410  [ 2192/ 3200]\n",
      "loss: 0.035474  [ 2208/ 3200]\n",
      "loss: 0.013263  [ 2224/ 3200]\n",
      "loss: 0.028335  [ 2240/ 3200]\n",
      "loss: 0.007514  [ 2256/ 3200]\n",
      "loss: 0.018610  [ 2272/ 3200]\n",
      "loss: 0.032202  [ 2288/ 3200]\n",
      "loss: 0.010115  [ 2304/ 3200]\n",
      "loss: 0.009662  [ 2320/ 3200]\n",
      "loss: 0.068095  [ 2336/ 3200]\n",
      "loss: 0.039172  [ 2352/ 3200]\n",
      "loss: 0.049877  [ 2368/ 3200]\n",
      "loss: 0.037119  [ 2384/ 3200]\n",
      "loss: 0.060504  [ 2400/ 3200]\n",
      "loss: 0.022827  [ 2416/ 3200]\n",
      "loss: 0.025649  [ 2432/ 3200]\n",
      "loss: 0.018320  [ 2448/ 3200]\n",
      "loss: 0.046319  [ 2464/ 3200]\n",
      "loss: 0.021260  [ 2480/ 3200]\n",
      "loss: 0.041718  [ 2496/ 3200]\n",
      "loss: 0.033390  [ 2512/ 3200]\n",
      "loss: 0.037869  [ 2528/ 3200]\n",
      "loss: 0.017246  [ 2544/ 3200]\n",
      "loss: 0.015011  [ 2560/ 3200]\n",
      "loss: 0.023296  [ 2576/ 3200]\n",
      "loss: 0.137838  [ 2592/ 3200]\n",
      "loss: 0.013834  [ 2608/ 3200]\n",
      "loss: 0.016988  [ 2624/ 3200]\n",
      "loss: 0.014193  [ 2640/ 3200]\n",
      "loss: 0.026968  [ 2656/ 3200]\n",
      "loss: 0.049423  [ 2672/ 3200]\n",
      "loss: 0.018825  [ 2688/ 3200]\n",
      "loss: 0.012425  [ 2704/ 3200]\n",
      "loss: 0.039147  [ 2720/ 3200]\n",
      "loss: 0.017419  [ 2736/ 3200]\n",
      "loss: 0.014904  [ 2752/ 3200]\n",
      "loss: 0.309078  [ 2768/ 3200]\n",
      "loss: 0.018080  [ 2784/ 3200]\n",
      "loss: 0.060737  [ 2800/ 3200]\n",
      "loss: 0.059329  [ 2816/ 3200]\n",
      "loss: 0.023379  [ 2832/ 3200]\n",
      "loss: 0.004768  [ 2848/ 3200]\n",
      "loss: 0.198530  [ 2864/ 3200]\n",
      "loss: 0.175243  [ 2880/ 3200]\n",
      "loss: 0.028519  [ 2896/ 3200]\n",
      "loss: 0.023179  [ 2912/ 3200]\n",
      "loss: 0.011328  [ 2928/ 3200]\n",
      "loss: 0.026634  [ 2944/ 3200]\n",
      "loss: 0.005970  [ 2960/ 3200]\n",
      "loss: 0.035704  [ 2976/ 3200]\n",
      "loss: 0.013964  [ 2992/ 3200]\n",
      "loss: 0.038920  [ 3008/ 3200]\n",
      "loss: 0.007774  [ 3024/ 3200]\n",
      "loss: 0.014413  [ 3040/ 3200]\n",
      "loss: 0.023761  [ 3056/ 3200]\n",
      "loss: 0.021178  [ 3072/ 3200]\n",
      "loss: 0.030902  [ 3088/ 3200]\n",
      "loss: 0.020942  [ 3104/ 3200]\n",
      "loss: 0.012602  [ 3120/ 3200]\n",
      "loss: 0.024252  [ 3136/ 3200]\n",
      "loss: 0.005717  [ 3152/ 3200]\n",
      "loss: 0.008647  [ 3168/ 3200]\n",
      "loss: 0.007298  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.026757  [    0/ 3200]\n",
      "loss: 0.020452  [   16/ 3200]\n",
      "loss: 0.003358  [   32/ 3200]\n",
      "loss: 0.034880  [   48/ 3200]\n",
      "loss: 0.013446  [   64/ 3200]\n",
      "loss: 0.011478  [   80/ 3200]\n",
      "loss: 0.009803  [   96/ 3200]\n",
      "loss: 0.009005  [  112/ 3200]\n",
      "loss: 0.013645  [  128/ 3200]\n",
      "loss: 0.024507  [  144/ 3200]\n",
      "loss: 0.018124  [  160/ 3200]\n",
      "loss: 0.017395  [  176/ 3200]\n",
      "loss: 0.024940  [  192/ 3200]\n",
      "loss: 0.006726  [  208/ 3200]\n",
      "loss: 0.018592  [  224/ 3200]\n",
      "loss: 0.026756  [  240/ 3200]\n",
      "loss: 0.028590  [  256/ 3200]\n",
      "loss: 0.036898  [  272/ 3200]\n",
      "loss: 0.030507  [  288/ 3200]\n",
      "loss: 0.045320  [  304/ 3200]\n",
      "loss: 0.042509  [  320/ 3200]\n",
      "loss: 0.031594  [  336/ 3200]\n",
      "loss: 0.011217  [  352/ 3200]\n",
      "loss: 0.006119  [  368/ 3200]\n",
      "loss: 0.019382  [  384/ 3200]\n",
      "loss: 0.006119  [  400/ 3200]\n",
      "loss: 0.012252  [  416/ 3200]\n",
      "loss: 0.037417  [  432/ 3200]\n",
      "loss: 0.007549  [  448/ 3200]\n",
      "loss: 0.022604  [  464/ 3200]\n",
      "loss: 0.016661  [  480/ 3200]\n",
      "loss: 0.012953  [  496/ 3200]\n",
      "loss: 0.021226  [  512/ 3200]\n",
      "loss: 0.012547  [  528/ 3200]\n",
      "loss: 0.011394  [  544/ 3200]\n",
      "loss: 0.019742  [  560/ 3200]\n",
      "loss: 0.027928  [  576/ 3200]\n",
      "loss: 0.016681  [  592/ 3200]\n",
      "loss: 0.016997  [  608/ 3200]\n",
      "loss: 0.004161  [  624/ 3200]\n",
      "loss: 0.010425  [  640/ 3200]\n",
      "loss: 0.015941  [  656/ 3200]\n",
      "loss: 0.015709  [  672/ 3200]\n",
      "loss: 0.018561  [  688/ 3200]\n",
      "loss: 0.012121  [  704/ 3200]\n",
      "loss: 0.021307  [  720/ 3200]\n",
      "loss: 0.016680  [  736/ 3200]\n",
      "loss: 0.023302  [  752/ 3200]\n",
      "loss: 0.015080  [  768/ 3200]\n",
      "loss: 0.062057  [  784/ 3200]\n",
      "loss: 0.008438  [  800/ 3200]\n",
      "loss: 0.025982  [  816/ 3200]\n",
      "loss: 0.006101  [  832/ 3200]\n",
      "loss: 0.017644  [  848/ 3200]\n",
      "loss: 0.016322  [  864/ 3200]\n",
      "loss: 0.048342  [  880/ 3200]\n",
      "loss: 0.014089  [  896/ 3200]\n",
      "loss: 0.003686  [  912/ 3200]\n",
      "loss: 0.027877  [  928/ 3200]\n",
      "loss: 0.062158  [  944/ 3200]\n",
      "loss: 0.165921  [  960/ 3200]\n",
      "loss: 0.035250  [  976/ 3200]\n",
      "loss: 0.051888  [  992/ 3200]\n",
      "loss: 0.016825  [ 1008/ 3200]\n",
      "loss: 0.012531  [ 1024/ 3200]\n",
      "loss: 0.037446  [ 1040/ 3200]\n",
      "loss: 0.030541  [ 1056/ 3200]\n",
      "loss: 0.031315  [ 1072/ 3200]\n",
      "loss: 0.015313  [ 1088/ 3200]\n",
      "loss: 0.025790  [ 1104/ 3200]\n",
      "loss: 0.017407  [ 1120/ 3200]\n",
      "loss: 0.004638  [ 1136/ 3200]\n",
      "loss: 0.012392  [ 1152/ 3200]\n",
      "loss: 0.028267  [ 1168/ 3200]\n",
      "loss: 0.013693  [ 1184/ 3200]\n",
      "loss: 0.008223  [ 1200/ 3200]\n",
      "loss: 0.011491  [ 1216/ 3200]\n",
      "loss: 0.008376  [ 1232/ 3200]\n",
      "loss: 0.036401  [ 1248/ 3200]\n",
      "loss: 0.024966  [ 1264/ 3200]\n",
      "loss: 0.007794  [ 1280/ 3200]\n",
      "loss: 0.011037  [ 1296/ 3200]\n",
      "loss: 0.020679  [ 1312/ 3200]\n",
      "loss: 0.015398  [ 1328/ 3200]\n",
      "loss: 0.014105  [ 1344/ 3200]\n",
      "loss: 0.013987  [ 1360/ 3200]\n",
      "loss: 0.013090  [ 1376/ 3200]\n",
      "loss: 0.013294  [ 1392/ 3200]\n",
      "loss: 0.025405  [ 1408/ 3200]\n",
      "loss: 0.009898  [ 1424/ 3200]\n",
      "loss: 0.016671  [ 1440/ 3200]\n",
      "loss: 0.009719  [ 1456/ 3200]\n",
      "loss: 0.034152  [ 1472/ 3200]\n",
      "loss: 0.004401  [ 1488/ 3200]\n",
      "loss: 0.012475  [ 1504/ 3200]\n",
      "loss: 0.010477  [ 1520/ 3200]\n",
      "loss: 0.006162  [ 1536/ 3200]\n",
      "loss: 0.029965  [ 1552/ 3200]\n",
      "loss: 0.009271  [ 1568/ 3200]\n",
      "loss: 0.014168  [ 1584/ 3200]\n",
      "loss: 0.016086  [ 1600/ 3200]\n",
      "loss: 0.021539  [ 1616/ 3200]\n",
      "loss: 0.001183  [ 1632/ 3200]\n",
      "loss: 0.019492  [ 1648/ 3200]\n",
      "loss: 0.007618  [ 1664/ 3200]\n",
      "loss: 0.306605  [ 1680/ 3200]\n",
      "loss: 0.112041  [ 1696/ 3200]\n",
      "loss: 0.059915  [ 1712/ 3200]\n",
      "loss: 0.019429  [ 1728/ 3200]\n",
      "loss: 0.006252  [ 1744/ 3200]\n",
      "loss: 0.018124  [ 1760/ 3200]\n",
      "loss: 0.009342  [ 1776/ 3200]\n",
      "loss: 0.049634  [ 1792/ 3200]\n",
      "loss: 0.003658  [ 1808/ 3200]\n",
      "loss: 0.006650  [ 1824/ 3200]\n",
      "loss: 0.011123  [ 1840/ 3200]\n",
      "loss: 0.013571  [ 1856/ 3200]\n",
      "loss: 0.011429  [ 1872/ 3200]\n",
      "loss: 0.010913  [ 1888/ 3200]\n",
      "loss: 0.004085  [ 1904/ 3200]\n",
      "loss: 0.005793  [ 1920/ 3200]\n",
      "loss: 0.038828  [ 1936/ 3200]\n",
      "loss: 0.020680  [ 1952/ 3200]\n",
      "loss: 0.001994  [ 1968/ 3200]\n",
      "loss: 0.035545  [ 1984/ 3200]\n",
      "loss: 0.019162  [ 2000/ 3200]\n",
      "loss: 0.017551  [ 2016/ 3200]\n",
      "loss: 0.054704  [ 2032/ 3200]\n",
      "loss: 0.011079  [ 2048/ 3200]\n",
      "loss: 0.033529  [ 2064/ 3200]\n",
      "loss: 0.019819  [ 2080/ 3200]\n",
      "loss: 0.006584  [ 2096/ 3200]\n",
      "loss: 0.011977  [ 2112/ 3200]\n",
      "loss: 0.018695  [ 2128/ 3200]\n",
      "loss: 0.018123  [ 2144/ 3200]\n",
      "loss: 0.009314  [ 2160/ 3200]\n",
      "loss: 0.024949  [ 2176/ 3200]\n",
      "loss: 0.002527  [ 2192/ 3200]\n",
      "loss: 0.005010  [ 2208/ 3200]\n",
      "loss: 0.028278  [ 2224/ 3200]\n",
      "loss: 0.019543  [ 2240/ 3200]\n",
      "loss: 0.006320  [ 2256/ 3200]\n",
      "loss: 0.010255  [ 2272/ 3200]\n",
      "loss: 0.006872  [ 2288/ 3200]\n",
      "loss: 0.008194  [ 2304/ 3200]\n",
      "loss: 0.005470  [ 2320/ 3200]\n",
      "loss: 0.015815  [ 2336/ 3200]\n",
      "loss: 0.002443  [ 2352/ 3200]\n",
      "loss: 0.231264  [ 2368/ 3200]\n",
      "loss: 0.042048  [ 2384/ 3200]\n",
      "loss: 0.004180  [ 2400/ 3200]\n",
      "loss: 0.020668  [ 2416/ 3200]\n",
      "loss: 0.019307  [ 2432/ 3200]\n",
      "loss: 0.012945  [ 2448/ 3200]\n",
      "loss: 0.012700  [ 2464/ 3200]\n",
      "loss: 0.025542  [ 2480/ 3200]\n",
      "loss: 0.035750  [ 2496/ 3200]\n",
      "loss: 0.011899  [ 2512/ 3200]\n",
      "loss: 0.022123  [ 2528/ 3200]\n",
      "loss: 0.029897  [ 2544/ 3200]\n",
      "loss: 0.024046  [ 2560/ 3200]\n",
      "loss: 0.030667  [ 2576/ 3200]\n",
      "loss: 0.016446  [ 2592/ 3200]\n",
      "loss: 0.066358  [ 2608/ 3200]\n",
      "loss: 0.009270  [ 2624/ 3200]\n",
      "loss: 0.039173  [ 2640/ 3200]\n",
      "loss: 0.022447  [ 2656/ 3200]\n",
      "loss: 0.011231  [ 2672/ 3200]\n",
      "loss: 0.028679  [ 2688/ 3200]\n",
      "loss: 0.033119  [ 2704/ 3200]\n",
      "loss: 0.006718  [ 2720/ 3200]\n",
      "loss: 0.006298  [ 2736/ 3200]\n",
      "loss: 0.014748  [ 2752/ 3200]\n",
      "loss: 0.021666  [ 2768/ 3200]\n",
      "loss: 0.060838  [ 2784/ 3200]\n",
      "loss: 0.056417  [ 2800/ 3200]\n",
      "loss: 0.105841  [ 2816/ 3200]\n",
      "loss: 0.066861  [ 2832/ 3200]\n",
      "loss: 0.062804  [ 2848/ 3200]\n",
      "loss: 0.020497  [ 2864/ 3200]\n",
      "loss: 0.011136  [ 2880/ 3200]\n",
      "loss: 0.022233  [ 2896/ 3200]\n",
      "loss: 0.008914  [ 2912/ 3200]\n",
      "loss: 0.014018  [ 2928/ 3200]\n",
      "loss: 0.009260  [ 2944/ 3200]\n",
      "loss: 0.010217  [ 2960/ 3200]\n",
      "loss: 0.025647  [ 2976/ 3200]\n",
      "loss: 0.039269  [ 2992/ 3200]\n",
      "loss: 0.052474  [ 3008/ 3200]\n",
      "loss: 0.014630  [ 3024/ 3200]\n",
      "loss: 0.008817  [ 3040/ 3200]\n",
      "loss: 0.022307  [ 3056/ 3200]\n",
      "loss: 0.006671  [ 3072/ 3200]\n",
      "loss: 0.039454  [ 3088/ 3200]\n",
      "loss: 0.014273  [ 3104/ 3200]\n",
      "loss: 0.017447  [ 3120/ 3200]\n",
      "loss: 0.074834  [ 3136/ 3200]\n",
      "loss: 0.027195  [ 3152/ 3200]\n",
      "loss: 0.011391  [ 3168/ 3200]\n",
      "loss: 0.044117  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 0.005690  [    0/ 3200]\n",
      "loss: 0.009495  [   16/ 3200]\n",
      "loss: 0.009265  [   32/ 3200]\n",
      "loss: 0.024301  [   48/ 3200]\n",
      "loss: 0.005333  [   64/ 3200]\n",
      "loss: 0.015977  [   80/ 3200]\n",
      "loss: 0.027210  [   96/ 3200]\n",
      "loss: 0.010218  [  112/ 3200]\n",
      "loss: 0.024433  [  128/ 3200]\n",
      "loss: 0.017427  [  144/ 3200]\n",
      "loss: 0.006371  [  160/ 3200]\n",
      "loss: 0.014293  [  176/ 3200]\n",
      "loss: 0.006564  [  192/ 3200]\n",
      "loss: 0.002328  [  208/ 3200]\n",
      "loss: 0.018090  [  224/ 3200]\n",
      "loss: 0.009457  [  240/ 3200]\n",
      "loss: 0.006241  [  256/ 3200]\n",
      "loss: 0.014302  [  272/ 3200]\n",
      "loss: 0.015993  [  288/ 3200]\n",
      "loss: 0.021347  [  304/ 3200]\n",
      "loss: 0.004654  [  320/ 3200]\n",
      "loss: 0.008030  [  336/ 3200]\n",
      "loss: 0.090049  [  352/ 3200]\n",
      "loss: 0.026592  [  368/ 3200]\n",
      "loss: 0.034427  [  384/ 3200]\n",
      "loss: 0.013275  [  400/ 3200]\n",
      "loss: 0.020434  [  416/ 3200]\n",
      "loss: 0.010800  [  432/ 3200]\n",
      "loss: 0.009410  [  448/ 3200]\n",
      "loss: 0.010232  [  464/ 3200]\n",
      "loss: 0.007931  [  480/ 3200]\n",
      "loss: 0.005765  [  496/ 3200]\n",
      "loss: 0.010262  [  512/ 3200]\n",
      "loss: 0.007981  [  528/ 3200]\n",
      "loss: 0.006737  [  544/ 3200]\n",
      "loss: 0.007007  [  560/ 3200]\n",
      "loss: 0.010039  [  576/ 3200]\n",
      "loss: 0.012924  [  592/ 3200]\n",
      "loss: 0.002116  [  608/ 3200]\n",
      "loss: 0.008068  [  624/ 3200]\n",
      "loss: 0.017541  [  640/ 3200]\n",
      "loss: 0.005836  [  656/ 3200]\n",
      "loss: 0.012686  [  672/ 3200]\n",
      "loss: 0.012172  [  688/ 3200]\n",
      "loss: 0.011345  [  704/ 3200]\n",
      "loss: 0.020989  [  720/ 3200]\n",
      "loss: 0.008975  [  736/ 3200]\n",
      "loss: 0.008200  [  752/ 3200]\n",
      "loss: 0.021916  [  768/ 3200]\n",
      "loss: 0.020406  [  784/ 3200]\n",
      "loss: 0.014506  [  800/ 3200]\n",
      "loss: 0.011260  [  816/ 3200]\n",
      "loss: 0.015316  [  832/ 3200]\n",
      "loss: 0.004869  [  848/ 3200]\n",
      "loss: 0.005514  [  864/ 3200]\n",
      "loss: 0.016681  [  880/ 3200]\n",
      "loss: 0.016618  [  896/ 3200]\n",
      "loss: 0.012448  [  912/ 3200]\n",
      "loss: 0.014855  [  928/ 3200]\n",
      "loss: 0.015183  [  944/ 3200]\n",
      "loss: 0.009296  [  960/ 3200]\n",
      "loss: 0.015963  [  976/ 3200]\n",
      "loss: 0.026082  [  992/ 3200]\n",
      "loss: 0.013963  [ 1008/ 3200]\n",
      "loss: 0.009842  [ 1024/ 3200]\n",
      "loss: 0.012804  [ 1040/ 3200]\n",
      "loss: 0.027738  [ 1056/ 3200]\n",
      "loss: 0.017096  [ 1072/ 3200]\n",
      "loss: 0.007481  [ 1088/ 3200]\n",
      "loss: 0.011590  [ 1104/ 3200]\n",
      "loss: 0.008024  [ 1120/ 3200]\n",
      "loss: 0.012149  [ 1136/ 3200]\n",
      "loss: 0.037494  [ 1152/ 3200]\n",
      "loss: 0.011520  [ 1168/ 3200]\n",
      "loss: 0.012965  [ 1184/ 3200]\n",
      "loss: 0.009100  [ 1200/ 3200]\n",
      "loss: 0.007474  [ 1216/ 3200]\n",
      "loss: 0.009108  [ 1232/ 3200]\n",
      "loss: 0.020134  [ 1248/ 3200]\n",
      "loss: 0.008110  [ 1264/ 3200]\n",
      "loss: 0.023654  [ 1280/ 3200]\n",
      "loss: 0.004418  [ 1296/ 3200]\n",
      "loss: 0.021827  [ 1312/ 3200]\n",
      "loss: 0.003232  [ 1328/ 3200]\n",
      "loss: 0.012962  [ 1344/ 3200]\n",
      "loss: 0.030976  [ 1360/ 3200]\n",
      "loss: 0.016651  [ 1376/ 3200]\n",
      "loss: 0.003462  [ 1392/ 3200]\n",
      "loss: 0.011781  [ 1408/ 3200]\n",
      "loss: 0.044890  [ 1424/ 3200]\n",
      "loss: 0.011741  [ 1440/ 3200]\n",
      "loss: 0.021466  [ 1456/ 3200]\n",
      "loss: 0.011270  [ 1472/ 3200]\n",
      "loss: 0.014652  [ 1488/ 3200]\n",
      "loss: 0.009712  [ 1504/ 3200]\n",
      "loss: 0.012464  [ 1520/ 3200]\n",
      "loss: 0.017185  [ 1536/ 3200]\n",
      "loss: 0.004240  [ 1552/ 3200]\n",
      "loss: 0.017832  [ 1568/ 3200]\n",
      "loss: 0.011949  [ 1584/ 3200]\n",
      "loss: 0.013853  [ 1600/ 3200]\n",
      "loss: 0.014673  [ 1616/ 3200]\n",
      "loss: 0.003006  [ 1632/ 3200]\n",
      "loss: 0.008535  [ 1648/ 3200]\n",
      "loss: 0.027687  [ 1664/ 3200]\n",
      "loss: 0.008119  [ 1680/ 3200]\n",
      "loss: 0.003724  [ 1696/ 3200]\n",
      "loss: 0.014725  [ 1712/ 3200]\n",
      "loss: 0.012928  [ 1728/ 3200]\n",
      "loss: 0.010168  [ 1744/ 3200]\n",
      "loss: 0.006757  [ 1760/ 3200]\n",
      "loss: 0.004675  [ 1776/ 3200]\n",
      "loss: 0.010560  [ 1792/ 3200]\n",
      "loss: 0.011448  [ 1808/ 3200]\n",
      "loss: 0.013094  [ 1824/ 3200]\n",
      "loss: 0.020536  [ 1840/ 3200]\n",
      "loss: 0.013141  [ 1856/ 3200]\n",
      "loss: 0.006311  [ 1872/ 3200]\n",
      "loss: 0.002295  [ 1888/ 3200]\n",
      "loss: 0.010339  [ 1904/ 3200]\n",
      "loss: 0.012714  [ 1920/ 3200]\n",
      "loss: 0.011573  [ 1936/ 3200]\n",
      "loss: 0.012976  [ 1952/ 3200]\n",
      "loss: 0.025485  [ 1968/ 3200]\n",
      "loss: 0.020309  [ 1984/ 3200]\n",
      "loss: 0.024903  [ 2000/ 3200]\n",
      "loss: 0.008302  [ 2016/ 3200]\n",
      "loss: 0.010427  [ 2032/ 3200]\n",
      "loss: 0.020731  [ 2048/ 3200]\n",
      "loss: 0.005861  [ 2064/ 3200]\n",
      "loss: 0.010572  [ 2080/ 3200]\n",
      "loss: 0.012127  [ 2096/ 3200]\n",
      "loss: 0.005729  [ 2112/ 3200]\n",
      "loss: 0.011849  [ 2128/ 3200]\n",
      "loss: 0.015798  [ 2144/ 3200]\n",
      "loss: 0.012855  [ 2160/ 3200]\n",
      "loss: 0.035089  [ 2176/ 3200]\n",
      "loss: 0.026929  [ 2192/ 3200]\n",
      "loss: 0.015387  [ 2208/ 3200]\n",
      "loss: 0.018226  [ 2224/ 3200]\n",
      "loss: 0.010323  [ 2240/ 3200]\n",
      "loss: 0.005150  [ 2256/ 3200]\n",
      "loss: 0.008797  [ 2272/ 3200]\n",
      "loss: 0.009115  [ 2288/ 3200]\n",
      "loss: 0.015148  [ 2304/ 3200]\n",
      "loss: 0.014305  [ 2320/ 3200]\n",
      "loss: 0.022937  [ 2336/ 3200]\n",
      "loss: 0.006493  [ 2352/ 3200]\n",
      "loss: 0.032344  [ 2368/ 3200]\n",
      "loss: 0.018652  [ 2384/ 3200]\n",
      "loss: 0.008740  [ 2400/ 3200]\n",
      "loss: 0.016357  [ 2416/ 3200]\n",
      "loss: 0.011770  [ 2432/ 3200]\n",
      "loss: 0.010089  [ 2448/ 3200]\n",
      "loss: 0.012769  [ 2464/ 3200]\n",
      "loss: 0.024132  [ 2480/ 3200]\n",
      "loss: 0.008434  [ 2496/ 3200]\n",
      "loss: 0.019467  [ 2512/ 3200]\n",
      "loss: 0.094514  [ 2528/ 3200]\n",
      "loss: 0.029917  [ 2544/ 3200]\n",
      "loss: 0.012148  [ 2560/ 3200]\n",
      "loss: 0.016706  [ 2576/ 3200]\n",
      "loss: 0.218206  [ 2592/ 3200]\n",
      "loss: 0.063729  [ 2608/ 3200]\n",
      "loss: 0.038317  [ 2624/ 3200]\n",
      "loss: 0.026272  [ 2640/ 3200]\n",
      "loss: 0.064103  [ 2656/ 3200]\n",
      "loss: 0.004633  [ 2672/ 3200]\n",
      "loss: 0.010810  [ 2688/ 3200]\n",
      "loss: 0.005874  [ 2704/ 3200]\n",
      "loss: 0.006995  [ 2720/ 3200]\n",
      "loss: 0.019749  [ 2736/ 3200]\n",
      "loss: 0.008360  [ 2752/ 3200]\n",
      "loss: 0.011853  [ 2768/ 3200]\n",
      "loss: 0.005647  [ 2784/ 3200]\n",
      "loss: 0.022673  [ 2800/ 3200]\n",
      "loss: 0.013498  [ 2816/ 3200]\n",
      "loss: 0.024158  [ 2832/ 3200]\n",
      "loss: 0.012821  [ 2848/ 3200]\n",
      "loss: 0.016056  [ 2864/ 3200]\n",
      "loss: 0.012402  [ 2880/ 3200]\n",
      "loss: 0.007773  [ 2896/ 3200]\n",
      "loss: 0.015916  [ 2912/ 3200]\n",
      "loss: 0.010722  [ 2928/ 3200]\n",
      "loss: 0.012960  [ 2944/ 3200]\n",
      "loss: 0.036704  [ 2960/ 3200]\n",
      "loss: 0.003681  [ 2976/ 3200]\n",
      "loss: 0.012386  [ 2992/ 3200]\n",
      "loss: 0.010393  [ 3008/ 3200]\n",
      "loss: 0.014885  [ 3024/ 3200]\n",
      "loss: 0.009574  [ 3040/ 3200]\n",
      "loss: 0.044631  [ 3056/ 3200]\n",
      "loss: 0.306852  [ 3072/ 3200]\n",
      "loss: 0.031770  [ 3088/ 3200]\n",
      "loss: 0.027004  [ 3104/ 3200]\n",
      "loss: 0.029058  [ 3120/ 3200]\n",
      "loss: 0.009647  [ 3136/ 3200]\n",
      "loss: 0.003516  [ 3152/ 3200]\n",
      "loss: 0.007983  [ 3168/ 3200]\n",
      "loss: 0.022842  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.108077  [    0/ 3200]\n",
      "loss: 0.028983  [   16/ 3200]\n",
      "loss: 0.010982  [   32/ 3200]\n",
      "loss: 0.012609  [   48/ 3200]\n",
      "loss: 0.011361  [   64/ 3200]\n",
      "loss: 0.005029  [   80/ 3200]\n",
      "loss: 0.012165  [   96/ 3200]\n",
      "loss: 0.008680  [  112/ 3200]\n",
      "loss: 0.004011  [  128/ 3200]\n",
      "loss: 0.024063  [  144/ 3200]\n",
      "loss: 0.010721  [  160/ 3200]\n",
      "loss: 0.002621  [  176/ 3200]\n",
      "loss: 0.018638  [  192/ 3200]\n",
      "loss: 0.021149  [  208/ 3200]\n",
      "loss: 0.010657  [  224/ 3200]\n",
      "loss: 0.012958  [  240/ 3200]\n",
      "loss: 0.004782  [  256/ 3200]\n",
      "loss: 0.029654  [  272/ 3200]\n",
      "loss: 0.015773  [  288/ 3200]\n",
      "loss: 0.005105  [  304/ 3200]\n",
      "loss: 0.008508  [  320/ 3200]\n",
      "loss: 0.017247  [  336/ 3200]\n",
      "loss: 0.010768  [  352/ 3200]\n",
      "loss: 0.019773  [  368/ 3200]\n",
      "loss: 0.014846  [  384/ 3200]\n",
      "loss: 0.020265  [  400/ 3200]\n",
      "loss: 0.005752  [  416/ 3200]\n",
      "loss: 0.008889  [  432/ 3200]\n",
      "loss: 0.009964  [  448/ 3200]\n",
      "loss: 0.022589  [  464/ 3200]\n",
      "loss: 0.013284  [  480/ 3200]\n",
      "loss: 0.008829  [  496/ 3200]\n",
      "loss: 0.007358  [  512/ 3200]\n",
      "loss: 0.015635  [  528/ 3200]\n",
      "loss: 0.027074  [  544/ 3200]\n",
      "loss: 0.007360  [  560/ 3200]\n",
      "loss: 0.008510  [  576/ 3200]\n",
      "loss: 0.010886  [  592/ 3200]\n",
      "loss: 0.006880  [  608/ 3200]\n",
      "loss: 0.004524  [  624/ 3200]\n",
      "loss: 0.023010  [  640/ 3200]\n",
      "loss: 0.019507  [  656/ 3200]\n",
      "loss: 0.020169  [  672/ 3200]\n",
      "loss: 0.005792  [  688/ 3200]\n",
      "loss: 0.026153  [  704/ 3200]\n",
      "loss: 0.008089  [  720/ 3200]\n",
      "loss: 0.010252  [  736/ 3200]\n",
      "loss: 0.031727  [  752/ 3200]\n",
      "loss: 0.009158  [  768/ 3200]\n",
      "loss: 0.010473  [  784/ 3200]\n",
      "loss: 0.005610  [  800/ 3200]\n",
      "loss: 0.006484  [  816/ 3200]\n",
      "loss: 0.012046  [  832/ 3200]\n",
      "loss: 0.002876  [  848/ 3200]\n",
      "loss: 0.008113  [  864/ 3200]\n",
      "loss: 0.021865  [  880/ 3200]\n",
      "loss: 0.011784  [  896/ 3200]\n",
      "loss: 0.007047  [  912/ 3200]\n",
      "loss: 0.007409  [  928/ 3200]\n",
      "loss: 0.008776  [  944/ 3200]\n",
      "loss: 0.001544  [  960/ 3200]\n",
      "loss: 0.266750  [  976/ 3200]\n",
      "loss: 0.021638  [  992/ 3200]\n",
      "loss: 0.025476  [ 1008/ 3200]\n",
      "loss: 0.006128  [ 1024/ 3200]\n",
      "loss: 0.011361  [ 1040/ 3200]\n",
      "loss: 0.014308  [ 1056/ 3200]\n",
      "loss: 0.004343  [ 1072/ 3200]\n",
      "loss: 0.008477  [ 1088/ 3200]\n",
      "loss: 0.016074  [ 1104/ 3200]\n",
      "loss: 0.004507  [ 1120/ 3200]\n",
      "loss: 0.004731  [ 1136/ 3200]\n",
      "loss: 0.009754  [ 1152/ 3200]\n",
      "loss: 0.007793  [ 1168/ 3200]\n",
      "loss: 0.008184  [ 1184/ 3200]\n",
      "loss: 0.022480  [ 1200/ 3200]\n",
      "loss: 0.008273  [ 1216/ 3200]\n",
      "loss: 0.015206  [ 1232/ 3200]\n",
      "loss: 0.007928  [ 1248/ 3200]\n",
      "loss: 0.010775  [ 1264/ 3200]\n",
      "loss: 0.009895  [ 1280/ 3200]\n",
      "loss: 0.004228  [ 1296/ 3200]\n",
      "loss: 0.004689  [ 1312/ 3200]\n",
      "loss: 0.005377  [ 1328/ 3200]\n",
      "loss: 0.005518  [ 1344/ 3200]\n",
      "loss: 0.015316  [ 1360/ 3200]\n",
      "loss: 0.017943  [ 1376/ 3200]\n",
      "loss: 0.010999  [ 1392/ 3200]\n",
      "loss: 0.030426  [ 1408/ 3200]\n",
      "loss: 0.002346  [ 1424/ 3200]\n",
      "loss: 0.011521  [ 1440/ 3200]\n",
      "loss: 0.008754  [ 1456/ 3200]\n",
      "loss: 0.011307  [ 1472/ 3200]\n",
      "loss: 0.011176  [ 1488/ 3200]\n",
      "loss: 0.006746  [ 1504/ 3200]\n",
      "loss: 0.016030  [ 1520/ 3200]\n",
      "loss: 0.005490  [ 1536/ 3200]\n",
      "loss: 0.009946  [ 1552/ 3200]\n",
      "loss: 0.010298  [ 1568/ 3200]\n",
      "loss: 0.004898  [ 1584/ 3200]\n",
      "loss: 0.010355  [ 1600/ 3200]\n",
      "loss: 0.008226  [ 1616/ 3200]\n",
      "loss: 0.007727  [ 1632/ 3200]\n",
      "loss: 0.008960  [ 1648/ 3200]\n",
      "loss: 0.012821  [ 1664/ 3200]\n",
      "loss: 0.024545  [ 1680/ 3200]\n",
      "loss: 0.004341  [ 1696/ 3200]\n",
      "loss: 0.009331  [ 1712/ 3200]\n",
      "loss: 0.007869  [ 1728/ 3200]\n",
      "loss: 0.014925  [ 1744/ 3200]\n",
      "loss: 0.011624  [ 1760/ 3200]\n",
      "loss: 0.006249  [ 1776/ 3200]\n",
      "loss: 0.006534  [ 1792/ 3200]\n",
      "loss: 0.012085  [ 1808/ 3200]\n",
      "loss: 0.008458  [ 1824/ 3200]\n",
      "loss: 0.010993  [ 1840/ 3200]\n",
      "loss: 0.018396  [ 1856/ 3200]\n",
      "loss: 0.017827  [ 1872/ 3200]\n",
      "loss: 0.003137  [ 1888/ 3200]\n",
      "loss: 0.005367  [ 1904/ 3200]\n",
      "loss: 0.006679  [ 1920/ 3200]\n",
      "loss: 0.006401  [ 1936/ 3200]\n",
      "loss: 0.011594  [ 1952/ 3200]\n",
      "loss: 0.007866  [ 1968/ 3200]\n",
      "loss: 0.011444  [ 1984/ 3200]\n",
      "loss: 0.010637  [ 2000/ 3200]\n",
      "loss: 0.011772  [ 2016/ 3200]\n",
      "loss: 0.005474  [ 2032/ 3200]\n",
      "loss: 0.030307  [ 2048/ 3200]\n",
      "loss: 0.006564  [ 2064/ 3200]\n",
      "loss: 0.006320  [ 2080/ 3200]\n",
      "loss: 0.018042  [ 2096/ 3200]\n",
      "loss: 0.018805  [ 2112/ 3200]\n",
      "loss: 0.008199  [ 2128/ 3200]\n",
      "loss: 0.016912  [ 2144/ 3200]\n",
      "loss: 0.007285  [ 2160/ 3200]\n",
      "loss: 0.008489  [ 2176/ 3200]\n",
      "loss: 0.006283  [ 2192/ 3200]\n",
      "loss: 0.019360  [ 2208/ 3200]\n",
      "loss: 0.008327  [ 2224/ 3200]\n",
      "loss: 0.005112  [ 2240/ 3200]\n",
      "loss: 0.022724  [ 2256/ 3200]\n",
      "loss: 0.014080  [ 2272/ 3200]\n",
      "loss: 0.011739  [ 2288/ 3200]\n",
      "loss: 0.010213  [ 2304/ 3200]\n",
      "loss: 0.002650  [ 2320/ 3200]\n",
      "loss: 0.017431  [ 2336/ 3200]\n",
      "loss: 0.013913  [ 2352/ 3200]\n",
      "loss: 0.021976  [ 2368/ 3200]\n",
      "loss: 0.010332  [ 2384/ 3200]\n",
      "loss: 0.007115  [ 2400/ 3200]\n",
      "loss: 0.008222  [ 2416/ 3200]\n",
      "loss: 0.013017  [ 2432/ 3200]\n",
      "loss: 0.011435  [ 2448/ 3200]\n",
      "loss: 0.011054  [ 2464/ 3200]\n",
      "loss: 0.013552  [ 2480/ 3200]\n",
      "loss: 0.020634  [ 2496/ 3200]\n",
      "loss: 0.010449  [ 2512/ 3200]\n",
      "loss: 0.013787  [ 2528/ 3200]\n",
      "loss: 0.010246  [ 2544/ 3200]\n",
      "loss: 0.013213  [ 2560/ 3200]\n",
      "loss: 0.041924  [ 2576/ 3200]\n",
      "loss: 0.028210  [ 2592/ 3200]\n",
      "loss: 0.009754  [ 2608/ 3200]\n",
      "loss: 0.014665  [ 2624/ 3200]\n",
      "loss: 0.003652  [ 2640/ 3200]\n",
      "loss: 0.016179  [ 2656/ 3200]\n",
      "loss: 0.016482  [ 2672/ 3200]\n",
      "loss: 0.003650  [ 2688/ 3200]\n",
      "loss: 0.006094  [ 2704/ 3200]\n",
      "loss: 0.010746  [ 2720/ 3200]\n",
      "loss: 0.010071  [ 2736/ 3200]\n",
      "loss: 0.009925  [ 2752/ 3200]\n",
      "loss: 0.078710  [ 2768/ 3200]\n",
      "loss: 0.002363  [ 2784/ 3200]\n",
      "loss: 0.014866  [ 2800/ 3200]\n",
      "loss: 0.009819  [ 2816/ 3200]\n",
      "loss: 0.008626  [ 2832/ 3200]\n",
      "loss: 0.013551  [ 2848/ 3200]\n",
      "loss: 0.004141  [ 2864/ 3200]\n",
      "loss: 0.011555  [ 2880/ 3200]\n",
      "loss: 0.017042  [ 2896/ 3200]\n",
      "loss: 0.006752  [ 2912/ 3200]\n",
      "loss: 0.014675  [ 2928/ 3200]\n",
      "loss: 0.001768  [ 2944/ 3200]\n",
      "loss: 0.013318  [ 2960/ 3200]\n",
      "loss: 0.018465  [ 2976/ 3200]\n",
      "loss: 0.011388  [ 2992/ 3200]\n",
      "loss: 0.021051  [ 3008/ 3200]\n",
      "loss: 0.013874  [ 3024/ 3200]\n",
      "loss: 0.015580  [ 3040/ 3200]\n",
      "loss: 0.007240  [ 3056/ 3200]\n",
      "loss: 0.013910  [ 3072/ 3200]\n",
      "loss: 0.016344  [ 3088/ 3200]\n",
      "loss: 0.006876  [ 3104/ 3200]\n",
      "loss: 0.006700  [ 3120/ 3200]\n",
      "loss: 0.002421  [ 3136/ 3200]\n",
      "loss: 0.005421  [ 3152/ 3200]\n",
      "loss: 0.010990  [ 3168/ 3200]\n",
      "loss: 0.006084  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.006075  [    0/ 3200]\n",
      "loss: 0.005602  [   16/ 3200]\n",
      "loss: 0.014772  [   32/ 3200]\n",
      "loss: 0.006298  [   48/ 3200]\n",
      "loss: 0.020451  [   64/ 3200]\n",
      "loss: 0.001872  [   80/ 3200]\n",
      "loss: 0.009930  [   96/ 3200]\n",
      "loss: 0.006707  [  112/ 3200]\n",
      "loss: 0.005197  [  128/ 3200]\n",
      "loss: 0.008744  [  144/ 3200]\n",
      "loss: 0.009621  [  160/ 3200]\n",
      "loss: 0.002179  [  176/ 3200]\n",
      "loss: 0.003944  [  192/ 3200]\n",
      "loss: 0.009901  [  208/ 3200]\n",
      "loss: 0.003399  [  224/ 3200]\n",
      "loss: 0.003824  [  240/ 3200]\n",
      "loss: 0.004671  [  256/ 3200]\n",
      "loss: 0.082522  [  272/ 3200]\n",
      "loss: 0.003134  [  288/ 3200]\n",
      "loss: 0.009611  [  304/ 3200]\n",
      "loss: 0.012997  [  320/ 3200]\n",
      "loss: 0.010342  [  336/ 3200]\n",
      "loss: 0.016227  [  352/ 3200]\n",
      "loss: 0.018635  [  368/ 3200]\n",
      "loss: 0.005626  [  384/ 3200]\n",
      "loss: 0.008213  [  400/ 3200]\n",
      "loss: 0.008767  [  416/ 3200]\n",
      "loss: 0.005098  [  432/ 3200]\n",
      "loss: 0.003254  [  448/ 3200]\n",
      "loss: 0.011728  [  464/ 3200]\n",
      "loss: 0.012513  [  480/ 3200]\n",
      "loss: 0.007932  [  496/ 3200]\n",
      "loss: 0.004504  [  512/ 3200]\n",
      "loss: 0.011280  [  528/ 3200]\n",
      "loss: 0.018190  [  544/ 3200]\n",
      "loss: 0.005911  [  560/ 3200]\n",
      "loss: 0.003388  [  576/ 3200]\n",
      "loss: 0.013510  [  592/ 3200]\n",
      "loss: 0.014628  [  608/ 3200]\n",
      "loss: 0.006607  [  624/ 3200]\n",
      "loss: 0.010198  [  640/ 3200]\n",
      "loss: 0.005380  [  656/ 3200]\n",
      "loss: 0.014524  [  672/ 3200]\n",
      "loss: 0.011229  [  688/ 3200]\n",
      "loss: 0.009227  [  704/ 3200]\n",
      "loss: 0.004624  [  720/ 3200]\n",
      "loss: 0.009510  [  736/ 3200]\n",
      "loss: 0.002808  [  752/ 3200]\n",
      "loss: 0.009862  [  768/ 3200]\n",
      "loss: 0.008375  [  784/ 3200]\n",
      "loss: 0.003674  [  800/ 3200]\n",
      "loss: 0.016438  [  816/ 3200]\n",
      "loss: 0.013787  [  832/ 3200]\n",
      "loss: 0.006877  [  848/ 3200]\n",
      "loss: 0.013859  [  864/ 3200]\n",
      "loss: 0.008021  [  880/ 3200]\n",
      "loss: 0.002634  [  896/ 3200]\n",
      "loss: 0.007083  [  912/ 3200]\n",
      "loss: 0.009999  [  928/ 3200]\n",
      "loss: 0.008845  [  944/ 3200]\n",
      "loss: 0.007581  [  960/ 3200]\n",
      "loss: 0.007211  [  976/ 3200]\n",
      "loss: 0.028747  [  992/ 3200]\n",
      "loss: 0.010330  [ 1008/ 3200]\n",
      "loss: 0.011283  [ 1024/ 3200]\n",
      "loss: 0.021950  [ 1040/ 3200]\n",
      "loss: 0.005637  [ 1056/ 3200]\n",
      "loss: 0.003746  [ 1072/ 3200]\n",
      "loss: 0.005641  [ 1088/ 3200]\n",
      "loss: 0.014789  [ 1104/ 3200]\n",
      "loss: 0.006801  [ 1120/ 3200]\n",
      "loss: 0.005718  [ 1136/ 3200]\n",
      "loss: 0.005039  [ 1152/ 3200]\n",
      "loss: 0.003081  [ 1168/ 3200]\n",
      "loss: 0.006631  [ 1184/ 3200]\n",
      "loss: 0.008271  [ 1200/ 3200]\n",
      "loss: 0.022920  [ 1216/ 3200]\n",
      "loss: 0.009025  [ 1232/ 3200]\n",
      "loss: 0.013858  [ 1248/ 3200]\n",
      "loss: 0.003239  [ 1264/ 3200]\n",
      "loss: 0.017053  [ 1280/ 3200]\n",
      "loss: 0.015733  [ 1296/ 3200]\n",
      "loss: 0.004314  [ 1312/ 3200]\n",
      "loss: 0.008802  [ 1328/ 3200]\n",
      "loss: 0.002708  [ 1344/ 3200]\n",
      "loss: 0.000770  [ 1360/ 3200]\n",
      "loss: 0.005262  [ 1376/ 3200]\n",
      "loss: 0.005253  [ 1392/ 3200]\n",
      "loss: 0.019206  [ 1408/ 3200]\n",
      "loss: 0.003103  [ 1424/ 3200]\n",
      "loss: 0.005231  [ 1440/ 3200]\n",
      "loss: 0.021276  [ 1456/ 3200]\n",
      "loss: 0.005535  [ 1472/ 3200]\n",
      "loss: 0.012823  [ 1488/ 3200]\n",
      "loss: 0.007956  [ 1504/ 3200]\n",
      "loss: 0.007469  [ 1520/ 3200]\n",
      "loss: 0.015220  [ 1536/ 3200]\n",
      "loss: 0.004633  [ 1552/ 3200]\n",
      "loss: 0.004146  [ 1568/ 3200]\n",
      "loss: 0.007773  [ 1584/ 3200]\n",
      "loss: 0.298228  [ 1600/ 3200]\n",
      "loss: 0.012591  [ 1616/ 3200]\n",
      "loss: 0.009495  [ 1632/ 3200]\n",
      "loss: 0.006890  [ 1648/ 3200]\n",
      "loss: 0.005982  [ 1664/ 3200]\n",
      "loss: 0.004900  [ 1680/ 3200]\n",
      "loss: 0.019760  [ 1696/ 3200]\n",
      "loss: 0.003622  [ 1712/ 3200]\n",
      "loss: 0.005159  [ 1728/ 3200]\n",
      "loss: 0.003736  [ 1744/ 3200]\n",
      "loss: 0.002926  [ 1760/ 3200]\n",
      "loss: 0.006898  [ 1776/ 3200]\n",
      "loss: 0.009168  [ 1792/ 3200]\n",
      "loss: 0.008199  [ 1808/ 3200]\n",
      "loss: 0.002903  [ 1824/ 3200]\n",
      "loss: 0.007697  [ 1840/ 3200]\n",
      "loss: 0.017664  [ 1856/ 3200]\n",
      "loss: 0.010314  [ 1872/ 3200]\n",
      "loss: 0.003720  [ 1888/ 3200]\n",
      "loss: 0.013341  [ 1904/ 3200]\n",
      "loss: 0.005624  [ 1920/ 3200]\n",
      "loss: 0.008765  [ 1936/ 3200]\n",
      "loss: 0.017129  [ 1952/ 3200]\n",
      "loss: 0.010272  [ 1968/ 3200]\n",
      "loss: 0.011739  [ 1984/ 3200]\n",
      "loss: 0.005861  [ 2000/ 3200]\n",
      "loss: 0.010191  [ 2016/ 3200]\n",
      "loss: 0.006231  [ 2032/ 3200]\n",
      "loss: 0.015990  [ 2048/ 3200]\n",
      "loss: 0.008133  [ 2064/ 3200]\n",
      "loss: 0.007954  [ 2080/ 3200]\n",
      "loss: 0.004709  [ 2096/ 3200]\n",
      "loss: 0.008225  [ 2112/ 3200]\n",
      "loss: 0.016876  [ 2128/ 3200]\n",
      "loss: 0.093481  [ 2144/ 3200]\n",
      "loss: 0.012973  [ 2160/ 3200]\n",
      "loss: 0.008113  [ 2176/ 3200]\n",
      "loss: 0.006135  [ 2192/ 3200]\n",
      "loss: 0.004372  [ 2208/ 3200]\n",
      "loss: 0.006428  [ 2224/ 3200]\n",
      "loss: 0.010839  [ 2240/ 3200]\n",
      "loss: 0.005226  [ 2256/ 3200]\n",
      "loss: 0.002572  [ 2272/ 3200]\n",
      "loss: 0.016537  [ 2288/ 3200]\n",
      "loss: 0.005844  [ 2304/ 3200]\n",
      "loss: 0.008632  [ 2320/ 3200]\n",
      "loss: 0.003790  [ 2336/ 3200]\n",
      "loss: 0.015913  [ 2352/ 3200]\n",
      "loss: 0.004520  [ 2368/ 3200]\n",
      "loss: 0.009380  [ 2384/ 3200]\n",
      "loss: 0.015558  [ 2400/ 3200]\n",
      "loss: 0.004512  [ 2416/ 3200]\n",
      "loss: 0.007224  [ 2432/ 3200]\n",
      "loss: 0.010616  [ 2448/ 3200]\n",
      "loss: 0.007399  [ 2464/ 3200]\n",
      "loss: 0.007237  [ 2480/ 3200]\n",
      "loss: 0.008321  [ 2496/ 3200]\n",
      "loss: 0.016030  [ 2512/ 3200]\n",
      "loss: 0.009751  [ 2528/ 3200]\n",
      "loss: 0.004401  [ 2544/ 3200]\n",
      "loss: 0.011139  [ 2560/ 3200]\n",
      "loss: 0.011132  [ 2576/ 3200]\n",
      "loss: 0.003305  [ 2592/ 3200]\n",
      "loss: 0.010696  [ 2608/ 3200]\n",
      "loss: 0.008065  [ 2624/ 3200]\n",
      "loss: 0.003337  [ 2640/ 3200]\n",
      "loss: 0.005831  [ 2656/ 3200]\n",
      "loss: 0.022519  [ 2672/ 3200]\n",
      "loss: 0.005682  [ 2688/ 3200]\n",
      "loss: 0.002040  [ 2704/ 3200]\n",
      "loss: 0.012348  [ 2720/ 3200]\n",
      "loss: 0.008891  [ 2736/ 3200]\n",
      "loss: 0.006649  [ 2752/ 3200]\n",
      "loss: 0.016239  [ 2768/ 3200]\n",
      "loss: 0.004657  [ 2784/ 3200]\n",
      "loss: 0.015820  [ 2800/ 3200]\n",
      "loss: 0.006044  [ 2816/ 3200]\n",
      "loss: 0.013597  [ 2832/ 3200]\n",
      "loss: 0.004865  [ 2848/ 3200]\n",
      "loss: 0.014492  [ 2864/ 3200]\n",
      "loss: 0.004870  [ 2880/ 3200]\n",
      "loss: 0.006993  [ 2896/ 3200]\n",
      "loss: 0.008874  [ 2912/ 3200]\n",
      "loss: 0.008183  [ 2928/ 3200]\n",
      "loss: 0.013126  [ 2944/ 3200]\n",
      "loss: 0.005307  [ 2960/ 3200]\n",
      "loss: 0.015073  [ 2976/ 3200]\n",
      "loss: 0.003335  [ 2992/ 3200]\n",
      "loss: 0.004721  [ 3008/ 3200]\n",
      "loss: 0.017138  [ 3024/ 3200]\n",
      "loss: 0.001201  [ 3040/ 3200]\n",
      "loss: 0.006368  [ 3056/ 3200]\n",
      "loss: 0.011050  [ 3072/ 3200]\n",
      "loss: 0.004340  [ 3088/ 3200]\n",
      "loss: 0.011895  [ 3104/ 3200]\n",
      "loss: 0.007140  [ 3120/ 3200]\n",
      "loss: 0.011924  [ 3136/ 3200]\n",
      "loss: 0.011926  [ 3152/ 3200]\n",
      "loss: 0.007134  [ 3168/ 3200]\n",
      "loss: 0.015134  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.008381  [    0/ 3200]\n",
      "loss: 0.005098  [   16/ 3200]\n",
      "loss: 0.008975  [   32/ 3200]\n",
      "loss: 0.010032  [   48/ 3200]\n",
      "loss: 0.007299  [   64/ 3200]\n",
      "loss: 0.005360  [   80/ 3200]\n",
      "loss: 0.006994  [   96/ 3200]\n",
      "loss: 0.009634  [  112/ 3200]\n",
      "loss: 0.016273  [  128/ 3200]\n",
      "loss: 0.002530  [  144/ 3200]\n",
      "loss: 0.006188  [  160/ 3200]\n",
      "loss: 0.005554  [  176/ 3200]\n",
      "loss: 0.006437  [  192/ 3200]\n",
      "loss: 0.010610  [  208/ 3200]\n",
      "loss: 0.005170  [  224/ 3200]\n",
      "loss: 0.003013  [  240/ 3200]\n",
      "loss: 0.006190  [  256/ 3200]\n",
      "loss: 0.006609  [  272/ 3200]\n",
      "loss: 0.007447  [  288/ 3200]\n",
      "loss: 0.003122  [  304/ 3200]\n",
      "loss: 0.010679  [  320/ 3200]\n",
      "loss: 0.011121  [  336/ 3200]\n",
      "loss: 0.004883  [  352/ 3200]\n",
      "loss: 0.005793  [  368/ 3200]\n",
      "loss: 0.086326  [  384/ 3200]\n",
      "loss: 0.016286  [  400/ 3200]\n",
      "loss: 0.008732  [  416/ 3200]\n",
      "loss: 0.003614  [  432/ 3200]\n",
      "loss: 0.002423  [  448/ 3200]\n",
      "loss: 0.005028  [  464/ 3200]\n",
      "loss: 0.009939  [  480/ 3200]\n",
      "loss: 0.006370  [  496/ 3200]\n",
      "loss: 0.008270  [  512/ 3200]\n",
      "loss: 0.004394  [  528/ 3200]\n",
      "loss: 0.013090  [  544/ 3200]\n",
      "loss: 0.002084  [  560/ 3200]\n",
      "loss: 0.018247  [  576/ 3200]\n",
      "loss: 0.014783  [  592/ 3200]\n",
      "loss: 0.010243  [  608/ 3200]\n",
      "loss: 0.016899  [  624/ 3200]\n",
      "loss: 0.001815  [  640/ 3200]\n",
      "loss: 0.005242  [  656/ 3200]\n",
      "loss: 0.006255  [  672/ 3200]\n",
      "loss: 0.002476  [  688/ 3200]\n",
      "loss: 0.016927  [  704/ 3200]\n",
      "loss: 0.004874  [  720/ 3200]\n",
      "loss: 0.001906  [  736/ 3200]\n",
      "loss: 0.007011  [  752/ 3200]\n",
      "loss: 0.001515  [  768/ 3200]\n",
      "loss: 0.021731  [  784/ 3200]\n",
      "loss: 0.007046  [  800/ 3200]\n",
      "loss: 0.012130  [  816/ 3200]\n",
      "loss: 0.004595  [  832/ 3200]\n",
      "loss: 0.004572  [  848/ 3200]\n",
      "loss: 0.009695  [  864/ 3200]\n",
      "loss: 0.003129  [  880/ 3200]\n",
      "loss: 0.004936  [  896/ 3200]\n",
      "loss: 0.007099  [  912/ 3200]\n",
      "loss: 0.008110  [  928/ 3200]\n",
      "loss: 0.004204  [  944/ 3200]\n",
      "loss: 0.018752  [  960/ 3200]\n",
      "loss: 0.005926  [  976/ 3200]\n",
      "loss: 0.008237  [  992/ 3200]\n",
      "loss: 0.008748  [ 1008/ 3200]\n",
      "loss: 0.012382  [ 1024/ 3200]\n",
      "loss: 0.015348  [ 1040/ 3200]\n",
      "loss: 0.003386  [ 1056/ 3200]\n",
      "loss: 0.006946  [ 1072/ 3200]\n",
      "loss: 0.008692  [ 1088/ 3200]\n",
      "loss: 0.005162  [ 1104/ 3200]\n",
      "loss: 0.004716  [ 1120/ 3200]\n",
      "loss: 0.011421  [ 1136/ 3200]\n",
      "loss: 0.002415  [ 1152/ 3200]\n",
      "loss: 0.010676  [ 1168/ 3200]\n",
      "loss: 0.009181  [ 1184/ 3200]\n",
      "loss: 0.011747  [ 1200/ 3200]\n",
      "loss: 0.011089  [ 1216/ 3200]\n",
      "loss: 0.006220  [ 1232/ 3200]\n",
      "loss: 0.003604  [ 1248/ 3200]\n",
      "loss: 0.005052  [ 1264/ 3200]\n",
      "loss: 0.010732  [ 1280/ 3200]\n",
      "loss: 0.008120  [ 1296/ 3200]\n",
      "loss: 0.004094  [ 1312/ 3200]\n",
      "loss: 0.003420  [ 1328/ 3200]\n",
      "loss: 0.005206  [ 1344/ 3200]\n",
      "loss: 0.002506  [ 1360/ 3200]\n",
      "loss: 0.008234  [ 1376/ 3200]\n",
      "loss: 0.009553  [ 1392/ 3200]\n",
      "loss: 0.009241  [ 1408/ 3200]\n",
      "loss: 0.012310  [ 1424/ 3200]\n",
      "loss: 0.007848  [ 1440/ 3200]\n",
      "loss: 0.003009  [ 1456/ 3200]\n",
      "loss: 0.007037  [ 1472/ 3200]\n",
      "loss: 0.006436  [ 1488/ 3200]\n",
      "loss: 0.003183  [ 1504/ 3200]\n",
      "loss: 0.005391  [ 1520/ 3200]\n",
      "loss: 0.011187  [ 1536/ 3200]\n",
      "loss: 0.006211  [ 1552/ 3200]\n",
      "loss: 0.009418  [ 1568/ 3200]\n",
      "loss: 0.002507  [ 1584/ 3200]\n",
      "loss: 0.006444  [ 1600/ 3200]\n",
      "loss: 0.006393  [ 1616/ 3200]\n",
      "loss: 0.002824  [ 1632/ 3200]\n",
      "loss: 0.002951  [ 1648/ 3200]\n",
      "loss: 0.008357  [ 1664/ 3200]\n",
      "loss: 0.002192  [ 1680/ 3200]\n",
      "loss: 0.002636  [ 1696/ 3200]\n",
      "loss: 0.011708  [ 1712/ 3200]\n",
      "loss: 0.011826  [ 1728/ 3200]\n",
      "loss: 0.016231  [ 1744/ 3200]\n",
      "loss: 0.013569  [ 1760/ 3200]\n",
      "loss: 0.006750  [ 1776/ 3200]\n",
      "loss: 0.006492  [ 1792/ 3200]\n",
      "loss: 0.008563  [ 1808/ 3200]\n",
      "loss: 0.011809  [ 1824/ 3200]\n",
      "loss: 0.004094  [ 1840/ 3200]\n",
      "loss: 0.011929  [ 1856/ 3200]\n",
      "loss: 0.004292  [ 1872/ 3200]\n",
      "loss: 0.011078  [ 1888/ 3200]\n",
      "loss: 0.009926  [ 1904/ 3200]\n",
      "loss: 0.007008  [ 1920/ 3200]\n",
      "loss: 0.002444  [ 1936/ 3200]\n",
      "loss: 0.003789  [ 1952/ 3200]\n",
      "loss: 0.007980  [ 1968/ 3200]\n",
      "loss: 0.005378  [ 1984/ 3200]\n",
      "loss: 0.009552  [ 2000/ 3200]\n",
      "loss: 0.009154  [ 2016/ 3200]\n",
      "loss: 0.015318  [ 2032/ 3200]\n",
      "loss: 0.006672  [ 2048/ 3200]\n",
      "loss: 0.009983  [ 2064/ 3200]\n",
      "loss: 0.010692  [ 2080/ 3200]\n",
      "loss: 0.023984  [ 2096/ 3200]\n",
      "loss: 0.005856  [ 2112/ 3200]\n",
      "loss: 0.002794  [ 2128/ 3200]\n",
      "loss: 0.007230  [ 2144/ 3200]\n",
      "loss: 0.003253  [ 2160/ 3200]\n",
      "loss: 0.006051  [ 2176/ 3200]\n",
      "loss: 0.013280  [ 2192/ 3200]\n",
      "loss: 0.006008  [ 2208/ 3200]\n",
      "loss: 0.005502  [ 2224/ 3200]\n",
      "loss: 0.001923  [ 2240/ 3200]\n",
      "loss: 0.002659  [ 2256/ 3200]\n",
      "loss: 0.005454  [ 2272/ 3200]\n",
      "loss: 0.005452  [ 2288/ 3200]\n",
      "loss: 0.003915  [ 2304/ 3200]\n",
      "loss: 0.009915  [ 2320/ 3200]\n",
      "loss: 0.015293  [ 2336/ 3200]\n",
      "loss: 0.003663  [ 2352/ 3200]\n",
      "loss: 0.014513  [ 2368/ 3200]\n",
      "loss: 0.007002  [ 2384/ 3200]\n",
      "loss: 0.006164  [ 2400/ 3200]\n",
      "loss: 0.011320  [ 2416/ 3200]\n",
      "loss: 0.004034  [ 2432/ 3200]\n",
      "loss: 0.002816  [ 2448/ 3200]\n",
      "loss: 0.007808  [ 2464/ 3200]\n",
      "loss: 0.009217  [ 2480/ 3200]\n",
      "loss: 0.010396  [ 2496/ 3200]\n",
      "loss: 0.009314  [ 2512/ 3200]\n",
      "loss: 0.006109  [ 2528/ 3200]\n",
      "loss: 0.006901  [ 2544/ 3200]\n",
      "loss: 0.009222  [ 2560/ 3200]\n",
      "loss: 0.001053  [ 2576/ 3200]\n",
      "loss: 0.004467  [ 2592/ 3200]\n",
      "loss: 0.004793  [ 2608/ 3200]\n",
      "loss: 0.006191  [ 2624/ 3200]\n",
      "loss: 0.006625  [ 2640/ 3200]\n",
      "loss: 0.012056  [ 2656/ 3200]\n",
      "loss: 0.005781  [ 2672/ 3200]\n",
      "loss: 0.021859  [ 2688/ 3200]\n",
      "loss: 0.006329  [ 2704/ 3200]\n",
      "loss: 0.003944  [ 2720/ 3200]\n",
      "loss: 0.003998  [ 2736/ 3200]\n",
      "loss: 0.006132  [ 2752/ 3200]\n",
      "loss: 0.008726  [ 2768/ 3200]\n",
      "loss: 0.010032  [ 2784/ 3200]\n",
      "loss: 0.005128  [ 2800/ 3200]\n",
      "loss: 0.002114  [ 2816/ 3200]\n",
      "loss: 0.005934  [ 2832/ 3200]\n",
      "loss: 0.002197  [ 2848/ 3200]\n",
      "loss: 0.006919  [ 2864/ 3200]\n",
      "loss: 0.007087  [ 2880/ 3200]\n",
      "loss: 0.008867  [ 2896/ 3200]\n",
      "loss: 0.007429  [ 2912/ 3200]\n",
      "loss: 0.012800  [ 2928/ 3200]\n",
      "loss: 0.008047  [ 2944/ 3200]\n",
      "loss: 0.012890  [ 2960/ 3200]\n",
      "loss: 0.002134  [ 2976/ 3200]\n",
      "loss: 0.002663  [ 2992/ 3200]\n",
      "loss: 0.269695  [ 3008/ 3200]\n",
      "loss: 0.008365  [ 3024/ 3200]\n",
      "loss: 0.046230  [ 3040/ 3200]\n",
      "loss: 0.003620  [ 3056/ 3200]\n",
      "loss: 0.011382  [ 3072/ 3200]\n",
      "loss: 0.007359  [ 3088/ 3200]\n",
      "loss: 0.008224  [ 3104/ 3200]\n",
      "loss: 0.003268  [ 3120/ 3200]\n",
      "loss: 0.007689  [ 3136/ 3200]\n",
      "loss: 0.008826  [ 3152/ 3200]\n",
      "loss: 0.000982  [ 3168/ 3200]\n",
      "loss: 0.008348  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.004893  [    0/ 3200]\n",
      "loss: 0.008310  [   16/ 3200]\n",
      "loss: 0.005398  [   32/ 3200]\n",
      "loss: 0.002875  [   48/ 3200]\n",
      "loss: 0.009818  [   64/ 3200]\n",
      "loss: 0.018980  [   80/ 3200]\n",
      "loss: 0.002804  [   96/ 3200]\n",
      "loss: 0.010657  [  112/ 3200]\n",
      "loss: 0.001798  [  128/ 3200]\n",
      "loss: 0.007879  [  144/ 3200]\n",
      "loss: 0.002971  [  160/ 3200]\n",
      "loss: 0.004178  [  176/ 3200]\n",
      "loss: 0.002458  [  192/ 3200]\n",
      "loss: 0.002230  [  208/ 3200]\n",
      "loss: 0.007169  [  224/ 3200]\n",
      "loss: 0.009251  [  240/ 3200]\n",
      "loss: 0.007497  [  256/ 3200]\n",
      "loss: 0.006718  [  272/ 3200]\n",
      "loss: 0.005538  [  288/ 3200]\n",
      "loss: 0.007600  [  304/ 3200]\n",
      "loss: 0.005024  [  320/ 3200]\n",
      "loss: 0.006961  [  336/ 3200]\n",
      "loss: 0.005637  [  352/ 3200]\n",
      "loss: 0.007462  [  368/ 3200]\n",
      "loss: 0.008924  [  384/ 3200]\n",
      "loss: 0.003572  [  400/ 3200]\n",
      "loss: 0.005879  [  416/ 3200]\n",
      "loss: 0.011083  [  432/ 3200]\n",
      "loss: 0.008429  [  448/ 3200]\n",
      "loss: 0.009263  [  464/ 3200]\n",
      "loss: 0.008073  [  480/ 3200]\n",
      "loss: 0.014498  [  496/ 3200]\n",
      "loss: 0.007039  [  512/ 3200]\n",
      "loss: 0.012421  [  528/ 3200]\n",
      "loss: 0.007511  [  544/ 3200]\n",
      "loss: 0.005992  [  560/ 3200]\n",
      "loss: 0.004810  [  576/ 3200]\n",
      "loss: 0.008365  [  592/ 3200]\n",
      "loss: 0.007107  [  608/ 3200]\n",
      "loss: 0.003748  [  624/ 3200]\n",
      "loss: 0.002105  [  640/ 3200]\n",
      "loss: 0.013981  [  656/ 3200]\n",
      "loss: 0.004714  [  672/ 3200]\n",
      "loss: 0.007140  [  688/ 3200]\n",
      "loss: 0.003930  [  704/ 3200]\n",
      "loss: 0.000971  [  720/ 3200]\n",
      "loss: 0.005339  [  736/ 3200]\n",
      "loss: 0.011480  [  752/ 3200]\n",
      "loss: 0.004518  [  768/ 3200]\n",
      "loss: 0.003577  [  784/ 3200]\n",
      "loss: 0.004193  [  800/ 3200]\n",
      "loss: 0.005254  [  816/ 3200]\n",
      "loss: 0.007804  [  832/ 3200]\n",
      "loss: 0.009841  [  848/ 3200]\n",
      "loss: 0.004251  [  864/ 3200]\n",
      "loss: 0.258484  [  880/ 3200]\n",
      "loss: 0.053281  [  896/ 3200]\n",
      "loss: 0.007375  [  912/ 3200]\n",
      "loss: 0.006703  [  928/ 3200]\n",
      "loss: 0.014159  [  944/ 3200]\n",
      "loss: 0.005781  [  960/ 3200]\n",
      "loss: 0.003692  [  976/ 3200]\n",
      "loss: 0.005209  [  992/ 3200]\n",
      "loss: 0.005716  [ 1008/ 3200]\n",
      "loss: 0.002351  [ 1024/ 3200]\n",
      "loss: 0.006383  [ 1040/ 3200]\n",
      "loss: 0.008251  [ 1056/ 3200]\n",
      "loss: 0.005283  [ 1072/ 3200]\n",
      "loss: 0.009318  [ 1088/ 3200]\n",
      "loss: 0.005525  [ 1104/ 3200]\n",
      "loss: 0.002502  [ 1120/ 3200]\n",
      "loss: 0.006725  [ 1136/ 3200]\n",
      "loss: 0.009311  [ 1152/ 3200]\n",
      "loss: 0.009047  [ 1168/ 3200]\n",
      "loss: 0.007388  [ 1184/ 3200]\n",
      "loss: 0.004188  [ 1200/ 3200]\n",
      "loss: 0.010618  [ 1216/ 3200]\n",
      "loss: 0.002927  [ 1232/ 3200]\n",
      "loss: 0.004350  [ 1248/ 3200]\n",
      "loss: 0.006223  [ 1264/ 3200]\n",
      "loss: 0.003302  [ 1280/ 3200]\n",
      "loss: 0.004733  [ 1296/ 3200]\n",
      "loss: 0.005347  [ 1312/ 3200]\n",
      "loss: 0.003314  [ 1328/ 3200]\n",
      "loss: 0.006897  [ 1344/ 3200]\n",
      "loss: 0.006871  [ 1360/ 3200]\n",
      "loss: 0.006474  [ 1376/ 3200]\n",
      "loss: 0.009958  [ 1392/ 3200]\n",
      "loss: 0.002351  [ 1408/ 3200]\n",
      "loss: 0.007890  [ 1424/ 3200]\n",
      "loss: 0.003476  [ 1440/ 3200]\n",
      "loss: 0.004776  [ 1456/ 3200]\n",
      "loss: 0.006767  [ 1472/ 3200]\n",
      "loss: 0.002827  [ 1488/ 3200]\n",
      "loss: 0.003348  [ 1504/ 3200]\n",
      "loss: 0.004528  [ 1520/ 3200]\n",
      "loss: 0.011142  [ 1536/ 3200]\n",
      "loss: 0.006087  [ 1552/ 3200]\n",
      "loss: 0.008823  [ 1568/ 3200]\n",
      "loss: 0.006671  [ 1584/ 3200]\n",
      "loss: 0.003587  [ 1600/ 3200]\n",
      "loss: 0.003724  [ 1616/ 3200]\n",
      "loss: 0.007687  [ 1632/ 3200]\n",
      "loss: 0.001549  [ 1648/ 3200]\n",
      "loss: 0.005272  [ 1664/ 3200]\n",
      "loss: 0.006963  [ 1680/ 3200]\n",
      "loss: 0.003810  [ 1696/ 3200]\n",
      "loss: 0.003480  [ 1712/ 3200]\n",
      "loss: 0.002192  [ 1728/ 3200]\n",
      "loss: 0.013214  [ 1744/ 3200]\n",
      "loss: 0.002361  [ 1760/ 3200]\n",
      "loss: 0.006611  [ 1776/ 3200]\n",
      "loss: 0.005875  [ 1792/ 3200]\n",
      "loss: 0.005993  [ 1808/ 3200]\n",
      "loss: 0.005544  [ 1824/ 3200]\n",
      "loss: 0.005058  [ 1840/ 3200]\n",
      "loss: 0.006456  [ 1856/ 3200]\n",
      "loss: 0.008986  [ 1872/ 3200]\n",
      "loss: 0.000400  [ 1888/ 3200]\n",
      "loss: 0.004668  [ 1904/ 3200]\n",
      "loss: 0.008073  [ 1920/ 3200]\n",
      "loss: 0.006583  [ 1936/ 3200]\n",
      "loss: 0.012193  [ 1952/ 3200]\n",
      "loss: 0.003899  [ 1968/ 3200]\n",
      "loss: 0.026360  [ 1984/ 3200]\n",
      "loss: 0.001782  [ 2000/ 3200]\n",
      "loss: 0.078097  [ 2016/ 3200]\n",
      "loss: 0.013717  [ 2032/ 3200]\n",
      "loss: 0.004740  [ 2048/ 3200]\n",
      "loss: 0.003617  [ 2064/ 3200]\n",
      "loss: 0.004636  [ 2080/ 3200]\n",
      "loss: 0.000826  [ 2096/ 3200]\n",
      "loss: 0.008612  [ 2112/ 3200]\n",
      "loss: 0.006369  [ 2128/ 3200]\n",
      "loss: 0.006274  [ 2144/ 3200]\n",
      "loss: 0.006068  [ 2160/ 3200]\n",
      "loss: 0.006724  [ 2176/ 3200]\n",
      "loss: 0.018275  [ 2192/ 3200]\n",
      "loss: 0.003574  [ 2208/ 3200]\n",
      "loss: 0.006368  [ 2224/ 3200]\n",
      "loss: 0.005630  [ 2240/ 3200]\n",
      "loss: 0.003054  [ 2256/ 3200]\n",
      "loss: 0.001961  [ 2272/ 3200]\n",
      "loss: 0.004177  [ 2288/ 3200]\n",
      "loss: 0.002130  [ 2304/ 3200]\n",
      "loss: 0.001142  [ 2320/ 3200]\n",
      "loss: 0.006268  [ 2336/ 3200]\n",
      "loss: 0.006576  [ 2352/ 3200]\n",
      "loss: 0.005853  [ 2368/ 3200]\n",
      "loss: 0.002822  [ 2384/ 3200]\n",
      "loss: 0.002868  [ 2400/ 3200]\n",
      "loss: 0.004303  [ 2416/ 3200]\n",
      "loss: 0.006209  [ 2432/ 3200]\n",
      "loss: 0.004002  [ 2448/ 3200]\n",
      "loss: 0.001321  [ 2464/ 3200]\n",
      "loss: 0.002479  [ 2480/ 3200]\n",
      "loss: 0.010242  [ 2496/ 3200]\n",
      "loss: 0.008033  [ 2512/ 3200]\n",
      "loss: 0.008102  [ 2528/ 3200]\n",
      "loss: 0.007715  [ 2544/ 3200]\n",
      "loss: 0.012826  [ 2560/ 3200]\n",
      "loss: 0.005878  [ 2576/ 3200]\n",
      "loss: 0.001877  [ 2592/ 3200]\n",
      "loss: 0.005887  [ 2608/ 3200]\n",
      "loss: 0.003474  [ 2624/ 3200]\n",
      "loss: 0.007288  [ 2640/ 3200]\n",
      "loss: 0.003561  [ 2656/ 3200]\n",
      "loss: 0.009021  [ 2672/ 3200]\n",
      "loss: 0.016383  [ 2688/ 3200]\n",
      "loss: 0.002063  [ 2704/ 3200]\n",
      "loss: 0.003101  [ 2720/ 3200]\n",
      "loss: 0.003387  [ 2736/ 3200]\n",
      "loss: 0.006484  [ 2752/ 3200]\n",
      "loss: 0.007191  [ 2768/ 3200]\n",
      "loss: 0.005143  [ 2784/ 3200]\n",
      "loss: 0.003527  [ 2800/ 3200]\n",
      "loss: 0.009164  [ 2816/ 3200]\n",
      "loss: 0.009065  [ 2832/ 3200]\n",
      "loss: 0.002451  [ 2848/ 3200]\n",
      "loss: 0.006064  [ 2864/ 3200]\n",
      "loss: 0.002952  [ 2880/ 3200]\n",
      "loss: 0.019070  [ 2896/ 3200]\n",
      "loss: 0.004108  [ 2912/ 3200]\n",
      "loss: 0.006161  [ 2928/ 3200]\n",
      "loss: 0.005771  [ 2944/ 3200]\n",
      "loss: 0.006028  [ 2960/ 3200]\n",
      "loss: 0.008162  [ 2976/ 3200]\n",
      "loss: 0.006580  [ 2992/ 3200]\n",
      "loss: 0.001964  [ 3008/ 3200]\n",
      "loss: 0.007416  [ 3024/ 3200]\n",
      "loss: 0.003678  [ 3040/ 3200]\n",
      "loss: 0.017027  [ 3056/ 3200]\n",
      "loss: 0.012241  [ 3072/ 3200]\n",
      "loss: 0.003962  [ 3088/ 3200]\n",
      "loss: 0.004530  [ 3104/ 3200]\n",
      "loss: 0.003024  [ 3120/ 3200]\n",
      "loss: 0.015819  [ 3136/ 3200]\n",
      "loss: 0.005620  [ 3152/ 3200]\n",
      "loss: 0.008457  [ 3168/ 3200]\n",
      "loss: 0.015330  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 0.003350  [    0/ 3200]\n",
      "loss: 0.009101  [   16/ 3200]\n",
      "loss: 0.005114  [   32/ 3200]\n",
      "loss: 0.001112  [   48/ 3200]\n",
      "loss: 0.001895  [   64/ 3200]\n",
      "loss: 0.002565  [   80/ 3200]\n",
      "loss: 0.003725  [   96/ 3200]\n",
      "loss: 0.007740  [  112/ 3200]\n",
      "loss: 0.007824  [  128/ 3200]\n",
      "loss: 0.013362  [  144/ 3200]\n",
      "loss: 0.008287  [  160/ 3200]\n",
      "loss: 0.008693  [  176/ 3200]\n",
      "loss: 0.007769  [  192/ 3200]\n",
      "loss: 0.003480  [  208/ 3200]\n",
      "loss: 0.004449  [  224/ 3200]\n",
      "loss: 0.005085  [  240/ 3200]\n",
      "loss: 0.003001  [  256/ 3200]\n",
      "loss: 0.004873  [  272/ 3200]\n",
      "loss: 0.008544  [  288/ 3200]\n",
      "loss: 0.004034  [  304/ 3200]\n",
      "loss: 0.001315  [  320/ 3200]\n",
      "loss: 0.014289  [  336/ 3200]\n",
      "loss: 0.004205  [  352/ 3200]\n",
      "loss: 0.002893  [  368/ 3200]\n",
      "loss: 0.007174  [  384/ 3200]\n",
      "loss: 0.006353  [  400/ 3200]\n",
      "loss: 0.008654  [  416/ 3200]\n",
      "loss: 0.008044  [  432/ 3200]\n",
      "loss: 0.004632  [  448/ 3200]\n",
      "loss: 0.004670  [  464/ 3200]\n",
      "loss: 0.005746  [  480/ 3200]\n",
      "loss: 0.004749  [  496/ 3200]\n",
      "loss: 0.003409  [  512/ 3200]\n",
      "loss: 0.001822  [  528/ 3200]\n",
      "loss: 0.004495  [  544/ 3200]\n",
      "loss: 0.006201  [  560/ 3200]\n",
      "loss: 0.004689  [  576/ 3200]\n",
      "loss: 0.007661  [  592/ 3200]\n",
      "loss: 0.007834  [  608/ 3200]\n",
      "loss: 0.010042  [  624/ 3200]\n",
      "loss: 0.002451  [  640/ 3200]\n",
      "loss: 0.002706  [  656/ 3200]\n",
      "loss: 0.003281  [  672/ 3200]\n",
      "loss: 0.003807  [  688/ 3200]\n",
      "loss: 0.004247  [  704/ 3200]\n",
      "loss: 0.002883  [  720/ 3200]\n",
      "loss: 0.000564  [  736/ 3200]\n",
      "loss: 0.002598  [  752/ 3200]\n",
      "loss: 0.005613  [  768/ 3200]\n",
      "loss: 0.002052  [  784/ 3200]\n",
      "loss: 0.004432  [  800/ 3200]\n",
      "loss: 0.005699  [  816/ 3200]\n",
      "loss: 0.007090  [  832/ 3200]\n",
      "loss: 0.003594  [  848/ 3200]\n",
      "loss: 0.008560  [  864/ 3200]\n",
      "loss: 0.010181  [  880/ 3200]\n",
      "loss: 0.003287  [  896/ 3200]\n",
      "loss: 0.010018  [  912/ 3200]\n",
      "loss: 0.007944  [  928/ 3200]\n",
      "loss: 0.005249  [  944/ 3200]\n",
      "loss: 0.005281  [  960/ 3200]\n",
      "loss: 0.002137  [  976/ 3200]\n",
      "loss: 0.002531  [  992/ 3200]\n",
      "loss: 0.006445  [ 1008/ 3200]\n",
      "loss: 0.005337  [ 1024/ 3200]\n",
      "loss: 0.004617  [ 1040/ 3200]\n",
      "loss: 0.009192  [ 1056/ 3200]\n",
      "loss: 0.003594  [ 1072/ 3200]\n",
      "loss: 0.003997  [ 1088/ 3200]\n",
      "loss: 0.007748  [ 1104/ 3200]\n",
      "loss: 0.002375  [ 1120/ 3200]\n",
      "loss: 0.005379  [ 1136/ 3200]\n",
      "loss: 0.002181  [ 1152/ 3200]\n",
      "loss: 0.005794  [ 1168/ 3200]\n",
      "loss: 0.005524  [ 1184/ 3200]\n",
      "loss: 0.005222  [ 1200/ 3200]\n",
      "loss: 0.000998  [ 1216/ 3200]\n",
      "loss: 0.003827  [ 1232/ 3200]\n",
      "loss: 0.005399  [ 1248/ 3200]\n",
      "loss: 0.003965  [ 1264/ 3200]\n",
      "loss: 0.001387  [ 1280/ 3200]\n",
      "loss: 0.004755  [ 1296/ 3200]\n",
      "loss: 0.003972  [ 1312/ 3200]\n",
      "loss: 0.003540  [ 1328/ 3200]\n",
      "loss: 0.007686  [ 1344/ 3200]\n",
      "loss: 0.005592  [ 1360/ 3200]\n",
      "loss: 0.004807  [ 1376/ 3200]\n",
      "loss: 0.008632  [ 1392/ 3200]\n",
      "loss: 0.006117  [ 1408/ 3200]\n",
      "loss: 0.003981  [ 1424/ 3200]\n",
      "loss: 0.007304  [ 1440/ 3200]\n",
      "loss: 0.009758  [ 1456/ 3200]\n",
      "loss: 0.006313  [ 1472/ 3200]\n",
      "loss: 0.004601  [ 1488/ 3200]\n",
      "loss: 0.002404  [ 1504/ 3200]\n",
      "loss: 0.003158  [ 1520/ 3200]\n",
      "loss: 0.001027  [ 1536/ 3200]\n",
      "loss: 0.008089  [ 1552/ 3200]\n",
      "loss: 0.001321  [ 1568/ 3200]\n",
      "loss: 0.009520  [ 1584/ 3200]\n",
      "loss: 0.007459  [ 1600/ 3200]\n",
      "loss: 0.003288  [ 1616/ 3200]\n",
      "loss: 0.002946  [ 1632/ 3200]\n",
      "loss: 0.002912  [ 1648/ 3200]\n",
      "loss: 0.003686  [ 1664/ 3200]\n",
      "loss: 0.006231  [ 1680/ 3200]\n",
      "loss: 0.005084  [ 1696/ 3200]\n",
      "loss: 0.009733  [ 1712/ 3200]\n",
      "loss: 0.001856  [ 1728/ 3200]\n",
      "loss: 0.004421  [ 1744/ 3200]\n",
      "loss: 0.007989  [ 1760/ 3200]\n",
      "loss: 0.005028  [ 1776/ 3200]\n",
      "loss: 0.004192  [ 1792/ 3200]\n",
      "loss: 0.003888  [ 1808/ 3200]\n",
      "loss: 0.006122  [ 1824/ 3200]\n",
      "loss: 0.004109  [ 1840/ 3200]\n",
      "loss: 0.004632  [ 1856/ 3200]\n",
      "loss: 0.019000  [ 1872/ 3200]\n",
      "loss: 0.001236  [ 1888/ 3200]\n",
      "loss: 0.008193  [ 1904/ 3200]\n",
      "loss: 0.011290  [ 1920/ 3200]\n",
      "loss: 0.004447  [ 1936/ 3200]\n",
      "loss: 0.003503  [ 1952/ 3200]\n",
      "loss: 0.002952  [ 1968/ 3200]\n",
      "loss: 0.002037  [ 1984/ 3200]\n",
      "loss: 0.004807  [ 2000/ 3200]\n",
      "loss: 0.001923  [ 2016/ 3200]\n",
      "loss: 0.002209  [ 2032/ 3200]\n",
      "loss: 0.007446  [ 2048/ 3200]\n",
      "loss: 0.009757  [ 2064/ 3200]\n",
      "loss: 0.008658  [ 2080/ 3200]\n",
      "loss: 0.004997  [ 2096/ 3200]\n",
      "loss: 0.009320  [ 2112/ 3200]\n",
      "loss: 0.002665  [ 2128/ 3200]\n",
      "loss: 0.005307  [ 2144/ 3200]\n",
      "loss: 0.009473  [ 2160/ 3200]\n",
      "loss: 0.016416  [ 2176/ 3200]\n",
      "loss: 0.000858  [ 2192/ 3200]\n",
      "loss: 0.008001  [ 2208/ 3200]\n",
      "loss: 0.006581  [ 2224/ 3200]\n",
      "loss: 0.005818  [ 2240/ 3200]\n",
      "loss: 0.002870  [ 2256/ 3200]\n",
      "loss: 0.002465  [ 2272/ 3200]\n",
      "loss: 0.003823  [ 2288/ 3200]\n",
      "loss: 0.006217  [ 2304/ 3200]\n",
      "loss: 0.004735  [ 2320/ 3200]\n",
      "loss: 0.008869  [ 2336/ 3200]\n",
      "loss: 0.004357  [ 2352/ 3200]\n",
      "loss: 0.002020  [ 2368/ 3200]\n",
      "loss: 0.002131  [ 2384/ 3200]\n",
      "loss: 0.003428  [ 2400/ 3200]\n",
      "loss: 0.003750  [ 2416/ 3200]\n",
      "loss: 0.006017  [ 2432/ 3200]\n",
      "loss: 0.004689  [ 2448/ 3200]\n",
      "loss: 0.003360  [ 2464/ 3200]\n",
      "loss: 0.001955  [ 2480/ 3200]\n",
      "loss: 0.005741  [ 2496/ 3200]\n",
      "loss: 0.008442  [ 2512/ 3200]\n",
      "loss: 0.003660  [ 2528/ 3200]\n",
      "loss: 0.003469  [ 2544/ 3200]\n",
      "loss: 0.004397  [ 2560/ 3200]\n",
      "loss: 0.000375  [ 2576/ 3200]\n",
      "loss: 0.008984  [ 2592/ 3200]\n",
      "loss: 0.001272  [ 2608/ 3200]\n",
      "loss: 0.004028  [ 2624/ 3200]\n",
      "loss: 0.002353  [ 2640/ 3200]\n",
      "loss: 0.008968  [ 2656/ 3200]\n",
      "loss: 0.004159  [ 2672/ 3200]\n",
      "loss: 0.004674  [ 2688/ 3200]\n",
      "loss: 0.006055  [ 2704/ 3200]\n",
      "loss: 0.081139  [ 2720/ 3200]\n",
      "loss: 0.010581  [ 2736/ 3200]\n",
      "loss: 0.003071  [ 2752/ 3200]\n",
      "loss: 0.003962  [ 2768/ 3200]\n",
      "loss: 0.003437  [ 2784/ 3200]\n",
      "loss: 0.009905  [ 2800/ 3200]\n",
      "loss: 0.001196  [ 2816/ 3200]\n",
      "loss: 0.007253  [ 2832/ 3200]\n",
      "loss: 0.009122  [ 2848/ 3200]\n",
      "loss: 0.003093  [ 2864/ 3200]\n",
      "loss: 0.009737  [ 2880/ 3200]\n",
      "loss: 0.006000  [ 2896/ 3200]\n",
      "loss: 0.006114  [ 2912/ 3200]\n",
      "loss: 0.013171  [ 2928/ 3200]\n",
      "loss: 0.258612  [ 2944/ 3200]\n",
      "loss: 0.012814  [ 2960/ 3200]\n",
      "loss: 0.008655  [ 2976/ 3200]\n",
      "loss: 0.007760  [ 2992/ 3200]\n",
      "loss: 0.018583  [ 3008/ 3200]\n",
      "loss: 0.002544  [ 3024/ 3200]\n",
      "loss: 0.006363  [ 3040/ 3200]\n",
      "loss: 0.010247  [ 3056/ 3200]\n",
      "loss: 0.003320  [ 3072/ 3200]\n",
      "loss: 0.005033  [ 3088/ 3200]\n",
      "loss: 0.009798  [ 3104/ 3200]\n",
      "loss: 0.005846  [ 3120/ 3200]\n",
      "loss: 0.007224  [ 3136/ 3200]\n",
      "loss: 0.004511  [ 3152/ 3200]\n",
      "loss: 0.003309  [ 3168/ 3200]\n",
      "loss: 0.007020  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.004140  [    0/ 3200]\n",
      "loss: 0.008358  [   16/ 3200]\n",
      "loss: 0.003540  [   32/ 3200]\n",
      "loss: 0.077646  [   48/ 3200]\n",
      "loss: 0.005004  [   64/ 3200]\n",
      "loss: 0.008612  [   80/ 3200]\n",
      "loss: 0.003529  [   96/ 3200]\n",
      "loss: 0.004406  [  112/ 3200]\n",
      "loss: 0.006000  [  128/ 3200]\n",
      "loss: 0.004768  [  144/ 3200]\n",
      "loss: 0.007866  [  160/ 3200]\n",
      "loss: 0.003930  [  176/ 3200]\n",
      "loss: 0.003766  [  192/ 3200]\n",
      "loss: 0.001548  [  208/ 3200]\n",
      "loss: 0.002610  [  224/ 3200]\n",
      "loss: 0.001868  [  240/ 3200]\n",
      "loss: 0.005674  [  256/ 3200]\n",
      "loss: 0.005210  [  272/ 3200]\n",
      "loss: 0.005544  [  288/ 3200]\n",
      "loss: 0.011287  [  304/ 3200]\n",
      "loss: 0.001123  [  320/ 3200]\n",
      "loss: 0.004498  [  336/ 3200]\n",
      "loss: 0.001385  [  352/ 3200]\n",
      "loss: 0.010059  [  368/ 3200]\n",
      "loss: 0.005455  [  384/ 3200]\n",
      "loss: 0.012952  [  400/ 3200]\n",
      "loss: 0.016466  [  416/ 3200]\n",
      "loss: 0.002959  [  432/ 3200]\n",
      "loss: 0.006511  [  448/ 3200]\n",
      "loss: 0.005209  [  464/ 3200]\n",
      "loss: 0.004604  [  480/ 3200]\n",
      "loss: 0.004513  [  496/ 3200]\n",
      "loss: 0.002763  [  512/ 3200]\n",
      "loss: 0.004675  [  528/ 3200]\n",
      "loss: 0.002649  [  544/ 3200]\n",
      "loss: 0.003592  [  560/ 3200]\n",
      "loss: 0.004419  [  576/ 3200]\n",
      "loss: 0.001582  [  592/ 3200]\n",
      "loss: 0.002468  [  608/ 3200]\n",
      "loss: 0.001131  [  624/ 3200]\n",
      "loss: 0.007039  [  640/ 3200]\n",
      "loss: 0.003237  [  656/ 3200]\n",
      "loss: 0.001863  [  672/ 3200]\n",
      "loss: 0.003804  [  688/ 3200]\n",
      "loss: 0.004843  [  704/ 3200]\n",
      "loss: 0.006099  [  720/ 3200]\n",
      "loss: 0.004648  [  736/ 3200]\n",
      "loss: 0.002014  [  752/ 3200]\n",
      "loss: 0.002433  [  768/ 3200]\n",
      "loss: 0.002517  [  784/ 3200]\n",
      "loss: 0.003074  [  800/ 3200]\n",
      "loss: 0.008059  [  816/ 3200]\n",
      "loss: 0.004425  [  832/ 3200]\n",
      "loss: 0.002703  [  848/ 3200]\n",
      "loss: 0.005396  [  864/ 3200]\n",
      "loss: 0.005212  [  880/ 3200]\n",
      "loss: 0.003671  [  896/ 3200]\n",
      "loss: 0.004748  [  912/ 3200]\n",
      "loss: 0.003137  [  928/ 3200]\n",
      "loss: 0.019017  [  944/ 3200]\n",
      "loss: 0.005958  [  960/ 3200]\n",
      "loss: 0.008454  [  976/ 3200]\n",
      "loss: 0.001612  [  992/ 3200]\n",
      "loss: 0.007746  [ 1008/ 3200]\n",
      "loss: 0.003525  [ 1024/ 3200]\n",
      "loss: 0.008995  [ 1040/ 3200]\n",
      "loss: 0.005504  [ 1056/ 3200]\n",
      "loss: 0.003757  [ 1072/ 3200]\n",
      "loss: 0.004159  [ 1088/ 3200]\n",
      "loss: 0.002637  [ 1104/ 3200]\n",
      "loss: 0.004042  [ 1120/ 3200]\n",
      "loss: 0.003531  [ 1136/ 3200]\n",
      "loss: 0.005654  [ 1152/ 3200]\n",
      "loss: 0.004231  [ 1168/ 3200]\n",
      "loss: 0.004684  [ 1184/ 3200]\n",
      "loss: 0.004335  [ 1200/ 3200]\n",
      "loss: 0.002405  [ 1216/ 3200]\n",
      "loss: 0.012751  [ 1232/ 3200]\n",
      "loss: 0.003233  [ 1248/ 3200]\n",
      "loss: 0.006175  [ 1264/ 3200]\n",
      "loss: 0.005968  [ 1280/ 3200]\n",
      "loss: 0.006247  [ 1296/ 3200]\n",
      "loss: 0.003265  [ 1312/ 3200]\n",
      "loss: 0.004757  [ 1328/ 3200]\n",
      "loss: 0.009836  [ 1344/ 3200]\n",
      "loss: 0.002533  [ 1360/ 3200]\n",
      "loss: 0.003486  [ 1376/ 3200]\n",
      "loss: 0.004080  [ 1392/ 3200]\n",
      "loss: 0.004879  [ 1408/ 3200]\n",
      "loss: 0.003156  [ 1424/ 3200]\n",
      "loss: 0.003181  [ 1440/ 3200]\n",
      "loss: 0.002805  [ 1456/ 3200]\n",
      "loss: 0.001328  [ 1472/ 3200]\n",
      "loss: 0.001249  [ 1488/ 3200]\n",
      "loss: 0.004444  [ 1504/ 3200]\n",
      "loss: 0.004313  [ 1520/ 3200]\n",
      "loss: 0.004065  [ 1536/ 3200]\n",
      "loss: 0.006171  [ 1552/ 3200]\n",
      "loss: 0.008173  [ 1568/ 3200]\n",
      "loss: 0.001831  [ 1584/ 3200]\n",
      "loss: 0.005518  [ 1600/ 3200]\n",
      "loss: 0.005958  [ 1616/ 3200]\n",
      "loss: 0.002787  [ 1632/ 3200]\n",
      "loss: 0.007415  [ 1648/ 3200]\n",
      "loss: 0.001957  [ 1664/ 3200]\n",
      "loss: 0.003737  [ 1680/ 3200]\n",
      "loss: 0.237201  [ 1696/ 3200]\n",
      "loss: 0.066676  [ 1712/ 3200]\n",
      "loss: 0.007963  [ 1728/ 3200]\n",
      "loss: 0.003107  [ 1744/ 3200]\n",
      "loss: 0.015419  [ 1760/ 3200]\n",
      "loss: 0.004869  [ 1776/ 3200]\n",
      "loss: 0.006188  [ 1792/ 3200]\n",
      "loss: 0.002243  [ 1808/ 3200]\n",
      "loss: 0.013839  [ 1824/ 3200]\n",
      "loss: 0.011776  [ 1840/ 3200]\n",
      "loss: 0.001649  [ 1856/ 3200]\n",
      "loss: 0.013784  [ 1872/ 3200]\n",
      "loss: 0.002669  [ 1888/ 3200]\n",
      "loss: 0.010150  [ 1904/ 3200]\n",
      "loss: 0.003238  [ 1920/ 3200]\n",
      "loss: 0.003862  [ 1936/ 3200]\n",
      "loss: 0.005951  [ 1952/ 3200]\n",
      "loss: 0.003488  [ 1968/ 3200]\n",
      "loss: 0.007596  [ 1984/ 3200]\n",
      "loss: 0.015782  [ 2000/ 3200]\n",
      "loss: 0.006851  [ 2016/ 3200]\n",
      "loss: 0.008294  [ 2032/ 3200]\n",
      "loss: 0.004399  [ 2048/ 3200]\n",
      "loss: 0.002874  [ 2064/ 3200]\n",
      "loss: 0.006300  [ 2080/ 3200]\n",
      "loss: 0.000673  [ 2096/ 3200]\n",
      "loss: 0.003183  [ 2112/ 3200]\n",
      "loss: 0.005381  [ 2128/ 3200]\n",
      "loss: 0.004622  [ 2144/ 3200]\n",
      "loss: 0.004853  [ 2160/ 3200]\n",
      "loss: 0.006674  [ 2176/ 3200]\n",
      "loss: 0.006216  [ 2192/ 3200]\n",
      "loss: 0.005455  [ 2208/ 3200]\n",
      "loss: 0.003125  [ 2224/ 3200]\n",
      "loss: 0.003644  [ 2240/ 3200]\n",
      "loss: 0.005995  [ 2256/ 3200]\n",
      "loss: 0.006799  [ 2272/ 3200]\n",
      "loss: 0.004735  [ 2288/ 3200]\n",
      "loss: 0.004438  [ 2304/ 3200]\n",
      "loss: 0.007532  [ 2320/ 3200]\n",
      "loss: 0.003685  [ 2336/ 3200]\n",
      "loss: 0.002471  [ 2352/ 3200]\n",
      "loss: 0.013219  [ 2368/ 3200]\n",
      "loss: 0.005843  [ 2384/ 3200]\n",
      "loss: 0.003137  [ 2400/ 3200]\n",
      "loss: 0.000709  [ 2416/ 3200]\n",
      "loss: 0.004587  [ 2432/ 3200]\n",
      "loss: 0.001512  [ 2448/ 3200]\n",
      "loss: 0.003218  [ 2464/ 3200]\n",
      "loss: 0.004854  [ 2480/ 3200]\n",
      "loss: 0.004811  [ 2496/ 3200]\n",
      "loss: 0.004785  [ 2512/ 3200]\n",
      "loss: 0.004817  [ 2528/ 3200]\n",
      "loss: 0.003803  [ 2544/ 3200]\n",
      "loss: 0.003393  [ 2560/ 3200]\n",
      "loss: 0.006928  [ 2576/ 3200]\n",
      "loss: 0.005244  [ 2592/ 3200]\n",
      "loss: 0.007926  [ 2608/ 3200]\n",
      "loss: 0.008626  [ 2624/ 3200]\n",
      "loss: 0.003777  [ 2640/ 3200]\n",
      "loss: 0.003043  [ 2656/ 3200]\n",
      "loss: 0.005805  [ 2672/ 3200]\n",
      "loss: 0.006559  [ 2688/ 3200]\n",
      "loss: 0.004021  [ 2704/ 3200]\n",
      "loss: 0.003260  [ 2720/ 3200]\n",
      "loss: 0.003884  [ 2736/ 3200]\n",
      "loss: 0.003653  [ 2752/ 3200]\n",
      "loss: 0.011863  [ 2768/ 3200]\n",
      "loss: 0.012211  [ 2784/ 3200]\n",
      "loss: 0.003161  [ 2800/ 3200]\n",
      "loss: 0.005443  [ 2816/ 3200]\n",
      "loss: 0.005313  [ 2832/ 3200]\n",
      "loss: 0.005095  [ 2848/ 3200]\n",
      "loss: 0.002191  [ 2864/ 3200]\n",
      "loss: 0.002800  [ 2880/ 3200]\n",
      "loss: 0.008465  [ 2896/ 3200]\n",
      "loss: 0.006125  [ 2912/ 3200]\n",
      "loss: 0.001493  [ 2928/ 3200]\n",
      "loss: 0.002019  [ 2944/ 3200]\n",
      "loss: 0.007039  [ 2960/ 3200]\n",
      "loss: 0.002664  [ 2976/ 3200]\n",
      "loss: 0.003664  [ 2992/ 3200]\n",
      "loss: 0.003198  [ 3008/ 3200]\n",
      "loss: 0.001605  [ 3024/ 3200]\n",
      "loss: 0.004231  [ 3040/ 3200]\n",
      "loss: 0.005394  [ 3056/ 3200]\n",
      "loss: 0.006713  [ 3072/ 3200]\n",
      "loss: 0.001966  [ 3088/ 3200]\n",
      "loss: 0.005028  [ 3104/ 3200]\n",
      "loss: 0.000901  [ 3120/ 3200]\n",
      "loss: 0.003150  [ 3136/ 3200]\n",
      "loss: 0.007949  [ 3152/ 3200]\n",
      "loss: 0.002496  [ 3168/ 3200]\n",
      "loss: 0.003283  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 0.002798  [    0/ 3200]\n",
      "loss: 0.002440  [   16/ 3200]\n",
      "loss: 0.003139  [   32/ 3200]\n",
      "loss: 0.004465  [   48/ 3200]\n",
      "loss: 0.005819  [   64/ 3200]\n",
      "loss: 0.001302  [   80/ 3200]\n",
      "loss: 0.006474  [   96/ 3200]\n",
      "loss: 0.004451  [  112/ 3200]\n",
      "loss: 0.003189  [  128/ 3200]\n",
      "loss: 0.005570  [  144/ 3200]\n",
      "loss: 0.005115  [  160/ 3200]\n",
      "loss: 0.004691  [  176/ 3200]\n",
      "loss: 0.006919  [  192/ 3200]\n",
      "loss: 0.003390  [  208/ 3200]\n",
      "loss: 0.001230  [  224/ 3200]\n",
      "loss: 0.003251  [  240/ 3200]\n",
      "loss: 0.007683  [  256/ 3200]\n",
      "loss: 0.002304  [  272/ 3200]\n",
      "loss: 0.010883  [  288/ 3200]\n",
      "loss: 0.005410  [  304/ 3200]\n",
      "loss: 0.002551  [  320/ 3200]\n",
      "loss: 0.004174  [  336/ 3200]\n",
      "loss: 0.002468  [  352/ 3200]\n",
      "loss: 0.005663  [  368/ 3200]\n",
      "loss: 0.001144  [  384/ 3200]\n",
      "loss: 0.006432  [  400/ 3200]\n",
      "loss: 0.009147  [  416/ 3200]\n",
      "loss: 0.008297  [  432/ 3200]\n",
      "loss: 0.002299  [  448/ 3200]\n",
      "loss: 0.001344  [  464/ 3200]\n",
      "loss: 0.002090  [  480/ 3200]\n",
      "loss: 0.074523  [  496/ 3200]\n",
      "loss: 0.002110  [  512/ 3200]\n",
      "loss: 0.008878  [  528/ 3200]\n",
      "loss: 0.002660  [  544/ 3200]\n",
      "loss: 0.004937  [  560/ 3200]\n",
      "loss: 0.003792  [  576/ 3200]\n",
      "loss: 0.002568  [  592/ 3200]\n",
      "loss: 0.002694  [  608/ 3200]\n",
      "loss: 0.002427  [  624/ 3200]\n",
      "loss: 0.007089  [  640/ 3200]\n",
      "loss: 0.007059  [  656/ 3200]\n",
      "loss: 0.005462  [  672/ 3200]\n",
      "loss: 0.006075  [  688/ 3200]\n",
      "loss: 0.004128  [  704/ 3200]\n",
      "loss: 0.003578  [  720/ 3200]\n",
      "loss: 0.004073  [  736/ 3200]\n",
      "loss: 0.003959  [  752/ 3200]\n",
      "loss: 0.002068  [  768/ 3200]\n",
      "loss: 0.002627  [  784/ 3200]\n",
      "loss: 0.001337  [  800/ 3200]\n",
      "loss: 0.003553  [  816/ 3200]\n",
      "loss: 0.006832  [  832/ 3200]\n",
      "loss: 0.005173  [  848/ 3200]\n",
      "loss: 0.001471  [  864/ 3200]\n",
      "loss: 0.003030  [  880/ 3200]\n",
      "loss: 0.005554  [  896/ 3200]\n",
      "loss: 0.003545  [  912/ 3200]\n",
      "loss: 0.002087  [  928/ 3200]\n",
      "loss: 0.001795  [  944/ 3200]\n",
      "loss: 0.004132  [  960/ 3200]\n",
      "loss: 0.005426  [  976/ 3200]\n",
      "loss: 0.005238  [  992/ 3200]\n",
      "loss: 0.002569  [ 1008/ 3200]\n",
      "loss: 0.003753  [ 1024/ 3200]\n",
      "loss: 0.002582  [ 1040/ 3200]\n",
      "loss: 0.007132  [ 1056/ 3200]\n",
      "loss: 0.002869  [ 1072/ 3200]\n",
      "loss: 0.009474  [ 1088/ 3200]\n",
      "loss: 0.002600  [ 1104/ 3200]\n",
      "loss: 0.002780  [ 1120/ 3200]\n",
      "loss: 0.004340  [ 1136/ 3200]\n",
      "loss: 0.002635  [ 1152/ 3200]\n",
      "loss: 0.009377  [ 1168/ 3200]\n",
      "loss: 0.002364  [ 1184/ 3200]\n",
      "loss: 0.006833  [ 1200/ 3200]\n",
      "loss: 0.007195  [ 1216/ 3200]\n",
      "loss: 0.002684  [ 1232/ 3200]\n",
      "loss: 0.003073  [ 1248/ 3200]\n",
      "loss: 0.000392  [ 1264/ 3200]\n",
      "loss: 0.003921  [ 1280/ 3200]\n",
      "loss: 0.002115  [ 1296/ 3200]\n",
      "loss: 0.005043  [ 1312/ 3200]\n",
      "loss: 0.003112  [ 1328/ 3200]\n",
      "loss: 0.001902  [ 1344/ 3200]\n",
      "loss: 0.000625  [ 1360/ 3200]\n",
      "loss: 0.000685  [ 1376/ 3200]\n",
      "loss: 0.004845  [ 1392/ 3200]\n",
      "loss: 0.005575  [ 1408/ 3200]\n",
      "loss: 0.002627  [ 1424/ 3200]\n",
      "loss: 0.003840  [ 1440/ 3200]\n",
      "loss: 0.009697  [ 1456/ 3200]\n",
      "loss: 0.004790  [ 1472/ 3200]\n",
      "loss: 0.003719  [ 1488/ 3200]\n",
      "loss: 0.004781  [ 1504/ 3200]\n",
      "loss: 0.003497  [ 1520/ 3200]\n",
      "loss: 0.005797  [ 1536/ 3200]\n",
      "loss: 0.009033  [ 1552/ 3200]\n",
      "loss: 0.004210  [ 1568/ 3200]\n",
      "loss: 0.004359  [ 1584/ 3200]\n",
      "loss: 0.003735  [ 1600/ 3200]\n",
      "loss: 0.002036  [ 1616/ 3200]\n",
      "loss: 0.001873  [ 1632/ 3200]\n",
      "loss: 0.004920  [ 1648/ 3200]\n",
      "loss: 0.007095  [ 1664/ 3200]\n",
      "loss: 0.005189  [ 1680/ 3200]\n",
      "loss: 0.002782  [ 1696/ 3200]\n",
      "loss: 0.003163  [ 1712/ 3200]\n",
      "loss: 0.002154  [ 1728/ 3200]\n",
      "loss: 0.003196  [ 1744/ 3200]\n",
      "loss: 0.003005  [ 1760/ 3200]\n",
      "loss: 0.003642  [ 1776/ 3200]\n",
      "loss: 0.004066  [ 1792/ 3200]\n",
      "loss: 0.001682  [ 1808/ 3200]\n",
      "loss: 0.006534  [ 1824/ 3200]\n",
      "loss: 0.004549  [ 1840/ 3200]\n",
      "loss: 0.002031  [ 1856/ 3200]\n",
      "loss: 0.002160  [ 1872/ 3200]\n",
      "loss: 0.237832  [ 1888/ 3200]\n",
      "loss: 0.163675  [ 1904/ 3200]\n",
      "loss: 0.297732  [ 1920/ 3200]\n",
      "loss: 0.156618  [ 1936/ 3200]\n",
      "loss: 0.058449  [ 1952/ 3200]\n",
      "loss: 0.008081  [ 1968/ 3200]\n",
      "loss: 0.006574  [ 1984/ 3200]\n",
      "loss: 0.003555  [ 2000/ 3200]\n",
      "loss: 0.001864  [ 2016/ 3200]\n",
      "loss: 0.004784  [ 2032/ 3200]\n",
      "loss: 0.008253  [ 2048/ 3200]\n",
      "loss: 0.003816  [ 2064/ 3200]\n",
      "loss: 0.016028  [ 2080/ 3200]\n",
      "loss: 0.021358  [ 2096/ 3200]\n",
      "loss: 0.002822  [ 2112/ 3200]\n",
      "loss: 0.006154  [ 2128/ 3200]\n",
      "loss: 0.004096  [ 2144/ 3200]\n",
      "loss: 0.004536  [ 2160/ 3200]\n",
      "loss: 0.008527  [ 2176/ 3200]\n",
      "loss: 0.013093  [ 2192/ 3200]\n",
      "loss: 0.000940  [ 2208/ 3200]\n",
      "loss: 0.019802  [ 2224/ 3200]\n",
      "loss: 0.013799  [ 2240/ 3200]\n",
      "loss: 0.005137  [ 2256/ 3200]\n",
      "loss: 0.010246  [ 2272/ 3200]\n",
      "loss: 0.003094  [ 2288/ 3200]\n",
      "loss: 0.008088  [ 2304/ 3200]\n",
      "loss: 0.011406  [ 2320/ 3200]\n",
      "loss: 0.004092  [ 2336/ 3200]\n",
      "loss: 0.008924  [ 2352/ 3200]\n",
      "loss: 0.003017  [ 2368/ 3200]\n",
      "loss: 0.004229  [ 2384/ 3200]\n",
      "loss: 0.007950  [ 2400/ 3200]\n",
      "loss: 0.003011  [ 2416/ 3200]\n",
      "loss: 0.037906  [ 2432/ 3200]\n",
      "loss: 0.030736  [ 2448/ 3200]\n",
      "loss: 0.005861  [ 2464/ 3200]\n",
      "loss: 0.004513  [ 2480/ 3200]\n",
      "loss: 0.002017  [ 2496/ 3200]\n",
      "loss: 0.002237  [ 2512/ 3200]\n",
      "loss: 0.004070  [ 2528/ 3200]\n",
      "loss: 0.002668  [ 2544/ 3200]\n",
      "loss: 0.005706  [ 2560/ 3200]\n",
      "loss: 0.002620  [ 2576/ 3200]\n",
      "loss: 0.003247  [ 2592/ 3200]\n",
      "loss: 0.005631  [ 2608/ 3200]\n",
      "loss: 0.004168  [ 2624/ 3200]\n",
      "loss: 0.006879  [ 2640/ 3200]\n",
      "loss: 0.003578  [ 2656/ 3200]\n",
      "loss: 0.004234  [ 2672/ 3200]\n",
      "loss: 0.002164  [ 2688/ 3200]\n",
      "loss: 0.010385  [ 2704/ 3200]\n",
      "loss: 0.001078  [ 2720/ 3200]\n",
      "loss: 0.004232  [ 2736/ 3200]\n",
      "loss: 0.002878  [ 2752/ 3200]\n",
      "loss: 0.002298  [ 2768/ 3200]\n",
      "loss: 0.013263  [ 2784/ 3200]\n",
      "loss: 0.003236  [ 2800/ 3200]\n",
      "loss: 0.004595  [ 2816/ 3200]\n",
      "loss: 0.003230  [ 2832/ 3200]\n",
      "loss: 0.007310  [ 2848/ 3200]\n",
      "loss: 0.006194  [ 2864/ 3200]\n",
      "loss: 0.007543  [ 2880/ 3200]\n",
      "loss: 0.009250  [ 2896/ 3200]\n",
      "loss: 0.001733  [ 2912/ 3200]\n",
      "loss: 0.008149  [ 2928/ 3200]\n",
      "loss: 0.018439  [ 2944/ 3200]\n",
      "loss: 0.005304  [ 2960/ 3200]\n",
      "loss: 0.006254  [ 2976/ 3200]\n",
      "loss: 0.003810  [ 2992/ 3200]\n",
      "loss: 0.007383  [ 3008/ 3200]\n",
      "loss: 0.004503  [ 3024/ 3200]\n",
      "loss: 0.015892  [ 3040/ 3200]\n",
      "loss: 0.001944  [ 3056/ 3200]\n",
      "loss: 0.008631  [ 3072/ 3200]\n",
      "loss: 0.001368  [ 3088/ 3200]\n",
      "loss: 0.010273  [ 3104/ 3200]\n",
      "loss: 0.006121  [ 3120/ 3200]\n",
      "loss: 0.015392  [ 3136/ 3200]\n",
      "loss: 0.007343  [ 3152/ 3200]\n",
      "loss: 0.003250  [ 3168/ 3200]\n",
      "loss: 0.009236  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.001079  [    0/ 3200]\n",
      "loss: 0.001731  [   16/ 3200]\n",
      "loss: 0.001262  [   32/ 3200]\n",
      "loss: 0.017822  [   48/ 3200]\n",
      "loss: 0.004411  [   64/ 3200]\n",
      "loss: 0.011058  [   80/ 3200]\n",
      "loss: 0.009646  [   96/ 3200]\n",
      "loss: 0.007326  [  112/ 3200]\n",
      "loss: 0.003378  [  128/ 3200]\n",
      "loss: 0.004731  [  144/ 3200]\n",
      "loss: 0.001308  [  160/ 3200]\n",
      "loss: 0.009071  [  176/ 3200]\n",
      "loss: 0.003275  [  192/ 3200]\n",
      "loss: 0.191493  [  208/ 3200]\n",
      "loss: 0.072810  [  224/ 3200]\n",
      "loss: 0.014928  [  240/ 3200]\n",
      "loss: 0.000890  [  256/ 3200]\n",
      "loss: 0.002938  [  272/ 3200]\n",
      "loss: 0.003532  [  288/ 3200]\n",
      "loss: 0.005167  [  304/ 3200]\n",
      "loss: 0.005853  [  320/ 3200]\n",
      "loss: 0.017821  [  336/ 3200]\n",
      "loss: 0.009263  [  352/ 3200]\n",
      "loss: 0.008828  [  368/ 3200]\n",
      "loss: 0.013254  [  384/ 3200]\n",
      "loss: 0.006871  [  400/ 3200]\n",
      "loss: 0.013721  [  416/ 3200]\n",
      "loss: 0.004321  [  432/ 3200]\n",
      "loss: 0.004092  [  448/ 3200]\n",
      "loss: 0.001889  [  464/ 3200]\n",
      "loss: 0.009735  [  480/ 3200]\n",
      "loss: 0.004865  [  496/ 3200]\n",
      "loss: 0.011121  [  512/ 3200]\n",
      "loss: 0.002543  [  528/ 3200]\n",
      "loss: 0.011344  [  544/ 3200]\n",
      "loss: 0.007039  [  560/ 3200]\n",
      "loss: 0.004390  [  576/ 3200]\n",
      "loss: 0.001460  [  592/ 3200]\n",
      "loss: 0.004447  [  608/ 3200]\n",
      "loss: 0.008004  [  624/ 3200]\n",
      "loss: 0.015496  [  640/ 3200]\n",
      "loss: 0.003377  [  656/ 3200]\n",
      "loss: 0.010401  [  672/ 3200]\n",
      "loss: 0.016706  [  688/ 3200]\n",
      "loss: 0.003963  [  704/ 3200]\n",
      "loss: 0.006834  [  720/ 3200]\n",
      "loss: 0.003334  [  736/ 3200]\n",
      "loss: 0.001578  [  752/ 3200]\n",
      "loss: 0.003202  [  768/ 3200]\n",
      "loss: 0.004604  [  784/ 3200]\n",
      "loss: 0.003123  [  800/ 3200]\n",
      "loss: 0.002640  [  816/ 3200]\n",
      "loss: 0.005548  [  832/ 3200]\n",
      "loss: 0.003791  [  848/ 3200]\n",
      "loss: 0.001981  [  864/ 3200]\n",
      "loss: 0.007635  [  880/ 3200]\n",
      "loss: 0.000836  [  896/ 3200]\n",
      "loss: 0.004843  [  912/ 3200]\n",
      "loss: 0.004472  [  928/ 3200]\n",
      "loss: 0.005319  [  944/ 3200]\n",
      "loss: 0.009511  [  960/ 3200]\n",
      "loss: 0.009437  [  976/ 3200]\n",
      "loss: 0.004961  [  992/ 3200]\n",
      "loss: 0.004772  [ 1008/ 3200]\n",
      "loss: 0.002955  [ 1024/ 3200]\n",
      "loss: 0.003784  [ 1040/ 3200]\n",
      "loss: 0.002627  [ 1056/ 3200]\n",
      "loss: 0.003959  [ 1072/ 3200]\n",
      "loss: 0.001835  [ 1088/ 3200]\n",
      "loss: 0.002168  [ 1104/ 3200]\n",
      "loss: 0.006198  [ 1120/ 3200]\n",
      "loss: 0.004974  [ 1136/ 3200]\n",
      "loss: 0.003199  [ 1152/ 3200]\n",
      "loss: 0.003772  [ 1168/ 3200]\n",
      "loss: 0.002265  [ 1184/ 3200]\n",
      "loss: 0.000751  [ 1200/ 3200]\n",
      "loss: 0.005736  [ 1216/ 3200]\n",
      "loss: 0.010719  [ 1232/ 3200]\n",
      "loss: 0.002097  [ 1248/ 3200]\n",
      "loss: 0.009762  [ 1264/ 3200]\n",
      "loss: 0.008436  [ 1280/ 3200]\n",
      "loss: 0.004481  [ 1296/ 3200]\n",
      "loss: 0.002913  [ 1312/ 3200]\n",
      "loss: 0.005337  [ 1328/ 3200]\n",
      "loss: 0.003049  [ 1344/ 3200]\n",
      "loss: 0.003865  [ 1360/ 3200]\n",
      "loss: 0.004063  [ 1376/ 3200]\n",
      "loss: 0.003717  [ 1392/ 3200]\n",
      "loss: 0.006076  [ 1408/ 3200]\n",
      "loss: 0.004350  [ 1424/ 3200]\n",
      "loss: 0.003894  [ 1440/ 3200]\n",
      "loss: 0.006931  [ 1456/ 3200]\n",
      "loss: 0.002575  [ 1472/ 3200]\n",
      "loss: 0.002768  [ 1488/ 3200]\n",
      "loss: 0.009603  [ 1504/ 3200]\n",
      "loss: 0.001064  [ 1520/ 3200]\n",
      "loss: 0.003150  [ 1536/ 3200]\n",
      "loss: 0.002007  [ 1552/ 3200]\n",
      "loss: 0.004255  [ 1568/ 3200]\n",
      "loss: 0.003595  [ 1584/ 3200]\n",
      "loss: 0.007748  [ 1600/ 3200]\n",
      "loss: 0.002576  [ 1616/ 3200]\n",
      "loss: 0.006571  [ 1632/ 3200]\n",
      "loss: 0.004115  [ 1648/ 3200]\n",
      "loss: 0.004498  [ 1664/ 3200]\n",
      "loss: 0.007055  [ 1680/ 3200]\n",
      "loss: 0.004363  [ 1696/ 3200]\n",
      "loss: 0.015575  [ 1712/ 3200]\n",
      "loss: 0.002892  [ 1728/ 3200]\n",
      "loss: 0.003003  [ 1744/ 3200]\n",
      "loss: 0.004907  [ 1760/ 3200]\n",
      "loss: 0.010702  [ 1776/ 3200]\n",
      "loss: 0.004263  [ 1792/ 3200]\n",
      "loss: 0.003147  [ 1808/ 3200]\n",
      "loss: 0.004872  [ 1824/ 3200]\n",
      "loss: 0.001502  [ 1840/ 3200]\n",
      "loss: 0.005045  [ 1856/ 3200]\n",
      "loss: 0.005830  [ 1872/ 3200]\n",
      "loss: 0.001175  [ 1888/ 3200]\n",
      "loss: 0.004190  [ 1904/ 3200]\n",
      "loss: 0.004427  [ 1920/ 3200]\n",
      "loss: 0.004071  [ 1936/ 3200]\n",
      "loss: 0.002226  [ 1952/ 3200]\n",
      "loss: 0.002963  [ 1968/ 3200]\n",
      "loss: 0.005347  [ 1984/ 3200]\n",
      "loss: 0.002352  [ 2000/ 3200]\n",
      "loss: 0.002321  [ 2016/ 3200]\n",
      "loss: 0.002493  [ 2032/ 3200]\n",
      "loss: 0.001452  [ 2048/ 3200]\n",
      "loss: 0.001309  [ 2064/ 3200]\n",
      "loss: 0.003367  [ 2080/ 3200]\n",
      "loss: 0.007060  [ 2096/ 3200]\n",
      "loss: 0.002516  [ 2112/ 3200]\n",
      "loss: 0.001917  [ 2128/ 3200]\n",
      "loss: 0.003066  [ 2144/ 3200]\n",
      "loss: 0.006685  [ 2160/ 3200]\n",
      "loss: 0.004453  [ 2176/ 3200]\n",
      "loss: 0.003834  [ 2192/ 3200]\n",
      "loss: 0.002942  [ 2208/ 3200]\n",
      "loss: 0.001863  [ 2224/ 3200]\n",
      "loss: 0.001823  [ 2240/ 3200]\n",
      "loss: 0.005614  [ 2256/ 3200]\n",
      "loss: 0.008417  [ 2272/ 3200]\n",
      "loss: 0.008888  [ 2288/ 3200]\n",
      "loss: 0.003922  [ 2304/ 3200]\n",
      "loss: 0.003726  [ 2320/ 3200]\n",
      "loss: 0.003983  [ 2336/ 3200]\n",
      "loss: 0.002985  [ 2352/ 3200]\n",
      "loss: 0.000619  [ 2368/ 3200]\n",
      "loss: 0.001773  [ 2384/ 3200]\n",
      "loss: 0.006245  [ 2400/ 3200]\n",
      "loss: 0.008188  [ 2416/ 3200]\n",
      "loss: 0.006366  [ 2432/ 3200]\n",
      "loss: 0.003830  [ 2448/ 3200]\n",
      "loss: 0.006928  [ 2464/ 3200]\n",
      "loss: 0.001347  [ 2480/ 3200]\n",
      "loss: 0.073384  [ 2496/ 3200]\n",
      "loss: 0.008702  [ 2512/ 3200]\n",
      "loss: 0.008005  [ 2528/ 3200]\n",
      "loss: 0.006450  [ 2544/ 3200]\n",
      "loss: 0.006181  [ 2560/ 3200]\n",
      "loss: 0.003635  [ 2576/ 3200]\n",
      "loss: 0.015740  [ 2592/ 3200]\n",
      "loss: 0.003849  [ 2608/ 3200]\n",
      "loss: 0.004703  [ 2624/ 3200]\n",
      "loss: 0.001820  [ 2640/ 3200]\n",
      "loss: 0.001744  [ 2656/ 3200]\n",
      "loss: 0.001632  [ 2672/ 3200]\n",
      "loss: 0.012473  [ 2688/ 3200]\n",
      "loss: 0.001519  [ 2704/ 3200]\n",
      "loss: 0.003792  [ 2720/ 3200]\n",
      "loss: 0.001497  [ 2736/ 3200]\n",
      "loss: 0.008517  [ 2752/ 3200]\n",
      "loss: 0.007781  [ 2768/ 3200]\n",
      "loss: 0.001349  [ 2784/ 3200]\n",
      "loss: 0.007234  [ 2800/ 3200]\n",
      "loss: 0.002549  [ 2816/ 3200]\n",
      "loss: 0.003907  [ 2832/ 3200]\n",
      "loss: 0.003306  [ 2848/ 3200]\n",
      "loss: 0.003057  [ 2864/ 3200]\n",
      "loss: 0.010526  [ 2880/ 3200]\n",
      "loss: 0.003505  [ 2896/ 3200]\n",
      "loss: 0.001648  [ 2912/ 3200]\n",
      "loss: 0.003696  [ 2928/ 3200]\n",
      "loss: 0.004775  [ 2944/ 3200]\n",
      "loss: 0.002146  [ 2960/ 3200]\n",
      "loss: 0.004241  [ 2976/ 3200]\n",
      "loss: 0.008649  [ 2992/ 3200]\n",
      "loss: 0.002626  [ 3008/ 3200]\n",
      "loss: 0.003317  [ 3024/ 3200]\n",
      "loss: 0.005602  [ 3040/ 3200]\n",
      "loss: 0.009510  [ 3056/ 3200]\n",
      "loss: 0.001224  [ 3072/ 3200]\n",
      "loss: 0.002920  [ 3088/ 3200]\n",
      "loss: 0.001465  [ 3104/ 3200]\n",
      "loss: 0.004150  [ 3120/ 3200]\n",
      "loss: 0.002414  [ 3136/ 3200]\n",
      "loss: 0.005896  [ 3152/ 3200]\n",
      "loss: 0.001631  [ 3168/ 3200]\n",
      "loss: 0.000802  [ 3184/ 3200]\n",
      "\n",
      "Test Error:\n",
      "Avg loss               : 0.089290\n",
      "f1 macro averaged score: 0.742370\n",
      "Accuracy               : 73.5%\n",
      "Confusion matrix       :\n",
      "tensor([[269,  18,   3,   7],\n",
      "        [ 14, 205,  26,  79],\n",
      "        [  4,  39, 274,  39],\n",
      "        [ 16,  89,  30, 264]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch_seed(0)\n",
    "cnn_model = Net().to(device)\n",
    "\n",
    "learning_rate = 0.002\n",
    "optimizer = Adagrad(params=cnn_model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "cnn_model = train_convolutional_neural_network(epochs, optimizer, train_dataloader, loss_function, cnn_model, True)\n",
    "results = test_convolutional_neural_network(test_dataloader, loss_function, cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TChQZwYN3FbT",
    "outputId": "63c49957-993b-4f59-e8c3-c1841812c96b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 0.772601  [ 1312/ 3200]\n",
      "loss: 0.603267  [ 1328/ 3200]\n",
      "loss: 0.389988  [ 1344/ 3200]\n",
      "loss: 0.312300  [ 1360/ 3200]\n",
      "loss: 0.649883  [ 1376/ 3200]\n",
      "loss: 0.402415  [ 1392/ 3200]\n",
      "loss: 0.379302  [ 1408/ 3200]\n",
      "loss: 0.487795  [ 1424/ 3200]\n",
      "loss: 0.817650  [ 1440/ 3200]\n",
      "loss: 0.879129  [ 1456/ 3200]\n",
      "loss: 0.573630  [ 1472/ 3200]\n",
      "loss: 0.795514  [ 1488/ 3200]\n",
      "loss: 0.538295  [ 1504/ 3200]\n",
      "loss: 0.242006  [ 1520/ 3200]\n",
      "loss: 0.562402  [ 1536/ 3200]\n",
      "loss: 0.397960  [ 1552/ 3200]\n",
      "loss: 0.680610  [ 1568/ 3200]\n",
      "loss: 0.461949  [ 1584/ 3200]\n",
      "loss: 0.516605  [ 1600/ 3200]\n",
      "loss: 0.414831  [ 1616/ 3200]\n",
      "loss: 0.348879  [ 1632/ 3200]\n",
      "loss: 0.289238  [ 1648/ 3200]\n",
      "loss: 0.464025  [ 1664/ 3200]\n",
      "loss: 0.299047  [ 1680/ 3200]\n",
      "loss: 0.517515  [ 1696/ 3200]\n",
      "loss: 0.367538  [ 1712/ 3200]\n",
      "loss: 0.354482  [ 1728/ 3200]\n",
      "loss: 0.381026  [ 1744/ 3200]\n",
      "loss: 0.514871  [ 1760/ 3200]\n",
      "loss: 0.283617  [ 1776/ 3200]\n",
      "loss: 0.847765  [ 1792/ 3200]\n",
      "loss: 0.800440  [ 1808/ 3200]\n",
      "loss: 0.441479  [ 1824/ 3200]\n",
      "loss: 0.393025  [ 1840/ 3200]\n",
      "loss: 0.506551  [ 1856/ 3200]\n",
      "loss: 0.707691  [ 1872/ 3200]\n",
      "loss: 0.276830  [ 1888/ 3200]\n",
      "loss: 0.212400  [ 1904/ 3200]\n",
      "loss: 0.353439  [ 1920/ 3200]\n",
      "loss: 0.342095  [ 1936/ 3200]\n",
      "loss: 0.416285  [ 1952/ 3200]\n",
      "loss: 1.338422  [ 1968/ 3200]\n",
      "loss: 0.333762  [ 1984/ 3200]\n",
      "loss: 0.399717  [ 2000/ 3200]\n",
      "loss: 0.569322  [ 2016/ 3200]\n",
      "loss: 0.518103  [ 2032/ 3200]\n",
      "loss: 0.437927  [ 2048/ 3200]\n",
      "loss: 0.287604  [ 2064/ 3200]\n",
      "loss: 0.506500  [ 2080/ 3200]\n",
      "loss: 0.696222  [ 2096/ 3200]\n",
      "loss: 0.314076  [ 2112/ 3200]\n",
      "loss: 0.357327  [ 2128/ 3200]\n",
      "loss: 0.705183  [ 2144/ 3200]\n",
      "loss: 0.912338  [ 2160/ 3200]\n",
      "loss: 0.869701  [ 2176/ 3200]\n",
      "loss: 0.681277  [ 2192/ 3200]\n",
      "loss: 0.614283  [ 2208/ 3200]\n",
      "loss: 0.846037  [ 2224/ 3200]\n",
      "loss: 0.747912  [ 2240/ 3200]\n",
      "loss: 0.622996  [ 2256/ 3200]\n",
      "loss: 0.959305  [ 2272/ 3200]\n",
      "loss: 0.554042  [ 2288/ 3200]\n",
      "loss: 0.866651  [ 2304/ 3200]\n",
      "loss: 0.481421  [ 2320/ 3200]\n",
      "loss: 0.481797  [ 2336/ 3200]\n",
      "loss: 1.029407  [ 2352/ 3200]\n",
      "loss: 0.548087  [ 2368/ 3200]\n",
      "loss: 0.532413  [ 2384/ 3200]\n",
      "loss: 0.702358  [ 2400/ 3200]\n",
      "loss: 0.701010  [ 2416/ 3200]\n",
      "loss: 0.401129  [ 2432/ 3200]\n",
      "loss: 0.512014  [ 2448/ 3200]\n",
      "loss: 0.221664  [ 2464/ 3200]\n",
      "loss: 0.596591  [ 2480/ 3200]\n",
      "loss: 0.528545  [ 2496/ 3200]\n",
      "loss: 0.639882  [ 2512/ 3200]\n",
      "loss: 0.345012  [ 2528/ 3200]\n",
      "loss: 0.633696  [ 2544/ 3200]\n",
      "loss: 0.292065  [ 2560/ 3200]\n",
      "loss: 0.370461  [ 2576/ 3200]\n",
      "loss: 0.365508  [ 2592/ 3200]\n",
      "loss: 0.388842  [ 2608/ 3200]\n",
      "loss: 0.803300  [ 2624/ 3200]\n",
      "loss: 0.479068  [ 2640/ 3200]\n",
      "loss: 0.654812  [ 2656/ 3200]\n",
      "loss: 0.268814  [ 2672/ 3200]\n",
      "loss: 0.477607  [ 2688/ 3200]\n",
      "loss: 0.444235  [ 2704/ 3200]\n",
      "loss: 0.724645  [ 2720/ 3200]\n",
      "loss: 0.404299  [ 2736/ 3200]\n",
      "loss: 0.569152  [ 2752/ 3200]\n",
      "loss: 0.362218  [ 2768/ 3200]\n",
      "loss: 0.408683  [ 2784/ 3200]\n",
      "loss: 0.188690  [ 2800/ 3200]\n",
      "loss: 0.628834  [ 2816/ 3200]\n",
      "loss: 0.208608  [ 2832/ 3200]\n",
      "loss: 0.540076  [ 2848/ 3200]\n",
      "loss: 0.244259  [ 2864/ 3200]\n",
      "loss: 0.239665  [ 2880/ 3200]\n",
      "loss: 0.798999  [ 2896/ 3200]\n",
      "loss: 0.672903  [ 2912/ 3200]\n",
      "loss: 0.272219  [ 2928/ 3200]\n",
      "loss: 0.578040  [ 2944/ 3200]\n",
      "loss: 0.461077  [ 2960/ 3200]\n",
      "loss: 0.346330  [ 2976/ 3200]\n",
      "loss: 0.389838  [ 2992/ 3200]\n",
      "loss: 0.532002  [ 3008/ 3200]\n",
      "loss: 0.417661  [ 3024/ 3200]\n",
      "loss: 0.563619  [ 3040/ 3200]\n",
      "loss: 0.279548  [ 3056/ 3200]\n",
      "loss: 0.443133  [ 3072/ 3200]\n",
      "loss: 0.758234  [ 3088/ 3200]\n",
      "loss: 0.442722  [ 3104/ 3200]\n",
      "loss: 0.420132  [ 3120/ 3200]\n",
      "loss: 0.514042  [ 3136/ 3200]\n",
      "loss: 0.825135  [ 3152/ 3200]\n",
      "loss: 0.427677  [ 3168/ 3200]\n",
      "loss: 1.111708  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 7\n",
      "-----------------------------\n",
      "loss: 0.604259  [    0/ 3200]\n",
      "loss: 0.545092  [   16/ 3200]\n",
      "loss: 0.385484  [   32/ 3200]\n",
      "loss: 0.553044  [   48/ 3200]\n",
      "loss: 0.483814  [   64/ 3200]\n",
      "loss: 0.418348  [   80/ 3200]\n",
      "loss: 0.608680  [   96/ 3200]\n",
      "loss: 0.508639  [  112/ 3200]\n",
      "loss: 0.642649  [  128/ 3200]\n",
      "loss: 0.849038  [  144/ 3200]\n",
      "loss: 0.729633  [  160/ 3200]\n",
      "loss: 0.270022  [  176/ 3200]\n",
      "loss: 0.480275  [  192/ 3200]\n",
      "loss: 0.368840  [  208/ 3200]\n",
      "loss: 0.475082  [  224/ 3200]\n",
      "loss: 0.777873  [  240/ 3200]\n",
      "loss: 0.519397  [  256/ 3200]\n",
      "loss: 0.436087  [  272/ 3200]\n",
      "loss: 0.345529  [  288/ 3200]\n",
      "loss: 0.423247  [  304/ 3200]\n",
      "loss: 0.454200  [  320/ 3200]\n",
      "loss: 0.698564  [  336/ 3200]\n",
      "loss: 0.410207  [  352/ 3200]\n",
      "loss: 0.499379  [  368/ 3200]\n",
      "loss: 0.386979  [  384/ 3200]\n",
      "loss: 0.569373  [  400/ 3200]\n",
      "loss: 0.624481  [  416/ 3200]\n",
      "loss: 0.821198  [  432/ 3200]\n",
      "loss: 0.543597  [  448/ 3200]\n",
      "loss: 0.428619  [  464/ 3200]\n",
      "loss: 0.266225  [  480/ 3200]\n",
      "loss: 0.595347  [  496/ 3200]\n",
      "loss: 0.646592  [  512/ 3200]\n",
      "loss: 0.451592  [  528/ 3200]\n",
      "loss: 0.422989  [  544/ 3200]\n",
      "loss: 0.241556  [  560/ 3200]\n",
      "loss: 0.617520  [  576/ 3200]\n",
      "loss: 0.510072  [  592/ 3200]\n",
      "loss: 0.218113  [  608/ 3200]\n",
      "loss: 0.532976  [  624/ 3200]\n",
      "loss: 0.299912  [  640/ 3200]\n",
      "loss: 0.568298  [  656/ 3200]\n",
      "loss: 0.711312  [  672/ 3200]\n",
      "loss: 0.545256  [  688/ 3200]\n",
      "loss: 0.254038  [  704/ 3200]\n",
      "loss: 0.548665  [  720/ 3200]\n",
      "loss: 0.256588  [  736/ 3200]\n",
      "loss: 0.459878  [  752/ 3200]\n",
      "loss: 0.434090  [  768/ 3200]\n",
      "loss: 0.293704  [  784/ 3200]\n",
      "loss: 0.407704  [  800/ 3200]\n",
      "loss: 0.615826  [  816/ 3200]\n",
      "loss: 0.466815  [  832/ 3200]\n",
      "loss: 0.356685  [  848/ 3200]\n",
      "loss: 0.639793  [  864/ 3200]\n",
      "loss: 0.324015  [  880/ 3200]\n",
      "loss: 0.332584  [  896/ 3200]\n",
      "loss: 0.414069  [  912/ 3200]\n",
      "loss: 0.795717  [  928/ 3200]\n",
      "loss: 0.315515  [  944/ 3200]\n",
      "loss: 0.443512  [  960/ 3200]\n",
      "loss: 0.519152  [  976/ 3200]\n",
      "loss: 0.147084  [  992/ 3200]\n",
      "loss: 0.294967  [ 1008/ 3200]\n",
      "loss: 0.510366  [ 1024/ 3200]\n",
      "loss: 0.482535  [ 1040/ 3200]\n",
      "loss: 0.357495  [ 1056/ 3200]\n",
      "loss: 0.313194  [ 1072/ 3200]\n",
      "loss: 0.540125  [ 1088/ 3200]\n",
      "loss: 0.336626  [ 1104/ 3200]\n",
      "loss: 0.213910  [ 1120/ 3200]\n",
      "loss: 0.658889  [ 1136/ 3200]\n",
      "loss: 0.584139  [ 1152/ 3200]\n",
      "loss: 0.521574  [ 1168/ 3200]\n",
      "loss: 0.483902  [ 1184/ 3200]\n",
      "loss: 0.853015  [ 1200/ 3200]\n",
      "loss: 0.662796  [ 1216/ 3200]\n",
      "loss: 0.395862  [ 1232/ 3200]\n",
      "loss: 0.368869  [ 1248/ 3200]\n",
      "loss: 0.228648  [ 1264/ 3200]\n",
      "loss: 0.449919  [ 1280/ 3200]\n",
      "loss: 0.129332  [ 1296/ 3200]\n",
      "loss: 0.628475  [ 1312/ 3200]\n",
      "loss: 0.288767  [ 1328/ 3200]\n",
      "loss: 0.598982  [ 1344/ 3200]\n",
      "loss: 0.456879  [ 1360/ 3200]\n",
      "loss: 0.975621  [ 1376/ 3200]\n",
      "loss: 0.977767  [ 1392/ 3200]\n",
      "loss: 0.381505  [ 1408/ 3200]\n",
      "loss: 0.295749  [ 1424/ 3200]\n",
      "loss: 0.538856  [ 1440/ 3200]\n",
      "loss: 0.447439  [ 1456/ 3200]\n",
      "loss: 0.537860  [ 1472/ 3200]\n",
      "loss: 0.540695  [ 1488/ 3200]\n",
      "loss: 0.625507  [ 1504/ 3200]\n",
      "loss: 0.292194  [ 1520/ 3200]\n",
      "loss: 0.715329  [ 1536/ 3200]\n",
      "loss: 0.501448  [ 1552/ 3200]\n",
      "loss: 0.509650  [ 1568/ 3200]\n",
      "loss: 0.560599  [ 1584/ 3200]\n",
      "loss: 0.188931  [ 1600/ 3200]\n",
      "loss: 0.430674  [ 1616/ 3200]\n",
      "loss: 0.269021  [ 1632/ 3200]\n",
      "loss: 1.210183  [ 1648/ 3200]\n",
      "loss: 0.602831  [ 1664/ 3200]\n",
      "loss: 0.576143  [ 1680/ 3200]\n",
      "loss: 0.345341  [ 1696/ 3200]\n",
      "loss: 0.365588  [ 1712/ 3200]\n",
      "loss: 0.384479  [ 1728/ 3200]\n",
      "loss: 0.551074  [ 1744/ 3200]\n",
      "loss: 0.299750  [ 1760/ 3200]\n",
      "loss: 0.416040  [ 1776/ 3200]\n",
      "loss: 0.330834  [ 1792/ 3200]\n",
      "loss: 0.336526  [ 1808/ 3200]\n",
      "loss: 0.820294  [ 1824/ 3200]\n",
      "loss: 0.172824  [ 1840/ 3200]\n",
      "loss: 0.426919  [ 1856/ 3200]\n",
      "loss: 0.726767  [ 1872/ 3200]\n",
      "loss: 0.420032  [ 1888/ 3200]\n",
      "loss: 0.449523  [ 1904/ 3200]\n",
      "loss: 0.231814  [ 1920/ 3200]\n",
      "loss: 0.208110  [ 1936/ 3200]\n",
      "loss: 0.266265  [ 1952/ 3200]\n",
      "loss: 0.442198  [ 1968/ 3200]\n",
      "loss: 0.220951  [ 1984/ 3200]\n",
      "loss: 0.234713  [ 2000/ 3200]\n",
      "loss: 0.301117  [ 2016/ 3200]\n",
      "loss: 0.623503  [ 2032/ 3200]\n",
      "loss: 0.302487  [ 2048/ 3200]\n",
      "loss: 0.282601  [ 2064/ 3200]\n",
      "loss: 0.161252  [ 2080/ 3200]\n",
      "loss: 0.653037  [ 2096/ 3200]\n",
      "loss: 0.404168  [ 2112/ 3200]\n",
      "loss: 0.499070  [ 2128/ 3200]\n",
      "loss: 0.646386  [ 2144/ 3200]\n",
      "loss: 0.412843  [ 2160/ 3200]\n",
      "loss: 0.620725  [ 2176/ 3200]\n",
      "loss: 0.628862  [ 2192/ 3200]\n",
      "loss: 0.413971  [ 2208/ 3200]\n",
      "loss: 0.444264  [ 2224/ 3200]\n",
      "loss: 0.337591  [ 2240/ 3200]\n",
      "loss: 0.724134  [ 2256/ 3200]\n",
      "loss: 1.307406  [ 2272/ 3200]\n",
      "loss: 0.652496  [ 2288/ 3200]\n",
      "loss: 0.449997  [ 2304/ 3200]\n",
      "loss: 0.704740  [ 2320/ 3200]\n",
      "loss: 0.502809  [ 2336/ 3200]\n",
      "loss: 0.573333  [ 2352/ 3200]\n",
      "loss: 0.554100  [ 2368/ 3200]\n",
      "loss: 0.318289  [ 2384/ 3200]\n",
      "loss: 0.722339  [ 2400/ 3200]\n",
      "loss: 0.582307  [ 2416/ 3200]\n",
      "loss: 0.402946  [ 2432/ 3200]\n",
      "loss: 0.441908  [ 2448/ 3200]\n",
      "loss: 0.301799  [ 2464/ 3200]\n",
      "loss: 0.458222  [ 2480/ 3200]\n",
      "loss: 0.594831  [ 2496/ 3200]\n",
      "loss: 0.633888  [ 2512/ 3200]\n",
      "loss: 0.421141  [ 2528/ 3200]\n",
      "loss: 0.266656  [ 2544/ 3200]\n",
      "loss: 0.390858  [ 2560/ 3200]\n",
      "loss: 0.906136  [ 2576/ 3200]\n",
      "loss: 0.497526  [ 2592/ 3200]\n",
      "loss: 0.241408  [ 2608/ 3200]\n",
      "loss: 0.365770  [ 2624/ 3200]\n",
      "loss: 0.140950  [ 2640/ 3200]\n",
      "loss: 0.451416  [ 2656/ 3200]\n",
      "loss: 0.350097  [ 2672/ 3200]\n",
      "loss: 0.479567  [ 2688/ 3200]\n",
      "loss: 0.265332  [ 2704/ 3200]\n",
      "loss: 0.202554  [ 2720/ 3200]\n",
      "loss: 0.317996  [ 2736/ 3200]\n",
      "loss: 0.496376  [ 2752/ 3200]\n",
      "loss: 0.337477  [ 2768/ 3200]\n",
      "loss: 0.204005  [ 2784/ 3200]\n",
      "loss: 0.468665  [ 2800/ 3200]\n",
      "loss: 0.295616  [ 2816/ 3200]\n",
      "loss: 0.147537  [ 2832/ 3200]\n",
      "loss: 0.238621  [ 2848/ 3200]\n",
      "loss: 0.902326  [ 2864/ 3200]\n",
      "loss: 0.651899  [ 2880/ 3200]\n",
      "loss: 0.282238  [ 2896/ 3200]\n",
      "loss: 0.389708  [ 2912/ 3200]\n",
      "loss: 0.465015  [ 2928/ 3200]\n",
      "loss: 0.217147  [ 2944/ 3200]\n",
      "loss: 0.236606  [ 2960/ 3200]\n",
      "loss: 0.377602  [ 2976/ 3200]\n",
      "loss: 0.259272  [ 2992/ 3200]\n",
      "loss: 0.655753  [ 3008/ 3200]\n",
      "loss: 0.368078  [ 3024/ 3200]\n",
      "loss: 0.432882  [ 3040/ 3200]\n",
      "loss: 0.685873  [ 3056/ 3200]\n",
      "loss: 0.541975  [ 3072/ 3200]\n",
      "loss: 0.263464  [ 3088/ 3200]\n",
      "loss: 0.085639  [ 3104/ 3200]\n",
      "loss: 0.600752  [ 3120/ 3200]\n",
      "loss: 0.311122  [ 3136/ 3200]\n",
      "loss: 0.452313  [ 3152/ 3200]\n",
      "loss: 0.294717  [ 3168/ 3200]\n",
      "loss: 0.449333  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 0.730692  [    0/ 3200]\n",
      "loss: 0.418290  [   16/ 3200]\n",
      "loss: 0.415663  [   32/ 3200]\n",
      "loss: 0.273909  [   48/ 3200]\n",
      "loss: 0.385639  [   64/ 3200]\n",
      "loss: 0.660233  [   80/ 3200]\n",
      "loss: 0.343834  [   96/ 3200]\n",
      "loss: 0.290912  [  112/ 3200]\n",
      "loss: 0.342789  [  128/ 3200]\n",
      "loss: 0.519524  [  144/ 3200]\n",
      "loss: 0.401969  [  160/ 3200]\n",
      "loss: 0.543492  [  176/ 3200]\n",
      "loss: 0.686928  [  192/ 3200]\n",
      "loss: 0.440355  [  208/ 3200]\n",
      "loss: 0.300967  [  224/ 3200]\n",
      "loss: 0.391135  [  240/ 3200]\n",
      "loss: 0.572804  [  256/ 3200]\n",
      "loss: 0.405310  [  272/ 3200]\n",
      "loss: 0.362865  [  288/ 3200]\n",
      "loss: 0.300015  [  304/ 3200]\n",
      "loss: 0.691227  [  320/ 3200]\n",
      "loss: 0.457319  [  336/ 3200]\n",
      "loss: 0.398999  [  352/ 3200]\n",
      "loss: 0.609491  [  368/ 3200]\n",
      "loss: 0.281662  [  384/ 3200]\n",
      "loss: 0.461720  [  400/ 3200]\n",
      "loss: 0.479036  [  416/ 3200]\n",
      "loss: 0.802609  [  432/ 3200]\n",
      "loss: 0.228982  [  448/ 3200]\n",
      "loss: 0.576552  [  464/ 3200]\n",
      "loss: 0.196686  [  480/ 3200]\n",
      "loss: 0.254992  [  496/ 3200]\n",
      "loss: 0.515286  [  512/ 3200]\n",
      "loss: 0.365727  [  528/ 3200]\n",
      "loss: 0.707812  [  544/ 3200]\n",
      "loss: 0.296174  [  560/ 3200]\n",
      "loss: 0.406022  [  576/ 3200]\n",
      "loss: 0.401028  [  592/ 3200]\n",
      "loss: 0.335310  [  608/ 3200]\n",
      "loss: 0.365531  [  624/ 3200]\n",
      "loss: 0.305278  [  640/ 3200]\n",
      "loss: 0.732411  [  656/ 3200]\n",
      "loss: 0.493632  [  672/ 3200]\n",
      "loss: 0.536010  [  688/ 3200]\n",
      "loss: 0.283907  [  704/ 3200]\n",
      "loss: 0.760826  [  720/ 3200]\n",
      "loss: 0.221942  [  736/ 3200]\n",
      "loss: 0.302358  [  752/ 3200]\n",
      "loss: 0.566054  [  768/ 3200]\n",
      "loss: 0.303111  [  784/ 3200]\n",
      "loss: 0.180629  [  800/ 3200]\n",
      "loss: 0.512565  [  816/ 3200]\n",
      "loss: 0.248846  [  832/ 3200]\n",
      "loss: 0.277159  [  848/ 3200]\n",
      "loss: 0.739590  [  864/ 3200]\n",
      "loss: 0.487173  [  880/ 3200]\n",
      "loss: 0.270477  [  896/ 3200]\n",
      "loss: 0.279367  [  912/ 3200]\n",
      "loss: 0.688743  [  928/ 3200]\n",
      "loss: 0.384696  [  944/ 3200]\n",
      "loss: 0.383562  [  960/ 3200]\n",
      "loss: 0.104604  [  976/ 3200]\n",
      "loss: 0.233280  [  992/ 3200]\n",
      "loss: 0.552496  [ 1008/ 3200]\n",
      "loss: 0.418627  [ 1024/ 3200]\n",
      "loss: 0.513200  [ 1040/ 3200]\n",
      "loss: 0.234934  [ 1056/ 3200]\n",
      "loss: 0.342996  [ 1072/ 3200]\n",
      "loss: 0.346557  [ 1088/ 3200]\n",
      "loss: 0.513984  [ 1104/ 3200]\n",
      "loss: 0.170752  [ 1120/ 3200]\n",
      "loss: 0.238275  [ 1136/ 3200]\n",
      "loss: 0.262568  [ 1152/ 3200]\n",
      "loss: 0.774727  [ 1168/ 3200]\n",
      "loss: 0.525797  [ 1184/ 3200]\n",
      "loss: 0.738121  [ 1200/ 3200]\n",
      "loss: 0.355354  [ 1216/ 3200]\n",
      "loss: 0.268315  [ 1232/ 3200]\n",
      "loss: 0.382326  [ 1248/ 3200]\n",
      "loss: 0.210766  [ 1264/ 3200]\n",
      "loss: 0.677180  [ 1280/ 3200]\n",
      "loss: 0.409166  [ 1296/ 3200]\n",
      "loss: 0.343075  [ 1312/ 3200]\n",
      "loss: 0.345202  [ 1328/ 3200]\n",
      "loss: 0.293614  [ 1344/ 3200]\n",
      "loss: 0.247206  [ 1360/ 3200]\n",
      "loss: 0.253629  [ 1376/ 3200]\n",
      "loss: 0.224049  [ 1392/ 3200]\n",
      "loss: 0.348924  [ 1408/ 3200]\n",
      "loss: 0.352133  [ 1424/ 3200]\n",
      "loss: 0.734014  [ 1440/ 3200]\n",
      "loss: 0.993742  [ 1456/ 3200]\n",
      "loss: 0.155026  [ 1472/ 3200]\n",
      "loss: 0.388245  [ 1488/ 3200]\n",
      "loss: 0.216691  [ 1504/ 3200]\n",
      "loss: 0.630468  [ 1520/ 3200]\n",
      "loss: 0.469183  [ 1536/ 3200]\n",
      "loss: 0.408424  [ 1552/ 3200]\n",
      "loss: 0.249624  [ 1568/ 3200]\n",
      "loss: 0.335143  [ 1584/ 3200]\n",
      "loss: 0.358867  [ 1600/ 3200]\n",
      "loss: 0.139657  [ 1616/ 3200]\n",
      "loss: 0.170215  [ 1632/ 3200]\n",
      "loss: 0.250187  [ 1648/ 3200]\n",
      "loss: 0.791692  [ 1664/ 3200]\n",
      "loss: 0.856992  [ 1680/ 3200]\n",
      "loss: 0.822764  [ 1696/ 3200]\n",
      "loss: 0.438748  [ 1712/ 3200]\n",
      "loss: 0.298408  [ 1728/ 3200]\n",
      "loss: 0.120345  [ 1744/ 3200]\n",
      "loss: 0.260800  [ 1760/ 3200]\n",
      "loss: 0.622354  [ 1776/ 3200]\n",
      "loss: 0.224949  [ 1792/ 3200]\n",
      "loss: 0.342468  [ 1808/ 3200]\n",
      "loss: 0.525311  [ 1824/ 3200]\n",
      "loss: 0.330075  [ 1840/ 3200]\n",
      "loss: 0.341129  [ 1856/ 3200]\n",
      "loss: 0.340783  [ 1872/ 3200]\n",
      "loss: 0.240855  [ 1888/ 3200]\n",
      "loss: 0.339070  [ 1904/ 3200]\n",
      "loss: 0.384591  [ 1920/ 3200]\n",
      "loss: 0.635462  [ 1936/ 3200]\n",
      "loss: 0.216663  [ 1952/ 3200]\n",
      "loss: 0.424211  [ 1968/ 3200]\n",
      "loss: 0.362593  [ 1984/ 3200]\n",
      "loss: 0.191772  [ 2000/ 3200]\n",
      "loss: 0.387619  [ 2016/ 3200]\n",
      "loss: 0.393991  [ 2032/ 3200]\n",
      "loss: 0.484420  [ 2048/ 3200]\n",
      "loss: 0.289537  [ 2064/ 3200]\n",
      "loss: 0.467406  [ 2080/ 3200]\n",
      "loss: 0.689857  [ 2096/ 3200]\n",
      "loss: 0.372905  [ 2112/ 3200]\n",
      "loss: 0.485377  [ 2128/ 3200]\n",
      "loss: 0.465392  [ 2144/ 3200]\n",
      "loss: 0.594780  [ 2160/ 3200]\n",
      "loss: 0.258261  [ 2176/ 3200]\n",
      "loss: 0.621469  [ 2192/ 3200]\n",
      "loss: 0.209397  [ 2208/ 3200]\n",
      "loss: 0.398597  [ 2224/ 3200]\n",
      "loss: 0.370424  [ 2240/ 3200]\n",
      "loss: 0.418812  [ 2256/ 3200]\n",
      "loss: 0.326631  [ 2272/ 3200]\n",
      "loss: 0.670571  [ 2288/ 3200]\n",
      "loss: 0.518925  [ 2304/ 3200]\n",
      "loss: 0.601585  [ 2320/ 3200]\n",
      "loss: 0.391033  [ 2336/ 3200]\n",
      "loss: 0.389838  [ 2352/ 3200]\n",
      "loss: 0.480824  [ 2368/ 3200]\n",
      "loss: 0.508403  [ 2384/ 3200]\n",
      "loss: 0.411191  [ 2400/ 3200]\n",
      "loss: 0.308113  [ 2416/ 3200]\n",
      "loss: 0.633500  [ 2432/ 3200]\n",
      "loss: 0.293617  [ 2448/ 3200]\n",
      "loss: 0.369657  [ 2464/ 3200]\n",
      "loss: 0.243679  [ 2480/ 3200]\n",
      "loss: 0.177325  [ 2496/ 3200]\n",
      "loss: 0.461775  [ 2512/ 3200]\n",
      "loss: 0.207772  [ 2528/ 3200]\n",
      "loss: 0.278778  [ 2544/ 3200]\n",
      "loss: 0.374944  [ 2560/ 3200]\n",
      "loss: 0.565897  [ 2576/ 3200]\n",
      "loss: 0.477087  [ 2592/ 3200]\n",
      "loss: 0.679063  [ 2608/ 3200]\n",
      "loss: 0.311628  [ 2624/ 3200]\n",
      "loss: 0.346700  [ 2640/ 3200]\n",
      "loss: 0.136461  [ 2656/ 3200]\n",
      "loss: 0.240950  [ 2672/ 3200]\n",
      "loss: 0.625037  [ 2688/ 3200]\n",
      "loss: 0.843279  [ 2704/ 3200]\n",
      "loss: 0.550582  [ 2720/ 3200]\n",
      "loss: 0.213174  [ 2736/ 3200]\n",
      "loss: 0.697197  [ 2752/ 3200]\n",
      "loss: 0.273543  [ 2768/ 3200]\n",
      "loss: 0.524656  [ 2784/ 3200]\n",
      "loss: 0.166599  [ 2800/ 3200]\n",
      "loss: 0.370341  [ 2816/ 3200]\n",
      "loss: 0.240677  [ 2832/ 3200]\n",
      "loss: 0.772440  [ 2848/ 3200]\n",
      "loss: 0.322903  [ 2864/ 3200]\n",
      "loss: 0.486742  [ 2880/ 3200]\n",
      "loss: 0.411417  [ 2896/ 3200]\n",
      "loss: 0.796415  [ 2912/ 3200]\n",
      "loss: 0.533057  [ 2928/ 3200]\n",
      "loss: 0.354326  [ 2944/ 3200]\n",
      "loss: 0.504860  [ 2960/ 3200]\n",
      "loss: 0.220557  [ 2976/ 3200]\n",
      "loss: 0.277409  [ 2992/ 3200]\n",
      "loss: 0.268525  [ 3008/ 3200]\n",
      "loss: 0.143743  [ 3024/ 3200]\n",
      "loss: 1.023033  [ 3040/ 3200]\n",
      "loss: 0.432756  [ 3056/ 3200]\n",
      "loss: 0.191012  [ 3072/ 3200]\n",
      "loss: 0.237393  [ 3088/ 3200]\n",
      "loss: 0.583804  [ 3104/ 3200]\n",
      "loss: 0.576465  [ 3120/ 3200]\n",
      "loss: 0.227227  [ 3136/ 3200]\n",
      "loss: 0.461343  [ 3152/ 3200]\n",
      "loss: 0.439558  [ 3168/ 3200]\n",
      "loss: 0.408311  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 0.352474  [    0/ 3200]\n",
      "loss: 0.482466  [   16/ 3200]\n",
      "loss: 0.292362  [   32/ 3200]\n",
      "loss: 0.299553  [   48/ 3200]\n",
      "loss: 0.455160  [   64/ 3200]\n",
      "loss: 0.394731  [   80/ 3200]\n",
      "loss: 0.656233  [   96/ 3200]\n",
      "loss: 0.209592  [  112/ 3200]\n",
      "loss: 0.227665  [  128/ 3200]\n",
      "loss: 0.583265  [  144/ 3200]\n",
      "loss: 0.720729  [  160/ 3200]\n",
      "loss: 0.419220  [  176/ 3200]\n",
      "loss: 0.486325  [  192/ 3200]\n",
      "loss: 0.261213  [  208/ 3200]\n",
      "loss: 0.392611  [  224/ 3200]\n",
      "loss: 0.343539  [  240/ 3200]\n",
      "loss: 0.293057  [  256/ 3200]\n",
      "loss: 0.217942  [  272/ 3200]\n",
      "loss: 0.535831  [  288/ 3200]\n",
      "loss: 0.508815  [  304/ 3200]\n",
      "loss: 0.685155  [  320/ 3200]\n",
      "loss: 0.421498  [  336/ 3200]\n",
      "loss: 0.525210  [  352/ 3200]\n",
      "loss: 0.388281  [  368/ 3200]\n",
      "loss: 0.135146  [  384/ 3200]\n",
      "loss: 0.495417  [  400/ 3200]\n",
      "loss: 0.474268  [  416/ 3200]\n",
      "loss: 0.100528  [  432/ 3200]\n",
      "loss: 0.306569  [  448/ 3200]\n",
      "loss: 0.259598  [  464/ 3200]\n",
      "loss: 0.418667  [  480/ 3200]\n",
      "loss: 0.179063  [  496/ 3200]\n",
      "loss: 0.308162  [  512/ 3200]\n",
      "loss: 0.293533  [  528/ 3200]\n",
      "loss: 0.390858  [  544/ 3200]\n",
      "loss: 0.348075  [  560/ 3200]\n",
      "loss: 0.369261  [  576/ 3200]\n",
      "loss: 0.459293  [  592/ 3200]\n",
      "loss: 0.578596  [  608/ 3200]\n",
      "loss: 0.589280  [  624/ 3200]\n",
      "loss: 0.451558  [  640/ 3200]\n",
      "loss: 0.328132  [  656/ 3200]\n",
      "loss: 0.434749  [  672/ 3200]\n",
      "loss: 0.228976  [  688/ 3200]\n",
      "loss: 0.516271  [  704/ 3200]\n",
      "loss: 0.693045  [  720/ 3200]\n",
      "loss: 0.470353  [  736/ 3200]\n",
      "loss: 0.317465  [  752/ 3200]\n",
      "loss: 0.242111  [  768/ 3200]\n",
      "loss: 0.356654  [  784/ 3200]\n",
      "loss: 0.322670  [  800/ 3200]\n",
      "loss: 0.205844  [  816/ 3200]\n",
      "loss: 0.129291  [  832/ 3200]\n",
      "loss: 0.159356  [  848/ 3200]\n",
      "loss: 0.544670  [  864/ 3200]\n",
      "loss: 0.366054  [  880/ 3200]\n",
      "loss: 0.185607  [  896/ 3200]\n",
      "loss: 0.480234  [  912/ 3200]\n",
      "loss: 0.193998  [  928/ 3200]\n",
      "loss: 0.286423  [  944/ 3200]\n",
      "loss: 0.407200  [  960/ 3200]\n",
      "loss: 0.413665  [  976/ 3200]\n",
      "loss: 0.495216  [  992/ 3200]\n",
      "loss: 0.442699  [ 1008/ 3200]\n",
      "loss: 0.381260  [ 1024/ 3200]\n",
      "loss: 0.275969  [ 1040/ 3200]\n",
      "loss: 0.420597  [ 1056/ 3200]\n",
      "loss: 0.624237  [ 1072/ 3200]\n",
      "loss: 1.030302  [ 1088/ 3200]\n",
      "loss: 0.306345  [ 1104/ 3200]\n",
      "loss: 0.352170  [ 1120/ 3200]\n",
      "loss: 0.516877  [ 1136/ 3200]\n",
      "loss: 0.290014  [ 1152/ 3200]\n",
      "loss: 0.472800  [ 1168/ 3200]\n",
      "loss: 0.305165  [ 1184/ 3200]\n",
      "loss: 0.290005  [ 1200/ 3200]\n",
      "loss: 0.522375  [ 1216/ 3200]\n",
      "loss: 0.365505  [ 1232/ 3200]\n",
      "loss: 0.354025  [ 1248/ 3200]\n",
      "loss: 0.489176  [ 1264/ 3200]\n",
      "loss: 0.260482  [ 1280/ 3200]\n",
      "loss: 0.329449  [ 1296/ 3200]\n",
      "loss: 0.419929  [ 1312/ 3200]\n",
      "loss: 0.232402  [ 1328/ 3200]\n",
      "loss: 0.577552  [ 1344/ 3200]\n",
      "loss: 0.312930  [ 1360/ 3200]\n",
      "loss: 0.244712  [ 1376/ 3200]\n",
      "loss: 0.187046  [ 1392/ 3200]\n",
      "loss: 0.900516  [ 1408/ 3200]\n",
      "loss: 0.359006  [ 1424/ 3200]\n",
      "loss: 0.760821  [ 1440/ 3200]\n",
      "loss: 0.565898  [ 1456/ 3200]\n",
      "loss: 0.231125  [ 1472/ 3200]\n",
      "loss: 0.650490  [ 1488/ 3200]\n",
      "loss: 0.218437  [ 1504/ 3200]\n",
      "loss: 0.126314  [ 1520/ 3200]\n",
      "loss: 0.368365  [ 1536/ 3200]\n",
      "loss: 0.417757  [ 1552/ 3200]\n",
      "loss: 0.290863  [ 1568/ 3200]\n",
      "loss: 0.089162  [ 1584/ 3200]\n",
      "loss: 0.331368  [ 1600/ 3200]\n",
      "loss: 0.432760  [ 1616/ 3200]\n",
      "loss: 0.360127  [ 1632/ 3200]\n",
      "loss: 0.258919  [ 1648/ 3200]\n",
      "loss: 0.420370  [ 1664/ 3200]\n",
      "loss: 0.367342  [ 1680/ 3200]\n",
      "loss: 0.330813  [ 1696/ 3200]\n",
      "loss: 0.238866  [ 1712/ 3200]\n",
      "loss: 0.274211  [ 1728/ 3200]\n",
      "loss: 0.411950  [ 1744/ 3200]\n",
      "loss: 0.249540  [ 1760/ 3200]\n",
      "loss: 0.306303  [ 1776/ 3200]\n",
      "loss: 0.588898  [ 1792/ 3200]\n",
      "loss: 0.141905  [ 1808/ 3200]\n",
      "loss: 0.190042  [ 1824/ 3200]\n",
      "loss: 0.364907  [ 1840/ 3200]\n",
      "loss: 0.423780  [ 1856/ 3200]\n",
      "loss: 0.323354  [ 1872/ 3200]\n",
      "loss: 0.212603  [ 1888/ 3200]\n",
      "loss: 0.310300  [ 1904/ 3200]\n",
      "loss: 0.445320  [ 1920/ 3200]\n",
      "loss: 0.184185  [ 1936/ 3200]\n",
      "loss: 0.639067  [ 1952/ 3200]\n",
      "loss: 0.197349  [ 1968/ 3200]\n",
      "loss: 0.263949  [ 1984/ 3200]\n",
      "loss: 0.223282  [ 2000/ 3200]\n",
      "loss: 0.294567  [ 2016/ 3200]\n",
      "loss: 0.262368  [ 2032/ 3200]\n",
      "loss: 0.287744  [ 2048/ 3200]\n",
      "loss: 0.221915  [ 2064/ 3200]\n",
      "loss: 0.281776  [ 2080/ 3200]\n",
      "loss: 0.207817  [ 2096/ 3200]\n",
      "loss: 0.320160  [ 2112/ 3200]\n",
      "loss: 0.223086  [ 2128/ 3200]\n",
      "loss: 0.164719  [ 2144/ 3200]\n",
      "loss: 1.032180  [ 2160/ 3200]\n",
      "loss: 0.261999  [ 2176/ 3200]\n",
      "loss: 0.144863  [ 2192/ 3200]\n",
      "loss: 0.234314  [ 2208/ 3200]\n",
      "loss: 0.448250  [ 2224/ 3200]\n",
      "loss: 0.244032  [ 2240/ 3200]\n",
      "loss: 0.237839  [ 2256/ 3200]\n",
      "loss: 0.261782  [ 2272/ 3200]\n",
      "loss: 0.520772  [ 2288/ 3200]\n",
      "loss: 0.535821  [ 2304/ 3200]\n",
      "loss: 0.246485  [ 2320/ 3200]\n",
      "loss: 0.350823  [ 2336/ 3200]\n",
      "loss: 0.550038  [ 2352/ 3200]\n",
      "loss: 0.668828  [ 2368/ 3200]\n",
      "loss: 0.243535  [ 2384/ 3200]\n",
      "loss: 0.217419  [ 2400/ 3200]\n",
      "loss: 0.502667  [ 2416/ 3200]\n",
      "loss: 0.337987  [ 2432/ 3200]\n",
      "loss: 0.159305  [ 2448/ 3200]\n",
      "loss: 0.185072  [ 2464/ 3200]\n",
      "loss: 0.262569  [ 2480/ 3200]\n",
      "loss: 0.188506  [ 2496/ 3200]\n",
      "loss: 0.256996  [ 2512/ 3200]\n",
      "loss: 0.201943  [ 2528/ 3200]\n",
      "loss: 0.384982  [ 2544/ 3200]\n",
      "loss: 0.337615  [ 2560/ 3200]\n",
      "loss: 0.175848  [ 2576/ 3200]\n",
      "loss: 0.326636  [ 2592/ 3200]\n",
      "loss: 0.382903  [ 2608/ 3200]\n",
      "loss: 0.494001  [ 2624/ 3200]\n",
      "loss: 0.309304  [ 2640/ 3200]\n",
      "loss: 0.241104  [ 2656/ 3200]\n",
      "loss: 0.228763  [ 2672/ 3200]\n",
      "loss: 0.225805  [ 2688/ 3200]\n",
      "loss: 0.304861  [ 2704/ 3200]\n",
      "loss: 0.701351  [ 2720/ 3200]\n",
      "loss: 0.428564  [ 2736/ 3200]\n",
      "loss: 0.213034  [ 2752/ 3200]\n",
      "loss: 0.731185  [ 2768/ 3200]\n",
      "loss: 0.473284  [ 2784/ 3200]\n",
      "loss: 0.149231  [ 2800/ 3200]\n",
      "loss: 0.321123  [ 2816/ 3200]\n",
      "loss: 0.409500  [ 2832/ 3200]\n",
      "loss: 0.739504  [ 2848/ 3200]\n",
      "loss: 0.313332  [ 2864/ 3200]\n",
      "loss: 0.407674  [ 2880/ 3200]\n",
      "loss: 0.374427  [ 2896/ 3200]\n",
      "loss: 0.400779  [ 2912/ 3200]\n",
      "loss: 0.320001  [ 2928/ 3200]\n",
      "loss: 0.348034  [ 2944/ 3200]\n",
      "loss: 0.233815  [ 2960/ 3200]\n",
      "loss: 0.811570  [ 2976/ 3200]\n",
      "loss: 0.477371  [ 2992/ 3200]\n",
      "loss: 0.459477  [ 3008/ 3200]\n",
      "loss: 0.209596  [ 3024/ 3200]\n",
      "loss: 0.251035  [ 3040/ 3200]\n",
      "loss: 0.445571  [ 3056/ 3200]\n",
      "loss: 0.485714  [ 3072/ 3200]\n",
      "loss: 0.327037  [ 3088/ 3200]\n",
      "loss: 0.240983  [ 3104/ 3200]\n",
      "loss: 0.307121  [ 3120/ 3200]\n",
      "loss: 0.566369  [ 3136/ 3200]\n",
      "loss: 0.454112  [ 3152/ 3200]\n",
      "loss: 0.222257  [ 3168/ 3200]\n",
      "loss: 0.527430  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 0.287191  [    0/ 3200]\n",
      "loss: 0.683948  [   16/ 3200]\n",
      "loss: 0.217894  [   32/ 3200]\n",
      "loss: 0.658478  [   48/ 3200]\n",
      "loss: 0.690379  [   64/ 3200]\n",
      "loss: 0.273541  [   80/ 3200]\n",
      "loss: 0.155088  [   96/ 3200]\n",
      "loss: 0.319267  [  112/ 3200]\n",
      "loss: 0.284800  [  128/ 3200]\n",
      "loss: 0.260800  [  144/ 3200]\n",
      "loss: 0.148191  [  160/ 3200]\n",
      "loss: 0.314890  [  176/ 3200]\n",
      "loss: 0.291564  [  192/ 3200]\n",
      "loss: 0.207966  [  208/ 3200]\n",
      "loss: 0.519707  [  224/ 3200]\n",
      "loss: 0.335158  [  240/ 3200]\n",
      "loss: 0.261011  [  256/ 3200]\n",
      "loss: 0.677687  [  272/ 3200]\n",
      "loss: 0.225963  [  288/ 3200]\n",
      "loss: 0.348700  [  304/ 3200]\n",
      "loss: 0.381369  [  320/ 3200]\n",
      "loss: 0.424367  [  336/ 3200]\n",
      "loss: 0.278411  [  352/ 3200]\n",
      "loss: 0.180309  [  368/ 3200]\n",
      "loss: 0.270671  [  384/ 3200]\n",
      "loss: 0.293626  [  400/ 3200]\n",
      "loss: 0.218324  [  416/ 3200]\n",
      "loss: 0.295042  [  432/ 3200]\n",
      "loss: 0.155243  [  448/ 3200]\n",
      "loss: 0.384243  [  464/ 3200]\n",
      "loss: 0.180045  [  480/ 3200]\n",
      "loss: 0.467331  [  496/ 3200]\n",
      "loss: 0.245929  [  512/ 3200]\n",
      "loss: 0.572783  [  528/ 3200]\n",
      "loss: 0.276430  [  544/ 3200]\n",
      "loss: 0.429238  [  560/ 3200]\n",
      "loss: 0.672792  [  576/ 3200]\n",
      "loss: 0.227365  [  592/ 3200]\n",
      "loss: 0.256323  [  608/ 3200]\n",
      "loss: 0.180688  [  624/ 3200]\n",
      "loss: 0.254679  [  640/ 3200]\n",
      "loss: 0.201571  [  656/ 3200]\n",
      "loss: 0.422254  [  672/ 3200]\n",
      "loss: 0.334365  [  688/ 3200]\n",
      "loss: 0.502549  [  704/ 3200]\n",
      "loss: 0.405273  [  720/ 3200]\n",
      "loss: 0.391604  [  736/ 3200]\n",
      "loss: 0.586898  [  752/ 3200]\n",
      "loss: 0.532773  [  768/ 3200]\n",
      "loss: 0.298303  [  784/ 3200]\n",
      "loss: 0.475791  [  800/ 3200]\n",
      "loss: 0.678943  [  816/ 3200]\n",
      "loss: 0.484709  [  832/ 3200]\n",
      "loss: 0.581463  [  848/ 3200]\n",
      "loss: 0.410080  [  864/ 3200]\n",
      "loss: 0.488966  [  880/ 3200]\n",
      "loss: 0.321901  [  896/ 3200]\n",
      "loss: 0.354493  [  912/ 3200]\n",
      "loss: 0.394313  [  928/ 3200]\n",
      "loss: 0.334725  [  944/ 3200]\n",
      "loss: 0.171392  [  960/ 3200]\n",
      "loss: 0.236032  [  976/ 3200]\n",
      "loss: 0.871763  [  992/ 3200]\n",
      "loss: 0.237737  [ 1008/ 3200]\n",
      "loss: 0.152585  [ 1024/ 3200]\n",
      "loss: 0.698753  [ 1040/ 3200]\n",
      "loss: 0.068522  [ 1056/ 3200]\n",
      "loss: 0.242860  [ 1072/ 3200]\n",
      "loss: 0.609023  [ 1088/ 3200]\n",
      "loss: 0.186277  [ 1104/ 3200]\n",
      "loss: 0.294215  [ 1120/ 3200]\n",
      "loss: 0.300683  [ 1136/ 3200]\n",
      "loss: 0.466896  [ 1152/ 3200]\n",
      "loss: 0.187502  [ 1168/ 3200]\n",
      "loss: 0.194845  [ 1184/ 3200]\n",
      "loss: 0.179395  [ 1200/ 3200]\n",
      "loss: 0.200438  [ 1216/ 3200]\n",
      "loss: 0.313871  [ 1232/ 3200]\n",
      "loss: 0.281547  [ 1248/ 3200]\n",
      "loss: 0.247120  [ 1264/ 3200]\n",
      "loss: 0.486139  [ 1280/ 3200]\n",
      "loss: 0.300211  [ 1296/ 3200]\n",
      "loss: 0.300315  [ 1312/ 3200]\n",
      "loss: 0.235281  [ 1328/ 3200]\n",
      "loss: 0.233784  [ 1344/ 3200]\n",
      "loss: 0.161728  [ 1360/ 3200]\n",
      "loss: 0.163723  [ 1376/ 3200]\n",
      "loss: 0.343878  [ 1392/ 3200]\n",
      "loss: 0.244848  [ 1408/ 3200]\n",
      "loss: 0.346365  [ 1424/ 3200]\n",
      "loss: 0.118630  [ 1440/ 3200]\n",
      "loss: 0.152721  [ 1456/ 3200]\n",
      "loss: 0.398806  [ 1472/ 3200]\n",
      "loss: 0.463115  [ 1488/ 3200]\n",
      "loss: 0.075111  [ 1504/ 3200]\n",
      "loss: 0.187534  [ 1520/ 3200]\n",
      "loss: 0.641167  [ 1536/ 3200]\n",
      "loss: 0.392988  [ 1552/ 3200]\n",
      "loss: 0.237658  [ 1568/ 3200]\n",
      "loss: 0.294059  [ 1584/ 3200]\n",
      "loss: 0.244064  [ 1600/ 3200]\n",
      "loss: 0.258816  [ 1616/ 3200]\n",
      "loss: 0.104372  [ 1632/ 3200]\n",
      "loss: 0.210797  [ 1648/ 3200]\n",
      "loss: 0.282870  [ 1664/ 3200]\n",
      "loss: 0.424540  [ 1680/ 3200]\n",
      "loss: 0.243829  [ 1696/ 3200]\n",
      "loss: 0.183747  [ 1712/ 3200]\n",
      "loss: 0.435862  [ 1728/ 3200]\n",
      "loss: 0.513737  [ 1744/ 3200]\n",
      "loss: 0.441873  [ 1760/ 3200]\n",
      "loss: 0.516762  [ 1776/ 3200]\n",
      "loss: 0.075084  [ 1792/ 3200]\n",
      "loss: 0.342063  [ 1808/ 3200]\n",
      "loss: 0.448302  [ 1824/ 3200]\n",
      "loss: 0.142083  [ 1840/ 3200]\n",
      "loss: 0.063183  [ 1856/ 3200]\n",
      "loss: 0.267839  [ 1872/ 3200]\n",
      "loss: 0.191335  [ 1888/ 3200]\n",
      "loss: 0.325181  [ 1904/ 3200]\n",
      "loss: 0.312878  [ 1920/ 3200]\n",
      "loss: 0.344586  [ 1936/ 3200]\n",
      "loss: 0.441058  [ 1952/ 3200]\n",
      "loss: 0.323008  [ 1968/ 3200]\n",
      "loss: 0.690827  [ 1984/ 3200]\n",
      "loss: 0.207732  [ 2000/ 3200]\n",
      "loss: 0.307468  [ 2016/ 3200]\n",
      "loss: 0.302497  [ 2032/ 3200]\n",
      "loss: 0.209366  [ 2048/ 3200]\n",
      "loss: 0.117279  [ 2064/ 3200]\n",
      "loss: 0.263322  [ 2080/ 3200]\n",
      "loss: 0.107777  [ 2096/ 3200]\n",
      "loss: 0.318539  [ 2112/ 3200]\n",
      "loss: 0.414685  [ 2128/ 3200]\n",
      "loss: 0.421492  [ 2144/ 3200]\n",
      "loss: 0.421325  [ 2160/ 3200]\n",
      "loss: 0.289335  [ 2176/ 3200]\n",
      "loss: 0.136132  [ 2192/ 3200]\n",
      "loss: 0.207939  [ 2208/ 3200]\n",
      "loss: 0.608390  [ 2224/ 3200]\n",
      "loss: 0.179791  [ 2240/ 3200]\n",
      "loss: 0.595113  [ 2256/ 3200]\n",
      "loss: 0.075013  [ 2272/ 3200]\n",
      "loss: 0.361463  [ 2288/ 3200]\n",
      "loss: 0.220481  [ 2304/ 3200]\n",
      "loss: 0.453914  [ 2320/ 3200]\n",
      "loss: 0.218887  [ 2336/ 3200]\n",
      "loss: 0.204244  [ 2352/ 3200]\n",
      "loss: 0.402498  [ 2368/ 3200]\n",
      "loss: 0.100886  [ 2384/ 3200]\n",
      "loss: 0.172182  [ 2400/ 3200]\n",
      "loss: 0.148796  [ 2416/ 3200]\n",
      "loss: 0.324155  [ 2432/ 3200]\n",
      "loss: 0.478427  [ 2448/ 3200]\n",
      "loss: 0.486732  [ 2464/ 3200]\n",
      "loss: 0.257645  [ 2480/ 3200]\n",
      "loss: 0.223602  [ 2496/ 3200]\n",
      "loss: 0.536086  [ 2512/ 3200]\n",
      "loss: 0.239744  [ 2528/ 3200]\n",
      "loss: 0.438968  [ 2544/ 3200]\n",
      "loss: 0.412167  [ 2560/ 3200]\n",
      "loss: 0.241562  [ 2576/ 3200]\n",
      "loss: 0.179341  [ 2592/ 3200]\n",
      "loss: 0.188908  [ 2608/ 3200]\n",
      "loss: 0.242166  [ 2624/ 3200]\n",
      "loss: 0.199064  [ 2640/ 3200]\n",
      "loss: 0.304343  [ 2656/ 3200]\n",
      "loss: 0.240503  [ 2672/ 3200]\n",
      "loss: 0.358993  [ 2688/ 3200]\n",
      "loss: 0.381657  [ 2704/ 3200]\n",
      "loss: 0.115859  [ 2720/ 3200]\n",
      "loss: 0.581762  [ 2736/ 3200]\n",
      "loss: 0.165330  [ 2752/ 3200]\n",
      "loss: 0.305128  [ 2768/ 3200]\n",
      "loss: 0.385540  [ 2784/ 3200]\n",
      "loss: 0.165747  [ 2800/ 3200]\n",
      "loss: 0.442038  [ 2816/ 3200]\n",
      "loss: 0.210208  [ 2832/ 3200]\n",
      "loss: 0.214103  [ 2848/ 3200]\n",
      "loss: 0.177034  [ 2864/ 3200]\n",
      "loss: 0.302220  [ 2880/ 3200]\n",
      "loss: 0.214283  [ 2896/ 3200]\n",
      "loss: 0.469159  [ 2912/ 3200]\n",
      "loss: 0.406761  [ 2928/ 3200]\n",
      "loss: 0.443470  [ 2944/ 3200]\n",
      "loss: 0.461343  [ 2960/ 3200]\n",
      "loss: 0.183529  [ 2976/ 3200]\n",
      "loss: 0.235056  [ 2992/ 3200]\n",
      "loss: 0.156285  [ 3008/ 3200]\n",
      "loss: 0.150504  [ 3024/ 3200]\n",
      "loss: 0.276987  [ 3040/ 3200]\n",
      "loss: 0.112157  [ 3056/ 3200]\n",
      "loss: 0.165636  [ 3072/ 3200]\n",
      "loss: 0.450679  [ 3088/ 3200]\n",
      "loss: 0.487291  [ 3104/ 3200]\n",
      "loss: 0.340651  [ 3120/ 3200]\n",
      "loss: 0.202785  [ 3136/ 3200]\n",
      "loss: 0.187127  [ 3152/ 3200]\n",
      "loss: 0.281919  [ 3168/ 3200]\n",
      "loss: 0.274674  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 0.249903  [    0/ 3200]\n",
      "loss: 0.307000  [   16/ 3200]\n",
      "loss: 0.144598  [   32/ 3200]\n",
      "loss: 0.527576  [   48/ 3200]\n",
      "loss: 0.610687  [   64/ 3200]\n",
      "loss: 0.270759  [   80/ 3200]\n",
      "loss: 0.372941  [   96/ 3200]\n",
      "loss: 0.308741  [  112/ 3200]\n",
      "loss: 0.227586  [  128/ 3200]\n",
      "loss: 0.133563  [  144/ 3200]\n",
      "loss: 0.353736  [  160/ 3200]\n",
      "loss: 0.260229  [  176/ 3200]\n",
      "loss: 0.165997  [  192/ 3200]\n",
      "loss: 0.087985  [  208/ 3200]\n",
      "loss: 0.163789  [  224/ 3200]\n",
      "loss: 0.436389  [  240/ 3200]\n",
      "loss: 0.126307  [  256/ 3200]\n",
      "loss: 0.204954  [  272/ 3200]\n",
      "loss: 0.137655  [  288/ 3200]\n",
      "loss: 0.311765  [  304/ 3200]\n",
      "loss: 0.679534  [  320/ 3200]\n",
      "loss: 0.498149  [  336/ 3200]\n",
      "loss: 0.438715  [  352/ 3200]\n",
      "loss: 0.500468  [  368/ 3200]\n",
      "loss: 0.171584  [  384/ 3200]\n",
      "loss: 0.166724  [  400/ 3200]\n",
      "loss: 0.652832  [  416/ 3200]\n",
      "loss: 0.207642  [  432/ 3200]\n",
      "loss: 0.353297  [  448/ 3200]\n",
      "loss: 0.312872  [  464/ 3200]\n",
      "loss: 0.100082  [  480/ 3200]\n",
      "loss: 0.297118  [  496/ 3200]\n",
      "loss: 0.273326  [  512/ 3200]\n",
      "loss: 0.312748  [  528/ 3200]\n",
      "loss: 0.388921  [  544/ 3200]\n",
      "loss: 0.391598  [  560/ 3200]\n",
      "loss: 0.102304  [  576/ 3200]\n",
      "loss: 0.114555  [  592/ 3200]\n",
      "loss: 0.284025  [  608/ 3200]\n",
      "loss: 0.604800  [  624/ 3200]\n",
      "loss: 0.926432  [  640/ 3200]\n",
      "loss: 0.274239  [  656/ 3200]\n",
      "loss: 0.227268  [  672/ 3200]\n",
      "loss: 0.766162  [  688/ 3200]\n",
      "loss: 0.279016  [  704/ 3200]\n",
      "loss: 0.216610  [  720/ 3200]\n",
      "loss: 0.274591  [  736/ 3200]\n",
      "loss: 0.098057  [  752/ 3200]\n",
      "loss: 0.099441  [  768/ 3200]\n",
      "loss: 0.099577  [  784/ 3200]\n",
      "loss: 0.153883  [  800/ 3200]\n",
      "loss: 0.233447  [  816/ 3200]\n",
      "loss: 0.269695  [  832/ 3200]\n",
      "loss: 0.310641  [  848/ 3200]\n",
      "loss: 0.346967  [  864/ 3200]\n",
      "loss: 0.382943  [  880/ 3200]\n",
      "loss: 0.208134  [  896/ 3200]\n",
      "loss: 0.534665  [  912/ 3200]\n",
      "loss: 0.368593  [  928/ 3200]\n",
      "loss: 0.374133  [  944/ 3200]\n",
      "loss: 0.128719  [  960/ 3200]\n",
      "loss: 0.138876  [  976/ 3200]\n",
      "loss: 0.176794  [  992/ 3200]\n",
      "loss: 0.201210  [ 1008/ 3200]\n",
      "loss: 0.410254  [ 1024/ 3200]\n",
      "loss: 0.258115  [ 1040/ 3200]\n",
      "loss: 0.257547  [ 1056/ 3200]\n",
      "loss: 0.196443  [ 1072/ 3200]\n",
      "loss: 0.148122  [ 1088/ 3200]\n",
      "loss: 0.342739  [ 1104/ 3200]\n",
      "loss: 0.352241  [ 1120/ 3200]\n",
      "loss: 0.064324  [ 1136/ 3200]\n",
      "loss: 0.223495  [ 1152/ 3200]\n",
      "loss: 0.121053  [ 1168/ 3200]\n",
      "loss: 0.272241  [ 1184/ 3200]\n",
      "loss: 0.257926  [ 1200/ 3200]\n",
      "loss: 0.160785  [ 1216/ 3200]\n",
      "loss: 0.274495  [ 1232/ 3200]\n",
      "loss: 0.134343  [ 1248/ 3200]\n",
      "loss: 0.318398  [ 1264/ 3200]\n",
      "loss: 0.236553  [ 1280/ 3200]\n",
      "loss: 0.106991  [ 1296/ 3200]\n",
      "loss: 0.134280  [ 1312/ 3200]\n",
      "loss: 0.316604  [ 1328/ 3200]\n",
      "loss: 0.041641  [ 1344/ 3200]\n",
      "loss: 0.473174  [ 1360/ 3200]\n",
      "loss: 0.279663  [ 1376/ 3200]\n",
      "loss: 0.510393  [ 1392/ 3200]\n",
      "loss: 0.711179  [ 1408/ 3200]\n",
      "loss: 0.232502  [ 1424/ 3200]\n",
      "loss: 0.186273  [ 1440/ 3200]\n",
      "loss: 0.571637  [ 1456/ 3200]\n",
      "loss: 0.074738  [ 1472/ 3200]\n",
      "loss: 0.283859  [ 1488/ 3200]\n",
      "loss: 0.176083  [ 1504/ 3200]\n",
      "loss: 0.271113  [ 1520/ 3200]\n",
      "loss: 0.295914  [ 1536/ 3200]\n",
      "loss: 0.172082  [ 1552/ 3200]\n",
      "loss: 0.097469  [ 1568/ 3200]\n",
      "loss: 0.176800  [ 1584/ 3200]\n",
      "loss: 0.412417  [ 1600/ 3200]\n",
      "loss: 0.595820  [ 1616/ 3200]\n",
      "loss: 0.374982  [ 1632/ 3200]\n",
      "loss: 0.410378  [ 1648/ 3200]\n",
      "loss: 0.431132  [ 1664/ 3200]\n",
      "loss: 0.340342  [ 1680/ 3200]\n",
      "loss: 0.166628  [ 1696/ 3200]\n",
      "loss: 0.166795  [ 1712/ 3200]\n",
      "loss: 0.301374  [ 1728/ 3200]\n",
      "loss: 0.247931  [ 1744/ 3200]\n",
      "loss: 0.069985  [ 1760/ 3200]\n",
      "loss: 0.359606  [ 1776/ 3200]\n",
      "loss: 0.151699  [ 1792/ 3200]\n",
      "loss: 0.108716  [ 1808/ 3200]\n",
      "loss: 0.144944  [ 1824/ 3200]\n",
      "loss: 0.298948  [ 1840/ 3200]\n",
      "loss: 0.031838  [ 1856/ 3200]\n",
      "loss: 0.123839  [ 1872/ 3200]\n",
      "loss: 0.437702  [ 1888/ 3200]\n",
      "loss: 0.557889  [ 1904/ 3200]\n",
      "loss: 0.126815  [ 1920/ 3200]\n",
      "loss: 0.399974  [ 1936/ 3200]\n",
      "loss: 0.195881  [ 1952/ 3200]\n",
      "loss: 0.100589  [ 1968/ 3200]\n",
      "loss: 0.093019  [ 1984/ 3200]\n",
      "loss: 0.269878  [ 2000/ 3200]\n",
      "loss: 0.045936  [ 2016/ 3200]\n",
      "loss: 0.401013  [ 2032/ 3200]\n",
      "loss: 0.430579  [ 2048/ 3200]\n",
      "loss: 0.451212  [ 2064/ 3200]\n",
      "loss: 0.431392  [ 2080/ 3200]\n",
      "loss: 0.462083  [ 2096/ 3200]\n",
      "loss: 0.452138  [ 2112/ 3200]\n",
      "loss: 0.112227  [ 2128/ 3200]\n",
      "loss: 0.151090  [ 2144/ 3200]\n",
      "loss: 0.521681  [ 2160/ 3200]\n",
      "loss: 0.604232  [ 2176/ 3200]\n",
      "loss: 0.402108  [ 2192/ 3200]\n",
      "loss: 0.227075  [ 2208/ 3200]\n",
      "loss: 0.139731  [ 2224/ 3200]\n",
      "loss: 0.110685  [ 2240/ 3200]\n",
      "loss: 0.281039  [ 2256/ 3200]\n",
      "loss: 0.406816  [ 2272/ 3200]\n",
      "loss: 0.114560  [ 2288/ 3200]\n",
      "loss: 0.178617  [ 2304/ 3200]\n",
      "loss: 0.171591  [ 2320/ 3200]\n",
      "loss: 0.266180  [ 2336/ 3200]\n",
      "loss: 0.305192  [ 2352/ 3200]\n",
      "loss: 0.130021  [ 2368/ 3200]\n",
      "loss: 0.307273  [ 2384/ 3200]\n",
      "loss: 0.228376  [ 2400/ 3200]\n",
      "loss: 0.297778  [ 2416/ 3200]\n",
      "loss: 0.267373  [ 2432/ 3200]\n",
      "loss: 0.187539  [ 2448/ 3200]\n",
      "loss: 0.255951  [ 2464/ 3200]\n",
      "loss: 0.106379  [ 2480/ 3200]\n",
      "loss: 0.255488  [ 2496/ 3200]\n",
      "loss: 0.382737  [ 2512/ 3200]\n",
      "loss: 0.239805  [ 2528/ 3200]\n",
      "loss: 0.095389  [ 2544/ 3200]\n",
      "loss: 0.183076  [ 2560/ 3200]\n",
      "loss: 0.282286  [ 2576/ 3200]\n",
      "loss: 0.528817  [ 2592/ 3200]\n",
      "loss: 0.183282  [ 2608/ 3200]\n",
      "loss: 0.240512  [ 2624/ 3200]\n",
      "loss: 0.269701  [ 2640/ 3200]\n",
      "loss: 0.436902  [ 2656/ 3200]\n",
      "loss: 0.571600  [ 2672/ 3200]\n",
      "loss: 0.192037  [ 2688/ 3200]\n",
      "loss: 0.209472  [ 2704/ 3200]\n",
      "loss: 0.069460  [ 2720/ 3200]\n",
      "loss: 0.202721  [ 2736/ 3200]\n",
      "loss: 0.316855  [ 2752/ 3200]\n",
      "loss: 0.246616  [ 2768/ 3200]\n",
      "loss: 0.082596  [ 2784/ 3200]\n",
      "loss: 0.210453  [ 2800/ 3200]\n",
      "loss: 0.125245  [ 2816/ 3200]\n",
      "loss: 0.326030  [ 2832/ 3200]\n",
      "loss: 0.310298  [ 2848/ 3200]\n",
      "loss: 0.338007  [ 2864/ 3200]\n",
      "loss: 0.342090  [ 2880/ 3200]\n",
      "loss: 0.469588  [ 2896/ 3200]\n",
      "loss: 0.223940  [ 2912/ 3200]\n",
      "loss: 0.214868  [ 2928/ 3200]\n",
      "loss: 0.094407  [ 2944/ 3200]\n",
      "loss: 0.463811  [ 2960/ 3200]\n",
      "loss: 0.299723  [ 2976/ 3200]\n",
      "loss: 0.216361  [ 2992/ 3200]\n",
      "loss: 0.180542  [ 3008/ 3200]\n",
      "loss: 0.253895  [ 3024/ 3200]\n",
      "loss: 0.078628  [ 3040/ 3200]\n",
      "loss: 0.142063  [ 3056/ 3200]\n",
      "loss: 0.394575  [ 3072/ 3200]\n",
      "loss: 0.487168  [ 3088/ 3200]\n",
      "loss: 0.380999  [ 3104/ 3200]\n",
      "loss: 0.232169  [ 3120/ 3200]\n",
      "loss: 0.200370  [ 3136/ 3200]\n",
      "loss: 0.514139  [ 3152/ 3200]\n",
      "loss: 0.803292  [ 3168/ 3200]\n",
      "loss: 0.734906  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 0.292623  [    0/ 3200]\n",
      "loss: 0.406979  [   16/ 3200]\n",
      "loss: 0.358834  [   32/ 3200]\n",
      "loss: 0.335738  [   48/ 3200]\n",
      "loss: 0.169467  [   64/ 3200]\n",
      "loss: 0.449638  [   80/ 3200]\n",
      "loss: 0.266904  [   96/ 3200]\n",
      "loss: 0.169398  [  112/ 3200]\n",
      "loss: 0.228658  [  128/ 3200]\n",
      "loss: 0.286757  [  144/ 3200]\n",
      "loss: 0.256627  [  160/ 3200]\n",
      "loss: 0.122832  [  176/ 3200]\n",
      "loss: 0.179433  [  192/ 3200]\n",
      "loss: 0.439581  [  208/ 3200]\n",
      "loss: 0.048975  [  224/ 3200]\n",
      "loss: 0.255731  [  240/ 3200]\n",
      "loss: 0.574495  [  256/ 3200]\n",
      "loss: 0.197250  [  272/ 3200]\n",
      "loss: 0.287696  [  288/ 3200]\n",
      "loss: 0.290355  [  304/ 3200]\n",
      "loss: 0.347311  [  320/ 3200]\n",
      "loss: 0.238851  [  336/ 3200]\n",
      "loss: 0.159946  [  352/ 3200]\n",
      "loss: 0.285486  [  368/ 3200]\n",
      "loss: 0.163545  [  384/ 3200]\n",
      "loss: 0.249375  [  400/ 3200]\n",
      "loss: 0.074622  [  416/ 3200]\n",
      "loss: 0.056570  [  432/ 3200]\n",
      "loss: 0.119192  [  448/ 3200]\n",
      "loss: 0.174211  [  464/ 3200]\n",
      "loss: 0.555494  [  480/ 3200]\n",
      "loss: 0.068769  [  496/ 3200]\n",
      "loss: 0.620641  [  512/ 3200]\n",
      "loss: 0.253405  [  528/ 3200]\n",
      "loss: 0.091956  [  544/ 3200]\n",
      "loss: 0.113145  [  560/ 3200]\n",
      "loss: 0.198666  [  576/ 3200]\n",
      "loss: 0.267315  [  592/ 3200]\n",
      "loss: 0.163451  [  608/ 3200]\n",
      "loss: 0.255617  [  624/ 3200]\n",
      "loss: 0.080499  [  640/ 3200]\n",
      "loss: 0.512140  [  656/ 3200]\n",
      "loss: 0.204833  [  672/ 3200]\n",
      "loss: 0.332368  [  688/ 3200]\n",
      "loss: 0.152683  [  704/ 3200]\n",
      "loss: 0.219662  [  720/ 3200]\n",
      "loss: 0.171292  [  736/ 3200]\n",
      "loss: 0.488440  [  752/ 3200]\n",
      "loss: 0.344472  [  768/ 3200]\n",
      "loss: 0.260130  [  784/ 3200]\n",
      "loss: 0.159541  [  800/ 3200]\n",
      "loss: 0.148200  [  816/ 3200]\n",
      "loss: 0.191896  [  832/ 3200]\n",
      "loss: 0.126851  [  848/ 3200]\n",
      "loss: 0.284942  [  864/ 3200]\n",
      "loss: 0.312164  [  880/ 3200]\n",
      "loss: 0.081266  [  896/ 3200]\n",
      "loss: 0.169816  [  912/ 3200]\n",
      "loss: 0.116250  [  928/ 3200]\n",
      "loss: 0.101115  [  944/ 3200]\n",
      "loss: 0.216660  [  960/ 3200]\n",
      "loss: 0.353590  [  976/ 3200]\n",
      "loss: 0.151850  [  992/ 3200]\n",
      "loss: 0.273424  [ 1008/ 3200]\n",
      "loss: 0.222865  [ 1024/ 3200]\n",
      "loss: 0.166232  [ 1040/ 3200]\n",
      "loss: 0.459844  [ 1056/ 3200]\n",
      "loss: 0.182630  [ 1072/ 3200]\n",
      "loss: 0.107528  [ 1088/ 3200]\n",
      "loss: 0.472178  [ 1104/ 3200]\n",
      "loss: 0.160370  [ 1120/ 3200]\n",
      "loss: 0.218270  [ 1136/ 3200]\n",
      "loss: 0.154762  [ 1152/ 3200]\n",
      "loss: 0.103794  [ 1168/ 3200]\n",
      "loss: 0.436209  [ 1184/ 3200]\n",
      "loss: 0.075257  [ 1200/ 3200]\n",
      "loss: 0.658881  [ 1216/ 3200]\n",
      "loss: 0.316116  [ 1232/ 3200]\n",
      "loss: 0.290141  [ 1248/ 3200]\n",
      "loss: 0.085859  [ 1264/ 3200]\n",
      "loss: 0.115936  [ 1280/ 3200]\n",
      "loss: 0.100439  [ 1296/ 3200]\n",
      "loss: 0.250926  [ 1312/ 3200]\n",
      "loss: 0.118144  [ 1328/ 3200]\n",
      "loss: 0.318618  [ 1344/ 3200]\n",
      "loss: 0.100371  [ 1360/ 3200]\n",
      "loss: 0.175848  [ 1376/ 3200]\n",
      "loss: 0.338578  [ 1392/ 3200]\n",
      "loss: 0.330610  [ 1408/ 3200]\n",
      "loss: 0.318036  [ 1424/ 3200]\n",
      "loss: 0.248452  [ 1440/ 3200]\n",
      "loss: 0.264111  [ 1456/ 3200]\n",
      "loss: 0.246648  [ 1472/ 3200]\n",
      "loss: 0.226554  [ 1488/ 3200]\n",
      "loss: 0.282739  [ 1504/ 3200]\n",
      "loss: 0.079471  [ 1520/ 3200]\n",
      "loss: 0.233728  [ 1536/ 3200]\n",
      "loss: 0.403320  [ 1552/ 3200]\n",
      "loss: 1.162694  [ 1568/ 3200]\n",
      "loss: 0.473164  [ 1584/ 3200]\n",
      "loss: 0.398760  [ 1600/ 3200]\n",
      "loss: 0.258985  [ 1616/ 3200]\n",
      "loss: 0.107464  [ 1632/ 3200]\n",
      "loss: 0.240649  [ 1648/ 3200]\n",
      "loss: 0.136468  [ 1664/ 3200]\n",
      "loss: 0.506827  [ 1680/ 3200]\n",
      "loss: 0.278076  [ 1696/ 3200]\n",
      "loss: 0.269658  [ 1712/ 3200]\n",
      "loss: 0.464467  [ 1728/ 3200]\n",
      "loss: 0.143694  [ 1744/ 3200]\n",
      "loss: 0.096692  [ 1760/ 3200]\n",
      "loss: 0.117766  [ 1776/ 3200]\n",
      "loss: 0.249052  [ 1792/ 3200]\n",
      "loss: 0.118492  [ 1808/ 3200]\n",
      "loss: 0.203934  [ 1824/ 3200]\n",
      "loss: 0.120956  [ 1840/ 3200]\n",
      "loss: 0.158528  [ 1856/ 3200]\n",
      "loss: 0.096966  [ 1872/ 3200]\n",
      "loss: 0.176963  [ 1888/ 3200]\n",
      "loss: 0.361911  [ 1904/ 3200]\n",
      "loss: 0.095673  [ 1920/ 3200]\n",
      "loss: 0.107059  [ 1936/ 3200]\n",
      "loss: 0.195204  [ 1952/ 3200]\n",
      "loss: 0.168515  [ 1968/ 3200]\n",
      "loss: 0.080274  [ 1984/ 3200]\n",
      "loss: 0.199721  [ 2000/ 3200]\n",
      "loss: 0.293729  [ 2016/ 3200]\n",
      "loss: 0.283483  [ 2032/ 3200]\n",
      "loss: 0.235061  [ 2048/ 3200]\n",
      "loss: 0.163296  [ 2064/ 3200]\n",
      "loss: 0.148402  [ 2080/ 3200]\n",
      "loss: 0.146203  [ 2096/ 3200]\n",
      "loss: 0.457939  [ 2112/ 3200]\n",
      "loss: 0.590315  [ 2128/ 3200]\n",
      "loss: 0.498633  [ 2144/ 3200]\n",
      "loss: 0.280378  [ 2160/ 3200]\n",
      "loss: 0.106291  [ 2176/ 3200]\n",
      "loss: 0.173270  [ 2192/ 3200]\n",
      "loss: 0.105194  [ 2208/ 3200]\n",
      "loss: 0.374734  [ 2224/ 3200]\n",
      "loss: 0.170761  [ 2240/ 3200]\n",
      "loss: 0.591683  [ 2256/ 3200]\n",
      "loss: 0.174592  [ 2272/ 3200]\n",
      "loss: 0.122112  [ 2288/ 3200]\n",
      "loss: 0.475947  [ 2304/ 3200]\n",
      "loss: 0.216674  [ 2320/ 3200]\n",
      "loss: 0.173434  [ 2336/ 3200]\n",
      "loss: 0.055297  [ 2352/ 3200]\n",
      "loss: 0.346908  [ 2368/ 3200]\n",
      "loss: 0.223798  [ 2384/ 3200]\n",
      "loss: 0.221738  [ 2400/ 3200]\n",
      "loss: 0.265975  [ 2416/ 3200]\n",
      "loss: 0.354075  [ 2432/ 3200]\n",
      "loss: 0.516222  [ 2448/ 3200]\n",
      "loss: 0.087547  [ 2464/ 3200]\n",
      "loss: 0.102482  [ 2480/ 3200]\n",
      "loss: 0.286134  [ 2496/ 3200]\n",
      "loss: 0.317240  [ 2512/ 3200]\n",
      "loss: 0.120659  [ 2528/ 3200]\n",
      "loss: 0.271481  [ 2544/ 3200]\n",
      "loss: 0.092084  [ 2560/ 3200]\n",
      "loss: 0.249286  [ 2576/ 3200]\n",
      "loss: 0.165609  [ 2592/ 3200]\n",
      "loss: 0.351385  [ 2608/ 3200]\n",
      "loss: 0.198879  [ 2624/ 3200]\n",
      "loss: 0.193656  [ 2640/ 3200]\n",
      "loss: 0.085470  [ 2656/ 3200]\n",
      "loss: 0.131287  [ 2672/ 3200]\n",
      "loss: 0.083847  [ 2688/ 3200]\n",
      "loss: 0.687289  [ 2704/ 3200]\n",
      "loss: 0.388753  [ 2720/ 3200]\n",
      "loss: 0.161886  [ 2736/ 3200]\n",
      "loss: 0.508521  [ 2752/ 3200]\n",
      "loss: 0.039274  [ 2768/ 3200]\n",
      "loss: 0.235426  [ 2784/ 3200]\n",
      "loss: 0.278260  [ 2800/ 3200]\n",
      "loss: 0.403667  [ 2816/ 3200]\n",
      "loss: 0.264380  [ 2832/ 3200]\n",
      "loss: 0.282320  [ 2848/ 3200]\n",
      "loss: 0.276968  [ 2864/ 3200]\n",
      "loss: 0.234357  [ 2880/ 3200]\n",
      "loss: 0.183813  [ 2896/ 3200]\n",
      "loss: 0.107435  [ 2912/ 3200]\n",
      "loss: 0.312553  [ 2928/ 3200]\n",
      "loss: 0.274580  [ 2944/ 3200]\n",
      "loss: 0.495862  [ 2960/ 3200]\n",
      "loss: 0.114776  [ 2976/ 3200]\n",
      "loss: 0.394422  [ 2992/ 3200]\n",
      "loss: 0.558025  [ 3008/ 3200]\n",
      "loss: 0.177135  [ 3024/ 3200]\n",
      "loss: 0.187550  [ 3040/ 3200]\n",
      "loss: 0.119677  [ 3056/ 3200]\n",
      "loss: 0.156818  [ 3072/ 3200]\n",
      "loss: 0.258865  [ 3088/ 3200]\n",
      "loss: 0.182681  [ 3104/ 3200]\n",
      "loss: 0.330140  [ 3120/ 3200]\n",
      "loss: 0.304983  [ 3136/ 3200]\n",
      "loss: 0.057285  [ 3152/ 3200]\n",
      "loss: 0.308571  [ 3168/ 3200]\n",
      "loss: 0.269586  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 0.304080  [    0/ 3200]\n",
      "loss: 0.245756  [   16/ 3200]\n",
      "loss: 0.119023  [   32/ 3200]\n",
      "loss: 0.245677  [   48/ 3200]\n",
      "loss: 0.238649  [   64/ 3200]\n",
      "loss: 0.206982  [   80/ 3200]\n",
      "loss: 0.615420  [   96/ 3200]\n",
      "loss: 0.384317  [  112/ 3200]\n",
      "loss: 0.193371  [  128/ 3200]\n",
      "loss: 0.179480  [  144/ 3200]\n",
      "loss: 0.238415  [  160/ 3200]\n",
      "loss: 0.357030  [  176/ 3200]\n",
      "loss: 0.178324  [  192/ 3200]\n",
      "loss: 0.202162  [  208/ 3200]\n",
      "loss: 0.156883  [  224/ 3200]\n",
      "loss: 0.080277  [  240/ 3200]\n",
      "loss: 0.474885  [  256/ 3200]\n",
      "loss: 0.050518  [  272/ 3200]\n",
      "loss: 0.089359  [  288/ 3200]\n",
      "loss: 0.189572  [  304/ 3200]\n",
      "loss: 0.123407  [  320/ 3200]\n",
      "loss: 0.224024  [  336/ 3200]\n",
      "loss: 0.328526  [  352/ 3200]\n",
      "loss: 0.154773  [  368/ 3200]\n",
      "loss: 0.192841  [  384/ 3200]\n",
      "loss: 0.352032  [  400/ 3200]\n",
      "loss: 0.160784  [  416/ 3200]\n",
      "loss: 0.251031  [  432/ 3200]\n",
      "loss: 0.146518  [  448/ 3200]\n",
      "loss: 0.207507  [  464/ 3200]\n",
      "loss: 0.128079  [  480/ 3200]\n",
      "loss: 0.228537  [  496/ 3200]\n",
      "loss: 0.198021  [  512/ 3200]\n",
      "loss: 0.152934  [  528/ 3200]\n",
      "loss: 0.114891  [  544/ 3200]\n",
      "loss: 0.316470  [  560/ 3200]\n",
      "loss: 0.087967  [  576/ 3200]\n",
      "loss: 0.118729  [  592/ 3200]\n",
      "loss: 0.137344  [  608/ 3200]\n",
      "loss: 0.269468  [  624/ 3200]\n",
      "loss: 0.253472  [  640/ 3200]\n",
      "loss: 0.245339  [  656/ 3200]\n",
      "loss: 0.104862  [  672/ 3200]\n",
      "loss: 0.469301  [  688/ 3200]\n",
      "loss: 0.207909  [  704/ 3200]\n",
      "loss: 0.227344  [  720/ 3200]\n",
      "loss: 0.157177  [  736/ 3200]\n",
      "loss: 0.121746  [  752/ 3200]\n",
      "loss: 0.166872  [  768/ 3200]\n",
      "loss: 0.136249  [  784/ 3200]\n",
      "loss: 0.125409  [  800/ 3200]\n",
      "loss: 0.253787  [  816/ 3200]\n",
      "loss: 0.254322  [  832/ 3200]\n",
      "loss: 0.186016  [  848/ 3200]\n",
      "loss: 0.097247  [  864/ 3200]\n",
      "loss: 0.251608  [  880/ 3200]\n",
      "loss: 0.072673  [  896/ 3200]\n",
      "loss: 0.052116  [  912/ 3200]\n",
      "loss: 0.203692  [  928/ 3200]\n",
      "loss: 0.195837  [  944/ 3200]\n",
      "loss: 0.201655  [  960/ 3200]\n",
      "loss: 0.132731  [  976/ 3200]\n",
      "loss: 0.166559  [  992/ 3200]\n",
      "loss: 0.118664  [ 1008/ 3200]\n",
      "loss: 0.174400  [ 1024/ 3200]\n",
      "loss: 0.127050  [ 1040/ 3200]\n",
      "loss: 0.158788  [ 1056/ 3200]\n",
      "loss: 0.153750  [ 1072/ 3200]\n",
      "loss: 0.290683  [ 1088/ 3200]\n",
      "loss: 0.264895  [ 1104/ 3200]\n",
      "loss: 0.144619  [ 1120/ 3200]\n",
      "loss: 0.232488  [ 1136/ 3200]\n",
      "loss: 0.255680  [ 1152/ 3200]\n",
      "loss: 0.298818  [ 1168/ 3200]\n",
      "loss: 0.353283  [ 1184/ 3200]\n",
      "loss: 0.316568  [ 1200/ 3200]\n",
      "loss: 0.294527  [ 1216/ 3200]\n",
      "loss: 0.183519  [ 1232/ 3200]\n",
      "loss: 0.137303  [ 1248/ 3200]\n",
      "loss: 0.109904  [ 1264/ 3200]\n",
      "loss: 0.237155  [ 1280/ 3200]\n",
      "loss: 0.169357  [ 1296/ 3200]\n",
      "loss: 0.321740  [ 1312/ 3200]\n",
      "loss: 0.219450  [ 1328/ 3200]\n",
      "loss: 0.086309  [ 1344/ 3200]\n",
      "loss: 0.097985  [ 1360/ 3200]\n",
      "loss: 0.254722  [ 1376/ 3200]\n",
      "loss: 0.170615  [ 1392/ 3200]\n",
      "loss: 0.194809  [ 1408/ 3200]\n",
      "loss: 0.163653  [ 1424/ 3200]\n",
      "loss: 0.086026  [ 1440/ 3200]\n",
      "loss: 0.055929  [ 1456/ 3200]\n",
      "loss: 0.069308  [ 1472/ 3200]\n",
      "loss: 0.361170  [ 1488/ 3200]\n",
      "loss: 0.201313  [ 1504/ 3200]\n",
      "loss: 0.042258  [ 1520/ 3200]\n",
      "loss: 0.095631  [ 1536/ 3200]\n",
      "loss: 0.143329  [ 1552/ 3200]\n",
      "loss: 0.377056  [ 1568/ 3200]\n",
      "loss: 0.109310  [ 1584/ 3200]\n",
      "loss: 0.053500  [ 1600/ 3200]\n",
      "loss: 0.183973  [ 1616/ 3200]\n",
      "loss: 0.137480  [ 1632/ 3200]\n",
      "loss: 0.112579  [ 1648/ 3200]\n",
      "loss: 0.106038  [ 1664/ 3200]\n",
      "loss: 0.292631  [ 1680/ 3200]\n",
      "loss: 0.260847  [ 1696/ 3200]\n",
      "loss: 0.023665  [ 1712/ 3200]\n",
      "loss: 0.130683  [ 1728/ 3200]\n",
      "loss: 0.330421  [ 1744/ 3200]\n",
      "loss: 0.203572  [ 1760/ 3200]\n",
      "loss: 0.249471  [ 1776/ 3200]\n",
      "loss: 0.104575  [ 1792/ 3200]\n",
      "loss: 0.106790  [ 1808/ 3200]\n",
      "loss: 0.383690  [ 1824/ 3200]\n",
      "loss: 0.164959  [ 1840/ 3200]\n",
      "loss: 0.257568  [ 1856/ 3200]\n",
      "loss: 0.104768  [ 1872/ 3200]\n",
      "loss: 0.090992  [ 1888/ 3200]\n",
      "loss: 0.077534  [ 1904/ 3200]\n",
      "loss: 0.087850  [ 1920/ 3200]\n",
      "loss: 0.023372  [ 1936/ 3200]\n",
      "loss: 0.177871  [ 1952/ 3200]\n",
      "loss: 0.171639  [ 1968/ 3200]\n",
      "loss: 0.253491  [ 1984/ 3200]\n",
      "loss: 0.229839  [ 2000/ 3200]\n",
      "loss: 0.101895  [ 2016/ 3200]\n",
      "loss: 0.112392  [ 2032/ 3200]\n",
      "loss: 0.113996  [ 2048/ 3200]\n",
      "loss: 0.110446  [ 2064/ 3200]\n",
      "loss: 0.300822  [ 2080/ 3200]\n",
      "loss: 0.540397  [ 2096/ 3200]\n",
      "loss: 0.189335  [ 2112/ 3200]\n",
      "loss: 0.132775  [ 2128/ 3200]\n",
      "loss: 0.077475  [ 2144/ 3200]\n",
      "loss: 0.266563  [ 2160/ 3200]\n",
      "loss: 0.154970  [ 2176/ 3200]\n",
      "loss: 0.052214  [ 2192/ 3200]\n",
      "loss: 0.108300  [ 2208/ 3200]\n",
      "loss: 0.258678  [ 2224/ 3200]\n",
      "loss: 0.221113  [ 2240/ 3200]\n",
      "loss: 0.219521  [ 2256/ 3200]\n",
      "loss: 0.118277  [ 2272/ 3200]\n",
      "loss: 0.137508  [ 2288/ 3200]\n",
      "loss: 0.121596  [ 2304/ 3200]\n",
      "loss: 0.320969  [ 2320/ 3200]\n",
      "loss: 0.082718  [ 2336/ 3200]\n",
      "loss: 0.140368  [ 2352/ 3200]\n",
      "loss: 0.142128  [ 2368/ 3200]\n",
      "loss: 0.386815  [ 2384/ 3200]\n",
      "loss: 0.326071  [ 2400/ 3200]\n",
      "loss: 0.204786  [ 2416/ 3200]\n",
      "loss: 0.195876  [ 2432/ 3200]\n",
      "loss: 0.316680  [ 2448/ 3200]\n",
      "loss: 0.092016  [ 2464/ 3200]\n",
      "loss: 0.152385  [ 2480/ 3200]\n",
      "loss: 0.084942  [ 2496/ 3200]\n",
      "loss: 0.084992  [ 2512/ 3200]\n",
      "loss: 0.054299  [ 2528/ 3200]\n",
      "loss: 0.192631  [ 2544/ 3200]\n",
      "loss: 0.181399  [ 2560/ 3200]\n",
      "loss: 0.095036  [ 2576/ 3200]\n",
      "loss: 0.218045  [ 2592/ 3200]\n",
      "loss: 0.218050  [ 2608/ 3200]\n",
      "loss: 0.352194  [ 2624/ 3200]\n",
      "loss: 0.792093  [ 2640/ 3200]\n",
      "loss: 0.126880  [ 2656/ 3200]\n",
      "loss: 0.482599  [ 2672/ 3200]\n",
      "loss: 0.624259  [ 2688/ 3200]\n",
      "loss: 0.210263  [ 2704/ 3200]\n",
      "loss: 0.126782  [ 2720/ 3200]\n",
      "loss: 0.494236  [ 2736/ 3200]\n",
      "loss: 0.360218  [ 2752/ 3200]\n",
      "loss: 0.554583  [ 2768/ 3200]\n",
      "loss: 0.159129  [ 2784/ 3200]\n",
      "loss: 0.171543  [ 2800/ 3200]\n",
      "loss: 0.206655  [ 2816/ 3200]\n",
      "loss: 0.269572  [ 2832/ 3200]\n",
      "loss: 0.104270  [ 2848/ 3200]\n",
      "loss: 0.159019  [ 2864/ 3200]\n",
      "loss: 0.294549  [ 2880/ 3200]\n",
      "loss: 0.390765  [ 2896/ 3200]\n",
      "loss: 0.062909  [ 2912/ 3200]\n",
      "loss: 0.171204  [ 2928/ 3200]\n",
      "loss: 0.483754  [ 2944/ 3200]\n",
      "loss: 0.393741  [ 2960/ 3200]\n",
      "loss: 0.075792  [ 2976/ 3200]\n",
      "loss: 0.083205  [ 2992/ 3200]\n",
      "loss: 0.396703  [ 3008/ 3200]\n",
      "loss: 0.304154  [ 3024/ 3200]\n",
      "loss: 0.161616  [ 3040/ 3200]\n",
      "loss: 0.151004  [ 3056/ 3200]\n",
      "loss: 0.301039  [ 3072/ 3200]\n",
      "loss: 0.953742  [ 3088/ 3200]\n",
      "loss: 0.473454  [ 3104/ 3200]\n",
      "loss: 0.123389  [ 3120/ 3200]\n",
      "loss: 0.194805  [ 3136/ 3200]\n",
      "loss: 0.092066  [ 3152/ 3200]\n",
      "loss: 0.267164  [ 3168/ 3200]\n",
      "loss: 0.603024  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 0.120776  [    0/ 3200]\n",
      "loss: 0.141454  [   16/ 3200]\n",
      "loss: 0.136535  [   32/ 3200]\n",
      "loss: 0.147934  [   48/ 3200]\n",
      "loss: 0.127419  [   64/ 3200]\n",
      "loss: 0.123107  [   80/ 3200]\n",
      "loss: 0.231794  [   96/ 3200]\n",
      "loss: 0.127788  [  112/ 3200]\n",
      "loss: 0.035800  [  128/ 3200]\n",
      "loss: 0.174828  [  144/ 3200]\n",
      "loss: 0.074892  [  160/ 3200]\n",
      "loss: 0.056642  [  176/ 3200]\n",
      "loss: 0.058069  [  192/ 3200]\n",
      "loss: 0.083072  [  208/ 3200]\n",
      "loss: 0.464602  [  224/ 3200]\n",
      "loss: 0.276451  [  240/ 3200]\n",
      "loss: 0.117616  [  256/ 3200]\n",
      "loss: 0.084704  [  272/ 3200]\n",
      "loss: 0.195044  [  288/ 3200]\n",
      "loss: 0.201495  [  304/ 3200]\n",
      "loss: 0.333514  [  320/ 3200]\n",
      "loss: 0.231651  [  336/ 3200]\n",
      "loss: 0.477073  [  352/ 3200]\n",
      "loss: 0.150384  [  368/ 3200]\n",
      "loss: 0.168456  [  384/ 3200]\n",
      "loss: 0.178952  [  400/ 3200]\n",
      "loss: 0.426045  [  416/ 3200]\n",
      "loss: 0.171075  [  432/ 3200]\n",
      "loss: 0.052026  [  448/ 3200]\n",
      "loss: 0.137152  [  464/ 3200]\n",
      "loss: 0.107451  [  480/ 3200]\n",
      "loss: 0.045339  [  496/ 3200]\n",
      "loss: 0.225757  [  512/ 3200]\n",
      "loss: 0.061422  [  528/ 3200]\n",
      "loss: 0.027880  [  544/ 3200]\n",
      "loss: 0.386639  [  560/ 3200]\n",
      "loss: 0.106231  [  576/ 3200]\n",
      "loss: 0.193230  [  592/ 3200]\n",
      "loss: 0.198433  [  608/ 3200]\n",
      "loss: 0.070619  [  624/ 3200]\n",
      "loss: 0.198333  [  640/ 3200]\n",
      "loss: 0.118278  [  656/ 3200]\n",
      "loss: 0.347631  [  672/ 3200]\n",
      "loss: 0.096088  [  688/ 3200]\n",
      "loss: 0.205604  [  704/ 3200]\n",
      "loss: 0.097820  [  720/ 3200]\n",
      "loss: 0.148421  [  736/ 3200]\n",
      "loss: 0.224173  [  752/ 3200]\n",
      "loss: 0.080591  [  768/ 3200]\n",
      "loss: 0.043783  [  784/ 3200]\n",
      "loss: 0.089175  [  800/ 3200]\n",
      "loss: 0.337870  [  816/ 3200]\n",
      "loss: 0.176500  [  832/ 3200]\n",
      "loss: 0.096807  [  848/ 3200]\n",
      "loss: 0.179393  [  864/ 3200]\n",
      "loss: 0.134291  [  880/ 3200]\n",
      "loss: 0.071066  [  896/ 3200]\n",
      "loss: 0.382487  [  912/ 3200]\n",
      "loss: 0.180037  [  928/ 3200]\n",
      "loss: 0.347085  [  944/ 3200]\n",
      "loss: 0.166786  [  960/ 3200]\n",
      "loss: 0.040573  [  976/ 3200]\n",
      "loss: 0.121055  [  992/ 3200]\n",
      "loss: 0.187485  [ 1008/ 3200]\n",
      "loss: 0.236032  [ 1024/ 3200]\n",
      "loss: 0.214521  [ 1040/ 3200]\n",
      "loss: 0.287516  [ 1056/ 3200]\n",
      "loss: 0.063409  [ 1072/ 3200]\n",
      "loss: 0.132167  [ 1088/ 3200]\n",
      "loss: 0.061793  [ 1104/ 3200]\n",
      "loss: 0.156824  [ 1120/ 3200]\n",
      "loss: 0.114531  [ 1136/ 3200]\n",
      "loss: 0.092112  [ 1152/ 3200]\n",
      "loss: 0.148958  [ 1168/ 3200]\n",
      "loss: 0.378908  [ 1184/ 3200]\n",
      "loss: 0.159349  [ 1200/ 3200]\n",
      "loss: 0.072390  [ 1216/ 3200]\n",
      "loss: 0.145314  [ 1232/ 3200]\n",
      "loss: 0.515159  [ 1248/ 3200]\n",
      "loss: 0.101245  [ 1264/ 3200]\n",
      "loss: 0.071255  [ 1280/ 3200]\n",
      "loss: 0.145199  [ 1296/ 3200]\n",
      "loss: 0.086837  [ 1312/ 3200]\n",
      "loss: 0.362199  [ 1328/ 3200]\n",
      "loss: 0.199400  [ 1344/ 3200]\n",
      "loss: 0.064312  [ 1360/ 3200]\n",
      "loss: 0.023323  [ 1376/ 3200]\n",
      "loss: 0.141927  [ 1392/ 3200]\n",
      "loss: 0.135884  [ 1408/ 3200]\n",
      "loss: 0.171362  [ 1424/ 3200]\n",
      "loss: 0.161360  [ 1440/ 3200]\n",
      "loss: 0.025661  [ 1456/ 3200]\n",
      "loss: 0.300318  [ 1472/ 3200]\n",
      "loss: 0.242658  [ 1488/ 3200]\n",
      "loss: 0.191415  [ 1504/ 3200]\n",
      "loss: 0.180138  [ 1520/ 3200]\n",
      "loss: 0.241626  [ 1536/ 3200]\n",
      "loss: 0.202218  [ 1552/ 3200]\n",
      "loss: 0.216950  [ 1568/ 3200]\n",
      "loss: 0.087841  [ 1584/ 3200]\n",
      "loss: 0.109417  [ 1600/ 3200]\n",
      "loss: 0.239682  [ 1616/ 3200]\n",
      "loss: 0.587279  [ 1632/ 3200]\n",
      "loss: 0.070172  [ 1648/ 3200]\n",
      "loss: 0.130847  [ 1664/ 3200]\n",
      "loss: 0.041510  [ 1680/ 3200]\n",
      "loss: 0.125722  [ 1696/ 3200]\n",
      "loss: 0.231825  [ 1712/ 3200]\n",
      "loss: 0.186868  [ 1728/ 3200]\n",
      "loss: 0.063779  [ 1744/ 3200]\n",
      "loss: 0.156394  [ 1760/ 3200]\n",
      "loss: 0.132585  [ 1776/ 3200]\n",
      "loss: 0.052195  [ 1792/ 3200]\n",
      "loss: 0.261221  [ 1808/ 3200]\n",
      "loss: 0.046608  [ 1824/ 3200]\n",
      "loss: 0.104904  [ 1840/ 3200]\n",
      "loss: 0.125781  [ 1856/ 3200]\n",
      "loss: 0.099487  [ 1872/ 3200]\n",
      "loss: 0.049138  [ 1888/ 3200]\n",
      "loss: 0.091723  [ 1904/ 3200]\n",
      "loss: 0.110052  [ 1920/ 3200]\n",
      "loss: 0.071101  [ 1936/ 3200]\n",
      "loss: 0.279638  [ 1952/ 3200]\n",
      "loss: 0.176704  [ 1968/ 3200]\n",
      "loss: 0.018908  [ 1984/ 3200]\n",
      "loss: 0.092632  [ 2000/ 3200]\n",
      "loss: 0.037623  [ 2016/ 3200]\n",
      "loss: 0.373457  [ 2032/ 3200]\n",
      "loss: 0.221932  [ 2048/ 3200]\n",
      "loss: 0.236208  [ 2064/ 3200]\n",
      "loss: 0.226939  [ 2080/ 3200]\n",
      "loss: 0.269723  [ 2096/ 3200]\n",
      "loss: 0.284927  [ 2112/ 3200]\n",
      "loss: 0.271318  [ 2128/ 3200]\n",
      "loss: 0.154054  [ 2144/ 3200]\n",
      "loss: 0.159139  [ 2160/ 3200]\n",
      "loss: 0.050805  [ 2176/ 3200]\n",
      "loss: 0.144230  [ 2192/ 3200]\n",
      "loss: 0.137221  [ 2208/ 3200]\n",
      "loss: 0.184365  [ 2224/ 3200]\n",
      "loss: 0.093425  [ 2240/ 3200]\n",
      "loss: 0.111041  [ 2256/ 3200]\n",
      "loss: 0.276361  [ 2272/ 3200]\n",
      "loss: 0.141816  [ 2288/ 3200]\n",
      "loss: 0.040444  [ 2304/ 3200]\n",
      "loss: 0.055567  [ 2320/ 3200]\n",
      "loss: 0.137862  [ 2336/ 3200]\n",
      "loss: 0.071825  [ 2352/ 3200]\n",
      "loss: 0.087897  [ 2368/ 3200]\n",
      "loss: 0.247564  [ 2384/ 3200]\n",
      "loss: 0.211884  [ 2400/ 3200]\n",
      "loss: 0.380198  [ 2416/ 3200]\n",
      "loss: 0.117033  [ 2432/ 3200]\n",
      "loss: 0.320521  [ 2448/ 3200]\n",
      "loss: 0.148856  [ 2464/ 3200]\n",
      "loss: 0.120437  [ 2480/ 3200]\n",
      "loss: 0.106989  [ 2496/ 3200]\n",
      "loss: 0.274590  [ 2512/ 3200]\n",
      "loss: 0.078636  [ 2528/ 3200]\n",
      "loss: 0.143941  [ 2544/ 3200]\n",
      "loss: 0.051974  [ 2560/ 3200]\n",
      "loss: 0.081038  [ 2576/ 3200]\n",
      "loss: 0.046431  [ 2592/ 3200]\n",
      "loss: 0.164565  [ 2608/ 3200]\n",
      "loss: 0.075429  [ 2624/ 3200]\n",
      "loss: 0.058451  [ 2640/ 3200]\n",
      "loss: 0.077170  [ 2656/ 3200]\n",
      "loss: 0.211938  [ 2672/ 3200]\n",
      "loss: 0.106518  [ 2688/ 3200]\n",
      "loss: 0.158120  [ 2704/ 3200]\n",
      "loss: 0.045926  [ 2720/ 3200]\n",
      "loss: 0.194350  [ 2736/ 3200]\n",
      "loss: 0.250136  [ 2752/ 3200]\n",
      "loss: 0.137389  [ 2768/ 3200]\n",
      "loss: 0.143266  [ 2784/ 3200]\n",
      "loss: 0.192165  [ 2800/ 3200]\n",
      "loss: 0.089046  [ 2816/ 3200]\n",
      "loss: 0.287471  [ 2832/ 3200]\n",
      "loss: 0.313270  [ 2848/ 3200]\n",
      "loss: 0.196454  [ 2864/ 3200]\n",
      "loss: 0.099722  [ 2880/ 3200]\n",
      "loss: 0.058918  [ 2896/ 3200]\n",
      "loss: 0.263263  [ 2912/ 3200]\n",
      "loss: 0.172645  [ 2928/ 3200]\n",
      "loss: 0.026668  [ 2944/ 3200]\n",
      "loss: 0.105403  [ 2960/ 3200]\n",
      "loss: 0.056259  [ 2976/ 3200]\n",
      "loss: 0.043417  [ 2992/ 3200]\n",
      "loss: 0.084761  [ 3008/ 3200]\n",
      "loss: 0.188909  [ 3024/ 3200]\n",
      "loss: 0.586762  [ 3040/ 3200]\n",
      "loss: 0.207215  [ 3056/ 3200]\n",
      "loss: 0.121052  [ 3072/ 3200]\n",
      "loss: 0.103005  [ 3088/ 3200]\n",
      "loss: 0.101594  [ 3104/ 3200]\n",
      "loss: 0.245875  [ 3120/ 3200]\n",
      "loss: 0.101542  [ 3136/ 3200]\n",
      "loss: 0.307414  [ 3152/ 3200]\n",
      "loss: 0.049929  [ 3168/ 3200]\n",
      "loss: 0.090821  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.135671  [    0/ 3200]\n",
      "loss: 0.118241  [   16/ 3200]\n",
      "loss: 0.105727  [   32/ 3200]\n",
      "loss: 0.064325  [   48/ 3200]\n",
      "loss: 0.059989  [   64/ 3200]\n",
      "loss: 0.200881  [   80/ 3200]\n",
      "loss: 0.127376  [   96/ 3200]\n",
      "loss: 0.276490  [  112/ 3200]\n",
      "loss: 0.049415  [  128/ 3200]\n",
      "loss: 0.047575  [  144/ 3200]\n",
      "loss: 0.058500  [  160/ 3200]\n",
      "loss: 0.093423  [  176/ 3200]\n",
      "loss: 0.169994  [  192/ 3200]\n",
      "loss: 0.154884  [  208/ 3200]\n",
      "loss: 0.055508  [  224/ 3200]\n",
      "loss: 0.114011  [  240/ 3200]\n",
      "loss: 0.257887  [  256/ 3200]\n",
      "loss: 0.050585  [  272/ 3200]\n",
      "loss: 0.080077  [  288/ 3200]\n",
      "loss: 0.036676  [  304/ 3200]\n",
      "loss: 0.200636  [  320/ 3200]\n",
      "loss: 0.084640  [  336/ 3200]\n",
      "loss: 0.079707  [  352/ 3200]\n",
      "loss: 0.046836  [  368/ 3200]\n",
      "loss: 0.138182  [  384/ 3200]\n",
      "loss: 0.168622  [  400/ 3200]\n",
      "loss: 0.119513  [  416/ 3200]\n",
      "loss: 0.102789  [  432/ 3200]\n",
      "loss: 0.157347  [  448/ 3200]\n",
      "loss: 0.086343  [  464/ 3200]\n",
      "loss: 0.164075  [  480/ 3200]\n",
      "loss: 0.279596  [  496/ 3200]\n",
      "loss: 0.082869  [  512/ 3200]\n",
      "loss: 0.178867  [  528/ 3200]\n",
      "loss: 0.076523  [  544/ 3200]\n",
      "loss: 0.052207  [  560/ 3200]\n",
      "loss: 0.112802  [  576/ 3200]\n",
      "loss: 0.263722  [  592/ 3200]\n",
      "loss: 0.267604  [  608/ 3200]\n",
      "loss: 0.144181  [  624/ 3200]\n",
      "loss: 0.071776  [  640/ 3200]\n",
      "loss: 0.047762  [  656/ 3200]\n",
      "loss: 0.100362  [  672/ 3200]\n",
      "loss: 0.172637  [  688/ 3200]\n",
      "loss: 0.136621  [  704/ 3200]\n",
      "loss: 0.090770  [  720/ 3200]\n",
      "loss: 0.041386  [  736/ 3200]\n",
      "loss: 0.080202  [  752/ 3200]\n",
      "loss: 0.073683  [  768/ 3200]\n",
      "loss: 0.034631  [  784/ 3200]\n",
      "loss: 0.185930  [  800/ 3200]\n",
      "loss: 0.086798  [  816/ 3200]\n",
      "loss: 0.176437  [  832/ 3200]\n",
      "loss: 0.074558  [  848/ 3200]\n",
      "loss: 0.067525  [  864/ 3200]\n",
      "loss: 0.075732  [  880/ 3200]\n",
      "loss: 0.100532  [  896/ 3200]\n",
      "loss: 0.054227  [  912/ 3200]\n",
      "loss: 0.196476  [  928/ 3200]\n",
      "loss: 0.039548  [  944/ 3200]\n",
      "loss: 0.132056  [  960/ 3200]\n",
      "loss: 0.079176  [  976/ 3200]\n",
      "loss: 0.024156  [  992/ 3200]\n",
      "loss: 0.079915  [ 1008/ 3200]\n",
      "loss: 0.124419  [ 1024/ 3200]\n",
      "loss: 0.055543  [ 1040/ 3200]\n",
      "loss: 0.153249  [ 1056/ 3200]\n",
      "loss: 0.102421  [ 1072/ 3200]\n",
      "loss: 0.174962  [ 1088/ 3200]\n",
      "loss: 0.062811  [ 1104/ 3200]\n",
      "loss: 0.050676  [ 1120/ 3200]\n",
      "loss: 0.026014  [ 1136/ 3200]\n",
      "loss: 0.028147  [ 1152/ 3200]\n",
      "loss: 0.057184  [ 1168/ 3200]\n",
      "loss: 0.133951  [ 1184/ 3200]\n",
      "loss: 0.053036  [ 1200/ 3200]\n",
      "loss: 0.017459  [ 1216/ 3200]\n",
      "loss: 0.070167  [ 1232/ 3200]\n",
      "loss: 0.044995  [ 1248/ 3200]\n",
      "loss: 0.113306  [ 1264/ 3200]\n",
      "loss: 0.069915  [ 1280/ 3200]\n",
      "loss: 0.043978  [ 1296/ 3200]\n",
      "loss: 0.059385  [ 1312/ 3200]\n",
      "loss: 0.180986  [ 1328/ 3200]\n",
      "loss: 0.633129  [ 1344/ 3200]\n",
      "loss: 0.121636  [ 1360/ 3200]\n",
      "loss: 0.055659  [ 1376/ 3200]\n",
      "loss: 0.046020  [ 1392/ 3200]\n",
      "loss: 0.122053  [ 1408/ 3200]\n",
      "loss: 0.072031  [ 1424/ 3200]\n",
      "loss: 0.143866  [ 1440/ 3200]\n",
      "loss: 0.041498  [ 1456/ 3200]\n",
      "loss: 0.065443  [ 1472/ 3200]\n",
      "loss: 0.218776  [ 1488/ 3200]\n",
      "loss: 0.551848  [ 1504/ 3200]\n",
      "loss: 0.116473  [ 1520/ 3200]\n",
      "loss: 0.116616  [ 1536/ 3200]\n",
      "loss: 0.062842  [ 1552/ 3200]\n",
      "loss: 0.106157  [ 1568/ 3200]\n",
      "loss: 0.123579  [ 1584/ 3200]\n",
      "loss: 0.057621  [ 1600/ 3200]\n",
      "loss: 0.119808  [ 1616/ 3200]\n",
      "loss: 0.043764  [ 1632/ 3200]\n",
      "loss: 0.052657  [ 1648/ 3200]\n",
      "loss: 0.115439  [ 1664/ 3200]\n",
      "loss: 0.324350  [ 1680/ 3200]\n",
      "loss: 0.195219  [ 1696/ 3200]\n",
      "loss: 0.106974  [ 1712/ 3200]\n",
      "loss: 0.090624  [ 1728/ 3200]\n",
      "loss: 0.464882  [ 1744/ 3200]\n",
      "loss: 0.087472  [ 1760/ 3200]\n",
      "loss: 0.113864  [ 1776/ 3200]\n",
      "loss: 0.187992  [ 1792/ 3200]\n",
      "loss: 0.180744  [ 1808/ 3200]\n",
      "loss: 0.140640  [ 1824/ 3200]\n",
      "loss: 0.021317  [ 1840/ 3200]\n",
      "loss: 0.302328  [ 1856/ 3200]\n",
      "loss: 0.191620  [ 1872/ 3200]\n",
      "loss: 0.117946  [ 1888/ 3200]\n",
      "loss: 0.670738  [ 1904/ 3200]\n",
      "loss: 0.212711  [ 1920/ 3200]\n",
      "loss: 0.339858  [ 1936/ 3200]\n",
      "loss: 0.043939  [ 1952/ 3200]\n",
      "loss: 0.067220  [ 1968/ 3200]\n",
      "loss: 0.203529  [ 1984/ 3200]\n",
      "loss: 0.112589  [ 2000/ 3200]\n",
      "loss: 0.061285  [ 2016/ 3200]\n",
      "loss: 0.038143  [ 2032/ 3200]\n",
      "loss: 0.133339  [ 2048/ 3200]\n",
      "loss: 0.112955  [ 2064/ 3200]\n",
      "loss: 0.362751  [ 2080/ 3200]\n",
      "loss: 0.051718  [ 2096/ 3200]\n",
      "loss: 0.175949  [ 2112/ 3200]\n",
      "loss: 0.157268  [ 2128/ 3200]\n",
      "loss: 0.152741  [ 2144/ 3200]\n",
      "loss: 0.126256  [ 2160/ 3200]\n",
      "loss: 0.149036  [ 2176/ 3200]\n",
      "loss: 0.061092  [ 2192/ 3200]\n",
      "loss: 0.237608  [ 2208/ 3200]\n",
      "loss: 0.194539  [ 2224/ 3200]\n",
      "loss: 0.290155  [ 2240/ 3200]\n",
      "loss: 0.143180  [ 2256/ 3200]\n",
      "loss: 0.157506  [ 2272/ 3200]\n",
      "loss: 0.067932  [ 2288/ 3200]\n",
      "loss: 0.075460  [ 2304/ 3200]\n",
      "loss: 0.029627  [ 2320/ 3200]\n",
      "loss: 0.104659  [ 2336/ 3200]\n",
      "loss: 0.262199  [ 2352/ 3200]\n",
      "loss: 0.104964  [ 2368/ 3200]\n",
      "loss: 0.123322  [ 2384/ 3200]\n",
      "loss: 0.132961  [ 2400/ 3200]\n",
      "loss: 0.259662  [ 2416/ 3200]\n",
      "loss: 0.191750  [ 2432/ 3200]\n",
      "loss: 0.156749  [ 2448/ 3200]\n",
      "loss: 0.181292  [ 2464/ 3200]\n",
      "loss: 0.312631  [ 2480/ 3200]\n",
      "loss: 0.020861  [ 2496/ 3200]\n",
      "loss: 0.063855  [ 2512/ 3200]\n",
      "loss: 0.110209  [ 2528/ 3200]\n",
      "loss: 0.073609  [ 2544/ 3200]\n",
      "loss: 0.052332  [ 2560/ 3200]\n",
      "loss: 0.091168  [ 2576/ 3200]\n",
      "loss: 0.086104  [ 2592/ 3200]\n",
      "loss: 0.089425  [ 2608/ 3200]\n",
      "loss: 0.068117  [ 2624/ 3200]\n",
      "loss: 0.065824  [ 2640/ 3200]\n",
      "loss: 0.135496  [ 2656/ 3200]\n",
      "loss: 0.452081  [ 2672/ 3200]\n",
      "loss: 0.063886  [ 2688/ 3200]\n",
      "loss: 0.086136  [ 2704/ 3200]\n",
      "loss: 0.189426  [ 2720/ 3200]\n",
      "loss: 0.149813  [ 2736/ 3200]\n",
      "loss: 0.083815  [ 2752/ 3200]\n",
      "loss: 0.151631  [ 2768/ 3200]\n",
      "loss: 0.094713  [ 2784/ 3200]\n",
      "loss: 0.197506  [ 2800/ 3200]\n",
      "loss: 0.200263  [ 2816/ 3200]\n",
      "loss: 0.064810  [ 2832/ 3200]\n",
      "loss: 0.201032  [ 2848/ 3200]\n",
      "loss: 0.072546  [ 2864/ 3200]\n",
      "loss: 0.123231  [ 2880/ 3200]\n",
      "loss: 0.099045  [ 2896/ 3200]\n",
      "loss: 0.054211  [ 2912/ 3200]\n",
      "loss: 0.078483  [ 2928/ 3200]\n",
      "loss: 0.057509  [ 2944/ 3200]\n",
      "loss: 0.028865  [ 2960/ 3200]\n",
      "loss: 0.100097  [ 2976/ 3200]\n",
      "loss: 0.348470  [ 2992/ 3200]\n",
      "loss: 0.260390  [ 3008/ 3200]\n",
      "loss: 0.058207  [ 3024/ 3200]\n",
      "loss: 0.047819  [ 3040/ 3200]\n",
      "loss: 0.208342  [ 3056/ 3200]\n",
      "loss: 0.196466  [ 3072/ 3200]\n",
      "loss: 0.175516  [ 3088/ 3200]\n",
      "loss: 0.377744  [ 3104/ 3200]\n",
      "loss: 0.059745  [ 3120/ 3200]\n",
      "loss: 0.061870  [ 3136/ 3200]\n",
      "loss: 0.186261  [ 3152/ 3200]\n",
      "loss: 0.060509  [ 3168/ 3200]\n",
      "loss: 0.154133  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 0.180974  [    0/ 3200]\n",
      "loss: 0.059962  [   16/ 3200]\n",
      "loss: 0.075393  [   32/ 3200]\n",
      "loss: 0.183416  [   48/ 3200]\n",
      "loss: 0.076314  [   64/ 3200]\n",
      "loss: 0.407551  [   80/ 3200]\n",
      "loss: 0.278830  [   96/ 3200]\n",
      "loss: 0.192761  [  112/ 3200]\n",
      "loss: 0.059410  [  128/ 3200]\n",
      "loss: 0.061840  [  144/ 3200]\n",
      "loss: 0.050131  [  160/ 3200]\n",
      "loss: 0.105444  [  176/ 3200]\n",
      "loss: 0.028244  [  192/ 3200]\n",
      "loss: 0.039817  [  208/ 3200]\n",
      "loss: 0.032980  [  224/ 3200]\n",
      "loss: 0.072330  [  240/ 3200]\n",
      "loss: 0.035878  [  256/ 3200]\n",
      "loss: 0.116102  [  272/ 3200]\n",
      "loss: 0.040010  [  288/ 3200]\n",
      "loss: 0.099136  [  304/ 3200]\n",
      "loss: 0.075950  [  320/ 3200]\n",
      "loss: 0.117979  [  336/ 3200]\n",
      "loss: 0.043683  [  352/ 3200]\n",
      "loss: 0.264848  [  368/ 3200]\n",
      "loss: 0.055063  [  384/ 3200]\n",
      "loss: 0.045570  [  400/ 3200]\n",
      "loss: 0.149007  [  416/ 3200]\n",
      "loss: 0.114745  [  432/ 3200]\n",
      "loss: 0.077352  [  448/ 3200]\n",
      "loss: 0.235595  [  464/ 3200]\n",
      "loss: 0.396136  [  480/ 3200]\n",
      "loss: 0.208925  [  496/ 3200]\n",
      "loss: 0.099133  [  512/ 3200]\n",
      "loss: 0.040888  [  528/ 3200]\n",
      "loss: 0.052848  [  544/ 3200]\n",
      "loss: 0.103845  [  560/ 3200]\n",
      "loss: 0.097934  [  576/ 3200]\n",
      "loss: 0.121793  [  592/ 3200]\n",
      "loss: 0.030149  [  608/ 3200]\n",
      "loss: 0.160635  [  624/ 3200]\n",
      "loss: 0.020232  [  640/ 3200]\n",
      "loss: 0.086726  [  656/ 3200]\n",
      "loss: 0.059732  [  672/ 3200]\n",
      "loss: 0.042157  [  688/ 3200]\n",
      "loss: 0.062507  [  704/ 3200]\n",
      "loss: 0.128861  [  720/ 3200]\n",
      "loss: 0.016690  [  736/ 3200]\n",
      "loss: 0.167153  [  752/ 3200]\n",
      "loss: 0.041412  [  768/ 3200]\n",
      "loss: 0.032190  [  784/ 3200]\n",
      "loss: 0.031791  [  800/ 3200]\n",
      "loss: 0.088901  [  816/ 3200]\n",
      "loss: 0.048944  [  832/ 3200]\n",
      "loss: 0.173815  [  848/ 3200]\n",
      "loss: 0.064715  [  864/ 3200]\n",
      "loss: 0.030988  [  880/ 3200]\n",
      "loss: 0.070539  [  896/ 3200]\n",
      "loss: 0.214095  [  912/ 3200]\n",
      "loss: 0.183527  [  928/ 3200]\n",
      "loss: 0.121963  [  944/ 3200]\n",
      "loss: 0.139887  [  960/ 3200]\n",
      "loss: 0.066476  [  976/ 3200]\n",
      "loss: 0.085774  [  992/ 3200]\n",
      "loss: 0.048926  [ 1008/ 3200]\n",
      "loss: 0.022304  [ 1024/ 3200]\n",
      "loss: 0.045033  [ 1040/ 3200]\n",
      "loss: 0.199737  [ 1056/ 3200]\n",
      "loss: 0.061946  [ 1072/ 3200]\n",
      "loss: 0.184579  [ 1088/ 3200]\n",
      "loss: 0.057773  [ 1104/ 3200]\n",
      "loss: 0.097785  [ 1120/ 3200]\n",
      "loss: 0.089650  [ 1136/ 3200]\n",
      "loss: 0.176632  [ 1152/ 3200]\n",
      "loss: 0.245210  [ 1168/ 3200]\n",
      "loss: 0.042999  [ 1184/ 3200]\n",
      "loss: 0.130451  [ 1200/ 3200]\n",
      "loss: 0.184558  [ 1216/ 3200]\n",
      "loss: 0.100354  [ 1232/ 3200]\n",
      "loss: 0.043516  [ 1248/ 3200]\n",
      "loss: 0.070959  [ 1264/ 3200]\n",
      "loss: 0.037365  [ 1280/ 3200]\n",
      "loss: 0.038736  [ 1296/ 3200]\n",
      "loss: 0.048565  [ 1312/ 3200]\n",
      "loss: 0.059758  [ 1328/ 3200]\n",
      "loss: 0.069317  [ 1344/ 3200]\n",
      "loss: 0.166543  [ 1360/ 3200]\n",
      "loss: 0.034360  [ 1376/ 3200]\n",
      "loss: 0.139512  [ 1392/ 3200]\n",
      "loss: 0.047906  [ 1408/ 3200]\n",
      "loss: 0.115390  [ 1424/ 3200]\n",
      "loss: 0.150849  [ 1440/ 3200]\n",
      "loss: 0.300316  [ 1456/ 3200]\n",
      "loss: 0.069456  [ 1472/ 3200]\n",
      "loss: 0.262419  [ 1488/ 3200]\n",
      "loss: 0.061138  [ 1504/ 3200]\n",
      "loss: 0.222305  [ 1520/ 3200]\n",
      "loss: 0.164106  [ 1536/ 3200]\n",
      "loss: 0.060166  [ 1552/ 3200]\n",
      "loss: 0.061732  [ 1568/ 3200]\n",
      "loss: 0.030535  [ 1584/ 3200]\n",
      "loss: 0.061654  [ 1600/ 3200]\n",
      "loss: 0.134447  [ 1616/ 3200]\n",
      "loss: 0.070911  [ 1632/ 3200]\n",
      "loss: 0.055683  [ 1648/ 3200]\n",
      "loss: 0.094809  [ 1664/ 3200]\n",
      "loss: 0.185423  [ 1680/ 3200]\n",
      "loss: 0.050311  [ 1696/ 3200]\n",
      "loss: 0.145138  [ 1712/ 3200]\n",
      "loss: 0.197719  [ 1728/ 3200]\n",
      "loss: 0.177606  [ 1744/ 3200]\n",
      "loss: 0.067284  [ 1760/ 3200]\n",
      "loss: 0.098686  [ 1776/ 3200]\n",
      "loss: 0.054753  [ 1792/ 3200]\n",
      "loss: 0.083297  [ 1808/ 3200]\n",
      "loss: 0.122485  [ 1824/ 3200]\n",
      "loss: 0.071697  [ 1840/ 3200]\n",
      "loss: 0.071955  [ 1856/ 3200]\n",
      "loss: 0.064184  [ 1872/ 3200]\n",
      "loss: 0.028186  [ 1888/ 3200]\n",
      "loss: 0.088128  [ 1904/ 3200]\n",
      "loss: 0.033157  [ 1920/ 3200]\n",
      "loss: 0.079963  [ 1936/ 3200]\n",
      "loss: 0.061955  [ 1952/ 3200]\n",
      "loss: 0.009765  [ 1968/ 3200]\n",
      "loss: 0.101257  [ 1984/ 3200]\n",
      "loss: 0.079302  [ 2000/ 3200]\n",
      "loss: 0.037188  [ 2016/ 3200]\n",
      "loss: 0.149394  [ 2032/ 3200]\n",
      "loss: 0.314059  [ 2048/ 3200]\n",
      "loss: 0.162394  [ 2064/ 3200]\n",
      "loss: 0.019911  [ 2080/ 3200]\n",
      "loss: 0.126742  [ 2096/ 3200]\n",
      "loss: 0.086467  [ 2112/ 3200]\n",
      "loss: 0.130177  [ 2128/ 3200]\n",
      "loss: 0.078998  [ 2144/ 3200]\n",
      "loss: 0.077248  [ 2160/ 3200]\n",
      "loss: 0.036516  [ 2176/ 3200]\n",
      "loss: 0.096909  [ 2192/ 3200]\n",
      "loss: 0.059443  [ 2208/ 3200]\n",
      "loss: 0.037474  [ 2224/ 3200]\n",
      "loss: 0.035008  [ 2240/ 3200]\n",
      "loss: 0.027006  [ 2256/ 3200]\n",
      "loss: 0.088452  [ 2272/ 3200]\n",
      "loss: 0.098273  [ 2288/ 3200]\n",
      "loss: 0.100302  [ 2304/ 3200]\n",
      "loss: 0.053205  [ 2320/ 3200]\n",
      "loss: 0.054543  [ 2336/ 3200]\n",
      "loss: 0.044255  [ 2352/ 3200]\n",
      "loss: 0.153378  [ 2368/ 3200]\n",
      "loss: 0.082533  [ 2384/ 3200]\n",
      "loss: 0.097717  [ 2400/ 3200]\n",
      "loss: 0.062479  [ 2416/ 3200]\n",
      "loss: 0.261617  [ 2432/ 3200]\n",
      "loss: 0.124952  [ 2448/ 3200]\n",
      "loss: 0.114588  [ 2464/ 3200]\n",
      "loss: 0.057012  [ 2480/ 3200]\n",
      "loss: 0.018776  [ 2496/ 3200]\n",
      "loss: 0.431881  [ 2512/ 3200]\n",
      "loss: 0.082127  [ 2528/ 3200]\n",
      "loss: 0.060501  [ 2544/ 3200]\n",
      "loss: 0.159018  [ 2560/ 3200]\n",
      "loss: 0.268037  [ 2576/ 3200]\n",
      "loss: 0.075931  [ 2592/ 3200]\n",
      "loss: 0.020142  [ 2608/ 3200]\n",
      "loss: 0.012811  [ 2624/ 3200]\n",
      "loss: 0.036080  [ 2640/ 3200]\n",
      "loss: 0.349371  [ 2656/ 3200]\n",
      "loss: 0.144423  [ 2672/ 3200]\n",
      "loss: 0.372819  [ 2688/ 3200]\n",
      "loss: 0.350409  [ 2704/ 3200]\n",
      "loss: 0.116432  [ 2720/ 3200]\n",
      "loss: 0.115757  [ 2736/ 3200]\n",
      "loss: 0.078207  [ 2752/ 3200]\n",
      "loss: 0.056960  [ 2768/ 3200]\n",
      "loss: 0.048075  [ 2784/ 3200]\n",
      "loss: 0.102131  [ 2800/ 3200]\n",
      "loss: 0.037725  [ 2816/ 3200]\n",
      "loss: 0.057836  [ 2832/ 3200]\n",
      "loss: 0.139851  [ 2848/ 3200]\n",
      "loss: 0.079245  [ 2864/ 3200]\n",
      "loss: 0.066562  [ 2880/ 3200]\n",
      "loss: 0.100691  [ 2896/ 3200]\n",
      "loss: 0.083252  [ 2912/ 3200]\n",
      "loss: 0.036534  [ 2928/ 3200]\n",
      "loss: 0.036580  [ 2944/ 3200]\n",
      "loss: 0.113945  [ 2960/ 3200]\n",
      "loss: 0.079508  [ 2976/ 3200]\n",
      "loss: 0.027922  [ 2992/ 3200]\n",
      "loss: 0.069072  [ 3008/ 3200]\n",
      "loss: 0.023347  [ 3024/ 3200]\n",
      "loss: 0.113049  [ 3040/ 3200]\n",
      "loss: 0.067458  [ 3056/ 3200]\n",
      "loss: 0.091299  [ 3072/ 3200]\n",
      "loss: 0.132375  [ 3088/ 3200]\n",
      "loss: 0.272681  [ 3104/ 3200]\n",
      "loss: 0.254609  [ 3120/ 3200]\n",
      "loss: 0.052850  [ 3136/ 3200]\n",
      "loss: 0.062197  [ 3152/ 3200]\n",
      "loss: 0.060560  [ 3168/ 3200]\n",
      "loss: 0.051903  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 0.065527  [    0/ 3200]\n",
      "loss: 0.047277  [   16/ 3200]\n",
      "loss: 0.024394  [   32/ 3200]\n",
      "loss: 0.050418  [   48/ 3200]\n",
      "loss: 0.296984  [   64/ 3200]\n",
      "loss: 0.142024  [   80/ 3200]\n",
      "loss: 0.092241  [   96/ 3200]\n",
      "loss: 0.098972  [  112/ 3200]\n",
      "loss: 0.030501  [  128/ 3200]\n",
      "loss: 0.017568  [  144/ 3200]\n",
      "loss: 0.049894  [  160/ 3200]\n",
      "loss: 0.244468  [  176/ 3200]\n",
      "loss: 0.045214  [  192/ 3200]\n",
      "loss: 0.146647  [  208/ 3200]\n",
      "loss: 0.049344  [  224/ 3200]\n",
      "loss: 0.080478  [  240/ 3200]\n",
      "loss: 0.089115  [  256/ 3200]\n",
      "loss: 0.019231  [  272/ 3200]\n",
      "loss: 0.070618  [  288/ 3200]\n",
      "loss: 0.060760  [  304/ 3200]\n",
      "loss: 0.042518  [  320/ 3200]\n",
      "loss: 0.013609  [  336/ 3200]\n",
      "loss: 0.057478  [  352/ 3200]\n",
      "loss: 0.308264  [  368/ 3200]\n",
      "loss: 0.053015  [  384/ 3200]\n",
      "loss: 0.055583  [  400/ 3200]\n",
      "loss: 0.182237  [  416/ 3200]\n",
      "loss: 0.138749  [  432/ 3200]\n",
      "loss: 0.085958  [  448/ 3200]\n",
      "loss: 0.044286  [  464/ 3200]\n",
      "loss: 0.046586  [  480/ 3200]\n",
      "loss: 0.031584  [  496/ 3200]\n",
      "loss: 0.027443  [  512/ 3200]\n",
      "loss: 0.022205  [  528/ 3200]\n",
      "loss: 0.023549  [  544/ 3200]\n",
      "loss: 0.058175  [  560/ 3200]\n",
      "loss: 0.065750  [  576/ 3200]\n",
      "loss: 0.108226  [  592/ 3200]\n",
      "loss: 0.195134  [  608/ 3200]\n",
      "loss: 0.025129  [  624/ 3200]\n",
      "loss: 0.016097  [  640/ 3200]\n",
      "loss: 0.084460  [  656/ 3200]\n",
      "loss: 0.078089  [  672/ 3200]\n",
      "loss: 0.041097  [  688/ 3200]\n",
      "loss: 0.124902  [  704/ 3200]\n",
      "loss: 0.119132  [  720/ 3200]\n",
      "loss: 0.129462  [  736/ 3200]\n",
      "loss: 0.093110  [  752/ 3200]\n",
      "loss: 0.015012  [  768/ 3200]\n",
      "loss: 0.030183  [  784/ 3200]\n",
      "loss: 0.018311  [  800/ 3200]\n",
      "loss: 0.145038  [  816/ 3200]\n",
      "loss: 0.189625  [  832/ 3200]\n",
      "loss: 0.184689  [  848/ 3200]\n",
      "loss: 0.118607  [  864/ 3200]\n",
      "loss: 0.026278  [  880/ 3200]\n",
      "loss: 0.169819  [  896/ 3200]\n",
      "loss: 0.112327  [  912/ 3200]\n",
      "loss: 0.102587  [  928/ 3200]\n",
      "loss: 0.072391  [  944/ 3200]\n",
      "loss: 0.080592  [  960/ 3200]\n",
      "loss: 0.032753  [  976/ 3200]\n",
      "loss: 0.101840  [  992/ 3200]\n",
      "loss: 0.060094  [ 1008/ 3200]\n",
      "loss: 0.037533  [ 1024/ 3200]\n",
      "loss: 0.033936  [ 1040/ 3200]\n",
      "loss: 0.025188  [ 1056/ 3200]\n",
      "loss: 0.017954  [ 1072/ 3200]\n",
      "loss: 0.068548  [ 1088/ 3200]\n",
      "loss: 0.079446  [ 1104/ 3200]\n",
      "loss: 0.012466  [ 1120/ 3200]\n",
      "loss: 0.118191  [ 1136/ 3200]\n",
      "loss: 0.038324  [ 1152/ 3200]\n",
      "loss: 0.062207  [ 1168/ 3200]\n",
      "loss: 0.122803  [ 1184/ 3200]\n",
      "loss: 0.061370  [ 1200/ 3200]\n",
      "loss: 0.213507  [ 1216/ 3200]\n",
      "loss: 0.131970  [ 1232/ 3200]\n",
      "loss: 0.221352  [ 1248/ 3200]\n",
      "loss: 0.073725  [ 1264/ 3200]\n",
      "loss: 0.065903  [ 1280/ 3200]\n",
      "loss: 0.045373  [ 1296/ 3200]\n",
      "loss: 0.086286  [ 1312/ 3200]\n",
      "loss: 0.065829  [ 1328/ 3200]\n",
      "loss: 0.080059  [ 1344/ 3200]\n",
      "loss: 0.057535  [ 1360/ 3200]\n",
      "loss: 0.090372  [ 1376/ 3200]\n",
      "loss: 0.266098  [ 1392/ 3200]\n",
      "loss: 0.043550  [ 1408/ 3200]\n",
      "loss: 0.065960  [ 1424/ 3200]\n",
      "loss: 0.036554  [ 1440/ 3200]\n",
      "loss: 0.069762  [ 1456/ 3200]\n",
      "loss: 0.107106  [ 1472/ 3200]\n",
      "loss: 0.169778  [ 1488/ 3200]\n",
      "loss: 0.147295  [ 1504/ 3200]\n",
      "loss: 0.083328  [ 1520/ 3200]\n",
      "loss: 0.053672  [ 1536/ 3200]\n",
      "loss: 0.410447  [ 1552/ 3200]\n",
      "loss: 0.027654  [ 1568/ 3200]\n",
      "loss: 0.048966  [ 1584/ 3200]\n",
      "loss: 0.020903  [ 1600/ 3200]\n",
      "loss: 0.129908  [ 1616/ 3200]\n",
      "loss: 0.030776  [ 1632/ 3200]\n",
      "loss: 0.044175  [ 1648/ 3200]\n",
      "loss: 0.059063  [ 1664/ 3200]\n",
      "loss: 0.019016  [ 1680/ 3200]\n",
      "loss: 0.141203  [ 1696/ 3200]\n",
      "loss: 0.026314  [ 1712/ 3200]\n",
      "loss: 0.029710  [ 1728/ 3200]\n",
      "loss: 0.093158  [ 1744/ 3200]\n",
      "loss: 0.059670  [ 1760/ 3200]\n",
      "loss: 0.035112  [ 1776/ 3200]\n",
      "loss: 0.095731  [ 1792/ 3200]\n",
      "loss: 0.123983  [ 1808/ 3200]\n",
      "loss: 0.036162  [ 1824/ 3200]\n",
      "loss: 0.035257  [ 1840/ 3200]\n",
      "loss: 0.075558  [ 1856/ 3200]\n",
      "loss: 0.041311  [ 1872/ 3200]\n",
      "loss: 0.080730  [ 1888/ 3200]\n",
      "loss: 0.038507  [ 1904/ 3200]\n",
      "loss: 0.041835  [ 1920/ 3200]\n",
      "loss: 0.088618  [ 1936/ 3200]\n",
      "loss: 0.045770  [ 1952/ 3200]\n",
      "loss: 0.068827  [ 1968/ 3200]\n",
      "loss: 0.031622  [ 1984/ 3200]\n",
      "loss: 0.083742  [ 2000/ 3200]\n",
      "loss: 0.020482  [ 2016/ 3200]\n",
      "loss: 0.079240  [ 2032/ 3200]\n",
      "loss: 0.048973  [ 2048/ 3200]\n",
      "loss: 0.366830  [ 2064/ 3200]\n",
      "loss: 0.031194  [ 2080/ 3200]\n",
      "loss: 0.027129  [ 2096/ 3200]\n",
      "loss: 0.122947  [ 2112/ 3200]\n",
      "loss: 0.261777  [ 2128/ 3200]\n",
      "loss: 0.062456  [ 2144/ 3200]\n",
      "loss: 0.047201  [ 2160/ 3200]\n",
      "loss: 0.078815  [ 2176/ 3200]\n",
      "loss: 0.023689  [ 2192/ 3200]\n",
      "loss: 0.022259  [ 2208/ 3200]\n",
      "loss: 0.082620  [ 2224/ 3200]\n",
      "loss: 0.023905  [ 2240/ 3200]\n",
      "loss: 0.134846  [ 2256/ 3200]\n",
      "loss: 0.065875  [ 2272/ 3200]\n",
      "loss: 0.025124  [ 2288/ 3200]\n",
      "loss: 0.009569  [ 2304/ 3200]\n",
      "loss: 0.009454  [ 2320/ 3200]\n",
      "loss: 0.038299  [ 2336/ 3200]\n",
      "loss: 0.016935  [ 2352/ 3200]\n",
      "loss: 0.076596  [ 2368/ 3200]\n",
      "loss: 0.007746  [ 2384/ 3200]\n",
      "loss: 0.106428  [ 2400/ 3200]\n",
      "loss: 0.056243  [ 2416/ 3200]\n",
      "loss: 0.063308  [ 2432/ 3200]\n",
      "loss: 0.070026  [ 2448/ 3200]\n",
      "loss: 0.037167  [ 2464/ 3200]\n",
      "loss: 0.022819  [ 2480/ 3200]\n",
      "loss: 0.033130  [ 2496/ 3200]\n",
      "loss: 0.022596  [ 2512/ 3200]\n",
      "loss: 0.019797  [ 2528/ 3200]\n",
      "loss: 0.044682  [ 2544/ 3200]\n",
      "loss: 0.029823  [ 2560/ 3200]\n",
      "loss: 0.052030  [ 2576/ 3200]\n",
      "loss: 0.033252  [ 2592/ 3200]\n",
      "loss: 0.045141  [ 2608/ 3200]\n",
      "loss: 0.044516  [ 2624/ 3200]\n",
      "loss: 0.089415  [ 2640/ 3200]\n",
      "loss: 0.142838  [ 2656/ 3200]\n",
      "loss: 0.052926  [ 2672/ 3200]\n",
      "loss: 0.047168  [ 2688/ 3200]\n",
      "loss: 0.169617  [ 2704/ 3200]\n",
      "loss: 0.028413  [ 2720/ 3200]\n",
      "loss: 0.072864  [ 2736/ 3200]\n",
      "loss: 0.109880  [ 2752/ 3200]\n",
      "loss: 0.131536  [ 2768/ 3200]\n",
      "loss: 0.040621  [ 2784/ 3200]\n",
      "loss: 0.090911  [ 2800/ 3200]\n",
      "loss: 0.018149  [ 2816/ 3200]\n",
      "loss: 0.021554  [ 2832/ 3200]\n",
      "loss: 0.089791  [ 2848/ 3200]\n",
      "loss: 0.029728  [ 2864/ 3200]\n",
      "loss: 0.096935  [ 2880/ 3200]\n",
      "loss: 0.015968  [ 2896/ 3200]\n",
      "loss: 0.026553  [ 2912/ 3200]\n",
      "loss: 0.091524  [ 2928/ 3200]\n",
      "loss: 0.079794  [ 2944/ 3200]\n",
      "loss: 0.426631  [ 2960/ 3200]\n",
      "loss: 0.021918  [ 2976/ 3200]\n",
      "loss: 0.014887  [ 2992/ 3200]\n",
      "loss: 0.035223  [ 3008/ 3200]\n",
      "loss: 0.043617  [ 3024/ 3200]\n",
      "loss: 0.157102  [ 3040/ 3200]\n",
      "loss: 0.120488  [ 3056/ 3200]\n",
      "loss: 0.019865  [ 3072/ 3200]\n",
      "loss: 0.543234  [ 3088/ 3200]\n",
      "loss: 0.251241  [ 3104/ 3200]\n",
      "loss: 0.089233  [ 3120/ 3200]\n",
      "loss: 0.091204  [ 3136/ 3200]\n",
      "loss: 0.025330  [ 3152/ 3200]\n",
      "loss: 0.179853  [ 3168/ 3200]\n",
      "loss: 0.083374  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 0.045975  [    0/ 3200]\n",
      "loss: 0.037675  [   16/ 3200]\n",
      "loss: 0.151916  [   32/ 3200]\n",
      "loss: 0.024625  [   48/ 3200]\n",
      "loss: 0.040769  [   64/ 3200]\n",
      "loss: 0.019729  [   80/ 3200]\n",
      "loss: 0.087637  [   96/ 3200]\n",
      "loss: 0.113423  [  112/ 3200]\n",
      "loss: 0.050346  [  128/ 3200]\n",
      "loss: 0.022839  [  144/ 3200]\n",
      "loss: 0.090698  [  160/ 3200]\n",
      "loss: 0.039791  [  176/ 3200]\n",
      "loss: 0.012061  [  192/ 3200]\n",
      "loss: 0.082432  [  208/ 3200]\n",
      "loss: 0.035895  [  224/ 3200]\n",
      "loss: 0.109476  [  240/ 3200]\n",
      "loss: 0.060343  [  256/ 3200]\n",
      "loss: 0.074168  [  272/ 3200]\n",
      "loss: 0.030066  [  288/ 3200]\n",
      "loss: 0.055978  [  304/ 3200]\n",
      "loss: 0.023454  [  320/ 3200]\n",
      "loss: 0.065890  [  336/ 3200]\n",
      "loss: 0.048818  [  352/ 3200]\n",
      "loss: 0.153140  [  368/ 3200]\n",
      "loss: 0.018521  [  384/ 3200]\n",
      "loss: 0.076913  [  400/ 3200]\n",
      "loss: 0.103900  [  416/ 3200]\n",
      "loss: 0.028937  [  432/ 3200]\n",
      "loss: 0.020647  [  448/ 3200]\n",
      "loss: 0.047495  [  464/ 3200]\n",
      "loss: 0.053899  [  480/ 3200]\n",
      "loss: 0.012582  [  496/ 3200]\n",
      "loss: 0.092963  [  512/ 3200]\n",
      "loss: 0.061337  [  528/ 3200]\n",
      "loss: 0.033619  [  544/ 3200]\n",
      "loss: 0.027785  [  560/ 3200]\n",
      "loss: 0.075282  [  576/ 3200]\n",
      "loss: 0.053199  [  592/ 3200]\n",
      "loss: 0.315454  [  608/ 3200]\n",
      "loss: 0.044228  [  624/ 3200]\n",
      "loss: 0.038143  [  640/ 3200]\n",
      "loss: 0.028690  [  656/ 3200]\n",
      "loss: 0.115444  [  672/ 3200]\n",
      "loss: 0.018507  [  688/ 3200]\n",
      "loss: 0.045969  [  704/ 3200]\n",
      "loss: 0.088219  [  720/ 3200]\n",
      "loss: 0.080745  [  736/ 3200]\n",
      "loss: 0.061722  [  752/ 3200]\n",
      "loss: 0.016456  [  768/ 3200]\n",
      "loss: 0.071514  [  784/ 3200]\n",
      "loss: 0.034592  [  800/ 3200]\n",
      "loss: 0.024762  [  816/ 3200]\n",
      "loss: 0.024709  [  832/ 3200]\n",
      "loss: 0.034373  [  848/ 3200]\n",
      "loss: 0.074197  [  864/ 3200]\n",
      "loss: 0.013816  [  880/ 3200]\n",
      "loss: 0.041160  [  896/ 3200]\n",
      "loss: 0.065358  [  912/ 3200]\n",
      "loss: 0.090576  [  928/ 3200]\n",
      "loss: 0.040314  [  944/ 3200]\n",
      "loss: 0.025664  [  960/ 3200]\n",
      "loss: 0.046119  [  976/ 3200]\n",
      "loss: 0.059052  [  992/ 3200]\n",
      "loss: 0.036395  [ 1008/ 3200]\n",
      "loss: 0.067240  [ 1024/ 3200]\n",
      "loss: 0.065430  [ 1040/ 3200]\n",
      "loss: 0.072952  [ 1056/ 3200]\n",
      "loss: 0.050415  [ 1072/ 3200]\n",
      "loss: 0.055885  [ 1088/ 3200]\n",
      "loss: 0.035342  [ 1104/ 3200]\n",
      "loss: 0.047435  [ 1120/ 3200]\n",
      "loss: 0.028853  [ 1136/ 3200]\n",
      "loss: 0.028433  [ 1152/ 3200]\n",
      "loss: 0.025175  [ 1168/ 3200]\n",
      "loss: 0.023787  [ 1184/ 3200]\n",
      "loss: 0.047463  [ 1200/ 3200]\n",
      "loss: 0.020774  [ 1216/ 3200]\n",
      "loss: 0.073715  [ 1232/ 3200]\n",
      "loss: 0.127401  [ 1248/ 3200]\n",
      "loss: 0.047201  [ 1264/ 3200]\n",
      "loss: 0.039870  [ 1280/ 3200]\n",
      "loss: 0.022313  [ 1296/ 3200]\n",
      "loss: 0.015104  [ 1312/ 3200]\n",
      "loss: 0.043103  [ 1328/ 3200]\n",
      "loss: 0.040680  [ 1344/ 3200]\n",
      "loss: 0.013188  [ 1360/ 3200]\n",
      "loss: 0.037931  [ 1376/ 3200]\n",
      "loss: 0.040635  [ 1392/ 3200]\n",
      "loss: 0.022380  [ 1408/ 3200]\n",
      "loss: 0.046597  [ 1424/ 3200]\n",
      "loss: 0.038769  [ 1440/ 3200]\n",
      "loss: 0.046515  [ 1456/ 3200]\n",
      "loss: 0.054600  [ 1472/ 3200]\n",
      "loss: 0.058352  [ 1488/ 3200]\n",
      "loss: 0.040475  [ 1504/ 3200]\n",
      "loss: 0.043226  [ 1520/ 3200]\n",
      "loss: 0.085970  [ 1536/ 3200]\n",
      "loss: 0.056404  [ 1552/ 3200]\n",
      "loss: 0.011411  [ 1568/ 3200]\n",
      "loss: 0.059234  [ 1584/ 3200]\n",
      "loss: 0.064217  [ 1600/ 3200]\n",
      "loss: 0.047876  [ 1616/ 3200]\n",
      "loss: 0.010096  [ 1632/ 3200]\n",
      "loss: 0.051251  [ 1648/ 3200]\n",
      "loss: 0.024069  [ 1664/ 3200]\n",
      "loss: 0.032503  [ 1680/ 3200]\n",
      "loss: 0.030485  [ 1696/ 3200]\n",
      "loss: 0.056321  [ 1712/ 3200]\n",
      "loss: 0.011293  [ 1728/ 3200]\n",
      "loss: 0.038301  [ 1744/ 3200]\n",
      "loss: 0.301323  [ 1760/ 3200]\n",
      "loss: 0.061287  [ 1776/ 3200]\n",
      "loss: 0.013527  [ 1792/ 3200]\n",
      "loss: 0.031599  [ 1808/ 3200]\n",
      "loss: 0.019226  [ 1824/ 3200]\n",
      "loss: 0.015356  [ 1840/ 3200]\n",
      "loss: 0.016225  [ 1856/ 3200]\n",
      "loss: 0.006246  [ 1872/ 3200]\n",
      "loss: 0.033919  [ 1888/ 3200]\n",
      "loss: 0.013447  [ 1904/ 3200]\n",
      "loss: 0.062242  [ 1920/ 3200]\n",
      "loss: 0.026076  [ 1936/ 3200]\n",
      "loss: 0.028998  [ 1952/ 3200]\n",
      "loss: 0.026200  [ 1968/ 3200]\n",
      "loss: 0.091784  [ 1984/ 3200]\n",
      "loss: 0.113929  [ 2000/ 3200]\n",
      "loss: 0.047074  [ 2016/ 3200]\n",
      "loss: 0.153972  [ 2032/ 3200]\n",
      "loss: 0.022250  [ 2048/ 3200]\n",
      "loss: 0.033186  [ 2064/ 3200]\n",
      "loss: 0.029450  [ 2080/ 3200]\n",
      "loss: 0.019662  [ 2096/ 3200]\n",
      "loss: 0.014478  [ 2112/ 3200]\n",
      "loss: 0.052497  [ 2128/ 3200]\n",
      "loss: 0.018705  [ 2144/ 3200]\n",
      "loss: 0.019878  [ 2160/ 3200]\n",
      "loss: 0.049116  [ 2176/ 3200]\n",
      "loss: 0.015509  [ 2192/ 3200]\n",
      "loss: 0.082295  [ 2208/ 3200]\n",
      "loss: 0.025423  [ 2224/ 3200]\n",
      "loss: 0.021438  [ 2240/ 3200]\n",
      "loss: 0.011819  [ 2256/ 3200]\n",
      "loss: 0.030056  [ 2272/ 3200]\n",
      "loss: 0.026216  [ 2288/ 3200]\n",
      "loss: 0.053641  [ 2304/ 3200]\n",
      "loss: 0.007802  [ 2320/ 3200]\n",
      "loss: 0.065625  [ 2336/ 3200]\n",
      "loss: 0.096965  [ 2352/ 3200]\n",
      "loss: 0.020624  [ 2368/ 3200]\n",
      "loss: 0.053749  [ 2384/ 3200]\n",
      "loss: 0.020511  [ 2400/ 3200]\n",
      "loss: 0.037808  [ 2416/ 3200]\n",
      "loss: 0.014216  [ 2432/ 3200]\n",
      "loss: 0.075995  [ 2448/ 3200]\n",
      "loss: 0.014754  [ 2464/ 3200]\n",
      "loss: 0.052056  [ 2480/ 3200]\n",
      "loss: 0.011227  [ 2496/ 3200]\n",
      "loss: 0.384519  [ 2512/ 3200]\n",
      "loss: 0.106538  [ 2528/ 3200]\n",
      "loss: 0.275003  [ 2544/ 3200]\n",
      "loss: 0.062377  [ 2560/ 3200]\n",
      "loss: 0.066213  [ 2576/ 3200]\n",
      "loss: 0.079694  [ 2592/ 3200]\n",
      "loss: 0.142051  [ 2608/ 3200]\n",
      "loss: 0.029152  [ 2624/ 3200]\n",
      "loss: 0.023902  [ 2640/ 3200]\n",
      "loss: 0.011778  [ 2656/ 3200]\n",
      "loss: 0.042075  [ 2672/ 3200]\n",
      "loss: 0.056560  [ 2688/ 3200]\n",
      "loss: 0.030229  [ 2704/ 3200]\n",
      "loss: 0.021426  [ 2720/ 3200]\n",
      "loss: 0.434699  [ 2736/ 3200]\n",
      "loss: 0.099030  [ 2752/ 3200]\n",
      "loss: 0.049169  [ 2768/ 3200]\n",
      "loss: 0.126214  [ 2784/ 3200]\n",
      "loss: 0.014564  [ 2800/ 3200]\n",
      "loss: 0.039588  [ 2816/ 3200]\n",
      "loss: 0.049373  [ 2832/ 3200]\n",
      "loss: 0.111663  [ 2848/ 3200]\n",
      "loss: 0.050209  [ 2864/ 3200]\n",
      "loss: 0.117372  [ 2880/ 3200]\n",
      "loss: 0.175323  [ 2896/ 3200]\n",
      "loss: 0.156144  [ 2912/ 3200]\n",
      "loss: 0.014060  [ 2928/ 3200]\n",
      "loss: 0.047980  [ 2944/ 3200]\n",
      "loss: 0.056015  [ 2960/ 3200]\n",
      "loss: 0.045952  [ 2976/ 3200]\n",
      "loss: 0.083070  [ 2992/ 3200]\n",
      "loss: 0.109142  [ 3008/ 3200]\n",
      "loss: 0.067667  [ 3024/ 3200]\n",
      "loss: 0.013567  [ 3040/ 3200]\n",
      "loss: 0.058219  [ 3056/ 3200]\n",
      "loss: 0.068183  [ 3072/ 3200]\n",
      "loss: 0.044545  [ 3088/ 3200]\n",
      "loss: 0.034987  [ 3104/ 3200]\n",
      "loss: 0.056437  [ 3120/ 3200]\n",
      "loss: 0.030274  [ 3136/ 3200]\n",
      "loss: 0.376952  [ 3152/ 3200]\n",
      "loss: 0.038438  [ 3168/ 3200]\n",
      "loss: 0.030489  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 0.050860  [    0/ 3200]\n",
      "loss: 0.007760  [   16/ 3200]\n",
      "loss: 0.014017  [   32/ 3200]\n",
      "loss: 0.025101  [   48/ 3200]\n",
      "loss: 0.012268  [   64/ 3200]\n",
      "loss: 0.064723  [   80/ 3200]\n",
      "loss: 0.068863  [   96/ 3200]\n",
      "loss: 0.026714  [  112/ 3200]\n",
      "loss: 0.049001  [  128/ 3200]\n",
      "loss: 0.011750  [  144/ 3200]\n",
      "loss: 0.028587  [  160/ 3200]\n",
      "loss: 0.033359  [  176/ 3200]\n",
      "loss: 0.009271  [  192/ 3200]\n",
      "loss: 0.034200  [  208/ 3200]\n",
      "loss: 0.023551  [  224/ 3200]\n",
      "loss: 0.011095  [  240/ 3200]\n",
      "loss: 0.025381  [  256/ 3200]\n",
      "loss: 0.045974  [  272/ 3200]\n",
      "loss: 0.012518  [  288/ 3200]\n",
      "loss: 0.017615  [  304/ 3200]\n",
      "loss: 0.047225  [  320/ 3200]\n",
      "loss: 0.087243  [  336/ 3200]\n",
      "loss: 0.048219  [  352/ 3200]\n",
      "loss: 0.036689  [  368/ 3200]\n",
      "loss: 0.034199  [  384/ 3200]\n",
      "loss: 0.033224  [  400/ 3200]\n",
      "loss: 0.007860  [  416/ 3200]\n",
      "loss: 0.027102  [  432/ 3200]\n",
      "loss: 0.040968  [  448/ 3200]\n",
      "loss: 0.031393  [  464/ 3200]\n",
      "loss: 0.047053  [  480/ 3200]\n",
      "loss: 0.030171  [  496/ 3200]\n",
      "loss: 0.007396  [  512/ 3200]\n",
      "loss: 0.009294  [  528/ 3200]\n",
      "loss: 0.015702  [  544/ 3200]\n",
      "loss: 0.078106  [  560/ 3200]\n",
      "loss: 0.025530  [  576/ 3200]\n",
      "loss: 0.010257  [  592/ 3200]\n",
      "loss: 0.021717  [  608/ 3200]\n",
      "loss: 0.025007  [  624/ 3200]\n",
      "loss: 0.023877  [  640/ 3200]\n",
      "loss: 0.037101  [  656/ 3200]\n",
      "loss: 0.037417  [  672/ 3200]\n",
      "loss: 0.029632  [  688/ 3200]\n",
      "loss: 0.013036  [  704/ 3200]\n",
      "loss: 0.014350  [  720/ 3200]\n",
      "loss: 0.044596  [  736/ 3200]\n",
      "loss: 0.022466  [  752/ 3200]\n",
      "loss: 0.039009  [  768/ 3200]\n",
      "loss: 0.034033  [  784/ 3200]\n",
      "loss: 0.045447  [  800/ 3200]\n",
      "loss: 0.067335  [  816/ 3200]\n",
      "loss: 0.031478  [  832/ 3200]\n",
      "loss: 0.026457  [  848/ 3200]\n",
      "loss: 0.021339  [  864/ 3200]\n",
      "loss: 0.020200  [  880/ 3200]\n",
      "loss: 0.035120  [  896/ 3200]\n",
      "loss: 0.022582  [  912/ 3200]\n",
      "loss: 0.019776  [  928/ 3200]\n",
      "loss: 0.011207  [  944/ 3200]\n",
      "loss: 0.024446  [  960/ 3200]\n",
      "loss: 0.014736  [  976/ 3200]\n",
      "loss: 0.013277  [  992/ 3200]\n",
      "loss: 0.056129  [ 1008/ 3200]\n",
      "loss: 0.029204  [ 1024/ 3200]\n",
      "loss: 0.021205  [ 1040/ 3200]\n",
      "loss: 0.020003  [ 1056/ 3200]\n",
      "loss: 0.072591  [ 1072/ 3200]\n",
      "loss: 0.025179  [ 1088/ 3200]\n",
      "loss: 0.040544  [ 1104/ 3200]\n",
      "loss: 0.043031  [ 1120/ 3200]\n",
      "loss: 0.040811  [ 1136/ 3200]\n",
      "loss: 0.076689  [ 1152/ 3200]\n",
      "loss: 0.005192  [ 1168/ 3200]\n",
      "loss: 0.008445  [ 1184/ 3200]\n",
      "loss: 0.362156  [ 1200/ 3200]\n",
      "loss: 0.032748  [ 1216/ 3200]\n",
      "loss: 0.028795  [ 1232/ 3200]\n",
      "loss: 0.038164  [ 1248/ 3200]\n",
      "loss: 0.113467  [ 1264/ 3200]\n",
      "loss: 0.060194  [ 1280/ 3200]\n",
      "loss: 0.021914  [ 1296/ 3200]\n",
      "loss: 0.020399  [ 1312/ 3200]\n",
      "loss: 0.024309  [ 1328/ 3200]\n",
      "loss: 0.044225  [ 1344/ 3200]\n",
      "loss: 0.026332  [ 1360/ 3200]\n",
      "loss: 0.049352  [ 1376/ 3200]\n",
      "loss: 0.040665  [ 1392/ 3200]\n",
      "loss: 0.029536  [ 1408/ 3200]\n",
      "loss: 0.030286  [ 1424/ 3200]\n",
      "loss: 0.071194  [ 1440/ 3200]\n",
      "loss: 0.034045  [ 1456/ 3200]\n",
      "loss: 0.049259  [ 1472/ 3200]\n",
      "loss: 0.014246  [ 1488/ 3200]\n",
      "loss: 0.016043  [ 1504/ 3200]\n",
      "loss: 0.018508  [ 1520/ 3200]\n",
      "loss: 0.041212  [ 1536/ 3200]\n",
      "loss: 0.011433  [ 1552/ 3200]\n",
      "loss: 0.019110  [ 1568/ 3200]\n",
      "loss: 0.035591  [ 1584/ 3200]\n",
      "loss: 0.013741  [ 1600/ 3200]\n",
      "loss: 0.031962  [ 1616/ 3200]\n",
      "loss: 0.029932  [ 1632/ 3200]\n",
      "loss: 0.010829  [ 1648/ 3200]\n",
      "loss: 0.049822  [ 1664/ 3200]\n",
      "loss: 0.043250  [ 1680/ 3200]\n",
      "loss: 0.020041  [ 1696/ 3200]\n",
      "loss: 0.018599  [ 1712/ 3200]\n",
      "loss: 0.035802  [ 1728/ 3200]\n",
      "loss: 0.038906  [ 1744/ 3200]\n",
      "loss: 0.020181  [ 1760/ 3200]\n",
      "loss: 0.016552  [ 1776/ 3200]\n",
      "loss: 0.013149  [ 1792/ 3200]\n",
      "loss: 0.020258  [ 1808/ 3200]\n",
      "loss: 0.066542  [ 1824/ 3200]\n",
      "loss: 0.032754  [ 1840/ 3200]\n",
      "loss: 0.017707  [ 1856/ 3200]\n",
      "loss: 0.059536  [ 1872/ 3200]\n",
      "loss: 0.030638  [ 1888/ 3200]\n",
      "loss: 0.018947  [ 1904/ 3200]\n",
      "loss: 0.008266  [ 1920/ 3200]\n",
      "loss: 0.129788  [ 1936/ 3200]\n",
      "loss: 0.098036  [ 1952/ 3200]\n",
      "loss: 0.023183  [ 1968/ 3200]\n",
      "loss: 0.008437  [ 1984/ 3200]\n",
      "loss: 0.073792  [ 2000/ 3200]\n",
      "loss: 0.034207  [ 2016/ 3200]\n",
      "loss: 0.023884  [ 2032/ 3200]\n",
      "loss: 0.032291  [ 2048/ 3200]\n",
      "loss: 0.043048  [ 2064/ 3200]\n",
      "loss: 0.012960  [ 2080/ 3200]\n",
      "loss: 0.042133  [ 2096/ 3200]\n",
      "loss: 0.015754  [ 2112/ 3200]\n",
      "loss: 0.006897  [ 2128/ 3200]\n",
      "loss: 0.031321  [ 2144/ 3200]\n",
      "loss: 0.024066  [ 2160/ 3200]\n",
      "loss: 0.060021  [ 2176/ 3200]\n",
      "loss: 0.032764  [ 2192/ 3200]\n",
      "loss: 0.013731  [ 2208/ 3200]\n",
      "loss: 0.014514  [ 2224/ 3200]\n",
      "loss: 0.017944  [ 2240/ 3200]\n",
      "loss: 0.010561  [ 2256/ 3200]\n",
      "loss: 0.020791  [ 2272/ 3200]\n",
      "loss: 0.021815  [ 2288/ 3200]\n",
      "loss: 0.043790  [ 2304/ 3200]\n",
      "loss: 0.012571  [ 2320/ 3200]\n",
      "loss: 0.036736  [ 2336/ 3200]\n",
      "loss: 0.015077  [ 2352/ 3200]\n",
      "loss: 0.066007  [ 2368/ 3200]\n",
      "loss: 0.437398  [ 2384/ 3200]\n",
      "loss: 0.079024  [ 2400/ 3200]\n",
      "loss: 0.020199  [ 2416/ 3200]\n",
      "loss: 0.009953  [ 2432/ 3200]\n",
      "loss: 0.198547  [ 2448/ 3200]\n",
      "loss: 0.018242  [ 2464/ 3200]\n",
      "loss: 0.352430  [ 2480/ 3200]\n",
      "loss: 0.023367  [ 2496/ 3200]\n",
      "loss: 0.008254  [ 2512/ 3200]\n",
      "loss: 0.004678  [ 2528/ 3200]\n",
      "loss: 0.010699  [ 2544/ 3200]\n",
      "loss: 0.048310  [ 2560/ 3200]\n",
      "loss: 0.008128  [ 2576/ 3200]\n",
      "loss: 0.005311  [ 2592/ 3200]\n",
      "loss: 0.052344  [ 2608/ 3200]\n",
      "loss: 0.034487  [ 2624/ 3200]\n",
      "loss: 0.040896  [ 2640/ 3200]\n",
      "loss: 0.031813  [ 2656/ 3200]\n",
      "loss: 0.022626  [ 2672/ 3200]\n",
      "loss: 0.010839  [ 2688/ 3200]\n",
      "loss: 0.036380  [ 2704/ 3200]\n",
      "loss: 0.057873  [ 2720/ 3200]\n",
      "loss: 0.045127  [ 2736/ 3200]\n",
      "loss: 0.036943  [ 2752/ 3200]\n",
      "loss: 0.018039  [ 2768/ 3200]\n",
      "loss: 0.046533  [ 2784/ 3200]\n",
      "loss: 0.054639  [ 2800/ 3200]\n",
      "loss: 0.016707  [ 2816/ 3200]\n",
      "loss: 0.097202  [ 2832/ 3200]\n",
      "loss: 0.016452  [ 2848/ 3200]\n",
      "loss: 0.009416  [ 2864/ 3200]\n",
      "loss: 0.037490  [ 2880/ 3200]\n",
      "loss: 0.009296  [ 2896/ 3200]\n",
      "loss: 0.039840  [ 2912/ 3200]\n",
      "loss: 0.332592  [ 2928/ 3200]\n",
      "loss: 0.014185  [ 2944/ 3200]\n",
      "loss: 0.030603  [ 2960/ 3200]\n",
      "loss: 0.037749  [ 2976/ 3200]\n",
      "loss: 0.051918  [ 2992/ 3200]\n",
      "loss: 0.025555  [ 3008/ 3200]\n",
      "loss: 0.027448  [ 3024/ 3200]\n",
      "loss: 0.012359  [ 3040/ 3200]\n",
      "loss: 0.042386  [ 3056/ 3200]\n",
      "loss: 0.084769  [ 3072/ 3200]\n",
      "loss: 0.051552  [ 3088/ 3200]\n",
      "loss: 0.009134  [ 3104/ 3200]\n",
      "loss: 0.025865  [ 3120/ 3200]\n",
      "loss: 0.026164  [ 3136/ 3200]\n",
      "loss: 0.020209  [ 3152/ 3200]\n",
      "loss: 0.320800  [ 3168/ 3200]\n",
      "loss: 0.083804  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 0.026698  [    0/ 3200]\n",
      "loss: 0.067529  [   16/ 3200]\n",
      "loss: 0.011121  [   32/ 3200]\n",
      "loss: 0.009590  [   48/ 3200]\n",
      "loss: 0.005325  [   64/ 3200]\n",
      "loss: 0.044535  [   80/ 3200]\n",
      "loss: 0.036508  [   96/ 3200]\n",
      "loss: 0.015621  [  112/ 3200]\n",
      "loss: 0.014402  [  128/ 3200]\n",
      "loss: 0.044267  [  144/ 3200]\n",
      "loss: 0.039591  [  160/ 3200]\n",
      "loss: 0.276368  [  176/ 3200]\n",
      "loss: 0.033170  [  192/ 3200]\n",
      "loss: 0.080420  [  208/ 3200]\n",
      "loss: 0.026935  [  224/ 3200]\n",
      "loss: 0.027706  [  240/ 3200]\n",
      "loss: 0.043972  [  256/ 3200]\n",
      "loss: 0.054220  [  272/ 3200]\n",
      "loss: 0.049961  [  288/ 3200]\n",
      "loss: 0.017926  [  304/ 3200]\n",
      "loss: 0.012406  [  320/ 3200]\n",
      "loss: 0.019655  [  336/ 3200]\n",
      "loss: 0.023097  [  352/ 3200]\n",
      "loss: 0.025643  [  368/ 3200]\n",
      "loss: 0.029318  [  384/ 3200]\n",
      "loss: 0.010269  [  400/ 3200]\n",
      "loss: 0.032718  [  416/ 3200]\n",
      "loss: 0.039086  [  432/ 3200]\n",
      "loss: 0.014520  [  448/ 3200]\n",
      "loss: 0.067270  [  464/ 3200]\n",
      "loss: 0.021739  [  480/ 3200]\n",
      "loss: 0.039424  [  496/ 3200]\n",
      "loss: 0.051803  [  512/ 3200]\n",
      "loss: 0.019505  [  528/ 3200]\n",
      "loss: 0.016081  [  544/ 3200]\n",
      "loss: 0.050749  [  560/ 3200]\n",
      "loss: 0.014416  [  576/ 3200]\n",
      "loss: 0.030766  [  592/ 3200]\n",
      "loss: 0.012710  [  608/ 3200]\n",
      "loss: 0.007017  [  624/ 3200]\n",
      "loss: 0.021656  [  640/ 3200]\n",
      "loss: 0.014854  [  656/ 3200]\n",
      "loss: 0.006789  [  672/ 3200]\n",
      "loss: 0.021989  [  688/ 3200]\n",
      "loss: 0.043225  [  704/ 3200]\n",
      "loss: 0.027276  [  720/ 3200]\n",
      "loss: 0.031012  [  736/ 3200]\n",
      "loss: 0.019748  [  752/ 3200]\n",
      "loss: 0.015646  [  768/ 3200]\n",
      "loss: 0.010620  [  784/ 3200]\n",
      "loss: 0.052725  [  800/ 3200]\n",
      "loss: 0.090665  [  816/ 3200]\n",
      "loss: 0.049182  [  832/ 3200]\n",
      "loss: 0.017456  [  848/ 3200]\n",
      "loss: 0.043946  [  864/ 3200]\n",
      "loss: 0.017589  [  880/ 3200]\n",
      "loss: 0.025887  [  896/ 3200]\n",
      "loss: 0.008989  [  912/ 3200]\n",
      "loss: 0.008435  [  928/ 3200]\n",
      "loss: 0.013013  [  944/ 3200]\n",
      "loss: 0.014191  [  960/ 3200]\n",
      "loss: 0.009531  [  976/ 3200]\n",
      "loss: 0.040591  [  992/ 3200]\n",
      "loss: 0.042080  [ 1008/ 3200]\n",
      "loss: 0.008070  [ 1024/ 3200]\n",
      "loss: 0.025496  [ 1040/ 3200]\n",
      "loss: 0.032303  [ 1056/ 3200]\n",
      "loss: 0.031677  [ 1072/ 3200]\n",
      "loss: 0.005268  [ 1088/ 3200]\n",
      "loss: 0.012676  [ 1104/ 3200]\n",
      "loss: 0.022220  [ 1120/ 3200]\n",
      "loss: 0.020094  [ 1136/ 3200]\n",
      "loss: 0.014361  [ 1152/ 3200]\n",
      "loss: 0.015741  [ 1168/ 3200]\n",
      "loss: 0.022263  [ 1184/ 3200]\n",
      "loss: 0.009272  [ 1200/ 3200]\n",
      "loss: 0.017069  [ 1216/ 3200]\n",
      "loss: 0.011645  [ 1232/ 3200]\n",
      "loss: 0.021363  [ 1248/ 3200]\n",
      "loss: 0.028339  [ 1264/ 3200]\n",
      "loss: 0.013000  [ 1280/ 3200]\n",
      "loss: 0.013339  [ 1296/ 3200]\n",
      "loss: 0.016285  [ 1312/ 3200]\n",
      "loss: 0.032433  [ 1328/ 3200]\n",
      "loss: 0.045747  [ 1344/ 3200]\n",
      "loss: 0.088875  [ 1360/ 3200]\n",
      "loss: 0.013817  [ 1376/ 3200]\n",
      "loss: 0.008362  [ 1392/ 3200]\n",
      "loss: 0.046964  [ 1408/ 3200]\n",
      "loss: 0.030367  [ 1424/ 3200]\n",
      "loss: 0.050103  [ 1440/ 3200]\n",
      "loss: 0.314213  [ 1456/ 3200]\n",
      "loss: 0.023919  [ 1472/ 3200]\n",
      "loss: 0.056263  [ 1488/ 3200]\n",
      "loss: 0.072656  [ 1504/ 3200]\n",
      "loss: 0.160765  [ 1520/ 3200]\n",
      "loss: 0.051285  [ 1536/ 3200]\n",
      "loss: 0.003264  [ 1552/ 3200]\n",
      "loss: 0.119795  [ 1568/ 3200]\n",
      "loss: 0.147771  [ 1584/ 3200]\n",
      "loss: 0.015453  [ 1600/ 3200]\n",
      "loss: 0.020271  [ 1616/ 3200]\n",
      "loss: 0.007556  [ 1632/ 3200]\n",
      "loss: 0.017552  [ 1648/ 3200]\n",
      "loss: 0.015042  [ 1664/ 3200]\n",
      "loss: 0.029886  [ 1680/ 3200]\n",
      "loss: 0.039508  [ 1696/ 3200]\n",
      "loss: 0.012710  [ 1712/ 3200]\n",
      "loss: 0.008030  [ 1728/ 3200]\n",
      "loss: 0.006526  [ 1744/ 3200]\n",
      "loss: 0.010885  [ 1760/ 3200]\n",
      "loss: 0.024210  [ 1776/ 3200]\n",
      "loss: 0.014418  [ 1792/ 3200]\n",
      "loss: 0.027812  [ 1808/ 3200]\n",
      "loss: 0.085443  [ 1824/ 3200]\n",
      "loss: 0.036263  [ 1840/ 3200]\n",
      "loss: 0.024685  [ 1856/ 3200]\n",
      "loss: 0.014820  [ 1872/ 3200]\n",
      "loss: 0.010637  [ 1888/ 3200]\n",
      "loss: 0.010811  [ 1904/ 3200]\n",
      "loss: 0.016310  [ 1920/ 3200]\n",
      "loss: 0.022307  [ 1936/ 3200]\n",
      "loss: 0.066225  [ 1952/ 3200]\n",
      "loss: 0.026842  [ 1968/ 3200]\n",
      "loss: 0.028143  [ 1984/ 3200]\n",
      "loss: 0.026250  [ 2000/ 3200]\n",
      "loss: 0.023103  [ 2016/ 3200]\n",
      "loss: 0.022314  [ 2032/ 3200]\n",
      "loss: 0.021967  [ 2048/ 3200]\n",
      "loss: 0.017608  [ 2064/ 3200]\n",
      "loss: 0.049132  [ 2080/ 3200]\n",
      "loss: 0.045027  [ 2096/ 3200]\n",
      "loss: 0.028894  [ 2112/ 3200]\n",
      "loss: 0.006250  [ 2128/ 3200]\n",
      "loss: 0.033770  [ 2144/ 3200]\n",
      "loss: 0.021141  [ 2160/ 3200]\n",
      "loss: 0.003563  [ 2176/ 3200]\n",
      "loss: 0.034410  [ 2192/ 3200]\n",
      "loss: 0.035474  [ 2208/ 3200]\n",
      "loss: 0.013263  [ 2224/ 3200]\n",
      "loss: 0.028335  [ 2240/ 3200]\n",
      "loss: 0.007514  [ 2256/ 3200]\n",
      "loss: 0.018610  [ 2272/ 3200]\n",
      "loss: 0.032202  [ 2288/ 3200]\n",
      "loss: 0.010115  [ 2304/ 3200]\n",
      "loss: 0.009662  [ 2320/ 3200]\n",
      "loss: 0.068095  [ 2336/ 3200]\n",
      "loss: 0.039172  [ 2352/ 3200]\n",
      "loss: 0.049877  [ 2368/ 3200]\n",
      "loss: 0.037119  [ 2384/ 3200]\n",
      "loss: 0.060504  [ 2400/ 3200]\n",
      "loss: 0.022827  [ 2416/ 3200]\n",
      "loss: 0.025649  [ 2432/ 3200]\n",
      "loss: 0.018320  [ 2448/ 3200]\n",
      "loss: 0.046319  [ 2464/ 3200]\n",
      "loss: 0.021260  [ 2480/ 3200]\n",
      "loss: 0.041718  [ 2496/ 3200]\n",
      "loss: 0.033390  [ 2512/ 3200]\n",
      "loss: 0.037869  [ 2528/ 3200]\n",
      "loss: 0.017246  [ 2544/ 3200]\n",
      "loss: 0.015011  [ 2560/ 3200]\n",
      "loss: 0.023296  [ 2576/ 3200]\n",
      "loss: 0.137838  [ 2592/ 3200]\n",
      "loss: 0.013834  [ 2608/ 3200]\n",
      "loss: 0.016988  [ 2624/ 3200]\n",
      "loss: 0.014193  [ 2640/ 3200]\n",
      "loss: 0.026968  [ 2656/ 3200]\n",
      "loss: 0.049423  [ 2672/ 3200]\n",
      "loss: 0.018825  [ 2688/ 3200]\n",
      "loss: 0.012425  [ 2704/ 3200]\n",
      "loss: 0.039147  [ 2720/ 3200]\n",
      "loss: 0.017419  [ 2736/ 3200]\n",
      "loss: 0.014904  [ 2752/ 3200]\n",
      "loss: 0.309078  [ 2768/ 3200]\n",
      "loss: 0.018080  [ 2784/ 3200]\n",
      "loss: 0.060737  [ 2800/ 3200]\n",
      "loss: 0.059329  [ 2816/ 3200]\n",
      "loss: 0.023379  [ 2832/ 3200]\n",
      "loss: 0.004768  [ 2848/ 3200]\n",
      "loss: 0.198530  [ 2864/ 3200]\n",
      "loss: 0.175243  [ 2880/ 3200]\n",
      "loss: 0.028519  [ 2896/ 3200]\n",
      "loss: 0.023179  [ 2912/ 3200]\n",
      "loss: 0.011328  [ 2928/ 3200]\n",
      "loss: 0.026634  [ 2944/ 3200]\n",
      "loss: 0.005970  [ 2960/ 3200]\n",
      "loss: 0.035704  [ 2976/ 3200]\n",
      "loss: 0.013964  [ 2992/ 3200]\n",
      "loss: 0.038920  [ 3008/ 3200]\n",
      "loss: 0.007774  [ 3024/ 3200]\n",
      "loss: 0.014413  [ 3040/ 3200]\n",
      "loss: 0.023761  [ 3056/ 3200]\n",
      "loss: 0.021178  [ 3072/ 3200]\n",
      "loss: 0.030902  [ 3088/ 3200]\n",
      "loss: 0.020942  [ 3104/ 3200]\n",
      "loss: 0.012602  [ 3120/ 3200]\n",
      "loss: 0.024252  [ 3136/ 3200]\n",
      "loss: 0.005717  [ 3152/ 3200]\n",
      "loss: 0.008647  [ 3168/ 3200]\n",
      "loss: 0.007298  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.026757  [    0/ 3200]\n",
      "loss: 0.020452  [   16/ 3200]\n",
      "loss: 0.003358  [   32/ 3200]\n",
      "loss: 0.034880  [   48/ 3200]\n",
      "loss: 0.013446  [   64/ 3200]\n",
      "loss: 0.011478  [   80/ 3200]\n",
      "loss: 0.009803  [   96/ 3200]\n",
      "loss: 0.009005  [  112/ 3200]\n",
      "loss: 0.013645  [  128/ 3200]\n",
      "loss: 0.024507  [  144/ 3200]\n",
      "loss: 0.018124  [  160/ 3200]\n",
      "loss: 0.017395  [  176/ 3200]\n",
      "loss: 0.024940  [  192/ 3200]\n",
      "loss: 0.006726  [  208/ 3200]\n",
      "loss: 0.018592  [  224/ 3200]\n",
      "loss: 0.026756  [  240/ 3200]\n",
      "loss: 0.028590  [  256/ 3200]\n",
      "loss: 0.036898  [  272/ 3200]\n",
      "loss: 0.030507  [  288/ 3200]\n",
      "loss: 0.045320  [  304/ 3200]\n",
      "loss: 0.042509  [  320/ 3200]\n",
      "loss: 0.031594  [  336/ 3200]\n",
      "loss: 0.011217  [  352/ 3200]\n",
      "loss: 0.006119  [  368/ 3200]\n",
      "loss: 0.019382  [  384/ 3200]\n",
      "loss: 0.006119  [  400/ 3200]\n",
      "loss: 0.012252  [  416/ 3200]\n",
      "loss: 0.037417  [  432/ 3200]\n",
      "loss: 0.007549  [  448/ 3200]\n",
      "loss: 0.022604  [  464/ 3200]\n",
      "loss: 0.016661  [  480/ 3200]\n",
      "loss: 0.012953  [  496/ 3200]\n",
      "loss: 0.021226  [  512/ 3200]\n",
      "loss: 0.012547  [  528/ 3200]\n",
      "loss: 0.011394  [  544/ 3200]\n",
      "loss: 0.019742  [  560/ 3200]\n",
      "loss: 0.027928  [  576/ 3200]\n",
      "loss: 0.016681  [  592/ 3200]\n",
      "loss: 0.016997  [  608/ 3200]\n",
      "loss: 0.004161  [  624/ 3200]\n",
      "loss: 0.010425  [  640/ 3200]\n",
      "loss: 0.015941  [  656/ 3200]\n",
      "loss: 0.015709  [  672/ 3200]\n",
      "loss: 0.018561  [  688/ 3200]\n",
      "loss: 0.012121  [  704/ 3200]\n",
      "loss: 0.021307  [  720/ 3200]\n",
      "loss: 0.016680  [  736/ 3200]\n",
      "loss: 0.023302  [  752/ 3200]\n",
      "loss: 0.015080  [  768/ 3200]\n",
      "loss: 0.062057  [  784/ 3200]\n",
      "loss: 0.008438  [  800/ 3200]\n",
      "loss: 0.025982  [  816/ 3200]\n",
      "loss: 0.006101  [  832/ 3200]\n",
      "loss: 0.017644  [  848/ 3200]\n",
      "loss: 0.016322  [  864/ 3200]\n",
      "loss: 0.048342  [  880/ 3200]\n",
      "loss: 0.014089  [  896/ 3200]\n",
      "loss: 0.003686  [  912/ 3200]\n",
      "loss: 0.027877  [  928/ 3200]\n",
      "loss: 0.062158  [  944/ 3200]\n",
      "loss: 0.165921  [  960/ 3200]\n",
      "loss: 0.035250  [  976/ 3200]\n",
      "loss: 0.051888  [  992/ 3200]\n",
      "loss: 0.016825  [ 1008/ 3200]\n",
      "loss: 0.012531  [ 1024/ 3200]\n",
      "loss: 0.037446  [ 1040/ 3200]\n",
      "loss: 0.030541  [ 1056/ 3200]\n",
      "loss: 0.031315  [ 1072/ 3200]\n",
      "loss: 0.015313  [ 1088/ 3200]\n",
      "loss: 0.025790  [ 1104/ 3200]\n",
      "loss: 0.017407  [ 1120/ 3200]\n",
      "loss: 0.004638  [ 1136/ 3200]\n",
      "loss: 0.012392  [ 1152/ 3200]\n",
      "loss: 0.028267  [ 1168/ 3200]\n",
      "loss: 0.013693  [ 1184/ 3200]\n",
      "loss: 0.008223  [ 1200/ 3200]\n",
      "loss: 0.011491  [ 1216/ 3200]\n",
      "loss: 0.008376  [ 1232/ 3200]\n",
      "loss: 0.036401  [ 1248/ 3200]\n",
      "loss: 0.024966  [ 1264/ 3200]\n",
      "loss: 0.007794  [ 1280/ 3200]\n",
      "loss: 0.011037  [ 1296/ 3200]\n",
      "loss: 0.020679  [ 1312/ 3200]\n",
      "loss: 0.015398  [ 1328/ 3200]\n",
      "loss: 0.014105  [ 1344/ 3200]\n",
      "loss: 0.013987  [ 1360/ 3200]\n",
      "loss: 0.013090  [ 1376/ 3200]\n",
      "loss: 0.013294  [ 1392/ 3200]\n",
      "loss: 0.025405  [ 1408/ 3200]\n",
      "loss: 0.009898  [ 1424/ 3200]\n",
      "loss: 0.016671  [ 1440/ 3200]\n",
      "loss: 0.009719  [ 1456/ 3200]\n",
      "loss: 0.034152  [ 1472/ 3200]\n",
      "loss: 0.004401  [ 1488/ 3200]\n",
      "loss: 0.012475  [ 1504/ 3200]\n",
      "loss: 0.010477  [ 1520/ 3200]\n",
      "loss: 0.006162  [ 1536/ 3200]\n",
      "loss: 0.029965  [ 1552/ 3200]\n",
      "loss: 0.009271  [ 1568/ 3200]\n",
      "loss: 0.014168  [ 1584/ 3200]\n",
      "loss: 0.016086  [ 1600/ 3200]\n",
      "loss: 0.021539  [ 1616/ 3200]\n",
      "loss: 0.001183  [ 1632/ 3200]\n",
      "loss: 0.019492  [ 1648/ 3200]\n",
      "loss: 0.007618  [ 1664/ 3200]\n",
      "loss: 0.306605  [ 1680/ 3200]\n",
      "loss: 0.112041  [ 1696/ 3200]\n",
      "loss: 0.059915  [ 1712/ 3200]\n",
      "loss: 0.019429  [ 1728/ 3200]\n",
      "loss: 0.006252  [ 1744/ 3200]\n",
      "loss: 0.018124  [ 1760/ 3200]\n",
      "loss: 0.009342  [ 1776/ 3200]\n",
      "loss: 0.049634  [ 1792/ 3200]\n",
      "loss: 0.003658  [ 1808/ 3200]\n",
      "loss: 0.006650  [ 1824/ 3200]\n",
      "loss: 0.011123  [ 1840/ 3200]\n",
      "loss: 0.013571  [ 1856/ 3200]\n",
      "loss: 0.011429  [ 1872/ 3200]\n",
      "loss: 0.010913  [ 1888/ 3200]\n",
      "loss: 0.004085  [ 1904/ 3200]\n",
      "loss: 0.005793  [ 1920/ 3200]\n",
      "loss: 0.038828  [ 1936/ 3200]\n",
      "loss: 0.020680  [ 1952/ 3200]\n",
      "loss: 0.001994  [ 1968/ 3200]\n",
      "loss: 0.035545  [ 1984/ 3200]\n",
      "loss: 0.019162  [ 2000/ 3200]\n",
      "loss: 0.017551  [ 2016/ 3200]\n",
      "loss: 0.054704  [ 2032/ 3200]\n",
      "loss: 0.011079  [ 2048/ 3200]\n",
      "loss: 0.033529  [ 2064/ 3200]\n",
      "loss: 0.019819  [ 2080/ 3200]\n",
      "loss: 0.006584  [ 2096/ 3200]\n",
      "loss: 0.011977  [ 2112/ 3200]\n",
      "loss: 0.018695  [ 2128/ 3200]\n",
      "loss: 0.018123  [ 2144/ 3200]\n",
      "loss: 0.009314  [ 2160/ 3200]\n",
      "loss: 0.024949  [ 2176/ 3200]\n",
      "loss: 0.002527  [ 2192/ 3200]\n",
      "loss: 0.005010  [ 2208/ 3200]\n",
      "loss: 0.028278  [ 2224/ 3200]\n",
      "loss: 0.019543  [ 2240/ 3200]\n",
      "loss: 0.006320  [ 2256/ 3200]\n",
      "loss: 0.010255  [ 2272/ 3200]\n",
      "loss: 0.006872  [ 2288/ 3200]\n",
      "loss: 0.008194  [ 2304/ 3200]\n",
      "loss: 0.005470  [ 2320/ 3200]\n",
      "loss: 0.015815  [ 2336/ 3200]\n",
      "loss: 0.002443  [ 2352/ 3200]\n",
      "loss: 0.231264  [ 2368/ 3200]\n",
      "loss: 0.042048  [ 2384/ 3200]\n",
      "loss: 0.004180  [ 2400/ 3200]\n",
      "loss: 0.020668  [ 2416/ 3200]\n",
      "loss: 0.019307  [ 2432/ 3200]\n",
      "loss: 0.012945  [ 2448/ 3200]\n",
      "loss: 0.012700  [ 2464/ 3200]\n",
      "loss: 0.025542  [ 2480/ 3200]\n",
      "loss: 0.035750  [ 2496/ 3200]\n",
      "loss: 0.011899  [ 2512/ 3200]\n",
      "loss: 0.022123  [ 2528/ 3200]\n",
      "loss: 0.029897  [ 2544/ 3200]\n",
      "loss: 0.024046  [ 2560/ 3200]\n",
      "loss: 0.030667  [ 2576/ 3200]\n",
      "loss: 0.016446  [ 2592/ 3200]\n",
      "loss: 0.066358  [ 2608/ 3200]\n",
      "loss: 0.009270  [ 2624/ 3200]\n",
      "loss: 0.039173  [ 2640/ 3200]\n",
      "loss: 0.022447  [ 2656/ 3200]\n",
      "loss: 0.011231  [ 2672/ 3200]\n",
      "loss: 0.028679  [ 2688/ 3200]\n",
      "loss: 0.033119  [ 2704/ 3200]\n",
      "loss: 0.006718  [ 2720/ 3200]\n",
      "loss: 0.006298  [ 2736/ 3200]\n",
      "loss: 0.014748  [ 2752/ 3200]\n",
      "loss: 0.021666  [ 2768/ 3200]\n",
      "loss: 0.060838  [ 2784/ 3200]\n",
      "loss: 0.056417  [ 2800/ 3200]\n",
      "loss: 0.105841  [ 2816/ 3200]\n",
      "loss: 0.066861  [ 2832/ 3200]\n",
      "loss: 0.062804  [ 2848/ 3200]\n",
      "loss: 0.020497  [ 2864/ 3200]\n",
      "loss: 0.011136  [ 2880/ 3200]\n",
      "loss: 0.022233  [ 2896/ 3200]\n",
      "loss: 0.008914  [ 2912/ 3200]\n",
      "loss: 0.014018  [ 2928/ 3200]\n",
      "loss: 0.009260  [ 2944/ 3200]\n",
      "loss: 0.010217  [ 2960/ 3200]\n",
      "loss: 0.025647  [ 2976/ 3200]\n",
      "loss: 0.039269  [ 2992/ 3200]\n",
      "loss: 0.052474  [ 3008/ 3200]\n",
      "loss: 0.014630  [ 3024/ 3200]\n",
      "loss: 0.008817  [ 3040/ 3200]\n",
      "loss: 0.022307  [ 3056/ 3200]\n",
      "loss: 0.006671  [ 3072/ 3200]\n",
      "loss: 0.039454  [ 3088/ 3200]\n",
      "loss: 0.014273  [ 3104/ 3200]\n",
      "loss: 0.017447  [ 3120/ 3200]\n",
      "loss: 0.074834  [ 3136/ 3200]\n",
      "loss: 0.027195  [ 3152/ 3200]\n",
      "loss: 0.011391  [ 3168/ 3200]\n",
      "loss: 0.044117  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 0.005690  [    0/ 3200]\n",
      "loss: 0.009495  [   16/ 3200]\n",
      "loss: 0.009265  [   32/ 3200]\n",
      "loss: 0.024301  [   48/ 3200]\n",
      "loss: 0.005333  [   64/ 3200]\n",
      "loss: 0.015977  [   80/ 3200]\n",
      "loss: 0.027210  [   96/ 3200]\n",
      "loss: 0.010218  [  112/ 3200]\n",
      "loss: 0.024433  [  128/ 3200]\n",
      "loss: 0.017427  [  144/ 3200]\n",
      "loss: 0.006371  [  160/ 3200]\n",
      "loss: 0.014293  [  176/ 3200]\n",
      "loss: 0.006564  [  192/ 3200]\n",
      "loss: 0.002328  [  208/ 3200]\n",
      "loss: 0.018090  [  224/ 3200]\n",
      "loss: 0.009457  [  240/ 3200]\n",
      "loss: 0.006241  [  256/ 3200]\n",
      "loss: 0.014302  [  272/ 3200]\n",
      "loss: 0.015993  [  288/ 3200]\n",
      "loss: 0.021347  [  304/ 3200]\n",
      "loss: 0.004654  [  320/ 3200]\n",
      "loss: 0.008030  [  336/ 3200]\n",
      "loss: 0.090049  [  352/ 3200]\n",
      "loss: 0.026592  [  368/ 3200]\n",
      "loss: 0.034427  [  384/ 3200]\n",
      "loss: 0.013275  [  400/ 3200]\n",
      "loss: 0.020434  [  416/ 3200]\n",
      "loss: 0.010800  [  432/ 3200]\n",
      "loss: 0.009410  [  448/ 3200]\n",
      "loss: 0.010232  [  464/ 3200]\n",
      "loss: 0.007931  [  480/ 3200]\n",
      "loss: 0.005765  [  496/ 3200]\n",
      "loss: 0.010262  [  512/ 3200]\n",
      "loss: 0.007981  [  528/ 3200]\n",
      "loss: 0.006737  [  544/ 3200]\n",
      "loss: 0.007007  [  560/ 3200]\n",
      "loss: 0.010039  [  576/ 3200]\n",
      "loss: 0.012924  [  592/ 3200]\n",
      "loss: 0.002116  [  608/ 3200]\n",
      "loss: 0.008068  [  624/ 3200]\n",
      "loss: 0.017541  [  640/ 3200]\n",
      "loss: 0.005836  [  656/ 3200]\n",
      "loss: 0.012686  [  672/ 3200]\n",
      "loss: 0.012172  [  688/ 3200]\n",
      "loss: 0.011345  [  704/ 3200]\n",
      "loss: 0.020989  [  720/ 3200]\n",
      "loss: 0.008975  [  736/ 3200]\n",
      "loss: 0.008200  [  752/ 3200]\n",
      "loss: 0.021916  [  768/ 3200]\n",
      "loss: 0.020406  [  784/ 3200]\n",
      "loss: 0.014506  [  800/ 3200]\n",
      "loss: 0.011260  [  816/ 3200]\n",
      "loss: 0.015316  [  832/ 3200]\n",
      "loss: 0.004869  [  848/ 3200]\n",
      "loss: 0.005514  [  864/ 3200]\n",
      "loss: 0.016681  [  880/ 3200]\n",
      "loss: 0.016618  [  896/ 3200]\n",
      "loss: 0.012448  [  912/ 3200]\n",
      "loss: 0.014855  [  928/ 3200]\n",
      "loss: 0.015183  [  944/ 3200]\n",
      "loss: 0.009296  [  960/ 3200]\n",
      "loss: 0.015963  [  976/ 3200]\n",
      "loss: 0.026082  [  992/ 3200]\n",
      "loss: 0.013963  [ 1008/ 3200]\n",
      "loss: 0.009842  [ 1024/ 3200]\n",
      "loss: 0.012804  [ 1040/ 3200]\n",
      "loss: 0.027738  [ 1056/ 3200]\n",
      "loss: 0.017096  [ 1072/ 3200]\n",
      "loss: 0.007481  [ 1088/ 3200]\n",
      "loss: 0.011590  [ 1104/ 3200]\n",
      "loss: 0.008024  [ 1120/ 3200]\n",
      "loss: 0.012149  [ 1136/ 3200]\n",
      "loss: 0.037494  [ 1152/ 3200]\n",
      "loss: 0.011520  [ 1168/ 3200]\n",
      "loss: 0.012965  [ 1184/ 3200]\n",
      "loss: 0.009100  [ 1200/ 3200]\n",
      "loss: 0.007474  [ 1216/ 3200]\n",
      "loss: 0.009108  [ 1232/ 3200]\n",
      "loss: 0.020134  [ 1248/ 3200]\n",
      "loss: 0.008110  [ 1264/ 3200]\n",
      "loss: 0.023654  [ 1280/ 3200]\n",
      "loss: 0.004418  [ 1296/ 3200]\n",
      "loss: 0.021827  [ 1312/ 3200]\n",
      "loss: 0.003232  [ 1328/ 3200]\n",
      "loss: 0.012962  [ 1344/ 3200]\n",
      "loss: 0.030976  [ 1360/ 3200]\n",
      "loss: 0.016651  [ 1376/ 3200]\n",
      "loss: 0.003462  [ 1392/ 3200]\n",
      "loss: 0.011781  [ 1408/ 3200]\n",
      "loss: 0.044890  [ 1424/ 3200]\n",
      "loss: 0.011741  [ 1440/ 3200]\n",
      "loss: 0.021466  [ 1456/ 3200]\n",
      "loss: 0.011270  [ 1472/ 3200]\n",
      "loss: 0.014652  [ 1488/ 3200]\n",
      "loss: 0.009712  [ 1504/ 3200]\n",
      "loss: 0.012464  [ 1520/ 3200]\n",
      "loss: 0.017185  [ 1536/ 3200]\n",
      "loss: 0.004240  [ 1552/ 3200]\n",
      "loss: 0.017832  [ 1568/ 3200]\n",
      "loss: 0.011949  [ 1584/ 3200]\n",
      "loss: 0.013853  [ 1600/ 3200]\n",
      "loss: 0.014673  [ 1616/ 3200]\n",
      "loss: 0.003006  [ 1632/ 3200]\n",
      "loss: 0.008535  [ 1648/ 3200]\n",
      "loss: 0.027687  [ 1664/ 3200]\n",
      "loss: 0.008119  [ 1680/ 3200]\n",
      "loss: 0.003724  [ 1696/ 3200]\n",
      "loss: 0.014725  [ 1712/ 3200]\n",
      "loss: 0.012928  [ 1728/ 3200]\n",
      "loss: 0.010168  [ 1744/ 3200]\n",
      "loss: 0.006757  [ 1760/ 3200]\n",
      "loss: 0.004675  [ 1776/ 3200]\n",
      "loss: 0.010560  [ 1792/ 3200]\n",
      "loss: 0.011448  [ 1808/ 3200]\n",
      "loss: 0.013094  [ 1824/ 3200]\n",
      "loss: 0.020536  [ 1840/ 3200]\n",
      "loss: 0.013141  [ 1856/ 3200]\n",
      "loss: 0.006311  [ 1872/ 3200]\n",
      "loss: 0.002295  [ 1888/ 3200]\n",
      "loss: 0.010339  [ 1904/ 3200]\n",
      "loss: 0.012714  [ 1920/ 3200]\n",
      "loss: 0.011573  [ 1936/ 3200]\n",
      "loss: 0.012976  [ 1952/ 3200]\n",
      "loss: 0.025485  [ 1968/ 3200]\n",
      "loss: 0.020309  [ 1984/ 3200]\n",
      "loss: 0.024903  [ 2000/ 3200]\n",
      "loss: 0.008302  [ 2016/ 3200]\n",
      "loss: 0.010427  [ 2032/ 3200]\n",
      "loss: 0.020731  [ 2048/ 3200]\n",
      "loss: 0.005861  [ 2064/ 3200]\n",
      "loss: 0.010572  [ 2080/ 3200]\n",
      "loss: 0.012127  [ 2096/ 3200]\n",
      "loss: 0.005729  [ 2112/ 3200]\n",
      "loss: 0.011849  [ 2128/ 3200]\n",
      "loss: 0.015798  [ 2144/ 3200]\n",
      "loss: 0.012855  [ 2160/ 3200]\n",
      "loss: 0.035089  [ 2176/ 3200]\n",
      "loss: 0.026929  [ 2192/ 3200]\n",
      "loss: 0.015387  [ 2208/ 3200]\n",
      "loss: 0.018226  [ 2224/ 3200]\n",
      "loss: 0.010323  [ 2240/ 3200]\n",
      "loss: 0.005150  [ 2256/ 3200]\n",
      "loss: 0.008797  [ 2272/ 3200]\n",
      "loss: 0.009115  [ 2288/ 3200]\n",
      "loss: 0.015148  [ 2304/ 3200]\n",
      "loss: 0.014305  [ 2320/ 3200]\n",
      "loss: 0.022937  [ 2336/ 3200]\n",
      "loss: 0.006493  [ 2352/ 3200]\n",
      "loss: 0.032344  [ 2368/ 3200]\n",
      "loss: 0.018652  [ 2384/ 3200]\n",
      "loss: 0.008740  [ 2400/ 3200]\n",
      "loss: 0.016357  [ 2416/ 3200]\n",
      "loss: 0.011770  [ 2432/ 3200]\n",
      "loss: 0.010089  [ 2448/ 3200]\n",
      "loss: 0.012769  [ 2464/ 3200]\n",
      "loss: 0.024132  [ 2480/ 3200]\n",
      "loss: 0.008434  [ 2496/ 3200]\n",
      "loss: 0.019467  [ 2512/ 3200]\n",
      "loss: 0.094514  [ 2528/ 3200]\n",
      "loss: 0.029917  [ 2544/ 3200]\n",
      "loss: 0.012148  [ 2560/ 3200]\n",
      "loss: 0.016706  [ 2576/ 3200]\n",
      "loss: 0.218206  [ 2592/ 3200]\n",
      "loss: 0.063729  [ 2608/ 3200]\n",
      "loss: 0.038317  [ 2624/ 3200]\n",
      "loss: 0.026272  [ 2640/ 3200]\n",
      "loss: 0.064103  [ 2656/ 3200]\n",
      "loss: 0.004633  [ 2672/ 3200]\n",
      "loss: 0.010810  [ 2688/ 3200]\n",
      "loss: 0.005874  [ 2704/ 3200]\n",
      "loss: 0.006995  [ 2720/ 3200]\n",
      "loss: 0.019749  [ 2736/ 3200]\n",
      "loss: 0.008360  [ 2752/ 3200]\n",
      "loss: 0.011853  [ 2768/ 3200]\n",
      "loss: 0.005647  [ 2784/ 3200]\n",
      "loss: 0.022673  [ 2800/ 3200]\n",
      "loss: 0.013498  [ 2816/ 3200]\n",
      "loss: 0.024158  [ 2832/ 3200]\n",
      "loss: 0.012821  [ 2848/ 3200]\n",
      "loss: 0.016056  [ 2864/ 3200]\n",
      "loss: 0.012402  [ 2880/ 3200]\n",
      "loss: 0.007773  [ 2896/ 3200]\n",
      "loss: 0.015916  [ 2912/ 3200]\n",
      "loss: 0.010722  [ 2928/ 3200]\n",
      "loss: 0.012960  [ 2944/ 3200]\n",
      "loss: 0.036704  [ 2960/ 3200]\n",
      "loss: 0.003681  [ 2976/ 3200]\n",
      "loss: 0.012386  [ 2992/ 3200]\n",
      "loss: 0.010393  [ 3008/ 3200]\n",
      "loss: 0.014885  [ 3024/ 3200]\n",
      "loss: 0.009574  [ 3040/ 3200]\n",
      "loss: 0.044631  [ 3056/ 3200]\n",
      "loss: 0.306852  [ 3072/ 3200]\n",
      "loss: 0.031770  [ 3088/ 3200]\n",
      "loss: 0.027004  [ 3104/ 3200]\n",
      "loss: 0.029058  [ 3120/ 3200]\n",
      "loss: 0.009647  [ 3136/ 3200]\n",
      "loss: 0.003516  [ 3152/ 3200]\n",
      "loss: 0.007983  [ 3168/ 3200]\n",
      "loss: 0.022842  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.108077  [    0/ 3200]\n",
      "loss: 0.028983  [   16/ 3200]\n",
      "loss: 0.010982  [   32/ 3200]\n",
      "loss: 0.012609  [   48/ 3200]\n",
      "loss: 0.011361  [   64/ 3200]\n",
      "loss: 0.005029  [   80/ 3200]\n",
      "loss: 0.012165  [   96/ 3200]\n",
      "loss: 0.008680  [  112/ 3200]\n",
      "loss: 0.004011  [  128/ 3200]\n",
      "loss: 0.024063  [  144/ 3200]\n",
      "loss: 0.010721  [  160/ 3200]\n",
      "loss: 0.002621  [  176/ 3200]\n",
      "loss: 0.018638  [  192/ 3200]\n",
      "loss: 0.021149  [  208/ 3200]\n",
      "loss: 0.010657  [  224/ 3200]\n",
      "loss: 0.012958  [  240/ 3200]\n",
      "loss: 0.004782  [  256/ 3200]\n",
      "loss: 0.029654  [  272/ 3200]\n",
      "loss: 0.015773  [  288/ 3200]\n",
      "loss: 0.005105  [  304/ 3200]\n",
      "loss: 0.008508  [  320/ 3200]\n",
      "loss: 0.017247  [  336/ 3200]\n",
      "loss: 0.010768  [  352/ 3200]\n",
      "loss: 0.019773  [  368/ 3200]\n",
      "loss: 0.014846  [  384/ 3200]\n",
      "loss: 0.020265  [  400/ 3200]\n",
      "loss: 0.005752  [  416/ 3200]\n",
      "loss: 0.008889  [  432/ 3200]\n",
      "loss: 0.009964  [  448/ 3200]\n",
      "loss: 0.022589  [  464/ 3200]\n",
      "loss: 0.013284  [  480/ 3200]\n",
      "loss: 0.008829  [  496/ 3200]\n",
      "loss: 0.007358  [  512/ 3200]\n",
      "loss: 0.015635  [  528/ 3200]\n",
      "loss: 0.027074  [  544/ 3200]\n",
      "loss: 0.007360  [  560/ 3200]\n",
      "loss: 0.008510  [  576/ 3200]\n",
      "loss: 0.010886  [  592/ 3200]\n",
      "loss: 0.006880  [  608/ 3200]\n",
      "loss: 0.004524  [  624/ 3200]\n",
      "loss: 0.023010  [  640/ 3200]\n",
      "loss: 0.019507  [  656/ 3200]\n",
      "loss: 0.020169  [  672/ 3200]\n",
      "loss: 0.005792  [  688/ 3200]\n",
      "loss: 0.026153  [  704/ 3200]\n",
      "loss: 0.008089  [  720/ 3200]\n",
      "loss: 0.010252  [  736/ 3200]\n",
      "loss: 0.031727  [  752/ 3200]\n",
      "loss: 0.009158  [  768/ 3200]\n",
      "loss: 0.010473  [  784/ 3200]\n",
      "loss: 0.005610  [  800/ 3200]\n",
      "loss: 0.006484  [  816/ 3200]\n",
      "loss: 0.012046  [  832/ 3200]\n",
      "loss: 0.002876  [  848/ 3200]\n",
      "loss: 0.008113  [  864/ 3200]\n",
      "loss: 0.021865  [  880/ 3200]\n",
      "loss: 0.011784  [  896/ 3200]\n",
      "loss: 0.007047  [  912/ 3200]\n",
      "loss: 0.007409  [  928/ 3200]\n",
      "loss: 0.008776  [  944/ 3200]\n",
      "loss: 0.001544  [  960/ 3200]\n",
      "loss: 0.266750  [  976/ 3200]\n",
      "loss: 0.021638  [  992/ 3200]\n",
      "loss: 0.025476  [ 1008/ 3200]\n",
      "loss: 0.006128  [ 1024/ 3200]\n",
      "loss: 0.011361  [ 1040/ 3200]\n",
      "loss: 0.014308  [ 1056/ 3200]\n",
      "loss: 0.004343  [ 1072/ 3200]\n",
      "loss: 0.008477  [ 1088/ 3200]\n",
      "loss: 0.016074  [ 1104/ 3200]\n",
      "loss: 0.004507  [ 1120/ 3200]\n",
      "loss: 0.004731  [ 1136/ 3200]\n",
      "loss: 0.009754  [ 1152/ 3200]\n",
      "loss: 0.007793  [ 1168/ 3200]\n",
      "loss: 0.008184  [ 1184/ 3200]\n",
      "loss: 0.022480  [ 1200/ 3200]\n",
      "loss: 0.008273  [ 1216/ 3200]\n",
      "loss: 0.015206  [ 1232/ 3200]\n",
      "loss: 0.007928  [ 1248/ 3200]\n",
      "loss: 0.010775  [ 1264/ 3200]\n",
      "loss: 0.009895  [ 1280/ 3200]\n",
      "loss: 0.004228  [ 1296/ 3200]\n",
      "loss: 0.004689  [ 1312/ 3200]\n",
      "loss: 0.005377  [ 1328/ 3200]\n",
      "loss: 0.005518  [ 1344/ 3200]\n",
      "loss: 0.015316  [ 1360/ 3200]\n",
      "loss: 0.017943  [ 1376/ 3200]\n",
      "loss: 0.010999  [ 1392/ 3200]\n",
      "loss: 0.030426  [ 1408/ 3200]\n",
      "loss: 0.002346  [ 1424/ 3200]\n",
      "loss: 0.011521  [ 1440/ 3200]\n",
      "loss: 0.008754  [ 1456/ 3200]\n",
      "loss: 0.011307  [ 1472/ 3200]\n",
      "loss: 0.011176  [ 1488/ 3200]\n",
      "loss: 0.006746  [ 1504/ 3200]\n",
      "loss: 0.016030  [ 1520/ 3200]\n",
      "loss: 0.005490  [ 1536/ 3200]\n",
      "loss: 0.009946  [ 1552/ 3200]\n",
      "loss: 0.010298  [ 1568/ 3200]\n",
      "loss: 0.004898  [ 1584/ 3200]\n",
      "loss: 0.010355  [ 1600/ 3200]\n",
      "loss: 0.008226  [ 1616/ 3200]\n",
      "loss: 0.007727  [ 1632/ 3200]\n",
      "loss: 0.008960  [ 1648/ 3200]\n",
      "loss: 0.012821  [ 1664/ 3200]\n",
      "loss: 0.024545  [ 1680/ 3200]\n",
      "loss: 0.004341  [ 1696/ 3200]\n",
      "loss: 0.009331  [ 1712/ 3200]\n",
      "loss: 0.007869  [ 1728/ 3200]\n",
      "loss: 0.014925  [ 1744/ 3200]\n",
      "loss: 0.011624  [ 1760/ 3200]\n",
      "loss: 0.006249  [ 1776/ 3200]\n",
      "loss: 0.006534  [ 1792/ 3200]\n",
      "loss: 0.012085  [ 1808/ 3200]\n",
      "loss: 0.008458  [ 1824/ 3200]\n",
      "loss: 0.010993  [ 1840/ 3200]\n",
      "loss: 0.018396  [ 1856/ 3200]\n",
      "loss: 0.017827  [ 1872/ 3200]\n",
      "loss: 0.003137  [ 1888/ 3200]\n",
      "loss: 0.005367  [ 1904/ 3200]\n",
      "loss: 0.006679  [ 1920/ 3200]\n",
      "loss: 0.006401  [ 1936/ 3200]\n",
      "loss: 0.011594  [ 1952/ 3200]\n",
      "loss: 0.007866  [ 1968/ 3200]\n",
      "loss: 0.011444  [ 1984/ 3200]\n",
      "loss: 0.010637  [ 2000/ 3200]\n",
      "loss: 0.011772  [ 2016/ 3200]\n",
      "loss: 0.005474  [ 2032/ 3200]\n",
      "loss: 0.030307  [ 2048/ 3200]\n",
      "loss: 0.006564  [ 2064/ 3200]\n",
      "loss: 0.006320  [ 2080/ 3200]\n",
      "loss: 0.018042  [ 2096/ 3200]\n",
      "loss: 0.018805  [ 2112/ 3200]\n",
      "loss: 0.008199  [ 2128/ 3200]\n",
      "loss: 0.016912  [ 2144/ 3200]\n",
      "loss: 0.007285  [ 2160/ 3200]\n",
      "loss: 0.008489  [ 2176/ 3200]\n",
      "loss: 0.006283  [ 2192/ 3200]\n",
      "loss: 0.019360  [ 2208/ 3200]\n",
      "loss: 0.008327  [ 2224/ 3200]\n",
      "loss: 0.005112  [ 2240/ 3200]\n",
      "loss: 0.022724  [ 2256/ 3200]\n",
      "loss: 0.014080  [ 2272/ 3200]\n",
      "loss: 0.011739  [ 2288/ 3200]\n",
      "loss: 0.010213  [ 2304/ 3200]\n",
      "loss: 0.002650  [ 2320/ 3200]\n",
      "loss: 0.017431  [ 2336/ 3200]\n",
      "loss: 0.013913  [ 2352/ 3200]\n",
      "loss: 0.021976  [ 2368/ 3200]\n",
      "loss: 0.010332  [ 2384/ 3200]\n",
      "loss: 0.007115  [ 2400/ 3200]\n",
      "loss: 0.008222  [ 2416/ 3200]\n",
      "loss: 0.013017  [ 2432/ 3200]\n",
      "loss: 0.011435  [ 2448/ 3200]\n",
      "loss: 0.011054  [ 2464/ 3200]\n",
      "loss: 0.013552  [ 2480/ 3200]\n",
      "loss: 0.020634  [ 2496/ 3200]\n",
      "loss: 0.010449  [ 2512/ 3200]\n",
      "loss: 0.013787  [ 2528/ 3200]\n",
      "loss: 0.010246  [ 2544/ 3200]\n",
      "loss: 0.013213  [ 2560/ 3200]\n",
      "loss: 0.041924  [ 2576/ 3200]\n",
      "loss: 0.028210  [ 2592/ 3200]\n",
      "loss: 0.009754  [ 2608/ 3200]\n",
      "loss: 0.014665  [ 2624/ 3200]\n",
      "loss: 0.003652  [ 2640/ 3200]\n",
      "loss: 0.016179  [ 2656/ 3200]\n",
      "loss: 0.016482  [ 2672/ 3200]\n",
      "loss: 0.003650  [ 2688/ 3200]\n",
      "loss: 0.006094  [ 2704/ 3200]\n",
      "loss: 0.010746  [ 2720/ 3200]\n",
      "loss: 0.010071  [ 2736/ 3200]\n",
      "loss: 0.009925  [ 2752/ 3200]\n",
      "loss: 0.078710  [ 2768/ 3200]\n",
      "loss: 0.002363  [ 2784/ 3200]\n",
      "loss: 0.014866  [ 2800/ 3200]\n",
      "loss: 0.009819  [ 2816/ 3200]\n",
      "loss: 0.008626  [ 2832/ 3200]\n",
      "loss: 0.013551  [ 2848/ 3200]\n",
      "loss: 0.004141  [ 2864/ 3200]\n",
      "loss: 0.011555  [ 2880/ 3200]\n",
      "loss: 0.017042  [ 2896/ 3200]\n",
      "loss: 0.006752  [ 2912/ 3200]\n",
      "loss: 0.014675  [ 2928/ 3200]\n",
      "loss: 0.001768  [ 2944/ 3200]\n",
      "loss: 0.013318  [ 2960/ 3200]\n",
      "loss: 0.018465  [ 2976/ 3200]\n",
      "loss: 0.011388  [ 2992/ 3200]\n",
      "loss: 0.021051  [ 3008/ 3200]\n",
      "loss: 0.013874  [ 3024/ 3200]\n",
      "loss: 0.015580  [ 3040/ 3200]\n",
      "loss: 0.007240  [ 3056/ 3200]\n",
      "loss: 0.013910  [ 3072/ 3200]\n",
      "loss: 0.016344  [ 3088/ 3200]\n",
      "loss: 0.006876  [ 3104/ 3200]\n",
      "loss: 0.006700  [ 3120/ 3200]\n",
      "loss: 0.002421  [ 3136/ 3200]\n",
      "loss: 0.005421  [ 3152/ 3200]\n",
      "loss: 0.010990  [ 3168/ 3200]\n",
      "loss: 0.006084  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.006075  [    0/ 3200]\n",
      "loss: 0.005602  [   16/ 3200]\n",
      "loss: 0.014772  [   32/ 3200]\n",
      "loss: 0.006298  [   48/ 3200]\n",
      "loss: 0.020451  [   64/ 3200]\n",
      "loss: 0.001872  [   80/ 3200]\n",
      "loss: 0.009930  [   96/ 3200]\n",
      "loss: 0.006707  [  112/ 3200]\n",
      "loss: 0.005197  [  128/ 3200]\n",
      "loss: 0.008744  [  144/ 3200]\n",
      "loss: 0.009621  [  160/ 3200]\n",
      "loss: 0.002179  [  176/ 3200]\n",
      "loss: 0.003944  [  192/ 3200]\n",
      "loss: 0.009901  [  208/ 3200]\n",
      "loss: 0.003399  [  224/ 3200]\n",
      "loss: 0.003824  [  240/ 3200]\n",
      "loss: 0.004671  [  256/ 3200]\n",
      "loss: 0.082522  [  272/ 3200]\n",
      "loss: 0.003134  [  288/ 3200]\n",
      "loss: 0.009611  [  304/ 3200]\n",
      "loss: 0.012997  [  320/ 3200]\n",
      "loss: 0.010342  [  336/ 3200]\n",
      "loss: 0.016227  [  352/ 3200]\n",
      "loss: 0.018635  [  368/ 3200]\n",
      "loss: 0.005626  [  384/ 3200]\n",
      "loss: 0.008213  [  400/ 3200]\n",
      "loss: 0.008767  [  416/ 3200]\n",
      "loss: 0.005098  [  432/ 3200]\n",
      "loss: 0.003254  [  448/ 3200]\n",
      "loss: 0.011728  [  464/ 3200]\n",
      "loss: 0.012513  [  480/ 3200]\n",
      "loss: 0.007932  [  496/ 3200]\n",
      "loss: 0.004504  [  512/ 3200]\n",
      "loss: 0.011280  [  528/ 3200]\n",
      "loss: 0.018190  [  544/ 3200]\n",
      "loss: 0.005911  [  560/ 3200]\n",
      "loss: 0.003388  [  576/ 3200]\n",
      "loss: 0.013510  [  592/ 3200]\n",
      "loss: 0.014628  [  608/ 3200]\n",
      "loss: 0.006607  [  624/ 3200]\n",
      "loss: 0.010198  [  640/ 3200]\n",
      "loss: 0.005380  [  656/ 3200]\n",
      "loss: 0.014524  [  672/ 3200]\n",
      "loss: 0.011229  [  688/ 3200]\n",
      "loss: 0.009227  [  704/ 3200]\n",
      "loss: 0.004624  [  720/ 3200]\n",
      "loss: 0.009510  [  736/ 3200]\n",
      "loss: 0.002808  [  752/ 3200]\n",
      "loss: 0.009862  [  768/ 3200]\n",
      "loss: 0.008375  [  784/ 3200]\n",
      "loss: 0.003674  [  800/ 3200]\n",
      "loss: 0.016438  [  816/ 3200]\n",
      "loss: 0.013787  [  832/ 3200]\n",
      "loss: 0.006877  [  848/ 3200]\n",
      "loss: 0.013859  [  864/ 3200]\n",
      "loss: 0.008021  [  880/ 3200]\n",
      "loss: 0.002634  [  896/ 3200]\n",
      "loss: 0.007083  [  912/ 3200]\n",
      "loss: 0.009999  [  928/ 3200]\n",
      "loss: 0.008845  [  944/ 3200]\n",
      "loss: 0.007581  [  960/ 3200]\n",
      "loss: 0.007211  [  976/ 3200]\n",
      "loss: 0.028747  [  992/ 3200]\n",
      "loss: 0.010330  [ 1008/ 3200]\n",
      "loss: 0.011283  [ 1024/ 3200]\n",
      "loss: 0.021950  [ 1040/ 3200]\n",
      "loss: 0.005637  [ 1056/ 3200]\n",
      "loss: 0.003746  [ 1072/ 3200]\n",
      "loss: 0.005641  [ 1088/ 3200]\n",
      "loss: 0.014789  [ 1104/ 3200]\n",
      "loss: 0.006801  [ 1120/ 3200]\n",
      "loss: 0.005718  [ 1136/ 3200]\n",
      "loss: 0.005039  [ 1152/ 3200]\n",
      "loss: 0.003081  [ 1168/ 3200]\n",
      "loss: 0.006631  [ 1184/ 3200]\n",
      "loss: 0.008271  [ 1200/ 3200]\n",
      "loss: 0.022920  [ 1216/ 3200]\n",
      "loss: 0.009025  [ 1232/ 3200]\n",
      "loss: 0.013858  [ 1248/ 3200]\n",
      "loss: 0.003239  [ 1264/ 3200]\n",
      "loss: 0.017053  [ 1280/ 3200]\n",
      "loss: 0.015733  [ 1296/ 3200]\n",
      "loss: 0.004314  [ 1312/ 3200]\n",
      "loss: 0.008802  [ 1328/ 3200]\n",
      "loss: 0.002708  [ 1344/ 3200]\n",
      "loss: 0.000770  [ 1360/ 3200]\n",
      "loss: 0.005262  [ 1376/ 3200]\n",
      "loss: 0.005253  [ 1392/ 3200]\n",
      "loss: 0.019206  [ 1408/ 3200]\n",
      "loss: 0.003103  [ 1424/ 3200]\n",
      "loss: 0.005231  [ 1440/ 3200]\n",
      "loss: 0.021276  [ 1456/ 3200]\n",
      "loss: 0.005535  [ 1472/ 3200]\n",
      "loss: 0.012823  [ 1488/ 3200]\n",
      "loss: 0.007956  [ 1504/ 3200]\n",
      "loss: 0.007469  [ 1520/ 3200]\n",
      "loss: 0.015220  [ 1536/ 3200]\n",
      "loss: 0.004633  [ 1552/ 3200]\n",
      "loss: 0.004146  [ 1568/ 3200]\n",
      "loss: 0.007773  [ 1584/ 3200]\n",
      "loss: 0.298228  [ 1600/ 3200]\n",
      "loss: 0.012591  [ 1616/ 3200]\n",
      "loss: 0.009495  [ 1632/ 3200]\n",
      "loss: 0.006890  [ 1648/ 3200]\n",
      "loss: 0.005982  [ 1664/ 3200]\n",
      "loss: 0.004900  [ 1680/ 3200]\n",
      "loss: 0.019760  [ 1696/ 3200]\n",
      "loss: 0.003622  [ 1712/ 3200]\n",
      "loss: 0.005159  [ 1728/ 3200]\n",
      "loss: 0.003736  [ 1744/ 3200]\n",
      "loss: 0.002926  [ 1760/ 3200]\n",
      "loss: 0.006898  [ 1776/ 3200]\n",
      "loss: 0.009168  [ 1792/ 3200]\n",
      "loss: 0.008199  [ 1808/ 3200]\n",
      "loss: 0.002903  [ 1824/ 3200]\n",
      "loss: 0.007697  [ 1840/ 3200]\n",
      "loss: 0.017664  [ 1856/ 3200]\n",
      "loss: 0.010314  [ 1872/ 3200]\n",
      "loss: 0.003720  [ 1888/ 3200]\n",
      "loss: 0.013341  [ 1904/ 3200]\n",
      "loss: 0.005624  [ 1920/ 3200]\n",
      "loss: 0.008765  [ 1936/ 3200]\n",
      "loss: 0.017129  [ 1952/ 3200]\n",
      "loss: 0.010272  [ 1968/ 3200]\n",
      "loss: 0.011739  [ 1984/ 3200]\n",
      "loss: 0.005861  [ 2000/ 3200]\n",
      "loss: 0.010191  [ 2016/ 3200]\n",
      "loss: 0.006231  [ 2032/ 3200]\n",
      "loss: 0.015990  [ 2048/ 3200]\n",
      "loss: 0.008133  [ 2064/ 3200]\n",
      "loss: 0.007954  [ 2080/ 3200]\n",
      "loss: 0.004709  [ 2096/ 3200]\n",
      "loss: 0.008225  [ 2112/ 3200]\n",
      "loss: 0.016876  [ 2128/ 3200]\n",
      "loss: 0.093481  [ 2144/ 3200]\n",
      "loss: 0.012973  [ 2160/ 3200]\n",
      "loss: 0.008113  [ 2176/ 3200]\n",
      "loss: 0.006135  [ 2192/ 3200]\n",
      "loss: 0.004372  [ 2208/ 3200]\n",
      "loss: 0.006428  [ 2224/ 3200]\n",
      "loss: 0.010839  [ 2240/ 3200]\n",
      "loss: 0.005226  [ 2256/ 3200]\n",
      "loss: 0.002572  [ 2272/ 3200]\n",
      "loss: 0.016537  [ 2288/ 3200]\n",
      "loss: 0.005844  [ 2304/ 3200]\n",
      "loss: 0.008632  [ 2320/ 3200]\n",
      "loss: 0.003790  [ 2336/ 3200]\n",
      "loss: 0.015913  [ 2352/ 3200]\n",
      "loss: 0.004520  [ 2368/ 3200]\n",
      "loss: 0.009380  [ 2384/ 3200]\n",
      "loss: 0.015558  [ 2400/ 3200]\n",
      "loss: 0.004512  [ 2416/ 3200]\n",
      "loss: 0.007224  [ 2432/ 3200]\n",
      "loss: 0.010616  [ 2448/ 3200]\n",
      "loss: 0.007399  [ 2464/ 3200]\n",
      "loss: 0.007237  [ 2480/ 3200]\n",
      "loss: 0.008321  [ 2496/ 3200]\n",
      "loss: 0.016030  [ 2512/ 3200]\n",
      "loss: 0.009751  [ 2528/ 3200]\n",
      "loss: 0.004401  [ 2544/ 3200]\n",
      "loss: 0.011139  [ 2560/ 3200]\n",
      "loss: 0.011132  [ 2576/ 3200]\n",
      "loss: 0.003305  [ 2592/ 3200]\n",
      "loss: 0.010696  [ 2608/ 3200]\n",
      "loss: 0.008065  [ 2624/ 3200]\n",
      "loss: 0.003337  [ 2640/ 3200]\n",
      "loss: 0.005831  [ 2656/ 3200]\n",
      "loss: 0.022519  [ 2672/ 3200]\n",
      "loss: 0.005682  [ 2688/ 3200]\n",
      "loss: 0.002040  [ 2704/ 3200]\n",
      "loss: 0.012348  [ 2720/ 3200]\n",
      "loss: 0.008891  [ 2736/ 3200]\n",
      "loss: 0.006649  [ 2752/ 3200]\n",
      "loss: 0.016239  [ 2768/ 3200]\n",
      "loss: 0.004657  [ 2784/ 3200]\n",
      "loss: 0.015820  [ 2800/ 3200]\n",
      "loss: 0.006044  [ 2816/ 3200]\n",
      "loss: 0.013597  [ 2832/ 3200]\n",
      "loss: 0.004865  [ 2848/ 3200]\n",
      "loss: 0.014492  [ 2864/ 3200]\n",
      "loss: 0.004870  [ 2880/ 3200]\n",
      "loss: 0.006993  [ 2896/ 3200]\n",
      "loss: 0.008874  [ 2912/ 3200]\n",
      "loss: 0.008183  [ 2928/ 3200]\n",
      "loss: 0.013126  [ 2944/ 3200]\n",
      "loss: 0.005307  [ 2960/ 3200]\n",
      "loss: 0.015073  [ 2976/ 3200]\n",
      "loss: 0.003335  [ 2992/ 3200]\n",
      "loss: 0.004721  [ 3008/ 3200]\n",
      "loss: 0.017138  [ 3024/ 3200]\n",
      "loss: 0.001201  [ 3040/ 3200]\n",
      "loss: 0.006368  [ 3056/ 3200]\n",
      "loss: 0.011050  [ 3072/ 3200]\n",
      "loss: 0.004340  [ 3088/ 3200]\n",
      "loss: 0.011895  [ 3104/ 3200]\n",
      "loss: 0.007140  [ 3120/ 3200]\n",
      "loss: 0.011924  [ 3136/ 3200]\n",
      "loss: 0.011926  [ 3152/ 3200]\n",
      "loss: 0.007134  [ 3168/ 3200]\n",
      "loss: 0.015134  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.008381  [    0/ 3200]\n",
      "loss: 0.005098  [   16/ 3200]\n",
      "loss: 0.008975  [   32/ 3200]\n",
      "loss: 0.010032  [   48/ 3200]\n",
      "loss: 0.007299  [   64/ 3200]\n",
      "loss: 0.005360  [   80/ 3200]\n",
      "loss: 0.006994  [   96/ 3200]\n",
      "loss: 0.009634  [  112/ 3200]\n",
      "loss: 0.016273  [  128/ 3200]\n",
      "loss: 0.002530  [  144/ 3200]\n",
      "loss: 0.006188  [  160/ 3200]\n",
      "loss: 0.005554  [  176/ 3200]\n",
      "loss: 0.006437  [  192/ 3200]\n",
      "loss: 0.010610  [  208/ 3200]\n",
      "loss: 0.005170  [  224/ 3200]\n",
      "loss: 0.003013  [  240/ 3200]\n",
      "loss: 0.006190  [  256/ 3200]\n",
      "loss: 0.006609  [  272/ 3200]\n",
      "loss: 0.007447  [  288/ 3200]\n",
      "loss: 0.003122  [  304/ 3200]\n",
      "loss: 0.010679  [  320/ 3200]\n",
      "loss: 0.011121  [  336/ 3200]\n",
      "loss: 0.004883  [  352/ 3200]\n",
      "loss: 0.005793  [  368/ 3200]\n",
      "loss: 0.086326  [  384/ 3200]\n",
      "loss: 0.016286  [  400/ 3200]\n",
      "loss: 0.008732  [  416/ 3200]\n",
      "loss: 0.003614  [  432/ 3200]\n",
      "loss: 0.002423  [  448/ 3200]\n",
      "loss: 0.005028  [  464/ 3200]\n",
      "loss: 0.009939  [  480/ 3200]\n",
      "loss: 0.006370  [  496/ 3200]\n",
      "loss: 0.008270  [  512/ 3200]\n",
      "loss: 0.004394  [  528/ 3200]\n",
      "loss: 0.013090  [  544/ 3200]\n",
      "loss: 0.002084  [  560/ 3200]\n",
      "loss: 0.018247  [  576/ 3200]\n",
      "loss: 0.014783  [  592/ 3200]\n",
      "loss: 0.010243  [  608/ 3200]\n",
      "loss: 0.016899  [  624/ 3200]\n",
      "loss: 0.001815  [  640/ 3200]\n",
      "loss: 0.005242  [  656/ 3200]\n",
      "loss: 0.006255  [  672/ 3200]\n",
      "loss: 0.002476  [  688/ 3200]\n",
      "loss: 0.016927  [  704/ 3200]\n",
      "loss: 0.004874  [  720/ 3200]\n",
      "loss: 0.001906  [  736/ 3200]\n",
      "loss: 0.007011  [  752/ 3200]\n",
      "loss: 0.001515  [  768/ 3200]\n",
      "loss: 0.021731  [  784/ 3200]\n",
      "loss: 0.007046  [  800/ 3200]\n",
      "loss: 0.012130  [  816/ 3200]\n",
      "loss: 0.004595  [  832/ 3200]\n",
      "loss: 0.004572  [  848/ 3200]\n",
      "loss: 0.009695  [  864/ 3200]\n",
      "loss: 0.003129  [  880/ 3200]\n",
      "loss: 0.004936  [  896/ 3200]\n",
      "loss: 0.007099  [  912/ 3200]\n",
      "loss: 0.008110  [  928/ 3200]\n",
      "loss: 0.004204  [  944/ 3200]\n",
      "loss: 0.018752  [  960/ 3200]\n",
      "loss: 0.005926  [  976/ 3200]\n",
      "loss: 0.008237  [  992/ 3200]\n",
      "loss: 0.008748  [ 1008/ 3200]\n",
      "loss: 0.012382  [ 1024/ 3200]\n",
      "loss: 0.015348  [ 1040/ 3200]\n",
      "loss: 0.003386  [ 1056/ 3200]\n",
      "loss: 0.006946  [ 1072/ 3200]\n",
      "loss: 0.008692  [ 1088/ 3200]\n",
      "loss: 0.005162  [ 1104/ 3200]\n",
      "loss: 0.004716  [ 1120/ 3200]\n",
      "loss: 0.011421  [ 1136/ 3200]\n",
      "loss: 0.002415  [ 1152/ 3200]\n",
      "loss: 0.010676  [ 1168/ 3200]\n",
      "loss: 0.009181  [ 1184/ 3200]\n",
      "loss: 0.011747  [ 1200/ 3200]\n",
      "loss: 0.011089  [ 1216/ 3200]\n",
      "loss: 0.006220  [ 1232/ 3200]\n",
      "loss: 0.003604  [ 1248/ 3200]\n",
      "loss: 0.005052  [ 1264/ 3200]\n",
      "loss: 0.010732  [ 1280/ 3200]\n",
      "loss: 0.008120  [ 1296/ 3200]\n",
      "loss: 0.004094  [ 1312/ 3200]\n",
      "loss: 0.003420  [ 1328/ 3200]\n",
      "loss: 0.005206  [ 1344/ 3200]\n",
      "loss: 0.002506  [ 1360/ 3200]\n",
      "loss: 0.008234  [ 1376/ 3200]\n",
      "loss: 0.009553  [ 1392/ 3200]\n",
      "loss: 0.009241  [ 1408/ 3200]\n",
      "loss: 0.012310  [ 1424/ 3200]\n",
      "loss: 0.007848  [ 1440/ 3200]\n",
      "loss: 0.003009  [ 1456/ 3200]\n",
      "loss: 0.007037  [ 1472/ 3200]\n",
      "loss: 0.006436  [ 1488/ 3200]\n",
      "loss: 0.003183  [ 1504/ 3200]\n",
      "loss: 0.005391  [ 1520/ 3200]\n",
      "loss: 0.011187  [ 1536/ 3200]\n",
      "loss: 0.006211  [ 1552/ 3200]\n",
      "loss: 0.009418  [ 1568/ 3200]\n",
      "loss: 0.002507  [ 1584/ 3200]\n",
      "loss: 0.006444  [ 1600/ 3200]\n",
      "loss: 0.006393  [ 1616/ 3200]\n",
      "loss: 0.002824  [ 1632/ 3200]\n",
      "loss: 0.002951  [ 1648/ 3200]\n",
      "loss: 0.008357  [ 1664/ 3200]\n",
      "loss: 0.002192  [ 1680/ 3200]\n",
      "loss: 0.002636  [ 1696/ 3200]\n",
      "loss: 0.011708  [ 1712/ 3200]\n",
      "loss: 0.011826  [ 1728/ 3200]\n",
      "loss: 0.016231  [ 1744/ 3200]\n",
      "loss: 0.013569  [ 1760/ 3200]\n",
      "loss: 0.006750  [ 1776/ 3200]\n",
      "loss: 0.006492  [ 1792/ 3200]\n",
      "loss: 0.008563  [ 1808/ 3200]\n",
      "loss: 0.011809  [ 1824/ 3200]\n",
      "loss: 0.004094  [ 1840/ 3200]\n",
      "loss: 0.011929  [ 1856/ 3200]\n",
      "loss: 0.004292  [ 1872/ 3200]\n",
      "loss: 0.011078  [ 1888/ 3200]\n",
      "loss: 0.009926  [ 1904/ 3200]\n",
      "loss: 0.007008  [ 1920/ 3200]\n",
      "loss: 0.002444  [ 1936/ 3200]\n",
      "loss: 0.003789  [ 1952/ 3200]\n",
      "loss: 0.007980  [ 1968/ 3200]\n",
      "loss: 0.005378  [ 1984/ 3200]\n",
      "loss: 0.009552  [ 2000/ 3200]\n",
      "loss: 0.009154  [ 2016/ 3200]\n",
      "loss: 0.015318  [ 2032/ 3200]\n",
      "loss: 0.006672  [ 2048/ 3200]\n",
      "loss: 0.009983  [ 2064/ 3200]\n",
      "loss: 0.010692  [ 2080/ 3200]\n",
      "loss: 0.023984  [ 2096/ 3200]\n",
      "loss: 0.005856  [ 2112/ 3200]\n",
      "loss: 0.002794  [ 2128/ 3200]\n",
      "loss: 0.007230  [ 2144/ 3200]\n",
      "loss: 0.003253  [ 2160/ 3200]\n",
      "loss: 0.006051  [ 2176/ 3200]\n",
      "loss: 0.013280  [ 2192/ 3200]\n",
      "loss: 0.006008  [ 2208/ 3200]\n",
      "loss: 0.005502  [ 2224/ 3200]\n",
      "loss: 0.001923  [ 2240/ 3200]\n",
      "loss: 0.002659  [ 2256/ 3200]\n",
      "loss: 0.005454  [ 2272/ 3200]\n",
      "loss: 0.005452  [ 2288/ 3200]\n",
      "loss: 0.003915  [ 2304/ 3200]\n",
      "loss: 0.009915  [ 2320/ 3200]\n",
      "loss: 0.015293  [ 2336/ 3200]\n",
      "loss: 0.003663  [ 2352/ 3200]\n",
      "loss: 0.014513  [ 2368/ 3200]\n",
      "loss: 0.007002  [ 2384/ 3200]\n",
      "loss: 0.006164  [ 2400/ 3200]\n",
      "loss: 0.011320  [ 2416/ 3200]\n",
      "loss: 0.004034  [ 2432/ 3200]\n",
      "loss: 0.002816  [ 2448/ 3200]\n",
      "loss: 0.007808  [ 2464/ 3200]\n",
      "loss: 0.009217  [ 2480/ 3200]\n",
      "loss: 0.010396  [ 2496/ 3200]\n",
      "loss: 0.009314  [ 2512/ 3200]\n",
      "loss: 0.006109  [ 2528/ 3200]\n",
      "loss: 0.006901  [ 2544/ 3200]\n",
      "loss: 0.009222  [ 2560/ 3200]\n",
      "loss: 0.001053  [ 2576/ 3200]\n",
      "loss: 0.004467  [ 2592/ 3200]\n",
      "loss: 0.004793  [ 2608/ 3200]\n",
      "loss: 0.006191  [ 2624/ 3200]\n",
      "loss: 0.006625  [ 2640/ 3200]\n",
      "loss: 0.012056  [ 2656/ 3200]\n",
      "loss: 0.005781  [ 2672/ 3200]\n",
      "loss: 0.021859  [ 2688/ 3200]\n",
      "loss: 0.006329  [ 2704/ 3200]\n",
      "loss: 0.003944  [ 2720/ 3200]\n",
      "loss: 0.003998  [ 2736/ 3200]\n",
      "loss: 0.006132  [ 2752/ 3200]\n",
      "loss: 0.008726  [ 2768/ 3200]\n",
      "loss: 0.010032  [ 2784/ 3200]\n",
      "loss: 0.005128  [ 2800/ 3200]\n",
      "loss: 0.002114  [ 2816/ 3200]\n",
      "loss: 0.005934  [ 2832/ 3200]\n",
      "loss: 0.002197  [ 2848/ 3200]\n",
      "loss: 0.006919  [ 2864/ 3200]\n",
      "loss: 0.007087  [ 2880/ 3200]\n",
      "loss: 0.008867  [ 2896/ 3200]\n",
      "loss: 0.007429  [ 2912/ 3200]\n",
      "loss: 0.012800  [ 2928/ 3200]\n",
      "loss: 0.008047  [ 2944/ 3200]\n",
      "loss: 0.012890  [ 2960/ 3200]\n",
      "loss: 0.002134  [ 2976/ 3200]\n",
      "loss: 0.002663  [ 2992/ 3200]\n",
      "loss: 0.269695  [ 3008/ 3200]\n",
      "loss: 0.008365  [ 3024/ 3200]\n",
      "loss: 0.046230  [ 3040/ 3200]\n",
      "loss: 0.003620  [ 3056/ 3200]\n",
      "loss: 0.011382  [ 3072/ 3200]\n",
      "loss: 0.007359  [ 3088/ 3200]\n",
      "loss: 0.008224  [ 3104/ 3200]\n",
      "loss: 0.003268  [ 3120/ 3200]\n",
      "loss: 0.007689  [ 3136/ 3200]\n",
      "loss: 0.008826  [ 3152/ 3200]\n",
      "loss: 0.000982  [ 3168/ 3200]\n",
      "loss: 0.008348  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.004893  [    0/ 3200]\n",
      "loss: 0.008310  [   16/ 3200]\n",
      "loss: 0.005398  [   32/ 3200]\n",
      "loss: 0.002875  [   48/ 3200]\n",
      "loss: 0.009818  [   64/ 3200]\n",
      "loss: 0.018980  [   80/ 3200]\n",
      "loss: 0.002804  [   96/ 3200]\n",
      "loss: 0.010657  [  112/ 3200]\n",
      "loss: 0.001798  [  128/ 3200]\n",
      "loss: 0.007879  [  144/ 3200]\n",
      "loss: 0.002971  [  160/ 3200]\n",
      "loss: 0.004178  [  176/ 3200]\n",
      "loss: 0.002458  [  192/ 3200]\n",
      "loss: 0.002230  [  208/ 3200]\n",
      "loss: 0.007169  [  224/ 3200]\n",
      "loss: 0.009251  [  240/ 3200]\n",
      "loss: 0.007497  [  256/ 3200]\n",
      "loss: 0.006718  [  272/ 3200]\n",
      "loss: 0.005538  [  288/ 3200]\n",
      "loss: 0.007600  [  304/ 3200]\n",
      "loss: 0.005024  [  320/ 3200]\n",
      "loss: 0.006961  [  336/ 3200]\n",
      "loss: 0.005637  [  352/ 3200]\n",
      "loss: 0.007462  [  368/ 3200]\n",
      "loss: 0.008924  [  384/ 3200]\n",
      "loss: 0.003572  [  400/ 3200]\n",
      "loss: 0.005879  [  416/ 3200]\n",
      "loss: 0.011083  [  432/ 3200]\n",
      "loss: 0.008429  [  448/ 3200]\n",
      "loss: 0.009263  [  464/ 3200]\n",
      "loss: 0.008073  [  480/ 3200]\n",
      "loss: 0.014498  [  496/ 3200]\n",
      "loss: 0.007039  [  512/ 3200]\n",
      "loss: 0.012421  [  528/ 3200]\n",
      "loss: 0.007511  [  544/ 3200]\n",
      "loss: 0.005992  [  560/ 3200]\n",
      "loss: 0.004810  [  576/ 3200]\n",
      "loss: 0.008365  [  592/ 3200]\n",
      "loss: 0.007107  [  608/ 3200]\n",
      "loss: 0.003748  [  624/ 3200]\n",
      "loss: 0.002105  [  640/ 3200]\n",
      "loss: 0.013981  [  656/ 3200]\n",
      "loss: 0.004714  [  672/ 3200]\n",
      "loss: 0.007140  [  688/ 3200]\n",
      "loss: 0.003930  [  704/ 3200]\n",
      "loss: 0.000971  [  720/ 3200]\n",
      "loss: 0.005339  [  736/ 3200]\n",
      "loss: 0.011480  [  752/ 3200]\n",
      "loss: 0.004518  [  768/ 3200]\n",
      "loss: 0.003577  [  784/ 3200]\n",
      "loss: 0.004193  [  800/ 3200]\n",
      "loss: 0.005254  [  816/ 3200]\n",
      "loss: 0.007804  [  832/ 3200]\n",
      "loss: 0.009841  [  848/ 3200]\n",
      "loss: 0.004251  [  864/ 3200]\n",
      "loss: 0.258484  [  880/ 3200]\n",
      "loss: 0.053281  [  896/ 3200]\n",
      "loss: 0.007375  [  912/ 3200]\n",
      "loss: 0.006703  [  928/ 3200]\n",
      "loss: 0.014159  [  944/ 3200]\n",
      "loss: 0.005781  [  960/ 3200]\n",
      "loss: 0.003692  [  976/ 3200]\n",
      "loss: 0.005209  [  992/ 3200]\n",
      "loss: 0.005716  [ 1008/ 3200]\n",
      "loss: 0.002351  [ 1024/ 3200]\n",
      "loss: 0.006383  [ 1040/ 3200]\n",
      "loss: 0.008251  [ 1056/ 3200]\n",
      "loss: 0.005283  [ 1072/ 3200]\n",
      "loss: 0.009318  [ 1088/ 3200]\n",
      "loss: 0.005525  [ 1104/ 3200]\n",
      "loss: 0.002502  [ 1120/ 3200]\n",
      "loss: 0.006725  [ 1136/ 3200]\n",
      "loss: 0.009311  [ 1152/ 3200]\n",
      "loss: 0.009047  [ 1168/ 3200]\n",
      "loss: 0.007388  [ 1184/ 3200]\n",
      "loss: 0.004188  [ 1200/ 3200]\n",
      "loss: 0.010618  [ 1216/ 3200]\n",
      "loss: 0.002927  [ 1232/ 3200]\n",
      "loss: 0.004350  [ 1248/ 3200]\n",
      "loss: 0.006223  [ 1264/ 3200]\n",
      "loss: 0.003302  [ 1280/ 3200]\n",
      "loss: 0.004733  [ 1296/ 3200]\n",
      "loss: 0.005347  [ 1312/ 3200]\n",
      "loss: 0.003314  [ 1328/ 3200]\n",
      "loss: 0.006897  [ 1344/ 3200]\n",
      "loss: 0.006871  [ 1360/ 3200]\n",
      "loss: 0.006474  [ 1376/ 3200]\n",
      "loss: 0.009958  [ 1392/ 3200]\n",
      "loss: 0.002351  [ 1408/ 3200]\n",
      "loss: 0.007890  [ 1424/ 3200]\n",
      "loss: 0.003476  [ 1440/ 3200]\n",
      "loss: 0.004776  [ 1456/ 3200]\n",
      "loss: 0.006767  [ 1472/ 3200]\n",
      "loss: 0.002827  [ 1488/ 3200]\n",
      "loss: 0.003348  [ 1504/ 3200]\n",
      "loss: 0.004528  [ 1520/ 3200]\n",
      "loss: 0.011142  [ 1536/ 3200]\n",
      "loss: 0.006087  [ 1552/ 3200]\n",
      "loss: 0.008823  [ 1568/ 3200]\n",
      "loss: 0.006671  [ 1584/ 3200]\n",
      "loss: 0.003587  [ 1600/ 3200]\n",
      "loss: 0.003724  [ 1616/ 3200]\n",
      "loss: 0.007687  [ 1632/ 3200]\n",
      "loss: 0.001549  [ 1648/ 3200]\n",
      "loss: 0.005272  [ 1664/ 3200]\n",
      "loss: 0.006963  [ 1680/ 3200]\n",
      "loss: 0.003810  [ 1696/ 3200]\n",
      "loss: 0.003480  [ 1712/ 3200]\n",
      "loss: 0.002192  [ 1728/ 3200]\n",
      "loss: 0.013214  [ 1744/ 3200]\n",
      "loss: 0.002361  [ 1760/ 3200]\n",
      "loss: 0.006611  [ 1776/ 3200]\n",
      "loss: 0.005875  [ 1792/ 3200]\n",
      "loss: 0.005993  [ 1808/ 3200]\n",
      "loss: 0.005544  [ 1824/ 3200]\n",
      "loss: 0.005058  [ 1840/ 3200]\n",
      "loss: 0.006456  [ 1856/ 3200]\n",
      "loss: 0.008986  [ 1872/ 3200]\n",
      "loss: 0.000400  [ 1888/ 3200]\n",
      "loss: 0.004668  [ 1904/ 3200]\n",
      "loss: 0.008073  [ 1920/ 3200]\n",
      "loss: 0.006583  [ 1936/ 3200]\n",
      "loss: 0.012193  [ 1952/ 3200]\n",
      "loss: 0.003899  [ 1968/ 3200]\n",
      "loss: 0.026360  [ 1984/ 3200]\n",
      "loss: 0.001782  [ 2000/ 3200]\n",
      "loss: 0.078097  [ 2016/ 3200]\n",
      "loss: 0.013717  [ 2032/ 3200]\n",
      "loss: 0.004740  [ 2048/ 3200]\n",
      "loss: 0.003617  [ 2064/ 3200]\n",
      "loss: 0.004636  [ 2080/ 3200]\n",
      "loss: 0.000826  [ 2096/ 3200]\n",
      "loss: 0.008612  [ 2112/ 3200]\n",
      "loss: 0.006369  [ 2128/ 3200]\n",
      "loss: 0.006274  [ 2144/ 3200]\n",
      "loss: 0.006068  [ 2160/ 3200]\n",
      "loss: 0.006724  [ 2176/ 3200]\n",
      "loss: 0.018275  [ 2192/ 3200]\n",
      "loss: 0.003574  [ 2208/ 3200]\n",
      "loss: 0.006368  [ 2224/ 3200]\n",
      "loss: 0.005630  [ 2240/ 3200]\n",
      "loss: 0.003054  [ 2256/ 3200]\n",
      "loss: 0.001961  [ 2272/ 3200]\n",
      "loss: 0.004177  [ 2288/ 3200]\n",
      "loss: 0.002130  [ 2304/ 3200]\n",
      "loss: 0.001142  [ 2320/ 3200]\n",
      "loss: 0.006268  [ 2336/ 3200]\n",
      "loss: 0.006576  [ 2352/ 3200]\n",
      "loss: 0.005853  [ 2368/ 3200]\n",
      "loss: 0.002822  [ 2384/ 3200]\n",
      "loss: 0.002868  [ 2400/ 3200]\n",
      "loss: 0.004303  [ 2416/ 3200]\n",
      "loss: 0.006209  [ 2432/ 3200]\n",
      "loss: 0.004002  [ 2448/ 3200]\n",
      "loss: 0.001321  [ 2464/ 3200]\n",
      "loss: 0.002479  [ 2480/ 3200]\n",
      "loss: 0.010242  [ 2496/ 3200]\n",
      "loss: 0.008033  [ 2512/ 3200]\n",
      "loss: 0.008102  [ 2528/ 3200]\n",
      "loss: 0.007715  [ 2544/ 3200]\n",
      "loss: 0.012826  [ 2560/ 3200]\n",
      "loss: 0.005878  [ 2576/ 3200]\n",
      "loss: 0.001877  [ 2592/ 3200]\n",
      "loss: 0.005887  [ 2608/ 3200]\n",
      "loss: 0.003474  [ 2624/ 3200]\n",
      "loss: 0.007288  [ 2640/ 3200]\n",
      "loss: 0.003561  [ 2656/ 3200]\n",
      "loss: 0.009021  [ 2672/ 3200]\n",
      "loss: 0.016383  [ 2688/ 3200]\n",
      "loss: 0.002063  [ 2704/ 3200]\n",
      "loss: 0.003101  [ 2720/ 3200]\n",
      "loss: 0.003387  [ 2736/ 3200]\n",
      "loss: 0.006484  [ 2752/ 3200]\n",
      "loss: 0.007191  [ 2768/ 3200]\n",
      "loss: 0.005143  [ 2784/ 3200]\n",
      "loss: 0.003527  [ 2800/ 3200]\n",
      "loss: 0.009164  [ 2816/ 3200]\n",
      "loss: 0.009065  [ 2832/ 3200]\n",
      "loss: 0.002451  [ 2848/ 3200]\n",
      "loss: 0.006064  [ 2864/ 3200]\n",
      "loss: 0.002952  [ 2880/ 3200]\n",
      "loss: 0.019070  [ 2896/ 3200]\n",
      "loss: 0.004108  [ 2912/ 3200]\n",
      "loss: 0.006161  [ 2928/ 3200]\n",
      "loss: 0.005771  [ 2944/ 3200]\n",
      "loss: 0.006028  [ 2960/ 3200]\n",
      "loss: 0.008162  [ 2976/ 3200]\n",
      "loss: 0.006580  [ 2992/ 3200]\n",
      "loss: 0.001964  [ 3008/ 3200]\n",
      "loss: 0.007416  [ 3024/ 3200]\n",
      "loss: 0.003678  [ 3040/ 3200]\n",
      "loss: 0.017027  [ 3056/ 3200]\n",
      "loss: 0.012241  [ 3072/ 3200]\n",
      "loss: 0.003962  [ 3088/ 3200]\n",
      "loss: 0.004530  [ 3104/ 3200]\n",
      "loss: 0.003024  [ 3120/ 3200]\n",
      "loss: 0.015819  [ 3136/ 3200]\n",
      "loss: 0.005620  [ 3152/ 3200]\n",
      "loss: 0.008457  [ 3168/ 3200]\n",
      "loss: 0.015330  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 0.003350  [    0/ 3200]\n",
      "loss: 0.009101  [   16/ 3200]\n",
      "loss: 0.005114  [   32/ 3200]\n",
      "loss: 0.001112  [   48/ 3200]\n",
      "loss: 0.001895  [   64/ 3200]\n",
      "loss: 0.002565  [   80/ 3200]\n",
      "loss: 0.003725  [   96/ 3200]\n",
      "loss: 0.007740  [  112/ 3200]\n",
      "loss: 0.007824  [  128/ 3200]\n",
      "loss: 0.013362  [  144/ 3200]\n",
      "loss: 0.008287  [  160/ 3200]\n",
      "loss: 0.008693  [  176/ 3200]\n",
      "loss: 0.007769  [  192/ 3200]\n",
      "loss: 0.003480  [  208/ 3200]\n",
      "loss: 0.004449  [  224/ 3200]\n",
      "loss: 0.005085  [  240/ 3200]\n",
      "loss: 0.003001  [  256/ 3200]\n",
      "loss: 0.004873  [  272/ 3200]\n",
      "loss: 0.008544  [  288/ 3200]\n",
      "loss: 0.004034  [  304/ 3200]\n",
      "loss: 0.001315  [  320/ 3200]\n",
      "loss: 0.014289  [  336/ 3200]\n",
      "loss: 0.004205  [  352/ 3200]\n",
      "loss: 0.002893  [  368/ 3200]\n",
      "loss: 0.007174  [  384/ 3200]\n",
      "loss: 0.006353  [  400/ 3200]\n",
      "loss: 0.008654  [  416/ 3200]\n",
      "loss: 0.008044  [  432/ 3200]\n",
      "loss: 0.004632  [  448/ 3200]\n",
      "loss: 0.004670  [  464/ 3200]\n",
      "loss: 0.005746  [  480/ 3200]\n",
      "loss: 0.004749  [  496/ 3200]\n",
      "loss: 0.003409  [  512/ 3200]\n",
      "loss: 0.001822  [  528/ 3200]\n",
      "loss: 0.004495  [  544/ 3200]\n",
      "loss: 0.006201  [  560/ 3200]\n",
      "loss: 0.004689  [  576/ 3200]\n",
      "loss: 0.007661  [  592/ 3200]\n",
      "loss: 0.007834  [  608/ 3200]\n",
      "loss: 0.010042  [  624/ 3200]\n",
      "loss: 0.002451  [  640/ 3200]\n",
      "loss: 0.002706  [  656/ 3200]\n",
      "loss: 0.003281  [  672/ 3200]\n",
      "loss: 0.003807  [  688/ 3200]\n",
      "loss: 0.004247  [  704/ 3200]\n",
      "loss: 0.002883  [  720/ 3200]\n",
      "loss: 0.000564  [  736/ 3200]\n",
      "loss: 0.002598  [  752/ 3200]\n",
      "loss: 0.005613  [  768/ 3200]\n",
      "loss: 0.002052  [  784/ 3200]\n",
      "loss: 0.004432  [  800/ 3200]\n",
      "loss: 0.005699  [  816/ 3200]\n",
      "loss: 0.007090  [  832/ 3200]\n",
      "loss: 0.003594  [  848/ 3200]\n",
      "loss: 0.008560  [  864/ 3200]\n",
      "loss: 0.010181  [  880/ 3200]\n",
      "loss: 0.003287  [  896/ 3200]\n",
      "loss: 0.010018  [  912/ 3200]\n",
      "loss: 0.007944  [  928/ 3200]\n",
      "loss: 0.005249  [  944/ 3200]\n",
      "loss: 0.005281  [  960/ 3200]\n",
      "loss: 0.002137  [  976/ 3200]\n",
      "loss: 0.002531  [  992/ 3200]\n",
      "loss: 0.006445  [ 1008/ 3200]\n",
      "loss: 0.005337  [ 1024/ 3200]\n",
      "loss: 0.004617  [ 1040/ 3200]\n",
      "loss: 0.009192  [ 1056/ 3200]\n",
      "loss: 0.003594  [ 1072/ 3200]\n",
      "loss: 0.003997  [ 1088/ 3200]\n",
      "loss: 0.007748  [ 1104/ 3200]\n",
      "loss: 0.002375  [ 1120/ 3200]\n",
      "loss: 0.005379  [ 1136/ 3200]\n",
      "loss: 0.002181  [ 1152/ 3200]\n",
      "loss: 0.005794  [ 1168/ 3200]\n",
      "loss: 0.005524  [ 1184/ 3200]\n",
      "loss: 0.005222  [ 1200/ 3200]\n",
      "loss: 0.000998  [ 1216/ 3200]\n",
      "loss: 0.003827  [ 1232/ 3200]\n",
      "loss: 0.005399  [ 1248/ 3200]\n",
      "loss: 0.003965  [ 1264/ 3200]\n",
      "loss: 0.001387  [ 1280/ 3200]\n",
      "loss: 0.004755  [ 1296/ 3200]\n",
      "loss: 0.003972  [ 1312/ 3200]\n",
      "loss: 0.003540  [ 1328/ 3200]\n",
      "loss: 0.007686  [ 1344/ 3200]\n",
      "loss: 0.005592  [ 1360/ 3200]\n",
      "loss: 0.004807  [ 1376/ 3200]\n",
      "loss: 0.008632  [ 1392/ 3200]\n",
      "loss: 0.006117  [ 1408/ 3200]\n",
      "loss: 0.003981  [ 1424/ 3200]\n",
      "loss: 0.007304  [ 1440/ 3200]\n",
      "loss: 0.009758  [ 1456/ 3200]\n",
      "loss: 0.006313  [ 1472/ 3200]\n",
      "loss: 0.004601  [ 1488/ 3200]\n",
      "loss: 0.002404  [ 1504/ 3200]\n",
      "loss: 0.003158  [ 1520/ 3200]\n",
      "loss: 0.001027  [ 1536/ 3200]\n",
      "loss: 0.008089  [ 1552/ 3200]\n",
      "loss: 0.001321  [ 1568/ 3200]\n",
      "loss: 0.009520  [ 1584/ 3200]\n",
      "loss: 0.007459  [ 1600/ 3200]\n",
      "loss: 0.003288  [ 1616/ 3200]\n",
      "loss: 0.002946  [ 1632/ 3200]\n",
      "loss: 0.002912  [ 1648/ 3200]\n",
      "loss: 0.003686  [ 1664/ 3200]\n",
      "loss: 0.006231  [ 1680/ 3200]\n",
      "loss: 0.005084  [ 1696/ 3200]\n",
      "loss: 0.009733  [ 1712/ 3200]\n",
      "loss: 0.001856  [ 1728/ 3200]\n",
      "loss: 0.004421  [ 1744/ 3200]\n",
      "loss: 0.007989  [ 1760/ 3200]\n",
      "loss: 0.005028  [ 1776/ 3200]\n",
      "loss: 0.004192  [ 1792/ 3200]\n",
      "loss: 0.003888  [ 1808/ 3200]\n",
      "loss: 0.006122  [ 1824/ 3200]\n",
      "loss: 0.004109  [ 1840/ 3200]\n",
      "loss: 0.004632  [ 1856/ 3200]\n",
      "loss: 0.019000  [ 1872/ 3200]\n",
      "loss: 0.001236  [ 1888/ 3200]\n",
      "loss: 0.008193  [ 1904/ 3200]\n",
      "loss: 0.011290  [ 1920/ 3200]\n",
      "loss: 0.004447  [ 1936/ 3200]\n",
      "loss: 0.003503  [ 1952/ 3200]\n",
      "loss: 0.002952  [ 1968/ 3200]\n",
      "loss: 0.002037  [ 1984/ 3200]\n",
      "loss: 0.004807  [ 2000/ 3200]\n",
      "loss: 0.001923  [ 2016/ 3200]\n",
      "loss: 0.002209  [ 2032/ 3200]\n",
      "loss: 0.007446  [ 2048/ 3200]\n",
      "loss: 0.009757  [ 2064/ 3200]\n",
      "loss: 0.008658  [ 2080/ 3200]\n",
      "loss: 0.004997  [ 2096/ 3200]\n",
      "loss: 0.009320  [ 2112/ 3200]\n",
      "loss: 0.002665  [ 2128/ 3200]\n",
      "loss: 0.005307  [ 2144/ 3200]\n",
      "loss: 0.009473  [ 2160/ 3200]\n",
      "loss: 0.016416  [ 2176/ 3200]\n",
      "loss: 0.000858  [ 2192/ 3200]\n",
      "loss: 0.008001  [ 2208/ 3200]\n",
      "loss: 0.006581  [ 2224/ 3200]\n",
      "loss: 0.005818  [ 2240/ 3200]\n",
      "loss: 0.002870  [ 2256/ 3200]\n",
      "loss: 0.002465  [ 2272/ 3200]\n",
      "loss: 0.003823  [ 2288/ 3200]\n",
      "loss: 0.006217  [ 2304/ 3200]\n",
      "loss: 0.004735  [ 2320/ 3200]\n",
      "loss: 0.008869  [ 2336/ 3200]\n",
      "loss: 0.004357  [ 2352/ 3200]\n",
      "loss: 0.002020  [ 2368/ 3200]\n",
      "loss: 0.002131  [ 2384/ 3200]\n",
      "loss: 0.003428  [ 2400/ 3200]\n",
      "loss: 0.003750  [ 2416/ 3200]\n",
      "loss: 0.006017  [ 2432/ 3200]\n",
      "loss: 0.004689  [ 2448/ 3200]\n",
      "loss: 0.003360  [ 2464/ 3200]\n",
      "loss: 0.001955  [ 2480/ 3200]\n",
      "loss: 0.005741  [ 2496/ 3200]\n",
      "loss: 0.008442  [ 2512/ 3200]\n",
      "loss: 0.003660  [ 2528/ 3200]\n",
      "loss: 0.003469  [ 2544/ 3200]\n",
      "loss: 0.004397  [ 2560/ 3200]\n",
      "loss: 0.000375  [ 2576/ 3200]\n",
      "loss: 0.008984  [ 2592/ 3200]\n",
      "loss: 0.001272  [ 2608/ 3200]\n",
      "loss: 0.004028  [ 2624/ 3200]\n",
      "loss: 0.002353  [ 2640/ 3200]\n",
      "loss: 0.008968  [ 2656/ 3200]\n",
      "loss: 0.004159  [ 2672/ 3200]\n",
      "loss: 0.004674  [ 2688/ 3200]\n",
      "loss: 0.006055  [ 2704/ 3200]\n",
      "loss: 0.081139  [ 2720/ 3200]\n",
      "loss: 0.010581  [ 2736/ 3200]\n",
      "loss: 0.003071  [ 2752/ 3200]\n",
      "loss: 0.003962  [ 2768/ 3200]\n",
      "loss: 0.003437  [ 2784/ 3200]\n",
      "loss: 0.009905  [ 2800/ 3200]\n",
      "loss: 0.001196  [ 2816/ 3200]\n",
      "loss: 0.007253  [ 2832/ 3200]\n",
      "loss: 0.009122  [ 2848/ 3200]\n",
      "loss: 0.003093  [ 2864/ 3200]\n",
      "loss: 0.009737  [ 2880/ 3200]\n",
      "loss: 0.006000  [ 2896/ 3200]\n",
      "loss: 0.006114  [ 2912/ 3200]\n",
      "loss: 0.013171  [ 2928/ 3200]\n",
      "loss: 0.258612  [ 2944/ 3200]\n",
      "loss: 0.012814  [ 2960/ 3200]\n",
      "loss: 0.008655  [ 2976/ 3200]\n",
      "loss: 0.007760  [ 2992/ 3200]\n",
      "loss: 0.018583  [ 3008/ 3200]\n",
      "loss: 0.002544  [ 3024/ 3200]\n",
      "loss: 0.006363  [ 3040/ 3200]\n",
      "loss: 0.010247  [ 3056/ 3200]\n",
      "loss: 0.003320  [ 3072/ 3200]\n",
      "loss: 0.005033  [ 3088/ 3200]\n",
      "loss: 0.009798  [ 3104/ 3200]\n",
      "loss: 0.005846  [ 3120/ 3200]\n",
      "loss: 0.007224  [ 3136/ 3200]\n",
      "loss: 0.004511  [ 3152/ 3200]\n",
      "loss: 0.003309  [ 3168/ 3200]\n",
      "loss: 0.007020  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.004140  [    0/ 3200]\n",
      "loss: 0.008358  [   16/ 3200]\n",
      "loss: 0.003540  [   32/ 3200]\n",
      "loss: 0.077646  [   48/ 3200]\n",
      "loss: 0.005004  [   64/ 3200]\n",
      "loss: 0.008612  [   80/ 3200]\n",
      "loss: 0.003529  [   96/ 3200]\n",
      "loss: 0.004406  [  112/ 3200]\n",
      "loss: 0.006000  [  128/ 3200]\n",
      "loss: 0.004768  [  144/ 3200]\n",
      "loss: 0.007866  [  160/ 3200]\n",
      "loss: 0.003930  [  176/ 3200]\n",
      "loss: 0.003766  [  192/ 3200]\n",
      "loss: 0.001548  [  208/ 3200]\n",
      "loss: 0.002610  [  224/ 3200]\n",
      "loss: 0.001868  [  240/ 3200]\n",
      "loss: 0.005674  [  256/ 3200]\n",
      "loss: 0.005210  [  272/ 3200]\n",
      "loss: 0.005544  [  288/ 3200]\n",
      "loss: 0.011287  [  304/ 3200]\n",
      "loss: 0.001123  [  320/ 3200]\n",
      "loss: 0.004498  [  336/ 3200]\n",
      "loss: 0.001385  [  352/ 3200]\n",
      "loss: 0.010059  [  368/ 3200]\n",
      "loss: 0.005455  [  384/ 3200]\n",
      "loss: 0.012952  [  400/ 3200]\n",
      "loss: 0.016466  [  416/ 3200]\n",
      "loss: 0.002959  [  432/ 3200]\n",
      "loss: 0.006511  [  448/ 3200]\n",
      "loss: 0.005209  [  464/ 3200]\n",
      "loss: 0.004604  [  480/ 3200]\n",
      "loss: 0.004513  [  496/ 3200]\n",
      "loss: 0.002763  [  512/ 3200]\n",
      "loss: 0.004675  [  528/ 3200]\n",
      "loss: 0.002649  [  544/ 3200]\n",
      "loss: 0.003592  [  560/ 3200]\n",
      "loss: 0.004419  [  576/ 3200]\n",
      "loss: 0.001582  [  592/ 3200]\n",
      "loss: 0.002468  [  608/ 3200]\n",
      "loss: 0.001131  [  624/ 3200]\n",
      "loss: 0.007039  [  640/ 3200]\n",
      "loss: 0.003237  [  656/ 3200]\n",
      "loss: 0.001863  [  672/ 3200]\n",
      "loss: 0.003804  [  688/ 3200]\n",
      "loss: 0.004843  [  704/ 3200]\n",
      "loss: 0.006099  [  720/ 3200]\n",
      "loss: 0.004648  [  736/ 3200]\n",
      "loss: 0.002014  [  752/ 3200]\n",
      "loss: 0.002433  [  768/ 3200]\n",
      "loss: 0.002517  [  784/ 3200]\n",
      "loss: 0.003074  [  800/ 3200]\n",
      "loss: 0.008059  [  816/ 3200]\n",
      "loss: 0.004425  [  832/ 3200]\n",
      "loss: 0.002703  [  848/ 3200]\n",
      "loss: 0.005396  [  864/ 3200]\n",
      "loss: 0.005212  [  880/ 3200]\n",
      "loss: 0.003671  [  896/ 3200]\n",
      "loss: 0.004748  [  912/ 3200]\n",
      "loss: 0.003137  [  928/ 3200]\n",
      "loss: 0.019017  [  944/ 3200]\n",
      "loss: 0.005958  [  960/ 3200]\n",
      "loss: 0.008454  [  976/ 3200]\n",
      "loss: 0.001612  [  992/ 3200]\n",
      "loss: 0.007746  [ 1008/ 3200]\n",
      "loss: 0.003525  [ 1024/ 3200]\n",
      "loss: 0.008995  [ 1040/ 3200]\n",
      "loss: 0.005504  [ 1056/ 3200]\n",
      "loss: 0.003757  [ 1072/ 3200]\n",
      "loss: 0.004159  [ 1088/ 3200]\n",
      "loss: 0.002637  [ 1104/ 3200]\n",
      "loss: 0.004042  [ 1120/ 3200]\n",
      "loss: 0.003531  [ 1136/ 3200]\n",
      "loss: 0.005654  [ 1152/ 3200]\n",
      "loss: 0.004231  [ 1168/ 3200]\n",
      "loss: 0.004684  [ 1184/ 3200]\n",
      "loss: 0.004335  [ 1200/ 3200]\n",
      "loss: 0.002405  [ 1216/ 3200]\n",
      "loss: 0.012751  [ 1232/ 3200]\n",
      "loss: 0.003233  [ 1248/ 3200]\n",
      "loss: 0.006175  [ 1264/ 3200]\n",
      "loss: 0.005968  [ 1280/ 3200]\n",
      "loss: 0.006247  [ 1296/ 3200]\n",
      "loss: 0.003265  [ 1312/ 3200]\n",
      "loss: 0.004757  [ 1328/ 3200]\n",
      "loss: 0.009836  [ 1344/ 3200]\n",
      "loss: 0.002533  [ 1360/ 3200]\n",
      "loss: 0.003486  [ 1376/ 3200]\n",
      "loss: 0.004080  [ 1392/ 3200]\n",
      "loss: 0.004879  [ 1408/ 3200]\n",
      "loss: 0.003156  [ 1424/ 3200]\n",
      "loss: 0.003181  [ 1440/ 3200]\n",
      "loss: 0.002805  [ 1456/ 3200]\n",
      "loss: 0.001328  [ 1472/ 3200]\n",
      "loss: 0.001249  [ 1488/ 3200]\n",
      "loss: 0.004444  [ 1504/ 3200]\n",
      "loss: 0.004313  [ 1520/ 3200]\n",
      "loss: 0.004065  [ 1536/ 3200]\n",
      "loss: 0.006171  [ 1552/ 3200]\n",
      "loss: 0.008173  [ 1568/ 3200]\n",
      "loss: 0.001831  [ 1584/ 3200]\n",
      "loss: 0.005518  [ 1600/ 3200]\n",
      "loss: 0.005958  [ 1616/ 3200]\n",
      "loss: 0.002787  [ 1632/ 3200]\n",
      "loss: 0.007415  [ 1648/ 3200]\n",
      "loss: 0.001957  [ 1664/ 3200]\n",
      "loss: 0.003737  [ 1680/ 3200]\n",
      "loss: 0.237201  [ 1696/ 3200]\n",
      "loss: 0.066676  [ 1712/ 3200]\n",
      "loss: 0.007963  [ 1728/ 3200]\n",
      "loss: 0.003107  [ 1744/ 3200]\n",
      "loss: 0.015419  [ 1760/ 3200]\n",
      "loss: 0.004869  [ 1776/ 3200]\n",
      "loss: 0.006188  [ 1792/ 3200]\n",
      "loss: 0.002243  [ 1808/ 3200]\n",
      "loss: 0.013839  [ 1824/ 3200]\n",
      "loss: 0.011776  [ 1840/ 3200]\n",
      "loss: 0.001649  [ 1856/ 3200]\n",
      "loss: 0.013784  [ 1872/ 3200]\n",
      "loss: 0.002669  [ 1888/ 3200]\n",
      "loss: 0.010150  [ 1904/ 3200]\n",
      "loss: 0.003238  [ 1920/ 3200]\n",
      "loss: 0.003862  [ 1936/ 3200]\n",
      "loss: 0.005951  [ 1952/ 3200]\n",
      "loss: 0.003488  [ 1968/ 3200]\n",
      "loss: 0.007596  [ 1984/ 3200]\n",
      "loss: 0.015782  [ 2000/ 3200]\n",
      "loss: 0.006851  [ 2016/ 3200]\n",
      "loss: 0.008294  [ 2032/ 3200]\n",
      "loss: 0.004399  [ 2048/ 3200]\n",
      "loss: 0.002874  [ 2064/ 3200]\n",
      "loss: 0.006300  [ 2080/ 3200]\n",
      "loss: 0.000673  [ 2096/ 3200]\n",
      "loss: 0.003183  [ 2112/ 3200]\n",
      "loss: 0.005381  [ 2128/ 3200]\n",
      "loss: 0.004622  [ 2144/ 3200]\n",
      "loss: 0.004853  [ 2160/ 3200]\n",
      "loss: 0.006674  [ 2176/ 3200]\n",
      "loss: 0.006216  [ 2192/ 3200]\n",
      "loss: 0.005455  [ 2208/ 3200]\n",
      "loss: 0.003125  [ 2224/ 3200]\n",
      "loss: 0.003644  [ 2240/ 3200]\n",
      "loss: 0.005995  [ 2256/ 3200]\n",
      "loss: 0.006799  [ 2272/ 3200]\n",
      "loss: 0.004735  [ 2288/ 3200]\n",
      "loss: 0.004438  [ 2304/ 3200]\n",
      "loss: 0.007532  [ 2320/ 3200]\n",
      "loss: 0.003685  [ 2336/ 3200]\n",
      "loss: 0.002471  [ 2352/ 3200]\n",
      "loss: 0.013219  [ 2368/ 3200]\n",
      "loss: 0.005843  [ 2384/ 3200]\n",
      "loss: 0.003137  [ 2400/ 3200]\n",
      "loss: 0.000709  [ 2416/ 3200]\n",
      "loss: 0.004587  [ 2432/ 3200]\n",
      "loss: 0.001512  [ 2448/ 3200]\n",
      "loss: 0.003218  [ 2464/ 3200]\n",
      "loss: 0.004854  [ 2480/ 3200]\n",
      "loss: 0.004811  [ 2496/ 3200]\n",
      "loss: 0.004785  [ 2512/ 3200]\n",
      "loss: 0.004817  [ 2528/ 3200]\n",
      "loss: 0.003803  [ 2544/ 3200]\n",
      "loss: 0.003393  [ 2560/ 3200]\n",
      "loss: 0.006928  [ 2576/ 3200]\n",
      "loss: 0.005244  [ 2592/ 3200]\n",
      "loss: 0.007926  [ 2608/ 3200]\n",
      "loss: 0.008626  [ 2624/ 3200]\n",
      "loss: 0.003777  [ 2640/ 3200]\n",
      "loss: 0.003043  [ 2656/ 3200]\n",
      "loss: 0.005805  [ 2672/ 3200]\n",
      "loss: 0.006559  [ 2688/ 3200]\n",
      "loss: 0.004021  [ 2704/ 3200]\n",
      "loss: 0.003260  [ 2720/ 3200]\n",
      "loss: 0.003884  [ 2736/ 3200]\n",
      "loss: 0.003653  [ 2752/ 3200]\n",
      "loss: 0.011863  [ 2768/ 3200]\n",
      "loss: 0.012211  [ 2784/ 3200]\n",
      "loss: 0.003161  [ 2800/ 3200]\n",
      "loss: 0.005443  [ 2816/ 3200]\n",
      "loss: 0.005313  [ 2832/ 3200]\n",
      "loss: 0.005095  [ 2848/ 3200]\n",
      "loss: 0.002191  [ 2864/ 3200]\n",
      "loss: 0.002800  [ 2880/ 3200]\n",
      "loss: 0.008465  [ 2896/ 3200]\n",
      "loss: 0.006125  [ 2912/ 3200]\n",
      "loss: 0.001493  [ 2928/ 3200]\n",
      "loss: 0.002019  [ 2944/ 3200]\n",
      "loss: 0.007039  [ 2960/ 3200]\n",
      "loss: 0.002664  [ 2976/ 3200]\n",
      "loss: 0.003664  [ 2992/ 3200]\n",
      "loss: 0.003198  [ 3008/ 3200]\n",
      "loss: 0.001605  [ 3024/ 3200]\n",
      "loss: 0.004231  [ 3040/ 3200]\n",
      "loss: 0.005394  [ 3056/ 3200]\n",
      "loss: 0.006713  [ 3072/ 3200]\n",
      "loss: 0.001966  [ 3088/ 3200]\n",
      "loss: 0.005028  [ 3104/ 3200]\n",
      "loss: 0.000901  [ 3120/ 3200]\n",
      "loss: 0.003150  [ 3136/ 3200]\n",
      "loss: 0.007949  [ 3152/ 3200]\n",
      "loss: 0.002496  [ 3168/ 3200]\n",
      "loss: 0.003283  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 0.002798  [    0/ 3200]\n",
      "loss: 0.002440  [   16/ 3200]\n",
      "loss: 0.003139  [   32/ 3200]\n",
      "loss: 0.004465  [   48/ 3200]\n",
      "loss: 0.005819  [   64/ 3200]\n",
      "loss: 0.001302  [   80/ 3200]\n",
      "loss: 0.006474  [   96/ 3200]\n",
      "loss: 0.004451  [  112/ 3200]\n",
      "loss: 0.003189  [  128/ 3200]\n",
      "loss: 0.005570  [  144/ 3200]\n",
      "loss: 0.005115  [  160/ 3200]\n",
      "loss: 0.004691  [  176/ 3200]\n",
      "loss: 0.006919  [  192/ 3200]\n",
      "loss: 0.003390  [  208/ 3200]\n",
      "loss: 0.001230  [  224/ 3200]\n",
      "loss: 0.003251  [  240/ 3200]\n",
      "loss: 0.007683  [  256/ 3200]\n",
      "loss: 0.002304  [  272/ 3200]\n",
      "loss: 0.010883  [  288/ 3200]\n",
      "loss: 0.005410  [  304/ 3200]\n",
      "loss: 0.002551  [  320/ 3200]\n",
      "loss: 0.004174  [  336/ 3200]\n",
      "loss: 0.002468  [  352/ 3200]\n",
      "loss: 0.005663  [  368/ 3200]\n",
      "loss: 0.001144  [  384/ 3200]\n",
      "loss: 0.006432  [  400/ 3200]\n",
      "loss: 0.009147  [  416/ 3200]\n",
      "loss: 0.008297  [  432/ 3200]\n",
      "loss: 0.002299  [  448/ 3200]\n",
      "loss: 0.001344  [  464/ 3200]\n",
      "loss: 0.002090  [  480/ 3200]\n",
      "loss: 0.074523  [  496/ 3200]\n",
      "loss: 0.002110  [  512/ 3200]\n",
      "loss: 0.008878  [  528/ 3200]\n",
      "loss: 0.002660  [  544/ 3200]\n",
      "loss: 0.004937  [  560/ 3200]\n",
      "loss: 0.003792  [  576/ 3200]\n",
      "loss: 0.002568  [  592/ 3200]\n",
      "loss: 0.002694  [  608/ 3200]\n",
      "loss: 0.002427  [  624/ 3200]\n",
      "loss: 0.007089  [  640/ 3200]\n",
      "loss: 0.007059  [  656/ 3200]\n",
      "loss: 0.005462  [  672/ 3200]\n",
      "loss: 0.006075  [  688/ 3200]\n",
      "loss: 0.004128  [  704/ 3200]\n",
      "loss: 0.003578  [  720/ 3200]\n",
      "loss: 0.004073  [  736/ 3200]\n",
      "loss: 0.003959  [  752/ 3200]\n",
      "loss: 0.002068  [  768/ 3200]\n",
      "loss: 0.002627  [  784/ 3200]\n",
      "loss: 0.001337  [  800/ 3200]\n",
      "loss: 0.003553  [  816/ 3200]\n",
      "loss: 0.006832  [  832/ 3200]\n",
      "loss: 0.005173  [  848/ 3200]\n",
      "loss: 0.001471  [  864/ 3200]\n",
      "loss: 0.003030  [  880/ 3200]\n",
      "loss: 0.005554  [  896/ 3200]\n",
      "loss: 0.003545  [  912/ 3200]\n",
      "loss: 0.002087  [  928/ 3200]\n",
      "loss: 0.001795  [  944/ 3200]\n",
      "loss: 0.004132  [  960/ 3200]\n",
      "loss: 0.005426  [  976/ 3200]\n",
      "loss: 0.005238  [  992/ 3200]\n",
      "loss: 0.002569  [ 1008/ 3200]\n",
      "loss: 0.003753  [ 1024/ 3200]\n",
      "loss: 0.002582  [ 1040/ 3200]\n",
      "loss: 0.007132  [ 1056/ 3200]\n",
      "loss: 0.002869  [ 1072/ 3200]\n",
      "loss: 0.009474  [ 1088/ 3200]\n",
      "loss: 0.002600  [ 1104/ 3200]\n",
      "loss: 0.002780  [ 1120/ 3200]\n",
      "loss: 0.004340  [ 1136/ 3200]\n",
      "loss: 0.002635  [ 1152/ 3200]\n",
      "loss: 0.009377  [ 1168/ 3200]\n",
      "loss: 0.002364  [ 1184/ 3200]\n",
      "loss: 0.006833  [ 1200/ 3200]\n",
      "loss: 0.007195  [ 1216/ 3200]\n",
      "loss: 0.002684  [ 1232/ 3200]\n",
      "loss: 0.003073  [ 1248/ 3200]\n",
      "loss: 0.000392  [ 1264/ 3200]\n",
      "loss: 0.003921  [ 1280/ 3200]\n",
      "loss: 0.002115  [ 1296/ 3200]\n",
      "loss: 0.005043  [ 1312/ 3200]\n",
      "loss: 0.003112  [ 1328/ 3200]\n",
      "loss: 0.001902  [ 1344/ 3200]\n",
      "loss: 0.000625  [ 1360/ 3200]\n",
      "loss: 0.000685  [ 1376/ 3200]\n",
      "loss: 0.004845  [ 1392/ 3200]\n",
      "loss: 0.005575  [ 1408/ 3200]\n",
      "loss: 0.002627  [ 1424/ 3200]\n",
      "loss: 0.003840  [ 1440/ 3200]\n",
      "loss: 0.009697  [ 1456/ 3200]\n",
      "loss: 0.004790  [ 1472/ 3200]\n",
      "loss: 0.003719  [ 1488/ 3200]\n",
      "loss: 0.004781  [ 1504/ 3200]\n",
      "loss: 0.003497  [ 1520/ 3200]\n",
      "loss: 0.005797  [ 1536/ 3200]\n",
      "loss: 0.009033  [ 1552/ 3200]\n",
      "loss: 0.004210  [ 1568/ 3200]\n",
      "loss: 0.004359  [ 1584/ 3200]\n",
      "loss: 0.003735  [ 1600/ 3200]\n",
      "loss: 0.002036  [ 1616/ 3200]\n",
      "loss: 0.001873  [ 1632/ 3200]\n",
      "loss: 0.004920  [ 1648/ 3200]\n",
      "loss: 0.007095  [ 1664/ 3200]\n",
      "loss: 0.005189  [ 1680/ 3200]\n",
      "loss: 0.002782  [ 1696/ 3200]\n",
      "loss: 0.003163  [ 1712/ 3200]\n",
      "loss: 0.002154  [ 1728/ 3200]\n",
      "loss: 0.003196  [ 1744/ 3200]\n",
      "loss: 0.003005  [ 1760/ 3200]\n",
      "loss: 0.003642  [ 1776/ 3200]\n",
      "loss: 0.004066  [ 1792/ 3200]\n",
      "loss: 0.001682  [ 1808/ 3200]\n",
      "loss: 0.006534  [ 1824/ 3200]\n",
      "loss: 0.004549  [ 1840/ 3200]\n",
      "loss: 0.002031  [ 1856/ 3200]\n",
      "loss: 0.002160  [ 1872/ 3200]\n",
      "loss: 0.237832  [ 1888/ 3200]\n",
      "loss: 0.163675  [ 1904/ 3200]\n",
      "loss: 0.297732  [ 1920/ 3200]\n",
      "loss: 0.156618  [ 1936/ 3200]\n",
      "loss: 0.058449  [ 1952/ 3200]\n",
      "loss: 0.008081  [ 1968/ 3200]\n",
      "loss: 0.006574  [ 1984/ 3200]\n",
      "loss: 0.003555  [ 2000/ 3200]\n",
      "loss: 0.001864  [ 2016/ 3200]\n",
      "loss: 0.004784  [ 2032/ 3200]\n",
      "loss: 0.008253  [ 2048/ 3200]\n",
      "loss: 0.003816  [ 2064/ 3200]\n",
      "loss: 0.016028  [ 2080/ 3200]\n",
      "loss: 0.021358  [ 2096/ 3200]\n",
      "loss: 0.002822  [ 2112/ 3200]\n",
      "loss: 0.006154  [ 2128/ 3200]\n",
      "loss: 0.004096  [ 2144/ 3200]\n",
      "loss: 0.004536  [ 2160/ 3200]\n",
      "loss: 0.008527  [ 2176/ 3200]\n",
      "loss: 0.013093  [ 2192/ 3200]\n",
      "loss: 0.000940  [ 2208/ 3200]\n",
      "loss: 0.019802  [ 2224/ 3200]\n",
      "loss: 0.013799  [ 2240/ 3200]\n",
      "loss: 0.005137  [ 2256/ 3200]\n",
      "loss: 0.010246  [ 2272/ 3200]\n",
      "loss: 0.003094  [ 2288/ 3200]\n",
      "loss: 0.008088  [ 2304/ 3200]\n",
      "loss: 0.011406  [ 2320/ 3200]\n",
      "loss: 0.004092  [ 2336/ 3200]\n",
      "loss: 0.008924  [ 2352/ 3200]\n",
      "loss: 0.003017  [ 2368/ 3200]\n",
      "loss: 0.004229  [ 2384/ 3200]\n",
      "loss: 0.007950  [ 2400/ 3200]\n",
      "loss: 0.003011  [ 2416/ 3200]\n",
      "loss: 0.037906  [ 2432/ 3200]\n",
      "loss: 0.030736  [ 2448/ 3200]\n",
      "loss: 0.005861  [ 2464/ 3200]\n",
      "loss: 0.004513  [ 2480/ 3200]\n",
      "loss: 0.002017  [ 2496/ 3200]\n",
      "loss: 0.002237  [ 2512/ 3200]\n",
      "loss: 0.004070  [ 2528/ 3200]\n",
      "loss: 0.002668  [ 2544/ 3200]\n",
      "loss: 0.005706  [ 2560/ 3200]\n",
      "loss: 0.002620  [ 2576/ 3200]\n",
      "loss: 0.003247  [ 2592/ 3200]\n",
      "loss: 0.005631  [ 2608/ 3200]\n",
      "loss: 0.004168  [ 2624/ 3200]\n",
      "loss: 0.006879  [ 2640/ 3200]\n",
      "loss: 0.003578  [ 2656/ 3200]\n",
      "loss: 0.004234  [ 2672/ 3200]\n",
      "loss: 0.002164  [ 2688/ 3200]\n",
      "loss: 0.010385  [ 2704/ 3200]\n",
      "loss: 0.001078  [ 2720/ 3200]\n",
      "loss: 0.004232  [ 2736/ 3200]\n",
      "loss: 0.002878  [ 2752/ 3200]\n",
      "loss: 0.002298  [ 2768/ 3200]\n",
      "loss: 0.013263  [ 2784/ 3200]\n",
      "loss: 0.003236  [ 2800/ 3200]\n",
      "loss: 0.004595  [ 2816/ 3200]\n",
      "loss: 0.003230  [ 2832/ 3200]\n",
      "loss: 0.007310  [ 2848/ 3200]\n",
      "loss: 0.006194  [ 2864/ 3200]\n",
      "loss: 0.007543  [ 2880/ 3200]\n",
      "loss: 0.009250  [ 2896/ 3200]\n",
      "loss: 0.001733  [ 2912/ 3200]\n",
      "loss: 0.008149  [ 2928/ 3200]\n",
      "loss: 0.018439  [ 2944/ 3200]\n",
      "loss: 0.005304  [ 2960/ 3200]\n",
      "loss: 0.006254  [ 2976/ 3200]\n",
      "loss: 0.003810  [ 2992/ 3200]\n",
      "loss: 0.007383  [ 3008/ 3200]\n",
      "loss: 0.004503  [ 3024/ 3200]\n",
      "loss: 0.015892  [ 3040/ 3200]\n",
      "loss: 0.001944  [ 3056/ 3200]\n",
      "loss: 0.008631  [ 3072/ 3200]\n",
      "loss: 0.001368  [ 3088/ 3200]\n",
      "loss: 0.010273  [ 3104/ 3200]\n",
      "loss: 0.006121  [ 3120/ 3200]\n",
      "loss: 0.015392  [ 3136/ 3200]\n",
      "loss: 0.007343  [ 3152/ 3200]\n",
      "loss: 0.003250  [ 3168/ 3200]\n",
      "loss: 0.009236  [ 3184/ 3200]\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.001079  [    0/ 3200]\n",
      "loss: 0.001731  [   16/ 3200]\n",
      "loss: 0.001262  [   32/ 3200]\n",
      "loss: 0.017822  [   48/ 3200]\n",
      "loss: 0.004411  [   64/ 3200]\n",
      "loss: 0.011058  [   80/ 3200]\n",
      "loss: 0.009646  [   96/ 3200]\n",
      "loss: 0.007326  [  112/ 3200]\n",
      "loss: 0.003378  [  128/ 3200]\n",
      "loss: 0.004731  [  144/ 3200]\n",
      "loss: 0.001308  [  160/ 3200]\n",
      "loss: 0.009071  [  176/ 3200]\n",
      "loss: 0.003275  [  192/ 3200]\n",
      "loss: 0.191493  [  208/ 3200]\n",
      "loss: 0.072810  [  224/ 3200]\n",
      "loss: 0.014928  [  240/ 3200]\n",
      "loss: 0.000890  [  256/ 3200]\n",
      "loss: 0.002938  [  272/ 3200]\n",
      "loss: 0.003532  [  288/ 3200]\n",
      "loss: 0.005167  [  304/ 3200]\n",
      "loss: 0.005853  [  320/ 3200]\n",
      "loss: 0.017821  [  336/ 3200]\n",
      "loss: 0.009263  [  352/ 3200]\n",
      "loss: 0.008828  [  368/ 3200]\n",
      "loss: 0.013254  [  384/ 3200]\n",
      "loss: 0.006871  [  400/ 3200]\n",
      "loss: 0.013721  [  416/ 3200]\n",
      "loss: 0.004321  [  432/ 3200]\n",
      "loss: 0.004092  [  448/ 3200]\n",
      "loss: 0.001889  [  464/ 3200]\n",
      "loss: 0.009735  [  480/ 3200]\n",
      "loss: 0.004865  [  496/ 3200]\n",
      "loss: 0.011121  [  512/ 3200]\n",
      "loss: 0.002543  [  528/ 3200]\n",
      "loss: 0.011344  [  544/ 3200]\n",
      "loss: 0.007039  [  560/ 3200]\n",
      "loss: 0.004390  [  576/ 3200]\n",
      "loss: 0.001460  [  592/ 3200]\n",
      "loss: 0.004447  [  608/ 3200]\n",
      "loss: 0.008004  [  624/ 3200]\n",
      "loss: 0.015496  [  640/ 3200]\n",
      "loss: 0.003377  [  656/ 3200]\n",
      "loss: 0.010401  [  672/ 3200]\n",
      "loss: 0.016706  [  688/ 3200]\n",
      "loss: 0.003963  [  704/ 3200]\n",
      "loss: 0.006834  [  720/ 3200]\n",
      "loss: 0.003334  [  736/ 3200]\n",
      "loss: 0.001578  [  752/ 3200]\n",
      "loss: 0.003202  [  768/ 3200]\n",
      "loss: 0.004604  [  784/ 3200]\n",
      "loss: 0.003123  [  800/ 3200]\n",
      "loss: 0.002640  [  816/ 3200]\n",
      "loss: 0.005548  [  832/ 3200]\n",
      "loss: 0.003791  [  848/ 3200]\n",
      "loss: 0.001981  [  864/ 3200]\n",
      "loss: 0.007635  [  880/ 3200]\n",
      "loss: 0.000836  [  896/ 3200]\n",
      "loss: 0.004843  [  912/ 3200]\n",
      "loss: 0.004472  [  928/ 3200]\n",
      "loss: 0.005319  [  944/ 3200]\n",
      "loss: 0.009511  [  960/ 3200]\n",
      "loss: 0.009437  [  976/ 3200]\n",
      "loss: 0.004961  [  992/ 3200]\n",
      "loss: 0.004772  [ 1008/ 3200]\n",
      "loss: 0.002955  [ 1024/ 3200]\n",
      "loss: 0.003784  [ 1040/ 3200]\n",
      "loss: 0.002627  [ 1056/ 3200]\n",
      "loss: 0.003959  [ 1072/ 3200]\n",
      "loss: 0.001835  [ 1088/ 3200]\n",
      "loss: 0.002168  [ 1104/ 3200]\n",
      "loss: 0.006198  [ 1120/ 3200]\n",
      "loss: 0.004974  [ 1136/ 3200]\n",
      "loss: 0.003199  [ 1152/ 3200]\n",
      "loss: 0.003772  [ 1168/ 3200]\n",
      "loss: 0.002265  [ 1184/ 3200]\n",
      "loss: 0.000751  [ 1200/ 3200]\n",
      "loss: 0.005736  [ 1216/ 3200]\n",
      "loss: 0.010719  [ 1232/ 3200]\n",
      "loss: 0.002097  [ 1248/ 3200]\n",
      "loss: 0.009762  [ 1264/ 3200]\n",
      "loss: 0.008436  [ 1280/ 3200]\n",
      "loss: 0.004481  [ 1296/ 3200]\n",
      "loss: 0.002913  [ 1312/ 3200]\n",
      "loss: 0.005337  [ 1328/ 3200]\n",
      "loss: 0.003049  [ 1344/ 3200]\n",
      "loss: 0.003865  [ 1360/ 3200]\n",
      "loss: 0.004063  [ 1376/ 3200]\n",
      "loss: 0.003717  [ 1392/ 3200]\n",
      "loss: 0.006076  [ 1408/ 3200]\n",
      "loss: 0.004350  [ 1424/ 3200]\n",
      "loss: 0.003894  [ 1440/ 3200]\n",
      "loss: 0.006931  [ 1456/ 3200]\n",
      "loss: 0.002575  [ 1472/ 3200]\n",
      "loss: 0.002768  [ 1488/ 3200]\n",
      "loss: 0.009603  [ 1504/ 3200]\n",
      "loss: 0.001064  [ 1520/ 3200]\n",
      "loss: 0.003150  [ 1536/ 3200]\n",
      "loss: 0.002007  [ 1552/ 3200]\n",
      "loss: 0.004255  [ 1568/ 3200]\n",
      "loss: 0.003595  [ 1584/ 3200]\n",
      "loss: 0.007748  [ 1600/ 3200]\n",
      "loss: 0.002576  [ 1616/ 3200]\n",
      "loss: 0.006571  [ 1632/ 3200]\n",
      "loss: 0.004115  [ 1648/ 3200]\n",
      "loss: 0.004498  [ 1664/ 3200]\n",
      "loss: 0.007055  [ 1680/ 3200]\n",
      "loss: 0.004363  [ 1696/ 3200]\n",
      "loss: 0.015575  [ 1712/ 3200]\n",
      "loss: 0.002892  [ 1728/ 3200]\n",
      "loss: 0.003003  [ 1744/ 3200]\n",
      "loss: 0.004907  [ 1760/ 3200]\n",
      "loss: 0.010702  [ 1776/ 3200]\n",
      "loss: 0.004263  [ 1792/ 3200]\n",
      "loss: 0.003147  [ 1808/ 3200]\n",
      "loss: 0.004872  [ 1824/ 3200]\n",
      "loss: 0.001502  [ 1840/ 3200]\n",
      "loss: 0.005045  [ 1856/ 3200]\n",
      "loss: 0.005830  [ 1872/ 3200]\n",
      "loss: 0.001175  [ 1888/ 3200]\n",
      "loss: 0.004190  [ 1904/ 3200]\n",
      "loss: 0.004427  [ 1920/ 3200]\n",
      "loss: 0.004071  [ 1936/ 3200]\n",
      "loss: 0.002226  [ 1952/ 3200]\n",
      "loss: 0.002963  [ 1968/ 3200]\n",
      "loss: 0.005347  [ 1984/ 3200]\n",
      "loss: 0.002352  [ 2000/ 3200]\n",
      "loss: 0.002321  [ 2016/ 3200]\n",
      "loss: 0.002493  [ 2032/ 3200]\n",
      "loss: 0.001452  [ 2048/ 3200]\n",
      "loss: 0.001309  [ 2064/ 3200]\n",
      "loss: 0.003367  [ 2080/ 3200]\n",
      "loss: 0.007060  [ 2096/ 3200]\n",
      "loss: 0.002516  [ 2112/ 3200]\n",
      "loss: 0.001917  [ 2128/ 3200]\n",
      "loss: 0.003066  [ 2144/ 3200]\n",
      "loss: 0.006685  [ 2160/ 3200]\n",
      "loss: 0.004453  [ 2176/ 3200]\n",
      "loss: 0.003834  [ 2192/ 3200]\n",
      "loss: 0.002942  [ 2208/ 3200]\n",
      "loss: 0.001863  [ 2224/ 3200]\n",
      "loss: 0.001823  [ 2240/ 3200]\n",
      "loss: 0.005614  [ 2256/ 3200]\n",
      "loss: 0.008417  [ 2272/ 3200]\n",
      "loss: 0.008888  [ 2288/ 3200]\n",
      "loss: 0.003922  [ 2304/ 3200]\n",
      "loss: 0.003726  [ 2320/ 3200]\n",
      "loss: 0.003983  [ 2336/ 3200]\n",
      "loss: 0.002985  [ 2352/ 3200]\n",
      "loss: 0.000619  [ 2368/ 3200]\n",
      "loss: 0.001773  [ 2384/ 3200]\n",
      "loss: 0.006245  [ 2400/ 3200]\n",
      "loss: 0.008188  [ 2416/ 3200]\n",
      "loss: 0.006366  [ 2432/ 3200]\n",
      "loss: 0.003830  [ 2448/ 3200]\n",
      "loss: 0.006928  [ 2464/ 3200]\n",
      "loss: 0.001347  [ 2480/ 3200]\n",
      "loss: 0.073384  [ 2496/ 3200]\n",
      "loss: 0.008702  [ 2512/ 3200]\n",
      "loss: 0.008005  [ 2528/ 3200]\n",
      "loss: 0.006450  [ 2544/ 3200]\n",
      "loss: 0.006181  [ 2560/ 3200]\n",
      "loss: 0.003635  [ 2576/ 3200]\n",
      "loss: 0.015740  [ 2592/ 3200]\n",
      "loss: 0.003849  [ 2608/ 3200]\n",
      "loss: 0.004703  [ 2624/ 3200]\n",
      "loss: 0.001820  [ 2640/ 3200]\n",
      "loss: 0.001744  [ 2656/ 3200]\n",
      "loss: 0.001632  [ 2672/ 3200]\n",
      "loss: 0.012473  [ 2688/ 3200]\n",
      "loss: 0.001519  [ 2704/ 3200]\n",
      "loss: 0.003792  [ 2720/ 3200]\n",
      "loss: 0.001497  [ 2736/ 3200]\n",
      "loss: 0.008517  [ 2752/ 3200]\n",
      "loss: 0.007781  [ 2768/ 3200]\n",
      "loss: 0.001349  [ 2784/ 3200]\n",
      "loss: 0.007234  [ 2800/ 3200]\n",
      "loss: 0.002549  [ 2816/ 3200]\n",
      "loss: 0.003907  [ 2832/ 3200]\n",
      "loss: 0.003306  [ 2848/ 3200]\n",
      "loss: 0.003057  [ 2864/ 3200]\n",
      "loss: 0.010526  [ 2880/ 3200]\n",
      "loss: 0.003505  [ 2896/ 3200]\n",
      "loss: 0.001648  [ 2912/ 3200]\n",
      "loss: 0.003696  [ 2928/ 3200]\n",
      "loss: 0.004775  [ 2944/ 3200]\n",
      "loss: 0.002146  [ 2960/ 3200]\n",
      "loss: 0.004241  [ 2976/ 3200]\n",
      "loss: 0.008649  [ 2992/ 3200]\n",
      "loss: 0.002626  [ 3008/ 3200]\n",
      "loss: 0.003317  [ 3024/ 3200]\n",
      "loss: 0.005602  [ 3040/ 3200]\n",
      "loss: 0.009510  [ 3056/ 3200]\n",
      "loss: 0.001224  [ 3072/ 3200]\n",
      "loss: 0.002920  [ 3088/ 3200]\n",
      "loss: 0.001465  [ 3104/ 3200]\n",
      "loss: 0.004150  [ 3120/ 3200]\n",
      "loss: 0.002414  [ 3136/ 3200]\n",
      "loss: 0.005896  [ 3152/ 3200]\n",
      "loss: 0.001631  [ 3168/ 3200]\n",
      "loss: 0.000802  [ 3184/ 3200]\n",
      "\n",
      "Test Error:\n",
      "Avg loss               : 0.089290\n",
      "f1 macro averaged score: 0.742370\n",
      "Accuracy               : 73.5%\n",
      "Confusion matrix       :\n",
      "tensor([[269,  18,   3,   7],\n",
      "        [ 14, 205,  26,  79],\n",
      "        [  4,  39, 274,  39],\n",
      "        [ 16,  89,  30, 264]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch_seed(0)\n",
    "cnn_model = Net().to(device)\n",
    "\n",
    "learning_rate = 0.002\n",
    "optimizer = Adagrad(params=cnn_model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "cnn_model = train_convolutional_neural_network(epochs, optimizer, train_dataloader, loss_function, cnn_model, True)\n",
    "results = test_convolutional_neural_network(test_dataloader, loss_function, cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_XxBAeMIN-G"
   },
   "source": [
    "We can verify that the two models have the same loss for each pair of batch and epoch during training and yield the exact same results during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D1yoXgZV6bu"
   },
   "source": [
    "### Step 2 - Activation functions\n",
    "\n",
    "Redefine the Convolutional Neural Network, so that the activation function can be given as an argument and be used in <code>forward()</code> instead of ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0R9qou1Cntoi"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, activation_function):\n",
    "    super().__init__()\n",
    "    # convolutional layers\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, padding=2)   # 1 channel   -> 16 channels\n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2)  # 16 channels -> 32 channels\n",
    "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)  # 32 channels -> 64 channels\n",
    "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2) # 64 channels -> 128 channels\n",
    "\n",
    "    self.activation_function = activation_function()\n",
    "\n",
    "    # fully connected (dense) layers\n",
    "    self.dense1 = nn.Linear(1024, 1024) # input size = 1024 -> number of perceptrons in 1st hidden layer = 1024\n",
    "    self.dense2 = nn.Linear(1024, 256)  # number of perceptrons in 1st hidden layer = 1024 ->\n",
    "                                        # number of perceptrons in 2nd hidden layer = 256\n",
    "    self.dense3 = nn.Linear(256, 32)    # number of perceptrons in 2nd hidden layer = 256 ->\n",
    "                                        # number of perceptrons in 3rd hidden layer = 32\n",
    "    self.dense4 = nn.Linear(32, 4)      # number of perceptrons in 3rd hidden layer = 32 ->\n",
    "                                        # number of different labels = 4\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv2(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv3(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv4(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    x = torch.flatten(x, 1)\n",
    "\n",
    "    x = self.dense1(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = self.dense2(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = self.dense3(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = self.dense4(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgTTa3x_sDTy"
   },
   "source": [
    "We validate our Convolutional Neural Network for $30$ epochs and test it.\n",
    "\n",
    "Our model uses the Adagrad optimizer, as stated in the previous steps.\n",
    "\n",
    "The activation functions used are the following: ELU, Hardshrink, Hardsigmoid, Hardtanh, Hardswish, LeakyReLU, LogSigmoid, PReLU, ReLU, ReLU6, RReLU, SELU, CELU, GELU, Sigmoid, SiLU, Mish, Softplus, Softshrink, Softsign, Tanh, Tanhshrink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_M0qrN7HtL4M",
    "outputId": "e21d6550-6e0c-4c42-e309-6c9760c1795f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 1.431832  [ 1568/ 3200]\n",
      "loss: 1.383111  [ 1584/ 3200]\n",
      "loss: 1.402261  [ 1600/ 3200]\n",
      "loss: 1.353530  [ 1616/ 3200]\n",
      "loss: 1.374574  [ 1632/ 3200]\n",
      "loss: 1.375576  [ 1648/ 3200]\n",
      "loss: 1.390301  [ 1664/ 3200]\n",
      "loss: 1.375280  [ 1680/ 3200]\n",
      "loss: 1.394672  [ 1696/ 3200]\n",
      "loss: 1.414776  [ 1712/ 3200]\n",
      "loss: 1.382787  [ 1728/ 3200]\n",
      "loss: 1.369140  [ 1744/ 3200]\n",
      "loss: 1.405612  [ 1760/ 3200]\n",
      "loss: 1.414398  [ 1776/ 3200]\n",
      "loss: 1.398048  [ 1792/ 3200]\n",
      "loss: 1.388941  [ 1808/ 3200]\n",
      "loss: 1.377401  [ 1824/ 3200]\n",
      "loss: 1.391904  [ 1840/ 3200]\n",
      "loss: 1.389728  [ 1856/ 3200]\n",
      "loss: 1.402747  [ 1872/ 3200]\n",
      "loss: 1.395945  [ 1888/ 3200]\n",
      "loss: 1.410254  [ 1904/ 3200]\n",
      "loss: 1.406414  [ 1920/ 3200]\n",
      "loss: 1.359241  [ 1936/ 3200]\n",
      "loss: 1.380731  [ 1952/ 3200]\n",
      "loss: 1.387135  [ 1968/ 3200]\n",
      "loss: 1.364876  [ 1984/ 3200]\n",
      "loss: 1.388739  [ 2000/ 3200]\n",
      "loss: 1.384979  [ 2016/ 3200]\n",
      "loss: 1.387516  [ 2032/ 3200]\n",
      "loss: 1.392961  [ 2048/ 3200]\n",
      "loss: 1.379023  [ 2064/ 3200]\n",
      "loss: 1.371499  [ 2080/ 3200]\n",
      "loss: 1.381280  [ 2096/ 3200]\n",
      "loss: 1.396825  [ 2112/ 3200]\n",
      "loss: 1.408976  [ 2128/ 3200]\n",
      "loss: 1.374542  [ 2144/ 3200]\n",
      "loss: 1.371946  [ 2160/ 3200]\n",
      "loss: 1.416499  [ 2176/ 3200]\n",
      "loss: 1.395900  [ 2192/ 3200]\n",
      "loss: 1.385433  [ 2208/ 3200]\n",
      "loss: 1.384020  [ 2224/ 3200]\n",
      "loss: 1.371585  [ 2240/ 3200]\n",
      "loss: 1.390340  [ 2256/ 3200]\n",
      "loss: 1.376138  [ 2272/ 3200]\n",
      "loss: 1.381666  [ 2288/ 3200]\n",
      "loss: 1.380053  [ 2304/ 3200]\n",
      "loss: 1.404302  [ 2320/ 3200]\n",
      "loss: 1.369460  [ 2336/ 3200]\n",
      "loss: 1.371304  [ 2352/ 3200]\n",
      "loss: 1.384530  [ 2368/ 3200]\n",
      "loss: 1.375285  [ 2384/ 3200]\n",
      "loss: 1.395857  [ 2400/ 3200]\n",
      "loss: 1.346827  [ 2416/ 3200]\n",
      "loss: 1.423452  [ 2432/ 3200]\n",
      "loss: 1.390726  [ 2448/ 3200]\n",
      "loss: 1.382634  [ 2464/ 3200]\n",
      "loss: 1.388278  [ 2480/ 3200]\n",
      "loss: 1.375288  [ 2496/ 3200]\n",
      "loss: 1.423552  [ 2512/ 3200]\n",
      "loss: 1.382899  [ 2528/ 3200]\n",
      "loss: 1.392959  [ 2544/ 3200]\n",
      "loss: 1.406046  [ 2560/ 3200]\n",
      "loss: 1.360544  [ 2576/ 3200]\n",
      "loss: 1.395410  [ 2592/ 3200]\n",
      "loss: 1.409417  [ 2608/ 3200]\n",
      "loss: 1.387006  [ 2624/ 3200]\n",
      "loss: 1.403063  [ 2640/ 3200]\n",
      "loss: 1.392599  [ 2656/ 3200]\n",
      "loss: 1.386420  [ 2672/ 3200]\n",
      "loss: 1.389406  [ 2688/ 3200]\n",
      "loss: 1.361164  [ 2704/ 3200]\n",
      "loss: 1.380693  [ 2720/ 3200]\n",
      "loss: 1.382471  [ 2736/ 3200]\n",
      "loss: 1.398476  [ 2752/ 3200]\n",
      "loss: 1.394690  [ 2768/ 3200]\n",
      "loss: 1.412149  [ 2784/ 3200]\n",
      "loss: 1.370636  [ 2800/ 3200]\n",
      "loss: 1.384981  [ 2816/ 3200]\n",
      "loss: 1.397948  [ 2832/ 3200]\n",
      "loss: 1.402934  [ 2848/ 3200]\n",
      "loss: 1.385969  [ 2864/ 3200]\n",
      "loss: 1.381273  [ 2880/ 3200]\n",
      "loss: 1.395097  [ 2896/ 3200]\n",
      "loss: 1.396603  [ 2912/ 3200]\n",
      "loss: 1.368895  [ 2928/ 3200]\n",
      "loss: 1.376785  [ 2944/ 3200]\n",
      "loss: 1.365252  [ 2960/ 3200]\n",
      "loss: 1.393314  [ 2976/ 3200]\n",
      "loss: 1.375678  [ 2992/ 3200]\n",
      "loss: 1.398506  [ 3008/ 3200]\n",
      "loss: 1.404256  [ 3024/ 3200]\n",
      "loss: 1.388143  [ 3040/ 3200]\n",
      "loss: 1.382910  [ 3056/ 3200]\n",
      "loss: 1.382001  [ 3072/ 3200]\n",
      "loss: 1.403801  [ 3088/ 3200]\n",
      "loss: 1.404531  [ 3104/ 3200]\n",
      "loss: 1.378694  [ 3120/ 3200]\n",
      "loss: 1.396303  [ 3136/ 3200]\n",
      "loss: 1.381140  [ 3152/ 3200]\n",
      "loss: 1.384575  [ 3168/ 3200]\n",
      "loss: 1.379806  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086801\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 1.387041  [    0/ 3200]\n",
      "loss: 1.372181  [   16/ 3200]\n",
      "loss: 1.396653  [   32/ 3200]\n",
      "loss: 1.386703  [   48/ 3200]\n",
      "loss: 1.385264  [   64/ 3200]\n",
      "loss: 1.392151  [   80/ 3200]\n",
      "loss: 1.390463  [   96/ 3200]\n",
      "loss: 1.413873  [  112/ 3200]\n",
      "loss: 1.408447  [  128/ 3200]\n",
      "loss: 1.413971  [  144/ 3200]\n",
      "loss: 1.390707  [  160/ 3200]\n",
      "loss: 1.405917  [  176/ 3200]\n",
      "loss: 1.372030  [  192/ 3200]\n",
      "loss: 1.374367  [  208/ 3200]\n",
      "loss: 1.406250  [  224/ 3200]\n",
      "loss: 1.370744  [  240/ 3200]\n",
      "loss: 1.421921  [  256/ 3200]\n",
      "loss: 1.373031  [  272/ 3200]\n",
      "loss: 1.400696  [  288/ 3200]\n",
      "loss: 1.377465  [  304/ 3200]\n",
      "loss: 1.370306  [  320/ 3200]\n",
      "loss: 1.393375  [  336/ 3200]\n",
      "loss: 1.400618  [  352/ 3200]\n",
      "loss: 1.373307  [  368/ 3200]\n",
      "loss: 1.389106  [  384/ 3200]\n",
      "loss: 1.399461  [  400/ 3200]\n",
      "loss: 1.404191  [  416/ 3200]\n",
      "loss: 1.395120  [  432/ 3200]\n",
      "loss: 1.401826  [  448/ 3200]\n",
      "loss: 1.394125  [  464/ 3200]\n",
      "loss: 1.399988  [  480/ 3200]\n",
      "loss: 1.391852  [  496/ 3200]\n",
      "loss: 1.384682  [  512/ 3200]\n",
      "loss: 1.401303  [  528/ 3200]\n",
      "loss: 1.387313  [  544/ 3200]\n",
      "loss: 1.384151  [  560/ 3200]\n",
      "loss: 1.388641  [  576/ 3200]\n",
      "loss: 1.407009  [  592/ 3200]\n",
      "loss: 1.397504  [  608/ 3200]\n",
      "loss: 1.389526  [  624/ 3200]\n",
      "loss: 1.373304  [  640/ 3200]\n",
      "loss: 1.396302  [  656/ 3200]\n",
      "loss: 1.360408  [  672/ 3200]\n",
      "loss: 1.368008  [  688/ 3200]\n",
      "loss: 1.382617  [  704/ 3200]\n",
      "loss: 1.395013  [  720/ 3200]\n",
      "loss: 1.384175  [  736/ 3200]\n",
      "loss: 1.411382  [  752/ 3200]\n",
      "loss: 1.370929  [  768/ 3200]\n",
      "loss: 1.376896  [  784/ 3200]\n",
      "loss: 1.385343  [  800/ 3200]\n",
      "loss: 1.379519  [  816/ 3200]\n",
      "loss: 1.389911  [  832/ 3200]\n",
      "loss: 1.356005  [  848/ 3200]\n",
      "loss: 1.388711  [  864/ 3200]\n",
      "loss: 1.361385  [  880/ 3200]\n",
      "loss: 1.385884  [  896/ 3200]\n",
      "loss: 1.379147  [  912/ 3200]\n",
      "loss: 1.372101  [  928/ 3200]\n",
      "loss: 1.367756  [  944/ 3200]\n",
      "loss: 1.401091  [  960/ 3200]\n",
      "loss: 1.383431  [  976/ 3200]\n",
      "loss: 1.401719  [  992/ 3200]\n",
      "loss: 1.406776  [ 1008/ 3200]\n",
      "loss: 1.363234  [ 1024/ 3200]\n",
      "loss: 1.385109  [ 1040/ 3200]\n",
      "loss: 1.407264  [ 1056/ 3200]\n",
      "loss: 1.347083  [ 1072/ 3200]\n",
      "loss: 1.389155  [ 1088/ 3200]\n",
      "loss: 1.385302  [ 1104/ 3200]\n",
      "loss: 1.394757  [ 1120/ 3200]\n",
      "loss: 1.375773  [ 1136/ 3200]\n",
      "loss: 1.397288  [ 1152/ 3200]\n",
      "loss: 1.395788  [ 1168/ 3200]\n",
      "loss: 1.364343  [ 1184/ 3200]\n",
      "loss: 1.398816  [ 1200/ 3200]\n",
      "loss: 1.384613  [ 1216/ 3200]\n",
      "loss: 1.375107  [ 1232/ 3200]\n",
      "loss: 1.369741  [ 1248/ 3200]\n",
      "loss: 1.385896  [ 1264/ 3200]\n",
      "loss: 1.370421  [ 1280/ 3200]\n",
      "loss: 1.397640  [ 1296/ 3200]\n",
      "loss: 1.372971  [ 1312/ 3200]\n",
      "loss: 1.369877  [ 1328/ 3200]\n",
      "loss: 1.373194  [ 1344/ 3200]\n",
      "loss: 1.372035  [ 1360/ 3200]\n",
      "loss: 1.394290  [ 1376/ 3200]\n",
      "loss: 1.357628  [ 1392/ 3200]\n",
      "loss: 1.362527  [ 1408/ 3200]\n",
      "loss: 1.403256  [ 1424/ 3200]\n",
      "loss: 1.385786  [ 1440/ 3200]\n",
      "loss: 1.412827  [ 1456/ 3200]\n",
      "loss: 1.376819  [ 1472/ 3200]\n",
      "loss: 1.374949  [ 1488/ 3200]\n",
      "loss: 1.347478  [ 1504/ 3200]\n",
      "loss: 1.381102  [ 1520/ 3200]\n",
      "loss: 1.391657  [ 1536/ 3200]\n",
      "loss: 1.399993  [ 1552/ 3200]\n",
      "loss: 1.402915  [ 1568/ 3200]\n",
      "loss: 1.347057  [ 1584/ 3200]\n",
      "loss: 1.388067  [ 1600/ 3200]\n",
      "loss: 1.381292  [ 1616/ 3200]\n",
      "loss: 1.402800  [ 1632/ 3200]\n",
      "loss: 1.373323  [ 1648/ 3200]\n",
      "loss: 1.388884  [ 1664/ 3200]\n",
      "loss: 1.400570  [ 1680/ 3200]\n",
      "loss: 1.380183  [ 1696/ 3200]\n",
      "loss: 1.402456  [ 1712/ 3200]\n",
      "loss: 1.417087  [ 1728/ 3200]\n",
      "loss: 1.424269  [ 1744/ 3200]\n",
      "loss: 1.393484  [ 1760/ 3200]\n",
      "loss: 1.385813  [ 1776/ 3200]\n",
      "loss: 1.416658  [ 1792/ 3200]\n",
      "loss: 1.387503  [ 1808/ 3200]\n",
      "loss: 1.363167  [ 1824/ 3200]\n",
      "loss: 1.393424  [ 1840/ 3200]\n",
      "loss: 1.371413  [ 1856/ 3200]\n",
      "loss: 1.418827  [ 1872/ 3200]\n",
      "loss: 1.366307  [ 1888/ 3200]\n",
      "loss: 1.368050  [ 1904/ 3200]\n",
      "loss: 1.363983  [ 1920/ 3200]\n",
      "loss: 1.426444  [ 1936/ 3200]\n",
      "loss: 1.362525  [ 1952/ 3200]\n",
      "loss: 1.423040  [ 1968/ 3200]\n",
      "loss: 1.391571  [ 1984/ 3200]\n",
      "loss: 1.391005  [ 2000/ 3200]\n",
      "loss: 1.405163  [ 2016/ 3200]\n",
      "loss: 1.399429  [ 2032/ 3200]\n",
      "loss: 1.330212  [ 2048/ 3200]\n",
      "loss: 1.364400  [ 2064/ 3200]\n",
      "loss: 1.375114  [ 2080/ 3200]\n",
      "loss: 1.350285  [ 2096/ 3200]\n",
      "loss: 1.418750  [ 2112/ 3200]\n",
      "loss: 1.419548  [ 2128/ 3200]\n",
      "loss: 1.384607  [ 2144/ 3200]\n",
      "loss: 1.373896  [ 2160/ 3200]\n",
      "loss: 1.375562  [ 2176/ 3200]\n",
      "loss: 1.398109  [ 2192/ 3200]\n",
      "loss: 1.418755  [ 2208/ 3200]\n",
      "loss: 1.399583  [ 2224/ 3200]\n",
      "loss: 1.421114  [ 2240/ 3200]\n",
      "loss: 1.385748  [ 2256/ 3200]\n",
      "loss: 1.416865  [ 2272/ 3200]\n",
      "loss: 1.382859  [ 2288/ 3200]\n",
      "loss: 1.415805  [ 2304/ 3200]\n",
      "loss: 1.365913  [ 2320/ 3200]\n",
      "loss: 1.389513  [ 2336/ 3200]\n",
      "loss: 1.380525  [ 2352/ 3200]\n",
      "loss: 1.363250  [ 2368/ 3200]\n",
      "loss: 1.420939  [ 2384/ 3200]\n",
      "loss: 1.405361  [ 2400/ 3200]\n",
      "loss: 1.402021  [ 2416/ 3200]\n",
      "loss: 1.391700  [ 2432/ 3200]\n",
      "loss: 1.372830  [ 2448/ 3200]\n",
      "loss: 1.376796  [ 2464/ 3200]\n",
      "loss: 1.414986  [ 2480/ 3200]\n",
      "loss: 1.420747  [ 2496/ 3200]\n",
      "loss: 1.379454  [ 2512/ 3200]\n",
      "loss: 1.400815  [ 2528/ 3200]\n",
      "loss: 1.395989  [ 2544/ 3200]\n",
      "loss: 1.426707  [ 2560/ 3200]\n",
      "loss: 1.384972  [ 2576/ 3200]\n",
      "loss: 1.392904  [ 2592/ 3200]\n",
      "loss: 1.365996  [ 2608/ 3200]\n",
      "loss: 1.402239  [ 2624/ 3200]\n",
      "loss: 1.358978  [ 2640/ 3200]\n",
      "loss: 1.369279  [ 2656/ 3200]\n",
      "loss: 1.396454  [ 2672/ 3200]\n",
      "loss: 1.390195  [ 2688/ 3200]\n",
      "loss: 1.387564  [ 2704/ 3200]\n",
      "loss: 1.407501  [ 2720/ 3200]\n",
      "loss: 1.389983  [ 2736/ 3200]\n",
      "loss: 1.388682  [ 2752/ 3200]\n",
      "loss: 1.393263  [ 2768/ 3200]\n",
      "loss: 1.396311  [ 2784/ 3200]\n",
      "loss: 1.392645  [ 2800/ 3200]\n",
      "loss: 1.411369  [ 2816/ 3200]\n",
      "loss: 1.375379  [ 2832/ 3200]\n",
      "loss: 1.389177  [ 2848/ 3200]\n",
      "loss: 1.391156  [ 2864/ 3200]\n",
      "loss: 1.409161  [ 2880/ 3200]\n",
      "loss: 1.382321  [ 2896/ 3200]\n",
      "loss: 1.399656  [ 2912/ 3200]\n",
      "loss: 1.383234  [ 2928/ 3200]\n",
      "loss: 1.403682  [ 2944/ 3200]\n",
      "loss: 1.383465  [ 2960/ 3200]\n",
      "loss: 1.407473  [ 2976/ 3200]\n",
      "loss: 1.402068  [ 2992/ 3200]\n",
      "loss: 1.351360  [ 3008/ 3200]\n",
      "loss: 1.382810  [ 3024/ 3200]\n",
      "loss: 1.391318  [ 3040/ 3200]\n",
      "loss: 1.404256  [ 3056/ 3200]\n",
      "loss: 1.386128  [ 3072/ 3200]\n",
      "loss: 1.416746  [ 3088/ 3200]\n",
      "loss: 1.390914  [ 3104/ 3200]\n",
      "loss: 1.371682  [ 3120/ 3200]\n",
      "loss: 1.393908  [ 3136/ 3200]\n",
      "loss: 1.384351  [ 3152/ 3200]\n",
      "loss: 1.391487  [ 3168/ 3200]\n",
      "loss: 1.396049  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086800\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 1.385178  [    0/ 3200]\n",
      "loss: 1.382897  [   16/ 3200]\n",
      "loss: 1.378762  [   32/ 3200]\n",
      "loss: 1.383013  [   48/ 3200]\n",
      "loss: 1.396362  [   64/ 3200]\n",
      "loss: 1.378746  [   80/ 3200]\n",
      "loss: 1.390997  [   96/ 3200]\n",
      "loss: 1.398976  [  112/ 3200]\n",
      "loss: 1.383195  [  128/ 3200]\n",
      "loss: 1.403629  [  144/ 3200]\n",
      "loss: 1.395047  [  160/ 3200]\n",
      "loss: 1.369634  [  176/ 3200]\n",
      "loss: 1.402058  [  192/ 3200]\n",
      "loss: 1.385654  [  208/ 3200]\n",
      "loss: 1.398124  [  224/ 3200]\n",
      "loss: 1.411207  [  240/ 3200]\n",
      "loss: 1.393117  [  256/ 3200]\n",
      "loss: 1.402138  [  272/ 3200]\n",
      "loss: 1.358994  [  288/ 3200]\n",
      "loss: 1.392902  [  304/ 3200]\n",
      "loss: 1.368120  [  320/ 3200]\n",
      "loss: 1.371485  [  336/ 3200]\n",
      "loss: 1.369003  [  352/ 3200]\n",
      "loss: 1.387026  [  368/ 3200]\n",
      "loss: 1.393235  [  384/ 3200]\n",
      "loss: 1.386223  [  400/ 3200]\n",
      "loss: 1.368999  [  416/ 3200]\n",
      "loss: 1.405989  [  432/ 3200]\n",
      "loss: 1.386706  [  448/ 3200]\n",
      "loss: 1.378515  [  464/ 3200]\n",
      "loss: 1.420458  [  480/ 3200]\n",
      "loss: 1.409586  [  496/ 3200]\n",
      "loss: 1.365859  [  512/ 3200]\n",
      "loss: 1.355517  [  528/ 3200]\n",
      "loss: 1.353676  [  544/ 3200]\n",
      "loss: 1.408064  [  560/ 3200]\n",
      "loss: 1.366650  [  576/ 3200]\n",
      "loss: 1.372909  [  592/ 3200]\n",
      "loss: 1.396735  [  608/ 3200]\n",
      "loss: 1.357586  [  624/ 3200]\n",
      "loss: 1.402156  [  640/ 3200]\n",
      "loss: 1.382813  [  656/ 3200]\n",
      "loss: 1.366343  [  672/ 3200]\n",
      "loss: 1.400953  [  688/ 3200]\n",
      "loss: 1.400286  [  704/ 3200]\n",
      "loss: 1.415960  [  720/ 3200]\n",
      "loss: 1.402808  [  736/ 3200]\n",
      "loss: 1.390603  [  752/ 3200]\n",
      "loss: 1.360549  [  768/ 3200]\n",
      "loss: 1.369989  [  784/ 3200]\n",
      "loss: 1.402357  [  800/ 3200]\n",
      "loss: 1.377563  [  816/ 3200]\n",
      "loss: 1.378769  [  832/ 3200]\n",
      "loss: 1.405421  [  848/ 3200]\n",
      "loss: 1.403911  [  864/ 3200]\n",
      "loss: 1.421792  [  880/ 3200]\n",
      "loss: 1.389800  [  896/ 3200]\n",
      "loss: 1.397707  [  912/ 3200]\n",
      "loss: 1.413886  [  928/ 3200]\n",
      "loss: 1.340175  [  944/ 3200]\n",
      "loss: 1.374004  [  960/ 3200]\n",
      "loss: 1.387664  [  976/ 3200]\n",
      "loss: 1.363608  [  992/ 3200]\n",
      "loss: 1.393656  [ 1008/ 3200]\n",
      "loss: 1.416343  [ 1024/ 3200]\n",
      "loss: 1.391365  [ 1040/ 3200]\n",
      "loss: 1.344031  [ 1056/ 3200]\n",
      "loss: 1.361495  [ 1072/ 3200]\n",
      "loss: 1.406384  [ 1088/ 3200]\n",
      "loss: 1.379406  [ 1104/ 3200]\n",
      "loss: 1.399541  [ 1120/ 3200]\n",
      "loss: 1.397682  [ 1136/ 3200]\n",
      "loss: 1.382099  [ 1152/ 3200]\n",
      "loss: 1.385954  [ 1168/ 3200]\n",
      "loss: 1.373654  [ 1184/ 3200]\n",
      "loss: 1.374804  [ 1200/ 3200]\n",
      "loss: 1.362664  [ 1216/ 3200]\n",
      "loss: 1.379687  [ 1232/ 3200]\n",
      "loss: 1.385158  [ 1248/ 3200]\n",
      "loss: 1.405931  [ 1264/ 3200]\n",
      "loss: 1.390889  [ 1280/ 3200]\n",
      "loss: 1.390303  [ 1296/ 3200]\n",
      "loss: 1.388901  [ 1312/ 3200]\n",
      "loss: 1.410617  [ 1328/ 3200]\n",
      "loss: 1.395993  [ 1344/ 3200]\n",
      "loss: 1.386287  [ 1360/ 3200]\n",
      "loss: 1.365937  [ 1376/ 3200]\n",
      "loss: 1.363352  [ 1392/ 3200]\n",
      "loss: 1.388982  [ 1408/ 3200]\n",
      "loss: 1.358642  [ 1424/ 3200]\n",
      "loss: 1.359459  [ 1440/ 3200]\n",
      "loss: 1.398403  [ 1456/ 3200]\n",
      "loss: 1.390075  [ 1472/ 3200]\n",
      "loss: 1.358117  [ 1488/ 3200]\n",
      "loss: 1.380120  [ 1504/ 3200]\n",
      "loss: 1.367824  [ 1520/ 3200]\n",
      "loss: 1.418413  [ 1536/ 3200]\n",
      "loss: 1.390295  [ 1552/ 3200]\n",
      "loss: 1.387730  [ 1568/ 3200]\n",
      "loss: 1.373365  [ 1584/ 3200]\n",
      "loss: 1.405451  [ 1600/ 3200]\n",
      "loss: 1.364843  [ 1616/ 3200]\n",
      "loss: 1.378617  [ 1632/ 3200]\n",
      "loss: 1.410575  [ 1648/ 3200]\n",
      "loss: 1.403983  [ 1664/ 3200]\n",
      "loss: 1.416820  [ 1680/ 3200]\n",
      "loss: 1.370504  [ 1696/ 3200]\n",
      "loss: 1.387875  [ 1712/ 3200]\n",
      "loss: 1.401272  [ 1728/ 3200]\n",
      "loss: 1.366783  [ 1744/ 3200]\n",
      "loss: 1.381254  [ 1760/ 3200]\n",
      "loss: 1.378869  [ 1776/ 3200]\n",
      "loss: 1.424138  [ 1792/ 3200]\n",
      "loss: 1.411149  [ 1808/ 3200]\n",
      "loss: 1.421322  [ 1824/ 3200]\n",
      "loss: 1.396795  [ 1840/ 3200]\n",
      "loss: 1.397492  [ 1856/ 3200]\n",
      "loss: 1.385536  [ 1872/ 3200]\n",
      "loss: 1.395482  [ 1888/ 3200]\n",
      "loss: 1.402592  [ 1904/ 3200]\n",
      "loss: 1.380584  [ 1920/ 3200]\n",
      "loss: 1.395020  [ 1936/ 3200]\n",
      "loss: 1.376790  [ 1952/ 3200]\n",
      "loss: 1.400682  [ 1968/ 3200]\n",
      "loss: 1.408627  [ 1984/ 3200]\n",
      "loss: 1.369833  [ 2000/ 3200]\n",
      "loss: 1.389350  [ 2016/ 3200]\n",
      "loss: 1.398237  [ 2032/ 3200]\n",
      "loss: 1.382177  [ 2048/ 3200]\n",
      "loss: 1.410938  [ 2064/ 3200]\n",
      "loss: 1.393142  [ 2080/ 3200]\n",
      "loss: 1.402473  [ 2096/ 3200]\n",
      "loss: 1.380137  [ 2112/ 3200]\n",
      "loss: 1.391337  [ 2128/ 3200]\n",
      "loss: 1.369737  [ 2144/ 3200]\n",
      "loss: 1.380960  [ 2160/ 3200]\n",
      "loss: 1.405030  [ 2176/ 3200]\n",
      "loss: 1.383598  [ 2192/ 3200]\n",
      "loss: 1.396799  [ 2208/ 3200]\n",
      "loss: 1.364954  [ 2224/ 3200]\n",
      "loss: 1.364424  [ 2240/ 3200]\n",
      "loss: 1.412781  [ 2256/ 3200]\n",
      "loss: 1.390685  [ 2272/ 3200]\n",
      "loss: 1.392337  [ 2288/ 3200]\n",
      "loss: 1.385152  [ 2304/ 3200]\n",
      "loss: 1.411609  [ 2320/ 3200]\n",
      "loss: 1.402512  [ 2336/ 3200]\n",
      "loss: 1.387667  [ 2352/ 3200]\n",
      "loss: 1.402345  [ 2368/ 3200]\n",
      "loss: 1.363824  [ 2384/ 3200]\n",
      "loss: 1.363223  [ 2400/ 3200]\n",
      "loss: 1.389820  [ 2416/ 3200]\n",
      "loss: 1.388012  [ 2432/ 3200]\n",
      "loss: 1.388176  [ 2448/ 3200]\n",
      "loss: 1.381111  [ 2464/ 3200]\n",
      "loss: 1.399462  [ 2480/ 3200]\n",
      "loss: 1.398440  [ 2496/ 3200]\n",
      "loss: 1.375620  [ 2512/ 3200]\n",
      "loss: 1.382535  [ 2528/ 3200]\n",
      "loss: 1.397876  [ 2544/ 3200]\n",
      "loss: 1.372082  [ 2560/ 3200]\n",
      "loss: 1.368000  [ 2576/ 3200]\n",
      "loss: 1.398729  [ 2592/ 3200]\n",
      "loss: 1.365494  [ 2608/ 3200]\n",
      "loss: 1.390942  [ 2624/ 3200]\n",
      "loss: 1.405868  [ 2640/ 3200]\n",
      "loss: 1.405033  [ 2656/ 3200]\n",
      "loss: 1.419833  [ 2672/ 3200]\n",
      "loss: 1.345083  [ 2688/ 3200]\n",
      "loss: 1.384716  [ 2704/ 3200]\n",
      "loss: 1.396860  [ 2720/ 3200]\n",
      "loss: 1.356299  [ 2736/ 3200]\n",
      "loss: 1.405121  [ 2752/ 3200]\n",
      "loss: 1.422198  [ 2768/ 3200]\n",
      "loss: 1.390608  [ 2784/ 3200]\n",
      "loss: 1.414578  [ 2800/ 3200]\n",
      "loss: 1.386203  [ 2816/ 3200]\n",
      "loss: 1.367572  [ 2832/ 3200]\n",
      "loss: 1.386046  [ 2848/ 3200]\n",
      "loss: 1.409254  [ 2864/ 3200]\n",
      "loss: 1.394664  [ 2880/ 3200]\n",
      "loss: 1.376880  [ 2896/ 3200]\n",
      "loss: 1.421652  [ 2912/ 3200]\n",
      "loss: 1.380390  [ 2928/ 3200]\n",
      "loss: 1.373918  [ 2944/ 3200]\n",
      "loss: 1.396070  [ 2960/ 3200]\n",
      "loss: 1.399041  [ 2976/ 3200]\n",
      "loss: 1.368830  [ 2992/ 3200]\n",
      "loss: 1.391117  [ 3008/ 3200]\n",
      "loss: 1.408225  [ 3024/ 3200]\n",
      "loss: 1.366741  [ 3040/ 3200]\n",
      "loss: 1.391434  [ 3056/ 3200]\n",
      "loss: 1.419664  [ 3072/ 3200]\n",
      "loss: 1.390924  [ 3088/ 3200]\n",
      "loss: 1.411029  [ 3104/ 3200]\n",
      "loss: 1.374304  [ 3120/ 3200]\n",
      "loss: 1.400728  [ 3136/ 3200]\n",
      "loss: 1.394183  [ 3152/ 3200]\n",
      "loss: 1.394976  [ 3168/ 3200]\n",
      "loss: 1.393854  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086845\n",
      "f1 macro averaged score: 0.102676\n",
      "Accuracy               : 25.1%\n",
      "Confusion matrix       :\n",
      "tensor([[  1, 199,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  1, 199,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 1.395519  [    0/ 3200]\n",
      "loss: 1.369659  [   16/ 3200]\n",
      "loss: 1.345760  [   32/ 3200]\n",
      "loss: 1.401275  [   48/ 3200]\n",
      "loss: 1.360041  [   64/ 3200]\n",
      "loss: 1.382916  [   80/ 3200]\n",
      "loss: 1.406253  [   96/ 3200]\n",
      "loss: 1.399423  [  112/ 3200]\n",
      "loss: 1.375226  [  128/ 3200]\n",
      "loss: 1.383853  [  144/ 3200]\n",
      "loss: 1.402202  [  160/ 3200]\n",
      "loss: 1.346891  [  176/ 3200]\n",
      "loss: 1.377663  [  192/ 3200]\n",
      "loss: 1.369480  [  208/ 3200]\n",
      "loss: 1.417015  [  224/ 3200]\n",
      "loss: 1.403039  [  240/ 3200]\n",
      "loss: 1.393592  [  256/ 3200]\n",
      "loss: 1.367590  [  272/ 3200]\n",
      "loss: 1.421657  [  288/ 3200]\n",
      "loss: 1.390232  [  304/ 3200]\n",
      "loss: 1.342580  [  320/ 3200]\n",
      "loss: 1.373141  [  336/ 3200]\n",
      "loss: 1.360821  [  352/ 3200]\n",
      "loss: 1.405916  [  368/ 3200]\n",
      "loss: 1.388709  [  384/ 3200]\n",
      "loss: 1.411297  [  400/ 3200]\n",
      "loss: 1.393863  [  416/ 3200]\n",
      "loss: 1.415814  [  432/ 3200]\n",
      "loss: 1.406205  [  448/ 3200]\n",
      "loss: 1.377070  [  464/ 3200]\n",
      "loss: 1.391517  [  480/ 3200]\n",
      "loss: 1.426910  [  496/ 3200]\n",
      "loss: 1.400913  [  512/ 3200]\n",
      "loss: 1.379783  [  528/ 3200]\n",
      "loss: 1.412062  [  544/ 3200]\n",
      "loss: 1.380609  [  560/ 3200]\n",
      "loss: 1.356952  [  576/ 3200]\n",
      "loss: 1.372645  [  592/ 3200]\n",
      "loss: 1.392355  [  608/ 3200]\n",
      "loss: 1.390029  [  624/ 3200]\n",
      "loss: 1.343356  [  640/ 3200]\n",
      "loss: 1.393193  [  656/ 3200]\n",
      "loss: 1.396499  [  672/ 3200]\n",
      "loss: 1.386606  [  688/ 3200]\n",
      "loss: 1.414540  [  704/ 3200]\n",
      "loss: 1.380032  [  720/ 3200]\n",
      "loss: 1.371795  [  736/ 3200]\n",
      "loss: 1.412028  [  752/ 3200]\n",
      "loss: 1.411681  [  768/ 3200]\n",
      "loss: 1.406546  [  784/ 3200]\n",
      "loss: 1.392455  [  800/ 3200]\n",
      "loss: 1.391238  [  816/ 3200]\n",
      "loss: 1.394460  [  832/ 3200]\n",
      "loss: 1.354096  [  848/ 3200]\n",
      "loss: 1.368754  [  864/ 3200]\n",
      "loss: 1.400880  [  880/ 3200]\n",
      "loss: 1.378612  [  896/ 3200]\n",
      "loss: 1.384299  [  912/ 3200]\n",
      "loss: 1.360358  [  928/ 3200]\n",
      "loss: 1.377504  [  944/ 3200]\n",
      "loss: 1.397275  [  960/ 3200]\n",
      "loss: 1.389693  [  976/ 3200]\n",
      "loss: 1.366719  [  992/ 3200]\n",
      "loss: 1.371585  [ 1008/ 3200]\n",
      "loss: 1.371863  [ 1024/ 3200]\n",
      "loss: 1.405363  [ 1040/ 3200]\n",
      "loss: 1.419139  [ 1056/ 3200]\n",
      "loss: 1.383164  [ 1072/ 3200]\n",
      "loss: 1.402424  [ 1088/ 3200]\n",
      "loss: 1.396684  [ 1104/ 3200]\n",
      "loss: 1.411878  [ 1120/ 3200]\n",
      "loss: 1.372524  [ 1136/ 3200]\n",
      "loss: 1.363716  [ 1152/ 3200]\n",
      "loss: 1.443287  [ 1168/ 3200]\n",
      "loss: 1.376019  [ 1184/ 3200]\n",
      "loss: 1.398304  [ 1200/ 3200]\n",
      "loss: 1.412928  [ 1216/ 3200]\n",
      "loss: 1.387344  [ 1232/ 3200]\n",
      "loss: 1.387973  [ 1248/ 3200]\n",
      "loss: 1.344349  [ 1264/ 3200]\n",
      "loss: 1.382185  [ 1280/ 3200]\n",
      "loss: 1.348763  [ 1296/ 3200]\n",
      "loss: 1.381938  [ 1312/ 3200]\n",
      "loss: 1.366882  [ 1328/ 3200]\n",
      "loss: 1.349148  [ 1344/ 3200]\n",
      "loss: 1.386787  [ 1360/ 3200]\n",
      "loss: 1.401327  [ 1376/ 3200]\n",
      "loss: 1.370028  [ 1392/ 3200]\n",
      "loss: 1.396659  [ 1408/ 3200]\n",
      "loss: 1.377291  [ 1424/ 3200]\n",
      "loss: 1.411227  [ 1440/ 3200]\n",
      "loss: 1.366859  [ 1456/ 3200]\n",
      "loss: 1.394411  [ 1472/ 3200]\n",
      "loss: 1.393149  [ 1488/ 3200]\n",
      "loss: 1.390990  [ 1504/ 3200]\n",
      "loss: 1.378884  [ 1520/ 3200]\n",
      "loss: 1.361980  [ 1536/ 3200]\n",
      "loss: 1.387961  [ 1552/ 3200]\n",
      "loss: 1.410956  [ 1568/ 3200]\n",
      "loss: 1.346484  [ 1584/ 3200]\n",
      "loss: 1.417031  [ 1600/ 3200]\n",
      "loss: 1.416343  [ 1616/ 3200]\n",
      "loss: 1.435857  [ 1632/ 3200]\n",
      "loss: 1.409648  [ 1648/ 3200]\n",
      "loss: 1.356800  [ 1664/ 3200]\n",
      "loss: 1.374836  [ 1680/ 3200]\n",
      "loss: 1.356963  [ 1696/ 3200]\n",
      "loss: 1.382086  [ 1712/ 3200]\n",
      "loss: 1.400565  [ 1728/ 3200]\n",
      "loss: 1.341910  [ 1744/ 3200]\n",
      "loss: 1.395047  [ 1760/ 3200]\n",
      "loss: 1.377255  [ 1776/ 3200]\n",
      "loss: 1.408764  [ 1792/ 3200]\n",
      "loss: 1.415475  [ 1808/ 3200]\n",
      "loss: 1.352328  [ 1824/ 3200]\n",
      "loss: 1.381052  [ 1840/ 3200]\n",
      "loss: 1.358713  [ 1856/ 3200]\n",
      "loss: 1.324785  [ 1872/ 3200]\n",
      "loss: 1.359904  [ 1888/ 3200]\n",
      "loss: 1.382937  [ 1904/ 3200]\n",
      "loss: 1.417116  [ 1920/ 3200]\n",
      "loss: 1.371748  [ 1936/ 3200]\n",
      "loss: 1.415573  [ 1952/ 3200]\n",
      "loss: 1.347120  [ 1968/ 3200]\n",
      "loss: 1.407482  [ 1984/ 3200]\n",
      "loss: 1.346346  [ 2000/ 3200]\n",
      "loss: 1.371627  [ 2016/ 3200]\n",
      "loss: 1.405003  [ 2032/ 3200]\n",
      "loss: 1.437463  [ 2048/ 3200]\n",
      "loss: 1.350492  [ 2064/ 3200]\n",
      "loss: 1.374018  [ 2080/ 3200]\n",
      "loss: 1.418592  [ 2096/ 3200]\n",
      "loss: 1.443722  [ 2112/ 3200]\n",
      "loss: 1.375869  [ 2128/ 3200]\n",
      "loss: 1.366276  [ 2144/ 3200]\n",
      "loss: 1.380879  [ 2160/ 3200]\n",
      "loss: 1.405076  [ 2176/ 3200]\n",
      "loss: 1.384287  [ 2192/ 3200]\n",
      "loss: 1.413634  [ 2208/ 3200]\n",
      "loss: 1.316670  [ 2224/ 3200]\n",
      "loss: 1.381748  [ 2240/ 3200]\n",
      "loss: 1.384686  [ 2256/ 3200]\n",
      "loss: 1.355204  [ 2272/ 3200]\n",
      "loss: 1.394819  [ 2288/ 3200]\n",
      "loss: 1.329768  [ 2304/ 3200]\n",
      "loss: 1.378471  [ 2320/ 3200]\n",
      "loss: 1.466174  [ 2336/ 3200]\n",
      "loss: 1.389539  [ 2352/ 3200]\n",
      "loss: 1.344791  [ 2368/ 3200]\n",
      "loss: 1.312182  [ 2384/ 3200]\n",
      "loss: 1.375507  [ 2400/ 3200]\n",
      "loss: 1.436825  [ 2416/ 3200]\n",
      "loss: 1.371723  [ 2432/ 3200]\n",
      "loss: 1.384394  [ 2448/ 3200]\n",
      "loss: 1.408625  [ 2464/ 3200]\n",
      "loss: 1.402619  [ 2480/ 3200]\n",
      "loss: 1.414151  [ 2496/ 3200]\n",
      "loss: 1.375502  [ 2512/ 3200]\n",
      "loss: 1.353995  [ 2528/ 3200]\n",
      "loss: 1.387485  [ 2544/ 3200]\n",
      "loss: 1.335298  [ 2560/ 3200]\n",
      "loss: 1.407429  [ 2576/ 3200]\n",
      "loss: 1.389267  [ 2592/ 3200]\n",
      "loss: 1.357380  [ 2608/ 3200]\n",
      "loss: 1.330047  [ 2624/ 3200]\n",
      "loss: 1.317877  [ 2640/ 3200]\n",
      "loss: 1.357086  [ 2656/ 3200]\n",
      "loss: 1.308689  [ 2672/ 3200]\n",
      "loss: 1.335514  [ 2688/ 3200]\n",
      "loss: 1.415613  [ 2704/ 3200]\n",
      "loss: 1.379323  [ 2720/ 3200]\n",
      "loss: 1.347908  [ 2736/ 3200]\n",
      "loss: 1.250998  [ 2752/ 3200]\n",
      "loss: 1.439649  [ 2768/ 3200]\n",
      "loss: 1.397562  [ 2784/ 3200]\n",
      "loss: 1.398379  [ 2800/ 3200]\n",
      "loss: 1.386003  [ 2816/ 3200]\n",
      "loss: 1.435190  [ 2832/ 3200]\n",
      "loss: 1.355240  [ 2848/ 3200]\n",
      "loss: 1.335606  [ 2864/ 3200]\n",
      "loss: 1.327875  [ 2880/ 3200]\n",
      "loss: 1.296125  [ 2896/ 3200]\n",
      "loss: 1.402472  [ 2912/ 3200]\n",
      "loss: 1.363276  [ 2928/ 3200]\n",
      "loss: 1.383104  [ 2944/ 3200]\n",
      "loss: 1.356709  [ 2960/ 3200]\n",
      "loss: 1.329446  [ 2976/ 3200]\n",
      "loss: 1.332405  [ 2992/ 3200]\n",
      "loss: 1.335912  [ 3008/ 3200]\n",
      "loss: 1.389664  [ 3024/ 3200]\n",
      "loss: 1.381031  [ 3040/ 3200]\n",
      "loss: 1.344238  [ 3056/ 3200]\n",
      "loss: 1.246783  [ 3072/ 3200]\n",
      "loss: 1.382737  [ 3088/ 3200]\n",
      "loss: 1.263582  [ 3104/ 3200]\n",
      "loss: 1.475890  [ 3120/ 3200]\n",
      "loss: 1.385098  [ 3136/ 3200]\n",
      "loss: 1.317495  [ 3152/ 3200]\n",
      "loss: 1.180939  [ 3168/ 3200]\n",
      "loss: 1.389899  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.084104\n",
      "f1 macro averaged score: 0.201639\n",
      "Accuracy               : 30.8%\n",
      "Confusion matrix       :\n",
      "tensor([[ 65, 135,   0,   0],\n",
      "        [ 19, 181,   0,   0],\n",
      "        [ 42, 158,   0,   0],\n",
      "        [  9, 191,   0,   0]], device='cuda:0')\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 1.356014  [    0/ 3200]\n",
      "loss: 1.374746  [   16/ 3200]\n",
      "loss: 1.335226  [   32/ 3200]\n",
      "loss: 1.439652  [   48/ 3200]\n",
      "loss: 1.343359  [   64/ 3200]\n",
      "loss: 1.376152  [   80/ 3200]\n",
      "loss: 1.340598  [   96/ 3200]\n",
      "loss: 1.267033  [  112/ 3200]\n",
      "loss: 1.276650  [  128/ 3200]\n",
      "loss: 1.385013  [  144/ 3200]\n",
      "loss: 1.328500  [  160/ 3200]\n",
      "loss: 1.335598  [  176/ 3200]\n",
      "loss: 1.312412  [  192/ 3200]\n",
      "loss: 1.302534  [  208/ 3200]\n",
      "loss: 1.336939  [  224/ 3200]\n",
      "loss: 1.311394  [  240/ 3200]\n",
      "loss: 1.216126  [  256/ 3200]\n",
      "loss: 1.276852  [  272/ 3200]\n",
      "loss: 1.274003  [  288/ 3200]\n",
      "loss: 1.338530  [  304/ 3200]\n",
      "loss: 1.315566  [  320/ 3200]\n",
      "loss: 1.319501  [  336/ 3200]\n",
      "loss: 1.266109  [  352/ 3200]\n",
      "loss: 1.332237  [  368/ 3200]\n",
      "loss: 1.284474  [  384/ 3200]\n",
      "loss: 1.004536  [  400/ 3200]\n",
      "loss: 1.374273  [  416/ 3200]\n",
      "loss: 1.312559  [  432/ 3200]\n",
      "loss: 1.288145  [  448/ 3200]\n",
      "loss: 1.217777  [  464/ 3200]\n",
      "loss: 1.158048  [  480/ 3200]\n",
      "loss: 1.378866  [  496/ 3200]\n",
      "loss: 1.316899  [  512/ 3200]\n",
      "loss: 1.337281  [  528/ 3200]\n",
      "loss: 1.365319  [  544/ 3200]\n",
      "loss: 1.203432  [  560/ 3200]\n",
      "loss: 1.250654  [  576/ 3200]\n",
      "loss: 1.130716  [  592/ 3200]\n",
      "loss: 1.200011  [  608/ 3200]\n",
      "loss: 1.222355  [  624/ 3200]\n",
      "loss: 1.358618  [  640/ 3200]\n",
      "loss: 1.344243  [  656/ 3200]\n",
      "loss: 1.291141  [  672/ 3200]\n",
      "loss: 1.413074  [  688/ 3200]\n",
      "loss: 1.306248  [  704/ 3200]\n",
      "loss: 1.135443  [  720/ 3200]\n",
      "loss: 1.484953  [  736/ 3200]\n",
      "loss: 1.168993  [  752/ 3200]\n",
      "loss: 1.326362  [  768/ 3200]\n",
      "loss: 1.324024  [  784/ 3200]\n",
      "loss: 1.203102  [  800/ 3200]\n",
      "loss: 1.295848  [  816/ 3200]\n",
      "loss: 1.310384  [  832/ 3200]\n",
      "loss: 1.213669  [  848/ 3200]\n",
      "loss: 1.297470  [  864/ 3200]\n",
      "loss: 1.267296  [  880/ 3200]\n",
      "loss: 1.190076  [  896/ 3200]\n",
      "loss: 1.411562  [  912/ 3200]\n",
      "loss: 1.230929  [  928/ 3200]\n",
      "loss: 1.217265  [  944/ 3200]\n",
      "loss: 1.259562  [  960/ 3200]\n",
      "loss: 1.238757  [  976/ 3200]\n",
      "loss: 1.219871  [  992/ 3200]\n",
      "loss: 1.251101  [ 1008/ 3200]\n",
      "loss: 1.234544  [ 1024/ 3200]\n",
      "loss: 1.408651  [ 1040/ 3200]\n",
      "loss: 1.191286  [ 1056/ 3200]\n",
      "loss: 1.013277  [ 1072/ 3200]\n",
      "loss: 1.117834  [ 1088/ 3200]\n",
      "loss: 1.278176  [ 1104/ 3200]\n",
      "loss: 1.267249  [ 1120/ 3200]\n",
      "loss: 1.159377  [ 1136/ 3200]\n",
      "loss: 1.421016  [ 1152/ 3200]\n",
      "loss: 1.312917  [ 1168/ 3200]\n",
      "loss: 1.308402  [ 1184/ 3200]\n",
      "loss: 1.375266  [ 1200/ 3200]\n",
      "loss: 1.136830  [ 1216/ 3200]\n",
      "loss: 1.245023  [ 1232/ 3200]\n",
      "loss: 1.325212  [ 1248/ 3200]\n",
      "loss: 1.205064  [ 1264/ 3200]\n",
      "loss: 1.359213  [ 1280/ 3200]\n",
      "loss: 1.045537  [ 1296/ 3200]\n",
      "loss: 1.298098  [ 1312/ 3200]\n",
      "loss: 1.241873  [ 1328/ 3200]\n",
      "loss: 1.075288  [ 1344/ 3200]\n",
      "loss: 1.260704  [ 1360/ 3200]\n",
      "loss: 1.406361  [ 1376/ 3200]\n",
      "loss: 1.237644  [ 1392/ 3200]\n",
      "loss: 1.299776  [ 1408/ 3200]\n",
      "loss: 1.102921  [ 1424/ 3200]\n",
      "loss: 1.275406  [ 1440/ 3200]\n",
      "loss: 1.288360  [ 1456/ 3200]\n",
      "loss: 1.218889  [ 1472/ 3200]\n",
      "loss: 1.289657  [ 1488/ 3200]\n",
      "loss: 1.256614  [ 1504/ 3200]\n",
      "loss: 1.082735  [ 1520/ 3200]\n",
      "loss: 1.173593  [ 1536/ 3200]\n",
      "loss: 1.069088  [ 1552/ 3200]\n",
      "loss: 1.178106  [ 1568/ 3200]\n",
      "loss: 1.454126  [ 1584/ 3200]\n",
      "loss: 1.414752  [ 1600/ 3200]\n",
      "loss: 1.235187  [ 1616/ 3200]\n",
      "loss: 1.259961  [ 1632/ 3200]\n",
      "loss: 1.268059  [ 1648/ 3200]\n",
      "loss: 1.343396  [ 1664/ 3200]\n",
      "loss: 1.333024  [ 1680/ 3200]\n",
      "loss: 1.272753  [ 1696/ 3200]\n",
      "loss: 1.150975  [ 1712/ 3200]\n",
      "loss: 1.257141  [ 1728/ 3200]\n",
      "loss: 1.428654  [ 1744/ 3200]\n",
      "loss: 1.440246  [ 1760/ 3200]\n",
      "loss: 1.212612  [ 1776/ 3200]\n",
      "loss: 1.255129  [ 1792/ 3200]\n",
      "loss: 1.249258  [ 1808/ 3200]\n",
      "loss: 1.319642  [ 1824/ 3200]\n",
      "loss: 1.173512  [ 1840/ 3200]\n",
      "loss: 1.121562  [ 1856/ 3200]\n",
      "loss: 1.220846  [ 1872/ 3200]\n",
      "loss: 1.372837  [ 1888/ 3200]\n",
      "loss: 1.356937  [ 1904/ 3200]\n",
      "loss: 1.287243  [ 1920/ 3200]\n",
      "loss: 1.184570  [ 1936/ 3200]\n",
      "loss: 1.246382  [ 1952/ 3200]\n",
      "loss: 1.077920  [ 1968/ 3200]\n",
      "loss: 1.219606  [ 1984/ 3200]\n",
      "loss: 1.068125  [ 2000/ 3200]\n",
      "loss: 0.992512  [ 2016/ 3200]\n",
      "loss: 1.063639  [ 2032/ 3200]\n",
      "loss: 1.100655  [ 2048/ 3200]\n",
      "loss: 1.434200  [ 2064/ 3200]\n",
      "loss: 1.378719  [ 2080/ 3200]\n",
      "loss: 1.357989  [ 2096/ 3200]\n",
      "loss: 1.343355  [ 2112/ 3200]\n",
      "loss: 1.276806  [ 2128/ 3200]\n",
      "loss: 1.216346  [ 2144/ 3200]\n",
      "loss: 1.342955  [ 2160/ 3200]\n",
      "loss: 1.394531  [ 2176/ 3200]\n",
      "loss: 1.303933  [ 2192/ 3200]\n",
      "loss: 1.185497  [ 2208/ 3200]\n",
      "loss: 1.158762  [ 2224/ 3200]\n",
      "loss: 1.034060  [ 2240/ 3200]\n",
      "loss: 1.375760  [ 2256/ 3200]\n",
      "loss: 1.246604  [ 2272/ 3200]\n",
      "loss: 1.224440  [ 2288/ 3200]\n",
      "loss: 1.127290  [ 2304/ 3200]\n",
      "loss: 1.184948  [ 2320/ 3200]\n",
      "loss: 1.033507  [ 2336/ 3200]\n",
      "loss: 1.147594  [ 2352/ 3200]\n",
      "loss: 1.192771  [ 2368/ 3200]\n",
      "loss: 1.199971  [ 2384/ 3200]\n",
      "loss: 1.421827  [ 2400/ 3200]\n",
      "loss: 1.197786  [ 2416/ 3200]\n",
      "loss: 1.288529  [ 2432/ 3200]\n",
      "loss: 1.221751  [ 2448/ 3200]\n",
      "loss: 1.312334  [ 2464/ 3200]\n",
      "loss: 1.266670  [ 2480/ 3200]\n",
      "loss: 1.241172  [ 2496/ 3200]\n",
      "loss: 1.199719  [ 2512/ 3200]\n",
      "loss: 1.224403  [ 2528/ 3200]\n",
      "loss: 1.143225  [ 2544/ 3200]\n",
      "loss: 1.088041  [ 2560/ 3200]\n",
      "loss: 1.300635  [ 2576/ 3200]\n",
      "loss: 1.155154  [ 2592/ 3200]\n",
      "loss: 1.298193  [ 2608/ 3200]\n",
      "loss: 1.115905  [ 2624/ 3200]\n",
      "loss: 1.159905  [ 2640/ 3200]\n",
      "loss: 1.468695  [ 2656/ 3200]\n",
      "loss: 1.233016  [ 2672/ 3200]\n",
      "loss: 1.187729  [ 2688/ 3200]\n",
      "loss: 1.118989  [ 2704/ 3200]\n",
      "loss: 0.996743  [ 2720/ 3200]\n",
      "loss: 1.085376  [ 2736/ 3200]\n",
      "loss: 1.164612  [ 2752/ 3200]\n",
      "loss: 1.047126  [ 2768/ 3200]\n",
      "loss: 1.189489  [ 2784/ 3200]\n",
      "loss: 1.233112  [ 2800/ 3200]\n",
      "loss: 1.087225  [ 2816/ 3200]\n",
      "loss: 0.995607  [ 2832/ 3200]\n",
      "loss: 0.929443  [ 2848/ 3200]\n",
      "loss: 0.961249  [ 2864/ 3200]\n",
      "loss: 1.267568  [ 2880/ 3200]\n",
      "loss: 1.076287  [ 2896/ 3200]\n",
      "loss: 1.217185  [ 2912/ 3200]\n",
      "loss: 1.060203  [ 2928/ 3200]\n",
      "loss: 0.999846  [ 2944/ 3200]\n",
      "loss: 0.994877  [ 2960/ 3200]\n",
      "loss: 1.051833  [ 2976/ 3200]\n",
      "loss: 1.255471  [ 2992/ 3200]\n",
      "loss: 1.187430  [ 3008/ 3200]\n",
      "loss: 1.041170  [ 3024/ 3200]\n",
      "loss: 0.982692  [ 3040/ 3200]\n",
      "loss: 1.252770  [ 3056/ 3200]\n",
      "loss: 1.101210  [ 3072/ 3200]\n",
      "loss: 1.247909  [ 3088/ 3200]\n",
      "loss: 1.975008  [ 3104/ 3200]\n",
      "loss: 1.388329  [ 3120/ 3200]\n",
      "loss: 1.389554  [ 3136/ 3200]\n",
      "loss: 1.372391  [ 3152/ 3200]\n",
      "loss: 1.364901  [ 3168/ 3200]\n",
      "loss: 1.352461  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.081094\n",
      "f1 macro averaged score: 0.383081\n",
      "Accuracy               : 45.5%\n",
      "Confusion matrix       :\n",
      "tensor([[ 47, 115,  38,   0],\n",
      "        [  4, 158,  38,   0],\n",
      "        [  0,  41, 159,   0],\n",
      "        [  0, 184,  16,   0]], device='cuda:0')\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 1.279006  [    0/ 3200]\n",
      "loss: 1.372917  [   16/ 3200]\n",
      "loss: 1.301108  [   32/ 3200]\n",
      "loss: 1.272429  [   48/ 3200]\n",
      "loss: 1.183910  [   64/ 3200]\n",
      "loss: 1.155614  [   80/ 3200]\n",
      "loss: 1.345711  [   96/ 3200]\n",
      "loss: 1.193663  [  112/ 3200]\n",
      "loss: 1.169064  [  128/ 3200]\n",
      "loss: 1.197806  [  144/ 3200]\n",
      "loss: 1.065846  [  160/ 3200]\n",
      "loss: 1.280159  [  176/ 3200]\n",
      "loss: 1.205371  [  192/ 3200]\n",
      "loss: 1.426390  [  208/ 3200]\n",
      "loss: 1.059776  [  224/ 3200]\n",
      "loss: 0.995542  [  240/ 3200]\n",
      "loss: 1.368537  [  256/ 3200]\n",
      "loss: 1.177236  [  272/ 3200]\n",
      "loss: 1.238176  [  288/ 3200]\n",
      "loss: 1.444846  [  304/ 3200]\n",
      "loss: 1.250147  [  320/ 3200]\n",
      "loss: 1.156418  [  336/ 3200]\n",
      "loss: 1.160327  [  352/ 3200]\n",
      "loss: 1.192542  [  368/ 3200]\n",
      "loss: 1.204839  [  384/ 3200]\n",
      "loss: 1.225392  [  400/ 3200]\n",
      "loss: 0.989532  [  416/ 3200]\n",
      "loss: 1.119215  [  432/ 3200]\n",
      "loss: 0.859515  [  448/ 3200]\n",
      "loss: 0.977562  [  464/ 3200]\n",
      "loss: 1.013334  [  480/ 3200]\n",
      "loss: 0.992536  [  496/ 3200]\n",
      "loss: 1.487007  [  512/ 3200]\n",
      "loss: 1.274442  [  528/ 3200]\n",
      "loss: 1.037568  [  544/ 3200]\n",
      "loss: 1.336745  [  560/ 3200]\n",
      "loss: 1.382849  [  576/ 3200]\n",
      "loss: 1.216278  [  592/ 3200]\n",
      "loss: 1.325848  [  608/ 3200]\n",
      "loss: 1.289794  [  624/ 3200]\n",
      "loss: 1.123580  [  640/ 3200]\n",
      "loss: 1.105384  [  656/ 3200]\n",
      "loss: 1.048982  [  672/ 3200]\n",
      "loss: 1.170003  [  688/ 3200]\n",
      "loss: 0.884310  [  704/ 3200]\n",
      "loss: 1.235849  [  720/ 3200]\n",
      "loss: 1.237249  [  736/ 3200]\n",
      "loss: 1.444568  [  752/ 3200]\n",
      "loss: 1.052791  [  768/ 3200]\n",
      "loss: 1.096565  [  784/ 3200]\n",
      "loss: 1.197972  [  800/ 3200]\n",
      "loss: 0.998876  [  816/ 3200]\n",
      "loss: 1.294479  [  832/ 3200]\n",
      "loss: 1.055334  [  848/ 3200]\n",
      "loss: 1.328757  [  864/ 3200]\n",
      "loss: 1.057766  [  880/ 3200]\n",
      "loss: 1.042066  [  896/ 3200]\n",
      "loss: 0.879498  [  912/ 3200]\n",
      "loss: 1.255338  [  928/ 3200]\n",
      "loss: 1.440072  [  944/ 3200]\n",
      "loss: 1.259475  [  960/ 3200]\n",
      "loss: 1.010576  [  976/ 3200]\n",
      "loss: 1.262829  [  992/ 3200]\n",
      "loss: 1.195683  [ 1008/ 3200]\n",
      "loss: 1.206855  [ 1024/ 3200]\n",
      "loss: 1.046156  [ 1040/ 3200]\n",
      "loss: 1.384824  [ 1056/ 3200]\n",
      "loss: 1.024733  [ 1072/ 3200]\n",
      "loss: 1.132415  [ 1088/ 3200]\n",
      "loss: 1.253109  [ 1104/ 3200]\n",
      "loss: 1.109260  [ 1120/ 3200]\n",
      "loss: 1.253345  [ 1136/ 3200]\n",
      "loss: 1.152211  [ 1152/ 3200]\n",
      "loss: 1.294366  [ 1168/ 3200]\n",
      "loss: 1.053620  [ 1184/ 3200]\n",
      "loss: 1.085481  [ 1200/ 3200]\n",
      "loss: 1.504588  [ 1216/ 3200]\n",
      "loss: 0.954923  [ 1232/ 3200]\n",
      "loss: 1.277411  [ 1248/ 3200]\n",
      "loss: 1.177933  [ 1264/ 3200]\n",
      "loss: 1.186117  [ 1280/ 3200]\n",
      "loss: 1.099874  [ 1296/ 3200]\n",
      "loss: 1.385627  [ 1312/ 3200]\n",
      "loss: 1.091957  [ 1328/ 3200]\n",
      "loss: 1.083651  [ 1344/ 3200]\n",
      "loss: 1.206535  [ 1360/ 3200]\n",
      "loss: 1.169593  [ 1376/ 3200]\n",
      "loss: 1.201808  [ 1392/ 3200]\n",
      "loss: 1.019341  [ 1408/ 3200]\n",
      "loss: 1.351801  [ 1424/ 3200]\n",
      "loss: 1.158630  [ 1440/ 3200]\n",
      "loss: 1.063548  [ 1456/ 3200]\n",
      "loss: 1.263529  [ 1472/ 3200]\n",
      "loss: 1.145699  [ 1488/ 3200]\n",
      "loss: 1.409981  [ 1504/ 3200]\n",
      "loss: 1.227203  [ 1520/ 3200]\n",
      "loss: 1.120685  [ 1536/ 3200]\n",
      "loss: 0.895631  [ 1552/ 3200]\n",
      "loss: 1.193541  [ 1568/ 3200]\n",
      "loss: 1.146084  [ 1584/ 3200]\n",
      "loss: 0.996543  [ 1600/ 3200]\n",
      "loss: 1.033531  [ 1616/ 3200]\n",
      "loss: 1.061650  [ 1632/ 3200]\n",
      "loss: 0.989889  [ 1648/ 3200]\n",
      "loss: 1.167369  [ 1664/ 3200]\n",
      "loss: 1.461269  [ 1680/ 3200]\n",
      "loss: 1.201778  [ 1696/ 3200]\n",
      "loss: 1.339233  [ 1712/ 3200]\n",
      "loss: 1.163624  [ 1728/ 3200]\n",
      "loss: 0.936566  [ 1744/ 3200]\n",
      "loss: 1.148407  [ 1760/ 3200]\n",
      "loss: 0.977636  [ 1776/ 3200]\n",
      "loss: 0.958339  [ 1792/ 3200]\n",
      "loss: 1.255032  [ 1808/ 3200]\n",
      "loss: 1.381462  [ 1824/ 3200]\n",
      "loss: 0.845158  [ 1840/ 3200]\n",
      "loss: 1.102524  [ 1856/ 3200]\n",
      "loss: 1.210631  [ 1872/ 3200]\n",
      "loss: 0.856285  [ 1888/ 3200]\n",
      "loss: 1.036741  [ 1904/ 3200]\n",
      "loss: 1.013200  [ 1920/ 3200]\n",
      "loss: 1.142787  [ 1936/ 3200]\n",
      "loss: 1.021952  [ 1952/ 3200]\n",
      "loss: 1.217927  [ 1968/ 3200]\n",
      "loss: 0.959462  [ 1984/ 3200]\n",
      "loss: 1.069808  [ 2000/ 3200]\n",
      "loss: 1.052263  [ 2016/ 3200]\n",
      "loss: 1.160581  [ 2032/ 3200]\n",
      "loss: 1.325727  [ 2048/ 3200]\n",
      "loss: 1.038752  [ 2064/ 3200]\n",
      "loss: 0.942498  [ 2080/ 3200]\n",
      "loss: 1.084765  [ 2096/ 3200]\n",
      "loss: 1.108608  [ 2112/ 3200]\n",
      "loss: 1.227690  [ 2128/ 3200]\n",
      "loss: 1.186975  [ 2144/ 3200]\n",
      "loss: 1.005539  [ 2160/ 3200]\n",
      "loss: 1.161150  [ 2176/ 3200]\n",
      "loss: 0.827740  [ 2192/ 3200]\n",
      "loss: 0.903034  [ 2208/ 3200]\n",
      "loss: 1.168573  [ 2224/ 3200]\n",
      "loss: 0.942436  [ 2240/ 3200]\n",
      "loss: 1.125284  [ 2256/ 3200]\n",
      "loss: 1.338684  [ 2272/ 3200]\n",
      "loss: 1.072523  [ 2288/ 3200]\n",
      "loss: 1.434683  [ 2304/ 3200]\n",
      "loss: 1.123428  [ 2320/ 3200]\n",
      "loss: 1.102091  [ 2336/ 3200]\n",
      "loss: 0.794579  [ 2352/ 3200]\n",
      "loss: 1.437444  [ 2368/ 3200]\n",
      "loss: 1.243991  [ 2384/ 3200]\n",
      "loss: 1.119942  [ 2400/ 3200]\n",
      "loss: 1.045678  [ 2416/ 3200]\n",
      "loss: 1.095397  [ 2432/ 3200]\n",
      "loss: 1.229778  [ 2448/ 3200]\n",
      "loss: 1.033871  [ 2464/ 3200]\n",
      "loss: 1.076014  [ 2480/ 3200]\n",
      "loss: 0.728058  [ 2496/ 3200]\n",
      "loss: 0.949559  [ 2512/ 3200]\n",
      "loss: 0.932216  [ 2528/ 3200]\n",
      "loss: 0.976538  [ 2544/ 3200]\n",
      "loss: 0.909983  [ 2560/ 3200]\n",
      "loss: 0.742666  [ 2576/ 3200]\n",
      "loss: 1.137084  [ 2592/ 3200]\n",
      "loss: 1.281467  [ 2608/ 3200]\n",
      "loss: 1.006878  [ 2624/ 3200]\n",
      "loss: 1.097685  [ 2640/ 3200]\n",
      "loss: 0.830101  [ 2656/ 3200]\n",
      "loss: 1.006902  [ 2672/ 3200]\n",
      "loss: 0.947201  [ 2688/ 3200]\n",
      "loss: 0.936059  [ 2704/ 3200]\n",
      "loss: 1.269690  [ 2720/ 3200]\n",
      "loss: 0.597446  [ 2736/ 3200]\n",
      "loss: 0.965734  [ 2752/ 3200]\n",
      "loss: 0.933179  [ 2768/ 3200]\n",
      "loss: 0.746197  [ 2784/ 3200]\n",
      "loss: 1.152240  [ 2800/ 3200]\n",
      "loss: 1.301503  [ 2816/ 3200]\n",
      "loss: 1.188319  [ 2832/ 3200]\n",
      "loss: 1.100519  [ 2848/ 3200]\n",
      "loss: 1.186040  [ 2864/ 3200]\n",
      "loss: 1.126661  [ 2880/ 3200]\n",
      "loss: 1.152963  [ 2896/ 3200]\n",
      "loss: 0.879074  [ 2912/ 3200]\n",
      "loss: 1.011685  [ 2928/ 3200]\n",
      "loss: 1.092114  [ 2944/ 3200]\n",
      "loss: 0.932673  [ 2960/ 3200]\n",
      "loss: 0.876891  [ 2976/ 3200]\n",
      "loss: 1.179596  [ 2992/ 3200]\n",
      "loss: 0.859787  [ 3008/ 3200]\n",
      "loss: 0.867812  [ 3024/ 3200]\n",
      "loss: 0.895931  [ 3040/ 3200]\n",
      "loss: 1.267825  [ 3056/ 3200]\n",
      "loss: 1.251885  [ 3072/ 3200]\n",
      "loss: 0.747135  [ 3088/ 3200]\n",
      "loss: 0.976647  [ 3104/ 3200]\n",
      "loss: 1.325829  [ 3120/ 3200]\n",
      "loss: 1.158063  [ 3136/ 3200]\n",
      "loss: 0.760015  [ 3152/ 3200]\n",
      "loss: 1.195547  [ 3168/ 3200]\n",
      "loss: 1.084091  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.063222\n",
      "f1 macro averaged score: 0.451277\n",
      "Accuracy               : 52.4%\n",
      "Confusion matrix       :\n",
      "tensor([[164,  21,  15,   0],\n",
      "        [ 41,  72,  87,   0],\n",
      "        [  6,  16, 178,   0],\n",
      "        [ 14,  91,  90,   5]], device='cuda:0')\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 0.939519  [    0/ 3200]\n",
      "loss: 1.138981  [   16/ 3200]\n",
      "loss: 1.115100  [   32/ 3200]\n",
      "loss: 0.842733  [   48/ 3200]\n",
      "loss: 1.009239  [   64/ 3200]\n",
      "loss: 0.887426  [   80/ 3200]\n",
      "loss: 0.994961  [   96/ 3200]\n",
      "loss: 0.946104  [  112/ 3200]\n",
      "loss: 0.851310  [  128/ 3200]\n",
      "loss: 1.120194  [  144/ 3200]\n",
      "loss: 1.034112  [  160/ 3200]\n",
      "loss: 0.933375  [  176/ 3200]\n",
      "loss: 0.992267  [  192/ 3200]\n",
      "loss: 0.960639  [  208/ 3200]\n",
      "loss: 1.013247  [  224/ 3200]\n",
      "loss: 1.041677  [  240/ 3200]\n",
      "loss: 1.006713  [  256/ 3200]\n",
      "loss: 1.194745  [  272/ 3200]\n",
      "loss: 1.137721  [  288/ 3200]\n",
      "loss: 1.297102  [  304/ 3200]\n",
      "loss: 1.202360  [  320/ 3200]\n",
      "loss: 1.141370  [  336/ 3200]\n",
      "loss: 1.062647  [  352/ 3200]\n",
      "loss: 0.926611  [  368/ 3200]\n",
      "loss: 1.001688  [  384/ 3200]\n",
      "loss: 1.057114  [  400/ 3200]\n",
      "loss: 0.969638  [  416/ 3200]\n",
      "loss: 1.073824  [  432/ 3200]\n",
      "loss: 0.982833  [  448/ 3200]\n",
      "loss: 1.051141  [  464/ 3200]\n",
      "loss: 1.145046  [  480/ 3200]\n",
      "loss: 1.389807  [  496/ 3200]\n",
      "loss: 1.392554  [  512/ 3200]\n",
      "loss: 1.068769  [  528/ 3200]\n",
      "loss: 1.112031  [  544/ 3200]\n",
      "loss: 1.133680  [  560/ 3200]\n",
      "loss: 1.130197  [  576/ 3200]\n",
      "loss: 0.909368  [  592/ 3200]\n",
      "loss: 0.974714  [  608/ 3200]\n",
      "loss: 1.048115  [  624/ 3200]\n",
      "loss: 1.240693  [  640/ 3200]\n",
      "loss: 1.102227  [  656/ 3200]\n",
      "loss: 0.856397  [  672/ 3200]\n",
      "loss: 1.088749  [  688/ 3200]\n",
      "loss: 0.979500  [  704/ 3200]\n",
      "loss: 0.986040  [  720/ 3200]\n",
      "loss: 1.008838  [  736/ 3200]\n",
      "loss: 0.915075  [  752/ 3200]\n",
      "loss: 1.100257  [  768/ 3200]\n",
      "loss: 0.958880  [  784/ 3200]\n",
      "loss: 1.043329  [  800/ 3200]\n",
      "loss: 1.077985  [  816/ 3200]\n",
      "loss: 0.879889  [  832/ 3200]\n",
      "loss: 1.062546  [  848/ 3200]\n",
      "loss: 1.096411  [  864/ 3200]\n",
      "loss: 1.202290  [  880/ 3200]\n",
      "loss: 1.113016  [  896/ 3200]\n",
      "loss: 1.124223  [  912/ 3200]\n",
      "loss: 0.964740  [  928/ 3200]\n",
      "loss: 1.059297  [  944/ 3200]\n",
      "loss: 1.037070  [  960/ 3200]\n",
      "loss: 0.912524  [  976/ 3200]\n",
      "loss: 1.184112  [  992/ 3200]\n",
      "loss: 1.276692  [ 1008/ 3200]\n",
      "loss: 1.179703  [ 1024/ 3200]\n",
      "loss: 0.866917  [ 1040/ 3200]\n",
      "loss: 0.801423  [ 1056/ 3200]\n",
      "loss: 1.330055  [ 1072/ 3200]\n",
      "loss: 0.993330  [ 1088/ 3200]\n",
      "loss: 1.008010  [ 1104/ 3200]\n",
      "loss: 0.861406  [ 1120/ 3200]\n",
      "loss: 0.882696  [ 1136/ 3200]\n",
      "loss: 1.356801  [ 1152/ 3200]\n",
      "loss: 1.214013  [ 1168/ 3200]\n",
      "loss: 0.893914  [ 1184/ 3200]\n",
      "loss: 0.822436  [ 1200/ 3200]\n",
      "loss: 1.005104  [ 1216/ 3200]\n",
      "loss: 1.040191  [ 1232/ 3200]\n",
      "loss: 0.921884  [ 1248/ 3200]\n",
      "loss: 1.091255  [ 1264/ 3200]\n",
      "loss: 1.036368  [ 1280/ 3200]\n",
      "loss: 0.992505  [ 1296/ 3200]\n",
      "loss: 1.194607  [ 1312/ 3200]\n",
      "loss: 0.905194  [ 1328/ 3200]\n",
      "loss: 1.012973  [ 1344/ 3200]\n",
      "loss: 1.014648  [ 1360/ 3200]\n",
      "loss: 0.963655  [ 1376/ 3200]\n",
      "loss: 0.906364  [ 1392/ 3200]\n",
      "loss: 1.133235  [ 1408/ 3200]\n",
      "loss: 0.837729  [ 1424/ 3200]\n",
      "loss: 1.089887  [ 1440/ 3200]\n",
      "loss: 0.708606  [ 1456/ 3200]\n",
      "loss: 0.941393  [ 1472/ 3200]\n",
      "loss: 0.876425  [ 1488/ 3200]\n",
      "loss: 1.063423  [ 1504/ 3200]\n",
      "loss: 1.009565  [ 1520/ 3200]\n",
      "loss: 0.915567  [ 1536/ 3200]\n",
      "loss: 1.006091  [ 1552/ 3200]\n",
      "loss: 1.039145  [ 1568/ 3200]\n",
      "loss: 1.299666  [ 1584/ 3200]\n",
      "loss: 0.949717  [ 1600/ 3200]\n",
      "loss: 1.134492  [ 1616/ 3200]\n",
      "loss: 0.798530  [ 1632/ 3200]\n",
      "loss: 0.812521  [ 1648/ 3200]\n",
      "loss: 1.158417  [ 1664/ 3200]\n",
      "loss: 0.904302  [ 1680/ 3200]\n",
      "loss: 0.975865  [ 1696/ 3200]\n",
      "loss: 1.088447  [ 1712/ 3200]\n",
      "loss: 1.203448  [ 1728/ 3200]\n",
      "loss: 1.064506  [ 1744/ 3200]\n",
      "loss: 0.919078  [ 1760/ 3200]\n",
      "loss: 0.970292  [ 1776/ 3200]\n",
      "loss: 1.021773  [ 1792/ 3200]\n",
      "loss: 1.054338  [ 1808/ 3200]\n",
      "loss: 1.477498  [ 1824/ 3200]\n",
      "loss: 1.124273  [ 1840/ 3200]\n",
      "loss: 0.978507  [ 1856/ 3200]\n",
      "loss: 0.627516  [ 1872/ 3200]\n",
      "loss: 0.740070  [ 1888/ 3200]\n",
      "loss: 0.752771  [ 1904/ 3200]\n",
      "loss: 0.782053  [ 1920/ 3200]\n",
      "loss: 0.726469  [ 1936/ 3200]\n",
      "loss: 1.059334  [ 1952/ 3200]\n",
      "loss: 0.849037  [ 1968/ 3200]\n",
      "loss: 0.919535  [ 1984/ 3200]\n",
      "loss: 0.993591  [ 2000/ 3200]\n",
      "loss: 1.372047  [ 2016/ 3200]\n",
      "loss: 0.818883  [ 2032/ 3200]\n",
      "loss: 0.891968  [ 2048/ 3200]\n",
      "loss: 1.013369  [ 2064/ 3200]\n",
      "loss: 1.177031  [ 2080/ 3200]\n",
      "loss: 1.237737  [ 2096/ 3200]\n",
      "loss: 0.885806  [ 2112/ 3200]\n",
      "loss: 0.924114  [ 2128/ 3200]\n",
      "loss: 0.786971  [ 2144/ 3200]\n",
      "loss: 1.129784  [ 2160/ 3200]\n",
      "loss: 0.940676  [ 2176/ 3200]\n",
      "loss: 0.800501  [ 2192/ 3200]\n",
      "loss: 0.854890  [ 2208/ 3200]\n",
      "loss: 0.950819  [ 2224/ 3200]\n",
      "loss: 0.972089  [ 2240/ 3200]\n",
      "loss: 1.066550  [ 2256/ 3200]\n",
      "loss: 1.020805  [ 2272/ 3200]\n",
      "loss: 1.084703  [ 2288/ 3200]\n",
      "loss: 0.755089  [ 2304/ 3200]\n",
      "loss: 0.956052  [ 2320/ 3200]\n",
      "loss: 0.792894  [ 2336/ 3200]\n",
      "loss: 1.185921  [ 2352/ 3200]\n",
      "loss: 0.724744  [ 2368/ 3200]\n",
      "loss: 0.999877  [ 2384/ 3200]\n",
      "loss: 1.221563  [ 2400/ 3200]\n",
      "loss: 1.038235  [ 2416/ 3200]\n",
      "loss: 1.174030  [ 2432/ 3200]\n",
      "loss: 0.903084  [ 2448/ 3200]\n",
      "loss: 0.689165  [ 2464/ 3200]\n",
      "loss: 0.921155  [ 2480/ 3200]\n",
      "loss: 0.757926  [ 2496/ 3200]\n",
      "loss: 0.607501  [ 2512/ 3200]\n",
      "loss: 0.660150  [ 2528/ 3200]\n",
      "loss: 0.837192  [ 2544/ 3200]\n",
      "loss: 0.993512  [ 2560/ 3200]\n",
      "loss: 0.974640  [ 2576/ 3200]\n",
      "loss: 1.033809  [ 2592/ 3200]\n",
      "loss: 0.645928  [ 2608/ 3200]\n",
      "loss: 1.184634  [ 2624/ 3200]\n",
      "loss: 1.258400  [ 2640/ 3200]\n",
      "loss: 0.811769  [ 2656/ 3200]\n",
      "loss: 0.867943  [ 2672/ 3200]\n",
      "loss: 1.231281  [ 2688/ 3200]\n",
      "loss: 0.847217  [ 2704/ 3200]\n",
      "loss: 0.801480  [ 2720/ 3200]\n",
      "loss: 1.101326  [ 2736/ 3200]\n",
      "loss: 1.128482  [ 2752/ 3200]\n",
      "loss: 0.967658  [ 2768/ 3200]\n",
      "loss: 0.734980  [ 2784/ 3200]\n",
      "loss: 0.922163  [ 2800/ 3200]\n",
      "loss: 0.757467  [ 2816/ 3200]\n",
      "loss: 0.904545  [ 2832/ 3200]\n",
      "loss: 0.822731  [ 2848/ 3200]\n",
      "loss: 0.982070  [ 2864/ 3200]\n",
      "loss: 0.767928  [ 2880/ 3200]\n",
      "loss: 1.071296  [ 2896/ 3200]\n",
      "loss: 1.028835  [ 2912/ 3200]\n",
      "loss: 0.716142  [ 2928/ 3200]\n",
      "loss: 0.998951  [ 2944/ 3200]\n",
      "loss: 1.326836  [ 2960/ 3200]\n",
      "loss: 0.894048  [ 2976/ 3200]\n",
      "loss: 0.979629  [ 2992/ 3200]\n",
      "loss: 0.965122  [ 3008/ 3200]\n",
      "loss: 1.192124  [ 3024/ 3200]\n",
      "loss: 0.872645  [ 3040/ 3200]\n",
      "loss: 1.206012  [ 3056/ 3200]\n",
      "loss: 1.062730  [ 3072/ 3200]\n",
      "loss: 0.882970  [ 3088/ 3200]\n",
      "loss: 1.221763  [ 3104/ 3200]\n",
      "loss: 0.772480  [ 3120/ 3200]\n",
      "loss: 0.909882  [ 3136/ 3200]\n",
      "loss: 1.270354  [ 3152/ 3200]\n",
      "loss: 2.233057  [ 3168/ 3200]\n",
      "loss: 0.932541  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.082727\n",
      "f1 macro averaged score: 0.394970\n",
      "Accuracy               : 46.6%\n",
      "Confusion matrix       :\n",
      "tensor([[  6, 143,  40,  11],\n",
      "        [  0,  40, 113,  47],\n",
      "        [  0,   2, 193,   5],\n",
      "        [  0,  15,  51, 134]], device='cuda:0')\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 1.737594  [    0/ 3200]\n",
      "loss: 1.166864  [   16/ 3200]\n",
      "loss: 1.033040  [   32/ 3200]\n",
      "loss: 1.012790  [   48/ 3200]\n",
      "loss: 0.976337  [   64/ 3200]\n",
      "loss: 0.878846  [   80/ 3200]\n",
      "loss: 0.837278  [   96/ 3200]\n",
      "loss: 1.240675  [  112/ 3200]\n",
      "loss: 0.860865  [  128/ 3200]\n",
      "loss: 0.873442  [  144/ 3200]\n",
      "loss: 0.880020  [  160/ 3200]\n",
      "loss: 0.827957  [  176/ 3200]\n",
      "loss: 0.704923  [  192/ 3200]\n",
      "loss: 0.982217  [  208/ 3200]\n",
      "loss: 0.967235  [  224/ 3200]\n",
      "loss: 1.073956  [  240/ 3200]\n",
      "loss: 0.804156  [  256/ 3200]\n",
      "loss: 0.767692  [  272/ 3200]\n",
      "loss: 0.823916  [  288/ 3200]\n",
      "loss: 0.853647  [  304/ 3200]\n",
      "loss: 1.209212  [  320/ 3200]\n",
      "loss: 1.073123  [  336/ 3200]\n",
      "loss: 0.698855  [  352/ 3200]\n",
      "loss: 0.968623  [  368/ 3200]\n",
      "loss: 1.017391  [  384/ 3200]\n",
      "loss: 0.849299  [  400/ 3200]\n",
      "loss: 1.104466  [  416/ 3200]\n",
      "loss: 0.741397  [  432/ 3200]\n",
      "loss: 0.700610  [  448/ 3200]\n",
      "loss: 0.961748  [  464/ 3200]\n",
      "loss: 1.035912  [  480/ 3200]\n",
      "loss: 0.773393  [  496/ 3200]\n",
      "loss: 1.025581  [  512/ 3200]\n",
      "loss: 0.503338  [  528/ 3200]\n",
      "loss: 0.817395  [  544/ 3200]\n",
      "loss: 1.056184  [  560/ 3200]\n",
      "loss: 0.916379  [  576/ 3200]\n",
      "loss: 1.123872  [  592/ 3200]\n",
      "loss: 0.998826  [  608/ 3200]\n",
      "loss: 0.737642  [  624/ 3200]\n",
      "loss: 0.913151  [  640/ 3200]\n",
      "loss: 1.050235  [  656/ 3200]\n",
      "loss: 0.692927  [  672/ 3200]\n",
      "loss: 0.850631  [  688/ 3200]\n",
      "loss: 0.914414  [  704/ 3200]\n",
      "loss: 0.877553  [  720/ 3200]\n",
      "loss: 0.974458  [  736/ 3200]\n",
      "loss: 0.834733  [  752/ 3200]\n",
      "loss: 0.826260  [  768/ 3200]\n",
      "loss: 0.970320  [  784/ 3200]\n",
      "loss: 0.538467  [  800/ 3200]\n",
      "loss: 0.581464  [  816/ 3200]\n",
      "loss: 1.092011  [  832/ 3200]\n",
      "loss: 0.789111  [  848/ 3200]\n",
      "loss: 0.893449  [  864/ 3200]\n",
      "loss: 1.029413  [  880/ 3200]\n",
      "loss: 0.689628  [  896/ 3200]\n",
      "loss: 0.936642  [  912/ 3200]\n",
      "loss: 0.834557  [  928/ 3200]\n",
      "loss: 1.168998  [  944/ 3200]\n",
      "loss: 1.057801  [  960/ 3200]\n",
      "loss: 0.608714  [  976/ 3200]\n",
      "loss: 1.237107  [  992/ 3200]\n",
      "loss: 0.821402  [ 1008/ 3200]\n",
      "loss: 1.342621  [ 1024/ 3200]\n",
      "loss: 0.949479  [ 1040/ 3200]\n",
      "loss: 0.907634  [ 1056/ 3200]\n",
      "loss: 0.790457  [ 1072/ 3200]\n",
      "loss: 1.211866  [ 1088/ 3200]\n",
      "loss: 0.701511  [ 1104/ 3200]\n",
      "loss: 0.860792  [ 1120/ 3200]\n",
      "loss: 0.557048  [ 1136/ 3200]\n",
      "loss: 1.096116  [ 1152/ 3200]\n",
      "loss: 0.939630  [ 1168/ 3200]\n",
      "loss: 0.965525  [ 1184/ 3200]\n",
      "loss: 0.989161  [ 1200/ 3200]\n",
      "loss: 0.506035  [ 1216/ 3200]\n",
      "loss: 0.986899  [ 1232/ 3200]\n",
      "loss: 1.113742  [ 1248/ 3200]\n",
      "loss: 0.795702  [ 1264/ 3200]\n",
      "loss: 0.825718  [ 1280/ 3200]\n",
      "loss: 0.876372  [ 1296/ 3200]\n",
      "loss: 0.801405  [ 1312/ 3200]\n",
      "loss: 0.944225  [ 1328/ 3200]\n",
      "loss: 1.072905  [ 1344/ 3200]\n",
      "loss: 0.640326  [ 1360/ 3200]\n",
      "loss: 0.460827  [ 1376/ 3200]\n",
      "loss: 0.756741  [ 1392/ 3200]\n",
      "loss: 0.917031  [ 1408/ 3200]\n",
      "loss: 0.654799  [ 1424/ 3200]\n",
      "loss: 0.988919  [ 1440/ 3200]\n",
      "loss: 1.086144  [ 1456/ 3200]\n",
      "loss: 0.902822  [ 1472/ 3200]\n",
      "loss: 0.831142  [ 1488/ 3200]\n",
      "loss: 0.693547  [ 1504/ 3200]\n",
      "loss: 1.355942  [ 1520/ 3200]\n",
      "loss: 0.872780  [ 1536/ 3200]\n",
      "loss: 1.066604  [ 1552/ 3200]\n",
      "loss: 0.938424  [ 1568/ 3200]\n",
      "loss: 1.173750  [ 1584/ 3200]\n",
      "loss: 0.969110  [ 1600/ 3200]\n",
      "loss: 1.159992  [ 1616/ 3200]\n",
      "loss: 1.127435  [ 1632/ 3200]\n",
      "loss: 0.887270  [ 1648/ 3200]\n",
      "loss: 0.854360  [ 1664/ 3200]\n",
      "loss: 0.865261  [ 1680/ 3200]\n",
      "loss: 1.179158  [ 1696/ 3200]\n",
      "loss: 1.023445  [ 1712/ 3200]\n",
      "loss: 0.781268  [ 1728/ 3200]\n",
      "loss: 0.881510  [ 1744/ 3200]\n",
      "loss: 0.942035  [ 1760/ 3200]\n",
      "loss: 0.995993  [ 1776/ 3200]\n",
      "loss: 0.777539  [ 1792/ 3200]\n",
      "loss: 1.117015  [ 1808/ 3200]\n",
      "loss: 0.760016  [ 1824/ 3200]\n",
      "loss: 0.999283  [ 1840/ 3200]\n",
      "loss: 1.497641  [ 1856/ 3200]\n",
      "loss: 0.969656  [ 1872/ 3200]\n",
      "loss: 0.744663  [ 1888/ 3200]\n",
      "loss: 0.694312  [ 1904/ 3200]\n",
      "loss: 0.622219  [ 1920/ 3200]\n",
      "loss: 0.710856  [ 1936/ 3200]\n",
      "loss: 0.925640  [ 1952/ 3200]\n",
      "loss: 0.702013  [ 1968/ 3200]\n",
      "loss: 0.325744  [ 1984/ 3200]\n",
      "loss: 1.003427  [ 2000/ 3200]\n",
      "loss: 0.811051  [ 2016/ 3200]\n",
      "loss: 1.284629  [ 2032/ 3200]\n",
      "loss: 0.993859  [ 2048/ 3200]\n",
      "loss: 0.886881  [ 2064/ 3200]\n",
      "loss: 0.959581  [ 2080/ 3200]\n",
      "loss: 1.166407  [ 2096/ 3200]\n",
      "loss: 1.048365  [ 2112/ 3200]\n",
      "loss: 0.614826  [ 2128/ 3200]\n",
      "loss: 1.021506  [ 2144/ 3200]\n",
      "loss: 1.062163  [ 2160/ 3200]\n",
      "loss: 0.907618  [ 2176/ 3200]\n",
      "loss: 0.866733  [ 2192/ 3200]\n",
      "loss: 0.690837  [ 2208/ 3200]\n",
      "loss: 0.818212  [ 2224/ 3200]\n",
      "loss: 0.943607  [ 2240/ 3200]\n",
      "loss: 0.857212  [ 2256/ 3200]\n",
      "loss: 0.896998  [ 2272/ 3200]\n",
      "loss: 0.781642  [ 2288/ 3200]\n",
      "loss: 0.670705  [ 2304/ 3200]\n",
      "loss: 0.684810  [ 2320/ 3200]\n",
      "loss: 1.155537  [ 2336/ 3200]\n",
      "loss: 0.741145  [ 2352/ 3200]\n",
      "loss: 0.912889  [ 2368/ 3200]\n",
      "loss: 1.038197  [ 2384/ 3200]\n",
      "loss: 0.951638  [ 2400/ 3200]\n",
      "loss: 1.050405  [ 2416/ 3200]\n",
      "loss: 0.988642  [ 2432/ 3200]\n",
      "loss: 1.004876  [ 2448/ 3200]\n",
      "loss: 1.368177  [ 2464/ 3200]\n",
      "loss: 1.167969  [ 2480/ 3200]\n",
      "loss: 0.715881  [ 2496/ 3200]\n",
      "loss: 0.691478  [ 2512/ 3200]\n",
      "loss: 0.713785  [ 2528/ 3200]\n",
      "loss: 0.902408  [ 2544/ 3200]\n",
      "loss: 0.646954  [ 2560/ 3200]\n",
      "loss: 0.787073  [ 2576/ 3200]\n",
      "loss: 0.544493  [ 2592/ 3200]\n",
      "loss: 0.934259  [ 2608/ 3200]\n",
      "loss: 0.728179  [ 2624/ 3200]\n",
      "loss: 0.709906  [ 2640/ 3200]\n",
      "loss: 1.191378  [ 2656/ 3200]\n",
      "loss: 0.872843  [ 2672/ 3200]\n",
      "loss: 0.726779  [ 2688/ 3200]\n",
      "loss: 1.068667  [ 2704/ 3200]\n",
      "loss: 0.960256  [ 2720/ 3200]\n",
      "loss: 1.030930  [ 2736/ 3200]\n",
      "loss: 0.978024  [ 2752/ 3200]\n",
      "loss: 1.168056  [ 2768/ 3200]\n",
      "loss: 0.849497  [ 2784/ 3200]\n",
      "loss: 1.136085  [ 2800/ 3200]\n",
      "loss: 0.942604  [ 2816/ 3200]\n",
      "loss: 1.092315  [ 2832/ 3200]\n",
      "loss: 0.799831  [ 2848/ 3200]\n",
      "loss: 1.198680  [ 2864/ 3200]\n",
      "loss: 0.908743  [ 2880/ 3200]\n",
      "loss: 0.779323  [ 2896/ 3200]\n",
      "loss: 0.941851  [ 2912/ 3200]\n",
      "loss: 0.862272  [ 2928/ 3200]\n",
      "loss: 0.553404  [ 2944/ 3200]\n",
      "loss: 0.725646  [ 2960/ 3200]\n",
      "loss: 0.593855  [ 2976/ 3200]\n",
      "loss: 0.636025  [ 2992/ 3200]\n",
      "loss: 1.049511  [ 3008/ 3200]\n",
      "loss: 0.819439  [ 3024/ 3200]\n",
      "loss: 1.523685  [ 3040/ 3200]\n",
      "loss: 1.119611  [ 3056/ 3200]\n",
      "loss: 0.699025  [ 3072/ 3200]\n",
      "loss: 0.730670  [ 3088/ 3200]\n",
      "loss: 1.181625  [ 3104/ 3200]\n",
      "loss: 0.877508  [ 3120/ 3200]\n",
      "loss: 0.443961  [ 3136/ 3200]\n",
      "loss: 0.876522  [ 3152/ 3200]\n",
      "loss: 0.703258  [ 3168/ 3200]\n",
      "loss: 0.909850  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.057760\n",
      "f1 macro averaged score: 0.587572\n",
      "Accuracy               : 61.5%\n",
      "Confusion matrix       :\n",
      "tensor([[193,   5,   0,   2],\n",
      "        [ 75,  54,   4,  67],\n",
      "        [ 29,  29,  90,  52],\n",
      "        [ 18,  21,   6, 155]], device='cuda:0')\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.984623  [    0/ 3200]\n",
      "loss: 0.783792  [   16/ 3200]\n",
      "loss: 0.853393  [   32/ 3200]\n",
      "loss: 0.534014  [   48/ 3200]\n",
      "loss: 0.740590  [   64/ 3200]\n",
      "loss: 1.032065  [   80/ 3200]\n",
      "loss: 0.958047  [   96/ 3200]\n",
      "loss: 1.135107  [  112/ 3200]\n",
      "loss: 0.900837  [  128/ 3200]\n",
      "loss: 0.683142  [  144/ 3200]\n",
      "loss: 0.813309  [  160/ 3200]\n",
      "loss: 0.851277  [  176/ 3200]\n",
      "loss: 1.392965  [  192/ 3200]\n",
      "loss: 0.840528  [  208/ 3200]\n",
      "loss: 0.644442  [  224/ 3200]\n",
      "loss: 0.736965  [  240/ 3200]\n",
      "loss: 0.768476  [  256/ 3200]\n",
      "loss: 0.600218  [  272/ 3200]\n",
      "loss: 0.655752  [  288/ 3200]\n",
      "loss: 0.676132  [  304/ 3200]\n",
      "loss: 0.908187  [  320/ 3200]\n",
      "loss: 1.301524  [  336/ 3200]\n",
      "loss: 1.025211  [  352/ 3200]\n",
      "loss: 0.705422  [  368/ 3200]\n",
      "loss: 0.664130  [  384/ 3200]\n",
      "loss: 0.890545  [  400/ 3200]\n",
      "loss: 0.871399  [  416/ 3200]\n",
      "loss: 0.712519  [  432/ 3200]\n",
      "loss: 0.979160  [  448/ 3200]\n",
      "loss: 0.794761  [  464/ 3200]\n",
      "loss: 1.137092  [  480/ 3200]\n",
      "loss: 1.085078  [  496/ 3200]\n",
      "loss: 0.969458  [  512/ 3200]\n",
      "loss: 0.979435  [  528/ 3200]\n",
      "loss: 0.894801  [  544/ 3200]\n",
      "loss: 0.824336  [  560/ 3200]\n",
      "loss: 0.968501  [  576/ 3200]\n",
      "loss: 0.891306  [  592/ 3200]\n",
      "loss: 0.914257  [  608/ 3200]\n",
      "loss: 0.763808  [  624/ 3200]\n",
      "loss: 0.584922  [  640/ 3200]\n",
      "loss: 0.899732  [  656/ 3200]\n",
      "loss: 0.727611  [  672/ 3200]\n",
      "loss: 0.831781  [  688/ 3200]\n",
      "loss: 0.902157  [  704/ 3200]\n",
      "loss: 0.837412  [  720/ 3200]\n",
      "loss: 0.844699  [  736/ 3200]\n",
      "loss: 1.230744  [  752/ 3200]\n",
      "loss: 0.700303  [  768/ 3200]\n",
      "loss: 0.696685  [  784/ 3200]\n",
      "loss: 0.959860  [  800/ 3200]\n",
      "loss: 0.746417  [  816/ 3200]\n",
      "loss: 0.648472  [  832/ 3200]\n",
      "loss: 0.649210  [  848/ 3200]\n",
      "loss: 0.587857  [  864/ 3200]\n",
      "loss: 1.004082  [  880/ 3200]\n",
      "loss: 0.846930  [  896/ 3200]\n",
      "loss: 0.751273  [  912/ 3200]\n",
      "loss: 0.869308  [  928/ 3200]\n",
      "loss: 0.538313  [  944/ 3200]\n",
      "loss: 0.724921  [  960/ 3200]\n",
      "loss: 0.597785  [  976/ 3200]\n",
      "loss: 0.540363  [  992/ 3200]\n",
      "loss: 0.940818  [ 1008/ 3200]\n",
      "loss: 0.550775  [ 1024/ 3200]\n",
      "loss: 0.660795  [ 1040/ 3200]\n",
      "loss: 1.043673  [ 1056/ 3200]\n",
      "loss: 1.275554  [ 1072/ 3200]\n",
      "loss: 0.960952  [ 1088/ 3200]\n",
      "loss: 0.795662  [ 1104/ 3200]\n",
      "loss: 0.856089  [ 1120/ 3200]\n",
      "loss: 0.698505  [ 1136/ 3200]\n",
      "loss: 0.752902  [ 1152/ 3200]\n",
      "loss: 0.803846  [ 1168/ 3200]\n",
      "loss: 0.703372  [ 1184/ 3200]\n",
      "loss: 0.638979  [ 1200/ 3200]\n",
      "loss: 0.665371  [ 1216/ 3200]\n",
      "loss: 0.874047  [ 1232/ 3200]\n",
      "loss: 0.747254  [ 1248/ 3200]\n",
      "loss: 0.687207  [ 1264/ 3200]\n",
      "loss: 0.999410  [ 1280/ 3200]\n",
      "loss: 1.033735  [ 1296/ 3200]\n",
      "loss: 0.832324  [ 1312/ 3200]\n",
      "loss: 0.874974  [ 1328/ 3200]\n",
      "loss: 0.929951  [ 1344/ 3200]\n",
      "loss: 0.710948  [ 1360/ 3200]\n",
      "loss: 0.924848  [ 1376/ 3200]\n",
      "loss: 0.681453  [ 1392/ 3200]\n",
      "loss: 0.896928  [ 1408/ 3200]\n",
      "loss: 0.606222  [ 1424/ 3200]\n",
      "loss: 0.847781  [ 1440/ 3200]\n",
      "loss: 0.629864  [ 1456/ 3200]\n",
      "loss: 0.821431  [ 1472/ 3200]\n",
      "loss: 0.950584  [ 1488/ 3200]\n",
      "loss: 1.029566  [ 1504/ 3200]\n",
      "loss: 0.579187  [ 1520/ 3200]\n",
      "loss: 0.685646  [ 1536/ 3200]\n",
      "loss: 0.881652  [ 1552/ 3200]\n",
      "loss: 1.241020  [ 1568/ 3200]\n",
      "loss: 1.122245  [ 1584/ 3200]\n",
      "loss: 0.889427  [ 1600/ 3200]\n",
      "loss: 0.825558  [ 1616/ 3200]\n",
      "loss: 0.955739  [ 1632/ 3200]\n",
      "loss: 0.628524  [ 1648/ 3200]\n",
      "loss: 0.766392  [ 1664/ 3200]\n",
      "loss: 1.050165  [ 1680/ 3200]\n",
      "loss: 0.691960  [ 1696/ 3200]\n",
      "loss: 0.815791  [ 1712/ 3200]\n",
      "loss: 0.707562  [ 1728/ 3200]\n",
      "loss: 0.981048  [ 1744/ 3200]\n",
      "loss: 0.766698  [ 1760/ 3200]\n",
      "loss: 0.754600  [ 1776/ 3200]\n",
      "loss: 0.854443  [ 1792/ 3200]\n",
      "loss: 1.137321  [ 1808/ 3200]\n",
      "loss: 0.859296  [ 1824/ 3200]\n",
      "loss: 0.965547  [ 1840/ 3200]\n",
      "loss: 0.669296  [ 1856/ 3200]\n",
      "loss: 0.972070  [ 1872/ 3200]\n",
      "loss: 0.772598  [ 1888/ 3200]\n",
      "loss: 0.902974  [ 1904/ 3200]\n",
      "loss: 0.856439  [ 1920/ 3200]\n",
      "loss: 0.956870  [ 1936/ 3200]\n",
      "loss: 0.946754  [ 1952/ 3200]\n",
      "loss: 0.526314  [ 1968/ 3200]\n",
      "loss: 0.713788  [ 1984/ 3200]\n",
      "loss: 0.905781  [ 2000/ 3200]\n",
      "loss: 0.789005  [ 2016/ 3200]\n",
      "loss: 0.586636  [ 2032/ 3200]\n",
      "loss: 0.585481  [ 2048/ 3200]\n",
      "loss: 0.537073  [ 2064/ 3200]\n",
      "loss: 1.207376  [ 2080/ 3200]\n",
      "loss: 0.998176  [ 2096/ 3200]\n",
      "loss: 1.041656  [ 2112/ 3200]\n",
      "loss: 0.684458  [ 2128/ 3200]\n",
      "loss: 0.768360  [ 2144/ 3200]\n",
      "loss: 0.719907  [ 2160/ 3200]\n",
      "loss: 1.064255  [ 2176/ 3200]\n",
      "loss: 0.734130  [ 2192/ 3200]\n",
      "loss: 0.658053  [ 2208/ 3200]\n",
      "loss: 1.062896  [ 2224/ 3200]\n",
      "loss: 1.110797  [ 2240/ 3200]\n",
      "loss: 0.912463  [ 2256/ 3200]\n",
      "loss: 0.776259  [ 2272/ 3200]\n",
      "loss: 0.694232  [ 2288/ 3200]\n",
      "loss: 0.770696  [ 2304/ 3200]\n",
      "loss: 0.842294  [ 2320/ 3200]\n",
      "loss: 1.094474  [ 2336/ 3200]\n",
      "loss: 0.835576  [ 2352/ 3200]\n",
      "loss: 0.543610  [ 2368/ 3200]\n",
      "loss: 0.593366  [ 2384/ 3200]\n",
      "loss: 0.774266  [ 2400/ 3200]\n",
      "loss: 1.168034  [ 2416/ 3200]\n",
      "loss: 0.916539  [ 2432/ 3200]\n",
      "loss: 0.917581  [ 2448/ 3200]\n",
      "loss: 0.912154  [ 2464/ 3200]\n",
      "loss: 1.094412  [ 2480/ 3200]\n",
      "loss: 0.421818  [ 2496/ 3200]\n",
      "loss: 0.543028  [ 2512/ 3200]\n",
      "loss: 0.778783  [ 2528/ 3200]\n",
      "loss: 0.978203  [ 2544/ 3200]\n",
      "loss: 0.839457  [ 2560/ 3200]\n",
      "loss: 0.686745  [ 2576/ 3200]\n",
      "loss: 0.620476  [ 2592/ 3200]\n",
      "loss: 0.777370  [ 2608/ 3200]\n",
      "loss: 0.966599  [ 2624/ 3200]\n",
      "loss: 0.908808  [ 2640/ 3200]\n",
      "loss: 0.737817  [ 2656/ 3200]\n",
      "loss: 0.687447  [ 2672/ 3200]\n",
      "loss: 0.710322  [ 2688/ 3200]\n",
      "loss: 0.639772  [ 2704/ 3200]\n",
      "loss: 0.951958  [ 2720/ 3200]\n",
      "loss: 0.745798  [ 2736/ 3200]\n",
      "loss: 0.998990  [ 2752/ 3200]\n",
      "loss: 0.707534  [ 2768/ 3200]\n",
      "loss: 0.758399  [ 2784/ 3200]\n",
      "loss: 0.947260  [ 2800/ 3200]\n",
      "loss: 0.779089  [ 2816/ 3200]\n",
      "loss: 0.645200  [ 2832/ 3200]\n",
      "loss: 0.840635  [ 2848/ 3200]\n",
      "loss: 0.717211  [ 2864/ 3200]\n",
      "loss: 0.714020  [ 2880/ 3200]\n",
      "loss: 0.781982  [ 2896/ 3200]\n",
      "loss: 0.824093  [ 2912/ 3200]\n",
      "loss: 1.169023  [ 2928/ 3200]\n",
      "loss: 0.833225  [ 2944/ 3200]\n",
      "loss: 0.707209  [ 2960/ 3200]\n",
      "loss: 1.072267  [ 2976/ 3200]\n",
      "loss: 0.945932  [ 2992/ 3200]\n",
      "loss: 1.000214  [ 3008/ 3200]\n",
      "loss: 0.659035  [ 3024/ 3200]\n",
      "loss: 0.473763  [ 3040/ 3200]\n",
      "loss: 0.866706  [ 3056/ 3200]\n",
      "loss: 1.166793  [ 3072/ 3200]\n",
      "loss: 0.770388  [ 3088/ 3200]\n",
      "loss: 1.067247  [ 3104/ 3200]\n",
      "loss: 0.722802  [ 3120/ 3200]\n",
      "loss: 0.727033  [ 3136/ 3200]\n",
      "loss: 0.825256  [ 3152/ 3200]\n",
      "loss: 0.927017  [ 3168/ 3200]\n",
      "loss: 0.758002  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.063246\n",
      "f1 macro averaged score: 0.531396\n",
      "Accuracy               : 54.9%\n",
      "Confusion matrix       :\n",
      "tensor([[136,  31,   0,  33],\n",
      "        [ 12,  41,   3, 144],\n",
      "        [  0,  10,  68, 122],\n",
      "        [  2,   4,   0, 194]], device='cuda:0')\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 1.084719  [    0/ 3200]\n",
      "loss: 0.950064  [   16/ 3200]\n",
      "loss: 0.920093  [   32/ 3200]\n",
      "loss: 0.647662  [   48/ 3200]\n",
      "loss: 0.667832  [   64/ 3200]\n",
      "loss: 1.111172  [   80/ 3200]\n",
      "loss: 0.873377  [   96/ 3200]\n",
      "loss: 0.834647  [  112/ 3200]\n",
      "loss: 0.813992  [  128/ 3200]\n",
      "loss: 0.699403  [  144/ 3200]\n",
      "loss: 0.648858  [  160/ 3200]\n",
      "loss: 0.678605  [  176/ 3200]\n",
      "loss: 0.500610  [  192/ 3200]\n",
      "loss: 0.585695  [  208/ 3200]\n",
      "loss: 0.707424  [  224/ 3200]\n",
      "loss: 1.255535  [  240/ 3200]\n",
      "loss: 0.571925  [  256/ 3200]\n",
      "loss: 0.877608  [  272/ 3200]\n",
      "loss: 0.568629  [  288/ 3200]\n",
      "loss: 0.783509  [  304/ 3200]\n",
      "loss: 0.733054  [  320/ 3200]\n",
      "loss: 0.704572  [  336/ 3200]\n",
      "loss: 0.916008  [  352/ 3200]\n",
      "loss: 0.867958  [  368/ 3200]\n",
      "loss: 0.635382  [  384/ 3200]\n",
      "loss: 0.932591  [  400/ 3200]\n",
      "loss: 1.063787  [  416/ 3200]\n",
      "loss: 0.658772  [  432/ 3200]\n",
      "loss: 0.915156  [  448/ 3200]\n",
      "loss: 1.420324  [  464/ 3200]\n",
      "loss: 1.011190  [  480/ 3200]\n",
      "loss: 0.798029  [  496/ 3200]\n",
      "loss: 0.691280  [  512/ 3200]\n",
      "loss: 0.584397  [  528/ 3200]\n",
      "loss: 0.850037  [  544/ 3200]\n",
      "loss: 0.714825  [  560/ 3200]\n",
      "loss: 0.977999  [  576/ 3200]\n",
      "loss: 0.657625  [  592/ 3200]\n",
      "loss: 0.689971  [  608/ 3200]\n",
      "loss: 0.994374  [  624/ 3200]\n",
      "loss: 0.488485  [  640/ 3200]\n",
      "loss: 0.974538  [  656/ 3200]\n",
      "loss: 0.835565  [  672/ 3200]\n",
      "loss: 0.783344  [  688/ 3200]\n",
      "loss: 0.801097  [  704/ 3200]\n",
      "loss: 0.844598  [  720/ 3200]\n",
      "loss: 0.687232  [  736/ 3200]\n",
      "loss: 0.884827  [  752/ 3200]\n",
      "loss: 0.660962  [  768/ 3200]\n",
      "loss: 0.900491  [  784/ 3200]\n",
      "loss: 0.756360  [  800/ 3200]\n",
      "loss: 0.808247  [  816/ 3200]\n",
      "loss: 0.859970  [  832/ 3200]\n",
      "loss: 1.051188  [  848/ 3200]\n",
      "loss: 0.450134  [  864/ 3200]\n",
      "loss: 0.684624  [  880/ 3200]\n",
      "loss: 0.944516  [  896/ 3200]\n",
      "loss: 0.908629  [  912/ 3200]\n",
      "loss: 0.898818  [  928/ 3200]\n",
      "loss: 0.729135  [  944/ 3200]\n",
      "loss: 0.635441  [  960/ 3200]\n",
      "loss: 0.569018  [  976/ 3200]\n",
      "loss: 0.606118  [  992/ 3200]\n",
      "loss: 0.657258  [ 1008/ 3200]\n",
      "loss: 0.988870  [ 1024/ 3200]\n",
      "loss: 0.644495  [ 1040/ 3200]\n",
      "loss: 0.888841  [ 1056/ 3200]\n",
      "loss: 0.608070  [ 1072/ 3200]\n",
      "loss: 0.677824  [ 1088/ 3200]\n",
      "loss: 0.725681  [ 1104/ 3200]\n",
      "loss: 0.674893  [ 1120/ 3200]\n",
      "loss: 0.930678  [ 1136/ 3200]\n",
      "loss: 0.929678  [ 1152/ 3200]\n",
      "loss: 0.850719  [ 1168/ 3200]\n",
      "loss: 0.631720  [ 1184/ 3200]\n",
      "loss: 0.783194  [ 1200/ 3200]\n",
      "loss: 0.973203  [ 1216/ 3200]\n",
      "loss: 0.875025  [ 1232/ 3200]\n",
      "loss: 0.968410  [ 1248/ 3200]\n",
      "loss: 0.833486  [ 1264/ 3200]\n",
      "loss: 0.432920  [ 1280/ 3200]\n",
      "loss: 0.609034  [ 1296/ 3200]\n",
      "loss: 0.817178  [ 1312/ 3200]\n",
      "loss: 0.795552  [ 1328/ 3200]\n",
      "loss: 0.568167  [ 1344/ 3200]\n",
      "loss: 1.241060  [ 1360/ 3200]\n",
      "loss: 0.966161  [ 1376/ 3200]\n",
      "loss: 0.767952  [ 1392/ 3200]\n",
      "loss: 0.796142  [ 1408/ 3200]\n",
      "loss: 0.523298  [ 1424/ 3200]\n",
      "loss: 0.999929  [ 1440/ 3200]\n",
      "loss: 1.117958  [ 1456/ 3200]\n",
      "loss: 0.794837  [ 1472/ 3200]\n",
      "loss: 1.043436  [ 1488/ 3200]\n",
      "loss: 0.856105  [ 1504/ 3200]\n",
      "loss: 0.723738  [ 1520/ 3200]\n",
      "loss: 0.994946  [ 1536/ 3200]\n",
      "loss: 0.779006  [ 1552/ 3200]\n",
      "loss: 0.652710  [ 1568/ 3200]\n",
      "loss: 0.718543  [ 1584/ 3200]\n",
      "loss: 0.577777  [ 1600/ 3200]\n",
      "loss: 0.563993  [ 1616/ 3200]\n",
      "loss: 0.866648  [ 1632/ 3200]\n",
      "loss: 0.688242  [ 1648/ 3200]\n",
      "loss: 0.745613  [ 1664/ 3200]\n",
      "loss: 0.540477  [ 1680/ 3200]\n",
      "loss: 0.759216  [ 1696/ 3200]\n",
      "loss: 0.816849  [ 1712/ 3200]\n",
      "loss: 1.008337  [ 1728/ 3200]\n",
      "loss: 0.986602  [ 1744/ 3200]\n",
      "loss: 0.590979  [ 1760/ 3200]\n",
      "loss: 0.915824  [ 1776/ 3200]\n",
      "loss: 0.595946  [ 1792/ 3200]\n",
      "loss: 0.631383  [ 1808/ 3200]\n",
      "loss: 1.071497  [ 1824/ 3200]\n",
      "loss: 0.838898  [ 1840/ 3200]\n",
      "loss: 0.638841  [ 1856/ 3200]\n",
      "loss: 0.966628  [ 1872/ 3200]\n",
      "loss: 0.445809  [ 1888/ 3200]\n",
      "loss: 0.744827  [ 1904/ 3200]\n",
      "loss: 1.225604  [ 1920/ 3200]\n",
      "loss: 0.691664  [ 1936/ 3200]\n",
      "loss: 0.901364  [ 1952/ 3200]\n",
      "loss: 0.482411  [ 1968/ 3200]\n",
      "loss: 0.675359  [ 1984/ 3200]\n",
      "loss: 1.099433  [ 2000/ 3200]\n",
      "loss: 0.681796  [ 2016/ 3200]\n",
      "loss: 0.650529  [ 2032/ 3200]\n",
      "loss: 0.664006  [ 2048/ 3200]\n",
      "loss: 0.986845  [ 2064/ 3200]\n",
      "loss: 0.896073  [ 2080/ 3200]\n",
      "loss: 1.525020  [ 2096/ 3200]\n",
      "loss: 0.988302  [ 2112/ 3200]\n",
      "loss: 0.908245  [ 2128/ 3200]\n",
      "loss: 0.692594  [ 2144/ 3200]\n",
      "loss: 0.661120  [ 2160/ 3200]\n",
      "loss: 0.499398  [ 2176/ 3200]\n",
      "loss: 0.795269  [ 2192/ 3200]\n",
      "loss: 0.726990  [ 2208/ 3200]\n",
      "loss: 0.602851  [ 2224/ 3200]\n",
      "loss: 0.851445  [ 2240/ 3200]\n",
      "loss: 0.705734  [ 2256/ 3200]\n",
      "loss: 1.209528  [ 2272/ 3200]\n",
      "loss: 0.815988  [ 2288/ 3200]\n",
      "loss: 0.683972  [ 2304/ 3200]\n",
      "loss: 0.818841  [ 2320/ 3200]\n",
      "loss: 0.821412  [ 2336/ 3200]\n",
      "loss: 0.717992  [ 2352/ 3200]\n",
      "loss: 0.779129  [ 2368/ 3200]\n",
      "loss: 0.758553  [ 2384/ 3200]\n",
      "loss: 0.761158  [ 2400/ 3200]\n",
      "loss: 0.685986  [ 2416/ 3200]\n",
      "loss: 0.682404  [ 2432/ 3200]\n",
      "loss: 0.739797  [ 2448/ 3200]\n",
      "loss: 0.517047  [ 2464/ 3200]\n",
      "loss: 0.542512  [ 2480/ 3200]\n",
      "loss: 0.603010  [ 2496/ 3200]\n",
      "loss: 1.025362  [ 2512/ 3200]\n",
      "loss: 0.739203  [ 2528/ 3200]\n",
      "loss: 0.733610  [ 2544/ 3200]\n",
      "loss: 0.642128  [ 2560/ 3200]\n",
      "loss: 0.995186  [ 2576/ 3200]\n",
      "loss: 0.829928  [ 2592/ 3200]\n",
      "loss: 0.720632  [ 2608/ 3200]\n",
      "loss: 0.970918  [ 2624/ 3200]\n",
      "loss: 0.663081  [ 2640/ 3200]\n",
      "loss: 0.779495  [ 2656/ 3200]\n",
      "loss: 0.838017  [ 2672/ 3200]\n",
      "loss: 1.229795  [ 2688/ 3200]\n",
      "loss: 0.895054  [ 2704/ 3200]\n",
      "loss: 0.758699  [ 2720/ 3200]\n",
      "loss: 0.878674  [ 2736/ 3200]\n",
      "loss: 1.252711  [ 2752/ 3200]\n",
      "loss: 0.724000  [ 2768/ 3200]\n",
      "loss: 0.931710  [ 2784/ 3200]\n",
      "loss: 0.514529  [ 2800/ 3200]\n",
      "loss: 0.555137  [ 2816/ 3200]\n",
      "loss: 0.709060  [ 2832/ 3200]\n",
      "loss: 1.288559  [ 2848/ 3200]\n",
      "loss: 0.996123  [ 2864/ 3200]\n",
      "loss: 0.724137  [ 2880/ 3200]\n",
      "loss: 0.504087  [ 2896/ 3200]\n",
      "loss: 0.742011  [ 2912/ 3200]\n",
      "loss: 0.837563  [ 2928/ 3200]\n",
      "loss: 0.517492  [ 2944/ 3200]\n",
      "loss: 0.718070  [ 2960/ 3200]\n",
      "loss: 0.661095  [ 2976/ 3200]\n",
      "loss: 0.819210  [ 2992/ 3200]\n",
      "loss: 0.832574  [ 3008/ 3200]\n",
      "loss: 0.654825  [ 3024/ 3200]\n",
      "loss: 1.074888  [ 3040/ 3200]\n",
      "loss: 0.651824  [ 3056/ 3200]\n",
      "loss: 0.962982  [ 3072/ 3200]\n",
      "loss: 0.652132  [ 3088/ 3200]\n",
      "loss: 0.766463  [ 3104/ 3200]\n",
      "loss: 1.092171  [ 3120/ 3200]\n",
      "loss: 1.171064  [ 3136/ 3200]\n",
      "loss: 0.912690  [ 3152/ 3200]\n",
      "loss: 0.734518  [ 3168/ 3200]\n",
      "loss: 0.572582  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.046386\n",
      "f1 macro averaged score: 0.716598\n",
      "Accuracy               : 70.5%\n",
      "Confusion matrix       :\n",
      "tensor([[159,  39,   0,   2],\n",
      "        [ 29, 150,  11,  10],\n",
      "        [  1,  65, 131,   3],\n",
      "        [  3,  62,  11, 124]], device='cuda:0')\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 0.626909  [    0/ 3200]\n",
      "loss: 0.919144  [   16/ 3200]\n",
      "loss: 0.761977  [   32/ 3200]\n",
      "loss: 0.459346  [   48/ 3200]\n",
      "loss: 0.903678  [   64/ 3200]\n",
      "loss: 1.205709  [   80/ 3200]\n",
      "loss: 0.944511  [   96/ 3200]\n",
      "loss: 0.625688  [  112/ 3200]\n",
      "loss: 0.684001  [  128/ 3200]\n",
      "loss: 0.460490  [  144/ 3200]\n",
      "loss: 0.638193  [  160/ 3200]\n",
      "loss: 0.717183  [  176/ 3200]\n",
      "loss: 0.449509  [  192/ 3200]\n",
      "loss: 1.009299  [  208/ 3200]\n",
      "loss: 0.759179  [  224/ 3200]\n",
      "loss: 0.681109  [  240/ 3200]\n",
      "loss: 0.640007  [  256/ 3200]\n",
      "loss: 0.894911  [  272/ 3200]\n",
      "loss: 1.145738  [  288/ 3200]\n",
      "loss: 1.106573  [  304/ 3200]\n",
      "loss: 1.230240  [  320/ 3200]\n",
      "loss: 0.702250  [  336/ 3200]\n",
      "loss: 0.756814  [  352/ 3200]\n",
      "loss: 0.825824  [  368/ 3200]\n",
      "loss: 0.961567  [  384/ 3200]\n",
      "loss: 0.775190  [  400/ 3200]\n",
      "loss: 0.815280  [  416/ 3200]\n",
      "loss: 0.643531  [  432/ 3200]\n",
      "loss: 0.880999  [  448/ 3200]\n",
      "loss: 0.684087  [  464/ 3200]\n",
      "loss: 0.632831  [  480/ 3200]\n",
      "loss: 0.757301  [  496/ 3200]\n",
      "loss: 0.714605  [  512/ 3200]\n",
      "loss: 0.637306  [  528/ 3200]\n",
      "loss: 0.806570  [  544/ 3200]\n",
      "loss: 1.056217  [  560/ 3200]\n",
      "loss: 0.610789  [  576/ 3200]\n",
      "loss: 0.625626  [  592/ 3200]\n",
      "loss: 0.949461  [  608/ 3200]\n",
      "loss: 0.389642  [  624/ 3200]\n",
      "loss: 0.463532  [  640/ 3200]\n",
      "loss: 0.811065  [  656/ 3200]\n",
      "loss: 0.772280  [  672/ 3200]\n",
      "loss: 0.866504  [  688/ 3200]\n",
      "loss: 1.471024  [  704/ 3200]\n",
      "loss: 0.763122  [  720/ 3200]\n",
      "loss: 0.671686  [  736/ 3200]\n",
      "loss: 0.771379  [  752/ 3200]\n",
      "loss: 0.523883  [  768/ 3200]\n",
      "loss: 0.480032  [  784/ 3200]\n",
      "loss: 0.440545  [  800/ 3200]\n",
      "loss: 0.907409  [  816/ 3200]\n",
      "loss: 0.804736  [  832/ 3200]\n",
      "loss: 1.043023  [  848/ 3200]\n",
      "loss: 0.486430  [  864/ 3200]\n",
      "loss: 0.601324  [  880/ 3200]\n",
      "loss: 0.780888  [  896/ 3200]\n",
      "loss: 0.612509  [  912/ 3200]\n",
      "loss: 0.446284  [  928/ 3200]\n",
      "loss: 0.714815  [  944/ 3200]\n",
      "loss: 0.699611  [  960/ 3200]\n",
      "loss: 0.718165  [  976/ 3200]\n",
      "loss: 0.655688  [  992/ 3200]\n",
      "loss: 0.698138  [ 1008/ 3200]\n",
      "loss: 0.291002  [ 1024/ 3200]\n",
      "loss: 0.591885  [ 1040/ 3200]\n",
      "loss: 0.771884  [ 1056/ 3200]\n",
      "loss: 0.577126  [ 1072/ 3200]\n",
      "loss: 0.716908  [ 1088/ 3200]\n",
      "loss: 0.644355  [ 1104/ 3200]\n",
      "loss: 0.627109  [ 1120/ 3200]\n",
      "loss: 1.858571  [ 1136/ 3200]\n",
      "loss: 1.586748  [ 1152/ 3200]\n",
      "loss: 0.780425  [ 1168/ 3200]\n",
      "loss: 0.989530  [ 1184/ 3200]\n",
      "loss: 0.936828  [ 1200/ 3200]\n",
      "loss: 0.928113  [ 1216/ 3200]\n",
      "loss: 0.707698  [ 1232/ 3200]\n",
      "loss: 1.049860  [ 1248/ 3200]\n",
      "loss: 0.583919  [ 1264/ 3200]\n",
      "loss: 0.658306  [ 1280/ 3200]\n",
      "loss: 0.581230  [ 1296/ 3200]\n",
      "loss: 0.824823  [ 1312/ 3200]\n",
      "loss: 0.657578  [ 1328/ 3200]\n",
      "loss: 0.838709  [ 1344/ 3200]\n",
      "loss: 0.743485  [ 1360/ 3200]\n",
      "loss: 0.528282  [ 1376/ 3200]\n",
      "loss: 0.803184  [ 1392/ 3200]\n",
      "loss: 0.756871  [ 1408/ 3200]\n",
      "loss: 0.601121  [ 1424/ 3200]\n",
      "loss: 0.657606  [ 1440/ 3200]\n",
      "loss: 0.667808  [ 1456/ 3200]\n",
      "loss: 0.473470  [ 1472/ 3200]\n",
      "loss: 0.486224  [ 1488/ 3200]\n",
      "loss: 0.761152  [ 1504/ 3200]\n",
      "loss: 0.498894  [ 1520/ 3200]\n",
      "loss: 0.651585  [ 1536/ 3200]\n",
      "loss: 0.987957  [ 1552/ 3200]\n",
      "loss: 0.789700  [ 1568/ 3200]\n",
      "loss: 0.772843  [ 1584/ 3200]\n",
      "loss: 0.730862  [ 1600/ 3200]\n",
      "loss: 1.235030  [ 1616/ 3200]\n",
      "loss: 0.738683  [ 1632/ 3200]\n",
      "loss: 0.611693  [ 1648/ 3200]\n",
      "loss: 0.572814  [ 1664/ 3200]\n",
      "loss: 0.645017  [ 1680/ 3200]\n",
      "loss: 0.743243  [ 1696/ 3200]\n",
      "loss: 0.502687  [ 1712/ 3200]\n",
      "loss: 0.770849  [ 1728/ 3200]\n",
      "loss: 0.780537  [ 1744/ 3200]\n",
      "loss: 0.502778  [ 1760/ 3200]\n",
      "loss: 0.715651  [ 1776/ 3200]\n",
      "loss: 1.437020  [ 1792/ 3200]\n",
      "loss: 1.365069  [ 1808/ 3200]\n",
      "loss: 0.919449  [ 1824/ 3200]\n",
      "loss: 0.796500  [ 1840/ 3200]\n",
      "loss: 0.937833  [ 1856/ 3200]\n",
      "loss: 0.779079  [ 1872/ 3200]\n",
      "loss: 0.686477  [ 1888/ 3200]\n",
      "loss: 0.454796  [ 1904/ 3200]\n",
      "loss: 0.382106  [ 1920/ 3200]\n",
      "loss: 1.164971  [ 1936/ 3200]\n",
      "loss: 0.673687  [ 1952/ 3200]\n",
      "loss: 0.538613  [ 1968/ 3200]\n",
      "loss: 0.376898  [ 1984/ 3200]\n",
      "loss: 0.742823  [ 2000/ 3200]\n",
      "loss: 0.545576  [ 2016/ 3200]\n",
      "loss: 0.712842  [ 2032/ 3200]\n",
      "loss: 0.534543  [ 2048/ 3200]\n",
      "loss: 1.044688  [ 2064/ 3200]\n",
      "loss: 0.911958  [ 2080/ 3200]\n",
      "loss: 0.867769  [ 2096/ 3200]\n",
      "loss: 1.085587  [ 2112/ 3200]\n",
      "loss: 0.881785  [ 2128/ 3200]\n",
      "loss: 0.422381  [ 2144/ 3200]\n",
      "loss: 0.681476  [ 2160/ 3200]\n",
      "loss: 0.853738  [ 2176/ 3200]\n",
      "loss: 0.612446  [ 2192/ 3200]\n",
      "loss: 0.601753  [ 2208/ 3200]\n",
      "loss: 1.129780  [ 2224/ 3200]\n",
      "loss: 0.676892  [ 2240/ 3200]\n",
      "loss: 0.559721  [ 2256/ 3200]\n",
      "loss: 0.741372  [ 2272/ 3200]\n",
      "loss: 0.762684  [ 2288/ 3200]\n",
      "loss: 0.444219  [ 2304/ 3200]\n",
      "loss: 0.611637  [ 2320/ 3200]\n",
      "loss: 0.372042  [ 2336/ 3200]\n",
      "loss: 0.526658  [ 2352/ 3200]\n",
      "loss: 0.689152  [ 2368/ 3200]\n",
      "loss: 1.025991  [ 2384/ 3200]\n",
      "loss: 0.903048  [ 2400/ 3200]\n",
      "loss: 0.588116  [ 2416/ 3200]\n",
      "loss: 0.546291  [ 2432/ 3200]\n",
      "loss: 0.662529  [ 2448/ 3200]\n",
      "loss: 0.489368  [ 2464/ 3200]\n",
      "loss: 0.689486  [ 2480/ 3200]\n",
      "loss: 0.469799  [ 2496/ 3200]\n",
      "loss: 0.472705  [ 2512/ 3200]\n",
      "loss: 0.595220  [ 2528/ 3200]\n",
      "loss: 0.461014  [ 2544/ 3200]\n",
      "loss: 1.351978  [ 2560/ 3200]\n",
      "loss: 1.011232  [ 2576/ 3200]\n",
      "loss: 0.948474  [ 2592/ 3200]\n",
      "loss: 0.779974  [ 2608/ 3200]\n",
      "loss: 0.714774  [ 2624/ 3200]\n",
      "loss: 0.771991  [ 2640/ 3200]\n",
      "loss: 0.746573  [ 2656/ 3200]\n",
      "loss: 0.672821  [ 2672/ 3200]\n",
      "loss: 0.644782  [ 2688/ 3200]\n",
      "loss: 0.722646  [ 2704/ 3200]\n",
      "loss: 0.550256  [ 2720/ 3200]\n",
      "loss: 0.617915  [ 2736/ 3200]\n",
      "loss: 0.787020  [ 2752/ 3200]\n",
      "loss: 0.635322  [ 2768/ 3200]\n",
      "loss: 0.944707  [ 2784/ 3200]\n",
      "loss: 0.898495  [ 2800/ 3200]\n",
      "loss: 0.896783  [ 2816/ 3200]\n",
      "loss: 0.589225  [ 2832/ 3200]\n",
      "loss: 0.834718  [ 2848/ 3200]\n",
      "loss: 0.595184  [ 2864/ 3200]\n",
      "loss: 0.812554  [ 2880/ 3200]\n",
      "loss: 0.481776  [ 2896/ 3200]\n",
      "loss: 0.798253  [ 2912/ 3200]\n",
      "loss: 0.716268  [ 2928/ 3200]\n",
      "loss: 0.470892  [ 2944/ 3200]\n",
      "loss: 0.876461  [ 2960/ 3200]\n",
      "loss: 0.676381  [ 2976/ 3200]\n",
      "loss: 0.431110  [ 2992/ 3200]\n",
      "loss: 0.425366  [ 3008/ 3200]\n",
      "loss: 1.299259  [ 3024/ 3200]\n",
      "loss: 0.769674  [ 3040/ 3200]\n",
      "loss: 0.743289  [ 3056/ 3200]\n",
      "loss: 0.503609  [ 3072/ 3200]\n",
      "loss: 1.069037  [ 3088/ 3200]\n",
      "loss: 0.783219  [ 3104/ 3200]\n",
      "loss: 0.605318  [ 3120/ 3200]\n",
      "loss: 0.797999  [ 3136/ 3200]\n",
      "loss: 0.645467  [ 3152/ 3200]\n",
      "loss: 0.740896  [ 3168/ 3200]\n",
      "loss: 0.603127  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.043221\n",
      "f1 macro averaged score: 0.738280\n",
      "Accuracy               : 74.0%\n",
      "Confusion matrix       :\n",
      "tensor([[163,  33,   2,   2],\n",
      "        [ 26, 108,  50,  16],\n",
      "        [  0,  14, 181,   5],\n",
      "        [  3,  37,  20, 140]], device='cuda:0')\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 0.831562  [    0/ 3200]\n",
      "loss: 0.554067  [   16/ 3200]\n",
      "loss: 0.980172  [   32/ 3200]\n",
      "loss: 0.439793  [   48/ 3200]\n",
      "loss: 0.605107  [   64/ 3200]\n",
      "loss: 0.617959  [   80/ 3200]\n",
      "loss: 0.704463  [   96/ 3200]\n",
      "loss: 1.466857  [  112/ 3200]\n",
      "loss: 1.068457  [  128/ 3200]\n",
      "loss: 0.821177  [  144/ 3200]\n",
      "loss: 0.738607  [  160/ 3200]\n",
      "loss: 0.551325  [  176/ 3200]\n",
      "loss: 0.493768  [  192/ 3200]\n",
      "loss: 0.725217  [  208/ 3200]\n",
      "loss: 0.720561  [  224/ 3200]\n",
      "loss: 0.696813  [  240/ 3200]\n",
      "loss: 0.423408  [  256/ 3200]\n",
      "loss: 0.742305  [  272/ 3200]\n",
      "loss: 0.776676  [  288/ 3200]\n",
      "loss: 0.527619  [  304/ 3200]\n",
      "loss: 0.673684  [  320/ 3200]\n",
      "loss: 1.128642  [  336/ 3200]\n",
      "loss: 0.914590  [  352/ 3200]\n",
      "loss: 0.831795  [  368/ 3200]\n",
      "loss: 0.560275  [  384/ 3200]\n",
      "loss: 0.923984  [  400/ 3200]\n",
      "loss: 0.906178  [  416/ 3200]\n",
      "loss: 0.624165  [  432/ 3200]\n",
      "loss: 0.707294  [  448/ 3200]\n",
      "loss: 0.806041  [  464/ 3200]\n",
      "loss: 0.685111  [  480/ 3200]\n",
      "loss: 0.656700  [  496/ 3200]\n",
      "loss: 0.557434  [  512/ 3200]\n",
      "loss: 0.537245  [  528/ 3200]\n",
      "loss: 0.740898  [  544/ 3200]\n",
      "loss: 0.668940  [  560/ 3200]\n",
      "loss: 0.887873  [  576/ 3200]\n",
      "loss: 0.422153  [  592/ 3200]\n",
      "loss: 1.258309  [  608/ 3200]\n",
      "loss: 0.651520  [  624/ 3200]\n",
      "loss: 0.707400  [  640/ 3200]\n",
      "loss: 0.876162  [  656/ 3200]\n",
      "loss: 0.772282  [  672/ 3200]\n",
      "loss: 0.845082  [  688/ 3200]\n",
      "loss: 0.712006  [  704/ 3200]\n",
      "loss: 0.779916  [  720/ 3200]\n",
      "loss: 0.631108  [  736/ 3200]\n",
      "loss: 0.722805  [  752/ 3200]\n",
      "loss: 0.482432  [  768/ 3200]\n",
      "loss: 0.467924  [  784/ 3200]\n",
      "loss: 0.552696  [  800/ 3200]\n",
      "loss: 1.163937  [  816/ 3200]\n",
      "loss: 0.907180  [  832/ 3200]\n",
      "loss: 0.624072  [  848/ 3200]\n",
      "loss: 0.693172  [  864/ 3200]\n",
      "loss: 0.310628  [  880/ 3200]\n",
      "loss: 0.671950  [  896/ 3200]\n",
      "loss: 0.636620  [  912/ 3200]\n",
      "loss: 0.666473  [  928/ 3200]\n",
      "loss: 0.704457  [  944/ 3200]\n",
      "loss: 0.683867  [  960/ 3200]\n",
      "loss: 0.568189  [  976/ 3200]\n",
      "loss: 0.925057  [  992/ 3200]\n",
      "loss: 0.485880  [ 1008/ 3200]\n",
      "loss: 0.632370  [ 1024/ 3200]\n",
      "loss: 0.655405  [ 1040/ 3200]\n",
      "loss: 0.795007  [ 1056/ 3200]\n",
      "loss: 1.007825  [ 1072/ 3200]\n",
      "loss: 0.528844  [ 1088/ 3200]\n",
      "loss: 0.665827  [ 1104/ 3200]\n",
      "loss: 0.670956  [ 1120/ 3200]\n",
      "loss: 0.521533  [ 1136/ 3200]\n",
      "loss: 0.869508  [ 1152/ 3200]\n",
      "loss: 0.655927  [ 1168/ 3200]\n",
      "loss: 0.398171  [ 1184/ 3200]\n",
      "loss: 0.969743  [ 1200/ 3200]\n",
      "loss: 0.678145  [ 1216/ 3200]\n",
      "loss: 1.110762  [ 1232/ 3200]\n",
      "loss: 1.046406  [ 1248/ 3200]\n",
      "loss: 0.720402  [ 1264/ 3200]\n",
      "loss: 0.584788  [ 1280/ 3200]\n",
      "loss: 0.399595  [ 1296/ 3200]\n",
      "loss: 0.631781  [ 1312/ 3200]\n",
      "loss: 0.850718  [ 1328/ 3200]\n",
      "loss: 0.580093  [ 1344/ 3200]\n",
      "loss: 0.561210  [ 1360/ 3200]\n",
      "loss: 0.518677  [ 1376/ 3200]\n",
      "loss: 0.659810  [ 1392/ 3200]\n",
      "loss: 0.684698  [ 1408/ 3200]\n",
      "loss: 0.791939  [ 1424/ 3200]\n",
      "loss: 0.711563  [ 1440/ 3200]\n",
      "loss: 0.672026  [ 1456/ 3200]\n",
      "loss: 0.762054  [ 1472/ 3200]\n",
      "loss: 0.517944  [ 1488/ 3200]\n",
      "loss: 0.624240  [ 1504/ 3200]\n",
      "loss: 0.675276  [ 1520/ 3200]\n",
      "loss: 0.623350  [ 1536/ 3200]\n",
      "loss: 0.505650  [ 1552/ 3200]\n",
      "loss: 0.631358  [ 1568/ 3200]\n",
      "loss: 0.898841  [ 1584/ 3200]\n",
      "loss: 0.766594  [ 1600/ 3200]\n",
      "loss: 0.707859  [ 1616/ 3200]\n",
      "loss: 0.483965  [ 1632/ 3200]\n",
      "loss: 1.077155  [ 1648/ 3200]\n",
      "loss: 0.642003  [ 1664/ 3200]\n",
      "loss: 0.455943  [ 1680/ 3200]\n",
      "loss: 0.519770  [ 1696/ 3200]\n",
      "loss: 0.714189  [ 1712/ 3200]\n",
      "loss: 0.487018  [ 1728/ 3200]\n",
      "loss: 1.006231  [ 1744/ 3200]\n",
      "loss: 0.944585  [ 1760/ 3200]\n",
      "loss: 1.030665  [ 1776/ 3200]\n",
      "loss: 0.501431  [ 1792/ 3200]\n",
      "loss: 0.880191  [ 1808/ 3200]\n",
      "loss: 0.739436  [ 1824/ 3200]\n",
      "loss: 0.671059  [ 1840/ 3200]\n",
      "loss: 0.904148  [ 1856/ 3200]\n",
      "loss: 0.254389  [ 1872/ 3200]\n",
      "loss: 0.565614  [ 1888/ 3200]\n",
      "loss: 0.529454  [ 1904/ 3200]\n",
      "loss: 0.711222  [ 1920/ 3200]\n",
      "loss: 0.722009  [ 1936/ 3200]\n",
      "loss: 0.853673  [ 1952/ 3200]\n",
      "loss: 0.529613  [ 1968/ 3200]\n",
      "loss: 0.594534  [ 1984/ 3200]\n",
      "loss: 0.750014  [ 2000/ 3200]\n",
      "loss: 1.019753  [ 2016/ 3200]\n",
      "loss: 0.933081  [ 2032/ 3200]\n",
      "loss: 0.491492  [ 2048/ 3200]\n",
      "loss: 0.661123  [ 2064/ 3200]\n",
      "loss: 0.548741  [ 2080/ 3200]\n",
      "loss: 0.566635  [ 2096/ 3200]\n",
      "loss: 0.434497  [ 2112/ 3200]\n",
      "loss: 1.025342  [ 2128/ 3200]\n",
      "loss: 0.591671  [ 2144/ 3200]\n",
      "loss: 0.569729  [ 2160/ 3200]\n",
      "loss: 0.744852  [ 2176/ 3200]\n",
      "loss: 0.475311  [ 2192/ 3200]\n",
      "loss: 1.041608  [ 2208/ 3200]\n",
      "loss: 0.945044  [ 2224/ 3200]\n",
      "loss: 1.024992  [ 2240/ 3200]\n",
      "loss: 0.639925  [ 2256/ 3200]\n",
      "loss: 0.404622  [ 2272/ 3200]\n",
      "loss: 0.764677  [ 2288/ 3200]\n",
      "loss: 0.557822  [ 2304/ 3200]\n",
      "loss: 0.350241  [ 2320/ 3200]\n",
      "loss: 1.029301  [ 2336/ 3200]\n",
      "loss: 0.895461  [ 2352/ 3200]\n",
      "loss: 0.353972  [ 2368/ 3200]\n",
      "loss: 0.611885  [ 2384/ 3200]\n",
      "loss: 0.776546  [ 2400/ 3200]\n",
      "loss: 0.882156  [ 2416/ 3200]\n",
      "loss: 0.783335  [ 2432/ 3200]\n",
      "loss: 0.495702  [ 2448/ 3200]\n",
      "loss: 0.643159  [ 2464/ 3200]\n",
      "loss: 0.603678  [ 2480/ 3200]\n",
      "loss: 0.429151  [ 2496/ 3200]\n",
      "loss: 1.087281  [ 2512/ 3200]\n",
      "loss: 0.838356  [ 2528/ 3200]\n",
      "loss: 0.924110  [ 2544/ 3200]\n",
      "loss: 0.698329  [ 2560/ 3200]\n",
      "loss: 0.870040  [ 2576/ 3200]\n",
      "loss: 1.320603  [ 2592/ 3200]\n",
      "loss: 1.395113  [ 2608/ 3200]\n",
      "loss: 0.699173  [ 2624/ 3200]\n",
      "loss: 0.711203  [ 2640/ 3200]\n",
      "loss: 0.437705  [ 2656/ 3200]\n",
      "loss: 0.660163  [ 2672/ 3200]\n",
      "loss: 0.617706  [ 2688/ 3200]\n",
      "loss: 0.811511  [ 2704/ 3200]\n",
      "loss: 0.451701  [ 2720/ 3200]\n",
      "loss: 0.589212  [ 2736/ 3200]\n",
      "loss: 1.154328  [ 2752/ 3200]\n",
      "loss: 0.529922  [ 2768/ 3200]\n",
      "loss: 0.546625  [ 2784/ 3200]\n",
      "loss: 0.452194  [ 2800/ 3200]\n",
      "loss: 0.755458  [ 2816/ 3200]\n",
      "loss: 0.434228  [ 2832/ 3200]\n",
      "loss: 0.883267  [ 2848/ 3200]\n",
      "loss: 0.708010  [ 2864/ 3200]\n",
      "loss: 0.974931  [ 2880/ 3200]\n",
      "loss: 0.872600  [ 2896/ 3200]\n",
      "loss: 0.718380  [ 2912/ 3200]\n",
      "loss: 0.399911  [ 2928/ 3200]\n",
      "loss: 0.652941  [ 2944/ 3200]\n",
      "loss: 0.753472  [ 2960/ 3200]\n",
      "loss: 0.729016  [ 2976/ 3200]\n",
      "loss: 0.952930  [ 2992/ 3200]\n",
      "loss: 0.604241  [ 3008/ 3200]\n",
      "loss: 0.873128  [ 3024/ 3200]\n",
      "loss: 0.449900  [ 3040/ 3200]\n",
      "loss: 0.675604  [ 3056/ 3200]\n",
      "loss: 0.940496  [ 3072/ 3200]\n",
      "loss: 0.877760  [ 3088/ 3200]\n",
      "loss: 0.749666  [ 3104/ 3200]\n",
      "loss: 1.090350  [ 3120/ 3200]\n",
      "loss: 0.840811  [ 3136/ 3200]\n",
      "loss: 1.003516  [ 3152/ 3200]\n",
      "loss: 0.545087  [ 3168/ 3200]\n",
      "loss: 0.706318  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.044804\n",
      "f1 macro averaged score: 0.707295\n",
      "Accuracy               : 71.5%\n",
      "Confusion matrix       :\n",
      "tensor([[151,  36,   7,   6],\n",
      "        [ 19,  86,  66,  29],\n",
      "        [  0,   7, 187,   6],\n",
      "        [  3,  25,  24, 148]], device='cuda:0')\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 0.614498  [    0/ 3200]\n",
      "loss: 0.535716  [   16/ 3200]\n",
      "loss: 1.026355  [   32/ 3200]\n",
      "loss: 0.717764  [   48/ 3200]\n",
      "loss: 0.509443  [   64/ 3200]\n",
      "loss: 0.783022  [   80/ 3200]\n",
      "loss: 0.908729  [   96/ 3200]\n",
      "loss: 0.462566  [  112/ 3200]\n",
      "loss: 0.738181  [  128/ 3200]\n",
      "loss: 0.521494  [  144/ 3200]\n",
      "loss: 0.685308  [  160/ 3200]\n",
      "loss: 0.682312  [  176/ 3200]\n",
      "loss: 0.843622  [  192/ 3200]\n",
      "loss: 0.587365  [  208/ 3200]\n",
      "loss: 1.317365  [  224/ 3200]\n",
      "loss: 1.016572  [  240/ 3200]\n",
      "loss: 0.927992  [  256/ 3200]\n",
      "loss: 0.929560  [  272/ 3200]\n",
      "loss: 0.958235  [  288/ 3200]\n",
      "loss: 0.621635  [  304/ 3200]\n",
      "loss: 0.535656  [  320/ 3200]\n",
      "loss: 0.759292  [  336/ 3200]\n",
      "loss: 0.912332  [  352/ 3200]\n",
      "loss: 0.771436  [  368/ 3200]\n",
      "loss: 0.987675  [  384/ 3200]\n",
      "loss: 0.641771  [  400/ 3200]\n",
      "loss: 0.809230  [  416/ 3200]\n",
      "loss: 0.502340  [  432/ 3200]\n",
      "loss: 0.561239  [  448/ 3200]\n",
      "loss: 0.572361  [  464/ 3200]\n",
      "loss: 0.788932  [  480/ 3200]\n",
      "loss: 0.742771  [  496/ 3200]\n",
      "loss: 0.414555  [  512/ 3200]\n",
      "loss: 0.455706  [  528/ 3200]\n",
      "loss: 0.866960  [  544/ 3200]\n",
      "loss: 0.861757  [  560/ 3200]\n",
      "loss: 0.803136  [  576/ 3200]\n",
      "loss: 0.554042  [  592/ 3200]\n",
      "loss: 1.047109  [  608/ 3200]\n",
      "loss: 0.467289  [  624/ 3200]\n",
      "loss: 0.654720  [  640/ 3200]\n",
      "loss: 0.631760  [  656/ 3200]\n",
      "loss: 0.692430  [  672/ 3200]\n",
      "loss: 0.686028  [  688/ 3200]\n",
      "loss: 0.394238  [  704/ 3200]\n",
      "loss: 0.554260  [  720/ 3200]\n",
      "loss: 0.565423  [  736/ 3200]\n",
      "loss: 1.086967  [  752/ 3200]\n",
      "loss: 0.652211  [  768/ 3200]\n",
      "loss: 1.076936  [  784/ 3200]\n",
      "loss: 1.386868  [  800/ 3200]\n",
      "loss: 0.627186  [  816/ 3200]\n",
      "loss: 0.358461  [  832/ 3200]\n",
      "loss: 0.826495  [  848/ 3200]\n",
      "loss: 1.223834  [  864/ 3200]\n",
      "loss: 0.462810  [  880/ 3200]\n",
      "loss: 0.758575  [  896/ 3200]\n",
      "loss: 0.614580  [  912/ 3200]\n",
      "loss: 0.815332  [  928/ 3200]\n",
      "loss: 0.588571  [  944/ 3200]\n",
      "loss: 0.668999  [  960/ 3200]\n",
      "loss: 0.637598  [  976/ 3200]\n",
      "loss: 0.633403  [  992/ 3200]\n",
      "loss: 0.735087  [ 1008/ 3200]\n",
      "loss: 0.517678  [ 1024/ 3200]\n",
      "loss: 0.616183  [ 1040/ 3200]\n",
      "loss: 0.645544  [ 1056/ 3200]\n",
      "loss: 0.616445  [ 1072/ 3200]\n",
      "loss: 0.625896  [ 1088/ 3200]\n",
      "loss: 0.672710  [ 1104/ 3200]\n",
      "loss: 1.251737  [ 1120/ 3200]\n",
      "loss: 0.809354  [ 1136/ 3200]\n",
      "loss: 0.719387  [ 1152/ 3200]\n",
      "loss: 0.606575  [ 1168/ 3200]\n",
      "loss: 0.581998  [ 1184/ 3200]\n",
      "loss: 1.441390  [ 1200/ 3200]\n",
      "loss: 0.608478  [ 1216/ 3200]\n",
      "loss: 0.933842  [ 1232/ 3200]\n",
      "loss: 0.586589  [ 1248/ 3200]\n",
      "loss: 0.794986  [ 1264/ 3200]\n",
      "loss: 0.618703  [ 1280/ 3200]\n",
      "loss: 0.722985  [ 1296/ 3200]\n",
      "loss: 0.790519  [ 1312/ 3200]\n",
      "loss: 0.902228  [ 1328/ 3200]\n",
      "loss: 0.669271  [ 1344/ 3200]\n",
      "loss: 0.821970  [ 1360/ 3200]\n",
      "loss: 0.749832  [ 1376/ 3200]\n",
      "loss: 0.629139  [ 1392/ 3200]\n",
      "loss: 0.541460  [ 1408/ 3200]\n",
      "loss: 0.975043  [ 1424/ 3200]\n",
      "loss: 0.919644  [ 1440/ 3200]\n",
      "loss: 0.610658  [ 1456/ 3200]\n",
      "loss: 1.040855  [ 1472/ 3200]\n",
      "loss: 0.461751  [ 1488/ 3200]\n",
      "loss: 0.435272  [ 1504/ 3200]\n",
      "loss: 0.530320  [ 1520/ 3200]\n",
      "loss: 0.714048  [ 1536/ 3200]\n",
      "loss: 0.553619  [ 1552/ 3200]\n",
      "loss: 0.713545  [ 1568/ 3200]\n",
      "loss: 0.698435  [ 1584/ 3200]\n",
      "loss: 0.602926  [ 1600/ 3200]\n",
      "loss: 0.717381  [ 1616/ 3200]\n",
      "loss: 0.538483  [ 1632/ 3200]\n",
      "loss: 0.641976  [ 1648/ 3200]\n",
      "loss: 0.835630  [ 1664/ 3200]\n",
      "loss: 0.741310  [ 1680/ 3200]\n",
      "loss: 0.577920  [ 1696/ 3200]\n",
      "loss: 0.418466  [ 1712/ 3200]\n",
      "loss: 0.835311  [ 1728/ 3200]\n",
      "loss: 0.717422  [ 1744/ 3200]\n",
      "loss: 0.431631  [ 1760/ 3200]\n",
      "loss: 0.438581  [ 1776/ 3200]\n",
      "loss: 0.643234  [ 1792/ 3200]\n",
      "loss: 0.515717  [ 1808/ 3200]\n",
      "loss: 0.692790  [ 1824/ 3200]\n",
      "loss: 0.800132  [ 1840/ 3200]\n",
      "loss: 0.731517  [ 1856/ 3200]\n",
      "loss: 0.467699  [ 1872/ 3200]\n",
      "loss: 0.687983  [ 1888/ 3200]\n",
      "loss: 0.499346  [ 1904/ 3200]\n",
      "loss: 0.515481  [ 1920/ 3200]\n",
      "loss: 0.639933  [ 1936/ 3200]\n",
      "loss: 0.751446  [ 1952/ 3200]\n",
      "loss: 0.383225  [ 1968/ 3200]\n",
      "loss: 0.272256  [ 1984/ 3200]\n",
      "loss: 0.533493  [ 2000/ 3200]\n",
      "loss: 0.953596  [ 2016/ 3200]\n",
      "loss: 0.692289  [ 2032/ 3200]\n",
      "loss: 0.977954  [ 2048/ 3200]\n",
      "loss: 1.121234  [ 2064/ 3200]\n",
      "loss: 0.610157  [ 2080/ 3200]\n",
      "loss: 0.700586  [ 2096/ 3200]\n",
      "loss: 0.456175  [ 2112/ 3200]\n",
      "loss: 0.493915  [ 2128/ 3200]\n",
      "loss: 1.011007  [ 2144/ 3200]\n",
      "loss: 0.479315  [ 2160/ 3200]\n",
      "loss: 0.835258  [ 2176/ 3200]\n",
      "loss: 0.531726  [ 2192/ 3200]\n",
      "loss: 0.401056  [ 2208/ 3200]\n",
      "loss: 0.604472  [ 2224/ 3200]\n",
      "loss: 0.330550  [ 2240/ 3200]\n",
      "loss: 0.552297  [ 2256/ 3200]\n",
      "loss: 0.942330  [ 2272/ 3200]\n",
      "loss: 0.892858  [ 2288/ 3200]\n",
      "loss: 0.915691  [ 2304/ 3200]\n",
      "loss: 0.582339  [ 2320/ 3200]\n",
      "loss: 0.725150  [ 2336/ 3200]\n",
      "loss: 0.581287  [ 2352/ 3200]\n",
      "loss: 0.695118  [ 2368/ 3200]\n",
      "loss: 0.642682  [ 2384/ 3200]\n",
      "loss: 0.635537  [ 2400/ 3200]\n",
      "loss: 1.045644  [ 2416/ 3200]\n",
      "loss: 0.561615  [ 2432/ 3200]\n",
      "loss: 0.504460  [ 2448/ 3200]\n",
      "loss: 0.639047  [ 2464/ 3200]\n",
      "loss: 0.908175  [ 2480/ 3200]\n",
      "loss: 0.913985  [ 2496/ 3200]\n",
      "loss: 0.466873  [ 2512/ 3200]\n",
      "loss: 0.456335  [ 2528/ 3200]\n",
      "loss: 0.700445  [ 2544/ 3200]\n",
      "loss: 0.421872  [ 2560/ 3200]\n",
      "loss: 0.967581  [ 2576/ 3200]\n",
      "loss: 0.611238  [ 2592/ 3200]\n",
      "loss: 0.908958  [ 2608/ 3200]\n",
      "loss: 0.611498  [ 2624/ 3200]\n",
      "loss: 1.239034  [ 2640/ 3200]\n",
      "loss: 0.666082  [ 2656/ 3200]\n",
      "loss: 0.568391  [ 2672/ 3200]\n",
      "loss: 0.607363  [ 2688/ 3200]\n",
      "loss: 0.465119  [ 2704/ 3200]\n",
      "loss: 0.685375  [ 2720/ 3200]\n",
      "loss: 0.666901  [ 2736/ 3200]\n",
      "loss: 0.736898  [ 2752/ 3200]\n",
      "loss: 0.416958  [ 2768/ 3200]\n",
      "loss: 0.867013  [ 2784/ 3200]\n",
      "loss: 0.786765  [ 2800/ 3200]\n",
      "loss: 0.584505  [ 2816/ 3200]\n",
      "loss: 0.822222  [ 2832/ 3200]\n",
      "loss: 0.450047  [ 2848/ 3200]\n",
      "loss: 0.392548  [ 2864/ 3200]\n",
      "loss: 0.365453  [ 2880/ 3200]\n",
      "loss: 0.440528  [ 2896/ 3200]\n",
      "loss: 0.703887  [ 2912/ 3200]\n",
      "loss: 0.877121  [ 2928/ 3200]\n",
      "loss: 0.641956  [ 2944/ 3200]\n",
      "loss: 0.532984  [ 2960/ 3200]\n",
      "loss: 0.329649  [ 2976/ 3200]\n",
      "loss: 0.532442  [ 2992/ 3200]\n",
      "loss: 0.319579  [ 3008/ 3200]\n",
      "loss: 0.534707  [ 3024/ 3200]\n",
      "loss: 0.629767  [ 3040/ 3200]\n",
      "loss: 0.699845  [ 3056/ 3200]\n",
      "loss: 0.769028  [ 3072/ 3200]\n",
      "loss: 0.753578  [ 3088/ 3200]\n",
      "loss: 0.371483  [ 3104/ 3200]\n",
      "loss: 0.738456  [ 3120/ 3200]\n",
      "loss: 0.602145  [ 3136/ 3200]\n",
      "loss: 0.360916  [ 3152/ 3200]\n",
      "loss: 0.767574  [ 3168/ 3200]\n",
      "loss: 0.680900  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.042503\n",
      "f1 macro averaged score: 0.719672\n",
      "Accuracy               : 72.4%\n",
      "Confusion matrix       :\n",
      "tensor([[148,  39,   2,  11],\n",
      "        [ 14,  96,  38,  52],\n",
      "        [  0,  13, 173,  14],\n",
      "        [  3,  19,  16, 162]], device='cuda:0')\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 0.671484  [    0/ 3200]\n",
      "loss: 0.985140  [   16/ 3200]\n",
      "loss: 0.881906  [   32/ 3200]\n",
      "loss: 0.637555  [   48/ 3200]\n",
      "loss: 0.498177  [   64/ 3200]\n",
      "loss: 0.813380  [   80/ 3200]\n",
      "loss: 0.770691  [   96/ 3200]\n",
      "loss: 0.617240  [  112/ 3200]\n",
      "loss: 0.540921  [  128/ 3200]\n",
      "loss: 0.865693  [  144/ 3200]\n",
      "loss: 1.135058  [  160/ 3200]\n",
      "loss: 0.591944  [  176/ 3200]\n",
      "loss: 0.795780  [  192/ 3200]\n",
      "loss: 0.927296  [  208/ 3200]\n",
      "loss: 0.705495  [  224/ 3200]\n",
      "loss: 1.181175  [  240/ 3200]\n",
      "loss: 1.064177  [  256/ 3200]\n",
      "loss: 0.808374  [  272/ 3200]\n",
      "loss: 0.687515  [  288/ 3200]\n",
      "loss: 0.809958  [  304/ 3200]\n",
      "loss: 1.035596  [  320/ 3200]\n",
      "loss: 0.578389  [  336/ 3200]\n",
      "loss: 0.682224  [  352/ 3200]\n",
      "loss: 0.650848  [  368/ 3200]\n",
      "loss: 0.685022  [  384/ 3200]\n",
      "loss: 0.535303  [  400/ 3200]\n",
      "loss: 0.552527  [  416/ 3200]\n",
      "loss: 0.941058  [  432/ 3200]\n",
      "loss: 0.684015  [  448/ 3200]\n",
      "loss: 0.568894  [  464/ 3200]\n",
      "loss: 0.925500  [  480/ 3200]\n",
      "loss: 1.227315  [  496/ 3200]\n",
      "loss: 0.648339  [  512/ 3200]\n",
      "loss: 0.581279  [  528/ 3200]\n",
      "loss: 0.415836  [  544/ 3200]\n",
      "loss: 0.586332  [  560/ 3200]\n",
      "loss: 0.586778  [  576/ 3200]\n",
      "loss: 0.678065  [  592/ 3200]\n",
      "loss: 0.590319  [  608/ 3200]\n",
      "loss: 0.499770  [  624/ 3200]\n",
      "loss: 0.500319  [  640/ 3200]\n",
      "loss: 0.800692  [  656/ 3200]\n",
      "loss: 1.156670  [  672/ 3200]\n",
      "loss: 0.741694  [  688/ 3200]\n",
      "loss: 1.029077  [  704/ 3200]\n",
      "loss: 0.532411  [  720/ 3200]\n",
      "loss: 0.780343  [  736/ 3200]\n",
      "loss: 0.672296  [  752/ 3200]\n",
      "loss: 0.287288  [  768/ 3200]\n",
      "loss: 0.629934  [  784/ 3200]\n",
      "loss: 0.605610  [  800/ 3200]\n",
      "loss: 0.520151  [  816/ 3200]\n",
      "loss: 0.559386  [  832/ 3200]\n",
      "loss: 0.478917  [  848/ 3200]\n",
      "loss: 0.936456  [  864/ 3200]\n",
      "loss: 0.620370  [  880/ 3200]\n",
      "loss: 0.611659  [  896/ 3200]\n",
      "loss: 0.402039  [  912/ 3200]\n",
      "loss: 0.489447  [  928/ 3200]\n",
      "loss: 0.760584  [  944/ 3200]\n",
      "loss: 0.989560  [  960/ 3200]\n",
      "loss: 0.336398  [  976/ 3200]\n",
      "loss: 0.686917  [  992/ 3200]\n",
      "loss: 0.990457  [ 1008/ 3200]\n",
      "loss: 0.503273  [ 1024/ 3200]\n",
      "loss: 0.711062  [ 1040/ 3200]\n",
      "loss: 0.997860  [ 1056/ 3200]\n",
      "loss: 0.384638  [ 1072/ 3200]\n",
      "loss: 0.595510  [ 1088/ 3200]\n",
      "loss: 0.716126  [ 1104/ 3200]\n",
      "loss: 0.582274  [ 1120/ 3200]\n",
      "loss: 0.460041  [ 1136/ 3200]\n",
      "loss: 0.508520  [ 1152/ 3200]\n",
      "loss: 0.893933  [ 1168/ 3200]\n",
      "loss: 1.219223  [ 1184/ 3200]\n",
      "loss: 0.643759  [ 1200/ 3200]\n",
      "loss: 0.574068  [ 1216/ 3200]\n",
      "loss: 0.707445  [ 1232/ 3200]\n",
      "loss: 0.719696  [ 1248/ 3200]\n",
      "loss: 0.696852  [ 1264/ 3200]\n",
      "loss: 0.555035  [ 1280/ 3200]\n",
      "loss: 0.646959  [ 1296/ 3200]\n",
      "loss: 0.602679  [ 1312/ 3200]\n",
      "loss: 0.770705  [ 1328/ 3200]\n",
      "loss: 0.785628  [ 1344/ 3200]\n",
      "loss: 0.419703  [ 1360/ 3200]\n",
      "loss: 0.547891  [ 1376/ 3200]\n",
      "loss: 0.486439  [ 1392/ 3200]\n",
      "loss: 0.605025  [ 1408/ 3200]\n",
      "loss: 0.700226  [ 1424/ 3200]\n",
      "loss: 0.666646  [ 1440/ 3200]\n",
      "loss: 1.004130  [ 1456/ 3200]\n",
      "loss: 0.631865  [ 1472/ 3200]\n",
      "loss: 0.892770  [ 1488/ 3200]\n",
      "loss: 0.730295  [ 1504/ 3200]\n",
      "loss: 1.186015  [ 1520/ 3200]\n",
      "loss: 0.741960  [ 1536/ 3200]\n",
      "loss: 0.278470  [ 1552/ 3200]\n",
      "loss: 0.891640  [ 1568/ 3200]\n",
      "loss: 0.383159  [ 1584/ 3200]\n",
      "loss: 0.879657  [ 1600/ 3200]\n",
      "loss: 0.535531  [ 1616/ 3200]\n",
      "loss: 0.630815  [ 1632/ 3200]\n",
      "loss: 0.854644  [ 1648/ 3200]\n",
      "loss: 0.500360  [ 1664/ 3200]\n",
      "loss: 0.495892  [ 1680/ 3200]\n",
      "loss: 1.029803  [ 1696/ 3200]\n",
      "loss: 0.407760  [ 1712/ 3200]\n",
      "loss: 0.575336  [ 1728/ 3200]\n",
      "loss: 0.492860  [ 1744/ 3200]\n",
      "loss: 0.281822  [ 1760/ 3200]\n",
      "loss: 0.573217  [ 1776/ 3200]\n",
      "loss: 0.428859  [ 1792/ 3200]\n",
      "loss: 0.516212  [ 1808/ 3200]\n",
      "loss: 0.866015  [ 1824/ 3200]\n",
      "loss: 0.291222  [ 1840/ 3200]\n",
      "loss: 0.894202  [ 1856/ 3200]\n",
      "loss: 1.274792  [ 1872/ 3200]\n",
      "loss: 0.741755  [ 1888/ 3200]\n",
      "loss: 0.482797  [ 1904/ 3200]\n",
      "loss: 0.338052  [ 1920/ 3200]\n",
      "loss: 0.677964  [ 1936/ 3200]\n",
      "loss: 0.869497  [ 1952/ 3200]\n",
      "loss: 0.666758  [ 1968/ 3200]\n",
      "loss: 0.540069  [ 1984/ 3200]\n",
      "loss: 0.849481  [ 2000/ 3200]\n",
      "loss: 0.432404  [ 2016/ 3200]\n",
      "loss: 0.539487  [ 2032/ 3200]\n",
      "loss: 1.091846  [ 2048/ 3200]\n",
      "loss: 0.581427  [ 2064/ 3200]\n",
      "loss: 0.395728  [ 2080/ 3200]\n",
      "loss: 1.001949  [ 2096/ 3200]\n",
      "loss: 0.551726  [ 2112/ 3200]\n",
      "loss: 0.477938  [ 2128/ 3200]\n",
      "loss: 0.728021  [ 2144/ 3200]\n",
      "loss: 0.376344  [ 2160/ 3200]\n",
      "loss: 0.516861  [ 2176/ 3200]\n",
      "loss: 0.612255  [ 2192/ 3200]\n",
      "loss: 0.552859  [ 2208/ 3200]\n",
      "loss: 0.412819  [ 2224/ 3200]\n",
      "loss: 0.707762  [ 2240/ 3200]\n",
      "loss: 0.487003  [ 2256/ 3200]\n",
      "loss: 0.880643  [ 2272/ 3200]\n",
      "loss: 0.683673  [ 2288/ 3200]\n",
      "loss: 0.608892  [ 2304/ 3200]\n",
      "loss: 0.505249  [ 2320/ 3200]\n",
      "loss: 0.901887  [ 2336/ 3200]\n",
      "loss: 0.723336  [ 2352/ 3200]\n",
      "loss: 0.784027  [ 2368/ 3200]\n",
      "loss: 0.610057  [ 2384/ 3200]\n",
      "loss: 1.196708  [ 2400/ 3200]\n",
      "loss: 1.038344  [ 2416/ 3200]\n",
      "loss: 0.612669  [ 2432/ 3200]\n",
      "loss: 0.402098  [ 2448/ 3200]\n",
      "loss: 0.641275  [ 2464/ 3200]\n",
      "loss: 0.410662  [ 2480/ 3200]\n",
      "loss: 0.576584  [ 2496/ 3200]\n",
      "loss: 0.943290  [ 2512/ 3200]\n",
      "loss: 0.917811  [ 2528/ 3200]\n",
      "loss: 0.582258  [ 2544/ 3200]\n",
      "loss: 0.668565  [ 2560/ 3200]\n",
      "loss: 0.433722  [ 2576/ 3200]\n",
      "loss: 0.576964  [ 2592/ 3200]\n",
      "loss: 0.335123  [ 2608/ 3200]\n",
      "loss: 0.627322  [ 2624/ 3200]\n",
      "loss: 1.048671  [ 2640/ 3200]\n",
      "loss: 0.579838  [ 2656/ 3200]\n",
      "loss: 0.720641  [ 2672/ 3200]\n",
      "loss: 0.404127  [ 2688/ 3200]\n",
      "loss: 0.465172  [ 2704/ 3200]\n",
      "loss: 0.636537  [ 2720/ 3200]\n",
      "loss: 0.394392  [ 2736/ 3200]\n",
      "loss: 0.271808  [ 2752/ 3200]\n",
      "loss: 0.641134  [ 2768/ 3200]\n",
      "loss: 0.917430  [ 2784/ 3200]\n",
      "loss: 0.526338  [ 2800/ 3200]\n",
      "loss: 0.523140  [ 2816/ 3200]\n",
      "loss: 0.490319  [ 2832/ 3200]\n",
      "loss: 0.263673  [ 2848/ 3200]\n",
      "loss: 0.789510  [ 2864/ 3200]\n",
      "loss: 0.809935  [ 2880/ 3200]\n",
      "loss: 0.614124  [ 2896/ 3200]\n",
      "loss: 0.870512  [ 2912/ 3200]\n",
      "loss: 0.664862  [ 2928/ 3200]\n",
      "loss: 0.465508  [ 2944/ 3200]\n",
      "loss: 0.367901  [ 2960/ 3200]\n",
      "loss: 0.806247  [ 2976/ 3200]\n",
      "loss: 0.643913  [ 2992/ 3200]\n",
      "loss: 0.664666  [ 3008/ 3200]\n",
      "loss: 0.476757  [ 3024/ 3200]\n",
      "loss: 0.394054  [ 3040/ 3200]\n",
      "loss: 1.068187  [ 3056/ 3200]\n",
      "loss: 0.474010  [ 3072/ 3200]\n",
      "loss: 0.587600  [ 3088/ 3200]\n",
      "loss: 0.495700  [ 3104/ 3200]\n",
      "loss: 0.560860  [ 3120/ 3200]\n",
      "loss: 0.930330  [ 3136/ 3200]\n",
      "loss: 0.725762  [ 3152/ 3200]\n",
      "loss: 0.971501  [ 3168/ 3200]\n",
      "loss: 0.437293  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.039210\n",
      "f1 macro averaged score: 0.736607\n",
      "Accuracy               : 74.4%\n",
      "Confusion matrix       :\n",
      "tensor([[182,   8,   0,  10],\n",
      "        [ 27,  94,  21,  58],\n",
      "        [  0,  18, 155,  27],\n",
      "        [  7,  17,  12, 164]], device='cuda:0')\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.482439  [    0/ 3200]\n",
      "loss: 0.557349  [   16/ 3200]\n",
      "loss: 0.391794  [   32/ 3200]\n",
      "loss: 0.741368  [   48/ 3200]\n",
      "loss: 0.507683  [   64/ 3200]\n",
      "loss: 0.424176  [   80/ 3200]\n",
      "loss: 0.747207  [   96/ 3200]\n",
      "loss: 0.509446  [  112/ 3200]\n",
      "loss: 0.692274  [  128/ 3200]\n",
      "loss: 0.410365  [  144/ 3200]\n",
      "loss: 1.109334  [  160/ 3200]\n",
      "loss: 0.893703  [  176/ 3200]\n",
      "loss: 0.610309  [  192/ 3200]\n",
      "loss: 0.468138  [  208/ 3200]\n",
      "loss: 0.516582  [  224/ 3200]\n",
      "loss: 0.678037  [  240/ 3200]\n",
      "loss: 0.519035  [  256/ 3200]\n",
      "loss: 0.580538  [  272/ 3200]\n",
      "loss: 0.715524  [  288/ 3200]\n",
      "loss: 0.615647  [  304/ 3200]\n",
      "loss: 0.587584  [  320/ 3200]\n",
      "loss: 0.945591  [  336/ 3200]\n",
      "loss: 0.829447  [  352/ 3200]\n",
      "loss: 0.873045  [  368/ 3200]\n",
      "loss: 0.571538  [  384/ 3200]\n",
      "loss: 0.357392  [  400/ 3200]\n",
      "loss: 0.597492  [  416/ 3200]\n",
      "loss: 0.945782  [  432/ 3200]\n",
      "loss: 1.533290  [  448/ 3200]\n",
      "loss: 0.329327  [  464/ 3200]\n",
      "loss: 0.366379  [  480/ 3200]\n",
      "loss: 1.053544  [  496/ 3200]\n",
      "loss: 0.434779  [  512/ 3200]\n",
      "loss: 0.913170  [  528/ 3200]\n",
      "loss: 0.900677  [  544/ 3200]\n",
      "loss: 0.827217  [  560/ 3200]\n",
      "loss: 0.515985  [  576/ 3200]\n",
      "loss: 0.421891  [  592/ 3200]\n",
      "loss: 0.746186  [  608/ 3200]\n",
      "loss: 0.361225  [  624/ 3200]\n",
      "loss: 0.317526  [  640/ 3200]\n",
      "loss: 1.001227  [  656/ 3200]\n",
      "loss: 0.655565  [  672/ 3200]\n",
      "loss: 0.575533  [  688/ 3200]\n",
      "loss: 0.446173  [  704/ 3200]\n",
      "loss: 0.695867  [  720/ 3200]\n",
      "loss: 0.777382  [  736/ 3200]\n",
      "loss: 0.656599  [  752/ 3200]\n",
      "loss: 0.639327  [  768/ 3200]\n",
      "loss: 0.510445  [  784/ 3200]\n",
      "loss: 0.473735  [  800/ 3200]\n",
      "loss: 0.910690  [  816/ 3200]\n",
      "loss: 0.620827  [  832/ 3200]\n",
      "loss: 0.710837  [  848/ 3200]\n",
      "loss: 0.900556  [  864/ 3200]\n",
      "loss: 0.966726  [  880/ 3200]\n",
      "loss: 0.678043  [  896/ 3200]\n",
      "loss: 0.351060  [  912/ 3200]\n",
      "loss: 0.783250  [  928/ 3200]\n",
      "loss: 0.623624  [  944/ 3200]\n",
      "loss: 0.491563  [  960/ 3200]\n",
      "loss: 0.677855  [  976/ 3200]\n",
      "loss: 0.841073  [  992/ 3200]\n",
      "loss: 0.529881  [ 1008/ 3200]\n",
      "loss: 0.697000  [ 1024/ 3200]\n",
      "loss: 0.657910  [ 1040/ 3200]\n",
      "loss: 0.473549  [ 1056/ 3200]\n",
      "loss: 0.714516  [ 1072/ 3200]\n",
      "loss: 0.359354  [ 1088/ 3200]\n",
      "loss: 0.619057  [ 1104/ 3200]\n",
      "loss: 1.003939  [ 1120/ 3200]\n",
      "loss: 0.385678  [ 1136/ 3200]\n",
      "loss: 0.696217  [ 1152/ 3200]\n",
      "loss: 0.509194  [ 1168/ 3200]\n",
      "loss: 0.438065  [ 1184/ 3200]\n",
      "loss: 0.621450  [ 1200/ 3200]\n",
      "loss: 0.668052  [ 1216/ 3200]\n",
      "loss: 0.561518  [ 1232/ 3200]\n",
      "loss: 0.799680  [ 1248/ 3200]\n",
      "loss: 0.628044  [ 1264/ 3200]\n",
      "loss: 0.469631  [ 1280/ 3200]\n",
      "loss: 0.599913  [ 1296/ 3200]\n",
      "loss: 0.582718  [ 1312/ 3200]\n",
      "loss: 0.517385  [ 1328/ 3200]\n",
      "loss: 0.661893  [ 1344/ 3200]\n",
      "loss: 0.622699  [ 1360/ 3200]\n",
      "loss: 0.824115  [ 1376/ 3200]\n",
      "loss: 0.460063  [ 1392/ 3200]\n",
      "loss: 0.571099  [ 1408/ 3200]\n",
      "loss: 0.927590  [ 1424/ 3200]\n",
      "loss: 0.783124  [ 1440/ 3200]\n",
      "loss: 0.405638  [ 1456/ 3200]\n",
      "loss: 0.610873  [ 1472/ 3200]\n",
      "loss: 0.737410  [ 1488/ 3200]\n",
      "loss: 0.452907  [ 1504/ 3200]\n",
      "loss: 0.417034  [ 1520/ 3200]\n",
      "loss: 0.523918  [ 1536/ 3200]\n",
      "loss: 0.826414  [ 1552/ 3200]\n",
      "loss: 0.561836  [ 1568/ 3200]\n",
      "loss: 0.543311  [ 1584/ 3200]\n",
      "loss: 0.701990  [ 1600/ 3200]\n",
      "loss: 0.641308  [ 1616/ 3200]\n",
      "loss: 0.550801  [ 1632/ 3200]\n",
      "loss: 0.774332  [ 1648/ 3200]\n",
      "loss: 0.294341  [ 1664/ 3200]\n",
      "loss: 0.643911  [ 1680/ 3200]\n",
      "loss: 0.572766  [ 1696/ 3200]\n",
      "loss: 0.832301  [ 1712/ 3200]\n",
      "loss: 0.287695  [ 1728/ 3200]\n",
      "loss: 0.625048  [ 1744/ 3200]\n",
      "loss: 0.523194  [ 1760/ 3200]\n",
      "loss: 0.425958  [ 1776/ 3200]\n",
      "loss: 0.543947  [ 1792/ 3200]\n",
      "loss: 0.385039  [ 1808/ 3200]\n",
      "loss: 0.768256  [ 1824/ 3200]\n",
      "loss: 0.818419  [ 1840/ 3200]\n",
      "loss: 0.582662  [ 1856/ 3200]\n",
      "loss: 0.869182  [ 1872/ 3200]\n",
      "loss: 0.578119  [ 1888/ 3200]\n",
      "loss: 0.824610  [ 1904/ 3200]\n",
      "loss: 0.548325  [ 1920/ 3200]\n",
      "loss: 0.863854  [ 1936/ 3200]\n",
      "loss: 0.562889  [ 1952/ 3200]\n",
      "loss: 0.385077  [ 1968/ 3200]\n",
      "loss: 0.804216  [ 1984/ 3200]\n",
      "loss: 0.557229  [ 2000/ 3200]\n",
      "loss: 0.436513  [ 2016/ 3200]\n",
      "loss: 1.087495  [ 2032/ 3200]\n",
      "loss: 0.339137  [ 2048/ 3200]\n",
      "loss: 1.247246  [ 2064/ 3200]\n",
      "loss: 0.494845  [ 2080/ 3200]\n",
      "loss: 0.721629  [ 2096/ 3200]\n",
      "loss: 0.843779  [ 2112/ 3200]\n",
      "loss: 0.763524  [ 2128/ 3200]\n",
      "loss: 0.820078  [ 2144/ 3200]\n",
      "loss: 0.850702  [ 2160/ 3200]\n",
      "loss: 0.442354  [ 2176/ 3200]\n",
      "loss: 0.560690  [ 2192/ 3200]\n",
      "loss: 0.403381  [ 2208/ 3200]\n",
      "loss: 0.754907  [ 2224/ 3200]\n",
      "loss: 0.596657  [ 2240/ 3200]\n",
      "loss: 0.439251  [ 2256/ 3200]\n",
      "loss: 1.015168  [ 2272/ 3200]\n",
      "loss: 0.717431  [ 2288/ 3200]\n",
      "loss: 0.612186  [ 2304/ 3200]\n",
      "loss: 0.640183  [ 2320/ 3200]\n",
      "loss: 0.714243  [ 2336/ 3200]\n",
      "loss: 0.483351  [ 2352/ 3200]\n",
      "loss: 0.972705  [ 2368/ 3200]\n",
      "loss: 0.996966  [ 2384/ 3200]\n",
      "loss: 0.639052  [ 2400/ 3200]\n",
      "loss: 1.011025  [ 2416/ 3200]\n",
      "loss: 0.667171  [ 2432/ 3200]\n",
      "loss: 0.419964  [ 2448/ 3200]\n",
      "loss: 0.574295  [ 2464/ 3200]\n",
      "loss: 0.663378  [ 2480/ 3200]\n",
      "loss: 0.853909  [ 2496/ 3200]\n",
      "loss: 0.411231  [ 2512/ 3200]\n",
      "loss: 0.333247  [ 2528/ 3200]\n",
      "loss: 0.514261  [ 2544/ 3200]\n",
      "loss: 0.930546  [ 2560/ 3200]\n",
      "loss: 0.857277  [ 2576/ 3200]\n",
      "loss: 0.587500  [ 2592/ 3200]\n",
      "loss: 0.472601  [ 2608/ 3200]\n",
      "loss: 0.490989  [ 2624/ 3200]\n",
      "loss: 0.484426  [ 2640/ 3200]\n",
      "loss: 0.369281  [ 2656/ 3200]\n",
      "loss: 0.814076  [ 2672/ 3200]\n",
      "loss: 1.019629  [ 2688/ 3200]\n",
      "loss: 0.851672  [ 2704/ 3200]\n",
      "loss: 0.401413  [ 2720/ 3200]\n",
      "loss: 0.270106  [ 2736/ 3200]\n",
      "loss: 0.433505  [ 2752/ 3200]\n",
      "loss: 0.611613  [ 2768/ 3200]\n",
      "loss: 0.990748  [ 2784/ 3200]\n",
      "loss: 0.451572  [ 2800/ 3200]\n",
      "loss: 1.079141  [ 2816/ 3200]\n",
      "loss: 0.874195  [ 2832/ 3200]\n",
      "loss: 0.704072  [ 2848/ 3200]\n",
      "loss: 0.485869  [ 2864/ 3200]\n",
      "loss: 0.408236  [ 2880/ 3200]\n",
      "loss: 0.704101  [ 2896/ 3200]\n",
      "loss: 0.815189  [ 2912/ 3200]\n",
      "loss: 0.890878  [ 2928/ 3200]\n",
      "loss: 0.612413  [ 2944/ 3200]\n",
      "loss: 0.639535  [ 2960/ 3200]\n",
      "loss: 0.817083  [ 2976/ 3200]\n",
      "loss: 0.557409  [ 2992/ 3200]\n",
      "loss: 0.731485  [ 3008/ 3200]\n",
      "loss: 0.520916  [ 3024/ 3200]\n",
      "loss: 0.667831  [ 3040/ 3200]\n",
      "loss: 0.866172  [ 3056/ 3200]\n",
      "loss: 0.686131  [ 3072/ 3200]\n",
      "loss: 1.104785  [ 3088/ 3200]\n",
      "loss: 0.576813  [ 3104/ 3200]\n",
      "loss: 0.368410  [ 3120/ 3200]\n",
      "loss: 0.549074  [ 3136/ 3200]\n",
      "loss: 0.371118  [ 3152/ 3200]\n",
      "loss: 0.563014  [ 3168/ 3200]\n",
      "loss: 0.760875  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.040899\n",
      "f1 macro averaged score: 0.728051\n",
      "Accuracy               : 73.1%\n",
      "Confusion matrix       :\n",
      "tensor([[160,  26,   1,  13],\n",
      "        [ 16,  96,  23,  65],\n",
      "        [  0,  14, 161,  25],\n",
      "        [  3,  16,  13, 168]], device='cuda:0')\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 0.597577  [    0/ 3200]\n",
      "loss: 0.552926  [   16/ 3200]\n",
      "loss: 0.523608  [   32/ 3200]\n",
      "loss: 0.559213  [   48/ 3200]\n",
      "loss: 0.292827  [   64/ 3200]\n",
      "loss: 0.429073  [   80/ 3200]\n",
      "loss: 0.896575  [   96/ 3200]\n",
      "loss: 0.769604  [  112/ 3200]\n",
      "loss: 1.446689  [  128/ 3200]\n",
      "loss: 0.818975  [  144/ 3200]\n",
      "loss: 0.678000  [  160/ 3200]\n",
      "loss: 0.740863  [  176/ 3200]\n",
      "loss: 0.618592  [  192/ 3200]\n",
      "loss: 0.549856  [  208/ 3200]\n",
      "loss: 0.543849  [  224/ 3200]\n",
      "loss: 0.516602  [  240/ 3200]\n",
      "loss: 0.550275  [  256/ 3200]\n",
      "loss: 0.608972  [  272/ 3200]\n",
      "loss: 0.557275  [  288/ 3200]\n",
      "loss: 1.394902  [  304/ 3200]\n",
      "loss: 0.527106  [  320/ 3200]\n",
      "loss: 0.353785  [  336/ 3200]\n",
      "loss: 0.852658  [  352/ 3200]\n",
      "loss: 1.108233  [  368/ 3200]\n",
      "loss: 0.839314  [  384/ 3200]\n",
      "loss: 0.624242  [  400/ 3200]\n",
      "loss: 0.535392  [  416/ 3200]\n",
      "loss: 0.626580  [  432/ 3200]\n",
      "loss: 0.602252  [  448/ 3200]\n",
      "loss: 0.583475  [  464/ 3200]\n",
      "loss: 0.485329  [  480/ 3200]\n",
      "loss: 0.457978  [  496/ 3200]\n",
      "loss: 0.558319  [  512/ 3200]\n",
      "loss: 0.371745  [  528/ 3200]\n",
      "loss: 0.540102  [  544/ 3200]\n",
      "loss: 0.274865  [  560/ 3200]\n",
      "loss: 0.384675  [  576/ 3200]\n",
      "loss: 0.442128  [  592/ 3200]\n",
      "loss: 0.224047  [  608/ 3200]\n",
      "loss: 0.409749  [  624/ 3200]\n",
      "loss: 0.750921  [  640/ 3200]\n",
      "loss: 0.640029  [  656/ 3200]\n",
      "loss: 0.531925  [  672/ 3200]\n",
      "loss: 0.509980  [  688/ 3200]\n",
      "loss: 0.784125  [  704/ 3200]\n",
      "loss: 0.445359  [  720/ 3200]\n",
      "loss: 0.407408  [  736/ 3200]\n",
      "loss: 0.416685  [  752/ 3200]\n",
      "loss: 0.579236  [  768/ 3200]\n",
      "loss: 0.612461  [  784/ 3200]\n",
      "loss: 0.529799  [  800/ 3200]\n",
      "loss: 0.572594  [  816/ 3200]\n",
      "loss: 0.569757  [  832/ 3200]\n",
      "loss: 0.910656  [  848/ 3200]\n",
      "loss: 0.294477  [  864/ 3200]\n",
      "loss: 0.558279  [  880/ 3200]\n",
      "loss: 0.919745  [  896/ 3200]\n",
      "loss: 0.365794  [  912/ 3200]\n",
      "loss: 0.489590  [  928/ 3200]\n",
      "loss: 0.861666  [  944/ 3200]\n",
      "loss: 0.511889  [  960/ 3200]\n",
      "loss: 0.816046  [  976/ 3200]\n",
      "loss: 0.456557  [  992/ 3200]\n",
      "loss: 0.628880  [ 1008/ 3200]\n",
      "loss: 0.913118  [ 1024/ 3200]\n",
      "loss: 0.938903  [ 1040/ 3200]\n",
      "loss: 0.669348  [ 1056/ 3200]\n",
      "loss: 0.561942  [ 1072/ 3200]\n",
      "loss: 0.435165  [ 1088/ 3200]\n",
      "loss: 0.823748  [ 1104/ 3200]\n",
      "loss: 1.224343  [ 1120/ 3200]\n",
      "loss: 0.399760  [ 1136/ 3200]\n",
      "loss: 1.114238  [ 1152/ 3200]\n",
      "loss: 0.697878  [ 1168/ 3200]\n",
      "loss: 0.503370  [ 1184/ 3200]\n",
      "loss: 0.619485  [ 1200/ 3200]\n",
      "loss: 0.683207  [ 1216/ 3200]\n",
      "loss: 0.758066  [ 1232/ 3200]\n",
      "loss: 0.743410  [ 1248/ 3200]\n",
      "loss: 0.545629  [ 1264/ 3200]\n",
      "loss: 0.689009  [ 1280/ 3200]\n",
      "loss: 0.410270  [ 1296/ 3200]\n",
      "loss: 0.266937  [ 1312/ 3200]\n",
      "loss: 0.451269  [ 1328/ 3200]\n",
      "loss: 0.496417  [ 1344/ 3200]\n",
      "loss: 0.657294  [ 1360/ 3200]\n",
      "loss: 0.578612  [ 1376/ 3200]\n",
      "loss: 0.318105  [ 1392/ 3200]\n",
      "loss: 0.440013  [ 1408/ 3200]\n",
      "loss: 0.569541  [ 1424/ 3200]\n",
      "loss: 0.707407  [ 1440/ 3200]\n",
      "loss: 0.594410  [ 1456/ 3200]\n",
      "loss: 0.627860  [ 1472/ 3200]\n",
      "loss: 0.489349  [ 1488/ 3200]\n",
      "loss: 0.523324  [ 1504/ 3200]\n",
      "loss: 0.648176  [ 1520/ 3200]\n",
      "loss: 0.550269  [ 1536/ 3200]\n",
      "loss: 0.485497  [ 1552/ 3200]\n",
      "loss: 0.475776  [ 1568/ 3200]\n",
      "loss: 0.618019  [ 1584/ 3200]\n",
      "loss: 0.940325  [ 1600/ 3200]\n",
      "loss: 0.909696  [ 1616/ 3200]\n",
      "loss: 0.621706  [ 1632/ 3200]\n",
      "loss: 0.792505  [ 1648/ 3200]\n",
      "loss: 0.495354  [ 1664/ 3200]\n",
      "loss: 0.492444  [ 1680/ 3200]\n",
      "loss: 0.542015  [ 1696/ 3200]\n",
      "loss: 0.857943  [ 1712/ 3200]\n",
      "loss: 0.435465  [ 1728/ 3200]\n",
      "loss: 0.503595  [ 1744/ 3200]\n",
      "loss: 0.529641  [ 1760/ 3200]\n",
      "loss: 0.471501  [ 1776/ 3200]\n",
      "loss: 0.747513  [ 1792/ 3200]\n",
      "loss: 0.565527  [ 1808/ 3200]\n",
      "loss: 0.827702  [ 1824/ 3200]\n",
      "loss: 0.758681  [ 1840/ 3200]\n",
      "loss: 1.066080  [ 1856/ 3200]\n",
      "loss: 0.544172  [ 1872/ 3200]\n",
      "loss: 0.262080  [ 1888/ 3200]\n",
      "loss: 0.811144  [ 1904/ 3200]\n",
      "loss: 0.431961  [ 1920/ 3200]\n",
      "loss: 0.568553  [ 1936/ 3200]\n",
      "loss: 0.764162  [ 1952/ 3200]\n",
      "loss: 0.697001  [ 1968/ 3200]\n",
      "loss: 0.575281  [ 1984/ 3200]\n",
      "loss: 0.680381  [ 2000/ 3200]\n",
      "loss: 0.622539  [ 2016/ 3200]\n",
      "loss: 0.613716  [ 2032/ 3200]\n",
      "loss: 0.479870  [ 2048/ 3200]\n",
      "loss: 0.561864  [ 2064/ 3200]\n",
      "loss: 0.968917  [ 2080/ 3200]\n",
      "loss: 0.643057  [ 2096/ 3200]\n",
      "loss: 0.571221  [ 2112/ 3200]\n",
      "loss: 1.113230  [ 2128/ 3200]\n",
      "loss: 0.880138  [ 2144/ 3200]\n",
      "loss: 0.363057  [ 2160/ 3200]\n",
      "loss: 0.789204  [ 2176/ 3200]\n",
      "loss: 0.639737  [ 2192/ 3200]\n",
      "loss: 0.781844  [ 2208/ 3200]\n",
      "loss: 0.477181  [ 2224/ 3200]\n",
      "loss: 0.532185  [ 2240/ 3200]\n",
      "loss: 0.617331  [ 2256/ 3200]\n",
      "loss: 0.755696  [ 2272/ 3200]\n",
      "loss: 0.675628  [ 2288/ 3200]\n",
      "loss: 0.512985  [ 2304/ 3200]\n",
      "loss: 0.504429  [ 2320/ 3200]\n",
      "loss: 0.542027  [ 2336/ 3200]\n",
      "loss: 0.442134  [ 2352/ 3200]\n",
      "loss: 1.200533  [ 2368/ 3200]\n",
      "loss: 0.906146  [ 2384/ 3200]\n",
      "loss: 0.554724  [ 2400/ 3200]\n",
      "loss: 0.715419  [ 2416/ 3200]\n",
      "loss: 0.503602  [ 2432/ 3200]\n",
      "loss: 0.405089  [ 2448/ 3200]\n",
      "loss: 0.482509  [ 2464/ 3200]\n",
      "loss: 0.531693  [ 2480/ 3200]\n",
      "loss: 0.876138  [ 2496/ 3200]\n",
      "loss: 0.283300  [ 2512/ 3200]\n",
      "loss: 0.506951  [ 2528/ 3200]\n",
      "loss: 0.651374  [ 2544/ 3200]\n",
      "loss: 0.538218  [ 2560/ 3200]\n",
      "loss: 0.444012  [ 2576/ 3200]\n",
      "loss: 0.679535  [ 2592/ 3200]\n",
      "loss: 0.793520  [ 2608/ 3200]\n",
      "loss: 0.685165  [ 2624/ 3200]\n",
      "loss: 0.763268  [ 2640/ 3200]\n",
      "loss: 0.652148  [ 2656/ 3200]\n",
      "loss: 0.726978  [ 2672/ 3200]\n",
      "loss: 0.604207  [ 2688/ 3200]\n",
      "loss: 0.495511  [ 2704/ 3200]\n",
      "loss: 0.277583  [ 2720/ 3200]\n",
      "loss: 0.596292  [ 2736/ 3200]\n",
      "loss: 0.396018  [ 2752/ 3200]\n",
      "loss: 0.587462  [ 2768/ 3200]\n",
      "loss: 0.359226  [ 2784/ 3200]\n",
      "loss: 1.126600  [ 2800/ 3200]\n",
      "loss: 0.759717  [ 2816/ 3200]\n",
      "loss: 1.037784  [ 2832/ 3200]\n",
      "loss: 0.769377  [ 2848/ 3200]\n",
      "loss: 0.554973  [ 2864/ 3200]\n",
      "loss: 0.576040  [ 2880/ 3200]\n",
      "loss: 0.759755  [ 2896/ 3200]\n",
      "loss: 0.777232  [ 2912/ 3200]\n",
      "loss: 0.502886  [ 2928/ 3200]\n",
      "loss: 0.470104  [ 2944/ 3200]\n",
      "loss: 1.098781  [ 2960/ 3200]\n",
      "loss: 0.372420  [ 2976/ 3200]\n",
      "loss: 0.430068  [ 2992/ 3200]\n",
      "loss: 0.693868  [ 3008/ 3200]\n",
      "loss: 0.841265  [ 3024/ 3200]\n",
      "loss: 0.448988  [ 3040/ 3200]\n",
      "loss: 1.236534  [ 3056/ 3200]\n",
      "loss: 1.111480  [ 3072/ 3200]\n",
      "loss: 0.536525  [ 3088/ 3200]\n",
      "loss: 0.663788  [ 3104/ 3200]\n",
      "loss: 1.235440  [ 3120/ 3200]\n",
      "loss: 0.782193  [ 3136/ 3200]\n",
      "loss: 0.532823  [ 3152/ 3200]\n",
      "loss: 0.600871  [ 3168/ 3200]\n",
      "loss: 0.655641  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.039989\n",
      "f1 macro averaged score: 0.729272\n",
      "Accuracy               : 74.2%\n",
      "Confusion matrix       :\n",
      "tensor([[180,   7,   0,  13],\n",
      "        [ 29,  80,  26,  65],\n",
      "        [  0,  13, 165,  22],\n",
      "        [  6,  10,  15, 169]], device='cuda:0')\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.491392  [    0/ 3200]\n",
      "loss: 0.710271  [   16/ 3200]\n",
      "loss: 0.738377  [   32/ 3200]\n",
      "loss: 0.741668  [   48/ 3200]\n",
      "loss: 0.489197  [   64/ 3200]\n",
      "loss: 0.387501  [   80/ 3200]\n",
      "loss: 0.550318  [   96/ 3200]\n",
      "loss: 0.545622  [  112/ 3200]\n",
      "loss: 0.468212  [  128/ 3200]\n",
      "loss: 0.824539  [  144/ 3200]\n",
      "loss: 0.648297  [  160/ 3200]\n",
      "loss: 0.360270  [  176/ 3200]\n",
      "loss: 1.074779  [  192/ 3200]\n",
      "loss: 0.330562  [  208/ 3200]\n",
      "loss: 0.559567  [  224/ 3200]\n",
      "loss: 0.488056  [  240/ 3200]\n",
      "loss: 0.387315  [  256/ 3200]\n",
      "loss: 0.443002  [  272/ 3200]\n",
      "loss: 0.757176  [  288/ 3200]\n",
      "loss: 0.569055  [  304/ 3200]\n",
      "loss: 0.662542  [  320/ 3200]\n",
      "loss: 0.668210  [  336/ 3200]\n",
      "loss: 0.638451  [  352/ 3200]\n",
      "loss: 0.527688  [  368/ 3200]\n",
      "loss: 0.563205  [  384/ 3200]\n",
      "loss: 1.058430  [  400/ 3200]\n",
      "loss: 0.558176  [  416/ 3200]\n",
      "loss: 0.492118  [  432/ 3200]\n",
      "loss: 0.762756  [  448/ 3200]\n",
      "loss: 0.724993  [  464/ 3200]\n",
      "loss: 0.639233  [  480/ 3200]\n",
      "loss: 0.490328  [  496/ 3200]\n",
      "loss: 0.423439  [  512/ 3200]\n",
      "loss: 0.413355  [  528/ 3200]\n",
      "loss: 0.705154  [  544/ 3200]\n",
      "loss: 0.587099  [  560/ 3200]\n",
      "loss: 0.498600  [  576/ 3200]\n",
      "loss: 0.599109  [  592/ 3200]\n",
      "loss: 0.540802  [  608/ 3200]\n",
      "loss: 0.526255  [  624/ 3200]\n",
      "loss: 0.659881  [  640/ 3200]\n",
      "loss: 1.057478  [  656/ 3200]\n",
      "loss: 0.386625  [  672/ 3200]\n",
      "loss: 0.660191  [  688/ 3200]\n",
      "loss: 0.454106  [  704/ 3200]\n",
      "loss: 0.205151  [  720/ 3200]\n",
      "loss: 0.699289  [  736/ 3200]\n",
      "loss: 0.685010  [  752/ 3200]\n",
      "loss: 0.737364  [  768/ 3200]\n",
      "loss: 0.794760  [  784/ 3200]\n",
      "loss: 0.426980  [  800/ 3200]\n",
      "loss: 0.527063  [  816/ 3200]\n",
      "loss: 0.616546  [  832/ 3200]\n",
      "loss: 0.345886  [  848/ 3200]\n",
      "loss: 0.352459  [  864/ 3200]\n",
      "loss: 0.736418  [  880/ 3200]\n",
      "loss: 0.801816  [  896/ 3200]\n",
      "loss: 0.477599  [  912/ 3200]\n",
      "loss: 0.696610  [  928/ 3200]\n",
      "loss: 0.587153  [  944/ 3200]\n",
      "loss: 0.481754  [  960/ 3200]\n",
      "loss: 0.768428  [  976/ 3200]\n",
      "loss: 0.989304  [  992/ 3200]\n",
      "loss: 0.375736  [ 1008/ 3200]\n",
      "loss: 0.611135  [ 1024/ 3200]\n",
      "loss: 0.783644  [ 1040/ 3200]\n",
      "loss: 0.480310  [ 1056/ 3200]\n",
      "loss: 0.532091  [ 1072/ 3200]\n",
      "loss: 0.456466  [ 1088/ 3200]\n",
      "loss: 0.833718  [ 1104/ 3200]\n",
      "loss: 0.497266  [ 1120/ 3200]\n",
      "loss: 0.914876  [ 1136/ 3200]\n",
      "loss: 0.821503  [ 1152/ 3200]\n",
      "loss: 0.429554  [ 1168/ 3200]\n",
      "loss: 0.683548  [ 1184/ 3200]\n",
      "loss: 0.316978  [ 1200/ 3200]\n",
      "loss: 0.534847  [ 1216/ 3200]\n",
      "loss: 0.365475  [ 1232/ 3200]\n",
      "loss: 0.707013  [ 1248/ 3200]\n",
      "loss: 0.404797  [ 1264/ 3200]\n",
      "loss: 0.608625  [ 1280/ 3200]\n",
      "loss: 0.454608  [ 1296/ 3200]\n",
      "loss: 0.378624  [ 1312/ 3200]\n",
      "loss: 0.637476  [ 1328/ 3200]\n",
      "loss: 0.249056  [ 1344/ 3200]\n",
      "loss: 0.537322  [ 1360/ 3200]\n",
      "loss: 0.422237  [ 1376/ 3200]\n",
      "loss: 0.677633  [ 1392/ 3200]\n",
      "loss: 1.133418  [ 1408/ 3200]\n",
      "loss: 0.492902  [ 1424/ 3200]\n",
      "loss: 0.605701  [ 1440/ 3200]\n",
      "loss: 0.687592  [ 1456/ 3200]\n",
      "loss: 0.694614  [ 1472/ 3200]\n",
      "loss: 0.524910  [ 1488/ 3200]\n",
      "loss: 0.738771  [ 1504/ 3200]\n",
      "loss: 0.527672  [ 1520/ 3200]\n",
      "loss: 0.400965  [ 1536/ 3200]\n",
      "loss: 0.619360  [ 1552/ 3200]\n",
      "loss: 0.214009  [ 1568/ 3200]\n",
      "loss: 0.530602  [ 1584/ 3200]\n",
      "loss: 0.453059  [ 1600/ 3200]\n",
      "loss: 0.477119  [ 1616/ 3200]\n",
      "loss: 0.362813  [ 1632/ 3200]\n",
      "loss: 0.876523  [ 1648/ 3200]\n",
      "loss: 0.474741  [ 1664/ 3200]\n",
      "loss: 0.736218  [ 1680/ 3200]\n",
      "loss: 0.655382  [ 1696/ 3200]\n",
      "loss: 1.025823  [ 1712/ 3200]\n",
      "loss: 0.487608  [ 1728/ 3200]\n",
      "loss: 0.472201  [ 1744/ 3200]\n",
      "loss: 0.921324  [ 1760/ 3200]\n",
      "loss: 0.624194  [ 1776/ 3200]\n",
      "loss: 0.490310  [ 1792/ 3200]\n",
      "loss: 0.596998  [ 1808/ 3200]\n",
      "loss: 0.399278  [ 1824/ 3200]\n",
      "loss: 0.768621  [ 1840/ 3200]\n",
      "loss: 0.940727  [ 1856/ 3200]\n",
      "loss: 0.815395  [ 1872/ 3200]\n",
      "loss: 0.343160  [ 1888/ 3200]\n",
      "loss: 0.454501  [ 1904/ 3200]\n",
      "loss: 0.409931  [ 1920/ 3200]\n",
      "loss: 0.475800  [ 1936/ 3200]\n",
      "loss: 0.694515  [ 1952/ 3200]\n",
      "loss: 0.300492  [ 1968/ 3200]\n",
      "loss: 0.961604  [ 1984/ 3200]\n",
      "loss: 1.078707  [ 2000/ 3200]\n",
      "loss: 0.521526  [ 2016/ 3200]\n",
      "loss: 0.656593  [ 2032/ 3200]\n",
      "loss: 0.880150  [ 2048/ 3200]\n",
      "loss: 0.343533  [ 2064/ 3200]\n",
      "loss: 0.612087  [ 2080/ 3200]\n",
      "loss: 1.477926  [ 2096/ 3200]\n",
      "loss: 1.214341  [ 2112/ 3200]\n",
      "loss: 0.583122  [ 2128/ 3200]\n",
      "loss: 0.702565  [ 2144/ 3200]\n",
      "loss: 0.541513  [ 2160/ 3200]\n",
      "loss: 0.379386  [ 2176/ 3200]\n",
      "loss: 0.289825  [ 2192/ 3200]\n",
      "loss: 0.656847  [ 2208/ 3200]\n",
      "loss: 0.580651  [ 2224/ 3200]\n",
      "loss: 0.470381  [ 2240/ 3200]\n",
      "loss: 0.760794  [ 2256/ 3200]\n",
      "loss: 0.522473  [ 2272/ 3200]\n",
      "loss: 0.510121  [ 2288/ 3200]\n",
      "loss: 0.622110  [ 2304/ 3200]\n",
      "loss: 0.753525  [ 2320/ 3200]\n",
      "loss: 0.856933  [ 2336/ 3200]\n",
      "loss: 0.433979  [ 2352/ 3200]\n",
      "loss: 0.889181  [ 2368/ 3200]\n",
      "loss: 0.682482  [ 2384/ 3200]\n",
      "loss: 0.804899  [ 2400/ 3200]\n",
      "loss: 0.711817  [ 2416/ 3200]\n",
      "loss: 0.442588  [ 2432/ 3200]\n",
      "loss: 0.809575  [ 2448/ 3200]\n",
      "loss: 0.480033  [ 2464/ 3200]\n",
      "loss: 0.699532  [ 2480/ 3200]\n",
      "loss: 0.606521  [ 2496/ 3200]\n",
      "loss: 0.339915  [ 2512/ 3200]\n",
      "loss: 1.010438  [ 2528/ 3200]\n",
      "loss: 0.695932  [ 2544/ 3200]\n",
      "loss: 0.559175  [ 2560/ 3200]\n",
      "loss: 0.470408  [ 2576/ 3200]\n",
      "loss: 0.780352  [ 2592/ 3200]\n",
      "loss: 0.578559  [ 2608/ 3200]\n",
      "loss: 0.488883  [ 2624/ 3200]\n",
      "loss: 0.522023  [ 2640/ 3200]\n",
      "loss: 1.002067  [ 2656/ 3200]\n",
      "loss: 0.320672  [ 2672/ 3200]\n",
      "loss: 0.715118  [ 2688/ 3200]\n",
      "loss: 0.467045  [ 2704/ 3200]\n",
      "loss: 0.723816  [ 2720/ 3200]\n",
      "loss: 0.492053  [ 2736/ 3200]\n",
      "loss: 0.622956  [ 2752/ 3200]\n",
      "loss: 0.392285  [ 2768/ 3200]\n",
      "loss: 0.468904  [ 2784/ 3200]\n",
      "loss: 0.538359  [ 2800/ 3200]\n",
      "loss: 0.962973  [ 2816/ 3200]\n",
      "loss: 0.539684  [ 2832/ 3200]\n",
      "loss: 0.542083  [ 2848/ 3200]\n",
      "loss: 0.289961  [ 2864/ 3200]\n",
      "loss: 0.535306  [ 2880/ 3200]\n",
      "loss: 0.399002  [ 2896/ 3200]\n",
      "loss: 0.771878  [ 2912/ 3200]\n",
      "loss: 0.722562  [ 2928/ 3200]\n",
      "loss: 0.288133  [ 2944/ 3200]\n",
      "loss: 0.476103  [ 2960/ 3200]\n",
      "loss: 0.795899  [ 2976/ 3200]\n",
      "loss: 0.957654  [ 2992/ 3200]\n",
      "loss: 0.838647  [ 3008/ 3200]\n",
      "loss: 0.997906  [ 3024/ 3200]\n",
      "loss: 1.044064  [ 3040/ 3200]\n",
      "loss: 0.740574  [ 3056/ 3200]\n",
      "loss: 0.463849  [ 3072/ 3200]\n",
      "loss: 0.534998  [ 3088/ 3200]\n",
      "loss: 0.485968  [ 3104/ 3200]\n",
      "loss: 0.449328  [ 3120/ 3200]\n",
      "loss: 0.451146  [ 3136/ 3200]\n",
      "loss: 0.459320  [ 3152/ 3200]\n",
      "loss: 0.533369  [ 3168/ 3200]\n",
      "loss: 0.494425  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.040511\n",
      "f1 macro averaged score: 0.707714\n",
      "Accuracy               : 72.0%\n",
      "Confusion matrix       :\n",
      "tensor([[183,   3,   0,  14],\n",
      "        [ 35,  75,  12,  78],\n",
      "        [  0,  20, 146,  34],\n",
      "        [  7,  11,  10, 172]], device='cuda:0')\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.433897  [    0/ 3200]\n",
      "loss: 0.458144  [   16/ 3200]\n",
      "loss: 0.843326  [   32/ 3200]\n",
      "loss: 0.610087  [   48/ 3200]\n",
      "loss: 0.680851  [   64/ 3200]\n",
      "loss: 0.481421  [   80/ 3200]\n",
      "loss: 0.565912  [   96/ 3200]\n",
      "loss: 0.394610  [  112/ 3200]\n",
      "loss: 0.685772  [  128/ 3200]\n",
      "loss: 0.522918  [  144/ 3200]\n",
      "loss: 0.425615  [  160/ 3200]\n",
      "loss: 0.226627  [  176/ 3200]\n",
      "loss: 0.412056  [  192/ 3200]\n",
      "loss: 0.727669  [  208/ 3200]\n",
      "loss: 0.414595  [  224/ 3200]\n",
      "loss: 0.546961  [  240/ 3200]\n",
      "loss: 0.489966  [  256/ 3200]\n",
      "loss: 0.726770  [  272/ 3200]\n",
      "loss: 0.874950  [  288/ 3200]\n",
      "loss: 0.621830  [  304/ 3200]\n",
      "loss: 0.452982  [  320/ 3200]\n",
      "loss: 0.590521  [  336/ 3200]\n",
      "loss: 0.784442  [  352/ 3200]\n",
      "loss: 0.888139  [  368/ 3200]\n",
      "loss: 0.471852  [  384/ 3200]\n",
      "loss: 0.416387  [  400/ 3200]\n",
      "loss: 0.639320  [  416/ 3200]\n",
      "loss: 0.396996  [  432/ 3200]\n",
      "loss: 0.496089  [  448/ 3200]\n",
      "loss: 0.417766  [  464/ 3200]\n",
      "loss: 0.807320  [  480/ 3200]\n",
      "loss: 0.405403  [  496/ 3200]\n",
      "loss: 0.535251  [  512/ 3200]\n",
      "loss: 0.741702  [  528/ 3200]\n",
      "loss: 0.432793  [  544/ 3200]\n",
      "loss: 0.406082  [  560/ 3200]\n",
      "loss: 0.807280  [  576/ 3200]\n",
      "loss: 1.012408  [  592/ 3200]\n",
      "loss: 0.956080  [  608/ 3200]\n",
      "loss: 0.847785  [  624/ 3200]\n",
      "loss: 0.523081  [  640/ 3200]\n",
      "loss: 0.340825  [  656/ 3200]\n",
      "loss: 0.582826  [  672/ 3200]\n",
      "loss: 0.665258  [  688/ 3200]\n",
      "loss: 0.552560  [  704/ 3200]\n",
      "loss: 0.380362  [  720/ 3200]\n",
      "loss: 0.629748  [  736/ 3200]\n",
      "loss: 0.765491  [  752/ 3200]\n",
      "loss: 0.630516  [  768/ 3200]\n",
      "loss: 0.703296  [  784/ 3200]\n",
      "loss: 0.576434  [  800/ 3200]\n",
      "loss: 0.558046  [  816/ 3200]\n",
      "loss: 0.901142  [  832/ 3200]\n",
      "loss: 0.464930  [  848/ 3200]\n",
      "loss: 0.800769  [  864/ 3200]\n",
      "loss: 0.654318  [  880/ 3200]\n",
      "loss: 0.343760  [  896/ 3200]\n",
      "loss: 0.416570  [  912/ 3200]\n",
      "loss: 0.619247  [  928/ 3200]\n",
      "loss: 0.551763  [  944/ 3200]\n",
      "loss: 0.574386  [  960/ 3200]\n",
      "loss: 0.657434  [  976/ 3200]\n",
      "loss: 0.726683  [  992/ 3200]\n",
      "loss: 0.832283  [ 1008/ 3200]\n",
      "loss: 0.612433  [ 1024/ 3200]\n",
      "loss: 0.774234  [ 1040/ 3200]\n",
      "loss: 0.452468  [ 1056/ 3200]\n",
      "loss: 0.410475  [ 1072/ 3200]\n",
      "loss: 1.011162  [ 1088/ 3200]\n",
      "loss: 0.779889  [ 1104/ 3200]\n",
      "loss: 0.654694  [ 1120/ 3200]\n",
      "loss: 0.490327  [ 1136/ 3200]\n",
      "loss: 0.418086  [ 1152/ 3200]\n",
      "loss: 0.448633  [ 1168/ 3200]\n",
      "loss: 0.886299  [ 1184/ 3200]\n",
      "loss: 0.829451  [ 1200/ 3200]\n",
      "loss: 0.838429  [ 1216/ 3200]\n",
      "loss: 0.586771  [ 1232/ 3200]\n",
      "loss: 0.806290  [ 1248/ 3200]\n",
      "loss: 0.386838  [ 1264/ 3200]\n",
      "loss: 0.710025  [ 1280/ 3200]\n",
      "loss: 1.032475  [ 1296/ 3200]\n",
      "loss: 0.403561  [ 1312/ 3200]\n",
      "loss: 0.501131  [ 1328/ 3200]\n",
      "loss: 0.550386  [ 1344/ 3200]\n",
      "loss: 0.370138  [ 1360/ 3200]\n",
      "loss: 0.285526  [ 1376/ 3200]\n",
      "loss: 0.442148  [ 1392/ 3200]\n",
      "loss: 0.891932  [ 1408/ 3200]\n",
      "loss: 0.437109  [ 1424/ 3200]\n",
      "loss: 0.522856  [ 1440/ 3200]\n",
      "loss: 0.498092  [ 1456/ 3200]\n",
      "loss: 0.835442  [ 1472/ 3200]\n",
      "loss: 0.564621  [ 1488/ 3200]\n",
      "loss: 0.448116  [ 1504/ 3200]\n",
      "loss: 0.492221  [ 1520/ 3200]\n",
      "loss: 0.816989  [ 1536/ 3200]\n",
      "loss: 0.394283  [ 1552/ 3200]\n",
      "loss: 0.490596  [ 1568/ 3200]\n",
      "loss: 0.556569  [ 1584/ 3200]\n",
      "loss: 0.802479  [ 1600/ 3200]\n",
      "loss: 0.675186  [ 1616/ 3200]\n",
      "loss: 0.424482  [ 1632/ 3200]\n",
      "loss: 0.501947  [ 1648/ 3200]\n",
      "loss: 0.572476  [ 1664/ 3200]\n",
      "loss: 0.346535  [ 1680/ 3200]\n",
      "loss: 0.481114  [ 1696/ 3200]\n",
      "loss: 0.239489  [ 1712/ 3200]\n",
      "loss: 0.402074  [ 1728/ 3200]\n",
      "loss: 0.289402  [ 1744/ 3200]\n",
      "loss: 0.397313  [ 1760/ 3200]\n",
      "loss: 0.823442  [ 1776/ 3200]\n",
      "loss: 0.567291  [ 1792/ 3200]\n",
      "loss: 0.510917  [ 1808/ 3200]\n",
      "loss: 0.371442  [ 1824/ 3200]\n",
      "loss: 0.643097  [ 1840/ 3200]\n",
      "loss: 0.579916  [ 1856/ 3200]\n",
      "loss: 0.861917  [ 1872/ 3200]\n",
      "loss: 0.573027  [ 1888/ 3200]\n",
      "loss: 0.370456  [ 1904/ 3200]\n",
      "loss: 0.721630  [ 1920/ 3200]\n",
      "loss: 0.448254  [ 1936/ 3200]\n",
      "loss: 0.834444  [ 1952/ 3200]\n",
      "loss: 0.522598  [ 1968/ 3200]\n",
      "loss: 0.655725  [ 1984/ 3200]\n",
      "loss: 0.704645  [ 2000/ 3200]\n",
      "loss: 0.628748  [ 2016/ 3200]\n",
      "loss: 0.656138  [ 2032/ 3200]\n",
      "loss: 0.585173  [ 2048/ 3200]\n",
      "loss: 0.434896  [ 2064/ 3200]\n",
      "loss: 0.459787  [ 2080/ 3200]\n",
      "loss: 0.471227  [ 2096/ 3200]\n",
      "loss: 0.975942  [ 2112/ 3200]\n",
      "loss: 0.630521  [ 2128/ 3200]\n",
      "loss: 0.746357  [ 2144/ 3200]\n",
      "loss: 0.633844  [ 2160/ 3200]\n",
      "loss: 0.658371  [ 2176/ 3200]\n",
      "loss: 0.262810  [ 2192/ 3200]\n",
      "loss: 0.449014  [ 2208/ 3200]\n",
      "loss: 0.840646  [ 2224/ 3200]\n",
      "loss: 0.426382  [ 2240/ 3200]\n",
      "loss: 0.662804  [ 2256/ 3200]\n",
      "loss: 0.245527  [ 2272/ 3200]\n",
      "loss: 1.001608  [ 2288/ 3200]\n",
      "loss: 0.518798  [ 2304/ 3200]\n",
      "loss: 0.395152  [ 2320/ 3200]\n",
      "loss: 0.715521  [ 2336/ 3200]\n",
      "loss: 0.348105  [ 2352/ 3200]\n",
      "loss: 0.635209  [ 2368/ 3200]\n",
      "loss: 1.050238  [ 2384/ 3200]\n",
      "loss: 0.686092  [ 2400/ 3200]\n",
      "loss: 0.757340  [ 2416/ 3200]\n",
      "loss: 0.599387  [ 2432/ 3200]\n",
      "loss: 0.642317  [ 2448/ 3200]\n",
      "loss: 0.640629  [ 2464/ 3200]\n",
      "loss: 0.451955  [ 2480/ 3200]\n",
      "loss: 0.474125  [ 2496/ 3200]\n",
      "loss: 0.606071  [ 2512/ 3200]\n",
      "loss: 0.720252  [ 2528/ 3200]\n",
      "loss: 0.794598  [ 2544/ 3200]\n",
      "loss: 0.630921  [ 2560/ 3200]\n",
      "loss: 0.751287  [ 2576/ 3200]\n",
      "loss: 0.469602  [ 2592/ 3200]\n",
      "loss: 0.484653  [ 2608/ 3200]\n",
      "loss: 0.526435  [ 2624/ 3200]\n",
      "loss: 0.663013  [ 2640/ 3200]\n",
      "loss: 0.510483  [ 2656/ 3200]\n",
      "loss: 0.437571  [ 2672/ 3200]\n",
      "loss: 0.433131  [ 2688/ 3200]\n",
      "loss: 0.444579  [ 2704/ 3200]\n",
      "loss: 0.900415  [ 2720/ 3200]\n",
      "loss: 0.680851  [ 2736/ 3200]\n",
      "loss: 0.672046  [ 2752/ 3200]\n",
      "loss: 0.350533  [ 2768/ 3200]\n",
      "loss: 0.372890  [ 2784/ 3200]\n",
      "loss: 0.607466  [ 2800/ 3200]\n",
      "loss: 0.632471  [ 2816/ 3200]\n",
      "loss: 0.776799  [ 2832/ 3200]\n",
      "loss: 0.277386  [ 2848/ 3200]\n",
      "loss: 0.644590  [ 2864/ 3200]\n",
      "loss: 0.560305  [ 2880/ 3200]\n",
      "loss: 0.625021  [ 2896/ 3200]\n",
      "loss: 1.067385  [ 2912/ 3200]\n",
      "loss: 0.461460  [ 2928/ 3200]\n",
      "loss: 0.764116  [ 2944/ 3200]\n",
      "loss: 0.667804  [ 2960/ 3200]\n",
      "loss: 0.572831  [ 2976/ 3200]\n",
      "loss: 0.521278  [ 2992/ 3200]\n",
      "loss: 0.551744  [ 3008/ 3200]\n",
      "loss: 0.695752  [ 3024/ 3200]\n",
      "loss: 0.359850  [ 3040/ 3200]\n",
      "loss: 0.791669  [ 3056/ 3200]\n",
      "loss: 0.852685  [ 3072/ 3200]\n",
      "loss: 0.454555  [ 3088/ 3200]\n",
      "loss: 0.546793  [ 3104/ 3200]\n",
      "loss: 0.383345  [ 3120/ 3200]\n",
      "loss: 0.895468  [ 3136/ 3200]\n",
      "loss: 0.503648  [ 3152/ 3200]\n",
      "loss: 0.789485  [ 3168/ 3200]\n",
      "loss: 0.591532  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036640\n",
      "f1 macro averaged score: 0.779697\n",
      "Accuracy               : 78.4%\n",
      "Confusion matrix       :\n",
      "tensor([[180,  11,   0,   9],\n",
      "        [ 23, 116,  25,  36],\n",
      "        [  0,  16, 170,  14],\n",
      "        [  6,  18,  15, 161]], device='cuda:0')\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.615601  [    0/ 3200]\n",
      "loss: 0.586479  [   16/ 3200]\n",
      "loss: 0.377427  [   32/ 3200]\n",
      "loss: 0.668350  [   48/ 3200]\n",
      "loss: 0.519871  [   64/ 3200]\n",
      "loss: 0.684932  [   80/ 3200]\n",
      "loss: 0.639846  [   96/ 3200]\n",
      "loss: 0.445258  [  112/ 3200]\n",
      "loss: 0.757635  [  128/ 3200]\n",
      "loss: 0.528049  [  144/ 3200]\n",
      "loss: 0.548702  [  160/ 3200]\n",
      "loss: 0.440482  [  176/ 3200]\n",
      "loss: 0.555016  [  192/ 3200]\n",
      "loss: 0.773817  [  208/ 3200]\n",
      "loss: 0.418249  [  224/ 3200]\n",
      "loss: 0.338299  [  240/ 3200]\n",
      "loss: 1.067883  [  256/ 3200]\n",
      "loss: 1.134169  [  272/ 3200]\n",
      "loss: 0.474703  [  288/ 3200]\n",
      "loss: 0.242140  [  304/ 3200]\n",
      "loss: 1.078826  [  320/ 3200]\n",
      "loss: 0.646037  [  336/ 3200]\n",
      "loss: 1.009345  [  352/ 3200]\n",
      "loss: 1.041540  [  368/ 3200]\n",
      "loss: 0.764424  [  384/ 3200]\n",
      "loss: 0.772073  [  400/ 3200]\n",
      "loss: 0.505347  [  416/ 3200]\n",
      "loss: 0.487033  [  432/ 3200]\n",
      "loss: 0.547406  [  448/ 3200]\n",
      "loss: 0.555346  [  464/ 3200]\n",
      "loss: 0.730593  [  480/ 3200]\n",
      "loss: 0.449371  [  496/ 3200]\n",
      "loss: 0.368509  [  512/ 3200]\n",
      "loss: 0.918998  [  528/ 3200]\n",
      "loss: 0.509436  [  544/ 3200]\n",
      "loss: 0.462192  [  560/ 3200]\n",
      "loss: 0.524971  [  576/ 3200]\n",
      "loss: 0.669852  [  592/ 3200]\n",
      "loss: 0.423446  [  608/ 3200]\n",
      "loss: 0.645476  [  624/ 3200]\n",
      "loss: 0.506970  [  640/ 3200]\n",
      "loss: 0.522953  [  656/ 3200]\n",
      "loss: 0.538416  [  672/ 3200]\n",
      "loss: 0.416049  [  688/ 3200]\n",
      "loss: 0.812859  [  704/ 3200]\n",
      "loss: 0.297792  [  720/ 3200]\n",
      "loss: 0.194304  [  736/ 3200]\n",
      "loss: 0.321802  [  752/ 3200]\n",
      "loss: 0.447582  [  768/ 3200]\n",
      "loss: 1.154174  [  784/ 3200]\n",
      "loss: 0.422106  [  800/ 3200]\n",
      "loss: 0.826191  [  816/ 3200]\n",
      "loss: 0.671015  [  832/ 3200]\n",
      "loss: 0.324208  [  848/ 3200]\n",
      "loss: 0.562185  [  864/ 3200]\n",
      "loss: 0.348782  [  880/ 3200]\n",
      "loss: 0.339335  [  896/ 3200]\n",
      "loss: 0.391175  [  912/ 3200]\n",
      "loss: 0.915688  [  928/ 3200]\n",
      "loss: 0.337785  [  944/ 3200]\n",
      "loss: 0.440592  [  960/ 3200]\n",
      "loss: 0.433605  [  976/ 3200]\n",
      "loss: 0.536200  [  992/ 3200]\n",
      "loss: 0.552047  [ 1008/ 3200]\n",
      "loss: 0.675365  [ 1024/ 3200]\n",
      "loss: 0.568921  [ 1040/ 3200]\n",
      "loss: 0.568624  [ 1056/ 3200]\n",
      "loss: 0.550613  [ 1072/ 3200]\n",
      "loss: 0.781288  [ 1088/ 3200]\n",
      "loss: 0.708286  [ 1104/ 3200]\n",
      "loss: 0.537684  [ 1120/ 3200]\n",
      "loss: 0.687384  [ 1136/ 3200]\n",
      "loss: 0.224027  [ 1152/ 3200]\n",
      "loss: 0.494547  [ 1168/ 3200]\n",
      "loss: 0.663977  [ 1184/ 3200]\n",
      "loss: 1.044782  [ 1200/ 3200]\n",
      "loss: 0.912190  [ 1216/ 3200]\n",
      "loss: 0.527012  [ 1232/ 3200]\n",
      "loss: 0.354818  [ 1248/ 3200]\n",
      "loss: 0.980606  [ 1264/ 3200]\n",
      "loss: 0.453872  [ 1280/ 3200]\n",
      "loss: 0.574881  [ 1296/ 3200]\n",
      "loss: 0.707301  [ 1312/ 3200]\n",
      "loss: 0.667215  [ 1328/ 3200]\n",
      "loss: 0.488734  [ 1344/ 3200]\n",
      "loss: 0.334433  [ 1360/ 3200]\n",
      "loss: 0.419418  [ 1376/ 3200]\n",
      "loss: 0.382483  [ 1392/ 3200]\n",
      "loss: 0.650194  [ 1408/ 3200]\n",
      "loss: 0.572445  [ 1424/ 3200]\n",
      "loss: 0.728929  [ 1440/ 3200]\n",
      "loss: 0.429152  [ 1456/ 3200]\n",
      "loss: 0.663534  [ 1472/ 3200]\n",
      "loss: 0.436283  [ 1488/ 3200]\n",
      "loss: 0.303900  [ 1504/ 3200]\n",
      "loss: 0.832477  [ 1520/ 3200]\n",
      "loss: 0.499477  [ 1536/ 3200]\n",
      "loss: 0.379133  [ 1552/ 3200]\n",
      "loss: 0.446096  [ 1568/ 3200]\n",
      "loss: 0.569515  [ 1584/ 3200]\n",
      "loss: 0.397562  [ 1600/ 3200]\n",
      "loss: 0.395671  [ 1616/ 3200]\n",
      "loss: 0.340305  [ 1632/ 3200]\n",
      "loss: 0.520308  [ 1648/ 3200]\n",
      "loss: 0.656303  [ 1664/ 3200]\n",
      "loss: 0.604888  [ 1680/ 3200]\n",
      "loss: 0.393536  [ 1696/ 3200]\n",
      "loss: 0.442411  [ 1712/ 3200]\n",
      "loss: 0.692496  [ 1728/ 3200]\n",
      "loss: 0.849393  [ 1744/ 3200]\n",
      "loss: 0.364596  [ 1760/ 3200]\n",
      "loss: 0.440413  [ 1776/ 3200]\n",
      "loss: 0.234629  [ 1792/ 3200]\n",
      "loss: 0.392321  [ 1808/ 3200]\n",
      "loss: 0.669528  [ 1824/ 3200]\n",
      "loss: 0.619723  [ 1840/ 3200]\n",
      "loss: 0.526095  [ 1856/ 3200]\n",
      "loss: 0.786867  [ 1872/ 3200]\n",
      "loss: 0.885017  [ 1888/ 3200]\n",
      "loss: 0.652107  [ 1904/ 3200]\n",
      "loss: 0.744611  [ 1920/ 3200]\n",
      "loss: 0.204835  [ 1936/ 3200]\n",
      "loss: 0.800396  [ 1952/ 3200]\n",
      "loss: 0.677074  [ 1968/ 3200]\n",
      "loss: 0.424714  [ 1984/ 3200]\n",
      "loss: 0.521059  [ 2000/ 3200]\n",
      "loss: 0.888674  [ 2016/ 3200]\n",
      "loss: 1.318271  [ 2032/ 3200]\n",
      "loss: 0.550437  [ 2048/ 3200]\n",
      "loss: 0.835696  [ 2064/ 3200]\n",
      "loss: 0.629128  [ 2080/ 3200]\n",
      "loss: 0.602940  [ 2096/ 3200]\n",
      "loss: 0.579947  [ 2112/ 3200]\n",
      "loss: 0.371637  [ 2128/ 3200]\n",
      "loss: 0.617903  [ 2144/ 3200]\n",
      "loss: 0.341544  [ 2160/ 3200]\n",
      "loss: 0.573047  [ 2176/ 3200]\n",
      "loss: 0.729566  [ 2192/ 3200]\n",
      "loss: 0.207830  [ 2208/ 3200]\n",
      "loss: 0.490214  [ 2224/ 3200]\n",
      "loss: 0.542302  [ 2240/ 3200]\n",
      "loss: 0.271313  [ 2256/ 3200]\n",
      "loss: 0.449736  [ 2272/ 3200]\n",
      "loss: 0.665103  [ 2288/ 3200]\n",
      "loss: 0.617447  [ 2304/ 3200]\n",
      "loss: 0.613878  [ 2320/ 3200]\n",
      "loss: 0.548761  [ 2336/ 3200]\n",
      "loss: 0.543860  [ 2352/ 3200]\n",
      "loss: 0.931933  [ 2368/ 3200]\n",
      "loss: 0.591065  [ 2384/ 3200]\n",
      "loss: 0.297450  [ 2400/ 3200]\n",
      "loss: 0.919888  [ 2416/ 3200]\n",
      "loss: 0.923889  [ 2432/ 3200]\n",
      "loss: 0.410107  [ 2448/ 3200]\n",
      "loss: 0.197781  [ 2464/ 3200]\n",
      "loss: 0.520021  [ 2480/ 3200]\n",
      "loss: 0.610135  [ 2496/ 3200]\n",
      "loss: 0.569970  [ 2512/ 3200]\n",
      "loss: 0.888696  [ 2528/ 3200]\n",
      "loss: 1.090146  [ 2544/ 3200]\n",
      "loss: 0.540706  [ 2560/ 3200]\n",
      "loss: 0.228177  [ 2576/ 3200]\n",
      "loss: 0.877212  [ 2592/ 3200]\n",
      "loss: 0.646964  [ 2608/ 3200]\n",
      "loss: 0.443678  [ 2624/ 3200]\n",
      "loss: 0.497756  [ 2640/ 3200]\n",
      "loss: 0.611441  [ 2656/ 3200]\n",
      "loss: 0.526699  [ 2672/ 3200]\n",
      "loss: 0.762058  [ 2688/ 3200]\n",
      "loss: 0.539039  [ 2704/ 3200]\n",
      "loss: 0.518232  [ 2720/ 3200]\n",
      "loss: 0.525531  [ 2736/ 3200]\n",
      "loss: 0.658993  [ 2752/ 3200]\n",
      "loss: 0.443706  [ 2768/ 3200]\n",
      "loss: 0.689231  [ 2784/ 3200]\n",
      "loss: 0.426119  [ 2800/ 3200]\n",
      "loss: 0.543348  [ 2816/ 3200]\n",
      "loss: 0.782806  [ 2832/ 3200]\n",
      "loss: 0.582604  [ 2848/ 3200]\n",
      "loss: 0.482454  [ 2864/ 3200]\n",
      "loss: 0.493312  [ 2880/ 3200]\n",
      "loss: 0.652833  [ 2896/ 3200]\n",
      "loss: 0.624761  [ 2912/ 3200]\n",
      "loss: 0.812334  [ 2928/ 3200]\n",
      "loss: 0.844774  [ 2944/ 3200]\n",
      "loss: 0.987469  [ 2960/ 3200]\n",
      "loss: 0.487466  [ 2976/ 3200]\n",
      "loss: 0.533339  [ 2992/ 3200]\n",
      "loss: 0.763927  [ 3008/ 3200]\n",
      "loss: 0.523039  [ 3024/ 3200]\n",
      "loss: 0.681798  [ 3040/ 3200]\n",
      "loss: 0.671032  [ 3056/ 3200]\n",
      "loss: 0.387016  [ 3072/ 3200]\n",
      "loss: 0.262142  [ 3088/ 3200]\n",
      "loss: 0.628854  [ 3104/ 3200]\n",
      "loss: 0.383414  [ 3120/ 3200]\n",
      "loss: 0.264964  [ 3136/ 3200]\n",
      "loss: 0.578261  [ 3152/ 3200]\n",
      "loss: 0.821508  [ 3168/ 3200]\n",
      "loss: 0.476007  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.037657\n",
      "f1 macro averaged score: 0.771862\n",
      "Accuracy               : 76.9%\n",
      "Confusion matrix       :\n",
      "tensor([[156,  38,   1,   5],\n",
      "        [ 10, 142,  25,  23],\n",
      "        [  0,  19, 169,  12],\n",
      "        [  4,  32,  16, 148]], device='cuda:0')\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.543280  [    0/ 3200]\n",
      "loss: 0.446117  [   16/ 3200]\n",
      "loss: 0.615680  [   32/ 3200]\n",
      "loss: 0.538561  [   48/ 3200]\n",
      "loss: 0.670150  [   64/ 3200]\n",
      "loss: 0.756836  [   80/ 3200]\n",
      "loss: 0.504829  [   96/ 3200]\n",
      "loss: 0.695217  [  112/ 3200]\n",
      "loss: 0.626423  [  128/ 3200]\n",
      "loss: 0.630446  [  144/ 3200]\n",
      "loss: 0.535113  [  160/ 3200]\n",
      "loss: 0.307363  [  176/ 3200]\n",
      "loss: 0.266185  [  192/ 3200]\n",
      "loss: 0.598486  [  208/ 3200]\n",
      "loss: 0.483163  [  224/ 3200]\n",
      "loss: 0.629846  [  240/ 3200]\n",
      "loss: 0.411270  [  256/ 3200]\n",
      "loss: 0.547959  [  272/ 3200]\n",
      "loss: 0.536693  [  288/ 3200]\n",
      "loss: 0.672901  [  304/ 3200]\n",
      "loss: 0.511475  [  320/ 3200]\n",
      "loss: 0.306586  [  336/ 3200]\n",
      "loss: 0.775513  [  352/ 3200]\n",
      "loss: 0.724128  [  368/ 3200]\n",
      "loss: 0.660258  [  384/ 3200]\n",
      "loss: 0.529118  [  400/ 3200]\n",
      "loss: 0.389732  [  416/ 3200]\n",
      "loss: 0.602588  [  432/ 3200]\n",
      "loss: 0.539978  [  448/ 3200]\n",
      "loss: 0.493785  [  464/ 3200]\n",
      "loss: 0.455443  [  480/ 3200]\n",
      "loss: 0.712685  [  496/ 3200]\n",
      "loss: 0.334305  [  512/ 3200]\n",
      "loss: 0.504736  [  528/ 3200]\n",
      "loss: 0.748868  [  544/ 3200]\n",
      "loss: 0.547452  [  560/ 3200]\n",
      "loss: 0.518075  [  576/ 3200]\n",
      "loss: 0.670918  [  592/ 3200]\n",
      "loss: 0.583135  [  608/ 3200]\n",
      "loss: 0.564055  [  624/ 3200]\n",
      "loss: 0.582387  [  640/ 3200]\n",
      "loss: 0.846676  [  656/ 3200]\n",
      "loss: 0.589831  [  672/ 3200]\n",
      "loss: 1.262618  [  688/ 3200]\n",
      "loss: 0.795055  [  704/ 3200]\n",
      "loss: 0.363630  [  720/ 3200]\n",
      "loss: 0.317504  [  736/ 3200]\n",
      "loss: 0.630295  [  752/ 3200]\n",
      "loss: 0.662850  [  768/ 3200]\n",
      "loss: 0.452316  [  784/ 3200]\n",
      "loss: 0.468026  [  800/ 3200]\n",
      "loss: 0.485249  [  816/ 3200]\n",
      "loss: 0.627434  [  832/ 3200]\n",
      "loss: 0.531194  [  848/ 3200]\n",
      "loss: 0.471487  [  864/ 3200]\n",
      "loss: 1.006430  [  880/ 3200]\n",
      "loss: 0.812690  [  896/ 3200]\n",
      "loss: 0.349605  [  912/ 3200]\n",
      "loss: 0.456168  [  928/ 3200]\n",
      "loss: 0.553207  [  944/ 3200]\n",
      "loss: 0.597024  [  960/ 3200]\n",
      "loss: 0.712075  [  976/ 3200]\n",
      "loss: 0.309114  [  992/ 3200]\n",
      "loss: 0.808524  [ 1008/ 3200]\n",
      "loss: 0.528544  [ 1024/ 3200]\n",
      "loss: 0.790486  [ 1040/ 3200]\n",
      "loss: 0.528788  [ 1056/ 3200]\n",
      "loss: 0.535254  [ 1072/ 3200]\n",
      "loss: 0.532112  [ 1088/ 3200]\n",
      "loss: 0.484643  [ 1104/ 3200]\n",
      "loss: 0.718506  [ 1120/ 3200]\n",
      "loss: 0.431544  [ 1136/ 3200]\n",
      "loss: 0.606532  [ 1152/ 3200]\n",
      "loss: 0.713346  [ 1168/ 3200]\n",
      "loss: 0.630882  [ 1184/ 3200]\n",
      "loss: 0.494069  [ 1200/ 3200]\n",
      "loss: 0.316546  [ 1216/ 3200]\n",
      "loss: 0.302046  [ 1232/ 3200]\n",
      "loss: 0.632078  [ 1248/ 3200]\n",
      "loss: 0.487127  [ 1264/ 3200]\n",
      "loss: 0.365380  [ 1280/ 3200]\n",
      "loss: 0.338918  [ 1296/ 3200]\n",
      "loss: 0.437178  [ 1312/ 3200]\n",
      "loss: 0.534822  [ 1328/ 3200]\n",
      "loss: 0.719261  [ 1344/ 3200]\n",
      "loss: 0.319356  [ 1360/ 3200]\n",
      "loss: 0.478452  [ 1376/ 3200]\n",
      "loss: 0.377436  [ 1392/ 3200]\n",
      "loss: 0.632774  [ 1408/ 3200]\n",
      "loss: 0.769724  [ 1424/ 3200]\n",
      "loss: 0.720538  [ 1440/ 3200]\n",
      "loss: 0.300617  [ 1456/ 3200]\n",
      "loss: 0.610673  [ 1472/ 3200]\n",
      "loss: 0.534840  [ 1488/ 3200]\n",
      "loss: 0.300602  [ 1504/ 3200]\n",
      "loss: 1.014003  [ 1520/ 3200]\n",
      "loss: 1.335541  [ 1536/ 3200]\n",
      "loss: 0.796989  [ 1552/ 3200]\n",
      "loss: 0.599876  [ 1568/ 3200]\n",
      "loss: 1.036175  [ 1584/ 3200]\n",
      "loss: 0.779250  [ 1600/ 3200]\n",
      "loss: 0.320741  [ 1616/ 3200]\n",
      "loss: 0.708721  [ 1632/ 3200]\n",
      "loss: 0.333209  [ 1648/ 3200]\n",
      "loss: 0.796933  [ 1664/ 3200]\n",
      "loss: 0.887681  [ 1680/ 3200]\n",
      "loss: 0.459440  [ 1696/ 3200]\n",
      "loss: 0.362905  [ 1712/ 3200]\n",
      "loss: 0.540148  [ 1728/ 3200]\n",
      "loss: 0.526728  [ 1744/ 3200]\n",
      "loss: 0.446012  [ 1760/ 3200]\n",
      "loss: 0.481403  [ 1776/ 3200]\n",
      "loss: 0.442612  [ 1792/ 3200]\n",
      "loss: 0.683466  [ 1808/ 3200]\n",
      "loss: 0.401648  [ 1824/ 3200]\n",
      "loss: 0.373504  [ 1840/ 3200]\n",
      "loss: 0.477264  [ 1856/ 3200]\n",
      "loss: 0.668150  [ 1872/ 3200]\n",
      "loss: 0.378546  [ 1888/ 3200]\n",
      "loss: 0.735038  [ 1904/ 3200]\n",
      "loss: 0.634519  [ 1920/ 3200]\n",
      "loss: 0.617689  [ 1936/ 3200]\n",
      "loss: 0.683418  [ 1952/ 3200]\n",
      "loss: 0.413387  [ 1968/ 3200]\n",
      "loss: 0.684556  [ 1984/ 3200]\n",
      "loss: 0.576371  [ 2000/ 3200]\n",
      "loss: 0.502675  [ 2016/ 3200]\n",
      "loss: 0.347478  [ 2032/ 3200]\n",
      "loss: 0.584252  [ 2048/ 3200]\n",
      "loss: 0.460574  [ 2064/ 3200]\n",
      "loss: 0.525171  [ 2080/ 3200]\n",
      "loss: 0.280502  [ 2096/ 3200]\n",
      "loss: 0.406967  [ 2112/ 3200]\n",
      "loss: 0.442084  [ 2128/ 3200]\n",
      "loss: 0.958909  [ 2144/ 3200]\n",
      "loss: 0.324129  [ 2160/ 3200]\n",
      "loss: 0.520717  [ 2176/ 3200]\n",
      "loss: 0.545200  [ 2192/ 3200]\n",
      "loss: 0.594002  [ 2208/ 3200]\n",
      "loss: 0.497824  [ 2224/ 3200]\n",
      "loss: 0.526721  [ 2240/ 3200]\n",
      "loss: 0.542803  [ 2256/ 3200]\n",
      "loss: 0.307043  [ 2272/ 3200]\n",
      "loss: 0.627623  [ 2288/ 3200]\n",
      "loss: 0.512975  [ 2304/ 3200]\n",
      "loss: 0.301122  [ 2320/ 3200]\n",
      "loss: 0.417440  [ 2336/ 3200]\n",
      "loss: 0.557877  [ 2352/ 3200]\n",
      "loss: 0.518322  [ 2368/ 3200]\n",
      "loss: 0.625385  [ 2384/ 3200]\n",
      "loss: 0.361803  [ 2400/ 3200]\n",
      "loss: 0.932113  [ 2416/ 3200]\n",
      "loss: 0.580156  [ 2432/ 3200]\n",
      "loss: 0.248699  [ 2448/ 3200]\n",
      "loss: 0.255534  [ 2464/ 3200]\n",
      "loss: 0.527095  [ 2480/ 3200]\n",
      "loss: 0.302696  [ 2496/ 3200]\n",
      "loss: 0.541140  [ 2512/ 3200]\n",
      "loss: 0.453970  [ 2528/ 3200]\n",
      "loss: 0.550999  [ 2544/ 3200]\n",
      "loss: 0.562988  [ 2560/ 3200]\n",
      "loss: 0.418787  [ 2576/ 3200]\n",
      "loss: 0.446101  [ 2592/ 3200]\n",
      "loss: 0.761689  [ 2608/ 3200]\n",
      "loss: 0.604120  [ 2624/ 3200]\n",
      "loss: 0.757197  [ 2640/ 3200]\n",
      "loss: 0.459485  [ 2656/ 3200]\n",
      "loss: 0.718907  [ 2672/ 3200]\n",
      "loss: 0.657974  [ 2688/ 3200]\n",
      "loss: 0.168534  [ 2704/ 3200]\n",
      "loss: 0.688123  [ 2720/ 3200]\n",
      "loss: 0.587527  [ 2736/ 3200]\n",
      "loss: 0.342880  [ 2752/ 3200]\n",
      "loss: 0.620284  [ 2768/ 3200]\n",
      "loss: 0.343265  [ 2784/ 3200]\n",
      "loss: 0.715605  [ 2800/ 3200]\n",
      "loss: 0.624673  [ 2816/ 3200]\n",
      "loss: 0.502102  [ 2832/ 3200]\n",
      "loss: 0.349715  [ 2848/ 3200]\n",
      "loss: 0.307780  [ 2864/ 3200]\n",
      "loss: 0.543900  [ 2880/ 3200]\n",
      "loss: 0.533642  [ 2896/ 3200]\n",
      "loss: 0.807876  [ 2912/ 3200]\n",
      "loss: 0.846316  [ 2928/ 3200]\n",
      "loss: 0.602912  [ 2944/ 3200]\n",
      "loss: 0.393349  [ 2960/ 3200]\n",
      "loss: 0.607842  [ 2976/ 3200]\n",
      "loss: 0.364337  [ 2992/ 3200]\n",
      "loss: 0.395020  [ 3008/ 3200]\n",
      "loss: 0.456715  [ 3024/ 3200]\n",
      "loss: 0.678576  [ 3040/ 3200]\n",
      "loss: 0.527460  [ 3056/ 3200]\n",
      "loss: 0.542001  [ 3072/ 3200]\n",
      "loss: 0.642532  [ 3088/ 3200]\n",
      "loss: 0.394330  [ 3104/ 3200]\n",
      "loss: 0.314921  [ 3120/ 3200]\n",
      "loss: 0.684840  [ 3136/ 3200]\n",
      "loss: 0.612378  [ 3152/ 3200]\n",
      "loss: 0.463100  [ 3168/ 3200]\n",
      "loss: 1.066864  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038143\n",
      "f1 macro averaged score: 0.779686\n",
      "Accuracy               : 78.0%\n",
      "Confusion matrix       :\n",
      "tensor([[193,   7,   0,   0],\n",
      "        [ 29, 157,  12,   2],\n",
      "        [  0,  42, 155,   3],\n",
      "        [ 12,  55,  14, 119]], device='cuda:0')\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 0.993948  [    0/ 3200]\n",
      "loss: 0.718004  [   16/ 3200]\n",
      "loss: 0.357775  [   32/ 3200]\n",
      "loss: 0.411525  [   48/ 3200]\n",
      "loss: 0.442659  [   64/ 3200]\n",
      "loss: 0.294520  [   80/ 3200]\n",
      "loss: 0.499338  [   96/ 3200]\n",
      "loss: 0.511749  [  112/ 3200]\n",
      "loss: 0.368973  [  128/ 3200]\n",
      "loss: 0.383211  [  144/ 3200]\n",
      "loss: 0.460057  [  160/ 3200]\n",
      "loss: 0.453032  [  176/ 3200]\n",
      "loss: 0.559260  [  192/ 3200]\n",
      "loss: 0.475253  [  208/ 3200]\n",
      "loss: 0.378084  [  224/ 3200]\n",
      "loss: 0.505915  [  240/ 3200]\n",
      "loss: 0.455908  [  256/ 3200]\n",
      "loss: 0.427784  [  272/ 3200]\n",
      "loss: 0.752933  [  288/ 3200]\n",
      "loss: 0.806080  [  304/ 3200]\n",
      "loss: 0.456910  [  320/ 3200]\n",
      "loss: 0.525147  [  336/ 3200]\n",
      "loss: 0.503167  [  352/ 3200]\n",
      "loss: 0.380257  [  368/ 3200]\n",
      "loss: 0.437274  [  384/ 3200]\n",
      "loss: 0.415109  [  400/ 3200]\n",
      "loss: 0.500607  [  416/ 3200]\n",
      "loss: 0.700827  [  432/ 3200]\n",
      "loss: 0.732884  [  448/ 3200]\n",
      "loss: 1.258277  [  464/ 3200]\n",
      "loss: 0.860652  [  480/ 3200]\n",
      "loss: 0.581368  [  496/ 3200]\n",
      "loss: 0.449900  [  512/ 3200]\n",
      "loss: 0.478525  [  528/ 3200]\n",
      "loss: 0.481681  [  544/ 3200]\n",
      "loss: 0.355971  [  560/ 3200]\n",
      "loss: 0.524161  [  576/ 3200]\n",
      "loss: 0.519240  [  592/ 3200]\n",
      "loss: 0.432108  [  608/ 3200]\n",
      "loss: 0.548552  [  624/ 3200]\n",
      "loss: 0.390345  [  640/ 3200]\n",
      "loss: 0.858428  [  656/ 3200]\n",
      "loss: 0.478881  [  672/ 3200]\n",
      "loss: 0.427076  [  688/ 3200]\n",
      "loss: 1.118348  [  704/ 3200]\n",
      "loss: 0.683569  [  720/ 3200]\n",
      "loss: 0.256381  [  736/ 3200]\n",
      "loss: 0.391103  [  752/ 3200]\n",
      "loss: 0.607199  [  768/ 3200]\n",
      "loss: 0.248352  [  784/ 3200]\n",
      "loss: 0.635494  [  800/ 3200]\n",
      "loss: 0.463588  [  816/ 3200]\n",
      "loss: 0.553625  [  832/ 3200]\n",
      "loss: 0.562591  [  848/ 3200]\n",
      "loss: 0.499108  [  864/ 3200]\n",
      "loss: 0.806952  [  880/ 3200]\n",
      "loss: 0.315257  [  896/ 3200]\n",
      "loss: 0.409400  [  912/ 3200]\n",
      "loss: 0.598183  [  928/ 3200]\n",
      "loss: 0.522726  [  944/ 3200]\n",
      "loss: 0.291874  [  960/ 3200]\n",
      "loss: 0.502340  [  976/ 3200]\n",
      "loss: 0.705050  [  992/ 3200]\n",
      "loss: 0.242550  [ 1008/ 3200]\n",
      "loss: 0.709873  [ 1024/ 3200]\n",
      "loss: 0.483109  [ 1040/ 3200]\n",
      "loss: 1.130294  [ 1056/ 3200]\n",
      "loss: 0.531196  [ 1072/ 3200]\n",
      "loss: 0.207164  [ 1088/ 3200]\n",
      "loss: 0.723495  [ 1104/ 3200]\n",
      "loss: 0.271608  [ 1120/ 3200]\n",
      "loss: 0.759031  [ 1136/ 3200]\n",
      "loss: 0.503730  [ 1152/ 3200]\n",
      "loss: 0.332434  [ 1168/ 3200]\n",
      "loss: 0.493213  [ 1184/ 3200]\n",
      "loss: 0.333162  [ 1200/ 3200]\n",
      "loss: 0.176462  [ 1216/ 3200]\n",
      "loss: 0.323557  [ 1232/ 3200]\n",
      "loss: 0.656431  [ 1248/ 3200]\n",
      "loss: 0.671130  [ 1264/ 3200]\n",
      "loss: 0.279334  [ 1280/ 3200]\n",
      "loss: 0.287799  [ 1296/ 3200]\n",
      "loss: 0.416854  [ 1312/ 3200]\n",
      "loss: 0.585904  [ 1328/ 3200]\n",
      "loss: 0.605328  [ 1344/ 3200]\n",
      "loss: 0.468810  [ 1360/ 3200]\n",
      "loss: 0.453109  [ 1376/ 3200]\n",
      "loss: 0.878751  [ 1392/ 3200]\n",
      "loss: 0.618369  [ 1408/ 3200]\n",
      "loss: 0.398777  [ 1424/ 3200]\n",
      "loss: 0.847929  [ 1440/ 3200]\n",
      "loss: 0.679435  [ 1456/ 3200]\n",
      "loss: 0.561966  [ 1472/ 3200]\n",
      "loss: 0.344306  [ 1488/ 3200]\n",
      "loss: 0.536916  [ 1504/ 3200]\n",
      "loss: 0.839838  [ 1520/ 3200]\n",
      "loss: 0.668517  [ 1536/ 3200]\n",
      "loss: 0.753184  [ 1552/ 3200]\n",
      "loss: 0.276862  [ 1568/ 3200]\n",
      "loss: 0.494279  [ 1584/ 3200]\n",
      "loss: 0.367083  [ 1600/ 3200]\n",
      "loss: 0.501375  [ 1616/ 3200]\n",
      "loss: 0.542562  [ 1632/ 3200]\n",
      "loss: 0.561415  [ 1648/ 3200]\n",
      "loss: 0.359341  [ 1664/ 3200]\n",
      "loss: 0.437950  [ 1680/ 3200]\n",
      "loss: 1.051320  [ 1696/ 3200]\n",
      "loss: 1.055048  [ 1712/ 3200]\n",
      "loss: 0.288318  [ 1728/ 3200]\n",
      "loss: 0.459575  [ 1744/ 3200]\n",
      "loss: 0.865860  [ 1760/ 3200]\n",
      "loss: 0.297621  [ 1776/ 3200]\n",
      "loss: 0.795554  [ 1792/ 3200]\n",
      "loss: 0.352874  [ 1808/ 3200]\n",
      "loss: 0.421983  [ 1824/ 3200]\n",
      "loss: 0.659424  [ 1840/ 3200]\n",
      "loss: 0.722000  [ 1856/ 3200]\n",
      "loss: 0.610569  [ 1872/ 3200]\n",
      "loss: 0.328066  [ 1888/ 3200]\n",
      "loss: 0.432636  [ 1904/ 3200]\n",
      "loss: 0.789804  [ 1920/ 3200]\n",
      "loss: 0.510053  [ 1936/ 3200]\n",
      "loss: 0.420547  [ 1952/ 3200]\n",
      "loss: 0.338368  [ 1968/ 3200]\n",
      "loss: 0.412654  [ 1984/ 3200]\n",
      "loss: 0.753755  [ 2000/ 3200]\n",
      "loss: 0.401592  [ 2016/ 3200]\n",
      "loss: 0.327637  [ 2032/ 3200]\n",
      "loss: 0.716712  [ 2048/ 3200]\n",
      "loss: 0.691482  [ 2064/ 3200]\n",
      "loss: 0.689877  [ 2080/ 3200]\n",
      "loss: 0.729724  [ 2096/ 3200]\n",
      "loss: 0.758893  [ 2112/ 3200]\n",
      "loss: 0.337992  [ 2128/ 3200]\n",
      "loss: 0.410399  [ 2144/ 3200]\n",
      "loss: 0.502905  [ 2160/ 3200]\n",
      "loss: 0.816878  [ 2176/ 3200]\n",
      "loss: 0.437999  [ 2192/ 3200]\n",
      "loss: 0.704455  [ 2208/ 3200]\n",
      "loss: 0.967704  [ 2224/ 3200]\n",
      "loss: 0.562199  [ 2240/ 3200]\n",
      "loss: 0.467628  [ 2256/ 3200]\n",
      "loss: 0.448511  [ 2272/ 3200]\n",
      "loss: 0.370618  [ 2288/ 3200]\n",
      "loss: 1.041454  [ 2304/ 3200]\n",
      "loss: 0.985115  [ 2320/ 3200]\n",
      "loss: 0.476417  [ 2336/ 3200]\n",
      "loss: 0.507067  [ 2352/ 3200]\n",
      "loss: 0.374185  [ 2368/ 3200]\n",
      "loss: 0.383013  [ 2384/ 3200]\n",
      "loss: 0.384217  [ 2400/ 3200]\n",
      "loss: 0.281915  [ 2416/ 3200]\n",
      "loss: 0.698879  [ 2432/ 3200]\n",
      "loss: 0.667281  [ 2448/ 3200]\n",
      "loss: 0.404573  [ 2464/ 3200]\n",
      "loss: 0.329099  [ 2480/ 3200]\n",
      "loss: 0.613059  [ 2496/ 3200]\n",
      "loss: 0.553727  [ 2512/ 3200]\n",
      "loss: 0.521256  [ 2528/ 3200]\n",
      "loss: 0.395506  [ 2544/ 3200]\n",
      "loss: 0.440060  [ 2560/ 3200]\n",
      "loss: 0.329938  [ 2576/ 3200]\n",
      "loss: 0.696384  [ 2592/ 3200]\n",
      "loss: 0.289115  [ 2608/ 3200]\n",
      "loss: 0.396005  [ 2624/ 3200]\n",
      "loss: 0.668085  [ 2640/ 3200]\n",
      "loss: 0.775486  [ 2656/ 3200]\n",
      "loss: 0.820651  [ 2672/ 3200]\n",
      "loss: 1.201590  [ 2688/ 3200]\n",
      "loss: 1.064014  [ 2704/ 3200]\n",
      "loss: 0.510315  [ 2720/ 3200]\n",
      "loss: 0.566833  [ 2736/ 3200]\n",
      "loss: 0.396627  [ 2752/ 3200]\n",
      "loss: 0.445006  [ 2768/ 3200]\n",
      "loss: 0.623232  [ 2784/ 3200]\n",
      "loss: 0.611269  [ 2800/ 3200]\n",
      "loss: 0.382312  [ 2816/ 3200]\n",
      "loss: 0.396343  [ 2832/ 3200]\n",
      "loss: 0.469468  [ 2848/ 3200]\n",
      "loss: 0.528713  [ 2864/ 3200]\n",
      "loss: 0.434426  [ 2880/ 3200]\n",
      "loss: 0.435849  [ 2896/ 3200]\n",
      "loss: 0.247294  [ 2912/ 3200]\n",
      "loss: 0.317125  [ 2928/ 3200]\n",
      "loss: 0.712251  [ 2944/ 3200]\n",
      "loss: 0.471379  [ 2960/ 3200]\n",
      "loss: 0.691921  [ 2976/ 3200]\n",
      "loss: 0.396383  [ 2992/ 3200]\n",
      "loss: 0.690481  [ 3008/ 3200]\n",
      "loss: 0.497263  [ 3024/ 3200]\n",
      "loss: 0.375134  [ 3040/ 3200]\n",
      "loss: 0.721443  [ 3056/ 3200]\n",
      "loss: 0.331663  [ 3072/ 3200]\n",
      "loss: 0.596687  [ 3088/ 3200]\n",
      "loss: 0.591363  [ 3104/ 3200]\n",
      "loss: 0.541472  [ 3120/ 3200]\n",
      "loss: 0.843817  [ 3136/ 3200]\n",
      "loss: 0.626360  [ 3152/ 3200]\n",
      "loss: 0.501022  [ 3168/ 3200]\n",
      "loss: 0.713761  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035937\n",
      "f1 macro averaged score: 0.783183\n",
      "Accuracy               : 78.2%\n",
      "Confusion matrix       :\n",
      "tensor([[177,  13,   0,  10],\n",
      "        [ 18, 135,  16,  31],\n",
      "        [  0,  27, 157,  16],\n",
      "        [  6,  27,  10, 157]], device='cuda:0')\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.490123  [    0/ 3200]\n",
      "loss: 0.391826  [   16/ 3200]\n",
      "loss: 0.685630  [   32/ 3200]\n",
      "loss: 0.627557  [   48/ 3200]\n",
      "loss: 0.458527  [   64/ 3200]\n",
      "loss: 0.742979  [   80/ 3200]\n",
      "loss: 0.407999  [   96/ 3200]\n",
      "loss: 0.419426  [  112/ 3200]\n",
      "loss: 0.541312  [  128/ 3200]\n",
      "loss: 0.429453  [  144/ 3200]\n",
      "loss: 0.882851  [  160/ 3200]\n",
      "loss: 0.589227  [  176/ 3200]\n",
      "loss: 0.384384  [  192/ 3200]\n",
      "loss: 0.511316  [  208/ 3200]\n",
      "loss: 0.588933  [  224/ 3200]\n",
      "loss: 0.796147  [  240/ 3200]\n",
      "loss: 0.460052  [  256/ 3200]\n",
      "loss: 0.595064  [  272/ 3200]\n",
      "loss: 0.297759  [  288/ 3200]\n",
      "loss: 0.974549  [  304/ 3200]\n",
      "loss: 0.361658  [  320/ 3200]\n",
      "loss: 0.646727  [  336/ 3200]\n",
      "loss: 0.168569  [  352/ 3200]\n",
      "loss: 0.977558  [  368/ 3200]\n",
      "loss: 1.001860  [  384/ 3200]\n",
      "loss: 1.017351  [  400/ 3200]\n",
      "loss: 0.841309  [  416/ 3200]\n",
      "loss: 0.494345  [  432/ 3200]\n",
      "loss: 0.770656  [  448/ 3200]\n",
      "loss: 0.467107  [  464/ 3200]\n",
      "loss: 0.207707  [  480/ 3200]\n",
      "loss: 0.559363  [  496/ 3200]\n",
      "loss: 0.673804  [  512/ 3200]\n",
      "loss: 0.316481  [  528/ 3200]\n",
      "loss: 0.219308  [  544/ 3200]\n",
      "loss: 0.281134  [  560/ 3200]\n",
      "loss: 0.899957  [  576/ 3200]\n",
      "loss: 0.249098  [  592/ 3200]\n",
      "loss: 0.252706  [  608/ 3200]\n",
      "loss: 0.285047  [  624/ 3200]\n",
      "loss: 0.763980  [  640/ 3200]\n",
      "loss: 0.580799  [  656/ 3200]\n",
      "loss: 0.539135  [  672/ 3200]\n",
      "loss: 0.774575  [  688/ 3200]\n",
      "loss: 0.330444  [  704/ 3200]\n",
      "loss: 0.191683  [  720/ 3200]\n",
      "loss: 0.808494  [  736/ 3200]\n",
      "loss: 0.363798  [  752/ 3200]\n",
      "loss: 0.612203  [  768/ 3200]\n",
      "loss: 0.443357  [  784/ 3200]\n",
      "loss: 0.518546  [  800/ 3200]\n",
      "loss: 0.369565  [  816/ 3200]\n",
      "loss: 0.455203  [  832/ 3200]\n",
      "loss: 0.914620  [  848/ 3200]\n",
      "loss: 0.743547  [  864/ 3200]\n",
      "loss: 0.543251  [  880/ 3200]\n",
      "loss: 0.319477  [  896/ 3200]\n",
      "loss: 0.570549  [  912/ 3200]\n",
      "loss: 0.540407  [  928/ 3200]\n",
      "loss: 0.615733  [  944/ 3200]\n",
      "loss: 0.357184  [  960/ 3200]\n",
      "loss: 0.574501  [  976/ 3200]\n",
      "loss: 0.365039  [  992/ 3200]\n",
      "loss: 0.574436  [ 1008/ 3200]\n",
      "loss: 0.645960  [ 1024/ 3200]\n",
      "loss: 0.807302  [ 1040/ 3200]\n",
      "loss: 0.624805  [ 1056/ 3200]\n",
      "loss: 0.469414  [ 1072/ 3200]\n",
      "loss: 0.603391  [ 1088/ 3200]\n",
      "loss: 0.288759  [ 1104/ 3200]\n",
      "loss: 0.667436  [ 1120/ 3200]\n",
      "loss: 0.670824  [ 1136/ 3200]\n",
      "loss: 0.389910  [ 1152/ 3200]\n",
      "loss: 0.594587  [ 1168/ 3200]\n",
      "loss: 0.673359  [ 1184/ 3200]\n",
      "loss: 0.430751  [ 1200/ 3200]\n",
      "loss: 0.390015  [ 1216/ 3200]\n",
      "loss: 0.452713  [ 1232/ 3200]\n",
      "loss: 0.416016  [ 1248/ 3200]\n",
      "loss: 0.718102  [ 1264/ 3200]\n",
      "loss: 0.579505  [ 1280/ 3200]\n",
      "loss: 0.824679  [ 1296/ 3200]\n",
      "loss: 0.702356  [ 1312/ 3200]\n",
      "loss: 0.557502  [ 1328/ 3200]\n",
      "loss: 0.568215  [ 1344/ 3200]\n",
      "loss: 0.534103  [ 1360/ 3200]\n",
      "loss: 0.467686  [ 1376/ 3200]\n",
      "loss: 0.350925  [ 1392/ 3200]\n",
      "loss: 0.607633  [ 1408/ 3200]\n",
      "loss: 0.469861  [ 1424/ 3200]\n",
      "loss: 0.375455  [ 1440/ 3200]\n",
      "loss: 0.776150  [ 1456/ 3200]\n",
      "loss: 0.543694  [ 1472/ 3200]\n",
      "loss: 0.246429  [ 1488/ 3200]\n",
      "loss: 0.602302  [ 1504/ 3200]\n",
      "loss: 0.440501  [ 1520/ 3200]\n",
      "loss: 0.515016  [ 1536/ 3200]\n",
      "loss: 0.729893  [ 1552/ 3200]\n",
      "loss: 0.527840  [ 1568/ 3200]\n",
      "loss: 0.434254  [ 1584/ 3200]\n",
      "loss: 0.654038  [ 1600/ 3200]\n",
      "loss: 0.207501  [ 1616/ 3200]\n",
      "loss: 0.278912  [ 1632/ 3200]\n",
      "loss: 0.317993  [ 1648/ 3200]\n",
      "loss: 0.438376  [ 1664/ 3200]\n",
      "loss: 0.278530  [ 1680/ 3200]\n",
      "loss: 0.768202  [ 1696/ 3200]\n",
      "loss: 0.681943  [ 1712/ 3200]\n",
      "loss: 0.368620  [ 1728/ 3200]\n",
      "loss: 0.272470  [ 1744/ 3200]\n",
      "loss: 0.601697  [ 1760/ 3200]\n",
      "loss: 0.493353  [ 1776/ 3200]\n",
      "loss: 0.264378  [ 1792/ 3200]\n",
      "loss: 0.426986  [ 1808/ 3200]\n",
      "loss: 0.506088  [ 1824/ 3200]\n",
      "loss: 0.618458  [ 1840/ 3200]\n",
      "loss: 0.397221  [ 1856/ 3200]\n",
      "loss: 0.393976  [ 1872/ 3200]\n",
      "loss: 0.417065  [ 1888/ 3200]\n",
      "loss: 0.764261  [ 1904/ 3200]\n",
      "loss: 0.395704  [ 1920/ 3200]\n",
      "loss: 0.294598  [ 1936/ 3200]\n",
      "loss: 0.563190  [ 1952/ 3200]\n",
      "loss: 0.487342  [ 1968/ 3200]\n",
      "loss: 0.739339  [ 1984/ 3200]\n",
      "loss: 0.892519  [ 2000/ 3200]\n",
      "loss: 0.534726  [ 2016/ 3200]\n",
      "loss: 0.576876  [ 2032/ 3200]\n",
      "loss: 0.290654  [ 2048/ 3200]\n",
      "loss: 0.581339  [ 2064/ 3200]\n",
      "loss: 0.463792  [ 2080/ 3200]\n",
      "loss: 0.422737  [ 2096/ 3200]\n",
      "loss: 0.297204  [ 2112/ 3200]\n",
      "loss: 0.236941  [ 2128/ 3200]\n",
      "loss: 0.583166  [ 2144/ 3200]\n",
      "loss: 0.457036  [ 2160/ 3200]\n",
      "loss: 0.674174  [ 2176/ 3200]\n",
      "loss: 0.429166  [ 2192/ 3200]\n",
      "loss: 0.578706  [ 2208/ 3200]\n",
      "loss: 0.423893  [ 2224/ 3200]\n",
      "loss: 1.302493  [ 2240/ 3200]\n",
      "loss: 0.599689  [ 2256/ 3200]\n",
      "loss: 0.480791  [ 2272/ 3200]\n",
      "loss: 0.467866  [ 2288/ 3200]\n",
      "loss: 0.320720  [ 2304/ 3200]\n",
      "loss: 0.662130  [ 2320/ 3200]\n",
      "loss: 0.634153  [ 2336/ 3200]\n",
      "loss: 0.685209  [ 2352/ 3200]\n",
      "loss: 0.848219  [ 2368/ 3200]\n",
      "loss: 0.832480  [ 2384/ 3200]\n",
      "loss: 0.476108  [ 2400/ 3200]\n",
      "loss: 0.246048  [ 2416/ 3200]\n",
      "loss: 0.860405  [ 2432/ 3200]\n",
      "loss: 0.277040  [ 2448/ 3200]\n",
      "loss: 0.753557  [ 2464/ 3200]\n",
      "loss: 0.617767  [ 2480/ 3200]\n",
      "loss: 0.960982  [ 2496/ 3200]\n",
      "loss: 0.647751  [ 2512/ 3200]\n",
      "loss: 0.385569  [ 2528/ 3200]\n",
      "loss: 0.322690  [ 2544/ 3200]\n",
      "loss: 0.475708  [ 2560/ 3200]\n",
      "loss: 0.397767  [ 2576/ 3200]\n",
      "loss: 0.386000  [ 2592/ 3200]\n",
      "loss: 0.761442  [ 2608/ 3200]\n",
      "loss: 1.409905  [ 2624/ 3200]\n",
      "loss: 0.534257  [ 2640/ 3200]\n",
      "loss: 0.383006  [ 2656/ 3200]\n",
      "loss: 0.794740  [ 2672/ 3200]\n",
      "loss: 0.599045  [ 2688/ 3200]\n",
      "loss: 0.557151  [ 2704/ 3200]\n",
      "loss: 0.420229  [ 2720/ 3200]\n",
      "loss: 0.246833  [ 2736/ 3200]\n",
      "loss: 0.652149  [ 2752/ 3200]\n",
      "loss: 0.332158  [ 2768/ 3200]\n",
      "loss: 0.601682  [ 2784/ 3200]\n",
      "loss: 0.346502  [ 2800/ 3200]\n",
      "loss: 0.483829  [ 2816/ 3200]\n",
      "loss: 0.788232  [ 2832/ 3200]\n",
      "loss: 0.428874  [ 2848/ 3200]\n",
      "loss: 0.340527  [ 2864/ 3200]\n",
      "loss: 0.372235  [ 2880/ 3200]\n",
      "loss: 0.572269  [ 2896/ 3200]\n",
      "loss: 0.816445  [ 2912/ 3200]\n",
      "loss: 0.481688  [ 2928/ 3200]\n",
      "loss: 0.785764  [ 2944/ 3200]\n",
      "loss: 0.461137  [ 2960/ 3200]\n",
      "loss: 0.568669  [ 2976/ 3200]\n",
      "loss: 0.497049  [ 2992/ 3200]\n",
      "loss: 0.710151  [ 3008/ 3200]\n",
      "loss: 0.315961  [ 3024/ 3200]\n",
      "loss: 0.506097  [ 3040/ 3200]\n",
      "loss: 0.430776  [ 3056/ 3200]\n",
      "loss: 0.278154  [ 3072/ 3200]\n",
      "loss: 0.208947  [ 3088/ 3200]\n",
      "loss: 0.432444  [ 3104/ 3200]\n",
      "loss: 0.633744  [ 3120/ 3200]\n",
      "loss: 0.532861  [ 3136/ 3200]\n",
      "loss: 0.975747  [ 3152/ 3200]\n",
      "loss: 0.395236  [ 3168/ 3200]\n",
      "loss: 0.650048  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038393\n",
      "f1 macro averaged score: 0.763713\n",
      "Accuracy               : 76.1%\n",
      "Confusion matrix       :\n",
      "tensor([[147,  38,   1,  14],\n",
      "        [  9, 140,  28,  23],\n",
      "        [  0,  18, 170,  12],\n",
      "        [  4,  29,  15, 152]], device='cuda:0')\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 0.445920  [    0/ 3200]\n",
      "loss: 0.448824  [   16/ 3200]\n",
      "loss: 0.437602  [   32/ 3200]\n",
      "loss: 0.410559  [   48/ 3200]\n",
      "loss: 0.904237  [   64/ 3200]\n",
      "loss: 0.413435  [   80/ 3200]\n",
      "loss: 0.704272  [   96/ 3200]\n",
      "loss: 0.698868  [  112/ 3200]\n",
      "loss: 0.512320  [  128/ 3200]\n",
      "loss: 0.359356  [  144/ 3200]\n",
      "loss: 0.506700  [  160/ 3200]\n",
      "loss: 0.240688  [  176/ 3200]\n",
      "loss: 0.476455  [  192/ 3200]\n",
      "loss: 0.635275  [  208/ 3200]\n",
      "loss: 0.383800  [  224/ 3200]\n",
      "loss: 0.698935  [  240/ 3200]\n",
      "loss: 0.592162  [  256/ 3200]\n",
      "loss: 0.318044  [  272/ 3200]\n",
      "loss: 0.760734  [  288/ 3200]\n",
      "loss: 0.558135  [  304/ 3200]\n",
      "loss: 0.362728  [  320/ 3200]\n",
      "loss: 0.495823  [  336/ 3200]\n",
      "loss: 0.231279  [  352/ 3200]\n",
      "loss: 0.512361  [  368/ 3200]\n",
      "loss: 0.225827  [  384/ 3200]\n",
      "loss: 0.700647  [  400/ 3200]\n",
      "loss: 1.048501  [  416/ 3200]\n",
      "loss: 1.099079  [  432/ 3200]\n",
      "loss: 0.561844  [  448/ 3200]\n",
      "loss: 0.164834  [  464/ 3200]\n",
      "loss: 0.613590  [  480/ 3200]\n",
      "loss: 0.479027  [  496/ 3200]\n",
      "loss: 0.540522  [  512/ 3200]\n",
      "loss: 0.576102  [  528/ 3200]\n",
      "loss: 0.839501  [  544/ 3200]\n",
      "loss: 0.226759  [  560/ 3200]\n",
      "loss: 0.678175  [  576/ 3200]\n",
      "loss: 0.641942  [  592/ 3200]\n",
      "loss: 0.348976  [  608/ 3200]\n",
      "loss: 0.333860  [  624/ 3200]\n",
      "loss: 0.634512  [  640/ 3200]\n",
      "loss: 0.570156  [  656/ 3200]\n",
      "loss: 1.055349  [  672/ 3200]\n",
      "loss: 0.644664  [  688/ 3200]\n",
      "loss: 0.737747  [  704/ 3200]\n",
      "loss: 0.961542  [  720/ 3200]\n",
      "loss: 0.682693  [  736/ 3200]\n",
      "loss: 0.404384  [  752/ 3200]\n",
      "loss: 0.218297  [  768/ 3200]\n",
      "loss: 0.688773  [  784/ 3200]\n",
      "loss: 0.321176  [  800/ 3200]\n",
      "loss: 0.453343  [  816/ 3200]\n",
      "loss: 0.507669  [  832/ 3200]\n",
      "loss: 0.728184  [  848/ 3200]\n",
      "loss: 0.328174  [  864/ 3200]\n",
      "loss: 0.481914  [  880/ 3200]\n",
      "loss: 0.477107  [  896/ 3200]\n",
      "loss: 0.521715  [  912/ 3200]\n",
      "loss: 0.290553  [  928/ 3200]\n",
      "loss: 0.306175  [  944/ 3200]\n",
      "loss: 0.764783  [  960/ 3200]\n",
      "loss: 0.664414  [  976/ 3200]\n",
      "loss: 0.529742  [  992/ 3200]\n",
      "loss: 0.392952  [ 1008/ 3200]\n",
      "loss: 0.519573  [ 1024/ 3200]\n",
      "loss: 0.616847  [ 1040/ 3200]\n",
      "loss: 0.787508  [ 1056/ 3200]\n",
      "loss: 0.574398  [ 1072/ 3200]\n",
      "loss: 0.643743  [ 1088/ 3200]\n",
      "loss: 0.319775  [ 1104/ 3200]\n",
      "loss: 0.474412  [ 1120/ 3200]\n",
      "loss: 0.478527  [ 1136/ 3200]\n",
      "loss: 0.616918  [ 1152/ 3200]\n",
      "loss: 0.682242  [ 1168/ 3200]\n",
      "loss: 0.901967  [ 1184/ 3200]\n",
      "loss: 1.216407  [ 1200/ 3200]\n",
      "loss: 0.511584  [ 1216/ 3200]\n",
      "loss: 0.352925  [ 1232/ 3200]\n",
      "loss: 0.418096  [ 1248/ 3200]\n",
      "loss: 0.310033  [ 1264/ 3200]\n",
      "loss: 0.637303  [ 1280/ 3200]\n",
      "loss: 0.457104  [ 1296/ 3200]\n",
      "loss: 0.530891  [ 1312/ 3200]\n",
      "loss: 0.531228  [ 1328/ 3200]\n",
      "loss: 0.407549  [ 1344/ 3200]\n",
      "loss: 0.210957  [ 1360/ 3200]\n",
      "loss: 0.225090  [ 1376/ 3200]\n",
      "loss: 0.571693  [ 1392/ 3200]\n",
      "loss: 0.729170  [ 1408/ 3200]\n",
      "loss: 0.260111  [ 1424/ 3200]\n",
      "loss: 0.414639  [ 1440/ 3200]\n",
      "loss: 0.648117  [ 1456/ 3200]\n",
      "loss: 0.738196  [ 1472/ 3200]\n",
      "loss: 0.680139  [ 1488/ 3200]\n",
      "loss: 0.468142  [ 1504/ 3200]\n",
      "loss: 0.586321  [ 1520/ 3200]\n",
      "loss: 0.395032  [ 1536/ 3200]\n",
      "loss: 0.822647  [ 1552/ 3200]\n",
      "loss: 0.890698  [ 1568/ 3200]\n",
      "loss: 0.446349  [ 1584/ 3200]\n",
      "loss: 0.551039  [ 1600/ 3200]\n",
      "loss: 0.395169  [ 1616/ 3200]\n",
      "loss: 0.440006  [ 1632/ 3200]\n",
      "loss: 0.476174  [ 1648/ 3200]\n",
      "loss: 0.512758  [ 1664/ 3200]\n",
      "loss: 0.434016  [ 1680/ 3200]\n",
      "loss: 0.663635  [ 1696/ 3200]\n",
      "loss: 0.644142  [ 1712/ 3200]\n",
      "loss: 0.783462  [ 1728/ 3200]\n",
      "loss: 0.667406  [ 1744/ 3200]\n",
      "loss: 0.513296  [ 1760/ 3200]\n",
      "loss: 0.411286  [ 1776/ 3200]\n",
      "loss: 0.711534  [ 1792/ 3200]\n",
      "loss: 0.445993  [ 1808/ 3200]\n",
      "loss: 0.841643  [ 1824/ 3200]\n",
      "loss: 0.703797  [ 1840/ 3200]\n",
      "loss: 0.356408  [ 1856/ 3200]\n",
      "loss: 0.484274  [ 1872/ 3200]\n",
      "loss: 1.168317  [ 1888/ 3200]\n",
      "loss: 0.844986  [ 1904/ 3200]\n",
      "loss: 0.345571  [ 1920/ 3200]\n",
      "loss: 0.563521  [ 1936/ 3200]\n",
      "loss: 0.513003  [ 1952/ 3200]\n",
      "loss: 0.619233  [ 1968/ 3200]\n",
      "loss: 0.811473  [ 1984/ 3200]\n",
      "loss: 0.783443  [ 2000/ 3200]\n",
      "loss: 0.419269  [ 2016/ 3200]\n",
      "loss: 0.324378  [ 2032/ 3200]\n",
      "loss: 0.740578  [ 2048/ 3200]\n",
      "loss: 0.389045  [ 2064/ 3200]\n",
      "loss: 0.344903  [ 2080/ 3200]\n",
      "loss: 0.787096  [ 2096/ 3200]\n",
      "loss: 0.363771  [ 2112/ 3200]\n",
      "loss: 0.520865  [ 2128/ 3200]\n",
      "loss: 0.431168  [ 2144/ 3200]\n",
      "loss: 0.566714  [ 2160/ 3200]\n",
      "loss: 0.428931  [ 2176/ 3200]\n",
      "loss: 0.500116  [ 2192/ 3200]\n",
      "loss: 0.214734  [ 2208/ 3200]\n",
      "loss: 0.531413  [ 2224/ 3200]\n",
      "loss: 0.479120  [ 2240/ 3200]\n",
      "loss: 0.350192  [ 2256/ 3200]\n",
      "loss: 0.716693  [ 2272/ 3200]\n",
      "loss: 0.595118  [ 2288/ 3200]\n",
      "loss: 1.182893  [ 2304/ 3200]\n",
      "loss: 0.707882  [ 2320/ 3200]\n",
      "loss: 0.267836  [ 2336/ 3200]\n",
      "loss: 0.361057  [ 2352/ 3200]\n",
      "loss: 0.464194  [ 2368/ 3200]\n",
      "loss: 0.270107  [ 2384/ 3200]\n",
      "loss: 0.489539  [ 2400/ 3200]\n",
      "loss: 0.430246  [ 2416/ 3200]\n",
      "loss: 0.772169  [ 2432/ 3200]\n",
      "loss: 0.225452  [ 2448/ 3200]\n",
      "loss: 0.291585  [ 2464/ 3200]\n",
      "loss: 0.577606  [ 2480/ 3200]\n",
      "loss: 0.354195  [ 2496/ 3200]\n",
      "loss: 0.292748  [ 2512/ 3200]\n",
      "loss: 0.397877  [ 2528/ 3200]\n",
      "loss: 0.390884  [ 2544/ 3200]\n",
      "loss: 0.584587  [ 2560/ 3200]\n",
      "loss: 0.372503  [ 2576/ 3200]\n",
      "loss: 0.328703  [ 2592/ 3200]\n",
      "loss: 0.599030  [ 2608/ 3200]\n",
      "loss: 0.360080  [ 2624/ 3200]\n",
      "loss: 0.813983  [ 2640/ 3200]\n",
      "loss: 0.867489  [ 2656/ 3200]\n",
      "loss: 0.380778  [ 2672/ 3200]\n",
      "loss: 0.488317  [ 2688/ 3200]\n",
      "loss: 0.790860  [ 2704/ 3200]\n",
      "loss: 0.699058  [ 2720/ 3200]\n",
      "loss: 0.651859  [ 2736/ 3200]\n",
      "loss: 0.441153  [ 2752/ 3200]\n",
      "loss: 0.462766  [ 2768/ 3200]\n",
      "loss: 0.660003  [ 2784/ 3200]\n",
      "loss: 0.294662  [ 2800/ 3200]\n",
      "loss: 0.529475  [ 2816/ 3200]\n",
      "loss: 0.399681  [ 2832/ 3200]\n",
      "loss: 0.309770  [ 2848/ 3200]\n",
      "loss: 0.561328  [ 2864/ 3200]\n",
      "loss: 0.532760  [ 2880/ 3200]\n",
      "loss: 0.510170  [ 2896/ 3200]\n",
      "loss: 0.387658  [ 2912/ 3200]\n",
      "loss: 0.373063  [ 2928/ 3200]\n",
      "loss: 0.817972  [ 2944/ 3200]\n",
      "loss: 0.535746  [ 2960/ 3200]\n",
      "loss: 0.365553  [ 2976/ 3200]\n",
      "loss: 0.252162  [ 2992/ 3200]\n",
      "loss: 0.508336  [ 3008/ 3200]\n",
      "loss: 0.834744  [ 3024/ 3200]\n",
      "loss: 0.481090  [ 3040/ 3200]\n",
      "loss: 0.766434  [ 3056/ 3200]\n",
      "loss: 0.861260  [ 3072/ 3200]\n",
      "loss: 0.516832  [ 3088/ 3200]\n",
      "loss: 0.390834  [ 3104/ 3200]\n",
      "loss: 0.384098  [ 3120/ 3200]\n",
      "loss: 0.830676  [ 3136/ 3200]\n",
      "loss: 0.748323  [ 3152/ 3200]\n",
      "loss: 0.624927  [ 3168/ 3200]\n",
      "loss: 0.692200  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035081\n",
      "f1 macro averaged score: 0.799066\n",
      "Accuracy               : 79.9%\n",
      "Confusion matrix       :\n",
      "tensor([[188,   9,   0,   3],\n",
      "        [ 23, 145,  15,  17],\n",
      "        [  0,  32, 159,   9],\n",
      "        [  6,  35,  12, 147]], device='cuda:0')\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.286863  [    0/ 3200]\n",
      "loss: 0.611429  [   16/ 3200]\n",
      "loss: 0.447981  [   32/ 3200]\n",
      "loss: 0.540977  [   48/ 3200]\n",
      "loss: 0.320242  [   64/ 3200]\n",
      "loss: 0.673653  [   80/ 3200]\n",
      "loss: 0.532837  [   96/ 3200]\n",
      "loss: 0.737810  [  112/ 3200]\n",
      "loss: 0.672016  [  128/ 3200]\n",
      "loss: 0.408498  [  144/ 3200]\n",
      "loss: 0.421690  [  160/ 3200]\n",
      "loss: 0.828947  [  176/ 3200]\n",
      "loss: 0.464386  [  192/ 3200]\n",
      "loss: 0.681198  [  208/ 3200]\n",
      "loss: 0.577419  [  224/ 3200]\n",
      "loss: 0.348211  [  240/ 3200]\n",
      "loss: 0.289496  [  256/ 3200]\n",
      "loss: 0.279509  [  272/ 3200]\n",
      "loss: 0.315610  [  288/ 3200]\n",
      "loss: 0.523021  [  304/ 3200]\n",
      "loss: 0.526084  [  320/ 3200]\n",
      "loss: 0.904136  [  336/ 3200]\n",
      "loss: 0.361408  [  352/ 3200]\n",
      "loss: 0.353239  [  368/ 3200]\n",
      "loss: 0.578568  [  384/ 3200]\n",
      "loss: 0.535727  [  400/ 3200]\n",
      "loss: 0.503775  [  416/ 3200]\n",
      "loss: 0.612764  [  432/ 3200]\n",
      "loss: 0.605507  [  448/ 3200]\n",
      "loss: 0.422704  [  464/ 3200]\n",
      "loss: 0.480207  [  480/ 3200]\n",
      "loss: 0.575822  [  496/ 3200]\n",
      "loss: 0.454833  [  512/ 3200]\n",
      "loss: 0.427318  [  528/ 3200]\n",
      "loss: 0.564285  [  544/ 3200]\n",
      "loss: 0.700737  [  560/ 3200]\n",
      "loss: 0.228513  [  576/ 3200]\n",
      "loss: 0.238550  [  592/ 3200]\n",
      "loss: 0.372524  [  608/ 3200]\n",
      "loss: 0.933608  [  624/ 3200]\n",
      "loss: 0.472215  [  640/ 3200]\n",
      "loss: 0.541714  [  656/ 3200]\n",
      "loss: 0.448769  [  672/ 3200]\n",
      "loss: 0.651380  [  688/ 3200]\n",
      "loss: 0.466552  [  704/ 3200]\n",
      "loss: 0.543872  [  720/ 3200]\n",
      "loss: 0.647490  [  736/ 3200]\n",
      "loss: 0.742726  [  752/ 3200]\n",
      "loss: 0.447209  [  768/ 3200]\n",
      "loss: 0.713103  [  784/ 3200]\n",
      "loss: 0.514266  [  800/ 3200]\n",
      "loss: 0.278147  [  816/ 3200]\n",
      "loss: 0.734081  [  832/ 3200]\n",
      "loss: 0.246787  [  848/ 3200]\n",
      "loss: 0.482657  [  864/ 3200]\n",
      "loss: 0.502975  [  880/ 3200]\n",
      "loss: 0.207608  [  896/ 3200]\n",
      "loss: 0.843520  [  912/ 3200]\n",
      "loss: 0.412790  [  928/ 3200]\n",
      "loss: 0.820818  [  944/ 3200]\n",
      "loss: 0.658066  [  960/ 3200]\n",
      "loss: 0.517087  [  976/ 3200]\n",
      "loss: 0.529898  [  992/ 3200]\n",
      "loss: 0.613323  [ 1008/ 3200]\n",
      "loss: 0.425007  [ 1024/ 3200]\n",
      "loss: 0.453852  [ 1040/ 3200]\n",
      "loss: 0.832444  [ 1056/ 3200]\n",
      "loss: 0.616006  [ 1072/ 3200]\n",
      "loss: 0.672342  [ 1088/ 3200]\n",
      "loss: 0.373025  [ 1104/ 3200]\n",
      "loss: 0.540275  [ 1120/ 3200]\n",
      "loss: 0.381086  [ 1136/ 3200]\n",
      "loss: 0.454351  [ 1152/ 3200]\n",
      "loss: 0.539797  [ 1168/ 3200]\n",
      "loss: 0.264498  [ 1184/ 3200]\n",
      "loss: 0.281282  [ 1200/ 3200]\n",
      "loss: 0.577948  [ 1216/ 3200]\n",
      "loss: 0.377205  [ 1232/ 3200]\n",
      "loss: 0.510923  [ 1248/ 3200]\n",
      "loss: 0.671711  [ 1264/ 3200]\n",
      "loss: 0.308058  [ 1280/ 3200]\n",
      "loss: 0.568309  [ 1296/ 3200]\n",
      "loss: 0.339524  [ 1312/ 3200]\n",
      "loss: 0.310945  [ 1328/ 3200]\n",
      "loss: 0.470473  [ 1344/ 3200]\n",
      "loss: 0.568063  [ 1360/ 3200]\n",
      "loss: 0.381012  [ 1376/ 3200]\n",
      "loss: 0.547979  [ 1392/ 3200]\n",
      "loss: 0.666759  [ 1408/ 3200]\n",
      "loss: 0.690506  [ 1424/ 3200]\n",
      "loss: 0.460792  [ 1440/ 3200]\n",
      "loss: 0.362655  [ 1456/ 3200]\n",
      "loss: 0.314611  [ 1472/ 3200]\n",
      "loss: 0.612543  [ 1488/ 3200]\n",
      "loss: 0.660766  [ 1504/ 3200]\n",
      "loss: 0.348303  [ 1520/ 3200]\n",
      "loss: 0.446567  [ 1536/ 3200]\n",
      "loss: 0.326761  [ 1552/ 3200]\n",
      "loss: 0.512074  [ 1568/ 3200]\n",
      "loss: 0.453694  [ 1584/ 3200]\n",
      "loss: 0.672214  [ 1600/ 3200]\n",
      "loss: 0.470925  [ 1616/ 3200]\n",
      "loss: 0.827768  [ 1632/ 3200]\n",
      "loss: 0.545537  [ 1648/ 3200]\n",
      "loss: 0.457602  [ 1664/ 3200]\n",
      "loss: 0.594164  [ 1680/ 3200]\n",
      "loss: 0.754778  [ 1696/ 3200]\n",
      "loss: 0.712702  [ 1712/ 3200]\n",
      "loss: 0.329665  [ 1728/ 3200]\n",
      "loss: 0.563442  [ 1744/ 3200]\n",
      "loss: 0.921395  [ 1760/ 3200]\n",
      "loss: 0.658607  [ 1776/ 3200]\n",
      "loss: 0.612691  [ 1792/ 3200]\n",
      "loss: 0.357783  [ 1808/ 3200]\n",
      "loss: 0.560710  [ 1824/ 3200]\n",
      "loss: 0.412297  [ 1840/ 3200]\n",
      "loss: 0.461947  [ 1856/ 3200]\n",
      "loss: 0.748360  [ 1872/ 3200]\n",
      "loss: 0.428447  [ 1888/ 3200]\n",
      "loss: 0.335393  [ 1904/ 3200]\n",
      "loss: 0.807475  [ 1920/ 3200]\n",
      "loss: 0.785644  [ 1936/ 3200]\n",
      "loss: 0.398677  [ 1952/ 3200]\n",
      "loss: 0.557953  [ 1968/ 3200]\n",
      "loss: 1.027843  [ 1984/ 3200]\n",
      "loss: 0.626041  [ 2000/ 3200]\n",
      "loss: 0.658289  [ 2016/ 3200]\n",
      "loss: 0.313064  [ 2032/ 3200]\n",
      "loss: 0.374367  [ 2048/ 3200]\n",
      "loss: 0.658325  [ 2064/ 3200]\n",
      "loss: 0.583868  [ 2080/ 3200]\n",
      "loss: 0.518616  [ 2096/ 3200]\n",
      "loss: 0.417731  [ 2112/ 3200]\n",
      "loss: 0.374842  [ 2128/ 3200]\n",
      "loss: 0.455536  [ 2144/ 3200]\n",
      "loss: 0.851700  [ 2160/ 3200]\n",
      "loss: 0.617454  [ 2176/ 3200]\n",
      "loss: 0.742076  [ 2192/ 3200]\n",
      "loss: 0.490983  [ 2208/ 3200]\n",
      "loss: 0.930912  [ 2224/ 3200]\n",
      "loss: 0.317104  [ 2240/ 3200]\n",
      "loss: 0.720752  [ 2256/ 3200]\n",
      "loss: 0.634091  [ 2272/ 3200]\n",
      "loss: 0.625004  [ 2288/ 3200]\n",
      "loss: 0.490961  [ 2304/ 3200]\n",
      "loss: 0.696613  [ 2320/ 3200]\n",
      "loss: 0.476221  [ 2336/ 3200]\n",
      "loss: 0.466236  [ 2352/ 3200]\n",
      "loss: 0.561712  [ 2368/ 3200]\n",
      "loss: 0.351509  [ 2384/ 3200]\n",
      "loss: 0.608146  [ 2400/ 3200]\n",
      "loss: 0.367366  [ 2416/ 3200]\n",
      "loss: 0.683964  [ 2432/ 3200]\n",
      "loss: 0.388028  [ 2448/ 3200]\n",
      "loss: 0.609391  [ 2464/ 3200]\n",
      "loss: 0.174246  [ 2480/ 3200]\n",
      "loss: 0.348343  [ 2496/ 3200]\n",
      "loss: 0.313300  [ 2512/ 3200]\n",
      "loss: 0.520344  [ 2528/ 3200]\n",
      "loss: 0.819428  [ 2544/ 3200]\n",
      "loss: 0.380733  [ 2560/ 3200]\n",
      "loss: 0.354223  [ 2576/ 3200]\n",
      "loss: 0.324593  [ 2592/ 3200]\n",
      "loss: 0.659284  [ 2608/ 3200]\n",
      "loss: 0.444095  [ 2624/ 3200]\n",
      "loss: 0.359168  [ 2640/ 3200]\n",
      "loss: 0.476431  [ 2656/ 3200]\n",
      "loss: 0.581673  [ 2672/ 3200]\n",
      "loss: 0.997721  [ 2688/ 3200]\n",
      "loss: 0.524816  [ 2704/ 3200]\n",
      "loss: 0.413116  [ 2720/ 3200]\n",
      "loss: 0.238444  [ 2736/ 3200]\n",
      "loss: 0.845002  [ 2752/ 3200]\n",
      "loss: 0.595205  [ 2768/ 3200]\n",
      "loss: 0.619141  [ 2784/ 3200]\n",
      "loss: 0.381622  [ 2800/ 3200]\n",
      "loss: 0.351334  [ 2816/ 3200]\n",
      "loss: 0.408845  [ 2832/ 3200]\n",
      "loss: 0.318016  [ 2848/ 3200]\n",
      "loss: 0.257179  [ 2864/ 3200]\n",
      "loss: 0.791345  [ 2880/ 3200]\n",
      "loss: 0.569637  [ 2896/ 3200]\n",
      "loss: 0.260255  [ 2912/ 3200]\n",
      "loss: 0.465049  [ 2928/ 3200]\n",
      "loss: 0.519163  [ 2944/ 3200]\n",
      "loss: 0.371102  [ 2960/ 3200]\n",
      "loss: 0.560671  [ 2976/ 3200]\n",
      "loss: 0.951975  [ 2992/ 3200]\n",
      "loss: 0.274821  [ 3008/ 3200]\n",
      "loss: 0.411958  [ 3024/ 3200]\n",
      "loss: 0.578454  [ 3040/ 3200]\n",
      "loss: 0.500785  [ 3056/ 3200]\n",
      "loss: 0.597777  [ 3072/ 3200]\n",
      "loss: 0.711025  [ 3088/ 3200]\n",
      "loss: 0.676537  [ 3104/ 3200]\n",
      "loss: 0.446443  [ 3120/ 3200]\n",
      "loss: 0.378662  [ 3136/ 3200]\n",
      "loss: 0.348114  [ 3152/ 3200]\n",
      "loss: 0.448817  [ 3168/ 3200]\n",
      "loss: 0.306165  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038461\n",
      "f1 macro averaged score: 0.761104\n",
      "Accuracy               : 76.5%\n",
      "Confusion matrix       :\n",
      "tensor([[163,  23,   1,  13],\n",
      "        [ 12, 109,  40,  39],\n",
      "        [  0,   7, 177,  16],\n",
      "        [  4,  16,  17, 163]], device='cuda:0')\n",
      "\n",
      "Best epoch: 29 with f1 macro averaged score: 0.7990659475326538\n",
      "Test Error:\n",
      "Avg loss               : 0.041826\n",
      "f1 macro averaged score: 0.738403\n",
      "Accuracy               : 74.3%\n",
      "Confusion matrix       :\n",
      "tensor([[264,  24,   7,   2],\n",
      "        [ 11, 148,  82,  83],\n",
      "        [  2,  17, 321,  16],\n",
      "        [  7,  52,  50, 290]], device='cuda:0')\n",
      "CPU times: user 11min 4s, sys: 14.8 s, total: 11min 18s\n",
      "Wall time: 11min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "activation_functions = {\"ELU\": ELU, \"Hardshrink\": Hardshrink, \"Hardsigmoid\": Hardsigmoid, \"Hardtanh\": Hardtanh, \"Hardswish\": Hardswish,\n",
    "                        \"LeakyReLU\": LeakyReLU, \"LogSigmoid\": LogSigmoid,\n",
    "                        \"PReLU\": PReLU, \"ReLU\": ReLU, \"ReLU6\": ReLU6, \"RReLU\": RReLU, \"SELU\": SELU, \"CELU\": CELU, \"GELU\": GELU,\n",
    "                        \"Sigmoid\": Sigmoid, \"SiLU\": SiLU, \"Mish\": Mish, \"Softplus\": Softplus, \"Softshrink\": Softshrink, \"Softsign\": Softsign,\n",
    "                        \"Tanh\": Tanh, \"Tanhshrink\": Tanhshrink}\n",
    "\n",
    "learning_rate = 0.002\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "f1_accuracy = []\n",
    "for act, activation_function in activation_functions.items():\n",
    "  print(\"  Activation function:\", act)\n",
    "  torch_seed(0)\n",
    "  cnn_model = Net(activation_function).to(device)\n",
    "  optimizer = Adagrad(params=cnn_model.parameters(), lr=learning_rate)\n",
    "  best_model, f1_per_epoch = validate_convolutional_neural_network(epochs, optimizer, train_dataloader, val_dataloader, loss_function, cnn_model, True)\n",
    "  results = test_convolutional_neural_network(test_dataloader, loss_function, best_model)\n",
    "  f1_accuracy.append((act, results[1], results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5bfSdCVuxcm",
    "outputId": "0832e1c4-3aa9-4a62-d858-92ce3b290301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act function\tf1_macro_avg\tAccuracy\n",
      "ELU\t\t0.780637\t77.761628\n",
      "CELU\t\t0.780637\t77.761628\n",
      "SELU\t\t0.775539\t77.180233\n",
      "Softplus\t0.756048\t75.872093\n",
      "ReLU6\t\t0.745794\t74.200581\n",
      "RReLU\t\t0.745444\t74.127907\n",
      "GELU\t\t0.744725\t74.055233\n",
      "Hardswish\t0.744151\t74.273256\n",
      "PReLU\t\t0.743645\t73.982558\n",
      "ReLU\t\t0.742370\t73.546512\n",
      "Hardshrink\t0.742286\t74.127907\n",
      "Softsign\t0.741381\t73.546512\n",
      "LeakyReLU\t0.740447\t73.401163\n",
      "Mish\t\t0.739895\t73.619186\n",
      "Tanh\t\t0.739474\t73.764535\n",
      "Tanhshrink\t0.738403\t74.345930\n",
      "LogSigmoid\t0.735301\t73.473837\n",
      "SiLU\t\t0.735135\t73.110465\n",
      "Sigmoid\t\t0.542419\t59.520349\n",
      "Hardsigmoid\t0.531221\t55.813953\n",
      "Softshrink\t0.095294\t23.546512\n",
      "Hardtanh\t0.088763\t21.584302\n"
     ]
    }
   ],
   "source": [
    "f1_accuracy = sorted(f1_accuracy, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Act function\\tf1_macro_avg\\tAccuracy\")\n",
    "for (act_function, f1_macro_avg, accuracy) in f1_accuracy:\n",
    "  if len(act_function) > 7: # for better visualization\n",
    "    print(f\"{act_function}\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")\n",
    "    continue\n",
    "  print(f\"{act_function}\\t\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CYdi1RZ-uyH"
   },
   "source": [
    "The table above shows that the optimizer plays a big role in the development of the model, since it can really change the results during testing.\n",
    "\n",
    "The best activation function for our Convolutional Neural Network was ELU with $77\\%$ accuracy and the worst was Hardtanh with $21\\%$ accuracy.\n",
    "<br></br>\n",
    "\n",
    "We conclude that ELU increased accuracy from $74\\%$ to $77\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_eM3DP0_SRq"
   },
   "source": [
    "### Step 3 - Learning Rate Scheduler\n",
    "\n",
    "Redefine the training and validation procedure, so that the learning rate scheduler can be given as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SpmBPr-_rGM"
   },
   "outputs": [],
   "source": [
    "def train_convolutional_neural_network(epochs, optimizer, dataloader, loss_function, model, reproducibility=False, scheduler=None):\n",
    "  size = len(dataloader.dataset)\n",
    "\n",
    "  for epoch in range(0, epochs):\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"-----------------------------\")\n",
    "    if reproducibility: # added in Question 3 - Step 1\n",
    "      dataloader.set_epoch(dataloader, epoch)\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(torch.unsqueeze(x, 1).type(torch.float))\n",
    "      loss = loss_function(prediction, y)\n",
    "\n",
    "      # backpropagation\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss, current = loss.item(), batch * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # adjust learning rate\n",
    "    scheduler.step()\n",
    "    print()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADVfqSPhC8cz"
   },
   "outputs": [],
   "source": [
    "def validate_convolutional_neural_network(epochs, optimizer, train_dataloader, val_dataloader, loss_function, model, reproducibility, scheduler):\n",
    "  size = len(train_dataloader.dataset)\n",
    "\n",
    "  best = (0, 0, 0) # best model, best epoch, best f1 score\n",
    "  f1_list = []\n",
    "\n",
    "  for epoch in range(0, epochs):\n",
    "    # train model with train data\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"-----------------------------\")\n",
    "    if reproducibility: # added\n",
    "      train_dataloader.set_epoch(train_dataloader, epoch)\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(torch.unsqueeze(x, 1))\n",
    "      loss = loss_function(prediction, y)\n",
    "\n",
    "      # backpropagation\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss, current = loss.item(), batch * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # test model on validation set\n",
    "    results = test_convolutional_neural_network(val_dataloader, loss_function, model)\n",
    "    f1_macro_avg = results[1]\n",
    "    f1_list.append(f1_macro_avg)\n",
    "    if f1_macro_avg > best[2]:\n",
    "      best = (model, epoch, f1_macro_avg)\n",
    "\n",
    "    # adjust learning rate\n",
    "    scheduler.step()\n",
    "    print()\n",
    "\n",
    "  print(f\"Best epoch: {(best[1] + 1)} with f1 macro averaged score: {best[2]}\")\n",
    "  return best[0], f1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmXBKyLcA-VZ"
   },
   "source": [
    "We validate our Convolutional Neural Network for $30$ epochs and test it.\n",
    "\n",
    "Our model uses the Adagrad optimizer and the ELU activation function, as stated in the previous steps.\n",
    "\n",
    "The schedulers used are the following: LambdaLR, MultiplicativeLR, StepLR, MultiStepLR, ConstantLR, LinearLR, ExponentialLR, PolynomialLR, CyclicLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVDZxTJSBGqK",
    "outputId": "db7a1946-5edc-4364-da70-65f223f317a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 0.599710  [ 1952/ 3200]\n",
      "loss: 0.526369  [ 1968/ 3200]\n",
      "loss: 0.464395  [ 1984/ 3200]\n",
      "loss: 0.362512  [ 2000/ 3200]\n",
      "loss: 0.613904  [ 2016/ 3200]\n",
      "loss: 0.617890  [ 2032/ 3200]\n",
      "loss: 0.458244  [ 2048/ 3200]\n",
      "loss: 0.277873  [ 2064/ 3200]\n",
      "loss: 0.300713  [ 2080/ 3200]\n",
      "loss: 0.579099  [ 2096/ 3200]\n",
      "loss: 0.473885  [ 2112/ 3200]\n",
      "loss: 0.879033  [ 2128/ 3200]\n",
      "loss: 0.723749  [ 2144/ 3200]\n",
      "loss: 0.414537  [ 2160/ 3200]\n",
      "loss: 0.563259  [ 2176/ 3200]\n",
      "loss: 0.684590  [ 2192/ 3200]\n",
      "loss: 0.789107  [ 2208/ 3200]\n",
      "loss: 0.329238  [ 2224/ 3200]\n",
      "loss: 0.605539  [ 2240/ 3200]\n",
      "loss: 0.820574  [ 2256/ 3200]\n",
      "loss: 0.974448  [ 2272/ 3200]\n",
      "loss: 0.651306  [ 2288/ 3200]\n",
      "loss: 0.435579  [ 2304/ 3200]\n",
      "loss: 0.704582  [ 2320/ 3200]\n",
      "loss: 0.826604  [ 2336/ 3200]\n",
      "loss: 0.918709  [ 2352/ 3200]\n",
      "loss: 0.537012  [ 2368/ 3200]\n",
      "loss: 0.541867  [ 2384/ 3200]\n",
      "loss: 0.925720  [ 2400/ 3200]\n",
      "loss: 0.743056  [ 2416/ 3200]\n",
      "loss: 0.338515  [ 2432/ 3200]\n",
      "loss: 0.491163  [ 2448/ 3200]\n",
      "loss: 0.498024  [ 2464/ 3200]\n",
      "loss: 0.583298  [ 2480/ 3200]\n",
      "loss: 0.621454  [ 2496/ 3200]\n",
      "loss: 0.418880  [ 2512/ 3200]\n",
      "loss: 0.683488  [ 2528/ 3200]\n",
      "loss: 0.616561  [ 2544/ 3200]\n",
      "loss: 0.556655  [ 2560/ 3200]\n",
      "loss: 0.980294  [ 2576/ 3200]\n",
      "loss: 0.765695  [ 2592/ 3200]\n",
      "loss: 0.305620  [ 2608/ 3200]\n",
      "loss: 0.714278  [ 2624/ 3200]\n",
      "loss: 0.211783  [ 2640/ 3200]\n",
      "loss: 0.839142  [ 2656/ 3200]\n",
      "loss: 0.545293  [ 2672/ 3200]\n",
      "loss: 0.365037  [ 2688/ 3200]\n",
      "loss: 0.336926  [ 2704/ 3200]\n",
      "loss: 0.409800  [ 2720/ 3200]\n",
      "loss: 0.330485  [ 2736/ 3200]\n",
      "loss: 0.762522  [ 2752/ 3200]\n",
      "loss: 0.701457  [ 2768/ 3200]\n",
      "loss: 0.250560  [ 2784/ 3200]\n",
      "loss: 0.988366  [ 2800/ 3200]\n",
      "loss: 0.512797  [ 2816/ 3200]\n",
      "loss: 0.337953  [ 2832/ 3200]\n",
      "loss: 0.458252  [ 2848/ 3200]\n",
      "loss: 0.902026  [ 2864/ 3200]\n",
      "loss: 0.882548  [ 2880/ 3200]\n",
      "loss: 0.510043  [ 2896/ 3200]\n",
      "loss: 0.539488  [ 2912/ 3200]\n",
      "loss: 0.592253  [ 2928/ 3200]\n",
      "loss: 0.465932  [ 2944/ 3200]\n",
      "loss: 0.482460  [ 2960/ 3200]\n",
      "loss: 0.882346  [ 2976/ 3200]\n",
      "loss: 0.537843  [ 2992/ 3200]\n",
      "loss: 0.701889  [ 3008/ 3200]\n",
      "loss: 0.887499  [ 3024/ 3200]\n",
      "loss: 0.824036  [ 3040/ 3200]\n",
      "loss: 0.924744  [ 3056/ 3200]\n",
      "loss: 0.662688  [ 3072/ 3200]\n",
      "loss: 0.325842  [ 3088/ 3200]\n",
      "loss: 0.229566  [ 3104/ 3200]\n",
      "loss: 0.684009  [ 3120/ 3200]\n",
      "loss: 0.422264  [ 3136/ 3200]\n",
      "loss: 0.504420  [ 3152/ 3200]\n",
      "loss: 0.742954  [ 3168/ 3200]\n",
      "loss: 0.530601  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038714\n",
      "f1 macro averaged score: 0.767281\n",
      "Accuracy               : 76.4%\n",
      "Confusion matrix       :\n",
      "tensor([[155,  35,   0,  10],\n",
      "        [ 15, 139,  36,  10],\n",
      "        [  0,  22, 172,   6],\n",
      "        [  1,  38,  16, 145]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0315e-02.\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 0.790639  [    0/ 3200]\n",
      "loss: 0.393708  [   16/ 3200]\n",
      "loss: 0.537700  [   32/ 3200]\n",
      "loss: 0.921889  [   48/ 3200]\n",
      "loss: 0.493841  [   64/ 3200]\n",
      "loss: 1.136616  [   80/ 3200]\n",
      "loss: 0.638135  [   96/ 3200]\n",
      "loss: 0.507960  [  112/ 3200]\n",
      "loss: 0.467412  [  128/ 3200]\n",
      "loss: 0.792293  [  144/ 3200]\n",
      "loss: 0.488366  [  160/ 3200]\n",
      "loss: 0.653921  [  176/ 3200]\n",
      "loss: 0.872212  [  192/ 3200]\n",
      "loss: 0.431088  [  208/ 3200]\n",
      "loss: 0.584316  [  224/ 3200]\n",
      "loss: 0.586340  [  240/ 3200]\n",
      "loss: 0.749519  [  256/ 3200]\n",
      "loss: 0.379027  [  272/ 3200]\n",
      "loss: 0.535613  [  288/ 3200]\n",
      "loss: 0.601793  [  304/ 3200]\n",
      "loss: 0.996544  [  320/ 3200]\n",
      "loss: 0.449605  [  336/ 3200]\n",
      "loss: 0.876059  [  352/ 3200]\n",
      "loss: 0.605893  [  368/ 3200]\n",
      "loss: 0.382166  [  384/ 3200]\n",
      "loss: 0.912839  [  400/ 3200]\n",
      "loss: 0.536812  [  416/ 3200]\n",
      "loss: 0.822653  [  432/ 3200]\n",
      "loss: 0.723744  [  448/ 3200]\n",
      "loss: 0.961148  [  464/ 3200]\n",
      "loss: 0.419218  [  480/ 3200]\n",
      "loss: 0.346065  [  496/ 3200]\n",
      "loss: 0.972441  [  512/ 3200]\n",
      "loss: 0.389515  [  528/ 3200]\n",
      "loss: 0.745649  [  544/ 3200]\n",
      "loss: 0.610967  [  560/ 3200]\n",
      "loss: 0.600889  [  576/ 3200]\n",
      "loss: 0.544896  [  592/ 3200]\n",
      "loss: 0.286259  [  608/ 3200]\n",
      "loss: 0.420261  [  624/ 3200]\n",
      "loss: 0.319567  [  640/ 3200]\n",
      "loss: 0.651106  [  656/ 3200]\n",
      "loss: 0.476832  [  672/ 3200]\n",
      "loss: 0.513450  [  688/ 3200]\n",
      "loss: 0.446390  [  704/ 3200]\n",
      "loss: 0.615993  [  720/ 3200]\n",
      "loss: 0.444559  [  736/ 3200]\n",
      "loss: 0.484019  [  752/ 3200]\n",
      "loss: 0.651089  [  768/ 3200]\n",
      "loss: 0.702089  [  784/ 3200]\n",
      "loss: 0.132172  [  800/ 3200]\n",
      "loss: 0.697329  [  816/ 3200]\n",
      "loss: 0.513089  [  832/ 3200]\n",
      "loss: 0.438024  [  848/ 3200]\n",
      "loss: 0.968766  [  864/ 3200]\n",
      "loss: 0.876376  [  880/ 3200]\n",
      "loss: 0.794152  [  896/ 3200]\n",
      "loss: 0.397456  [  912/ 3200]\n",
      "loss: 0.582444  [  928/ 3200]\n",
      "loss: 0.460680  [  944/ 3200]\n",
      "loss: 0.674298  [  960/ 3200]\n",
      "loss: 0.381988  [  976/ 3200]\n",
      "loss: 0.452248  [  992/ 3200]\n",
      "loss: 0.569358  [ 1008/ 3200]\n",
      "loss: 0.872535  [ 1024/ 3200]\n",
      "loss: 0.608643  [ 1040/ 3200]\n",
      "loss: 0.512770  [ 1056/ 3200]\n",
      "loss: 0.591802  [ 1072/ 3200]\n",
      "loss: 0.635506  [ 1088/ 3200]\n",
      "loss: 0.745435  [ 1104/ 3200]\n",
      "loss: 0.182866  [ 1120/ 3200]\n",
      "loss: 0.570977  [ 1136/ 3200]\n",
      "loss: 0.538708  [ 1152/ 3200]\n",
      "loss: 1.153812  [ 1168/ 3200]\n",
      "loss: 0.673460  [ 1184/ 3200]\n",
      "loss: 0.667064  [ 1200/ 3200]\n",
      "loss: 0.587801  [ 1216/ 3200]\n",
      "loss: 0.339269  [ 1232/ 3200]\n",
      "loss: 0.413884  [ 1248/ 3200]\n",
      "loss: 0.503074  [ 1264/ 3200]\n",
      "loss: 0.559721  [ 1280/ 3200]\n",
      "loss: 0.571372  [ 1296/ 3200]\n",
      "loss: 0.657714  [ 1312/ 3200]\n",
      "loss: 0.392507  [ 1328/ 3200]\n",
      "loss: 0.425767  [ 1344/ 3200]\n",
      "loss: 0.419793  [ 1360/ 3200]\n",
      "loss: 0.487894  [ 1376/ 3200]\n",
      "loss: 0.195570  [ 1392/ 3200]\n",
      "loss: 0.883372  [ 1408/ 3200]\n",
      "loss: 0.594029  [ 1424/ 3200]\n",
      "loss: 0.682744  [ 1440/ 3200]\n",
      "loss: 0.374963  [ 1456/ 3200]\n",
      "loss: 0.191577  [ 1472/ 3200]\n",
      "loss: 0.410607  [ 1488/ 3200]\n",
      "loss: 0.491331  [ 1504/ 3200]\n",
      "loss: 1.012652  [ 1520/ 3200]\n",
      "loss: 0.674677  [ 1536/ 3200]\n",
      "loss: 0.684664  [ 1552/ 3200]\n",
      "loss: 0.350367  [ 1568/ 3200]\n",
      "loss: 0.707950  [ 1584/ 3200]\n",
      "loss: 0.582095  [ 1600/ 3200]\n",
      "loss: 0.431165  [ 1616/ 3200]\n",
      "loss: 0.445579  [ 1632/ 3200]\n",
      "loss: 0.590250  [ 1648/ 3200]\n",
      "loss: 0.932542  [ 1664/ 3200]\n",
      "loss: 0.920448  [ 1680/ 3200]\n",
      "loss: 0.756649  [ 1696/ 3200]\n",
      "loss: 0.583876  [ 1712/ 3200]\n",
      "loss: 0.428099  [ 1728/ 3200]\n",
      "loss: 0.424452  [ 1744/ 3200]\n",
      "loss: 0.338931  [ 1760/ 3200]\n",
      "loss: 0.834882  [ 1776/ 3200]\n",
      "loss: 0.483665  [ 1792/ 3200]\n",
      "loss: 0.656720  [ 1808/ 3200]\n",
      "loss: 0.730052  [ 1824/ 3200]\n",
      "loss: 0.515372  [ 1840/ 3200]\n",
      "loss: 0.481307  [ 1856/ 3200]\n",
      "loss: 0.349492  [ 1872/ 3200]\n",
      "loss: 0.722435  [ 1888/ 3200]\n",
      "loss: 0.330734  [ 1904/ 3200]\n",
      "loss: 0.405241  [ 1920/ 3200]\n",
      "loss: 0.699443  [ 1936/ 3200]\n",
      "loss: 0.281082  [ 1952/ 3200]\n",
      "loss: 0.228956  [ 1968/ 3200]\n",
      "loss: 0.581902  [ 1984/ 3200]\n",
      "loss: 0.404303  [ 2000/ 3200]\n",
      "loss: 0.716102  [ 2016/ 3200]\n",
      "loss: 0.622650  [ 2032/ 3200]\n",
      "loss: 0.559187  [ 2048/ 3200]\n",
      "loss: 0.369961  [ 2064/ 3200]\n",
      "loss: 0.454883  [ 2080/ 3200]\n",
      "loss: 0.896200  [ 2096/ 3200]\n",
      "loss: 0.731557  [ 2112/ 3200]\n",
      "loss: 0.353006  [ 2128/ 3200]\n",
      "loss: 0.553505  [ 2144/ 3200]\n",
      "loss: 0.728980  [ 2160/ 3200]\n",
      "loss: 0.530853  [ 2176/ 3200]\n",
      "loss: 0.567271  [ 2192/ 3200]\n",
      "loss: 0.379046  [ 2208/ 3200]\n",
      "loss: 0.399337  [ 2224/ 3200]\n",
      "loss: 0.963515  [ 2240/ 3200]\n",
      "loss: 0.493449  [ 2256/ 3200]\n",
      "loss: 0.745010  [ 2272/ 3200]\n",
      "loss: 0.921754  [ 2288/ 3200]\n",
      "loss: 0.655020  [ 2304/ 3200]\n",
      "loss: 0.871515  [ 2320/ 3200]\n",
      "loss: 0.728611  [ 2336/ 3200]\n",
      "loss: 0.546308  [ 2352/ 3200]\n",
      "loss: 0.596744  [ 2368/ 3200]\n",
      "loss: 0.967711  [ 2384/ 3200]\n",
      "loss: 0.825727  [ 2400/ 3200]\n",
      "loss: 0.374949  [ 2416/ 3200]\n",
      "loss: 0.868916  [ 2432/ 3200]\n",
      "loss: 0.764460  [ 2448/ 3200]\n",
      "loss: 0.858019  [ 2464/ 3200]\n",
      "loss: 0.265728  [ 2480/ 3200]\n",
      "loss: 0.369033  [ 2496/ 3200]\n",
      "loss: 0.736233  [ 2512/ 3200]\n",
      "loss: 0.231533  [ 2528/ 3200]\n",
      "loss: 0.353481  [ 2544/ 3200]\n",
      "loss: 0.527334  [ 2560/ 3200]\n",
      "loss: 0.748213  [ 2576/ 3200]\n",
      "loss: 0.310479  [ 2592/ 3200]\n",
      "loss: 0.758978  [ 2608/ 3200]\n",
      "loss: 0.526883  [ 2624/ 3200]\n",
      "loss: 0.466861  [ 2640/ 3200]\n",
      "loss: 0.511078  [ 2656/ 3200]\n",
      "loss: 0.463656  [ 2672/ 3200]\n",
      "loss: 0.957760  [ 2688/ 3200]\n",
      "loss: 0.859484  [ 2704/ 3200]\n",
      "loss: 0.953371  [ 2720/ 3200]\n",
      "loss: 0.479096  [ 2736/ 3200]\n",
      "loss: 0.707749  [ 2752/ 3200]\n",
      "loss: 0.323907  [ 2768/ 3200]\n",
      "loss: 0.389331  [ 2784/ 3200]\n",
      "loss: 0.497364  [ 2800/ 3200]\n",
      "loss: 0.626921  [ 2816/ 3200]\n",
      "loss: 0.750426  [ 2832/ 3200]\n",
      "loss: 0.508887  [ 2848/ 3200]\n",
      "loss: 0.559269  [ 2864/ 3200]\n",
      "loss: 0.624366  [ 2880/ 3200]\n",
      "loss: 0.474089  [ 2896/ 3200]\n",
      "loss: 0.651835  [ 2912/ 3200]\n",
      "loss: 0.861052  [ 2928/ 3200]\n",
      "loss: 0.714263  [ 2944/ 3200]\n",
      "loss: 0.677384  [ 2960/ 3200]\n",
      "loss: 0.359568  [ 2976/ 3200]\n",
      "loss: 0.589045  [ 2992/ 3200]\n",
      "loss: 0.349310  [ 3008/ 3200]\n",
      "loss: 0.208461  [ 3024/ 3200]\n",
      "loss: 0.707003  [ 3040/ 3200]\n",
      "loss: 0.384406  [ 3056/ 3200]\n",
      "loss: 0.257737  [ 3072/ 3200]\n",
      "loss: 0.256198  [ 3088/ 3200]\n",
      "loss: 0.400340  [ 3104/ 3200]\n",
      "loss: 0.727599  [ 3120/ 3200]\n",
      "loss: 0.463964  [ 3136/ 3200]\n",
      "loss: 0.417926  [ 3152/ 3200]\n",
      "loss: 0.273373  [ 3168/ 3200]\n",
      "loss: 0.564042  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038450\n",
      "f1 macro averaged score: 0.739475\n",
      "Accuracy               : 74.9%\n",
      "Confusion matrix       :\n",
      "tensor([[173,  11,   0,  16],\n",
      "        [ 31,  92,  36,  41],\n",
      "        [  1,  17, 168,  14],\n",
      "        [  3,  18,  13, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0360e-02.\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 0.451856  [    0/ 3200]\n",
      "loss: 0.370793  [   16/ 3200]\n",
      "loss: 0.593492  [   32/ 3200]\n",
      "loss: 0.459740  [   48/ 3200]\n",
      "loss: 0.492983  [   64/ 3200]\n",
      "loss: 0.768156  [   80/ 3200]\n",
      "loss: 0.676815  [   96/ 3200]\n",
      "loss: 0.316583  [  112/ 3200]\n",
      "loss: 0.443993  [  128/ 3200]\n",
      "loss: 0.659867  [  144/ 3200]\n",
      "loss: 0.590507  [  160/ 3200]\n",
      "loss: 0.570834  [  176/ 3200]\n",
      "loss: 0.687434  [  192/ 3200]\n",
      "loss: 0.282991  [  208/ 3200]\n",
      "loss: 0.663561  [  224/ 3200]\n",
      "loss: 0.523242  [  240/ 3200]\n",
      "loss: 0.474283  [  256/ 3200]\n",
      "loss: 0.536634  [  272/ 3200]\n",
      "loss: 0.726323  [  288/ 3200]\n",
      "loss: 0.775367  [  304/ 3200]\n",
      "loss: 0.652234  [  320/ 3200]\n",
      "loss: 0.696494  [  336/ 3200]\n",
      "loss: 0.576447  [  352/ 3200]\n",
      "loss: 0.569539  [  368/ 3200]\n",
      "loss: 0.299197  [  384/ 3200]\n",
      "loss: 0.421524  [  400/ 3200]\n",
      "loss: 0.668679  [  416/ 3200]\n",
      "loss: 0.522806  [  432/ 3200]\n",
      "loss: 0.450835  [  448/ 3200]\n",
      "loss: 0.497267  [  464/ 3200]\n",
      "loss: 0.448059  [  480/ 3200]\n",
      "loss: 0.301926  [  496/ 3200]\n",
      "loss: 0.389575  [  512/ 3200]\n",
      "loss: 0.746668  [  528/ 3200]\n",
      "loss: 0.845347  [  544/ 3200]\n",
      "loss: 0.989485  [  560/ 3200]\n",
      "loss: 0.565516  [  576/ 3200]\n",
      "loss: 0.477717  [  592/ 3200]\n",
      "loss: 0.520478  [  608/ 3200]\n",
      "loss: 0.777143  [  624/ 3200]\n",
      "loss: 0.772644  [  640/ 3200]\n",
      "loss: 0.413263  [  656/ 3200]\n",
      "loss: 0.581749  [  672/ 3200]\n",
      "loss: 0.379250  [  688/ 3200]\n",
      "loss: 0.557639  [  704/ 3200]\n",
      "loss: 0.782900  [  720/ 3200]\n",
      "loss: 0.459534  [  736/ 3200]\n",
      "loss: 0.642808  [  752/ 3200]\n",
      "loss: 0.511306  [  768/ 3200]\n",
      "loss: 0.367126  [  784/ 3200]\n",
      "loss: 0.592499  [  800/ 3200]\n",
      "loss: 0.439047  [  816/ 3200]\n",
      "loss: 0.479885  [  832/ 3200]\n",
      "loss: 0.470270  [  848/ 3200]\n",
      "loss: 0.612443  [  864/ 3200]\n",
      "loss: 0.369690  [  880/ 3200]\n",
      "loss: 0.550402  [  896/ 3200]\n",
      "loss: 0.612330  [  912/ 3200]\n",
      "loss: 0.252075  [  928/ 3200]\n",
      "loss: 0.643874  [  944/ 3200]\n",
      "loss: 0.538125  [  960/ 3200]\n",
      "loss: 0.526547  [  976/ 3200]\n",
      "loss: 0.955639  [  992/ 3200]\n",
      "loss: 0.377689  [ 1008/ 3200]\n",
      "loss: 1.000988  [ 1024/ 3200]\n",
      "loss: 0.803285  [ 1040/ 3200]\n",
      "loss: 0.461499  [ 1056/ 3200]\n",
      "loss: 0.744138  [ 1072/ 3200]\n",
      "loss: 1.060737  [ 1088/ 3200]\n",
      "loss: 0.427613  [ 1104/ 3200]\n",
      "loss: 0.420932  [ 1120/ 3200]\n",
      "loss: 0.694309  [ 1136/ 3200]\n",
      "loss: 0.435781  [ 1152/ 3200]\n",
      "loss: 0.550281  [ 1168/ 3200]\n",
      "loss: 0.592649  [ 1184/ 3200]\n",
      "loss: 0.386986  [ 1200/ 3200]\n",
      "loss: 0.712263  [ 1216/ 3200]\n",
      "loss: 0.711361  [ 1232/ 3200]\n",
      "loss: 0.344526  [ 1248/ 3200]\n",
      "loss: 0.588871  [ 1264/ 3200]\n",
      "loss: 0.535420  [ 1280/ 3200]\n",
      "loss: 0.908947  [ 1296/ 3200]\n",
      "loss: 0.437466  [ 1312/ 3200]\n",
      "loss: 0.501938  [ 1328/ 3200]\n",
      "loss: 0.578730  [ 1344/ 3200]\n",
      "loss: 0.555852  [ 1360/ 3200]\n",
      "loss: 0.352254  [ 1376/ 3200]\n",
      "loss: 0.430195  [ 1392/ 3200]\n",
      "loss: 0.786205  [ 1408/ 3200]\n",
      "loss: 0.235318  [ 1424/ 3200]\n",
      "loss: 0.874689  [ 1440/ 3200]\n",
      "loss: 0.664465  [ 1456/ 3200]\n",
      "loss: 0.461575  [ 1472/ 3200]\n",
      "loss: 1.014476  [ 1488/ 3200]\n",
      "loss: 0.497550  [ 1504/ 3200]\n",
      "loss: 0.221320  [ 1520/ 3200]\n",
      "loss: 0.320951  [ 1536/ 3200]\n",
      "loss: 0.627859  [ 1552/ 3200]\n",
      "loss: 0.404322  [ 1568/ 3200]\n",
      "loss: 0.268361  [ 1584/ 3200]\n",
      "loss: 0.536940  [ 1600/ 3200]\n",
      "loss: 0.634970  [ 1616/ 3200]\n",
      "loss: 0.412703  [ 1632/ 3200]\n",
      "loss: 0.491340  [ 1648/ 3200]\n",
      "loss: 0.694730  [ 1664/ 3200]\n",
      "loss: 0.434025  [ 1680/ 3200]\n",
      "loss: 0.333169  [ 1696/ 3200]\n",
      "loss: 0.411421  [ 1712/ 3200]\n",
      "loss: 0.291526  [ 1728/ 3200]\n",
      "loss: 0.743894  [ 1744/ 3200]\n",
      "loss: 0.422906  [ 1760/ 3200]\n",
      "loss: 0.421261  [ 1776/ 3200]\n",
      "loss: 1.005871  [ 1792/ 3200]\n",
      "loss: 0.954839  [ 1808/ 3200]\n",
      "loss: 0.449363  [ 1824/ 3200]\n",
      "loss: 0.490267  [ 1840/ 3200]\n",
      "loss: 0.593318  [ 1856/ 3200]\n",
      "loss: 0.556823  [ 1872/ 3200]\n",
      "loss: 0.415113  [ 1888/ 3200]\n",
      "loss: 0.437050  [ 1904/ 3200]\n",
      "loss: 0.379071  [ 1920/ 3200]\n",
      "loss: 0.674872  [ 1936/ 3200]\n",
      "loss: 0.968335  [ 1952/ 3200]\n",
      "loss: 0.423653  [ 1968/ 3200]\n",
      "loss: 0.380869  [ 1984/ 3200]\n",
      "loss: 0.473205  [ 2000/ 3200]\n",
      "loss: 0.424297  [ 2016/ 3200]\n",
      "loss: 0.369056  [ 2032/ 3200]\n",
      "loss: 0.693884  [ 2048/ 3200]\n",
      "loss: 0.317190  [ 2064/ 3200]\n",
      "loss: 0.305016  [ 2080/ 3200]\n",
      "loss: 0.276170  [ 2096/ 3200]\n",
      "loss: 0.548993  [ 2112/ 3200]\n",
      "loss: 0.386048  [ 2128/ 3200]\n",
      "loss: 0.426352  [ 2144/ 3200]\n",
      "loss: 1.008804  [ 2160/ 3200]\n",
      "loss: 0.378257  [ 2176/ 3200]\n",
      "loss: 0.219708  [ 2192/ 3200]\n",
      "loss: 0.466593  [ 2208/ 3200]\n",
      "loss: 0.833008  [ 2224/ 3200]\n",
      "loss: 0.677169  [ 2240/ 3200]\n",
      "loss: 0.499601  [ 2256/ 3200]\n",
      "loss: 0.480902  [ 2272/ 3200]\n",
      "loss: 0.743710  [ 2288/ 3200]\n",
      "loss: 0.657738  [ 2304/ 3200]\n",
      "loss: 1.216887  [ 2320/ 3200]\n",
      "loss: 0.677142  [ 2336/ 3200]\n",
      "loss: 0.526907  [ 2352/ 3200]\n",
      "loss: 0.644873  [ 2368/ 3200]\n",
      "loss: 0.520302  [ 2384/ 3200]\n",
      "loss: 0.520643  [ 2400/ 3200]\n",
      "loss: 0.648844  [ 2416/ 3200]\n",
      "loss: 0.350051  [ 2432/ 3200]\n",
      "loss: 0.467901  [ 2448/ 3200]\n",
      "loss: 0.415891  [ 2464/ 3200]\n",
      "loss: 0.730497  [ 2480/ 3200]\n",
      "loss: 0.350254  [ 2496/ 3200]\n",
      "loss: 0.731507  [ 2512/ 3200]\n",
      "loss: 0.471776  [ 2528/ 3200]\n",
      "loss: 0.569313  [ 2544/ 3200]\n",
      "loss: 0.675889  [ 2560/ 3200]\n",
      "loss: 0.803173  [ 2576/ 3200]\n",
      "loss: 0.491313  [ 2592/ 3200]\n",
      "loss: 0.685953  [ 2608/ 3200]\n",
      "loss: 0.547081  [ 2624/ 3200]\n",
      "loss: 0.581812  [ 2640/ 3200]\n",
      "loss: 0.264333  [ 2656/ 3200]\n",
      "loss: 0.427558  [ 2672/ 3200]\n",
      "loss: 0.694660  [ 2688/ 3200]\n",
      "loss: 0.503143  [ 2704/ 3200]\n",
      "loss: 0.811401  [ 2720/ 3200]\n",
      "loss: 0.705331  [ 2736/ 3200]\n",
      "loss: 0.425598  [ 2752/ 3200]\n",
      "loss: 0.917972  [ 2768/ 3200]\n",
      "loss: 0.938687  [ 2784/ 3200]\n",
      "loss: 0.365469  [ 2800/ 3200]\n",
      "loss: 0.387225  [ 2816/ 3200]\n",
      "loss: 0.520977  [ 2832/ 3200]\n",
      "loss: 0.999922  [ 2848/ 3200]\n",
      "loss: 0.752537  [ 2864/ 3200]\n",
      "loss: 0.592689  [ 2880/ 3200]\n",
      "loss: 0.615545  [ 2896/ 3200]\n",
      "loss: 0.662709  [ 2912/ 3200]\n",
      "loss: 0.568813  [ 2928/ 3200]\n",
      "loss: 1.030167  [ 2944/ 3200]\n",
      "loss: 0.766419  [ 2960/ 3200]\n",
      "loss: 0.882118  [ 2976/ 3200]\n",
      "loss: 0.661832  [ 2992/ 3200]\n",
      "loss: 0.511633  [ 3008/ 3200]\n",
      "loss: 0.411919  [ 3024/ 3200]\n",
      "loss: 0.472637  [ 3040/ 3200]\n",
      "loss: 0.549539  [ 3056/ 3200]\n",
      "loss: 0.441197  [ 3072/ 3200]\n",
      "loss: 0.566261  [ 3088/ 3200]\n",
      "loss: 0.375284  [ 3104/ 3200]\n",
      "loss: 0.464690  [ 3120/ 3200]\n",
      "loss: 0.472321  [ 3136/ 3200]\n",
      "loss: 0.543067  [ 3152/ 3200]\n",
      "loss: 0.294406  [ 3168/ 3200]\n",
      "loss: 0.716176  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.037037\n",
      "f1 macro averaged score: 0.783091\n",
      "Accuracy               : 78.6%\n",
      "Confusion matrix       :\n",
      "tensor([[189,   5,   0,   6],\n",
      "        [ 37, 122,  31,  10],\n",
      "        [  1,  25, 168,   6],\n",
      "        [  6,  32,  12, 150]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0405e-02.\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 0.452248  [    0/ 3200]\n",
      "loss: 0.796246  [   16/ 3200]\n",
      "loss: 0.452968  [   32/ 3200]\n",
      "loss: 0.816913  [   48/ 3200]\n",
      "loss: 1.087780  [   64/ 3200]\n",
      "loss: 0.509621  [   80/ 3200]\n",
      "loss: 0.445999  [   96/ 3200]\n",
      "loss: 0.447023  [  112/ 3200]\n",
      "loss: 0.344293  [  128/ 3200]\n",
      "loss: 0.311821  [  144/ 3200]\n",
      "loss: 0.451128  [  160/ 3200]\n",
      "loss: 0.440393  [  176/ 3200]\n",
      "loss: 0.434457  [  192/ 3200]\n",
      "loss: 0.680057  [  208/ 3200]\n",
      "loss: 0.510072  [  224/ 3200]\n",
      "loss: 0.448938  [  240/ 3200]\n",
      "loss: 0.558480  [  256/ 3200]\n",
      "loss: 1.029591  [  272/ 3200]\n",
      "loss: 0.436608  [  288/ 3200]\n",
      "loss: 0.825687  [  304/ 3200]\n",
      "loss: 0.646068  [  320/ 3200]\n",
      "loss: 0.726641  [  336/ 3200]\n",
      "loss: 0.807028  [  352/ 3200]\n",
      "loss: 0.440879  [  368/ 3200]\n",
      "loss: 0.506564  [  384/ 3200]\n",
      "loss: 0.496199  [  400/ 3200]\n",
      "loss: 0.473224  [  416/ 3200]\n",
      "loss: 0.592122  [  432/ 3200]\n",
      "loss: 0.729801  [  448/ 3200]\n",
      "loss: 0.713214  [  464/ 3200]\n",
      "loss: 0.487763  [  480/ 3200]\n",
      "loss: 0.671487  [  496/ 3200]\n",
      "loss: 0.393292  [  512/ 3200]\n",
      "loss: 0.540782  [  528/ 3200]\n",
      "loss: 0.341037  [  544/ 3200]\n",
      "loss: 0.397308  [  560/ 3200]\n",
      "loss: 0.757273  [  576/ 3200]\n",
      "loss: 0.619853  [  592/ 3200]\n",
      "loss: 0.539408  [  608/ 3200]\n",
      "loss: 0.154520  [  624/ 3200]\n",
      "loss: 0.417455  [  640/ 3200]\n",
      "loss: 0.372450  [  656/ 3200]\n",
      "loss: 0.604064  [  672/ 3200]\n",
      "loss: 0.351469  [  688/ 3200]\n",
      "loss: 0.804098  [  704/ 3200]\n",
      "loss: 0.693274  [  720/ 3200]\n",
      "loss: 0.591654  [  736/ 3200]\n",
      "loss: 0.528823  [  752/ 3200]\n",
      "loss: 1.073679  [  768/ 3200]\n",
      "loss: 0.513205  [  784/ 3200]\n",
      "loss: 0.596670  [  800/ 3200]\n",
      "loss: 0.861947  [  816/ 3200]\n",
      "loss: 0.590902  [  832/ 3200]\n",
      "loss: 0.530778  [  848/ 3200]\n",
      "loss: 0.794505  [  864/ 3200]\n",
      "loss: 0.532269  [  880/ 3200]\n",
      "loss: 0.493151  [  896/ 3200]\n",
      "loss: 0.746161  [  912/ 3200]\n",
      "loss: 0.390174  [  928/ 3200]\n",
      "loss: 0.638600  [  944/ 3200]\n",
      "loss: 0.619573  [  960/ 3200]\n",
      "loss: 0.285841  [  976/ 3200]\n",
      "loss: 1.081099  [  992/ 3200]\n",
      "loss: 0.395642  [ 1008/ 3200]\n",
      "loss: 0.323023  [ 1024/ 3200]\n",
      "loss: 0.762430  [ 1040/ 3200]\n",
      "loss: 0.431296  [ 1056/ 3200]\n",
      "loss: 0.409207  [ 1072/ 3200]\n",
      "loss: 0.572729  [ 1088/ 3200]\n",
      "loss: 0.212526  [ 1104/ 3200]\n",
      "loss: 0.508996  [ 1120/ 3200]\n",
      "loss: 0.448348  [ 1136/ 3200]\n",
      "loss: 0.688112  [ 1152/ 3200]\n",
      "loss: 0.446312  [ 1168/ 3200]\n",
      "loss: 0.543284  [ 1184/ 3200]\n",
      "loss: 0.442561  [ 1200/ 3200]\n",
      "loss: 0.225797  [ 1216/ 3200]\n",
      "loss: 0.412724  [ 1232/ 3200]\n",
      "loss: 0.745576  [ 1248/ 3200]\n",
      "loss: 0.854229  [ 1264/ 3200]\n",
      "loss: 0.465804  [ 1280/ 3200]\n",
      "loss: 0.236388  [ 1296/ 3200]\n",
      "loss: 0.570387  [ 1312/ 3200]\n",
      "loss: 0.202421  [ 1328/ 3200]\n",
      "loss: 0.434269  [ 1344/ 3200]\n",
      "loss: 0.363877  [ 1360/ 3200]\n",
      "loss: 0.230673  [ 1376/ 3200]\n",
      "loss: 0.299596  [ 1392/ 3200]\n",
      "loss: 0.362322  [ 1408/ 3200]\n",
      "loss: 0.263789  [ 1424/ 3200]\n",
      "loss: 0.369354  [ 1440/ 3200]\n",
      "loss: 0.221526  [ 1456/ 3200]\n",
      "loss: 0.623325  [ 1472/ 3200]\n",
      "loss: 0.509085  [ 1488/ 3200]\n",
      "loss: 0.249559  [ 1504/ 3200]\n",
      "loss: 0.292189  [ 1520/ 3200]\n",
      "loss: 0.911718  [ 1536/ 3200]\n",
      "loss: 0.463475  [ 1552/ 3200]\n",
      "loss: 0.528693  [ 1568/ 3200]\n",
      "loss: 0.224569  [ 1584/ 3200]\n",
      "loss: 0.348362  [ 1600/ 3200]\n",
      "loss: 0.482230  [ 1616/ 3200]\n",
      "loss: 0.552850  [ 1632/ 3200]\n",
      "loss: 0.347896  [ 1648/ 3200]\n",
      "loss: 0.506117  [ 1664/ 3200]\n",
      "loss: 0.804388  [ 1680/ 3200]\n",
      "loss: 0.316879  [ 1696/ 3200]\n",
      "loss: 0.546654  [ 1712/ 3200]\n",
      "loss: 0.218337  [ 1728/ 3200]\n",
      "loss: 0.787683  [ 1744/ 3200]\n",
      "loss: 0.717577  [ 1760/ 3200]\n",
      "loss: 1.501210  [ 1776/ 3200]\n",
      "loss: 0.218688  [ 1792/ 3200]\n",
      "loss: 1.071122  [ 1808/ 3200]\n",
      "loss: 0.427237  [ 1824/ 3200]\n",
      "loss: 0.382379  [ 1840/ 3200]\n",
      "loss: 0.410353  [ 1856/ 3200]\n",
      "loss: 0.524826  [ 1872/ 3200]\n",
      "loss: 0.330427  [ 1888/ 3200]\n",
      "loss: 0.434829  [ 1904/ 3200]\n",
      "loss: 0.685140  [ 1920/ 3200]\n",
      "loss: 0.892985  [ 1936/ 3200]\n",
      "loss: 0.835701  [ 1952/ 3200]\n",
      "loss: 0.294599  [ 1968/ 3200]\n",
      "loss: 0.758249  [ 1984/ 3200]\n",
      "loss: 0.455257  [ 2000/ 3200]\n",
      "loss: 0.624275  [ 2016/ 3200]\n",
      "loss: 0.448047  [ 2032/ 3200]\n",
      "loss: 0.460596  [ 2048/ 3200]\n",
      "loss: 0.321809  [ 2064/ 3200]\n",
      "loss: 1.256976  [ 2080/ 3200]\n",
      "loss: 0.249699  [ 2096/ 3200]\n",
      "loss: 0.425229  [ 2112/ 3200]\n",
      "loss: 0.542007  [ 2128/ 3200]\n",
      "loss: 0.490850  [ 2144/ 3200]\n",
      "loss: 0.694652  [ 2160/ 3200]\n",
      "loss: 0.469129  [ 2176/ 3200]\n",
      "loss: 0.315258  [ 2192/ 3200]\n",
      "loss: 0.353607  [ 2208/ 3200]\n",
      "loss: 0.696777  [ 2224/ 3200]\n",
      "loss: 0.522023  [ 2240/ 3200]\n",
      "loss: 0.559157  [ 2256/ 3200]\n",
      "loss: 0.142915  [ 2272/ 3200]\n",
      "loss: 0.646072  [ 2288/ 3200]\n",
      "loss: 0.630439  [ 2304/ 3200]\n",
      "loss: 0.544213  [ 2320/ 3200]\n",
      "loss: 0.861070  [ 2336/ 3200]\n",
      "loss: 0.423230  [ 2352/ 3200]\n",
      "loss: 0.322327  [ 2368/ 3200]\n",
      "loss: 0.425434  [ 2384/ 3200]\n",
      "loss: 0.528494  [ 2400/ 3200]\n",
      "loss: 0.578672  [ 2416/ 3200]\n",
      "loss: 0.391190  [ 2432/ 3200]\n",
      "loss: 0.599417  [ 2448/ 3200]\n",
      "loss: 0.599299  [ 2464/ 3200]\n",
      "loss: 0.686451  [ 2480/ 3200]\n",
      "loss: 0.454489  [ 2496/ 3200]\n",
      "loss: 0.649003  [ 2512/ 3200]\n",
      "loss: 0.471699  [ 2528/ 3200]\n",
      "loss: 0.386727  [ 2544/ 3200]\n",
      "loss: 0.781270  [ 2560/ 3200]\n",
      "loss: 0.386008  [ 2576/ 3200]\n",
      "loss: 0.316299  [ 2592/ 3200]\n",
      "loss: 0.450897  [ 2608/ 3200]\n",
      "loss: 0.427214  [ 2624/ 3200]\n",
      "loss: 0.405451  [ 2640/ 3200]\n",
      "loss: 0.332188  [ 2656/ 3200]\n",
      "loss: 0.647743  [ 2672/ 3200]\n",
      "loss: 0.509453  [ 2688/ 3200]\n",
      "loss: 0.491621  [ 2704/ 3200]\n",
      "loss: 0.335805  [ 2720/ 3200]\n",
      "loss: 0.478610  [ 2736/ 3200]\n",
      "loss: 0.518752  [ 2752/ 3200]\n",
      "loss: 0.524397  [ 2768/ 3200]\n",
      "loss: 0.945344  [ 2784/ 3200]\n",
      "loss: 0.197132  [ 2800/ 3200]\n",
      "loss: 0.689455  [ 2816/ 3200]\n",
      "loss: 0.390673  [ 2832/ 3200]\n",
      "loss: 0.311166  [ 2848/ 3200]\n",
      "loss: 0.426456  [ 2864/ 3200]\n",
      "loss: 0.495748  [ 2880/ 3200]\n",
      "loss: 0.551454  [ 2896/ 3200]\n",
      "loss: 0.705410  [ 2912/ 3200]\n",
      "loss: 0.447586  [ 2928/ 3200]\n",
      "loss: 0.758938  [ 2944/ 3200]\n",
      "loss: 0.649240  [ 2960/ 3200]\n",
      "loss: 0.553403  [ 2976/ 3200]\n",
      "loss: 0.377513  [ 2992/ 3200]\n",
      "loss: 0.354022  [ 3008/ 3200]\n",
      "loss: 0.316725  [ 3024/ 3200]\n",
      "loss: 0.417099  [ 3040/ 3200]\n",
      "loss: 0.342285  [ 3056/ 3200]\n",
      "loss: 0.661281  [ 3072/ 3200]\n",
      "loss: 0.861877  [ 3088/ 3200]\n",
      "loss: 0.709436  [ 3104/ 3200]\n",
      "loss: 0.777946  [ 3120/ 3200]\n",
      "loss: 0.321382  [ 3136/ 3200]\n",
      "loss: 0.358249  [ 3152/ 3200]\n",
      "loss: 0.156488  [ 3168/ 3200]\n",
      "loss: 0.915460  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038004\n",
      "f1 macro averaged score: 0.761641\n",
      "Accuracy               : 76.2%\n",
      "Confusion matrix       :\n",
      "tensor([[162,  26,   0,  12],\n",
      "        [ 17, 117,  43,  23],\n",
      "        [  1,  20, 174,   5],\n",
      "        [  1,  26,  16, 157]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0450e-02.\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 0.550565  [    0/ 3200]\n",
      "loss: 0.732050  [   16/ 3200]\n",
      "loss: 0.774233  [   32/ 3200]\n",
      "loss: 0.784222  [   48/ 3200]\n",
      "loss: 0.661215  [   64/ 3200]\n",
      "loss: 0.522544  [   80/ 3200]\n",
      "loss: 0.540194  [   96/ 3200]\n",
      "loss: 0.598298  [  112/ 3200]\n",
      "loss: 0.414530  [  128/ 3200]\n",
      "loss: 0.343978  [  144/ 3200]\n",
      "loss: 0.402917  [  160/ 3200]\n",
      "loss: 0.453906  [  176/ 3200]\n",
      "loss: 0.366593  [  192/ 3200]\n",
      "loss: 0.237693  [  208/ 3200]\n",
      "loss: 0.485627  [  224/ 3200]\n",
      "loss: 0.833161  [  240/ 3200]\n",
      "loss: 0.320065  [  256/ 3200]\n",
      "loss: 0.435573  [  272/ 3200]\n",
      "loss: 0.448626  [  288/ 3200]\n",
      "loss: 0.761691  [  304/ 3200]\n",
      "loss: 0.966185  [  320/ 3200]\n",
      "loss: 0.868059  [  336/ 3200]\n",
      "loss: 0.463947  [  352/ 3200]\n",
      "loss: 0.501497  [  368/ 3200]\n",
      "loss: 0.472127  [  384/ 3200]\n",
      "loss: 0.237350  [  400/ 3200]\n",
      "loss: 0.859090  [  416/ 3200]\n",
      "loss: 0.164665  [  432/ 3200]\n",
      "loss: 0.748047  [  448/ 3200]\n",
      "loss: 0.437418  [  464/ 3200]\n",
      "loss: 0.365469  [  480/ 3200]\n",
      "loss: 0.517748  [  496/ 3200]\n",
      "loss: 0.502116  [  512/ 3200]\n",
      "loss: 0.535384  [  528/ 3200]\n",
      "loss: 0.504763  [  544/ 3200]\n",
      "loss: 0.571768  [  560/ 3200]\n",
      "loss: 0.173165  [  576/ 3200]\n",
      "loss: 0.286236  [  592/ 3200]\n",
      "loss: 0.349387  [  608/ 3200]\n",
      "loss: 0.747411  [  624/ 3200]\n",
      "loss: 0.651446  [  640/ 3200]\n",
      "loss: 0.998876  [  656/ 3200]\n",
      "loss: 0.354469  [  672/ 3200]\n",
      "loss: 0.770213  [  688/ 3200]\n",
      "loss: 0.433692  [  704/ 3200]\n",
      "loss: 0.780843  [  720/ 3200]\n",
      "loss: 0.586617  [  736/ 3200]\n",
      "loss: 0.491125  [  752/ 3200]\n",
      "loss: 0.471937  [  768/ 3200]\n",
      "loss: 0.521657  [  784/ 3200]\n",
      "loss: 0.256763  [  800/ 3200]\n",
      "loss: 0.544953  [  816/ 3200]\n",
      "loss: 0.446825  [  832/ 3200]\n",
      "loss: 0.566089  [  848/ 3200]\n",
      "loss: 1.106503  [  864/ 3200]\n",
      "loss: 0.469255  [  880/ 3200]\n",
      "loss: 0.491545  [  896/ 3200]\n",
      "loss: 0.733303  [  912/ 3200]\n",
      "loss: 0.637169  [  928/ 3200]\n",
      "loss: 0.683919  [  944/ 3200]\n",
      "loss: 0.406658  [  960/ 3200]\n",
      "loss: 0.422765  [  976/ 3200]\n",
      "loss: 0.496891  [  992/ 3200]\n",
      "loss: 0.268254  [ 1008/ 3200]\n",
      "loss: 0.407467  [ 1024/ 3200]\n",
      "loss: 0.492282  [ 1040/ 3200]\n",
      "loss: 0.386073  [ 1056/ 3200]\n",
      "loss: 0.475283  [ 1072/ 3200]\n",
      "loss: 0.322549  [ 1088/ 3200]\n",
      "loss: 0.495270  [ 1104/ 3200]\n",
      "loss: 0.606311  [ 1120/ 3200]\n",
      "loss: 0.209020  [ 1136/ 3200]\n",
      "loss: 0.755956  [ 1152/ 3200]\n",
      "loss: 0.343596  [ 1168/ 3200]\n",
      "loss: 0.417681  [ 1184/ 3200]\n",
      "loss: 0.572234  [ 1200/ 3200]\n",
      "loss: 0.327367  [ 1216/ 3200]\n",
      "loss: 0.327845  [ 1232/ 3200]\n",
      "loss: 0.459026  [ 1248/ 3200]\n",
      "loss: 0.328460  [ 1264/ 3200]\n",
      "loss: 0.644442  [ 1280/ 3200]\n",
      "loss: 0.393870  [ 1296/ 3200]\n",
      "loss: 0.330294  [ 1312/ 3200]\n",
      "loss: 0.607305  [ 1328/ 3200]\n",
      "loss: 0.176189  [ 1344/ 3200]\n",
      "loss: 0.897040  [ 1360/ 3200]\n",
      "loss: 1.248072  [ 1376/ 3200]\n",
      "loss: 0.620559  [ 1392/ 3200]\n",
      "loss: 0.542061  [ 1408/ 3200]\n",
      "loss: 0.301571  [ 1424/ 3200]\n",
      "loss: 0.411403  [ 1440/ 3200]\n",
      "loss: 0.727536  [ 1456/ 3200]\n",
      "loss: 0.211498  [ 1472/ 3200]\n",
      "loss: 0.411796  [ 1488/ 3200]\n",
      "loss: 0.403904  [ 1504/ 3200]\n",
      "loss: 0.197377  [ 1520/ 3200]\n",
      "loss: 0.344783  [ 1536/ 3200]\n",
      "loss: 0.370938  [ 1552/ 3200]\n",
      "loss: 0.505440  [ 1568/ 3200]\n",
      "loss: 0.425880  [ 1584/ 3200]\n",
      "loss: 0.427964  [ 1600/ 3200]\n",
      "loss: 0.845066  [ 1616/ 3200]\n",
      "loss: 0.480359  [ 1632/ 3200]\n",
      "loss: 0.652849  [ 1648/ 3200]\n",
      "loss: 0.844938  [ 1664/ 3200]\n",
      "loss: 0.769330  [ 1680/ 3200]\n",
      "loss: 0.749267  [ 1696/ 3200]\n",
      "loss: 0.236123  [ 1712/ 3200]\n",
      "loss: 0.795755  [ 1728/ 3200]\n",
      "loss: 0.812345  [ 1744/ 3200]\n",
      "loss: 0.296805  [ 1760/ 3200]\n",
      "loss: 0.510129  [ 1776/ 3200]\n",
      "loss: 0.294773  [ 1792/ 3200]\n",
      "loss: 0.445416  [ 1808/ 3200]\n",
      "loss: 0.595144  [ 1824/ 3200]\n",
      "loss: 0.571748  [ 1840/ 3200]\n",
      "loss: 0.198100  [ 1856/ 3200]\n",
      "loss: 0.334263  [ 1872/ 3200]\n",
      "loss: 0.900650  [ 1888/ 3200]\n",
      "loss: 0.862469  [ 1904/ 3200]\n",
      "loss: 0.492056  [ 1920/ 3200]\n",
      "loss: 0.550727  [ 1936/ 3200]\n",
      "loss: 0.513504  [ 1952/ 3200]\n",
      "loss: 0.251946  [ 1968/ 3200]\n",
      "loss: 0.251147  [ 1984/ 3200]\n",
      "loss: 0.589835  [ 2000/ 3200]\n",
      "loss: 0.221273  [ 2016/ 3200]\n",
      "loss: 0.377301  [ 2032/ 3200]\n",
      "loss: 0.325123  [ 2048/ 3200]\n",
      "loss: 0.650048  [ 2064/ 3200]\n",
      "loss: 0.856294  [ 2080/ 3200]\n",
      "loss: 0.389530  [ 2096/ 3200]\n",
      "loss: 0.397730  [ 2112/ 3200]\n",
      "loss: 0.186400  [ 2128/ 3200]\n",
      "loss: 0.352911  [ 2144/ 3200]\n",
      "loss: 0.887968  [ 2160/ 3200]\n",
      "loss: 1.421496  [ 2176/ 3200]\n",
      "loss: 0.623238  [ 2192/ 3200]\n",
      "loss: 0.487028  [ 2208/ 3200]\n",
      "loss: 0.322988  [ 2224/ 3200]\n",
      "loss: 0.466395  [ 2240/ 3200]\n",
      "loss: 0.663526  [ 2256/ 3200]\n",
      "loss: 0.734080  [ 2272/ 3200]\n",
      "loss: 0.434517  [ 2288/ 3200]\n",
      "loss: 0.275442  [ 2304/ 3200]\n",
      "loss: 0.260288  [ 2320/ 3200]\n",
      "loss: 0.619651  [ 2336/ 3200]\n",
      "loss: 0.457857  [ 2352/ 3200]\n",
      "loss: 0.326557  [ 2368/ 3200]\n",
      "loss: 0.430494  [ 2384/ 3200]\n",
      "loss: 0.673972  [ 2400/ 3200]\n",
      "loss: 0.442162  [ 2416/ 3200]\n",
      "loss: 0.409203  [ 2432/ 3200]\n",
      "loss: 0.599190  [ 2448/ 3200]\n",
      "loss: 0.553810  [ 2464/ 3200]\n",
      "loss: 0.201147  [ 2480/ 3200]\n",
      "loss: 0.448953  [ 2496/ 3200]\n",
      "loss: 0.511243  [ 2512/ 3200]\n",
      "loss: 0.417875  [ 2528/ 3200]\n",
      "loss: 0.280063  [ 2544/ 3200]\n",
      "loss: 0.550581  [ 2560/ 3200]\n",
      "loss: 0.834027  [ 2576/ 3200]\n",
      "loss: 0.342780  [ 2592/ 3200]\n",
      "loss: 0.446962  [ 2608/ 3200]\n",
      "loss: 0.607698  [ 2624/ 3200]\n",
      "loss: 0.369705  [ 2640/ 3200]\n",
      "loss: 0.744901  [ 2656/ 3200]\n",
      "loss: 0.989681  [ 2672/ 3200]\n",
      "loss: 0.413367  [ 2688/ 3200]\n",
      "loss: 0.495617  [ 2704/ 3200]\n",
      "loss: 0.243344  [ 2720/ 3200]\n",
      "loss: 0.691125  [ 2736/ 3200]\n",
      "loss: 0.482348  [ 2752/ 3200]\n",
      "loss: 0.361462  [ 2768/ 3200]\n",
      "loss: 0.425615  [ 2784/ 3200]\n",
      "loss: 0.417028  [ 2800/ 3200]\n",
      "loss: 0.339301  [ 2816/ 3200]\n",
      "loss: 0.681198  [ 2832/ 3200]\n",
      "loss: 0.383022  [ 2848/ 3200]\n",
      "loss: 0.684248  [ 2864/ 3200]\n",
      "loss: 0.877021  [ 2880/ 3200]\n",
      "loss: 0.387247  [ 2896/ 3200]\n",
      "loss: 0.419764  [ 2912/ 3200]\n",
      "loss: 0.934454  [ 2928/ 3200]\n",
      "loss: 0.325114  [ 2944/ 3200]\n",
      "loss: 0.671030  [ 2960/ 3200]\n",
      "loss: 0.494115  [ 2976/ 3200]\n",
      "loss: 0.322753  [ 2992/ 3200]\n",
      "loss: 0.219085  [ 3008/ 3200]\n",
      "loss: 0.282992  [ 3024/ 3200]\n",
      "loss: 0.241204  [ 3040/ 3200]\n",
      "loss: 0.516431  [ 3056/ 3200]\n",
      "loss: 0.478629  [ 3072/ 3200]\n",
      "loss: 0.756027  [ 3088/ 3200]\n",
      "loss: 0.736056  [ 3104/ 3200]\n",
      "loss: 0.468867  [ 3120/ 3200]\n",
      "loss: 0.621500  [ 3136/ 3200]\n",
      "loss: 0.551078  [ 3152/ 3200]\n",
      "loss: 0.839485  [ 3168/ 3200]\n",
      "loss: 1.059975  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036450\n",
      "f1 macro averaged score: 0.756668\n",
      "Accuracy               : 76.4%\n",
      "Confusion matrix       :\n",
      "tensor([[175,   7,   0,  18],\n",
      "        [ 30, 101,  31,  38],\n",
      "        [  0,  18, 169,  13],\n",
      "        [  3,  18,  13, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0495e-02.\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 0.752971  [    0/ 3200]\n",
      "loss: 1.100135  [   16/ 3200]\n",
      "loss: 0.420983  [   32/ 3200]\n",
      "loss: 0.358877  [   48/ 3200]\n",
      "loss: 0.302822  [   64/ 3200]\n",
      "loss: 0.771357  [   80/ 3200]\n",
      "loss: 0.370779  [   96/ 3200]\n",
      "loss: 0.526139  [  112/ 3200]\n",
      "loss: 0.893369  [  128/ 3200]\n",
      "loss: 1.278031  [  144/ 3200]\n",
      "loss: 0.398083  [  160/ 3200]\n",
      "loss: 0.264518  [  176/ 3200]\n",
      "loss: 0.492439  [  192/ 3200]\n",
      "loss: 0.684040  [  208/ 3200]\n",
      "loss: 0.525598  [  224/ 3200]\n",
      "loss: 0.602536  [  240/ 3200]\n",
      "loss: 0.503109  [  256/ 3200]\n",
      "loss: 0.589715  [  272/ 3200]\n",
      "loss: 0.694602  [  288/ 3200]\n",
      "loss: 0.876607  [  304/ 3200]\n",
      "loss: 0.778355  [  320/ 3200]\n",
      "loss: 0.365283  [  336/ 3200]\n",
      "loss: 0.529564  [  352/ 3200]\n",
      "loss: 0.543341  [  368/ 3200]\n",
      "loss: 0.358110  [  384/ 3200]\n",
      "loss: 0.465898  [  400/ 3200]\n",
      "loss: 0.315808  [  416/ 3200]\n",
      "loss: 0.199309  [  432/ 3200]\n",
      "loss: 0.215304  [  448/ 3200]\n",
      "loss: 0.190524  [  464/ 3200]\n",
      "loss: 0.921143  [  480/ 3200]\n",
      "loss: 0.421710  [  496/ 3200]\n",
      "loss: 0.746277  [  512/ 3200]\n",
      "loss: 0.288976  [  528/ 3200]\n",
      "loss: 0.175815  [  544/ 3200]\n",
      "loss: 0.630979  [  560/ 3200]\n",
      "loss: 0.417647  [  576/ 3200]\n",
      "loss: 0.348805  [  592/ 3200]\n",
      "loss: 0.602185  [  608/ 3200]\n",
      "loss: 0.849832  [  624/ 3200]\n",
      "loss: 0.172161  [  640/ 3200]\n",
      "loss: 0.628010  [  656/ 3200]\n",
      "loss: 0.490895  [  672/ 3200]\n",
      "loss: 0.441811  [  688/ 3200]\n",
      "loss: 0.148723  [  704/ 3200]\n",
      "loss: 0.415016  [  720/ 3200]\n",
      "loss: 0.446478  [  736/ 3200]\n",
      "loss: 0.638187  [  752/ 3200]\n",
      "loss: 0.510053  [  768/ 3200]\n",
      "loss: 0.489806  [  784/ 3200]\n",
      "loss: 0.386820  [  800/ 3200]\n",
      "loss: 0.362661  [  816/ 3200]\n",
      "loss: 0.546072  [  832/ 3200]\n",
      "loss: 0.290905  [  848/ 3200]\n",
      "loss: 1.142355  [  864/ 3200]\n",
      "loss: 0.700411  [  880/ 3200]\n",
      "loss: 0.136085  [  896/ 3200]\n",
      "loss: 0.329751  [  912/ 3200]\n",
      "loss: 0.421425  [  928/ 3200]\n",
      "loss: 0.542243  [  944/ 3200]\n",
      "loss: 0.449079  [  960/ 3200]\n",
      "loss: 0.505990  [  976/ 3200]\n",
      "loss: 0.771373  [  992/ 3200]\n",
      "loss: 0.711740  [ 1008/ 3200]\n",
      "loss: 0.642399  [ 1024/ 3200]\n",
      "loss: 0.460193  [ 1040/ 3200]\n",
      "loss: 0.480657  [ 1056/ 3200]\n",
      "loss: 0.643958  [ 1072/ 3200]\n",
      "loss: 0.329445  [ 1088/ 3200]\n",
      "loss: 0.900907  [ 1104/ 3200]\n",
      "loss: 0.418444  [ 1120/ 3200]\n",
      "loss: 0.675731  [ 1136/ 3200]\n",
      "loss: 0.332692  [ 1152/ 3200]\n",
      "loss: 0.419291  [ 1168/ 3200]\n",
      "loss: 0.463920  [ 1184/ 3200]\n",
      "loss: 0.703552  [ 1200/ 3200]\n",
      "loss: 1.347034  [ 1216/ 3200]\n",
      "loss: 0.278505  [ 1232/ 3200]\n",
      "loss: 0.744148  [ 1248/ 3200]\n",
      "loss: 0.319667  [ 1264/ 3200]\n",
      "loss: 0.362629  [ 1280/ 3200]\n",
      "loss: 0.357838  [ 1296/ 3200]\n",
      "loss: 0.679581  [ 1312/ 3200]\n",
      "loss: 0.209246  [ 1328/ 3200]\n",
      "loss: 0.400360  [ 1344/ 3200]\n",
      "loss: 0.262561  [ 1360/ 3200]\n",
      "loss: 0.673848  [ 1376/ 3200]\n",
      "loss: 0.902704  [ 1392/ 3200]\n",
      "loss: 1.067912  [ 1408/ 3200]\n",
      "loss: 0.557329  [ 1424/ 3200]\n",
      "loss: 0.477578  [ 1440/ 3200]\n",
      "loss: 0.496065  [ 1456/ 3200]\n",
      "loss: 0.467552  [ 1472/ 3200]\n",
      "loss: 0.460675  [ 1488/ 3200]\n",
      "loss: 0.506705  [ 1504/ 3200]\n",
      "loss: 0.396473  [ 1520/ 3200]\n",
      "loss: 0.685940  [ 1536/ 3200]\n",
      "loss: 0.582714  [ 1552/ 3200]\n",
      "loss: 0.888260  [ 1568/ 3200]\n",
      "loss: 0.509911  [ 1584/ 3200]\n",
      "loss: 0.482889  [ 1600/ 3200]\n",
      "loss: 0.312772  [ 1616/ 3200]\n",
      "loss: 0.434798  [ 1632/ 3200]\n",
      "loss: 0.463939  [ 1648/ 3200]\n",
      "loss: 0.407706  [ 1664/ 3200]\n",
      "loss: 0.819918  [ 1680/ 3200]\n",
      "loss: 0.524774  [ 1696/ 3200]\n",
      "loss: 0.563827  [ 1712/ 3200]\n",
      "loss: 0.430278  [ 1728/ 3200]\n",
      "loss: 0.171169  [ 1744/ 3200]\n",
      "loss: 0.335453  [ 1760/ 3200]\n",
      "loss: 0.292932  [ 1776/ 3200]\n",
      "loss: 0.244973  [ 1792/ 3200]\n",
      "loss: 0.405752  [ 1808/ 3200]\n",
      "loss: 0.571761  [ 1824/ 3200]\n",
      "loss: 0.204798  [ 1840/ 3200]\n",
      "loss: 0.353487  [ 1856/ 3200]\n",
      "loss: 0.409055  [ 1872/ 3200]\n",
      "loss: 0.427708  [ 1888/ 3200]\n",
      "loss: 0.647360  [ 1904/ 3200]\n",
      "loss: 0.415261  [ 1920/ 3200]\n",
      "loss: 0.305239  [ 1936/ 3200]\n",
      "loss: 0.391976  [ 1952/ 3200]\n",
      "loss: 0.497368  [ 1968/ 3200]\n",
      "loss: 0.391562  [ 1984/ 3200]\n",
      "loss: 0.338831  [ 2000/ 3200]\n",
      "loss: 0.439597  [ 2016/ 3200]\n",
      "loss: 0.453827  [ 2032/ 3200]\n",
      "loss: 0.673592  [ 2048/ 3200]\n",
      "loss: 0.409540  [ 2064/ 3200]\n",
      "loss: 0.645575  [ 2080/ 3200]\n",
      "loss: 0.350696  [ 2096/ 3200]\n",
      "loss: 0.474825  [ 2112/ 3200]\n",
      "loss: 0.596822  [ 2128/ 3200]\n",
      "loss: 0.535330  [ 2144/ 3200]\n",
      "loss: 0.291378  [ 2160/ 3200]\n",
      "loss: 0.309494  [ 2176/ 3200]\n",
      "loss: 0.645886  [ 2192/ 3200]\n",
      "loss: 0.418145  [ 2208/ 3200]\n",
      "loss: 0.395463  [ 2224/ 3200]\n",
      "loss: 0.581617  [ 2240/ 3200]\n",
      "loss: 0.783049  [ 2256/ 3200]\n",
      "loss: 0.282830  [ 2272/ 3200]\n",
      "loss: 0.383213  [ 2288/ 3200]\n",
      "loss: 1.026441  [ 2304/ 3200]\n",
      "loss: 0.335093  [ 2320/ 3200]\n",
      "loss: 0.478801  [ 2336/ 3200]\n",
      "loss: 0.228180  [ 2352/ 3200]\n",
      "loss: 0.609899  [ 2368/ 3200]\n",
      "loss: 0.243734  [ 2384/ 3200]\n",
      "loss: 0.702106  [ 2400/ 3200]\n",
      "loss: 0.320431  [ 2416/ 3200]\n",
      "loss: 0.610042  [ 2432/ 3200]\n",
      "loss: 1.078457  [ 2448/ 3200]\n",
      "loss: 0.471675  [ 2464/ 3200]\n",
      "loss: 0.305323  [ 2480/ 3200]\n",
      "loss: 0.479922  [ 2496/ 3200]\n",
      "loss: 0.451753  [ 2512/ 3200]\n",
      "loss: 0.298875  [ 2528/ 3200]\n",
      "loss: 0.475525  [ 2544/ 3200]\n",
      "loss: 0.298408  [ 2560/ 3200]\n",
      "loss: 0.419711  [ 2576/ 3200]\n",
      "loss: 0.301321  [ 2592/ 3200]\n",
      "loss: 0.664629  [ 2608/ 3200]\n",
      "loss: 0.369156  [ 2624/ 3200]\n",
      "loss: 0.241185  [ 2640/ 3200]\n",
      "loss: 0.519993  [ 2656/ 3200]\n",
      "loss: 0.324061  [ 2672/ 3200]\n",
      "loss: 0.737239  [ 2688/ 3200]\n",
      "loss: 0.728014  [ 2704/ 3200]\n",
      "loss: 0.691288  [ 2720/ 3200]\n",
      "loss: 0.194400  [ 2736/ 3200]\n",
      "loss: 0.428732  [ 2752/ 3200]\n",
      "loss: 0.230208  [ 2768/ 3200]\n",
      "loss: 0.200139  [ 2784/ 3200]\n",
      "loss: 0.469292  [ 2800/ 3200]\n",
      "loss: 0.828016  [ 2816/ 3200]\n",
      "loss: 0.512439  [ 2832/ 3200]\n",
      "loss: 0.525704  [ 2848/ 3200]\n",
      "loss: 0.662374  [ 2864/ 3200]\n",
      "loss: 0.581909  [ 2880/ 3200]\n",
      "loss: 0.596557  [ 2896/ 3200]\n",
      "loss: 0.295992  [ 2912/ 3200]\n",
      "loss: 0.625365  [ 2928/ 3200]\n",
      "loss: 0.384484  [ 2944/ 3200]\n",
      "loss: 0.495035  [ 2960/ 3200]\n",
      "loss: 0.560682  [ 2976/ 3200]\n",
      "loss: 0.383593  [ 2992/ 3200]\n",
      "loss: 0.853841  [ 3008/ 3200]\n",
      "loss: 0.573682  [ 3024/ 3200]\n",
      "loss: 0.388315  [ 3040/ 3200]\n",
      "loss: 0.425425  [ 3056/ 3200]\n",
      "loss: 0.266476  [ 3072/ 3200]\n",
      "loss: 0.522615  [ 3088/ 3200]\n",
      "loss: 0.624467  [ 3104/ 3200]\n",
      "loss: 0.666485  [ 3120/ 3200]\n",
      "loss: 0.387759  [ 3136/ 3200]\n",
      "loss: 0.137779  [ 3152/ 3200]\n",
      "loss: 0.371079  [ 3168/ 3200]\n",
      "loss: 0.576109  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036407\n",
      "f1 macro averaged score: 0.762515\n",
      "Accuracy               : 77.5%\n",
      "Confusion matrix       :\n",
      "tensor([[185,   8,   0,   7],\n",
      "        [ 32,  91,  42,  35],\n",
      "        [  0,  14, 179,   7],\n",
      "        [  4,  15,  16, 165]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0540e-02.\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 0.337477  [    0/ 3200]\n",
      "loss: 0.399690  [   16/ 3200]\n",
      "loss: 0.257198  [   32/ 3200]\n",
      "loss: 0.642515  [   48/ 3200]\n",
      "loss: 0.444590  [   64/ 3200]\n",
      "loss: 0.229019  [   80/ 3200]\n",
      "loss: 0.445792  [   96/ 3200]\n",
      "loss: 0.546170  [  112/ 3200]\n",
      "loss: 0.581904  [  128/ 3200]\n",
      "loss: 0.603512  [  144/ 3200]\n",
      "loss: 0.538700  [  160/ 3200]\n",
      "loss: 0.578664  [  176/ 3200]\n",
      "loss: 0.540333  [  192/ 3200]\n",
      "loss: 0.446219  [  208/ 3200]\n",
      "loss: 0.222592  [  224/ 3200]\n",
      "loss: 0.222777  [  240/ 3200]\n",
      "loss: 0.518550  [  256/ 3200]\n",
      "loss: 0.205580  [  272/ 3200]\n",
      "loss: 0.307473  [  288/ 3200]\n",
      "loss: 0.409203  [  304/ 3200]\n",
      "loss: 0.732088  [  320/ 3200]\n",
      "loss: 0.671438  [  336/ 3200]\n",
      "loss: 0.651706  [  352/ 3200]\n",
      "loss: 0.415260  [  368/ 3200]\n",
      "loss: 0.671056  [  384/ 3200]\n",
      "loss: 0.470526  [  400/ 3200]\n",
      "loss: 0.295434  [  416/ 3200]\n",
      "loss: 0.434966  [  432/ 3200]\n",
      "loss: 0.231325  [  448/ 3200]\n",
      "loss: 0.396189  [  464/ 3200]\n",
      "loss: 0.236310  [  480/ 3200]\n",
      "loss: 0.534940  [  496/ 3200]\n",
      "loss: 0.647063  [  512/ 3200]\n",
      "loss: 0.304605  [  528/ 3200]\n",
      "loss: 0.378824  [  544/ 3200]\n",
      "loss: 0.629999  [  560/ 3200]\n",
      "loss: 0.542360  [  576/ 3200]\n",
      "loss: 0.359858  [  592/ 3200]\n",
      "loss: 0.534696  [  608/ 3200]\n",
      "loss: 0.498041  [  624/ 3200]\n",
      "loss: 0.588867  [  640/ 3200]\n",
      "loss: 0.489615  [  656/ 3200]\n",
      "loss: 0.553144  [  672/ 3200]\n",
      "loss: 0.847902  [  688/ 3200]\n",
      "loss: 0.605797  [  704/ 3200]\n",
      "loss: 0.744869  [  720/ 3200]\n",
      "loss: 0.405471  [  736/ 3200]\n",
      "loss: 0.493473  [  752/ 3200]\n",
      "loss: 0.407840  [  768/ 3200]\n",
      "loss: 0.522932  [  784/ 3200]\n",
      "loss: 0.336632  [  800/ 3200]\n",
      "loss: 0.382893  [  816/ 3200]\n",
      "loss: 0.532602  [  832/ 3200]\n",
      "loss: 0.375194  [  848/ 3200]\n",
      "loss: 0.292755  [  864/ 3200]\n",
      "loss: 0.742609  [  880/ 3200]\n",
      "loss: 0.424929  [  896/ 3200]\n",
      "loss: 0.200212  [  912/ 3200]\n",
      "loss: 0.478401  [  928/ 3200]\n",
      "loss: 0.418865  [  944/ 3200]\n",
      "loss: 0.507750  [  960/ 3200]\n",
      "loss: 0.456914  [  976/ 3200]\n",
      "loss: 0.396073  [  992/ 3200]\n",
      "loss: 0.511636  [ 1008/ 3200]\n",
      "loss: 0.391958  [ 1024/ 3200]\n",
      "loss: 0.314213  [ 1040/ 3200]\n",
      "loss: 0.366231  [ 1056/ 3200]\n",
      "loss: 0.686486  [ 1072/ 3200]\n",
      "loss: 0.695558  [ 1088/ 3200]\n",
      "loss: 0.655953  [ 1104/ 3200]\n",
      "loss: 0.512982  [ 1120/ 3200]\n",
      "loss: 0.298271  [ 1136/ 3200]\n",
      "loss: 0.739563  [ 1152/ 3200]\n",
      "loss: 0.452315  [ 1168/ 3200]\n",
      "loss: 0.422790  [ 1184/ 3200]\n",
      "loss: 0.560924  [ 1200/ 3200]\n",
      "loss: 0.889701  [ 1216/ 3200]\n",
      "loss: 0.486632  [ 1232/ 3200]\n",
      "loss: 0.100165  [ 1248/ 3200]\n",
      "loss: 0.355587  [ 1264/ 3200]\n",
      "loss: 0.574145  [ 1280/ 3200]\n",
      "loss: 0.425125  [ 1296/ 3200]\n",
      "loss: 0.641240  [ 1312/ 3200]\n",
      "loss: 0.311861  [ 1328/ 3200]\n",
      "loss: 0.217159  [ 1344/ 3200]\n",
      "loss: 0.458317  [ 1360/ 3200]\n",
      "loss: 0.437092  [ 1376/ 3200]\n",
      "loss: 0.373414  [ 1392/ 3200]\n",
      "loss: 0.848783  [ 1408/ 3200]\n",
      "loss: 0.491887  [ 1424/ 3200]\n",
      "loss: 0.638708  [ 1440/ 3200]\n",
      "loss: 0.186983  [ 1456/ 3200]\n",
      "loss: 0.270539  [ 1472/ 3200]\n",
      "loss: 0.550060  [ 1488/ 3200]\n",
      "loss: 0.513526  [ 1504/ 3200]\n",
      "loss: 0.322193  [ 1520/ 3200]\n",
      "loss: 0.163597  [ 1536/ 3200]\n",
      "loss: 0.247070  [ 1552/ 3200]\n",
      "loss: 0.545419  [ 1568/ 3200]\n",
      "loss: 0.750862  [ 1584/ 3200]\n",
      "loss: 0.531999  [ 1600/ 3200]\n",
      "loss: 0.730605  [ 1616/ 3200]\n",
      "loss: 0.322217  [ 1632/ 3200]\n",
      "loss: 0.166545  [ 1648/ 3200]\n",
      "loss: 0.253348  [ 1664/ 3200]\n",
      "loss: 0.495421  [ 1680/ 3200]\n",
      "loss: 0.388891  [ 1696/ 3200]\n",
      "loss: 0.189805  [ 1712/ 3200]\n",
      "loss: 0.394639  [ 1728/ 3200]\n",
      "loss: 0.572533  [ 1744/ 3200]\n",
      "loss: 0.536427  [ 1760/ 3200]\n",
      "loss: 0.536584  [ 1776/ 3200]\n",
      "loss: 0.645311  [ 1792/ 3200]\n",
      "loss: 0.349471  [ 1808/ 3200]\n",
      "loss: 0.598274  [ 1824/ 3200]\n",
      "loss: 0.793344  [ 1840/ 3200]\n",
      "loss: 0.483432  [ 1856/ 3200]\n",
      "loss: 0.090401  [ 1872/ 3200]\n",
      "loss: 0.213384  [ 1888/ 3200]\n",
      "loss: 0.409002  [ 1904/ 3200]\n",
      "loss: 0.297458  [ 1920/ 3200]\n",
      "loss: 0.122169  [ 1936/ 3200]\n",
      "loss: 0.606807  [ 1952/ 3200]\n",
      "loss: 0.404617  [ 1968/ 3200]\n",
      "loss: 0.446779  [ 1984/ 3200]\n",
      "loss: 0.553841  [ 2000/ 3200]\n",
      "loss: 0.397733  [ 2016/ 3200]\n",
      "loss: 0.290869  [ 2032/ 3200]\n",
      "loss: 0.231349  [ 2048/ 3200]\n",
      "loss: 0.449066  [ 2064/ 3200]\n",
      "loss: 0.806655  [ 2080/ 3200]\n",
      "loss: 1.208461  [ 2096/ 3200]\n",
      "loss: 0.442704  [ 2112/ 3200]\n",
      "loss: 0.418282  [ 2128/ 3200]\n",
      "loss: 0.269300  [ 2144/ 3200]\n",
      "loss: 0.580131  [ 2160/ 3200]\n",
      "loss: 0.739493  [ 2176/ 3200]\n",
      "loss: 0.407416  [ 2192/ 3200]\n",
      "loss: 0.301784  [ 2208/ 3200]\n",
      "loss: 0.481883  [ 2224/ 3200]\n",
      "loss: 0.572693  [ 2240/ 3200]\n",
      "loss: 0.728208  [ 2256/ 3200]\n",
      "loss: 0.711474  [ 2272/ 3200]\n",
      "loss: 0.342401  [ 2288/ 3200]\n",
      "loss: 0.300438  [ 2304/ 3200]\n",
      "loss: 0.490301  [ 2320/ 3200]\n",
      "loss: 0.410326  [ 2336/ 3200]\n",
      "loss: 0.521341  [ 2352/ 3200]\n",
      "loss: 0.353881  [ 2368/ 3200]\n",
      "loss: 0.523075  [ 2384/ 3200]\n",
      "loss: 1.103785  [ 2400/ 3200]\n",
      "loss: 1.004493  [ 2416/ 3200]\n",
      "loss: 0.523349  [ 2432/ 3200]\n",
      "loss: 0.517675  [ 2448/ 3200]\n",
      "loss: 0.312130  [ 2464/ 3200]\n",
      "loss: 0.631037  [ 2480/ 3200]\n",
      "loss: 0.247341  [ 2496/ 3200]\n",
      "loss: 0.545055  [ 2512/ 3200]\n",
      "loss: 0.351611  [ 2528/ 3200]\n",
      "loss: 0.338222  [ 2544/ 3200]\n",
      "loss: 0.648839  [ 2560/ 3200]\n",
      "loss: 0.498006  [ 2576/ 3200]\n",
      "loss: 0.523317  [ 2592/ 3200]\n",
      "loss: 0.265746  [ 2608/ 3200]\n",
      "loss: 0.644652  [ 2624/ 3200]\n",
      "loss: 0.890117  [ 2640/ 3200]\n",
      "loss: 0.287980  [ 2656/ 3200]\n",
      "loss: 0.474788  [ 2672/ 3200]\n",
      "loss: 0.809385  [ 2688/ 3200]\n",
      "loss: 0.481232  [ 2704/ 3200]\n",
      "loss: 0.464211  [ 2720/ 3200]\n",
      "loss: 0.610150  [ 2736/ 3200]\n",
      "loss: 0.612756  [ 2752/ 3200]\n",
      "loss: 0.516263  [ 2768/ 3200]\n",
      "loss: 0.198930  [ 2784/ 3200]\n",
      "loss: 0.461279  [ 2800/ 3200]\n",
      "loss: 0.284354  [ 2816/ 3200]\n",
      "loss: 0.457727  [ 2832/ 3200]\n",
      "loss: 0.283947  [ 2848/ 3200]\n",
      "loss: 0.490872  [ 2864/ 3200]\n",
      "loss: 0.309309  [ 2880/ 3200]\n",
      "loss: 0.876163  [ 2896/ 3200]\n",
      "loss: 0.372725  [ 2912/ 3200]\n",
      "loss: 0.253741  [ 2928/ 3200]\n",
      "loss: 0.898951  [ 2944/ 3200]\n",
      "loss: 0.967526  [ 2960/ 3200]\n",
      "loss: 0.226447  [ 2976/ 3200]\n",
      "loss: 0.560535  [ 2992/ 3200]\n",
      "loss: 0.250886  [ 3008/ 3200]\n",
      "loss: 0.474913  [ 3024/ 3200]\n",
      "loss: 0.238343  [ 3040/ 3200]\n",
      "loss: 0.260474  [ 3056/ 3200]\n",
      "loss: 0.463374  [ 3072/ 3200]\n",
      "loss: 0.670407  [ 3088/ 3200]\n",
      "loss: 0.421224  [ 3104/ 3200]\n",
      "loss: 0.260835  [ 3120/ 3200]\n",
      "loss: 0.359175  [ 3136/ 3200]\n",
      "loss: 0.359555  [ 3152/ 3200]\n",
      "loss: 0.720609  [ 3168/ 3200]\n",
      "loss: 1.134862  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034976\n",
      "f1 macro averaged score: 0.783150\n",
      "Accuracy               : 78.1%\n",
      "Confusion matrix       :\n",
      "tensor([[167,  18,   0,  15],\n",
      "        [ 25, 141,  17,  17],\n",
      "        [  0,  31, 158,  11],\n",
      "        [  1,  29,  11, 159]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0585e-02.\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 0.532803  [    0/ 3200]\n",
      "loss: 0.582431  [   16/ 3200]\n",
      "loss: 0.227800  [   32/ 3200]\n",
      "loss: 0.547146  [   48/ 3200]\n",
      "loss: 0.351205  [   64/ 3200]\n",
      "loss: 0.269890  [   80/ 3200]\n",
      "loss: 0.510926  [   96/ 3200]\n",
      "loss: 0.493846  [  112/ 3200]\n",
      "loss: 0.138087  [  128/ 3200]\n",
      "loss: 0.426517  [  144/ 3200]\n",
      "loss: 0.359500  [  160/ 3200]\n",
      "loss: 0.198233  [  176/ 3200]\n",
      "loss: 0.388232  [  192/ 3200]\n",
      "loss: 0.312709  [  208/ 3200]\n",
      "loss: 0.440953  [  224/ 3200]\n",
      "loss: 0.442684  [  240/ 3200]\n",
      "loss: 0.484128  [  256/ 3200]\n",
      "loss: 0.395691  [  272/ 3200]\n",
      "loss: 0.665691  [  288/ 3200]\n",
      "loss: 0.292442  [  304/ 3200]\n",
      "loss: 0.861790  [  320/ 3200]\n",
      "loss: 0.436632  [  336/ 3200]\n",
      "loss: 0.302949  [  352/ 3200]\n",
      "loss: 0.342148  [  368/ 3200]\n",
      "loss: 0.386404  [  384/ 3200]\n",
      "loss: 0.479463  [  400/ 3200]\n",
      "loss: 0.823804  [  416/ 3200]\n",
      "loss: 0.368959  [  432/ 3200]\n",
      "loss: 0.208415  [  448/ 3200]\n",
      "loss: 0.451573  [  464/ 3200]\n",
      "loss: 0.574308  [  480/ 3200]\n",
      "loss: 0.142657  [  496/ 3200]\n",
      "loss: 0.683151  [  512/ 3200]\n",
      "loss: 0.132703  [  528/ 3200]\n",
      "loss: 0.154270  [  544/ 3200]\n",
      "loss: 0.692236  [  560/ 3200]\n",
      "loss: 0.432817  [  576/ 3200]\n",
      "loss: 0.697224  [  592/ 3200]\n",
      "loss: 0.429256  [  608/ 3200]\n",
      "loss: 0.190419  [  624/ 3200]\n",
      "loss: 0.410853  [  640/ 3200]\n",
      "loss: 0.378768  [  656/ 3200]\n",
      "loss: 0.899697  [  672/ 3200]\n",
      "loss: 0.369304  [  688/ 3200]\n",
      "loss: 0.847717  [  704/ 3200]\n",
      "loss: 0.706482  [  720/ 3200]\n",
      "loss: 0.424959  [  736/ 3200]\n",
      "loss: 0.645676  [  752/ 3200]\n",
      "loss: 0.601382  [  768/ 3200]\n",
      "loss: 0.269382  [  784/ 3200]\n",
      "loss: 0.213942  [  800/ 3200]\n",
      "loss: 0.304661  [  816/ 3200]\n",
      "loss: 0.312121  [  832/ 3200]\n",
      "loss: 0.537702  [  848/ 3200]\n",
      "loss: 0.480571  [  864/ 3200]\n",
      "loss: 0.543905  [  880/ 3200]\n",
      "loss: 0.195448  [  896/ 3200]\n",
      "loss: 0.573847  [  912/ 3200]\n",
      "loss: 0.359807  [  928/ 3200]\n",
      "loss: 0.604161  [  944/ 3200]\n",
      "loss: 0.447835  [  960/ 3200]\n",
      "loss: 0.165588  [  976/ 3200]\n",
      "loss: 0.403216  [  992/ 3200]\n",
      "loss: 0.387991  [ 1008/ 3200]\n",
      "loss: 0.365876  [ 1024/ 3200]\n",
      "loss: 0.445741  [ 1040/ 3200]\n",
      "loss: 0.362332  [ 1056/ 3200]\n",
      "loss: 0.353011  [ 1072/ 3200]\n",
      "loss: 0.846944  [ 1088/ 3200]\n",
      "loss: 0.215816  [ 1104/ 3200]\n",
      "loss: 0.627293  [ 1120/ 3200]\n",
      "loss: 0.186840  [ 1136/ 3200]\n",
      "loss: 0.503125  [ 1152/ 3200]\n",
      "loss: 0.881320  [ 1168/ 3200]\n",
      "loss: 0.605023  [ 1184/ 3200]\n",
      "loss: 0.553063  [ 1200/ 3200]\n",
      "loss: 0.203565  [ 1216/ 3200]\n",
      "loss: 0.577106  [ 1232/ 3200]\n",
      "loss: 0.697180  [ 1248/ 3200]\n",
      "loss: 0.284614  [ 1264/ 3200]\n",
      "loss: 0.374508  [ 1280/ 3200]\n",
      "loss: 0.505998  [ 1296/ 3200]\n",
      "loss: 0.286953  [ 1312/ 3200]\n",
      "loss: 0.608533  [ 1328/ 3200]\n",
      "loss: 0.797616  [ 1344/ 3200]\n",
      "loss: 0.226405  [ 1360/ 3200]\n",
      "loss: 0.160847  [ 1376/ 3200]\n",
      "loss: 0.101450  [ 1392/ 3200]\n",
      "loss: 0.300794  [ 1408/ 3200]\n",
      "loss: 0.278307  [ 1424/ 3200]\n",
      "loss: 0.729779  [ 1440/ 3200]\n",
      "loss: 0.223451  [ 1456/ 3200]\n",
      "loss: 0.721293  [ 1472/ 3200]\n",
      "loss: 0.553142  [ 1488/ 3200]\n",
      "loss: 0.161346  [ 1504/ 3200]\n",
      "loss: 0.461362  [ 1520/ 3200]\n",
      "loss: 0.378627  [ 1536/ 3200]\n",
      "loss: 0.863477  [ 1552/ 3200]\n",
      "loss: 0.445195  [ 1568/ 3200]\n",
      "loss: 0.937650  [ 1584/ 3200]\n",
      "loss: 0.640963  [ 1600/ 3200]\n",
      "loss: 0.725868  [ 1616/ 3200]\n",
      "loss: 0.704635  [ 1632/ 3200]\n",
      "loss: 0.235995  [ 1648/ 3200]\n",
      "loss: 0.409010  [ 1664/ 3200]\n",
      "loss: 0.446554  [ 1680/ 3200]\n",
      "loss: 0.453609  [ 1696/ 3200]\n",
      "loss: 0.382312  [ 1712/ 3200]\n",
      "loss: 0.435729  [ 1728/ 3200]\n",
      "loss: 0.356292  [ 1744/ 3200]\n",
      "loss: 0.437898  [ 1760/ 3200]\n",
      "loss: 0.334639  [ 1776/ 3200]\n",
      "loss: 0.465114  [ 1792/ 3200]\n",
      "loss: 0.484962  [ 1808/ 3200]\n",
      "loss: 0.171643  [ 1824/ 3200]\n",
      "loss: 0.323021  [ 1840/ 3200]\n",
      "loss: 0.695892  [ 1856/ 3200]\n",
      "loss: 0.389937  [ 1872/ 3200]\n",
      "loss: 0.110633  [ 1888/ 3200]\n",
      "loss: 0.501312  [ 1904/ 3200]\n",
      "loss: 0.216200  [ 1920/ 3200]\n",
      "loss: 0.194729  [ 1936/ 3200]\n",
      "loss: 0.606024  [ 1952/ 3200]\n",
      "loss: 0.394969  [ 1968/ 3200]\n",
      "loss: 0.043450  [ 1984/ 3200]\n",
      "loss: 0.394181  [ 2000/ 3200]\n",
      "loss: 0.187136  [ 2016/ 3200]\n",
      "loss: 0.665620  [ 2032/ 3200]\n",
      "loss: 0.645290  [ 2048/ 3200]\n",
      "loss: 0.553768  [ 2064/ 3200]\n",
      "loss: 0.475759  [ 2080/ 3200]\n",
      "loss: 0.654619  [ 2096/ 3200]\n",
      "loss: 0.780941  [ 2112/ 3200]\n",
      "loss: 0.470552  [ 2128/ 3200]\n",
      "loss: 0.458503  [ 2144/ 3200]\n",
      "loss: 0.316842  [ 2160/ 3200]\n",
      "loss: 0.377248  [ 2176/ 3200]\n",
      "loss: 0.591798  [ 2192/ 3200]\n",
      "loss: 0.387248  [ 2208/ 3200]\n",
      "loss: 0.307704  [ 2224/ 3200]\n",
      "loss: 0.462276  [ 2240/ 3200]\n",
      "loss: 0.497911  [ 2256/ 3200]\n",
      "loss: 0.885582  [ 2272/ 3200]\n",
      "loss: 0.256120  [ 2288/ 3200]\n",
      "loss: 0.325997  [ 2304/ 3200]\n",
      "loss: 0.255832  [ 2320/ 3200]\n",
      "loss: 0.635477  [ 2336/ 3200]\n",
      "loss: 0.327755  [ 2352/ 3200]\n",
      "loss: 0.359734  [ 2368/ 3200]\n",
      "loss: 0.275677  [ 2384/ 3200]\n",
      "loss: 0.698871  [ 2400/ 3200]\n",
      "loss: 0.535972  [ 2416/ 3200]\n",
      "loss: 0.569370  [ 2432/ 3200]\n",
      "loss: 0.844113  [ 2448/ 3200]\n",
      "loss: 0.852395  [ 2464/ 3200]\n",
      "loss: 0.782776  [ 2480/ 3200]\n",
      "loss: 0.497688  [ 2496/ 3200]\n",
      "loss: 1.100501  [ 2512/ 3200]\n",
      "loss: 0.643097  [ 2528/ 3200]\n",
      "loss: 0.668997  [ 2544/ 3200]\n",
      "loss: 0.211042  [ 2560/ 3200]\n",
      "loss: 0.400131  [ 2576/ 3200]\n",
      "loss: 0.288819  [ 2592/ 3200]\n",
      "loss: 0.425347  [ 2608/ 3200]\n",
      "loss: 0.363685  [ 2624/ 3200]\n",
      "loss: 0.287409  [ 2640/ 3200]\n",
      "loss: 0.328999  [ 2656/ 3200]\n",
      "loss: 0.497011  [ 2672/ 3200]\n",
      "loss: 0.501966  [ 2688/ 3200]\n",
      "loss: 0.412059  [ 2704/ 3200]\n",
      "loss: 0.414802  [ 2720/ 3200]\n",
      "loss: 0.577542  [ 2736/ 3200]\n",
      "loss: 0.481117  [ 2752/ 3200]\n",
      "loss: 0.910248  [ 2768/ 3200]\n",
      "loss: 0.458293  [ 2784/ 3200]\n",
      "loss: 0.722843  [ 2800/ 3200]\n",
      "loss: 0.649622  [ 2816/ 3200]\n",
      "loss: 0.644862  [ 2832/ 3200]\n",
      "loss: 0.414861  [ 2848/ 3200]\n",
      "loss: 0.675802  [ 2864/ 3200]\n",
      "loss: 0.235162  [ 2880/ 3200]\n",
      "loss: 0.403330  [ 2896/ 3200]\n",
      "loss: 0.368712  [ 2912/ 3200]\n",
      "loss: 0.264583  [ 2928/ 3200]\n",
      "loss: 0.120305  [ 2944/ 3200]\n",
      "loss: 0.373158  [ 2960/ 3200]\n",
      "loss: 0.186470  [ 2976/ 3200]\n",
      "loss: 0.151929  [ 2992/ 3200]\n",
      "loss: 0.214614  [ 3008/ 3200]\n",
      "loss: 0.319897  [ 3024/ 3200]\n",
      "loss: 1.065979  [ 3040/ 3200]\n",
      "loss: 0.740222  [ 3056/ 3200]\n",
      "loss: 0.518995  [ 3072/ 3200]\n",
      "loss: 0.326248  [ 3088/ 3200]\n",
      "loss: 0.583310  [ 3104/ 3200]\n",
      "loss: 0.340259  [ 3120/ 3200]\n",
      "loss: 0.057017  [ 3136/ 3200]\n",
      "loss: 0.705044  [ 3152/ 3200]\n",
      "loss: 0.213967  [ 3168/ 3200]\n",
      "loss: 0.536711  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038646\n",
      "f1 macro averaged score: 0.743245\n",
      "Accuracy               : 75.9%\n",
      "Confusion matrix       :\n",
      "tensor([[179,   7,   0,  14],\n",
      "        [ 30,  81,  44,  45],\n",
      "        [  1,  11, 176,  12],\n",
      "        [  2,  13,  14, 171]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0630e-02.\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.499438  [    0/ 3200]\n",
      "loss: 0.794089  [   16/ 3200]\n",
      "loss: 0.320626  [   32/ 3200]\n",
      "loss: 0.359467  [   48/ 3200]\n",
      "loss: 0.198493  [   64/ 3200]\n",
      "loss: 0.470068  [   80/ 3200]\n",
      "loss: 0.335528  [   96/ 3200]\n",
      "loss: 0.743253  [  112/ 3200]\n",
      "loss: 0.361247  [  128/ 3200]\n",
      "loss: 0.191564  [  144/ 3200]\n",
      "loss: 0.665949  [  160/ 3200]\n",
      "loss: 0.306748  [  176/ 3200]\n",
      "loss: 0.814277  [  192/ 3200]\n",
      "loss: 0.849712  [  208/ 3200]\n",
      "loss: 0.234886  [  224/ 3200]\n",
      "loss: 0.454806  [  240/ 3200]\n",
      "loss: 0.395259  [  256/ 3200]\n",
      "loss: 0.301358  [  272/ 3200]\n",
      "loss: 0.263655  [  288/ 3200]\n",
      "loss: 0.260103  [  304/ 3200]\n",
      "loss: 0.362711  [  320/ 3200]\n",
      "loss: 0.609466  [  336/ 3200]\n",
      "loss: 0.302133  [  352/ 3200]\n",
      "loss: 0.418356  [  368/ 3200]\n",
      "loss: 0.233481  [  384/ 3200]\n",
      "loss: 0.321273  [  400/ 3200]\n",
      "loss: 0.262742  [  416/ 3200]\n",
      "loss: 0.203192  [  432/ 3200]\n",
      "loss: 0.638456  [  448/ 3200]\n",
      "loss: 0.327033  [  464/ 3200]\n",
      "loss: 0.752652  [  480/ 3200]\n",
      "loss: 0.392247  [  496/ 3200]\n",
      "loss: 0.738459  [  512/ 3200]\n",
      "loss: 0.538499  [  528/ 3200]\n",
      "loss: 0.427928  [  544/ 3200]\n",
      "loss: 0.389935  [  560/ 3200]\n",
      "loss: 0.601402  [  576/ 3200]\n",
      "loss: 0.436072  [  592/ 3200]\n",
      "loss: 0.654983  [  608/ 3200]\n",
      "loss: 0.377018  [  624/ 3200]\n",
      "loss: 0.239290  [  640/ 3200]\n",
      "loss: 0.158739  [  656/ 3200]\n",
      "loss: 0.537917  [  672/ 3200]\n",
      "loss: 0.435415  [  688/ 3200]\n",
      "loss: 0.588540  [  704/ 3200]\n",
      "loss: 0.535753  [  720/ 3200]\n",
      "loss: 0.118315  [  736/ 3200]\n",
      "loss: 0.385248  [  752/ 3200]\n",
      "loss: 0.228160  [  768/ 3200]\n",
      "loss: 0.239427  [  784/ 3200]\n",
      "loss: 0.696497  [  800/ 3200]\n",
      "loss: 0.644009  [  816/ 3200]\n",
      "loss: 0.163889  [  832/ 3200]\n",
      "loss: 0.570656  [  848/ 3200]\n",
      "loss: 0.167872  [  864/ 3200]\n",
      "loss: 0.479614  [  880/ 3200]\n",
      "loss: 0.590497  [  896/ 3200]\n",
      "loss: 0.351767  [  912/ 3200]\n",
      "loss: 0.509971  [  928/ 3200]\n",
      "loss: 0.136190  [  944/ 3200]\n",
      "loss: 0.456734  [  960/ 3200]\n",
      "loss: 0.189091  [  976/ 3200]\n",
      "loss: 0.112069  [  992/ 3200]\n",
      "loss: 0.752507  [ 1008/ 3200]\n",
      "loss: 0.321732  [ 1024/ 3200]\n",
      "loss: 0.274610  [ 1040/ 3200]\n",
      "loss: 0.464880  [ 1056/ 3200]\n",
      "loss: 0.667349  [ 1072/ 3200]\n",
      "loss: 0.403428  [ 1088/ 3200]\n",
      "loss: 0.347735  [ 1104/ 3200]\n",
      "loss: 0.582225  [ 1120/ 3200]\n",
      "loss: 0.610745  [ 1136/ 3200]\n",
      "loss: 0.618368  [ 1152/ 3200]\n",
      "loss: 0.278413  [ 1168/ 3200]\n",
      "loss: 0.404379  [ 1184/ 3200]\n",
      "loss: 0.222007  [ 1200/ 3200]\n",
      "loss: 0.312502  [ 1216/ 3200]\n",
      "loss: 0.291677  [ 1232/ 3200]\n",
      "loss: 0.231404  [ 1248/ 3200]\n",
      "loss: 0.393577  [ 1264/ 3200]\n",
      "loss: 0.303606  [ 1280/ 3200]\n",
      "loss: 0.295883  [ 1296/ 3200]\n",
      "loss: 0.311073  [ 1312/ 3200]\n",
      "loss: 0.402022  [ 1328/ 3200]\n",
      "loss: 0.488960  [ 1344/ 3200]\n",
      "loss: 0.536218  [ 1360/ 3200]\n",
      "loss: 0.227707  [ 1376/ 3200]\n",
      "loss: 0.248400  [ 1392/ 3200]\n",
      "loss: 0.481001  [ 1408/ 3200]\n",
      "loss: 0.412996  [ 1424/ 3200]\n",
      "loss: 0.318758  [ 1440/ 3200]\n",
      "loss: 0.259248  [ 1456/ 3200]\n",
      "loss: 0.505342  [ 1472/ 3200]\n",
      "loss: 0.439097  [ 1488/ 3200]\n",
      "loss: 0.872649  [ 1504/ 3200]\n",
      "loss: 0.118527  [ 1520/ 3200]\n",
      "loss: 0.481759  [ 1536/ 3200]\n",
      "loss: 0.175621  [ 1552/ 3200]\n",
      "loss: 0.743634  [ 1568/ 3200]\n",
      "loss: 0.327191  [ 1584/ 3200]\n",
      "loss: 0.298455  [ 1600/ 3200]\n",
      "loss: 0.317022  [ 1616/ 3200]\n",
      "loss: 0.298893  [ 1632/ 3200]\n",
      "loss: 0.395037  [ 1648/ 3200]\n",
      "loss: 0.498212  [ 1664/ 3200]\n",
      "loss: 0.780273  [ 1680/ 3200]\n",
      "loss: 0.389738  [ 1696/ 3200]\n",
      "loss: 0.299131  [ 1712/ 3200]\n",
      "loss: 0.387199  [ 1728/ 3200]\n",
      "loss: 0.819740  [ 1744/ 3200]\n",
      "loss: 0.408964  [ 1760/ 3200]\n",
      "loss: 0.355532  [ 1776/ 3200]\n",
      "loss: 0.812623  [ 1792/ 3200]\n",
      "loss: 0.556552  [ 1808/ 3200]\n",
      "loss: 0.267991  [ 1824/ 3200]\n",
      "loss: 0.445100  [ 1840/ 3200]\n",
      "loss: 0.765015  [ 1856/ 3200]\n",
      "loss: 0.541139  [ 1872/ 3200]\n",
      "loss: 0.565599  [ 1888/ 3200]\n",
      "loss: 0.922347  [ 1904/ 3200]\n",
      "loss: 0.546941  [ 1920/ 3200]\n",
      "loss: 0.665556  [ 1936/ 3200]\n",
      "loss: 0.418369  [ 1952/ 3200]\n",
      "loss: 0.263073  [ 1968/ 3200]\n",
      "loss: 0.495058  [ 1984/ 3200]\n",
      "loss: 0.320230  [ 2000/ 3200]\n",
      "loss: 0.487710  [ 2016/ 3200]\n",
      "loss: 0.285736  [ 2032/ 3200]\n",
      "loss: 0.307792  [ 2048/ 3200]\n",
      "loss: 0.204954  [ 2064/ 3200]\n",
      "loss: 0.374826  [ 2080/ 3200]\n",
      "loss: 0.501928  [ 2096/ 3200]\n",
      "loss: 0.511578  [ 2112/ 3200]\n",
      "loss: 0.307975  [ 2128/ 3200]\n",
      "loss: 0.497862  [ 2144/ 3200]\n",
      "loss: 0.436989  [ 2160/ 3200]\n",
      "loss: 0.576125  [ 2176/ 3200]\n",
      "loss: 0.200351  [ 2192/ 3200]\n",
      "loss: 0.362476  [ 2208/ 3200]\n",
      "loss: 0.507005  [ 2224/ 3200]\n",
      "loss: 0.618884  [ 2240/ 3200]\n",
      "loss: 0.676611  [ 2256/ 3200]\n",
      "loss: 0.517283  [ 2272/ 3200]\n",
      "loss: 0.381182  [ 2288/ 3200]\n",
      "loss: 0.714248  [ 2304/ 3200]\n",
      "loss: 0.376215  [ 2320/ 3200]\n",
      "loss: 0.706866  [ 2336/ 3200]\n",
      "loss: 0.496671  [ 2352/ 3200]\n",
      "loss: 0.161307  [ 2368/ 3200]\n",
      "loss: 0.303764  [ 2384/ 3200]\n",
      "loss: 0.498863  [ 2400/ 3200]\n",
      "loss: 0.507677  [ 2416/ 3200]\n",
      "loss: 0.616238  [ 2432/ 3200]\n",
      "loss: 0.260165  [ 2448/ 3200]\n",
      "loss: 0.509215  [ 2464/ 3200]\n",
      "loss: 0.932531  [ 2480/ 3200]\n",
      "loss: 0.108604  [ 2496/ 3200]\n",
      "loss: 0.243849  [ 2512/ 3200]\n",
      "loss: 0.566144  [ 2528/ 3200]\n",
      "loss: 0.507212  [ 2544/ 3200]\n",
      "loss: 0.397982  [ 2560/ 3200]\n",
      "loss: 0.312204  [ 2576/ 3200]\n",
      "loss: 0.391697  [ 2592/ 3200]\n",
      "loss: 0.342850  [ 2608/ 3200]\n",
      "loss: 0.568803  [ 2624/ 3200]\n",
      "loss: 0.579269  [ 2640/ 3200]\n",
      "loss: 0.457895  [ 2656/ 3200]\n",
      "loss: 0.660140  [ 2672/ 3200]\n",
      "loss: 0.373157  [ 2688/ 3200]\n",
      "loss: 0.220934  [ 2704/ 3200]\n",
      "loss: 0.423443  [ 2720/ 3200]\n",
      "loss: 0.359199  [ 2736/ 3200]\n",
      "loss: 0.263834  [ 2752/ 3200]\n",
      "loss: 0.361255  [ 2768/ 3200]\n",
      "loss: 0.646029  [ 2784/ 3200]\n",
      "loss: 0.419666  [ 2800/ 3200]\n",
      "loss: 0.586937  [ 2816/ 3200]\n",
      "loss: 0.293134  [ 2832/ 3200]\n",
      "loss: 0.647751  [ 2848/ 3200]\n",
      "loss: 0.547859  [ 2864/ 3200]\n",
      "loss: 0.324781  [ 2880/ 3200]\n",
      "loss: 0.276953  [ 2896/ 3200]\n",
      "loss: 0.109476  [ 2912/ 3200]\n",
      "loss: 0.571961  [ 2928/ 3200]\n",
      "loss: 0.490507  [ 2944/ 3200]\n",
      "loss: 0.338488  [ 2960/ 3200]\n",
      "loss: 0.633073  [ 2976/ 3200]\n",
      "loss: 0.340098  [ 2992/ 3200]\n",
      "loss: 0.738483  [ 3008/ 3200]\n",
      "loss: 0.475109  [ 3024/ 3200]\n",
      "loss: 0.212162  [ 3040/ 3200]\n",
      "loss: 0.461730  [ 3056/ 3200]\n",
      "loss: 0.768428  [ 3072/ 3200]\n",
      "loss: 0.457354  [ 3088/ 3200]\n",
      "loss: 0.916639  [ 3104/ 3200]\n",
      "loss: 0.368998  [ 3120/ 3200]\n",
      "loss: 0.319388  [ 3136/ 3200]\n",
      "loss: 0.453618  [ 3152/ 3200]\n",
      "loss: 0.492242  [ 3168/ 3200]\n",
      "loss: 0.281114  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.037829\n",
      "f1 macro averaged score: 0.756793\n",
      "Accuracy               : 75.8%\n",
      "Confusion matrix       :\n",
      "tensor([[165,  11,   0,  24],\n",
      "        [ 30, 115,  10,  45],\n",
      "        [  1,  24, 149,  26],\n",
      "        [  1,  18,   4, 177]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0675e-02.\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 0.327222  [    0/ 3200]\n",
      "loss: 0.362044  [   16/ 3200]\n",
      "loss: 0.384808  [   32/ 3200]\n",
      "loss: 0.445512  [   48/ 3200]\n",
      "loss: 0.243331  [   64/ 3200]\n",
      "loss: 0.584591  [   80/ 3200]\n",
      "loss: 0.696807  [   96/ 3200]\n",
      "loss: 0.513838  [  112/ 3200]\n",
      "loss: 0.345354  [  128/ 3200]\n",
      "loss: 0.306562  [  144/ 3200]\n",
      "loss: 0.209414  [  160/ 3200]\n",
      "loss: 0.211335  [  176/ 3200]\n",
      "loss: 0.270165  [  192/ 3200]\n",
      "loss: 0.409090  [  208/ 3200]\n",
      "loss: 0.476219  [  224/ 3200]\n",
      "loss: 0.171934  [  240/ 3200]\n",
      "loss: 0.517457  [  256/ 3200]\n",
      "loss: 0.620175  [  272/ 3200]\n",
      "loss: 0.346590  [  288/ 3200]\n",
      "loss: 0.512917  [  304/ 3200]\n",
      "loss: 0.215760  [  320/ 3200]\n",
      "loss: 0.317655  [  336/ 3200]\n",
      "loss: 0.800208  [  352/ 3200]\n",
      "loss: 0.470915  [  368/ 3200]\n",
      "loss: 0.154053  [  384/ 3200]\n",
      "loss: 0.442946  [  400/ 3200]\n",
      "loss: 0.980265  [  416/ 3200]\n",
      "loss: 0.385310  [  432/ 3200]\n",
      "loss: 0.376130  [  448/ 3200]\n",
      "loss: 0.728949  [  464/ 3200]\n",
      "loss: 1.305604  [  480/ 3200]\n",
      "loss: 0.581149  [  496/ 3200]\n",
      "loss: 0.255887  [  512/ 3200]\n",
      "loss: 0.289802  [  528/ 3200]\n",
      "loss: 0.713661  [  544/ 3200]\n",
      "loss: 0.704032  [  560/ 3200]\n",
      "loss: 0.555771  [  576/ 3200]\n",
      "loss: 0.408591  [  592/ 3200]\n",
      "loss: 0.391811  [  608/ 3200]\n",
      "loss: 0.600178  [  624/ 3200]\n",
      "loss: 0.066388  [  640/ 3200]\n",
      "loss: 0.456565  [  656/ 3200]\n",
      "loss: 0.285234  [  672/ 3200]\n",
      "loss: 0.317609  [  688/ 3200]\n",
      "loss: 0.353583  [  704/ 3200]\n",
      "loss: 0.416698  [  720/ 3200]\n",
      "loss: 0.184491  [  736/ 3200]\n",
      "loss: 0.524545  [  752/ 3200]\n",
      "loss: 0.228577  [  768/ 3200]\n",
      "loss: 0.658350  [  784/ 3200]\n",
      "loss: 0.114574  [  800/ 3200]\n",
      "loss: 0.833270  [  816/ 3200]\n",
      "loss: 0.563068  [  832/ 3200]\n",
      "loss: 0.461671  [  848/ 3200]\n",
      "loss: 0.144926  [  864/ 3200]\n",
      "loss: 0.329589  [  880/ 3200]\n",
      "loss: 0.452125  [  896/ 3200]\n",
      "loss: 0.513759  [  912/ 3200]\n",
      "loss: 0.518038  [  928/ 3200]\n",
      "loss: 0.387406  [  944/ 3200]\n",
      "loss: 0.421460  [  960/ 3200]\n",
      "loss: 0.265414  [  976/ 3200]\n",
      "loss: 0.241998  [  992/ 3200]\n",
      "loss: 0.408883  [ 1008/ 3200]\n",
      "loss: 0.591022  [ 1024/ 3200]\n",
      "loss: 0.380900  [ 1040/ 3200]\n",
      "loss: 0.609296  [ 1056/ 3200]\n",
      "loss: 0.398319  [ 1072/ 3200]\n",
      "loss: 0.439368  [ 1088/ 3200]\n",
      "loss: 0.271343  [ 1104/ 3200]\n",
      "loss: 0.515390  [ 1120/ 3200]\n",
      "loss: 0.683155  [ 1136/ 3200]\n",
      "loss: 0.774841  [ 1152/ 3200]\n",
      "loss: 0.607378  [ 1168/ 3200]\n",
      "loss: 0.415081  [ 1184/ 3200]\n",
      "loss: 0.617068  [ 1200/ 3200]\n",
      "loss: 0.422988  [ 1216/ 3200]\n",
      "loss: 0.347176  [ 1232/ 3200]\n",
      "loss: 0.338738  [ 1248/ 3200]\n",
      "loss: 0.273091  [ 1264/ 3200]\n",
      "loss: 0.229528  [ 1280/ 3200]\n",
      "loss: 0.177639  [ 1296/ 3200]\n",
      "loss: 0.316567  [ 1312/ 3200]\n",
      "loss: 0.605008  [ 1328/ 3200]\n",
      "loss: 0.220431  [ 1344/ 3200]\n",
      "loss: 1.080416  [ 1360/ 3200]\n",
      "loss: 0.429356  [ 1376/ 3200]\n",
      "loss: 0.461468  [ 1392/ 3200]\n",
      "loss: 0.363130  [ 1408/ 3200]\n",
      "loss: 0.232418  [ 1424/ 3200]\n",
      "loss: 0.342681  [ 1440/ 3200]\n",
      "loss: 0.571782  [ 1456/ 3200]\n",
      "loss: 0.468595  [ 1472/ 3200]\n",
      "loss: 0.801776  [ 1488/ 3200]\n",
      "loss: 0.425019  [ 1504/ 3200]\n",
      "loss: 0.485108  [ 1520/ 3200]\n",
      "loss: 0.345066  [ 1536/ 3200]\n",
      "loss: 0.322042  [ 1552/ 3200]\n",
      "loss: 0.291550  [ 1568/ 3200]\n",
      "loss: 0.265953  [ 1584/ 3200]\n",
      "loss: 0.317573  [ 1600/ 3200]\n",
      "loss: 0.456651  [ 1616/ 3200]\n",
      "loss: 0.546720  [ 1632/ 3200]\n",
      "loss: 0.427516  [ 1648/ 3200]\n",
      "loss: 0.490633  [ 1664/ 3200]\n",
      "loss: 0.306196  [ 1680/ 3200]\n",
      "loss: 0.341890  [ 1696/ 3200]\n",
      "loss: 0.369355  [ 1712/ 3200]\n",
      "loss: 0.652168  [ 1728/ 3200]\n",
      "loss: 0.607968  [ 1744/ 3200]\n",
      "loss: 0.193963  [ 1760/ 3200]\n",
      "loss: 0.527708  [ 1776/ 3200]\n",
      "loss: 0.310800  [ 1792/ 3200]\n",
      "loss: 0.346997  [ 1808/ 3200]\n",
      "loss: 0.294850  [ 1824/ 3200]\n",
      "loss: 0.217834  [ 1840/ 3200]\n",
      "loss: 0.393956  [ 1856/ 3200]\n",
      "loss: 0.302906  [ 1872/ 3200]\n",
      "loss: 0.139142  [ 1888/ 3200]\n",
      "loss: 0.460642  [ 1904/ 3200]\n",
      "loss: 0.744133  [ 1920/ 3200]\n",
      "loss: 0.439688  [ 1936/ 3200]\n",
      "loss: 0.563636  [ 1952/ 3200]\n",
      "loss: 0.259055  [ 1968/ 3200]\n",
      "loss: 0.495902  [ 1984/ 3200]\n",
      "loss: 0.453366  [ 2000/ 3200]\n",
      "loss: 0.271472  [ 2016/ 3200]\n",
      "loss: 0.226289  [ 2032/ 3200]\n",
      "loss: 0.362305  [ 2048/ 3200]\n",
      "loss: 0.631579  [ 2064/ 3200]\n",
      "loss: 0.294064  [ 2080/ 3200]\n",
      "loss: 0.585975  [ 2096/ 3200]\n",
      "loss: 0.430338  [ 2112/ 3200]\n",
      "loss: 0.457846  [ 2128/ 3200]\n",
      "loss: 0.297690  [ 2144/ 3200]\n",
      "loss: 0.298964  [ 2160/ 3200]\n",
      "loss: 0.239796  [ 2176/ 3200]\n",
      "loss: 0.419666  [ 2192/ 3200]\n",
      "loss: 0.326948  [ 2208/ 3200]\n",
      "loss: 0.502573  [ 2224/ 3200]\n",
      "loss: 0.273898  [ 2240/ 3200]\n",
      "loss: 0.220240  [ 2256/ 3200]\n",
      "loss: 0.727559  [ 2272/ 3200]\n",
      "loss: 0.761049  [ 2288/ 3200]\n",
      "loss: 0.492012  [ 2304/ 3200]\n",
      "loss: 0.325114  [ 2320/ 3200]\n",
      "loss: 0.467014  [ 2336/ 3200]\n",
      "loss: 0.365049  [ 2352/ 3200]\n",
      "loss: 0.403031  [ 2368/ 3200]\n",
      "loss: 0.562754  [ 2384/ 3200]\n",
      "loss: 0.450696  [ 2400/ 3200]\n",
      "loss: 0.458616  [ 2416/ 3200]\n",
      "loss: 0.495041  [ 2432/ 3200]\n",
      "loss: 0.512996  [ 2448/ 3200]\n",
      "loss: 0.156247  [ 2464/ 3200]\n",
      "loss: 0.186345  [ 2480/ 3200]\n",
      "loss: 0.077041  [ 2496/ 3200]\n",
      "loss: 0.624787  [ 2512/ 3200]\n",
      "loss: 0.232676  [ 2528/ 3200]\n",
      "loss: 0.391075  [ 2544/ 3200]\n",
      "loss: 0.430862  [ 2560/ 3200]\n",
      "loss: 0.425753  [ 2576/ 3200]\n",
      "loss: 0.358571  [ 2592/ 3200]\n",
      "loss: 0.134097  [ 2608/ 3200]\n",
      "loss: 0.236598  [ 2624/ 3200]\n",
      "loss: 0.168640  [ 2640/ 3200]\n",
      "loss: 0.686714  [ 2656/ 3200]\n",
      "loss: 0.598877  [ 2672/ 3200]\n",
      "loss: 0.747294  [ 2688/ 3200]\n",
      "loss: 0.884001  [ 2704/ 3200]\n",
      "loss: 0.578898  [ 2720/ 3200]\n",
      "loss: 0.828356  [ 2736/ 3200]\n",
      "loss: 0.948522  [ 2752/ 3200]\n",
      "loss: 0.178464  [ 2768/ 3200]\n",
      "loss: 0.423773  [ 2784/ 3200]\n",
      "loss: 0.332413  [ 2800/ 3200]\n",
      "loss: 0.262811  [ 2816/ 3200]\n",
      "loss: 0.653402  [ 2832/ 3200]\n",
      "loss: 0.654023  [ 2848/ 3200]\n",
      "loss: 0.341380  [ 2864/ 3200]\n",
      "loss: 0.361656  [ 2880/ 3200]\n",
      "loss: 0.208901  [ 2896/ 3200]\n",
      "loss: 0.494658  [ 2912/ 3200]\n",
      "loss: 0.394554  [ 2928/ 3200]\n",
      "loss: 0.258312  [ 2944/ 3200]\n",
      "loss: 0.465604  [ 2960/ 3200]\n",
      "loss: 0.377168  [ 2976/ 3200]\n",
      "loss: 0.387993  [ 2992/ 3200]\n",
      "loss: 0.480981  [ 3008/ 3200]\n",
      "loss: 0.131287  [ 3024/ 3200]\n",
      "loss: 0.397210  [ 3040/ 3200]\n",
      "loss: 0.291134  [ 3056/ 3200]\n",
      "loss: 0.459674  [ 3072/ 3200]\n",
      "loss: 0.358251  [ 3088/ 3200]\n",
      "loss: 0.610466  [ 3104/ 3200]\n",
      "loss: 0.573575  [ 3120/ 3200]\n",
      "loss: 0.682865  [ 3136/ 3200]\n",
      "loss: 0.425321  [ 3152/ 3200]\n",
      "loss: 0.478631  [ 3168/ 3200]\n",
      "loss: 0.430482  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035712\n",
      "f1 macro averaged score: 0.794549\n",
      "Accuracy               : 79.2%\n",
      "Confusion matrix       :\n",
      "tensor([[159,  21,   0,  20],\n",
      "        [ 15, 151,  19,  15],\n",
      "        [  0,  22, 167,  11],\n",
      "        [  1,  29,  13, 157]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0720e-02.\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 0.270786  [    0/ 3200]\n",
      "loss: 0.352266  [   16/ 3200]\n",
      "loss: 0.218035  [   32/ 3200]\n",
      "loss: 0.168891  [   48/ 3200]\n",
      "loss: 0.701546  [   64/ 3200]\n",
      "loss: 0.741553  [   80/ 3200]\n",
      "loss: 0.768537  [   96/ 3200]\n",
      "loss: 0.292265  [  112/ 3200]\n",
      "loss: 0.293191  [  128/ 3200]\n",
      "loss: 0.054503  [  144/ 3200]\n",
      "loss: 0.295847  [  160/ 3200]\n",
      "loss: 0.456991  [  176/ 3200]\n",
      "loss: 0.291770  [  192/ 3200]\n",
      "loss: 0.411891  [  208/ 3200]\n",
      "loss: 0.406535  [  224/ 3200]\n",
      "loss: 0.255221  [  240/ 3200]\n",
      "loss: 0.352621  [  256/ 3200]\n",
      "loss: 0.444178  [  272/ 3200]\n",
      "loss: 0.519368  [  288/ 3200]\n",
      "loss: 0.494906  [  304/ 3200]\n",
      "loss: 0.407905  [  320/ 3200]\n",
      "loss: 0.253318  [  336/ 3200]\n",
      "loss: 0.313473  [  352/ 3200]\n",
      "loss: 0.584319  [  368/ 3200]\n",
      "loss: 0.511856  [  384/ 3200]\n",
      "loss: 0.576121  [  400/ 3200]\n",
      "loss: 0.292674  [  416/ 3200]\n",
      "loss: 0.375494  [  432/ 3200]\n",
      "loss: 0.552743  [  448/ 3200]\n",
      "loss: 0.421600  [  464/ 3200]\n",
      "loss: 0.176313  [  480/ 3200]\n",
      "loss: 0.183759  [  496/ 3200]\n",
      "loss: 0.155012  [  512/ 3200]\n",
      "loss: 0.336066  [  528/ 3200]\n",
      "loss: 0.250230  [  544/ 3200]\n",
      "loss: 0.746695  [  560/ 3200]\n",
      "loss: 0.354918  [  576/ 3200]\n",
      "loss: 0.352692  [  592/ 3200]\n",
      "loss: 0.494201  [  608/ 3200]\n",
      "loss: 0.235488  [  624/ 3200]\n",
      "loss: 0.192097  [  640/ 3200]\n",
      "loss: 0.493993  [  656/ 3200]\n",
      "loss: 0.490330  [  672/ 3200]\n",
      "loss: 0.614637  [  688/ 3200]\n",
      "loss: 0.273111  [  704/ 3200]\n",
      "loss: 0.629802  [  720/ 3200]\n",
      "loss: 0.250436  [  736/ 3200]\n",
      "loss: 0.588935  [  752/ 3200]\n",
      "loss: 0.213116  [  768/ 3200]\n",
      "loss: 0.126327  [  784/ 3200]\n",
      "loss: 0.229753  [  800/ 3200]\n",
      "loss: 0.685140  [  816/ 3200]\n",
      "loss: 0.519041  [  832/ 3200]\n",
      "loss: 0.758934  [  848/ 3200]\n",
      "loss: 0.195524  [  864/ 3200]\n",
      "loss: 0.260422  [  880/ 3200]\n",
      "loss: 0.467388  [  896/ 3200]\n",
      "loss: 0.218705  [  912/ 3200]\n",
      "loss: 0.162850  [  928/ 3200]\n",
      "loss: 0.521734  [  944/ 3200]\n",
      "loss: 0.495057  [  960/ 3200]\n",
      "loss: 0.564265  [  976/ 3200]\n",
      "loss: 0.478953  [  992/ 3200]\n",
      "loss: 0.533340  [ 1008/ 3200]\n",
      "loss: 0.066419  [ 1024/ 3200]\n",
      "loss: 0.362356  [ 1040/ 3200]\n",
      "loss: 0.364911  [ 1056/ 3200]\n",
      "loss: 0.295084  [ 1072/ 3200]\n",
      "loss: 0.469035  [ 1088/ 3200]\n",
      "loss: 0.504848  [ 1104/ 3200]\n",
      "loss: 0.216598  [ 1120/ 3200]\n",
      "loss: 1.168100  [ 1136/ 3200]\n",
      "loss: 0.524110  [ 1152/ 3200]\n",
      "loss: 0.502011  [ 1168/ 3200]\n",
      "loss: 0.345055  [ 1184/ 3200]\n",
      "loss: 0.360667  [ 1200/ 3200]\n",
      "loss: 0.646910  [ 1216/ 3200]\n",
      "loss: 0.448053  [ 1232/ 3200]\n",
      "loss: 0.563513  [ 1248/ 3200]\n",
      "loss: 0.408753  [ 1264/ 3200]\n",
      "loss: 0.530680  [ 1280/ 3200]\n",
      "loss: 0.869366  [ 1296/ 3200]\n",
      "loss: 0.436487  [ 1312/ 3200]\n",
      "loss: 0.405408  [ 1328/ 3200]\n",
      "loss: 0.938359  [ 1344/ 3200]\n",
      "loss: 0.313813  [ 1360/ 3200]\n",
      "loss: 0.368525  [ 1376/ 3200]\n",
      "loss: 0.706338  [ 1392/ 3200]\n",
      "loss: 0.281534  [ 1408/ 3200]\n",
      "loss: 0.585203  [ 1424/ 3200]\n",
      "loss: 0.284291  [ 1440/ 3200]\n",
      "loss: 0.334231  [ 1456/ 3200]\n",
      "loss: 0.308185  [ 1472/ 3200]\n",
      "loss: 0.293285  [ 1488/ 3200]\n",
      "loss: 0.361110  [ 1504/ 3200]\n",
      "loss: 0.266957  [ 1520/ 3200]\n",
      "loss: 0.243664  [ 1536/ 3200]\n",
      "loss: 0.673757  [ 1552/ 3200]\n",
      "loss: 0.502481  [ 1568/ 3200]\n",
      "loss: 0.459546  [ 1584/ 3200]\n",
      "loss: 0.304846  [ 1600/ 3200]\n",
      "loss: 0.717065  [ 1616/ 3200]\n",
      "loss: 0.305116  [ 1632/ 3200]\n",
      "loss: 0.207775  [ 1648/ 3200]\n",
      "loss: 0.307877  [ 1664/ 3200]\n",
      "loss: 0.324202  [ 1680/ 3200]\n",
      "loss: 0.391073  [ 1696/ 3200]\n",
      "loss: 0.463165  [ 1712/ 3200]\n",
      "loss: 0.488983  [ 1728/ 3200]\n",
      "loss: 0.478799  [ 1744/ 3200]\n",
      "loss: 0.355138  [ 1760/ 3200]\n",
      "loss: 0.241907  [ 1776/ 3200]\n",
      "loss: 0.747968  [ 1792/ 3200]\n",
      "loss: 0.947552  [ 1808/ 3200]\n",
      "loss: 0.312758  [ 1824/ 3200]\n",
      "loss: 0.432180  [ 1840/ 3200]\n",
      "loss: 0.814358  [ 1856/ 3200]\n",
      "loss: 0.494971  [ 1872/ 3200]\n",
      "loss: 0.707234  [ 1888/ 3200]\n",
      "loss: 0.144789  [ 1904/ 3200]\n",
      "loss: 0.310611  [ 1920/ 3200]\n",
      "loss: 0.967381  [ 1936/ 3200]\n",
      "loss: 0.375018  [ 1952/ 3200]\n",
      "loss: 0.222207  [ 1968/ 3200]\n",
      "loss: 0.352842  [ 1984/ 3200]\n",
      "loss: 0.499012  [ 2000/ 3200]\n",
      "loss: 0.190477  [ 2016/ 3200]\n",
      "loss: 0.572552  [ 2032/ 3200]\n",
      "loss: 0.269400  [ 2048/ 3200]\n",
      "loss: 0.479173  [ 2064/ 3200]\n",
      "loss: 0.265477  [ 2080/ 3200]\n",
      "loss: 0.211164  [ 2096/ 3200]\n",
      "loss: 0.442101  [ 2112/ 3200]\n",
      "loss: 0.831623  [ 2128/ 3200]\n",
      "loss: 0.299564  [ 2144/ 3200]\n",
      "loss: 0.314370  [ 2160/ 3200]\n",
      "loss: 0.354800  [ 2176/ 3200]\n",
      "loss: 0.203154  [ 2192/ 3200]\n",
      "loss: 0.403433  [ 2208/ 3200]\n",
      "loss: 0.589398  [ 2224/ 3200]\n",
      "loss: 0.556556  [ 2240/ 3200]\n",
      "loss: 0.368775  [ 2256/ 3200]\n",
      "loss: 0.459826  [ 2272/ 3200]\n",
      "loss: 0.326433  [ 2288/ 3200]\n",
      "loss: 0.111787  [ 2304/ 3200]\n",
      "loss: 0.203150  [ 2320/ 3200]\n",
      "loss: 0.198328  [ 2336/ 3200]\n",
      "loss: 0.222502  [ 2352/ 3200]\n",
      "loss: 0.414861  [ 2368/ 3200]\n",
      "loss: 0.248797  [ 2384/ 3200]\n",
      "loss: 0.656124  [ 2400/ 3200]\n",
      "loss: 0.537513  [ 2416/ 3200]\n",
      "loss: 0.503297  [ 2432/ 3200]\n",
      "loss: 0.424739  [ 2448/ 3200]\n",
      "loss: 0.353685  [ 2464/ 3200]\n",
      "loss: 0.235330  [ 2480/ 3200]\n",
      "loss: 0.403008  [ 2496/ 3200]\n",
      "loss: 0.219027  [ 2512/ 3200]\n",
      "loss: 0.226382  [ 2528/ 3200]\n",
      "loss: 0.194369  [ 2544/ 3200]\n",
      "loss: 0.782894  [ 2560/ 3200]\n",
      "loss: 0.619131  [ 2576/ 3200]\n",
      "loss: 0.426201  [ 2592/ 3200]\n",
      "loss: 0.344079  [ 2608/ 3200]\n",
      "loss: 0.486493  [ 2624/ 3200]\n",
      "loss: 0.343677  [ 2640/ 3200]\n",
      "loss: 0.446517  [ 2656/ 3200]\n",
      "loss: 0.337515  [ 2672/ 3200]\n",
      "loss: 0.233668  [ 2688/ 3200]\n",
      "loss: 0.325086  [ 2704/ 3200]\n",
      "loss: 0.092745  [ 2720/ 3200]\n",
      "loss: 0.390231  [ 2736/ 3200]\n",
      "loss: 0.468640  [ 2752/ 3200]\n",
      "loss: 0.393777  [ 2768/ 3200]\n",
      "loss: 0.366215  [ 2784/ 3200]\n",
      "loss: 0.389945  [ 2800/ 3200]\n",
      "loss: 0.189260  [ 2816/ 3200]\n",
      "loss: 0.211647  [ 2832/ 3200]\n",
      "loss: 0.673469  [ 2848/ 3200]\n",
      "loss: 0.322948  [ 2864/ 3200]\n",
      "loss: 0.503712  [ 2880/ 3200]\n",
      "loss: 0.444460  [ 2896/ 3200]\n",
      "loss: 0.207988  [ 2912/ 3200]\n",
      "loss: 0.317268  [ 2928/ 3200]\n",
      "loss: 0.222131  [ 2944/ 3200]\n",
      "loss: 0.257996  [ 2960/ 3200]\n",
      "loss: 0.186913  [ 2976/ 3200]\n",
      "loss: 0.166983  [ 2992/ 3200]\n",
      "loss: 0.347477  [ 3008/ 3200]\n",
      "loss: 0.983878  [ 3024/ 3200]\n",
      "loss: 0.439722  [ 3040/ 3200]\n",
      "loss: 0.397059  [ 3056/ 3200]\n",
      "loss: 0.437720  [ 3072/ 3200]\n",
      "loss: 0.609616  [ 3088/ 3200]\n",
      "loss: 0.448760  [ 3104/ 3200]\n",
      "loss: 0.363859  [ 3120/ 3200]\n",
      "loss: 0.629572  [ 3136/ 3200]\n",
      "loss: 0.390296  [ 3152/ 3200]\n",
      "loss: 0.678280  [ 3168/ 3200]\n",
      "loss: 0.323633  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035566\n",
      "f1 macro averaged score: 0.788747\n",
      "Accuracy               : 79.2%\n",
      "Confusion matrix       :\n",
      "tensor([[176,  11,   0,  13],\n",
      "        [ 32, 120,  30,  18],\n",
      "        [  1,  17, 176,   6],\n",
      "        [  2,  21,  15, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0765e-02.\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 0.350781  [    0/ 3200]\n",
      "loss: 0.283590  [   16/ 3200]\n",
      "loss: 0.874537  [   32/ 3200]\n",
      "loss: 0.197504  [   48/ 3200]\n",
      "loss: 0.276176  [   64/ 3200]\n",
      "loss: 0.322572  [   80/ 3200]\n",
      "loss: 0.237764  [   96/ 3200]\n",
      "loss: 0.309031  [  112/ 3200]\n",
      "loss: 0.497591  [  128/ 3200]\n",
      "loss: 0.160267  [  144/ 3200]\n",
      "loss: 0.329124  [  160/ 3200]\n",
      "loss: 0.389021  [  176/ 3200]\n",
      "loss: 0.120693  [  192/ 3200]\n",
      "loss: 0.482664  [  208/ 3200]\n",
      "loss: 0.434373  [  224/ 3200]\n",
      "loss: 0.551704  [  240/ 3200]\n",
      "loss: 0.185724  [  256/ 3200]\n",
      "loss: 0.270065  [  272/ 3200]\n",
      "loss: 0.362810  [  288/ 3200]\n",
      "loss: 0.411223  [  304/ 3200]\n",
      "loss: 0.355532  [  320/ 3200]\n",
      "loss: 0.456449  [  336/ 3200]\n",
      "loss: 0.464644  [  352/ 3200]\n",
      "loss: 0.223328  [  368/ 3200]\n",
      "loss: 0.152321  [  384/ 3200]\n",
      "loss: 0.381890  [  400/ 3200]\n",
      "loss: 0.754943  [  416/ 3200]\n",
      "loss: 0.288813  [  432/ 3200]\n",
      "loss: 0.431532  [  448/ 3200]\n",
      "loss: 0.574564  [  464/ 3200]\n",
      "loss: 0.477771  [  480/ 3200]\n",
      "loss: 0.337749  [  496/ 3200]\n",
      "loss: 0.408202  [  512/ 3200]\n",
      "loss: 0.400999  [  528/ 3200]\n",
      "loss: 0.530462  [  544/ 3200]\n",
      "loss: 0.395551  [  560/ 3200]\n",
      "loss: 0.553316  [  576/ 3200]\n",
      "loss: 0.533299  [  592/ 3200]\n",
      "loss: 0.768794  [  608/ 3200]\n",
      "loss: 0.388778  [  624/ 3200]\n",
      "loss: 0.398709  [  640/ 3200]\n",
      "loss: 0.573993  [  656/ 3200]\n",
      "loss: 0.377672  [  672/ 3200]\n",
      "loss: 0.422877  [  688/ 3200]\n",
      "loss: 0.222838  [  704/ 3200]\n",
      "loss: 0.764018  [  720/ 3200]\n",
      "loss: 0.399165  [  736/ 3200]\n",
      "loss: 0.530633  [  752/ 3200]\n",
      "loss: 0.441195  [  768/ 3200]\n",
      "loss: 0.339686  [  784/ 3200]\n",
      "loss: 0.448608  [  800/ 3200]\n",
      "loss: 0.437280  [  816/ 3200]\n",
      "loss: 0.160383  [  832/ 3200]\n",
      "loss: 0.255047  [  848/ 3200]\n",
      "loss: 0.354196  [  864/ 3200]\n",
      "loss: 0.166357  [  880/ 3200]\n",
      "loss: 0.127297  [  896/ 3200]\n",
      "loss: 0.238229  [  912/ 3200]\n",
      "loss: 0.635308  [  928/ 3200]\n",
      "loss: 0.534150  [  944/ 3200]\n",
      "loss: 0.443832  [  960/ 3200]\n",
      "loss: 0.348950  [  976/ 3200]\n",
      "loss: 0.496470  [  992/ 3200]\n",
      "loss: 0.198312  [ 1008/ 3200]\n",
      "loss: 0.342602  [ 1024/ 3200]\n",
      "loss: 0.434201  [ 1040/ 3200]\n",
      "loss: 0.500689  [ 1056/ 3200]\n",
      "loss: 0.706187  [ 1072/ 3200]\n",
      "loss: 0.760501  [ 1088/ 3200]\n",
      "loss: 0.401552  [ 1104/ 3200]\n",
      "loss: 0.316104  [ 1120/ 3200]\n",
      "loss: 0.171284  [ 1136/ 3200]\n",
      "loss: 0.506430  [ 1152/ 3200]\n",
      "loss: 0.148635  [ 1168/ 3200]\n",
      "loss: 0.161860  [ 1184/ 3200]\n",
      "loss: 0.602481  [ 1200/ 3200]\n",
      "loss: 0.315864  [ 1216/ 3200]\n",
      "loss: 0.369486  [ 1232/ 3200]\n",
      "loss: 0.677890  [ 1248/ 3200]\n",
      "loss: 0.390853  [ 1264/ 3200]\n",
      "loss: 0.233688  [ 1280/ 3200]\n",
      "loss: 0.082791  [ 1296/ 3200]\n",
      "loss: 0.348895  [ 1312/ 3200]\n",
      "loss: 0.380619  [ 1328/ 3200]\n",
      "loss: 0.246215  [ 1344/ 3200]\n",
      "loss: 0.301790  [ 1360/ 3200]\n",
      "loss: 0.557187  [ 1376/ 3200]\n",
      "loss: 0.364294  [ 1392/ 3200]\n",
      "loss: 0.166964  [ 1408/ 3200]\n",
      "loss: 0.294629  [ 1424/ 3200]\n",
      "loss: 0.244893  [ 1440/ 3200]\n",
      "loss: 0.406645  [ 1456/ 3200]\n",
      "loss: 0.371595  [ 1472/ 3200]\n",
      "loss: 0.399017  [ 1488/ 3200]\n",
      "loss: 0.314196  [ 1504/ 3200]\n",
      "loss: 0.232276  [ 1520/ 3200]\n",
      "loss: 0.284240  [ 1536/ 3200]\n",
      "loss: 0.328517  [ 1552/ 3200]\n",
      "loss: 0.298530  [ 1568/ 3200]\n",
      "loss: 0.594595  [ 1584/ 3200]\n",
      "loss: 0.507543  [ 1600/ 3200]\n",
      "loss: 0.391832  [ 1616/ 3200]\n",
      "loss: 0.367089  [ 1632/ 3200]\n",
      "loss: 0.453236  [ 1648/ 3200]\n",
      "loss: 0.421178  [ 1664/ 3200]\n",
      "loss: 0.236204  [ 1680/ 3200]\n",
      "loss: 0.348674  [ 1696/ 3200]\n",
      "loss: 0.357966  [ 1712/ 3200]\n",
      "loss: 0.242134  [ 1728/ 3200]\n",
      "loss: 0.592210  [ 1744/ 3200]\n",
      "loss: 0.688385  [ 1760/ 3200]\n",
      "loss: 0.413514  [ 1776/ 3200]\n",
      "loss: 0.137656  [ 1792/ 3200]\n",
      "loss: 0.294336  [ 1808/ 3200]\n",
      "loss: 0.592750  [ 1824/ 3200]\n",
      "loss: 0.449337  [ 1840/ 3200]\n",
      "loss: 0.546071  [ 1856/ 3200]\n",
      "loss: 0.142043  [ 1872/ 3200]\n",
      "loss: 0.187141  [ 1888/ 3200]\n",
      "loss: 0.267749  [ 1904/ 3200]\n",
      "loss: 0.418837  [ 1920/ 3200]\n",
      "loss: 0.373649  [ 1936/ 3200]\n",
      "loss: 0.681292  [ 1952/ 3200]\n",
      "loss: 0.263073  [ 1968/ 3200]\n",
      "loss: 0.277679  [ 1984/ 3200]\n",
      "loss: 0.279503  [ 2000/ 3200]\n",
      "loss: 0.517144  [ 2016/ 3200]\n",
      "loss: 0.708242  [ 2032/ 3200]\n",
      "loss: 0.212213  [ 2048/ 3200]\n",
      "loss: 0.325943  [ 2064/ 3200]\n",
      "loss: 0.241355  [ 2080/ 3200]\n",
      "loss: 0.170458  [ 2096/ 3200]\n",
      "loss: 0.214473  [ 2112/ 3200]\n",
      "loss: 0.811441  [ 2128/ 3200]\n",
      "loss: 0.347886  [ 2144/ 3200]\n",
      "loss: 0.293612  [ 2160/ 3200]\n",
      "loss: 0.533479  [ 2176/ 3200]\n",
      "loss: 0.162600  [ 2192/ 3200]\n",
      "loss: 0.560372  [ 2208/ 3200]\n",
      "loss: 0.566653  [ 2224/ 3200]\n",
      "loss: 0.452885  [ 2240/ 3200]\n",
      "loss: 0.461883  [ 2256/ 3200]\n",
      "loss: 0.089414  [ 2272/ 3200]\n",
      "loss: 0.374833  [ 2288/ 3200]\n",
      "loss: 0.198880  [ 2304/ 3200]\n",
      "loss: 0.124432  [ 2320/ 3200]\n",
      "loss: 0.821942  [ 2336/ 3200]\n",
      "loss: 0.873134  [ 2352/ 3200]\n",
      "loss: 0.262132  [ 2368/ 3200]\n",
      "loss: 0.229947  [ 2384/ 3200]\n",
      "loss: 0.186183  [ 2400/ 3200]\n",
      "loss: 0.413475  [ 2416/ 3200]\n",
      "loss: 0.249284  [ 2432/ 3200]\n",
      "loss: 0.285325  [ 2448/ 3200]\n",
      "loss: 0.235377  [ 2464/ 3200]\n",
      "loss: 0.164729  [ 2480/ 3200]\n",
      "loss: 0.104376  [ 2496/ 3200]\n",
      "loss: 0.438204  [ 2512/ 3200]\n",
      "loss: 0.183266  [ 2528/ 3200]\n",
      "loss: 0.576173  [ 2544/ 3200]\n",
      "loss: 0.473205  [ 2560/ 3200]\n",
      "loss: 0.672663  [ 2576/ 3200]\n",
      "loss: 0.807454  [ 2592/ 3200]\n",
      "loss: 0.251299  [ 2608/ 3200]\n",
      "loss: 0.558419  [ 2624/ 3200]\n",
      "loss: 0.650588  [ 2640/ 3200]\n",
      "loss: 0.445388  [ 2656/ 3200]\n",
      "loss: 0.353792  [ 2672/ 3200]\n",
      "loss: 0.168648  [ 2688/ 3200]\n",
      "loss: 0.377599  [ 2704/ 3200]\n",
      "loss: 0.223310  [ 2720/ 3200]\n",
      "loss: 0.558232  [ 2736/ 3200]\n",
      "loss: 0.627349  [ 2752/ 3200]\n",
      "loss: 0.490253  [ 2768/ 3200]\n",
      "loss: 0.369932  [ 2784/ 3200]\n",
      "loss: 0.301924  [ 2800/ 3200]\n",
      "loss: 0.373995  [ 2816/ 3200]\n",
      "loss: 0.311952  [ 2832/ 3200]\n",
      "loss: 0.370107  [ 2848/ 3200]\n",
      "loss: 0.195121  [ 2864/ 3200]\n",
      "loss: 0.338335  [ 2880/ 3200]\n",
      "loss: 0.429873  [ 2896/ 3200]\n",
      "loss: 0.281027  [ 2912/ 3200]\n",
      "loss: 0.139003  [ 2928/ 3200]\n",
      "loss: 0.450484  [ 2944/ 3200]\n",
      "loss: 0.327957  [ 2960/ 3200]\n",
      "loss: 0.563355  [ 2976/ 3200]\n",
      "loss: 0.526848  [ 2992/ 3200]\n",
      "loss: 0.516730  [ 3008/ 3200]\n",
      "loss: 0.591136  [ 3024/ 3200]\n",
      "loss: 0.367368  [ 3040/ 3200]\n",
      "loss: 0.642338  [ 3056/ 3200]\n",
      "loss: 0.532333  [ 3072/ 3200]\n",
      "loss: 0.396035  [ 3088/ 3200]\n",
      "loss: 0.454354  [ 3104/ 3200]\n",
      "loss: 0.493381  [ 3120/ 3200]\n",
      "loss: 0.495991  [ 3136/ 3200]\n",
      "loss: 0.812983  [ 3152/ 3200]\n",
      "loss: 0.463436  [ 3168/ 3200]\n",
      "loss: 0.376047  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035595\n",
      "f1 macro averaged score: 0.785664\n",
      "Accuracy               : 79.0%\n",
      "Confusion matrix       :\n",
      "tensor([[181,  12,   0,   7],\n",
      "        [ 30, 118,  38,  14],\n",
      "        [  1,  15, 177,   7],\n",
      "        [  6,  20,  18, 156]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0810e-02.\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 0.360088  [    0/ 3200]\n",
      "loss: 0.203042  [   16/ 3200]\n",
      "loss: 0.261451  [   32/ 3200]\n",
      "loss: 0.344034  [   48/ 3200]\n",
      "loss: 0.105359  [   64/ 3200]\n",
      "loss: 0.504474  [   80/ 3200]\n",
      "loss: 0.683362  [   96/ 3200]\n",
      "loss: 0.162968  [  112/ 3200]\n",
      "loss: 0.491121  [  128/ 3200]\n",
      "loss: 0.304998  [  144/ 3200]\n",
      "loss: 0.351266  [  160/ 3200]\n",
      "loss: 0.238522  [  176/ 3200]\n",
      "loss: 0.348164  [  192/ 3200]\n",
      "loss: 0.372527  [  208/ 3200]\n",
      "loss: 0.561580  [  224/ 3200]\n",
      "loss: 0.368845  [  240/ 3200]\n",
      "loss: 0.215917  [  256/ 3200]\n",
      "loss: 0.284952  [  272/ 3200]\n",
      "loss: 0.637729  [  288/ 3200]\n",
      "loss: 0.320647  [  304/ 3200]\n",
      "loss: 0.231899  [  320/ 3200]\n",
      "loss: 0.292004  [  336/ 3200]\n",
      "loss: 0.394684  [  352/ 3200]\n",
      "loss: 0.242126  [  368/ 3200]\n",
      "loss: 0.524314  [  384/ 3200]\n",
      "loss: 0.273149  [  400/ 3200]\n",
      "loss: 0.368807  [  416/ 3200]\n",
      "loss: 0.200614  [  432/ 3200]\n",
      "loss: 0.274679  [  448/ 3200]\n",
      "loss: 0.618567  [  464/ 3200]\n",
      "loss: 0.681362  [  480/ 3200]\n",
      "loss: 0.557610  [  496/ 3200]\n",
      "loss: 0.115749  [  512/ 3200]\n",
      "loss: 0.236158  [  528/ 3200]\n",
      "loss: 0.317086  [  544/ 3200]\n",
      "loss: 0.363010  [  560/ 3200]\n",
      "loss: 0.314532  [  576/ 3200]\n",
      "loss: 0.161129  [  592/ 3200]\n",
      "loss: 0.246090  [  608/ 3200]\n",
      "loss: 0.201419  [  624/ 3200]\n",
      "loss: 0.326216  [  640/ 3200]\n",
      "loss: 0.324460  [  656/ 3200]\n",
      "loss: 0.263970  [  672/ 3200]\n",
      "loss: 0.425061  [  688/ 3200]\n",
      "loss: 0.198762  [  704/ 3200]\n",
      "loss: 0.395729  [  720/ 3200]\n",
      "loss: 0.242080  [  736/ 3200]\n",
      "loss: 0.495530  [  752/ 3200]\n",
      "loss: 0.086729  [  768/ 3200]\n",
      "loss: 1.002973  [  784/ 3200]\n",
      "loss: 0.855945  [  800/ 3200]\n",
      "loss: 0.426975  [  816/ 3200]\n",
      "loss: 0.235028  [  832/ 3200]\n",
      "loss: 0.508750  [  848/ 3200]\n",
      "loss: 0.447486  [  864/ 3200]\n",
      "loss: 0.371705  [  880/ 3200]\n",
      "loss: 0.506953  [  896/ 3200]\n",
      "loss: 0.358684  [  912/ 3200]\n",
      "loss: 0.493054  [  928/ 3200]\n",
      "loss: 0.212370  [  944/ 3200]\n",
      "loss: 0.378386  [  960/ 3200]\n",
      "loss: 0.325160  [  976/ 3200]\n",
      "loss: 0.273962  [  992/ 3200]\n",
      "loss: 0.184525  [ 1008/ 3200]\n",
      "loss: 0.258246  [ 1024/ 3200]\n",
      "loss: 0.251651  [ 1040/ 3200]\n",
      "loss: 0.252956  [ 1056/ 3200]\n",
      "loss: 0.301948  [ 1072/ 3200]\n",
      "loss: 0.321184  [ 1088/ 3200]\n",
      "loss: 0.191645  [ 1104/ 3200]\n",
      "loss: 0.992596  [ 1120/ 3200]\n",
      "loss: 0.274593  [ 1136/ 3200]\n",
      "loss: 0.382739  [ 1152/ 3200]\n",
      "loss: 0.199752  [ 1168/ 3200]\n",
      "loss: 0.133351  [ 1184/ 3200]\n",
      "loss: 0.786062  [ 1200/ 3200]\n",
      "loss: 0.469665  [ 1216/ 3200]\n",
      "loss: 0.630235  [ 1232/ 3200]\n",
      "loss: 0.325003  [ 1248/ 3200]\n",
      "loss: 0.563038  [ 1264/ 3200]\n",
      "loss: 0.205540  [ 1280/ 3200]\n",
      "loss: 0.361119  [ 1296/ 3200]\n",
      "loss: 0.654123  [ 1312/ 3200]\n",
      "loss: 0.570459  [ 1328/ 3200]\n",
      "loss: 0.636133  [ 1344/ 3200]\n",
      "loss: 0.563442  [ 1360/ 3200]\n",
      "loss: 0.329213  [ 1376/ 3200]\n",
      "loss: 0.228596  [ 1392/ 3200]\n",
      "loss: 0.213383  [ 1408/ 3200]\n",
      "loss: 0.639844  [ 1424/ 3200]\n",
      "loss: 0.676479  [ 1440/ 3200]\n",
      "loss: 0.490685  [ 1456/ 3200]\n",
      "loss: 0.862086  [ 1472/ 3200]\n",
      "loss: 0.317022  [ 1488/ 3200]\n",
      "loss: 0.225787  [ 1504/ 3200]\n",
      "loss: 0.219780  [ 1520/ 3200]\n",
      "loss: 0.577770  [ 1536/ 3200]\n",
      "loss: 0.314517  [ 1552/ 3200]\n",
      "loss: 0.296443  [ 1568/ 3200]\n",
      "loss: 0.480740  [ 1584/ 3200]\n",
      "loss: 0.205172  [ 1600/ 3200]\n",
      "loss: 0.314255  [ 1616/ 3200]\n",
      "loss: 0.204203  [ 1632/ 3200]\n",
      "loss: 0.450058  [ 1648/ 3200]\n",
      "loss: 0.473755  [ 1664/ 3200]\n",
      "loss: 0.328230  [ 1680/ 3200]\n",
      "loss: 0.313361  [ 1696/ 3200]\n",
      "loss: 0.200722  [ 1712/ 3200]\n",
      "loss: 0.244819  [ 1728/ 3200]\n",
      "loss: 0.216384  [ 1744/ 3200]\n",
      "loss: 0.485924  [ 1760/ 3200]\n",
      "loss: 0.216585  [ 1776/ 3200]\n",
      "loss: 0.221361  [ 1792/ 3200]\n",
      "loss: 0.249885  [ 1808/ 3200]\n",
      "loss: 0.837772  [ 1824/ 3200]\n",
      "loss: 0.319639  [ 1840/ 3200]\n",
      "loss: 0.213933  [ 1856/ 3200]\n",
      "loss: 0.320934  [ 1872/ 3200]\n",
      "loss: 0.215568  [ 1888/ 3200]\n",
      "loss: 0.084106  [ 1904/ 3200]\n",
      "loss: 0.284655  [ 1920/ 3200]\n",
      "loss: 0.445642  [ 1936/ 3200]\n",
      "loss: 0.874515  [ 1952/ 3200]\n",
      "loss: 0.145973  [ 1968/ 3200]\n",
      "loss: 0.271378  [ 1984/ 3200]\n",
      "loss: 0.244447  [ 2000/ 3200]\n",
      "loss: 0.928254  [ 2016/ 3200]\n",
      "loss: 0.437090  [ 2032/ 3200]\n",
      "loss: 0.744127  [ 2048/ 3200]\n",
      "loss: 0.597000  [ 2064/ 3200]\n",
      "loss: 0.207263  [ 2080/ 3200]\n",
      "loss: 0.342797  [ 2096/ 3200]\n",
      "loss: 0.289280  [ 2112/ 3200]\n",
      "loss: 0.375943  [ 2128/ 3200]\n",
      "loss: 0.743746  [ 2144/ 3200]\n",
      "loss: 0.304027  [ 2160/ 3200]\n",
      "loss: 0.373349  [ 2176/ 3200]\n",
      "loss: 0.176981  [ 2192/ 3200]\n",
      "loss: 0.109834  [ 2208/ 3200]\n",
      "loss: 0.301630  [ 2224/ 3200]\n",
      "loss: 0.119123  [ 2240/ 3200]\n",
      "loss: 0.265608  [ 2256/ 3200]\n",
      "loss: 0.594187  [ 2272/ 3200]\n",
      "loss: 0.283011  [ 2288/ 3200]\n",
      "loss: 0.693438  [ 2304/ 3200]\n",
      "loss: 0.236954  [ 2320/ 3200]\n",
      "loss: 0.638383  [ 2336/ 3200]\n",
      "loss: 0.487802  [ 2352/ 3200]\n",
      "loss: 0.281397  [ 2368/ 3200]\n",
      "loss: 0.696415  [ 2384/ 3200]\n",
      "loss: 0.492768  [ 2400/ 3200]\n",
      "loss: 0.993384  [ 2416/ 3200]\n",
      "loss: 0.260281  [ 2432/ 3200]\n",
      "loss: 0.511074  [ 2448/ 3200]\n",
      "loss: 0.505666  [ 2464/ 3200]\n",
      "loss: 0.540991  [ 2480/ 3200]\n",
      "loss: 0.540483  [ 2496/ 3200]\n",
      "loss: 0.228800  [ 2512/ 3200]\n",
      "loss: 0.181664  [ 2528/ 3200]\n",
      "loss: 0.383789  [ 2544/ 3200]\n",
      "loss: 0.433312  [ 2560/ 3200]\n",
      "loss: 0.528371  [ 2576/ 3200]\n",
      "loss: 0.261676  [ 2592/ 3200]\n",
      "loss: 0.550333  [ 2608/ 3200]\n",
      "loss: 0.455890  [ 2624/ 3200]\n",
      "loss: 0.591982  [ 2640/ 3200]\n",
      "loss: 0.376500  [ 2656/ 3200]\n",
      "loss: 0.210550  [ 2672/ 3200]\n",
      "loss: 0.152253  [ 2688/ 3200]\n",
      "loss: 0.187718  [ 2704/ 3200]\n",
      "loss: 0.309085  [ 2720/ 3200]\n",
      "loss: 0.349173  [ 2736/ 3200]\n",
      "loss: 0.272799  [ 2752/ 3200]\n",
      "loss: 0.208311  [ 2768/ 3200]\n",
      "loss: 0.554367  [ 2784/ 3200]\n",
      "loss: 0.275188  [ 2800/ 3200]\n",
      "loss: 0.127823  [ 2816/ 3200]\n",
      "loss: 0.625463  [ 2832/ 3200]\n",
      "loss: 0.217106  [ 2848/ 3200]\n",
      "loss: 0.181203  [ 2864/ 3200]\n",
      "loss: 0.111813  [ 2880/ 3200]\n",
      "loss: 0.191393  [ 2896/ 3200]\n",
      "loss: 0.419999  [ 2912/ 3200]\n",
      "loss: 0.583131  [ 2928/ 3200]\n",
      "loss: 0.249333  [ 2944/ 3200]\n",
      "loss: 0.269788  [ 2960/ 3200]\n",
      "loss: 0.288393  [ 2976/ 3200]\n",
      "loss: 0.297218  [ 2992/ 3200]\n",
      "loss: 0.201090  [ 3008/ 3200]\n",
      "loss: 0.308768  [ 3024/ 3200]\n",
      "loss: 0.196077  [ 3040/ 3200]\n",
      "loss: 0.301013  [ 3056/ 3200]\n",
      "loss: 0.342574  [ 3072/ 3200]\n",
      "loss: 0.424908  [ 3088/ 3200]\n",
      "loss: 0.244509  [ 3104/ 3200]\n",
      "loss: 0.598705  [ 3120/ 3200]\n",
      "loss: 0.376355  [ 3136/ 3200]\n",
      "loss: 0.122100  [ 3152/ 3200]\n",
      "loss: 0.662334  [ 3168/ 3200]\n",
      "loss: 0.205578  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036644\n",
      "f1 macro averaged score: 0.798846\n",
      "Accuracy               : 80.4%\n",
      "Confusion matrix       :\n",
      "tensor([[183,  11,   0,   6],\n",
      "        [ 30, 118,  23,  29],\n",
      "        [  1,  14, 177,   8],\n",
      "        [  2,  18,  15, 165]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0855e-02.\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 0.315969  [    0/ 3200]\n",
      "loss: 0.321314  [   16/ 3200]\n",
      "loss: 0.286272  [   32/ 3200]\n",
      "loss: 0.092324  [   48/ 3200]\n",
      "loss: 0.226099  [   64/ 3200]\n",
      "loss: 0.716772  [   80/ 3200]\n",
      "loss: 0.606047  [   96/ 3200]\n",
      "loss: 0.433557  [  112/ 3200]\n",
      "loss: 0.280523  [  128/ 3200]\n",
      "loss: 0.531529  [  144/ 3200]\n",
      "loss: 0.529367  [  160/ 3200]\n",
      "loss: 0.469875  [  176/ 3200]\n",
      "loss: 0.336875  [  192/ 3200]\n",
      "loss: 0.420734  [  208/ 3200]\n",
      "loss: 0.328647  [  224/ 3200]\n",
      "loss: 0.675315  [  240/ 3200]\n",
      "loss: 0.424905  [  256/ 3200]\n",
      "loss: 0.846142  [  272/ 3200]\n",
      "loss: 0.622477  [  288/ 3200]\n",
      "loss: 0.426226  [  304/ 3200]\n",
      "loss: 0.597886  [  320/ 3200]\n",
      "loss: 0.347824  [  336/ 3200]\n",
      "loss: 0.479996  [  352/ 3200]\n",
      "loss: 0.309316  [  368/ 3200]\n",
      "loss: 0.600856  [  384/ 3200]\n",
      "loss: 0.326078  [  400/ 3200]\n",
      "loss: 0.427463  [  416/ 3200]\n",
      "loss: 0.614613  [  432/ 3200]\n",
      "loss: 0.386219  [  448/ 3200]\n",
      "loss: 0.230318  [  464/ 3200]\n",
      "loss: 0.527873  [  480/ 3200]\n",
      "loss: 0.684185  [  496/ 3200]\n",
      "loss: 0.469573  [  512/ 3200]\n",
      "loss: 0.355213  [  528/ 3200]\n",
      "loss: 0.250611  [  544/ 3200]\n",
      "loss: 0.438389  [  560/ 3200]\n",
      "loss: 0.269370  [  576/ 3200]\n",
      "loss: 0.343107  [  592/ 3200]\n",
      "loss: 0.264733  [  608/ 3200]\n",
      "loss: 0.225593  [  624/ 3200]\n",
      "loss: 0.373019  [  640/ 3200]\n",
      "loss: 0.305566  [  656/ 3200]\n",
      "loss: 0.362305  [  672/ 3200]\n",
      "loss: 0.461964  [  688/ 3200]\n",
      "loss: 0.272661  [  704/ 3200]\n",
      "loss: 0.383221  [  720/ 3200]\n",
      "loss: 0.410739  [  736/ 3200]\n",
      "loss: 0.487050  [  752/ 3200]\n",
      "loss: 0.196313  [  768/ 3200]\n",
      "loss: 0.314018  [  784/ 3200]\n",
      "loss: 0.330175  [  800/ 3200]\n",
      "loss: 0.273555  [  816/ 3200]\n",
      "loss: 0.339177  [  832/ 3200]\n",
      "loss: 0.267540  [  848/ 3200]\n",
      "loss: 0.348627  [  864/ 3200]\n",
      "loss: 0.429710  [  880/ 3200]\n",
      "loss: 0.252402  [  896/ 3200]\n",
      "loss: 0.234620  [  912/ 3200]\n",
      "loss: 0.210344  [  928/ 3200]\n",
      "loss: 0.167784  [  944/ 3200]\n",
      "loss: 0.270473  [  960/ 3200]\n",
      "loss: 0.131123  [  976/ 3200]\n",
      "loss: 0.381343  [  992/ 3200]\n",
      "loss: 0.641692  [ 1008/ 3200]\n",
      "loss: 0.238875  [ 1024/ 3200]\n",
      "loss: 0.264592  [ 1040/ 3200]\n",
      "loss: 0.491915  [ 1056/ 3200]\n",
      "loss: 0.173513  [ 1072/ 3200]\n",
      "loss: 0.235278  [ 1088/ 3200]\n",
      "loss: 0.343250  [ 1104/ 3200]\n",
      "loss: 0.235266  [ 1120/ 3200]\n",
      "loss: 0.386000  [ 1136/ 3200]\n",
      "loss: 0.553669  [ 1152/ 3200]\n",
      "loss: 0.416753  [ 1168/ 3200]\n",
      "loss: 0.573732  [ 1184/ 3200]\n",
      "loss: 0.367266  [ 1200/ 3200]\n",
      "loss: 0.315985  [ 1216/ 3200]\n",
      "loss: 0.227395  [ 1232/ 3200]\n",
      "loss: 0.350260  [ 1248/ 3200]\n",
      "loss: 0.397345  [ 1264/ 3200]\n",
      "loss: 0.154752  [ 1280/ 3200]\n",
      "loss: 0.354456  [ 1296/ 3200]\n",
      "loss: 0.182610  [ 1312/ 3200]\n",
      "loss: 0.462715  [ 1328/ 3200]\n",
      "loss: 0.444796  [ 1344/ 3200]\n",
      "loss: 0.290897  [ 1360/ 3200]\n",
      "loss: 0.354173  [ 1376/ 3200]\n",
      "loss: 0.419653  [ 1392/ 3200]\n",
      "loss: 0.513704  [ 1408/ 3200]\n",
      "loss: 0.525378  [ 1424/ 3200]\n",
      "loss: 0.338633  [ 1440/ 3200]\n",
      "loss: 0.651965  [ 1456/ 3200]\n",
      "loss: 0.350138  [ 1472/ 3200]\n",
      "loss: 0.457149  [ 1488/ 3200]\n",
      "loss: 0.337645  [ 1504/ 3200]\n",
      "loss: 0.618102  [ 1520/ 3200]\n",
      "loss: 0.463650  [ 1536/ 3200]\n",
      "loss: 0.137768  [ 1552/ 3200]\n",
      "loss: 0.542230  [ 1568/ 3200]\n",
      "loss: 0.262789  [ 1584/ 3200]\n",
      "loss: 0.531689  [ 1600/ 3200]\n",
      "loss: 0.395931  [ 1616/ 3200]\n",
      "loss: 0.353196  [ 1632/ 3200]\n",
      "loss: 0.227028  [ 1648/ 3200]\n",
      "loss: 0.138251  [ 1664/ 3200]\n",
      "loss: 0.232680  [ 1680/ 3200]\n",
      "loss: 0.362363  [ 1696/ 3200]\n",
      "loss: 0.228587  [ 1712/ 3200]\n",
      "loss: 0.252565  [ 1728/ 3200]\n",
      "loss: 0.115244  [ 1744/ 3200]\n",
      "loss: 0.111673  [ 1760/ 3200]\n",
      "loss: 0.251087  [ 1776/ 3200]\n",
      "loss: 0.229899  [ 1792/ 3200]\n",
      "loss: 0.315616  [ 1808/ 3200]\n",
      "loss: 0.669684  [ 1824/ 3200]\n",
      "loss: 0.112961  [ 1840/ 3200]\n",
      "loss: 0.397864  [ 1856/ 3200]\n",
      "loss: 0.234374  [ 1872/ 3200]\n",
      "loss: 0.078548  [ 1888/ 3200]\n",
      "loss: 0.318710  [ 1904/ 3200]\n",
      "loss: 0.209535  [ 1920/ 3200]\n",
      "loss: 0.264523  [ 1936/ 3200]\n",
      "loss: 0.989391  [ 1952/ 3200]\n",
      "loss: 0.669507  [ 1968/ 3200]\n",
      "loss: 0.364871  [ 1984/ 3200]\n",
      "loss: 0.357973  [ 2000/ 3200]\n",
      "loss: 0.266113  [ 2016/ 3200]\n",
      "loss: 0.389795  [ 2032/ 3200]\n",
      "loss: 0.562594  [ 2048/ 3200]\n",
      "loss: 0.191806  [ 2064/ 3200]\n",
      "loss: 0.117516  [ 2080/ 3200]\n",
      "loss: 0.450151  [ 2096/ 3200]\n",
      "loss: 0.343503  [ 2112/ 3200]\n",
      "loss: 0.341105  [ 2128/ 3200]\n",
      "loss: 0.352356  [ 2144/ 3200]\n",
      "loss: 0.374373  [ 2160/ 3200]\n",
      "loss: 0.122905  [ 2176/ 3200]\n",
      "loss: 0.606321  [ 2192/ 3200]\n",
      "loss: 0.320022  [ 2208/ 3200]\n",
      "loss: 0.369300  [ 2224/ 3200]\n",
      "loss: 0.385026  [ 2240/ 3200]\n",
      "loss: 0.199598  [ 2256/ 3200]\n",
      "loss: 0.750516  [ 2272/ 3200]\n",
      "loss: 0.639797  [ 2288/ 3200]\n",
      "loss: 0.239497  [ 2304/ 3200]\n",
      "loss: 0.130247  [ 2320/ 3200]\n",
      "loss: 0.434788  [ 2336/ 3200]\n",
      "loss: 0.175807  [ 2352/ 3200]\n",
      "loss: 0.754219  [ 2368/ 3200]\n",
      "loss: 0.369256  [ 2384/ 3200]\n",
      "loss: 0.845066  [ 2400/ 3200]\n",
      "loss: 0.805324  [ 2416/ 3200]\n",
      "loss: 0.509564  [ 2432/ 3200]\n",
      "loss: 0.301452  [ 2448/ 3200]\n",
      "loss: 0.506439  [ 2464/ 3200]\n",
      "loss: 0.090356  [ 2480/ 3200]\n",
      "loss: 0.387522  [ 2496/ 3200]\n",
      "loss: 0.327122  [ 2512/ 3200]\n",
      "loss: 0.395789  [ 2528/ 3200]\n",
      "loss: 0.278908  [ 2544/ 3200]\n",
      "loss: 0.381069  [ 2560/ 3200]\n",
      "loss: 0.268592  [ 2576/ 3200]\n",
      "loss: 0.271364  [ 2592/ 3200]\n",
      "loss: 0.159537  [ 2608/ 3200]\n",
      "loss: 0.237249  [ 2624/ 3200]\n",
      "loss: 0.343223  [ 2640/ 3200]\n",
      "loss: 0.527636  [ 2656/ 3200]\n",
      "loss: 0.453203  [ 2672/ 3200]\n",
      "loss: 0.126800  [ 2688/ 3200]\n",
      "loss: 0.110750  [ 2704/ 3200]\n",
      "loss: 0.140882  [ 2720/ 3200]\n",
      "loss: 0.479840  [ 2736/ 3200]\n",
      "loss: 0.110674  [ 2752/ 3200]\n",
      "loss: 0.870338  [ 2768/ 3200]\n",
      "loss: 0.486411  [ 2784/ 3200]\n",
      "loss: 0.198449  [ 2800/ 3200]\n",
      "loss: 0.537110  [ 2816/ 3200]\n",
      "loss: 0.387779  [ 2832/ 3200]\n",
      "loss: 0.141606  [ 2848/ 3200]\n",
      "loss: 0.373881  [ 2864/ 3200]\n",
      "loss: 0.292944  [ 2880/ 3200]\n",
      "loss: 0.291375  [ 2896/ 3200]\n",
      "loss: 0.437031  [ 2912/ 3200]\n",
      "loss: 0.298832  [ 2928/ 3200]\n",
      "loss: 0.121756  [ 2944/ 3200]\n",
      "loss: 0.097793  [ 2960/ 3200]\n",
      "loss: 0.448163  [ 2976/ 3200]\n",
      "loss: 0.258718  [ 2992/ 3200]\n",
      "loss: 0.200136  [ 3008/ 3200]\n",
      "loss: 0.111369  [ 3024/ 3200]\n",
      "loss: 0.143141  [ 3040/ 3200]\n",
      "loss: 0.655604  [ 3056/ 3200]\n",
      "loss: 0.604699  [ 3072/ 3200]\n",
      "loss: 0.398697  [ 3088/ 3200]\n",
      "loss: 0.282838  [ 3104/ 3200]\n",
      "loss: 0.191875  [ 3120/ 3200]\n",
      "loss: 0.356263  [ 3136/ 3200]\n",
      "loss: 0.411510  [ 3152/ 3200]\n",
      "loss: 0.215693  [ 3168/ 3200]\n",
      "loss: 0.105117  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035658\n",
      "f1 macro averaged score: 0.765536\n",
      "Accuracy               : 77.2%\n",
      "Confusion matrix       :\n",
      "tensor([[181,  12,   0,   7],\n",
      "        [ 31, 101,  18,  50],\n",
      "        [  1,  14, 167,  18],\n",
      "        [  4,  17,  10, 169]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0900e-02.\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.149728  [    0/ 3200]\n",
      "loss: 0.417649  [   16/ 3200]\n",
      "loss: 0.178110  [   32/ 3200]\n",
      "loss: 0.476014  [   48/ 3200]\n",
      "loss: 0.382177  [   64/ 3200]\n",
      "loss: 0.172449  [   80/ 3200]\n",
      "loss: 0.558933  [   96/ 3200]\n",
      "loss: 0.176787  [  112/ 3200]\n",
      "loss: 0.272061  [  128/ 3200]\n",
      "loss: 0.153732  [  144/ 3200]\n",
      "loss: 0.479836  [  160/ 3200]\n",
      "loss: 0.328796  [  176/ 3200]\n",
      "loss: 0.658976  [  192/ 3200]\n",
      "loss: 0.165854  [  208/ 3200]\n",
      "loss: 0.674847  [  224/ 3200]\n",
      "loss: 0.252309  [  240/ 3200]\n",
      "loss: 0.181334  [  256/ 3200]\n",
      "loss: 0.411590  [  272/ 3200]\n",
      "loss: 0.447897  [  288/ 3200]\n",
      "loss: 0.278996  [  304/ 3200]\n",
      "loss: 0.531791  [  320/ 3200]\n",
      "loss: 0.525804  [  336/ 3200]\n",
      "loss: 0.183790  [  352/ 3200]\n",
      "loss: 0.296988  [  368/ 3200]\n",
      "loss: 0.324484  [  384/ 3200]\n",
      "loss: 0.102637  [  400/ 3200]\n",
      "loss: 0.195359  [  416/ 3200]\n",
      "loss: 0.610900  [  432/ 3200]\n",
      "loss: 0.842603  [  448/ 3200]\n",
      "loss: 0.387960  [  464/ 3200]\n",
      "loss: 0.408914  [  480/ 3200]\n",
      "loss: 0.535827  [  496/ 3200]\n",
      "loss: 0.137578  [  512/ 3200]\n",
      "loss: 0.499922  [  528/ 3200]\n",
      "loss: 0.627253  [  544/ 3200]\n",
      "loss: 0.576590  [  560/ 3200]\n",
      "loss: 0.379701  [  576/ 3200]\n",
      "loss: 0.398362  [  592/ 3200]\n",
      "loss: 0.475431  [  608/ 3200]\n",
      "loss: 0.082533  [  624/ 3200]\n",
      "loss: 0.095931  [  640/ 3200]\n",
      "loss: 0.413423  [  656/ 3200]\n",
      "loss: 0.252989  [  672/ 3200]\n",
      "loss: 0.601909  [  688/ 3200]\n",
      "loss: 0.350889  [  704/ 3200]\n",
      "loss: 0.450403  [  720/ 3200]\n",
      "loss: 0.339538  [  736/ 3200]\n",
      "loss: 0.440633  [  752/ 3200]\n",
      "loss: 0.221639  [  768/ 3200]\n",
      "loss: 0.182589  [  784/ 3200]\n",
      "loss: 0.194652  [  800/ 3200]\n",
      "loss: 0.624112  [  816/ 3200]\n",
      "loss: 0.323095  [  832/ 3200]\n",
      "loss: 0.494450  [  848/ 3200]\n",
      "loss: 0.503778  [  864/ 3200]\n",
      "loss: 0.503487  [  880/ 3200]\n",
      "loss: 0.232235  [  896/ 3200]\n",
      "loss: 0.058172  [  912/ 3200]\n",
      "loss: 0.491783  [  928/ 3200]\n",
      "loss: 0.195510  [  944/ 3200]\n",
      "loss: 0.425116  [  960/ 3200]\n",
      "loss: 0.471550  [  976/ 3200]\n",
      "loss: 0.459429  [  992/ 3200]\n",
      "loss: 0.299538  [ 1008/ 3200]\n",
      "loss: 0.278678  [ 1024/ 3200]\n",
      "loss: 0.488803  [ 1040/ 3200]\n",
      "loss: 0.413704  [ 1056/ 3200]\n",
      "loss: 0.464419  [ 1072/ 3200]\n",
      "loss: 0.124751  [ 1088/ 3200]\n",
      "loss: 0.466788  [ 1104/ 3200]\n",
      "loss: 0.453394  [ 1120/ 3200]\n",
      "loss: 0.226970  [ 1136/ 3200]\n",
      "loss: 0.442100  [ 1152/ 3200]\n",
      "loss: 0.477065  [ 1168/ 3200]\n",
      "loss: 0.120535  [ 1184/ 3200]\n",
      "loss: 0.511731  [ 1200/ 3200]\n",
      "loss: 0.263987  [ 1216/ 3200]\n",
      "loss: 0.087598  [ 1232/ 3200]\n",
      "loss: 0.262160  [ 1248/ 3200]\n",
      "loss: 0.447593  [ 1264/ 3200]\n",
      "loss: 0.196332  [ 1280/ 3200]\n",
      "loss: 0.108547  [ 1296/ 3200]\n",
      "loss: 0.238775  [ 1312/ 3200]\n",
      "loss: 0.222982  [ 1328/ 3200]\n",
      "loss: 0.348115  [ 1344/ 3200]\n",
      "loss: 0.232050  [ 1360/ 3200]\n",
      "loss: 0.381396  [ 1376/ 3200]\n",
      "loss: 0.176979  [ 1392/ 3200]\n",
      "loss: 0.286821  [ 1408/ 3200]\n",
      "loss: 0.388576  [ 1424/ 3200]\n",
      "loss: 0.257111  [ 1440/ 3200]\n",
      "loss: 0.184366  [ 1456/ 3200]\n",
      "loss: 0.274114  [ 1472/ 3200]\n",
      "loss: 0.251817  [ 1488/ 3200]\n",
      "loss: 0.228959  [ 1504/ 3200]\n",
      "loss: 0.262795  [ 1520/ 3200]\n",
      "loss: 0.153535  [ 1536/ 3200]\n",
      "loss: 0.481156  [ 1552/ 3200]\n",
      "loss: 0.234741  [ 1568/ 3200]\n",
      "loss: 0.355731  [ 1584/ 3200]\n",
      "loss: 0.297743  [ 1600/ 3200]\n",
      "loss: 0.447682  [ 1616/ 3200]\n",
      "loss: 0.223166  [ 1632/ 3200]\n",
      "loss: 0.496410  [ 1648/ 3200]\n",
      "loss: 0.098039  [ 1664/ 3200]\n",
      "loss: 0.326440  [ 1680/ 3200]\n",
      "loss: 0.210169  [ 1696/ 3200]\n",
      "loss: 0.712489  [ 1712/ 3200]\n",
      "loss: 0.041303  [ 1728/ 3200]\n",
      "loss: 0.227584  [ 1744/ 3200]\n",
      "loss: 0.307166  [ 1760/ 3200]\n",
      "loss: 0.457575  [ 1776/ 3200]\n",
      "loss: 0.447677  [ 1792/ 3200]\n",
      "loss: 0.264696  [ 1808/ 3200]\n",
      "loss: 0.502563  [ 1824/ 3200]\n",
      "loss: 0.441261  [ 1840/ 3200]\n",
      "loss: 0.282896  [ 1856/ 3200]\n",
      "loss: 0.266227  [ 1872/ 3200]\n",
      "loss: 0.376810  [ 1888/ 3200]\n",
      "loss: 0.216309  [ 1904/ 3200]\n",
      "loss: 0.284581  [ 1920/ 3200]\n",
      "loss: 0.585088  [ 1936/ 3200]\n",
      "loss: 0.332522  [ 1952/ 3200]\n",
      "loss: 0.215580  [ 1968/ 3200]\n",
      "loss: 0.405037  [ 1984/ 3200]\n",
      "loss: 0.429001  [ 2000/ 3200]\n",
      "loss: 0.188743  [ 2016/ 3200]\n",
      "loss: 0.499627  [ 2032/ 3200]\n",
      "loss: 0.293662  [ 2048/ 3200]\n",
      "loss: 0.431712  [ 2064/ 3200]\n",
      "loss: 0.325546  [ 2080/ 3200]\n",
      "loss: 0.385809  [ 2096/ 3200]\n",
      "loss: 0.701172  [ 2112/ 3200]\n",
      "loss: 0.282964  [ 2128/ 3200]\n",
      "loss: 0.326539  [ 2144/ 3200]\n",
      "loss: 0.406660  [ 2160/ 3200]\n",
      "loss: 0.147188  [ 2176/ 3200]\n",
      "loss: 0.161555  [ 2192/ 3200]\n",
      "loss: 0.060381  [ 2208/ 3200]\n",
      "loss: 0.427379  [ 2224/ 3200]\n",
      "loss: 0.288587  [ 2240/ 3200]\n",
      "loss: 0.089589  [ 2256/ 3200]\n",
      "loss: 0.151727  [ 2272/ 3200]\n",
      "loss: 0.360193  [ 2288/ 3200]\n",
      "loss: 0.391129  [ 2304/ 3200]\n",
      "loss: 0.115675  [ 2320/ 3200]\n",
      "loss: 0.411072  [ 2336/ 3200]\n",
      "loss: 0.187501  [ 2352/ 3200]\n",
      "loss: 0.441991  [ 2368/ 3200]\n",
      "loss: 0.544010  [ 2384/ 3200]\n",
      "loss: 0.194338  [ 2400/ 3200]\n",
      "loss: 0.410076  [ 2416/ 3200]\n",
      "loss: 0.485034  [ 2432/ 3200]\n",
      "loss: 0.273956  [ 2448/ 3200]\n",
      "loss: 0.094179  [ 2464/ 3200]\n",
      "loss: 0.449493  [ 2480/ 3200]\n",
      "loss: 0.754600  [ 2496/ 3200]\n",
      "loss: 0.447836  [ 2512/ 3200]\n",
      "loss: 0.182406  [ 2528/ 3200]\n",
      "loss: 0.266467  [ 2544/ 3200]\n",
      "loss: 0.744800  [ 2560/ 3200]\n",
      "loss: 0.432889  [ 2576/ 3200]\n",
      "loss: 0.135188  [ 2592/ 3200]\n",
      "loss: 0.315677  [ 2608/ 3200]\n",
      "loss: 0.211923  [ 2624/ 3200]\n",
      "loss: 0.308688  [ 2640/ 3200]\n",
      "loss: 0.244155  [ 2656/ 3200]\n",
      "loss: 0.269889  [ 2672/ 3200]\n",
      "loss: 0.361570  [ 2688/ 3200]\n",
      "loss: 0.684403  [ 2704/ 3200]\n",
      "loss: 0.233079  [ 2720/ 3200]\n",
      "loss: 0.254729  [ 2736/ 3200]\n",
      "loss: 0.129109  [ 2752/ 3200]\n",
      "loss: 0.591326  [ 2768/ 3200]\n",
      "loss: 0.985898  [ 2784/ 3200]\n",
      "loss: 0.283299  [ 2800/ 3200]\n",
      "loss: 0.558585  [ 2816/ 3200]\n",
      "loss: 0.418328  [ 2832/ 3200]\n",
      "loss: 0.454881  [ 2848/ 3200]\n",
      "loss: 0.428435  [ 2864/ 3200]\n",
      "loss: 0.299748  [ 2880/ 3200]\n",
      "loss: 0.552191  [ 2896/ 3200]\n",
      "loss: 0.179726  [ 2912/ 3200]\n",
      "loss: 0.491523  [ 2928/ 3200]\n",
      "loss: 0.267131  [ 2944/ 3200]\n",
      "loss: 0.202976  [ 2960/ 3200]\n",
      "loss: 0.580810  [ 2976/ 3200]\n",
      "loss: 0.467089  [ 2992/ 3200]\n",
      "loss: 0.354599  [ 3008/ 3200]\n",
      "loss: 0.214481  [ 3024/ 3200]\n",
      "loss: 0.196986  [ 3040/ 3200]\n",
      "loss: 0.565242  [ 3056/ 3200]\n",
      "loss: 0.110862  [ 3072/ 3200]\n",
      "loss: 0.718482  [ 3088/ 3200]\n",
      "loss: 0.156304  [ 3104/ 3200]\n",
      "loss: 0.147941  [ 3120/ 3200]\n",
      "loss: 0.285515  [ 3136/ 3200]\n",
      "loss: 0.135638  [ 3152/ 3200]\n",
      "loss: 0.203995  [ 3168/ 3200]\n",
      "loss: 0.538822  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036291\n",
      "f1 macro averaged score: 0.787741\n",
      "Accuracy               : 78.8%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  16,   0,  12],\n",
      "        [ 21, 127,  11,  41],\n",
      "        [  1,  15, 162,  22],\n",
      "        [  1,  22,   8, 169]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0945e-02.\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 0.435255  [    0/ 3200]\n",
      "loss: 0.195930  [   16/ 3200]\n",
      "loss: 0.109640  [   32/ 3200]\n",
      "loss: 0.233797  [   48/ 3200]\n",
      "loss: 0.062935  [   64/ 3200]\n",
      "loss: 0.230414  [   80/ 3200]\n",
      "loss: 0.692646  [   96/ 3200]\n",
      "loss: 0.386221  [  112/ 3200]\n",
      "loss: 0.347721  [  128/ 3200]\n",
      "loss: 0.257990  [  144/ 3200]\n",
      "loss: 0.277460  [  160/ 3200]\n",
      "loss: 0.302778  [  176/ 3200]\n",
      "loss: 0.362517  [  192/ 3200]\n",
      "loss: 0.515519  [  208/ 3200]\n",
      "loss: 0.340593  [  224/ 3200]\n",
      "loss: 0.315132  [  240/ 3200]\n",
      "loss: 0.261891  [  256/ 3200]\n",
      "loss: 0.331429  [  272/ 3200]\n",
      "loss: 0.346177  [  288/ 3200]\n",
      "loss: 0.943705  [  304/ 3200]\n",
      "loss: 0.138984  [  320/ 3200]\n",
      "loss: 0.185662  [  336/ 3200]\n",
      "loss: 0.272422  [  352/ 3200]\n",
      "loss: 0.683752  [  368/ 3200]\n",
      "loss: 0.602731  [  384/ 3200]\n",
      "loss: 0.361757  [  400/ 3200]\n",
      "loss: 0.206954  [  416/ 3200]\n",
      "loss: 0.326293  [  432/ 3200]\n",
      "loss: 0.394517  [  448/ 3200]\n",
      "loss: 0.306185  [  464/ 3200]\n",
      "loss: 0.211525  [  480/ 3200]\n",
      "loss: 0.203225  [  496/ 3200]\n",
      "loss: 0.229406  [  512/ 3200]\n",
      "loss: 0.070089  [  528/ 3200]\n",
      "loss: 0.375121  [  544/ 3200]\n",
      "loss: 0.184569  [  560/ 3200]\n",
      "loss: 0.327218  [  576/ 3200]\n",
      "loss: 0.208068  [  592/ 3200]\n",
      "loss: 0.120574  [  608/ 3200]\n",
      "loss: 0.105841  [  624/ 3200]\n",
      "loss: 0.654821  [  640/ 3200]\n",
      "loss: 0.322935  [  656/ 3200]\n",
      "loss: 0.326319  [  672/ 3200]\n",
      "loss: 0.067697  [  688/ 3200]\n",
      "loss: 0.368964  [  704/ 3200]\n",
      "loss: 0.411629  [  720/ 3200]\n",
      "loss: 0.298129  [  736/ 3200]\n",
      "loss: 0.449035  [  752/ 3200]\n",
      "loss: 0.308652  [  768/ 3200]\n",
      "loss: 0.242277  [  784/ 3200]\n",
      "loss: 0.164279  [  800/ 3200]\n",
      "loss: 0.368193  [  816/ 3200]\n",
      "loss: 0.731228  [  832/ 3200]\n",
      "loss: 2.358623  [  848/ 3200]\n",
      "loss: 0.330746  [  864/ 3200]\n",
      "loss: 0.489820  [  880/ 3200]\n",
      "loss: 0.652531  [  896/ 3200]\n",
      "loss: 0.257121  [  912/ 3200]\n",
      "loss: 0.315574  [  928/ 3200]\n",
      "loss: 0.502489  [  944/ 3200]\n",
      "loss: 0.513529  [  960/ 3200]\n",
      "loss: 0.394872  [  976/ 3200]\n",
      "loss: 0.354054  [  992/ 3200]\n",
      "loss: 0.441628  [ 1008/ 3200]\n",
      "loss: 0.384237  [ 1024/ 3200]\n",
      "loss: 0.334230  [ 1040/ 3200]\n",
      "loss: 0.167931  [ 1056/ 3200]\n",
      "loss: 0.473185  [ 1072/ 3200]\n",
      "loss: 0.080388  [ 1088/ 3200]\n",
      "loss: 0.368925  [ 1104/ 3200]\n",
      "loss: 0.349847  [ 1120/ 3200]\n",
      "loss: 0.157667  [ 1136/ 3200]\n",
      "loss: 0.755796  [ 1152/ 3200]\n",
      "loss: 0.700552  [ 1168/ 3200]\n",
      "loss: 0.218356  [ 1184/ 3200]\n",
      "loss: 0.515198  [ 1200/ 3200]\n",
      "loss: 0.343997  [ 1216/ 3200]\n",
      "loss: 0.320025  [ 1232/ 3200]\n",
      "loss: 0.298602  [ 1248/ 3200]\n",
      "loss: 0.305807  [ 1264/ 3200]\n",
      "loss: 0.332130  [ 1280/ 3200]\n",
      "loss: 0.237903  [ 1296/ 3200]\n",
      "loss: 0.185331  [ 1312/ 3200]\n",
      "loss: 0.195479  [ 1328/ 3200]\n",
      "loss: 0.200919  [ 1344/ 3200]\n",
      "loss: 0.244418  [ 1360/ 3200]\n",
      "loss: 0.339834  [ 1376/ 3200]\n",
      "loss: 0.162252  [ 1392/ 3200]\n",
      "loss: 0.102944  [ 1408/ 3200]\n",
      "loss: 0.279904  [ 1424/ 3200]\n",
      "loss: 0.418989  [ 1440/ 3200]\n",
      "loss: 0.443541  [ 1456/ 3200]\n",
      "loss: 0.330714  [ 1472/ 3200]\n",
      "loss: 0.140073  [ 1488/ 3200]\n",
      "loss: 0.586711  [ 1504/ 3200]\n",
      "loss: 0.445716  [ 1520/ 3200]\n",
      "loss: 0.264764  [ 1536/ 3200]\n",
      "loss: 0.183273  [ 1552/ 3200]\n",
      "loss: 0.343977  [ 1568/ 3200]\n",
      "loss: 0.229193  [ 1584/ 3200]\n",
      "loss: 0.500925  [ 1600/ 3200]\n",
      "loss: 0.338446  [ 1616/ 3200]\n",
      "loss: 0.269038  [ 1632/ 3200]\n",
      "loss: 0.372652  [ 1648/ 3200]\n",
      "loss: 0.388112  [ 1664/ 3200]\n",
      "loss: 0.014558  [ 1680/ 3200]\n",
      "loss: 0.085104  [ 1696/ 3200]\n",
      "loss: 0.276451  [ 1712/ 3200]\n",
      "loss: 0.248284  [ 1728/ 3200]\n",
      "loss: 0.151193  [ 1744/ 3200]\n",
      "loss: 0.431959  [ 1760/ 3200]\n",
      "loss: 0.184002  [ 1776/ 3200]\n",
      "loss: 0.360946  [ 1792/ 3200]\n",
      "loss: 0.157372  [ 1808/ 3200]\n",
      "loss: 0.505784  [ 1824/ 3200]\n",
      "loss: 0.356824  [ 1840/ 3200]\n",
      "loss: 0.527403  [ 1856/ 3200]\n",
      "loss: 0.125970  [ 1872/ 3200]\n",
      "loss: 0.130753  [ 1888/ 3200]\n",
      "loss: 0.381205  [ 1904/ 3200]\n",
      "loss: 0.360226  [ 1920/ 3200]\n",
      "loss: 0.436413  [ 1936/ 3200]\n",
      "loss: 0.619500  [ 1952/ 3200]\n",
      "loss: 0.248377  [ 1968/ 3200]\n",
      "loss: 0.227944  [ 1984/ 3200]\n",
      "loss: 0.459235  [ 2000/ 3200]\n",
      "loss: 0.380290  [ 2016/ 3200]\n",
      "loss: 0.303254  [ 2032/ 3200]\n",
      "loss: 0.285082  [ 2048/ 3200]\n",
      "loss: 0.153170  [ 2064/ 3200]\n",
      "loss: 0.562685  [ 2080/ 3200]\n",
      "loss: 0.423383  [ 2096/ 3200]\n",
      "loss: 0.155148  [ 2112/ 3200]\n",
      "loss: 0.461825  [ 2128/ 3200]\n",
      "loss: 0.375614  [ 2144/ 3200]\n",
      "loss: 0.423215  [ 2160/ 3200]\n",
      "loss: 0.593711  [ 2176/ 3200]\n",
      "loss: 0.515911  [ 2192/ 3200]\n",
      "loss: 0.477288  [ 2208/ 3200]\n",
      "loss: 0.488200  [ 2224/ 3200]\n",
      "loss: 0.307602  [ 2240/ 3200]\n",
      "loss: 0.663851  [ 2256/ 3200]\n",
      "loss: 0.222399  [ 2272/ 3200]\n",
      "loss: 0.165553  [ 2288/ 3200]\n",
      "loss: 0.253359  [ 2304/ 3200]\n",
      "loss: 0.411510  [ 2320/ 3200]\n",
      "loss: 0.446366  [ 2336/ 3200]\n",
      "loss: 0.249979  [ 2352/ 3200]\n",
      "loss: 0.575423  [ 2368/ 3200]\n",
      "loss: 0.540370  [ 2384/ 3200]\n",
      "loss: 0.202273  [ 2400/ 3200]\n",
      "loss: 0.266415  [ 2416/ 3200]\n",
      "loss: 0.198137  [ 2432/ 3200]\n",
      "loss: 0.228862  [ 2448/ 3200]\n",
      "loss: 0.278188  [ 2464/ 3200]\n",
      "loss: 0.400108  [ 2480/ 3200]\n",
      "loss: 0.365755  [ 2496/ 3200]\n",
      "loss: 0.103214  [ 2512/ 3200]\n",
      "loss: 0.226553  [ 2528/ 3200]\n",
      "loss: 0.266042  [ 2544/ 3200]\n",
      "loss: 0.131358  [ 2560/ 3200]\n",
      "loss: 0.436071  [ 2576/ 3200]\n",
      "loss: 0.637512  [ 2592/ 3200]\n",
      "loss: 0.328846  [ 2608/ 3200]\n",
      "loss: 0.301172  [ 2624/ 3200]\n",
      "loss: 0.503011  [ 2640/ 3200]\n",
      "loss: 0.561292  [ 2656/ 3200]\n",
      "loss: 0.412286  [ 2672/ 3200]\n",
      "loss: 0.297775  [ 2688/ 3200]\n",
      "loss: 0.164946  [ 2704/ 3200]\n",
      "loss: 0.079298  [ 2720/ 3200]\n",
      "loss: 0.176864  [ 2736/ 3200]\n",
      "loss: 0.213633  [ 2752/ 3200]\n",
      "loss: 0.166375  [ 2768/ 3200]\n",
      "loss: 0.164650  [ 2784/ 3200]\n",
      "loss: 0.656892  [ 2800/ 3200]\n",
      "loss: 0.332579  [ 2816/ 3200]\n",
      "loss: 0.622897  [ 2832/ 3200]\n",
      "loss: 0.479821  [ 2848/ 3200]\n",
      "loss: 0.248736  [ 2864/ 3200]\n",
      "loss: 0.372486  [ 2880/ 3200]\n",
      "loss: 0.294222  [ 2896/ 3200]\n",
      "loss: 0.510036  [ 2912/ 3200]\n",
      "loss: 0.287589  [ 2928/ 3200]\n",
      "loss: 0.364370  [ 2944/ 3200]\n",
      "loss: 0.654387  [ 2960/ 3200]\n",
      "loss: 0.057983  [ 2976/ 3200]\n",
      "loss: 0.162516  [ 2992/ 3200]\n",
      "loss: 0.353664  [ 3008/ 3200]\n",
      "loss: 0.382958  [ 3024/ 3200]\n",
      "loss: 0.284660  [ 3040/ 3200]\n",
      "loss: 0.734555  [ 3056/ 3200]\n",
      "loss: 1.198549  [ 3072/ 3200]\n",
      "loss: 0.390949  [ 3088/ 3200]\n",
      "loss: 0.400130  [ 3104/ 3200]\n",
      "loss: 0.689743  [ 3120/ 3200]\n",
      "loss: 0.299049  [ 3136/ 3200]\n",
      "loss: 0.148516  [ 3152/ 3200]\n",
      "loss: 0.409006  [ 3168/ 3200]\n",
      "loss: 0.379717  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.040100\n",
      "f1 macro averaged score: 0.747316\n",
      "Accuracy               : 75.8%\n",
      "Confusion matrix       :\n",
      "tensor([[165,  11,   0,  24],\n",
      "        [ 32,  93,  36,  39],\n",
      "        [  0,  10, 181,   9],\n",
      "        [  2,  15,  16, 167]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0990e-02.\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.406055  [    0/ 3200]\n",
      "loss: 0.320756  [   16/ 3200]\n",
      "loss: 0.293880  [   32/ 3200]\n",
      "loss: 0.240729  [   48/ 3200]\n",
      "loss: 0.114919  [   64/ 3200]\n",
      "loss: 0.049852  [   80/ 3200]\n",
      "loss: 0.288684  [   96/ 3200]\n",
      "loss: 0.250586  [  112/ 3200]\n",
      "loss: 0.234096  [  128/ 3200]\n",
      "loss: 0.522684  [  144/ 3200]\n",
      "loss: 0.410667  [  160/ 3200]\n",
      "loss: 0.065370  [  176/ 3200]\n",
      "loss: 0.685097  [  192/ 3200]\n",
      "loss: 0.168758  [  208/ 3200]\n",
      "loss: 0.231982  [  224/ 3200]\n",
      "loss: 0.188919  [  240/ 3200]\n",
      "loss: 0.245227  [  256/ 3200]\n",
      "loss: 0.366801  [  272/ 3200]\n",
      "loss: 0.477260  [  288/ 3200]\n",
      "loss: 0.572430  [  304/ 3200]\n",
      "loss: 0.388245  [  320/ 3200]\n",
      "loss: 0.402261  [  336/ 3200]\n",
      "loss: 0.213797  [  352/ 3200]\n",
      "loss: 0.280262  [  368/ 3200]\n",
      "loss: 0.191838  [  384/ 3200]\n",
      "loss: 0.361977  [  400/ 3200]\n",
      "loss: 0.273780  [  416/ 3200]\n",
      "loss: 0.400511  [  432/ 3200]\n",
      "loss: 0.335440  [  448/ 3200]\n",
      "loss: 0.338512  [  464/ 3200]\n",
      "loss: 0.091969  [  480/ 3200]\n",
      "loss: 0.225745  [  496/ 3200]\n",
      "loss: 0.380888  [  512/ 3200]\n",
      "loss: 0.188665  [  528/ 3200]\n",
      "loss: 0.245689  [  544/ 3200]\n",
      "loss: 0.192322  [  560/ 3200]\n",
      "loss: 0.380980  [  576/ 3200]\n",
      "loss: 0.115171  [  592/ 3200]\n",
      "loss: 0.258186  [  608/ 3200]\n",
      "loss: 0.460638  [  624/ 3200]\n",
      "loss: 0.359212  [  640/ 3200]\n",
      "loss: 0.426601  [  656/ 3200]\n",
      "loss: 0.224013  [  672/ 3200]\n",
      "loss: 0.361650  [  688/ 3200]\n",
      "loss: 0.255622  [  704/ 3200]\n",
      "loss: 0.087442  [  720/ 3200]\n",
      "loss: 0.353399  [  736/ 3200]\n",
      "loss: 0.377211  [  752/ 3200]\n",
      "loss: 0.257575  [  768/ 3200]\n",
      "loss: 0.388471  [  784/ 3200]\n",
      "loss: 0.472951  [  800/ 3200]\n",
      "loss: 0.268819  [  816/ 3200]\n",
      "loss: 0.280294  [  832/ 3200]\n",
      "loss: 0.185249  [  848/ 3200]\n",
      "loss: 0.204167  [  864/ 3200]\n",
      "loss: 0.203422  [  880/ 3200]\n",
      "loss: 0.264628  [  896/ 3200]\n",
      "loss: 0.147706  [  912/ 3200]\n",
      "loss: 0.193906  [  928/ 3200]\n",
      "loss: 0.234227  [  944/ 3200]\n",
      "loss: 0.066372  [  960/ 3200]\n",
      "loss: 0.331653  [  976/ 3200]\n",
      "loss: 0.738417  [  992/ 3200]\n",
      "loss: 0.214774  [ 1008/ 3200]\n",
      "loss: 0.260826  [ 1024/ 3200]\n",
      "loss: 0.111151  [ 1040/ 3200]\n",
      "loss: 0.165777  [ 1056/ 3200]\n",
      "loss: 0.262854  [ 1072/ 3200]\n",
      "loss: 0.181093  [ 1088/ 3200]\n",
      "loss: 0.443369  [ 1104/ 3200]\n",
      "loss: 0.142149  [ 1120/ 3200]\n",
      "loss: 0.645229  [ 1136/ 3200]\n",
      "loss: 0.150461  [ 1152/ 3200]\n",
      "loss: 0.134351  [ 1168/ 3200]\n",
      "loss: 0.431543  [ 1184/ 3200]\n",
      "loss: 0.128649  [ 1200/ 3200]\n",
      "loss: 0.232748  [ 1216/ 3200]\n",
      "loss: 0.132538  [ 1232/ 3200]\n",
      "loss: 0.387781  [ 1248/ 3200]\n",
      "loss: 0.110376  [ 1264/ 3200]\n",
      "loss: 0.563405  [ 1280/ 3200]\n",
      "loss: 0.084510  [ 1296/ 3200]\n",
      "loss: 0.180709  [ 1312/ 3200]\n",
      "loss: 0.247125  [ 1328/ 3200]\n",
      "loss: 0.208436  [ 1344/ 3200]\n",
      "loss: 0.280716  [ 1360/ 3200]\n",
      "loss: 0.169782  [ 1376/ 3200]\n",
      "loss: 0.391073  [ 1392/ 3200]\n",
      "loss: 0.840358  [ 1408/ 3200]\n",
      "loss: 0.388699  [ 1424/ 3200]\n",
      "loss: 0.843235  [ 1440/ 3200]\n",
      "loss: 0.297989  [ 1456/ 3200]\n",
      "loss: 0.256685  [ 1472/ 3200]\n",
      "loss: 0.339367  [ 1488/ 3200]\n",
      "loss: 0.428843  [ 1504/ 3200]\n",
      "loss: 0.193054  [ 1520/ 3200]\n",
      "loss: 0.081780  [ 1536/ 3200]\n",
      "loss: 0.191254  [ 1552/ 3200]\n",
      "loss: 0.131967  [ 1568/ 3200]\n",
      "loss: 0.256431  [ 1584/ 3200]\n",
      "loss: 0.334421  [ 1600/ 3200]\n",
      "loss: 0.339859  [ 1616/ 3200]\n",
      "loss: 0.106777  [ 1632/ 3200]\n",
      "loss: 0.207048  [ 1648/ 3200]\n",
      "loss: 0.232589  [ 1664/ 3200]\n",
      "loss: 0.560657  [ 1680/ 3200]\n",
      "loss: 0.288197  [ 1696/ 3200]\n",
      "loss: 0.591143  [ 1712/ 3200]\n",
      "loss: 0.246709  [ 1728/ 3200]\n",
      "loss: 0.364478  [ 1744/ 3200]\n",
      "loss: 0.355356  [ 1760/ 3200]\n",
      "loss: 0.473240  [ 1776/ 3200]\n",
      "loss: 0.180463  [ 1792/ 3200]\n",
      "loss: 0.263495  [ 1808/ 3200]\n",
      "loss: 0.334488  [ 1824/ 3200]\n",
      "loss: 0.382906  [ 1840/ 3200]\n",
      "loss: 0.412276  [ 1856/ 3200]\n",
      "loss: 0.436851  [ 1872/ 3200]\n",
      "loss: 0.174116  [ 1888/ 3200]\n",
      "loss: 0.296508  [ 1904/ 3200]\n",
      "loss: 0.111103  [ 1920/ 3200]\n",
      "loss: 0.171610  [ 1936/ 3200]\n",
      "loss: 0.347350  [ 1952/ 3200]\n",
      "loss: 0.162768  [ 1968/ 3200]\n",
      "loss: 0.501197  [ 1984/ 3200]\n",
      "loss: 0.686953  [ 2000/ 3200]\n",
      "loss: 0.263774  [ 2016/ 3200]\n",
      "loss: 0.337511  [ 2032/ 3200]\n",
      "loss: 0.304806  [ 2048/ 3200]\n",
      "loss: 0.174587  [ 2064/ 3200]\n",
      "loss: 0.170667  [ 2080/ 3200]\n",
      "loss: 0.596929  [ 2096/ 3200]\n",
      "loss: 0.839048  [ 2112/ 3200]\n",
      "loss: 0.509417  [ 2128/ 3200]\n",
      "loss: 0.416472  [ 2144/ 3200]\n",
      "loss: 0.347434  [ 2160/ 3200]\n",
      "loss: 0.311730  [ 2176/ 3200]\n",
      "loss: 0.224371  [ 2192/ 3200]\n",
      "loss: 0.456939  [ 2208/ 3200]\n",
      "loss: 0.387421  [ 2224/ 3200]\n",
      "loss: 0.298118  [ 2240/ 3200]\n",
      "loss: 0.615246  [ 2256/ 3200]\n",
      "loss: 0.400686  [ 2272/ 3200]\n",
      "loss: 0.187316  [ 2288/ 3200]\n",
      "loss: 0.166978  [ 2304/ 3200]\n",
      "loss: 0.058342  [ 2320/ 3200]\n",
      "loss: 0.578031  [ 2336/ 3200]\n",
      "loss: 0.265531  [ 2352/ 3200]\n",
      "loss: 0.732644  [ 2368/ 3200]\n",
      "loss: 0.483133  [ 2384/ 3200]\n",
      "loss: 0.552132  [ 2400/ 3200]\n",
      "loss: 0.175136  [ 2416/ 3200]\n",
      "loss: 0.280738  [ 2432/ 3200]\n",
      "loss: 0.555401  [ 2448/ 3200]\n",
      "loss: 0.210269  [ 2464/ 3200]\n",
      "loss: 0.210977  [ 2480/ 3200]\n",
      "loss: 0.445383  [ 2496/ 3200]\n",
      "loss: 0.362970  [ 2512/ 3200]\n",
      "loss: 0.340864  [ 2528/ 3200]\n",
      "loss: 0.355802  [ 2544/ 3200]\n",
      "loss: 0.462169  [ 2560/ 3200]\n",
      "loss: 0.428066  [ 2576/ 3200]\n",
      "loss: 0.628398  [ 2592/ 3200]\n",
      "loss: 0.430142  [ 2608/ 3200]\n",
      "loss: 0.180813  [ 2624/ 3200]\n",
      "loss: 0.134388  [ 2640/ 3200]\n",
      "loss: 0.401590  [ 2656/ 3200]\n",
      "loss: 0.126449  [ 2672/ 3200]\n",
      "loss: 0.091661  [ 2688/ 3200]\n",
      "loss: 0.230017  [ 2704/ 3200]\n",
      "loss: 0.299512  [ 2720/ 3200]\n",
      "loss: 0.236578  [ 2736/ 3200]\n",
      "loss: 0.198152  [ 2752/ 3200]\n",
      "loss: 0.037889  [ 2768/ 3200]\n",
      "loss: 0.136077  [ 2784/ 3200]\n",
      "loss: 0.288861  [ 2800/ 3200]\n",
      "loss: 0.332778  [ 2816/ 3200]\n",
      "loss: 0.184473  [ 2832/ 3200]\n",
      "loss: 0.401540  [ 2848/ 3200]\n",
      "loss: 0.017689  [ 2864/ 3200]\n",
      "loss: 0.469222  [ 2880/ 3200]\n",
      "loss: 0.255937  [ 2896/ 3200]\n",
      "loss: 0.362284  [ 2912/ 3200]\n",
      "loss: 0.330050  [ 2928/ 3200]\n",
      "loss: 0.146931  [ 2944/ 3200]\n",
      "loss: 0.462387  [ 2960/ 3200]\n",
      "loss: 0.391954  [ 2976/ 3200]\n",
      "loss: 0.740381  [ 2992/ 3200]\n",
      "loss: 0.440781  [ 3008/ 3200]\n",
      "loss: 0.197793  [ 3024/ 3200]\n",
      "loss: 0.366781  [ 3040/ 3200]\n",
      "loss: 0.333582  [ 3056/ 3200]\n",
      "loss: 0.293100  [ 3072/ 3200]\n",
      "loss: 0.287682  [ 3088/ 3200]\n",
      "loss: 0.199644  [ 3104/ 3200]\n",
      "loss: 0.355639  [ 3120/ 3200]\n",
      "loss: 0.192648  [ 3136/ 3200]\n",
      "loss: 0.518850  [ 3152/ 3200]\n",
      "loss: 0.271771  [ 3168/ 3200]\n",
      "loss: 0.436218  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.050438\n",
      "f1 macro averaged score: 0.705151\n",
      "Accuracy               : 72.6%\n",
      "Confusion matrix       :\n",
      "tensor([[182,   8,   0,  10],\n",
      "        [ 38,  60,  15,  87],\n",
      "        [  0,   9, 155,  36],\n",
      "        [  2,   9,   5, 184]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1035e-02.\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.419437  [    0/ 3200]\n",
      "loss: 0.458987  [   16/ 3200]\n",
      "loss: 0.384016  [   32/ 3200]\n",
      "loss: 0.333024  [   48/ 3200]\n",
      "loss: 0.408881  [   64/ 3200]\n",
      "loss: 0.107538  [   80/ 3200]\n",
      "loss: 0.168989  [   96/ 3200]\n",
      "loss: 0.220639  [  112/ 3200]\n",
      "loss: 0.404908  [  128/ 3200]\n",
      "loss: 0.322885  [  144/ 3200]\n",
      "loss: 0.079094  [  160/ 3200]\n",
      "loss: 0.096274  [  176/ 3200]\n",
      "loss: 0.253431  [  192/ 3200]\n",
      "loss: 0.412999  [  208/ 3200]\n",
      "loss: 0.151280  [  224/ 3200]\n",
      "loss: 0.111724  [  240/ 3200]\n",
      "loss: 0.135648  [  256/ 3200]\n",
      "loss: 0.318499  [  272/ 3200]\n",
      "loss: 0.306526  [  288/ 3200]\n",
      "loss: 0.311717  [  304/ 3200]\n",
      "loss: 0.078553  [  320/ 3200]\n",
      "loss: 0.284948  [  336/ 3200]\n",
      "loss: 0.392497  [  352/ 3200]\n",
      "loss: 0.604956  [  368/ 3200]\n",
      "loss: 0.481559  [  384/ 3200]\n",
      "loss: 0.163467  [  400/ 3200]\n",
      "loss: 0.373391  [  416/ 3200]\n",
      "loss: 0.254574  [  432/ 3200]\n",
      "loss: 0.136959  [  448/ 3200]\n",
      "loss: 0.312666  [  464/ 3200]\n",
      "loss: 0.295889  [  480/ 3200]\n",
      "loss: 0.134234  [  496/ 3200]\n",
      "loss: 0.234635  [  512/ 3200]\n",
      "loss: 0.203171  [  528/ 3200]\n",
      "loss: 0.323151  [  544/ 3200]\n",
      "loss: 0.139618  [  560/ 3200]\n",
      "loss: 0.349228  [  576/ 3200]\n",
      "loss: 0.315367  [  592/ 3200]\n",
      "loss: 0.595544  [  608/ 3200]\n",
      "loss: 0.355988  [  624/ 3200]\n",
      "loss: 0.092819  [  640/ 3200]\n",
      "loss: 0.119354  [  656/ 3200]\n",
      "loss: 0.293130  [  672/ 3200]\n",
      "loss: 0.109871  [  688/ 3200]\n",
      "loss: 0.296001  [  704/ 3200]\n",
      "loss: 0.214082  [  720/ 3200]\n",
      "loss: 0.269357  [  736/ 3200]\n",
      "loss: 0.265671  [  752/ 3200]\n",
      "loss: 0.533606  [  768/ 3200]\n",
      "loss: 0.459949  [  784/ 3200]\n",
      "loss: 0.201914  [  800/ 3200]\n",
      "loss: 0.269196  [  816/ 3200]\n",
      "loss: 0.555150  [  832/ 3200]\n",
      "loss: 0.207404  [  848/ 3200]\n",
      "loss: 0.590027  [  864/ 3200]\n",
      "loss: 0.225126  [  880/ 3200]\n",
      "loss: 0.142538  [  896/ 3200]\n",
      "loss: 0.287854  [  912/ 3200]\n",
      "loss: 0.238790  [  928/ 3200]\n",
      "loss: 0.154632  [  944/ 3200]\n",
      "loss: 0.243331  [  960/ 3200]\n",
      "loss: 0.532258  [  976/ 3200]\n",
      "loss: 0.560909  [  992/ 3200]\n",
      "loss: 0.320276  [ 1008/ 3200]\n",
      "loss: 0.340651  [ 1024/ 3200]\n",
      "loss: 0.433413  [ 1040/ 3200]\n",
      "loss: 0.371613  [ 1056/ 3200]\n",
      "loss: 0.252724  [ 1072/ 3200]\n",
      "loss: 0.495575  [ 1088/ 3200]\n",
      "loss: 0.422300  [ 1104/ 3200]\n",
      "loss: 0.245119  [ 1120/ 3200]\n",
      "loss: 0.383724  [ 1136/ 3200]\n",
      "loss: 0.145787  [ 1152/ 3200]\n",
      "loss: 0.284027  [ 1168/ 3200]\n",
      "loss: 0.340591  [ 1184/ 3200]\n",
      "loss: 0.272568  [ 1200/ 3200]\n",
      "loss: 0.797051  [ 1216/ 3200]\n",
      "loss: 0.177939  [ 1232/ 3200]\n",
      "loss: 0.337467  [ 1248/ 3200]\n",
      "loss: 0.112787  [ 1264/ 3200]\n",
      "loss: 0.312546  [ 1280/ 3200]\n",
      "loss: 0.702316  [ 1296/ 3200]\n",
      "loss: 0.086231  [ 1312/ 3200]\n",
      "loss: 0.179914  [ 1328/ 3200]\n",
      "loss: 0.471860  [ 1344/ 3200]\n",
      "loss: 0.105200  [ 1360/ 3200]\n",
      "loss: 0.329486  [ 1376/ 3200]\n",
      "loss: 0.183405  [ 1392/ 3200]\n",
      "loss: 0.400092  [ 1408/ 3200]\n",
      "loss: 0.201855  [ 1424/ 3200]\n",
      "loss: 0.210680  [ 1440/ 3200]\n",
      "loss: 0.292418  [ 1456/ 3200]\n",
      "loss: 0.306606  [ 1472/ 3200]\n",
      "loss: 0.197294  [ 1488/ 3200]\n",
      "loss: 0.230952  [ 1504/ 3200]\n",
      "loss: 0.218855  [ 1520/ 3200]\n",
      "loss: 0.345986  [ 1536/ 3200]\n",
      "loss: 0.076922  [ 1552/ 3200]\n",
      "loss: 0.265006  [ 1568/ 3200]\n",
      "loss: 0.229150  [ 1584/ 3200]\n",
      "loss: 0.523645  [ 1600/ 3200]\n",
      "loss: 0.360248  [ 1616/ 3200]\n",
      "loss: 0.255245  [ 1632/ 3200]\n",
      "loss: 0.215882  [ 1648/ 3200]\n",
      "loss: 0.262213  [ 1664/ 3200]\n",
      "loss: 0.161240  [ 1680/ 3200]\n",
      "loss: 0.239406  [ 1696/ 3200]\n",
      "loss: 0.069407  [ 1712/ 3200]\n",
      "loss: 0.219620  [ 1728/ 3200]\n",
      "loss: 0.123829  [ 1744/ 3200]\n",
      "loss: 0.113940  [ 1760/ 3200]\n",
      "loss: 0.251903  [ 1776/ 3200]\n",
      "loss: 0.310739  [ 1792/ 3200]\n",
      "loss: 0.227407  [ 1808/ 3200]\n",
      "loss: 0.047979  [ 1824/ 3200]\n",
      "loss: 0.309184  [ 1840/ 3200]\n",
      "loss: 0.200753  [ 1856/ 3200]\n",
      "loss: 0.505461  [ 1872/ 3200]\n",
      "loss: 0.332281  [ 1888/ 3200]\n",
      "loss: 0.164860  [ 1904/ 3200]\n",
      "loss: 0.187093  [ 1920/ 3200]\n",
      "loss: 0.143579  [ 1936/ 3200]\n",
      "loss: 0.550695  [ 1952/ 3200]\n",
      "loss: 0.230557  [ 1968/ 3200]\n",
      "loss: 0.203067  [ 1984/ 3200]\n",
      "loss: 0.517861  [ 2000/ 3200]\n",
      "loss: 0.176042  [ 2016/ 3200]\n",
      "loss: 0.174882  [ 2032/ 3200]\n",
      "loss: 0.322985  [ 2048/ 3200]\n",
      "loss: 0.452264  [ 2064/ 3200]\n",
      "loss: 0.077958  [ 2080/ 3200]\n",
      "loss: 0.079784  [ 2096/ 3200]\n",
      "loss: 0.542473  [ 2112/ 3200]\n",
      "loss: 0.415044  [ 2128/ 3200]\n",
      "loss: 0.519877  [ 2144/ 3200]\n",
      "loss: 0.153178  [ 2160/ 3200]\n",
      "loss: 0.302560  [ 2176/ 3200]\n",
      "loss: 0.219688  [ 2192/ 3200]\n",
      "loss: 0.158004  [ 2208/ 3200]\n",
      "loss: 0.246590  [ 2224/ 3200]\n",
      "loss: 0.158281  [ 2240/ 3200]\n",
      "loss: 0.367894  [ 2256/ 3200]\n",
      "loss: 0.144261  [ 2272/ 3200]\n",
      "loss: 0.365863  [ 2288/ 3200]\n",
      "loss: 0.272811  [ 2304/ 3200]\n",
      "loss: 0.118394  [ 2320/ 3200]\n",
      "loss: 0.292913  [ 2336/ 3200]\n",
      "loss: 0.249568  [ 2352/ 3200]\n",
      "loss: 0.423287  [ 2368/ 3200]\n",
      "loss: 0.438119  [ 2384/ 3200]\n",
      "loss: 0.466140  [ 2400/ 3200]\n",
      "loss: 0.223646  [ 2416/ 3200]\n",
      "loss: 0.118234  [ 2432/ 3200]\n",
      "loss: 0.208243  [ 2448/ 3200]\n",
      "loss: 0.430120  [ 2464/ 3200]\n",
      "loss: 0.386307  [ 2480/ 3200]\n",
      "loss: 0.230755  [ 2496/ 3200]\n",
      "loss: 0.331771  [ 2512/ 3200]\n",
      "loss: 0.644304  [ 2528/ 3200]\n",
      "loss: 0.595192  [ 2544/ 3200]\n",
      "loss: 0.316983  [ 2560/ 3200]\n",
      "loss: 0.241998  [ 2576/ 3200]\n",
      "loss: 0.055250  [ 2592/ 3200]\n",
      "loss: 0.352449  [ 2608/ 3200]\n",
      "loss: 0.160670  [ 2624/ 3200]\n",
      "loss: 0.193970  [ 2640/ 3200]\n",
      "loss: 0.109042  [ 2656/ 3200]\n",
      "loss: 0.318210  [ 2672/ 3200]\n",
      "loss: 0.293447  [ 2688/ 3200]\n",
      "loss: 0.213826  [ 2704/ 3200]\n",
      "loss: 0.211193  [ 2720/ 3200]\n",
      "loss: 0.217121  [ 2736/ 3200]\n",
      "loss: 0.270578  [ 2752/ 3200]\n",
      "loss: 0.179039  [ 2768/ 3200]\n",
      "loss: 0.127586  [ 2784/ 3200]\n",
      "loss: 0.325684  [ 2800/ 3200]\n",
      "loss: 0.340054  [ 2816/ 3200]\n",
      "loss: 0.612047  [ 2832/ 3200]\n",
      "loss: 0.113259  [ 2848/ 3200]\n",
      "loss: 0.379871  [ 2864/ 3200]\n",
      "loss: 0.195462  [ 2880/ 3200]\n",
      "loss: 0.144869  [ 2896/ 3200]\n",
      "loss: 0.310572  [ 2912/ 3200]\n",
      "loss: 0.279978  [ 2928/ 3200]\n",
      "loss: 0.432594  [ 2944/ 3200]\n",
      "loss: 0.240249  [ 2960/ 3200]\n",
      "loss: 0.768604  [ 2976/ 3200]\n",
      "loss: 0.206902  [ 2992/ 3200]\n",
      "loss: 0.314049  [ 3008/ 3200]\n",
      "loss: 0.479803  [ 3024/ 3200]\n",
      "loss: 0.129836  [ 3040/ 3200]\n",
      "loss: 0.258508  [ 3056/ 3200]\n",
      "loss: 0.324095  [ 3072/ 3200]\n",
      "loss: 0.255995  [ 3088/ 3200]\n",
      "loss: 0.352029  [ 3104/ 3200]\n",
      "loss: 0.154531  [ 3120/ 3200]\n",
      "loss: 0.531394  [ 3136/ 3200]\n",
      "loss: 0.103468  [ 3152/ 3200]\n",
      "loss: 0.261618  [ 3168/ 3200]\n",
      "loss: 0.315161  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.040632\n",
      "f1 macro averaged score: 0.772192\n",
      "Accuracy               : 77.9%\n",
      "Confusion matrix       :\n",
      "tensor([[180,  10,   0,  10],\n",
      "        [ 37, 106,  17,  40],\n",
      "        [  1,  14, 174,  11],\n",
      "        [  5,  19,  13, 163]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1080e-02.\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.174504  [    0/ 3200]\n",
      "loss: 0.447239  [   16/ 3200]\n",
      "loss: 0.167112  [   32/ 3200]\n",
      "loss: 0.211422  [   48/ 3200]\n",
      "loss: 0.300856  [   64/ 3200]\n",
      "loss: 0.141781  [   80/ 3200]\n",
      "loss: 0.189458  [   96/ 3200]\n",
      "loss: 0.061844  [  112/ 3200]\n",
      "loss: 0.315329  [  128/ 3200]\n",
      "loss: 0.111301  [  144/ 3200]\n",
      "loss: 0.289153  [  160/ 3200]\n",
      "loss: 0.114809  [  176/ 3200]\n",
      "loss: 0.199313  [  192/ 3200]\n",
      "loss: 0.503667  [  208/ 3200]\n",
      "loss: 0.256291  [  224/ 3200]\n",
      "loss: 0.103452  [  240/ 3200]\n",
      "loss: 0.616036  [  256/ 3200]\n",
      "loss: 0.499738  [  272/ 3200]\n",
      "loss: 0.225768  [  288/ 3200]\n",
      "loss: 0.075190  [  304/ 3200]\n",
      "loss: 0.264452  [  320/ 3200]\n",
      "loss: 0.375077  [  336/ 3200]\n",
      "loss: 0.207910  [  352/ 3200]\n",
      "loss: 0.214006  [  368/ 3200]\n",
      "loss: 0.475086  [  384/ 3200]\n",
      "loss: 0.528618  [  400/ 3200]\n",
      "loss: 0.549556  [  416/ 3200]\n",
      "loss: 0.128800  [  432/ 3200]\n",
      "loss: 0.131982  [  448/ 3200]\n",
      "loss: 0.375926  [  464/ 3200]\n",
      "loss: 0.173024  [  480/ 3200]\n",
      "loss: 0.275230  [  496/ 3200]\n",
      "loss: 0.134423  [  512/ 3200]\n",
      "loss: 0.484844  [  528/ 3200]\n",
      "loss: 0.380257  [  544/ 3200]\n",
      "loss: 0.255702  [  560/ 3200]\n",
      "loss: 0.197470  [  576/ 3200]\n",
      "loss: 0.303357  [  592/ 3200]\n",
      "loss: 0.205486  [  608/ 3200]\n",
      "loss: 0.501341  [  624/ 3200]\n",
      "loss: 0.105534  [  640/ 3200]\n",
      "loss: 0.152934  [  656/ 3200]\n",
      "loss: 0.413425  [  672/ 3200]\n",
      "loss: 0.042063  [  688/ 3200]\n",
      "loss: 0.357123  [  704/ 3200]\n",
      "loss: 0.042541  [  720/ 3200]\n",
      "loss: 0.078463  [  736/ 3200]\n",
      "loss: 0.105126  [  752/ 3200]\n",
      "loss: 0.223263  [  768/ 3200]\n",
      "loss: 1.016374  [  784/ 3200]\n",
      "loss: 0.292814  [  800/ 3200]\n",
      "loss: 0.235967  [  816/ 3200]\n",
      "loss: 0.168260  [  832/ 3200]\n",
      "loss: 0.124945  [  848/ 3200]\n",
      "loss: 0.220382  [  864/ 3200]\n",
      "loss: 0.102516  [  880/ 3200]\n",
      "loss: 0.215352  [  896/ 3200]\n",
      "loss: 0.264752  [  912/ 3200]\n",
      "loss: 0.794693  [  928/ 3200]\n",
      "loss: 0.161731  [  944/ 3200]\n",
      "loss: 0.197112  [  960/ 3200]\n",
      "loss: 0.236438  [  976/ 3200]\n",
      "loss: 0.223111  [  992/ 3200]\n",
      "loss: 0.121451  [ 1008/ 3200]\n",
      "loss: 0.410279  [ 1024/ 3200]\n",
      "loss: 0.195036  [ 1040/ 3200]\n",
      "loss: 0.415691  [ 1056/ 3200]\n",
      "loss: 0.239206  [ 1072/ 3200]\n",
      "loss: 0.474226  [ 1088/ 3200]\n",
      "loss: 0.397837  [ 1104/ 3200]\n",
      "loss: 0.538014  [ 1120/ 3200]\n",
      "loss: 0.457950  [ 1136/ 3200]\n",
      "loss: 0.062867  [ 1152/ 3200]\n",
      "loss: 0.204867  [ 1168/ 3200]\n",
      "loss: 0.224422  [ 1184/ 3200]\n",
      "loss: 0.764764  [ 1200/ 3200]\n",
      "loss: 0.304901  [ 1216/ 3200]\n",
      "loss: 0.200235  [ 1232/ 3200]\n",
      "loss: 0.135104  [ 1248/ 3200]\n",
      "loss: 0.626511  [ 1264/ 3200]\n",
      "loss: 0.324952  [ 1280/ 3200]\n",
      "loss: 0.243013  [ 1296/ 3200]\n",
      "loss: 0.248354  [ 1312/ 3200]\n",
      "loss: 0.354223  [ 1328/ 3200]\n",
      "loss: 0.273342  [ 1344/ 3200]\n",
      "loss: 0.190342  [ 1360/ 3200]\n",
      "loss: 0.241056  [ 1376/ 3200]\n",
      "loss: 0.170732  [ 1392/ 3200]\n",
      "loss: 0.079400  [ 1408/ 3200]\n",
      "loss: 0.171995  [ 1424/ 3200]\n",
      "loss: 0.311283  [ 1440/ 3200]\n",
      "loss: 0.311321  [ 1456/ 3200]\n",
      "loss: 0.414786  [ 1472/ 3200]\n",
      "loss: 0.252498  [ 1488/ 3200]\n",
      "loss: 0.174336  [ 1504/ 3200]\n",
      "loss: 0.500209  [ 1520/ 3200]\n",
      "loss: 0.208361  [ 1536/ 3200]\n",
      "loss: 0.216345  [ 1552/ 3200]\n",
      "loss: 0.274685  [ 1568/ 3200]\n",
      "loss: 0.608372  [ 1584/ 3200]\n",
      "loss: 0.126401  [ 1600/ 3200]\n",
      "loss: 0.240123  [ 1616/ 3200]\n",
      "loss: 0.185908  [ 1632/ 3200]\n",
      "loss: 0.104350  [ 1648/ 3200]\n",
      "loss: 0.581283  [ 1664/ 3200]\n",
      "loss: 0.168299  [ 1680/ 3200]\n",
      "loss: 0.183899  [ 1696/ 3200]\n",
      "loss: 0.167485  [ 1712/ 3200]\n",
      "loss: 0.369382  [ 1728/ 3200]\n",
      "loss: 0.465917  [ 1744/ 3200]\n",
      "loss: 0.065812  [ 1760/ 3200]\n",
      "loss: 0.094455  [ 1776/ 3200]\n",
      "loss: 0.183820  [ 1792/ 3200]\n",
      "loss: 0.344416  [ 1808/ 3200]\n",
      "loss: 0.192777  [ 1824/ 3200]\n",
      "loss: 0.224828  [ 1840/ 3200]\n",
      "loss: 0.220054  [ 1856/ 3200]\n",
      "loss: 0.353290  [ 1872/ 3200]\n",
      "loss: 0.305398  [ 1888/ 3200]\n",
      "loss: 0.194897  [ 1904/ 3200]\n",
      "loss: 0.469958  [ 1920/ 3200]\n",
      "loss: 0.102609  [ 1936/ 3200]\n",
      "loss: 0.458764  [ 1952/ 3200]\n",
      "loss: 0.403030  [ 1968/ 3200]\n",
      "loss: 0.121259  [ 1984/ 3200]\n",
      "loss: 0.343408  [ 2000/ 3200]\n",
      "loss: 0.270714  [ 2016/ 3200]\n",
      "loss: 0.546712  [ 2032/ 3200]\n",
      "loss: 0.249828  [ 2048/ 3200]\n",
      "loss: 0.375992  [ 2064/ 3200]\n",
      "loss: 0.475762  [ 2080/ 3200]\n",
      "loss: 0.457873  [ 2096/ 3200]\n",
      "loss: 0.344614  [ 2112/ 3200]\n",
      "loss: 0.109768  [ 2128/ 3200]\n",
      "loss: 0.398599  [ 2144/ 3200]\n",
      "loss: 0.182865  [ 2160/ 3200]\n",
      "loss: 0.188673  [ 2176/ 3200]\n",
      "loss: 0.511625  [ 2192/ 3200]\n",
      "loss: 0.104782  [ 2208/ 3200]\n",
      "loss: 0.150524  [ 2224/ 3200]\n",
      "loss: 0.219285  [ 2240/ 3200]\n",
      "loss: 0.053656  [ 2256/ 3200]\n",
      "loss: 0.159895  [ 2272/ 3200]\n",
      "loss: 0.226123  [ 2288/ 3200]\n",
      "loss: 0.162229  [ 2304/ 3200]\n",
      "loss: 0.429533  [ 2320/ 3200]\n",
      "loss: 0.312457  [ 2336/ 3200]\n",
      "loss: 0.114339  [ 2352/ 3200]\n",
      "loss: 0.558401  [ 2368/ 3200]\n",
      "loss: 0.241988  [ 2384/ 3200]\n",
      "loss: 0.123425  [ 2400/ 3200]\n",
      "loss: 0.361158  [ 2416/ 3200]\n",
      "loss: 0.459231  [ 2432/ 3200]\n",
      "loss: 0.098410  [ 2448/ 3200]\n",
      "loss: 0.173735  [ 2464/ 3200]\n",
      "loss: 0.176316  [ 2480/ 3200]\n",
      "loss: 0.388022  [ 2496/ 3200]\n",
      "loss: 0.537269  [ 2512/ 3200]\n",
      "loss: 0.901677  [ 2528/ 3200]\n",
      "loss: 0.782301  [ 2544/ 3200]\n",
      "loss: 0.253607  [ 2560/ 3200]\n",
      "loss: 0.249835  [ 2576/ 3200]\n",
      "loss: 0.411351  [ 2592/ 3200]\n",
      "loss: 0.190939  [ 2608/ 3200]\n",
      "loss: 0.239684  [ 2624/ 3200]\n",
      "loss: 0.422918  [ 2640/ 3200]\n",
      "loss: 0.359793  [ 2656/ 3200]\n",
      "loss: 0.299159  [ 2672/ 3200]\n",
      "loss: 0.347126  [ 2688/ 3200]\n",
      "loss: 0.161007  [ 2704/ 3200]\n",
      "loss: 0.458788  [ 2720/ 3200]\n",
      "loss: 0.158188  [ 2736/ 3200]\n",
      "loss: 0.231181  [ 2752/ 3200]\n",
      "loss: 0.085840  [ 2768/ 3200]\n",
      "loss: 0.215413  [ 2784/ 3200]\n",
      "loss: 0.096789  [ 2800/ 3200]\n",
      "loss: 0.178126  [ 2816/ 3200]\n",
      "loss: 0.118639  [ 2832/ 3200]\n",
      "loss: 0.224321  [ 2848/ 3200]\n",
      "loss: 0.161972  [ 2864/ 3200]\n",
      "loss: 0.248364  [ 2880/ 3200]\n",
      "loss: 0.271137  [ 2896/ 3200]\n",
      "loss: 0.511880  [ 2912/ 3200]\n",
      "loss: 0.233234  [ 2928/ 3200]\n",
      "loss: 0.583954  [ 2944/ 3200]\n",
      "loss: 0.353389  [ 2960/ 3200]\n",
      "loss: 0.144631  [ 2976/ 3200]\n",
      "loss: 0.274785  [ 2992/ 3200]\n",
      "loss: 0.792106  [ 3008/ 3200]\n",
      "loss: 0.198392  [ 3024/ 3200]\n",
      "loss: 0.329854  [ 3040/ 3200]\n",
      "loss: 0.133485  [ 3056/ 3200]\n",
      "loss: 0.088998  [ 3072/ 3200]\n",
      "loss: 0.063693  [ 3088/ 3200]\n",
      "loss: 0.259926  [ 3104/ 3200]\n",
      "loss: 0.119992  [ 3120/ 3200]\n",
      "loss: 0.148118  [ 3136/ 3200]\n",
      "loss: 0.259366  [ 3152/ 3200]\n",
      "loss: 0.166519  [ 3168/ 3200]\n",
      "loss: 0.467231  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.039734\n",
      "f1 macro averaged score: 0.779857\n",
      "Accuracy               : 77.9%\n",
      "Confusion matrix       :\n",
      "tensor([[165,  21,   0,  14],\n",
      "        [ 21, 137,  18,  24],\n",
      "        [  0,  18, 170,  12],\n",
      "        [  1,  33,  15, 151]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1125e-02.\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.153463  [    0/ 3200]\n",
      "loss: 0.251053  [   16/ 3200]\n",
      "loss: 0.193192  [   32/ 3200]\n",
      "loss: 0.116339  [   48/ 3200]\n",
      "loss: 0.354688  [   64/ 3200]\n",
      "loss: 0.323630  [   80/ 3200]\n",
      "loss: 0.186123  [   96/ 3200]\n",
      "loss: 0.463654  [  112/ 3200]\n",
      "loss: 0.243665  [  128/ 3200]\n",
      "loss: 0.201823  [  144/ 3200]\n",
      "loss: 0.282719  [  160/ 3200]\n",
      "loss: 0.123975  [  176/ 3200]\n",
      "loss: 0.124998  [  192/ 3200]\n",
      "loss: 0.206426  [  208/ 3200]\n",
      "loss: 0.294700  [  224/ 3200]\n",
      "loss: 0.221007  [  240/ 3200]\n",
      "loss: 0.266766  [  256/ 3200]\n",
      "loss: 0.361441  [  272/ 3200]\n",
      "loss: 0.279725  [  288/ 3200]\n",
      "loss: 0.154208  [  304/ 3200]\n",
      "loss: 0.324847  [  320/ 3200]\n",
      "loss: 0.213018  [  336/ 3200]\n",
      "loss: 0.308687  [  352/ 3200]\n",
      "loss: 0.366653  [  368/ 3200]\n",
      "loss: 0.301079  [  384/ 3200]\n",
      "loss: 0.116776  [  400/ 3200]\n",
      "loss: 0.090664  [  416/ 3200]\n",
      "loss: 0.292497  [  432/ 3200]\n",
      "loss: 0.268872  [  448/ 3200]\n",
      "loss: 0.123902  [  464/ 3200]\n",
      "loss: 0.227238  [  480/ 3200]\n",
      "loss: 0.208963  [  496/ 3200]\n",
      "loss: 0.083136  [  512/ 3200]\n",
      "loss: 0.203270  [  528/ 3200]\n",
      "loss: 0.290866  [  544/ 3200]\n",
      "loss: 0.122944  [  560/ 3200]\n",
      "loss: 0.092157  [  576/ 3200]\n",
      "loss: 0.262785  [  592/ 3200]\n",
      "loss: 0.385768  [  608/ 3200]\n",
      "loss: 0.165002  [  624/ 3200]\n",
      "loss: 0.089519  [  640/ 3200]\n",
      "loss: 0.452060  [  656/ 3200]\n",
      "loss: 0.462555  [  672/ 3200]\n",
      "loss: 0.773863  [  688/ 3200]\n",
      "loss: 0.366373  [  704/ 3200]\n",
      "loss: 0.097560  [  720/ 3200]\n",
      "loss: 0.155118  [  736/ 3200]\n",
      "loss: 0.125124  [  752/ 3200]\n",
      "loss: 0.380775  [  768/ 3200]\n",
      "loss: 0.205408  [  784/ 3200]\n",
      "loss: 0.339643  [  800/ 3200]\n",
      "loss: 0.077705  [  816/ 3200]\n",
      "loss: 0.135114  [  832/ 3200]\n",
      "loss: 0.318564  [  848/ 3200]\n",
      "loss: 0.210105  [  864/ 3200]\n",
      "loss: 0.400321  [  880/ 3200]\n",
      "loss: 0.590499  [  896/ 3200]\n",
      "loss: 0.150184  [  912/ 3200]\n",
      "loss: 0.581435  [  928/ 3200]\n",
      "loss: 0.798460  [  944/ 3200]\n",
      "loss: 0.662583  [  960/ 3200]\n",
      "loss: 0.213222  [  976/ 3200]\n",
      "loss: 0.198824  [  992/ 3200]\n",
      "loss: 0.372032  [ 1008/ 3200]\n",
      "loss: 0.204522  [ 1024/ 3200]\n",
      "loss: 0.427138  [ 1040/ 3200]\n",
      "loss: 0.328137  [ 1056/ 3200]\n",
      "loss: 0.130623  [ 1072/ 3200]\n",
      "loss: 0.148375  [ 1088/ 3200]\n",
      "loss: 0.082958  [ 1104/ 3200]\n",
      "loss: 0.245325  [ 1120/ 3200]\n",
      "loss: 0.243392  [ 1136/ 3200]\n",
      "loss: 0.469876  [ 1152/ 3200]\n",
      "loss: 0.244058  [ 1168/ 3200]\n",
      "loss: 0.190929  [ 1184/ 3200]\n",
      "loss: 0.154367  [ 1200/ 3200]\n",
      "loss: 0.239164  [ 1216/ 3200]\n",
      "loss: 0.132657  [ 1232/ 3200]\n",
      "loss: 0.174383  [ 1248/ 3200]\n",
      "loss: 0.158539  [ 1264/ 3200]\n",
      "loss: 0.074979  [ 1280/ 3200]\n",
      "loss: 0.155754  [ 1296/ 3200]\n",
      "loss: 0.185235  [ 1312/ 3200]\n",
      "loss: 0.267689  [ 1328/ 3200]\n",
      "loss: 0.450128  [ 1344/ 3200]\n",
      "loss: 0.116670  [ 1360/ 3200]\n",
      "loss: 0.201392  [ 1376/ 3200]\n",
      "loss: 0.159540  [ 1392/ 3200]\n",
      "loss: 0.185889  [ 1408/ 3200]\n",
      "loss: 0.181437  [ 1424/ 3200]\n",
      "loss: 0.198325  [ 1440/ 3200]\n",
      "loss: 0.124377  [ 1456/ 3200]\n",
      "loss: 0.181526  [ 1472/ 3200]\n",
      "loss: 0.189553  [ 1488/ 3200]\n",
      "loss: 0.137251  [ 1504/ 3200]\n",
      "loss: 0.150176  [ 1520/ 3200]\n",
      "loss: 0.387382  [ 1536/ 3200]\n",
      "loss: 0.215781  [ 1552/ 3200]\n",
      "loss: 0.328343  [ 1568/ 3200]\n",
      "loss: 0.409308  [ 1584/ 3200]\n",
      "loss: 0.312626  [ 1600/ 3200]\n",
      "loss: 0.160551  [ 1616/ 3200]\n",
      "loss: 0.504115  [ 1632/ 3200]\n",
      "loss: 0.030809  [ 1648/ 3200]\n",
      "loss: 0.317012  [ 1664/ 3200]\n",
      "loss: 0.532022  [ 1680/ 3200]\n",
      "loss: 0.385526  [ 1696/ 3200]\n",
      "loss: 0.132788  [ 1712/ 3200]\n",
      "loss: 0.198087  [ 1728/ 3200]\n",
      "loss: 0.124539  [ 1744/ 3200]\n",
      "loss: 0.070539  [ 1760/ 3200]\n",
      "loss: 0.292651  [ 1776/ 3200]\n",
      "loss: 0.218783  [ 1792/ 3200]\n",
      "loss: 0.614971  [ 1808/ 3200]\n",
      "loss: 0.158480  [ 1824/ 3200]\n",
      "loss: 0.117666  [ 1840/ 3200]\n",
      "loss: 0.147564  [ 1856/ 3200]\n",
      "loss: 0.362815  [ 1872/ 3200]\n",
      "loss: 0.106864  [ 1888/ 3200]\n",
      "loss: 0.353118  [ 1904/ 3200]\n",
      "loss: 0.141973  [ 1920/ 3200]\n",
      "loss: 0.271722  [ 1936/ 3200]\n",
      "loss: 0.533099  [ 1952/ 3200]\n",
      "loss: 0.110627  [ 1968/ 3200]\n",
      "loss: 0.570851  [ 1984/ 3200]\n",
      "loss: 0.126661  [ 2000/ 3200]\n",
      "loss: 0.222141  [ 2016/ 3200]\n",
      "loss: 0.367943  [ 2032/ 3200]\n",
      "loss: 0.282545  [ 2048/ 3200]\n",
      "loss: 0.177022  [ 2064/ 3200]\n",
      "loss: 0.397550  [ 2080/ 3200]\n",
      "loss: 0.180422  [ 2096/ 3200]\n",
      "loss: 0.150191  [ 2112/ 3200]\n",
      "loss: 0.190073  [ 2128/ 3200]\n",
      "loss: 0.595637  [ 2144/ 3200]\n",
      "loss: 0.230026  [ 2160/ 3200]\n",
      "loss: 0.209469  [ 2176/ 3200]\n",
      "loss: 0.302604  [ 2192/ 3200]\n",
      "loss: 0.250647  [ 2208/ 3200]\n",
      "loss: 0.272336  [ 2224/ 3200]\n",
      "loss: 0.351892  [ 2240/ 3200]\n",
      "loss: 0.221795  [ 2256/ 3200]\n",
      "loss: 0.097332  [ 2272/ 3200]\n",
      "loss: 0.260835  [ 2288/ 3200]\n",
      "loss: 0.123396  [ 2304/ 3200]\n",
      "loss: 0.094970  [ 2320/ 3200]\n",
      "loss: 0.158655  [ 2336/ 3200]\n",
      "loss: 0.548112  [ 2352/ 3200]\n",
      "loss: 0.427484  [ 2368/ 3200]\n",
      "loss: 0.096555  [ 2384/ 3200]\n",
      "loss: 0.316060  [ 2400/ 3200]\n",
      "loss: 0.201603  [ 2416/ 3200]\n",
      "loss: 0.318503  [ 2432/ 3200]\n",
      "loss: 0.179629  [ 2448/ 3200]\n",
      "loss: 0.107082  [ 2464/ 3200]\n",
      "loss: 0.151141  [ 2480/ 3200]\n",
      "loss: 0.119382  [ 2496/ 3200]\n",
      "loss: 0.050177  [ 2512/ 3200]\n",
      "loss: 0.099656  [ 2528/ 3200]\n",
      "loss: 0.074815  [ 2544/ 3200]\n",
      "loss: 0.219680  [ 2560/ 3200]\n",
      "loss: 0.265165  [ 2576/ 3200]\n",
      "loss: 0.286590  [ 2592/ 3200]\n",
      "loss: 0.392815  [ 2608/ 3200]\n",
      "loss: 0.283222  [ 2624/ 3200]\n",
      "loss: 0.501127  [ 2640/ 3200]\n",
      "loss: 0.273902  [ 2656/ 3200]\n",
      "loss: 0.313020  [ 2672/ 3200]\n",
      "loss: 0.191816  [ 2688/ 3200]\n",
      "loss: 0.015454  [ 2704/ 3200]\n",
      "loss: 0.377936  [ 2720/ 3200]\n",
      "loss: 0.106965  [ 2736/ 3200]\n",
      "loss: 0.084242  [ 2752/ 3200]\n",
      "loss: 0.341716  [ 2768/ 3200]\n",
      "loss: 0.132880  [ 2784/ 3200]\n",
      "loss: 0.375950  [ 2800/ 3200]\n",
      "loss: 0.333512  [ 2816/ 3200]\n",
      "loss: 0.337158  [ 2832/ 3200]\n",
      "loss: 0.104263  [ 2848/ 3200]\n",
      "loss: 0.057511  [ 2864/ 3200]\n",
      "loss: 0.144544  [ 2880/ 3200]\n",
      "loss: 0.455452  [ 2896/ 3200]\n",
      "loss: 0.477729  [ 2912/ 3200]\n",
      "loss: 0.395784  [ 2928/ 3200]\n",
      "loss: 0.338860  [ 2944/ 3200]\n",
      "loss: 0.151258  [ 2960/ 3200]\n",
      "loss: 0.422238  [ 2976/ 3200]\n",
      "loss: 0.201051  [ 2992/ 3200]\n",
      "loss: 0.245709  [ 3008/ 3200]\n",
      "loss: 0.339769  [ 3024/ 3200]\n",
      "loss: 0.383009  [ 3040/ 3200]\n",
      "loss: 0.283107  [ 3056/ 3200]\n",
      "loss: 0.235953  [ 3072/ 3200]\n",
      "loss: 0.232550  [ 3088/ 3200]\n",
      "loss: 0.251058  [ 3104/ 3200]\n",
      "loss: 0.267633  [ 3120/ 3200]\n",
      "loss: 0.360088  [ 3136/ 3200]\n",
      "loss: 0.507596  [ 3152/ 3200]\n",
      "loss: 0.279674  [ 3168/ 3200]\n",
      "loss: 0.674993  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.041273\n",
      "f1 macro averaged score: 0.799686\n",
      "Accuracy               : 80.0%\n",
      "Confusion matrix       :\n",
      "tensor([[186,   9,   0,   5],\n",
      "        [ 38, 141,  10,  11],\n",
      "        [  1,  24, 166,   9],\n",
      "        [ 11,  33,   9, 147]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1170e-02.\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 0.252172  [    0/ 3200]\n",
      "loss: 0.323714  [   16/ 3200]\n",
      "loss: 0.238527  [   32/ 3200]\n",
      "loss: 0.095621  [   48/ 3200]\n",
      "loss: 0.214108  [   64/ 3200]\n",
      "loss: 0.252306  [   80/ 3200]\n",
      "loss: 0.270574  [   96/ 3200]\n",
      "loss: 0.254180  [  112/ 3200]\n",
      "loss: 0.337583  [  128/ 3200]\n",
      "loss: 0.349030  [  144/ 3200]\n",
      "loss: 0.170043  [  160/ 3200]\n",
      "loss: 0.153554  [  176/ 3200]\n",
      "loss: 0.225572  [  192/ 3200]\n",
      "loss: 0.135326  [  208/ 3200]\n",
      "loss: 0.102731  [  224/ 3200]\n",
      "loss: 0.220179  [  240/ 3200]\n",
      "loss: 0.261897  [  256/ 3200]\n",
      "loss: 0.153733  [  272/ 3200]\n",
      "loss: 0.282317  [  288/ 3200]\n",
      "loss: 0.146869  [  304/ 3200]\n",
      "loss: 0.209298  [  320/ 3200]\n",
      "loss: 0.227135  [  336/ 3200]\n",
      "loss: 0.369958  [  352/ 3200]\n",
      "loss: 0.171934  [  368/ 3200]\n",
      "loss: 0.151766  [  384/ 3200]\n",
      "loss: 0.242664  [  400/ 3200]\n",
      "loss: 0.222067  [  416/ 3200]\n",
      "loss: 0.195194  [  432/ 3200]\n",
      "loss: 0.195705  [  448/ 3200]\n",
      "loss: 0.323090  [  464/ 3200]\n",
      "loss: 0.356256  [  480/ 3200]\n",
      "loss: 0.269416  [  496/ 3200]\n",
      "loss: 0.144515  [  512/ 3200]\n",
      "loss: 0.222561  [  528/ 3200]\n",
      "loss: 0.232378  [  544/ 3200]\n",
      "loss: 0.062507  [  560/ 3200]\n",
      "loss: 0.162650  [  576/ 3200]\n",
      "loss: 0.144421  [  592/ 3200]\n",
      "loss: 0.189644  [  608/ 3200]\n",
      "loss: 0.334443  [  624/ 3200]\n",
      "loss: 0.071573  [  640/ 3200]\n",
      "loss: 0.768492  [  656/ 3200]\n",
      "loss: 0.256920  [  672/ 3200]\n",
      "loss: 0.249151  [  688/ 3200]\n",
      "loss: 0.357780  [  704/ 3200]\n",
      "loss: 0.068993  [  720/ 3200]\n",
      "loss: 0.106893  [  736/ 3200]\n",
      "loss: 0.109351  [  752/ 3200]\n",
      "loss: 0.148701  [  768/ 3200]\n",
      "loss: 0.251671  [  784/ 3200]\n",
      "loss: 0.238492  [  800/ 3200]\n",
      "loss: 0.250913  [  816/ 3200]\n",
      "loss: 0.200745  [  832/ 3200]\n",
      "loss: 0.137565  [  848/ 3200]\n",
      "loss: 0.156045  [  864/ 3200]\n",
      "loss: 0.251909  [  880/ 3200]\n",
      "loss: 0.138584  [  896/ 3200]\n",
      "loss: 0.291641  [  912/ 3200]\n",
      "loss: 0.358303  [  928/ 3200]\n",
      "loss: 0.217890  [  944/ 3200]\n",
      "loss: 0.083369  [  960/ 3200]\n",
      "loss: 0.343458  [  976/ 3200]\n",
      "loss: 0.351669  [  992/ 3200]\n",
      "loss: 0.090595  [ 1008/ 3200]\n",
      "loss: 0.440268  [ 1024/ 3200]\n",
      "loss: 0.222633  [ 1040/ 3200]\n",
      "loss: 0.874699  [ 1056/ 3200]\n",
      "loss: 0.308207  [ 1072/ 3200]\n",
      "loss: 0.100435  [ 1088/ 3200]\n",
      "loss: 0.434279  [ 1104/ 3200]\n",
      "loss: 0.088162  [ 1120/ 3200]\n",
      "loss: 0.110879  [ 1136/ 3200]\n",
      "loss: 0.014245  [ 1152/ 3200]\n",
      "loss: 0.122836  [ 1168/ 3200]\n",
      "loss: 0.317513  [ 1184/ 3200]\n",
      "loss: 0.232799  [ 1200/ 3200]\n",
      "loss: 0.096672  [ 1216/ 3200]\n",
      "loss: 0.129938  [ 1232/ 3200]\n",
      "loss: 0.191799  [ 1248/ 3200]\n",
      "loss: 0.370236  [ 1264/ 3200]\n",
      "loss: 0.039757  [ 1280/ 3200]\n",
      "loss: 0.033396  [ 1296/ 3200]\n",
      "loss: 0.175110  [ 1312/ 3200]\n",
      "loss: 0.285822  [ 1328/ 3200]\n",
      "loss: 0.375459  [ 1344/ 3200]\n",
      "loss: 0.199217  [ 1360/ 3200]\n",
      "loss: 0.289483  [ 1376/ 3200]\n",
      "loss: 0.506737  [ 1392/ 3200]\n",
      "loss: 0.233632  [ 1408/ 3200]\n",
      "loss: 0.108437  [ 1424/ 3200]\n",
      "loss: 0.337206  [ 1440/ 3200]\n",
      "loss: 0.492224  [ 1456/ 3200]\n",
      "loss: 0.460309  [ 1472/ 3200]\n",
      "loss: 0.177376  [ 1488/ 3200]\n",
      "loss: 0.112686  [ 1504/ 3200]\n",
      "loss: 0.232459  [ 1520/ 3200]\n",
      "loss: 0.206906  [ 1536/ 3200]\n",
      "loss: 0.647011  [ 1552/ 3200]\n",
      "loss: 0.025127  [ 1568/ 3200]\n",
      "loss: 0.231313  [ 1584/ 3200]\n",
      "loss: 0.091792  [ 1600/ 3200]\n",
      "loss: 0.177340  [ 1616/ 3200]\n",
      "loss: 0.285668  [ 1632/ 3200]\n",
      "loss: 0.205703  [ 1648/ 3200]\n",
      "loss: 0.166785  [ 1664/ 3200]\n",
      "loss: 0.322453  [ 1680/ 3200]\n",
      "loss: 0.598392  [ 1696/ 3200]\n",
      "loss: 0.549697  [ 1712/ 3200]\n",
      "loss: 0.104752  [ 1728/ 3200]\n",
      "loss: 0.138601  [ 1744/ 3200]\n",
      "loss: 0.527060  [ 1760/ 3200]\n",
      "loss: 0.132561  [ 1776/ 3200]\n",
      "loss: 0.263648  [ 1792/ 3200]\n",
      "loss: 0.169633  [ 1808/ 3200]\n",
      "loss: 0.278971  [ 1824/ 3200]\n",
      "loss: 0.392612  [ 1840/ 3200]\n",
      "loss: 0.174863  [ 1856/ 3200]\n",
      "loss: 0.458736  [ 1872/ 3200]\n",
      "loss: 0.101258  [ 1888/ 3200]\n",
      "loss: 0.292249  [ 1904/ 3200]\n",
      "loss: 0.294867  [ 1920/ 3200]\n",
      "loss: 0.162266  [ 1936/ 3200]\n",
      "loss: 0.186810  [ 1952/ 3200]\n",
      "loss: 0.170687  [ 1968/ 3200]\n",
      "loss: 0.106970  [ 1984/ 3200]\n",
      "loss: 0.203162  [ 2000/ 3200]\n",
      "loss: 0.045352  [ 2016/ 3200]\n",
      "loss: 0.062952  [ 2032/ 3200]\n",
      "loss: 0.362323  [ 2048/ 3200]\n",
      "loss: 0.114415  [ 2064/ 3200]\n",
      "loss: 0.108683  [ 2080/ 3200]\n",
      "loss: 0.367798  [ 2096/ 3200]\n",
      "loss: 0.360103  [ 2112/ 3200]\n",
      "loss: 0.103361  [ 2128/ 3200]\n",
      "loss: 0.536685  [ 2144/ 3200]\n",
      "loss: 0.367501  [ 2160/ 3200]\n",
      "loss: 0.228298  [ 2176/ 3200]\n",
      "loss: 0.282717  [ 2192/ 3200]\n",
      "loss: 0.423095  [ 2208/ 3200]\n",
      "loss: 0.353997  [ 2224/ 3200]\n",
      "loss: 0.444305  [ 2240/ 3200]\n",
      "loss: 0.160698  [ 2256/ 3200]\n",
      "loss: 0.251407  [ 2272/ 3200]\n",
      "loss: 0.044406  [ 2288/ 3200]\n",
      "loss: 0.646096  [ 2304/ 3200]\n",
      "loss: 0.193761  [ 2320/ 3200]\n",
      "loss: 0.123138  [ 2336/ 3200]\n",
      "loss: 0.182284  [ 2352/ 3200]\n",
      "loss: 0.068410  [ 2368/ 3200]\n",
      "loss: 0.243162  [ 2384/ 3200]\n",
      "loss: 0.239699  [ 2400/ 3200]\n",
      "loss: 0.166086  [ 2416/ 3200]\n",
      "loss: 0.541873  [ 2432/ 3200]\n",
      "loss: 0.435997  [ 2448/ 3200]\n",
      "loss: 0.155759  [ 2464/ 3200]\n",
      "loss: 0.368906  [ 2480/ 3200]\n",
      "loss: 0.381190  [ 2496/ 3200]\n",
      "loss: 0.269438  [ 2512/ 3200]\n",
      "loss: 0.247230  [ 2528/ 3200]\n",
      "loss: 0.142478  [ 2544/ 3200]\n",
      "loss: 0.082741  [ 2560/ 3200]\n",
      "loss: 0.089050  [ 2576/ 3200]\n",
      "loss: 0.419619  [ 2592/ 3200]\n",
      "loss: 0.338644  [ 2608/ 3200]\n",
      "loss: 0.269813  [ 2624/ 3200]\n",
      "loss: 0.363273  [ 2640/ 3200]\n",
      "loss: 0.418757  [ 2656/ 3200]\n",
      "loss: 0.221393  [ 2672/ 3200]\n",
      "loss: 0.510946  [ 2688/ 3200]\n",
      "loss: 0.570806  [ 2704/ 3200]\n",
      "loss: 0.265980  [ 2720/ 3200]\n",
      "loss: 0.150841  [ 2736/ 3200]\n",
      "loss: 0.174718  [ 2752/ 3200]\n",
      "loss: 0.101087  [ 2768/ 3200]\n",
      "loss: 0.318213  [ 2784/ 3200]\n",
      "loss: 0.188190  [ 2800/ 3200]\n",
      "loss: 0.055991  [ 2816/ 3200]\n",
      "loss: 0.180284  [ 2832/ 3200]\n",
      "loss: 0.324958  [ 2848/ 3200]\n",
      "loss: 0.194035  [ 2864/ 3200]\n",
      "loss: 0.274698  [ 2880/ 3200]\n",
      "loss: 0.105909  [ 2896/ 3200]\n",
      "loss: 0.207051  [ 2912/ 3200]\n",
      "loss: 0.232708  [ 2928/ 3200]\n",
      "loss: 0.623872  [ 2944/ 3200]\n",
      "loss: 0.105848  [ 2960/ 3200]\n",
      "loss: 0.541430  [ 2976/ 3200]\n",
      "loss: 0.114835  [ 2992/ 3200]\n",
      "loss: 0.338690  [ 3008/ 3200]\n",
      "loss: 0.227621  [ 3024/ 3200]\n",
      "loss: 0.425135  [ 3040/ 3200]\n",
      "loss: 0.239501  [ 3056/ 3200]\n",
      "loss: 0.454644  [ 3072/ 3200]\n",
      "loss: 0.269517  [ 3088/ 3200]\n",
      "loss: 0.324061  [ 3104/ 3200]\n",
      "loss: 0.107237  [ 3120/ 3200]\n",
      "loss: 0.314003  [ 3136/ 3200]\n",
      "loss: 0.192191  [ 3152/ 3200]\n",
      "loss: 0.341608  [ 3168/ 3200]\n",
      "loss: 0.412368  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.047176\n",
      "f1 macro averaged score: 0.755023\n",
      "Accuracy               : 75.0%\n",
      "Confusion matrix       :\n",
      "tensor([[169,  23,   0,   8],\n",
      "        [ 16, 171,   3,  10],\n",
      "        [  0,  45, 130,  25],\n",
      "        [  4,  62,   4, 130]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1215e-02.\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.178239  [    0/ 3200]\n",
      "loss: 0.158123  [   16/ 3200]\n",
      "loss: 0.128538  [   32/ 3200]\n",
      "loss: 0.108048  [   48/ 3200]\n",
      "loss: 0.110910  [   64/ 3200]\n",
      "loss: 0.311688  [   80/ 3200]\n",
      "loss: 0.292007  [   96/ 3200]\n",
      "loss: 0.065945  [  112/ 3200]\n",
      "loss: 0.157806  [  128/ 3200]\n",
      "loss: 0.202823  [  144/ 3200]\n",
      "loss: 0.285855  [  160/ 3200]\n",
      "loss: 0.390812  [  176/ 3200]\n",
      "loss: 0.151443  [  192/ 3200]\n",
      "loss: 0.055304  [  208/ 3200]\n",
      "loss: 0.226821  [  224/ 3200]\n",
      "loss: 0.177604  [  240/ 3200]\n",
      "loss: 0.241795  [  256/ 3200]\n",
      "loss: 0.132364  [  272/ 3200]\n",
      "loss: 0.079257  [  288/ 3200]\n",
      "loss: 0.662404  [  304/ 3200]\n",
      "loss: 0.228034  [  320/ 3200]\n",
      "loss: 0.150650  [  336/ 3200]\n",
      "loss: 0.047790  [  352/ 3200]\n",
      "loss: 0.704497  [  368/ 3200]\n",
      "loss: 0.309229  [  384/ 3200]\n",
      "loss: 0.352241  [  400/ 3200]\n",
      "loss: 0.271683  [  416/ 3200]\n",
      "loss: 0.237692  [  432/ 3200]\n",
      "loss: 0.486133  [  448/ 3200]\n",
      "loss: 0.233116  [  464/ 3200]\n",
      "loss: 0.096535  [  480/ 3200]\n",
      "loss: 0.310669  [  496/ 3200]\n",
      "loss: 0.121646  [  512/ 3200]\n",
      "loss: 0.117329  [  528/ 3200]\n",
      "loss: 0.046974  [  544/ 3200]\n",
      "loss: 0.117781  [  560/ 3200]\n",
      "loss: 0.552417  [  576/ 3200]\n",
      "loss: 0.114013  [  592/ 3200]\n",
      "loss: 0.080445  [  608/ 3200]\n",
      "loss: 0.114233  [  624/ 3200]\n",
      "loss: 0.301926  [  640/ 3200]\n",
      "loss: 0.243636  [  656/ 3200]\n",
      "loss: 0.147633  [  672/ 3200]\n",
      "loss: 0.249748  [  688/ 3200]\n",
      "loss: 0.074751  [  704/ 3200]\n",
      "loss: 0.053964  [  720/ 3200]\n",
      "loss: 0.311135  [  736/ 3200]\n",
      "loss: 0.018086  [  752/ 3200]\n",
      "loss: 0.529411  [  768/ 3200]\n",
      "loss: 0.068885  [  784/ 3200]\n",
      "loss: 0.091714  [  800/ 3200]\n",
      "loss: 0.117041  [  816/ 3200]\n",
      "loss: 0.087102  [  832/ 3200]\n",
      "loss: 0.466693  [  848/ 3200]\n",
      "loss: 0.266911  [  864/ 3200]\n",
      "loss: 0.425736  [  880/ 3200]\n",
      "loss: 0.129356  [  896/ 3200]\n",
      "loss: 0.241623  [  912/ 3200]\n",
      "loss: 0.160940  [  928/ 3200]\n",
      "loss: 0.479285  [  944/ 3200]\n",
      "loss: 0.189912  [  960/ 3200]\n",
      "loss: 0.199831  [  976/ 3200]\n",
      "loss: 0.270501  [  992/ 3200]\n",
      "loss: 0.303274  [ 1008/ 3200]\n",
      "loss: 0.334724  [ 1024/ 3200]\n",
      "loss: 1.045155  [ 1040/ 3200]\n",
      "loss: 0.230745  [ 1056/ 3200]\n",
      "loss: 0.294414  [ 1072/ 3200]\n",
      "loss: 0.079577  [ 1088/ 3200]\n",
      "loss: 0.222611  [ 1104/ 3200]\n",
      "loss: 0.153347  [ 1120/ 3200]\n",
      "loss: 0.078607  [ 1136/ 3200]\n",
      "loss: 0.135241  [ 1152/ 3200]\n",
      "loss: 0.309992  [ 1168/ 3200]\n",
      "loss: 0.490254  [ 1184/ 3200]\n",
      "loss: 0.174963  [ 1200/ 3200]\n",
      "loss: 0.055588  [ 1216/ 3200]\n",
      "loss: 0.202805  [ 1232/ 3200]\n",
      "loss: 0.106783  [ 1248/ 3200]\n",
      "loss: 0.226035  [ 1264/ 3200]\n",
      "loss: 0.195914  [ 1280/ 3200]\n",
      "loss: 0.180882  [ 1296/ 3200]\n",
      "loss: 0.127052  [ 1312/ 3200]\n",
      "loss: 0.500812  [ 1328/ 3200]\n",
      "loss: 0.331221  [ 1344/ 3200]\n",
      "loss: 0.134220  [ 1360/ 3200]\n",
      "loss: 0.224793  [ 1376/ 3200]\n",
      "loss: 0.108183  [ 1392/ 3200]\n",
      "loss: 0.245204  [ 1408/ 3200]\n",
      "loss: 0.100380  [ 1424/ 3200]\n",
      "loss: 0.152055  [ 1440/ 3200]\n",
      "loss: 0.157440  [ 1456/ 3200]\n",
      "loss: 0.106975  [ 1472/ 3200]\n",
      "loss: 0.059997  [ 1488/ 3200]\n",
      "loss: 0.175899  [ 1504/ 3200]\n",
      "loss: 0.100918  [ 1520/ 3200]\n",
      "loss: 0.306817  [ 1536/ 3200]\n",
      "loss: 0.284029  [ 1552/ 3200]\n",
      "loss: 0.380436  [ 1568/ 3200]\n",
      "loss: 0.057163  [ 1584/ 3200]\n",
      "loss: 0.289845  [ 1600/ 3200]\n",
      "loss: 0.051285  [ 1616/ 3200]\n",
      "loss: 0.102165  [ 1632/ 3200]\n",
      "loss: 0.142573  [ 1648/ 3200]\n",
      "loss: 0.161713  [ 1664/ 3200]\n",
      "loss: 0.061453  [ 1680/ 3200]\n",
      "loss: 0.460079  [ 1696/ 3200]\n",
      "loss: 0.123995  [ 1712/ 3200]\n",
      "loss: 0.114139  [ 1728/ 3200]\n",
      "loss: 0.122864  [ 1744/ 3200]\n",
      "loss: 0.266591  [ 1760/ 3200]\n",
      "loss: 0.260061  [ 1776/ 3200]\n",
      "loss: 0.097990  [ 1792/ 3200]\n",
      "loss: 0.108096  [ 1808/ 3200]\n",
      "loss: 0.148873  [ 1824/ 3200]\n",
      "loss: 0.248004  [ 1840/ 3200]\n",
      "loss: 0.038990  [ 1856/ 3200]\n",
      "loss: 0.292236  [ 1872/ 3200]\n",
      "loss: 0.154571  [ 1888/ 3200]\n",
      "loss: 0.455707  [ 1904/ 3200]\n",
      "loss: 0.095261  [ 1920/ 3200]\n",
      "loss: 0.100694  [ 1936/ 3200]\n",
      "loss: 0.131091  [ 1952/ 3200]\n",
      "loss: 0.133975  [ 1968/ 3200]\n",
      "loss: 0.456786  [ 1984/ 3200]\n",
      "loss: 0.466222  [ 2000/ 3200]\n",
      "loss: 0.285421  [ 2016/ 3200]\n",
      "loss: 0.085438  [ 2032/ 3200]\n",
      "loss: 0.140189  [ 2048/ 3200]\n",
      "loss: 0.057837  [ 2064/ 3200]\n",
      "loss: 0.242784  [ 2080/ 3200]\n",
      "loss: 0.196859  [ 2096/ 3200]\n",
      "loss: 0.063443  [ 2112/ 3200]\n",
      "loss: 0.107118  [ 2128/ 3200]\n",
      "loss: 0.181525  [ 2144/ 3200]\n",
      "loss: 0.302006  [ 2160/ 3200]\n",
      "loss: 0.526249  [ 2176/ 3200]\n",
      "loss: 0.213922  [ 2192/ 3200]\n",
      "loss: 0.122601  [ 2208/ 3200]\n",
      "loss: 0.311325  [ 2224/ 3200]\n",
      "loss: 0.549937  [ 2240/ 3200]\n",
      "loss: 0.220713  [ 2256/ 3200]\n",
      "loss: 0.074603  [ 2272/ 3200]\n",
      "loss: 0.065045  [ 2288/ 3200]\n",
      "loss: 0.149877  [ 2304/ 3200]\n",
      "loss: 0.262159  [ 2320/ 3200]\n",
      "loss: 0.131328  [ 2336/ 3200]\n",
      "loss: 0.382738  [ 2352/ 3200]\n",
      "loss: 0.306571  [ 2368/ 3200]\n",
      "loss: 0.442876  [ 2384/ 3200]\n",
      "loss: 0.296295  [ 2400/ 3200]\n",
      "loss: 0.138896  [ 2416/ 3200]\n",
      "loss: 0.127909  [ 2432/ 3200]\n",
      "loss: 0.021360  [ 2448/ 3200]\n",
      "loss: 0.313973  [ 2464/ 3200]\n",
      "loss: 0.501261  [ 2480/ 3200]\n",
      "loss: 0.545684  [ 2496/ 3200]\n",
      "loss: 0.587412  [ 2512/ 3200]\n",
      "loss: 0.089581  [ 2528/ 3200]\n",
      "loss: 0.078266  [ 2544/ 3200]\n",
      "loss: 0.144146  [ 2560/ 3200]\n",
      "loss: 0.257631  [ 2576/ 3200]\n",
      "loss: 0.124978  [ 2592/ 3200]\n",
      "loss: 0.502841  [ 2608/ 3200]\n",
      "loss: 0.669825  [ 2624/ 3200]\n",
      "loss: 0.234328  [ 2640/ 3200]\n",
      "loss: 0.680591  [ 2656/ 3200]\n",
      "loss: 0.222376  [ 2672/ 3200]\n",
      "loss: 0.387838  [ 2688/ 3200]\n",
      "loss: 0.106091  [ 2704/ 3200]\n",
      "loss: 0.091284  [ 2720/ 3200]\n",
      "loss: 0.048474  [ 2736/ 3200]\n",
      "loss: 0.114383  [ 2752/ 3200]\n",
      "loss: 0.146035  [ 2768/ 3200]\n",
      "loss: 0.304034  [ 2784/ 3200]\n",
      "loss: 0.074665  [ 2800/ 3200]\n",
      "loss: 0.243665  [ 2816/ 3200]\n",
      "loss: 0.174638  [ 2832/ 3200]\n",
      "loss: 0.120324  [ 2848/ 3200]\n",
      "loss: 0.185003  [ 2864/ 3200]\n",
      "loss: 0.166396  [ 2880/ 3200]\n",
      "loss: 0.399638  [ 2896/ 3200]\n",
      "loss: 0.225822  [ 2912/ 3200]\n",
      "loss: 0.121410  [ 2928/ 3200]\n",
      "loss: 0.300835  [ 2944/ 3200]\n",
      "loss: 0.103584  [ 2960/ 3200]\n",
      "loss: 0.232388  [ 2976/ 3200]\n",
      "loss: 0.463581  [ 2992/ 3200]\n",
      "loss: 0.407146  [ 3008/ 3200]\n",
      "loss: 0.023350  [ 3024/ 3200]\n",
      "loss: 0.173977  [ 3040/ 3200]\n",
      "loss: 0.155698  [ 3056/ 3200]\n",
      "loss: 0.059405  [ 3072/ 3200]\n",
      "loss: 0.063066  [ 3088/ 3200]\n",
      "loss: 0.220768  [ 3104/ 3200]\n",
      "loss: 0.130616  [ 3120/ 3200]\n",
      "loss: 0.116995  [ 3136/ 3200]\n",
      "loss: 0.694519  [ 3152/ 3200]\n",
      "loss: 0.104648  [ 3168/ 3200]\n",
      "loss: 0.384916  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.045165\n",
      "f1 macro averaged score: 0.792323\n",
      "Accuracy               : 79.1%\n",
      "Confusion matrix       :\n",
      "tensor([[153,  26,   0,  21],\n",
      "        [ 19, 142,  20,  19],\n",
      "        [  0,  13, 175,  12],\n",
      "        [  0,  29,   8, 163]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1260e-02.\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 0.069639  [    0/ 3200]\n",
      "loss: 0.130972  [   16/ 3200]\n",
      "loss: 0.138058  [   32/ 3200]\n",
      "loss: 0.192667  [   48/ 3200]\n",
      "loss: 0.566703  [   64/ 3200]\n",
      "loss: 0.404142  [   80/ 3200]\n",
      "loss: 0.250900  [   96/ 3200]\n",
      "loss: 0.169447  [  112/ 3200]\n",
      "loss: 0.283746  [  128/ 3200]\n",
      "loss: 0.201479  [  144/ 3200]\n",
      "loss: 0.309298  [  160/ 3200]\n",
      "loss: 0.140294  [  176/ 3200]\n",
      "loss: 0.171989  [  192/ 3200]\n",
      "loss: 0.341615  [  208/ 3200]\n",
      "loss: 0.095648  [  224/ 3200]\n",
      "loss: 0.216612  [  240/ 3200]\n",
      "loss: 0.225884  [  256/ 3200]\n",
      "loss: 0.080439  [  272/ 3200]\n",
      "loss: 0.362674  [  288/ 3200]\n",
      "loss: 0.201259  [  304/ 3200]\n",
      "loss: 0.167472  [  320/ 3200]\n",
      "loss: 0.144889  [  336/ 3200]\n",
      "loss: 0.100007  [  352/ 3200]\n",
      "loss: 0.091883  [  368/ 3200]\n",
      "loss: 0.046531  [  384/ 3200]\n",
      "loss: 0.647694  [  400/ 3200]\n",
      "loss: 0.377891  [  416/ 3200]\n",
      "loss: 0.189870  [  432/ 3200]\n",
      "loss: 0.124460  [  448/ 3200]\n",
      "loss: 0.021222  [  464/ 3200]\n",
      "loss: 0.299513  [  480/ 3200]\n",
      "loss: 0.099418  [  496/ 3200]\n",
      "loss: 0.121165  [  512/ 3200]\n",
      "loss: 0.381470  [  528/ 3200]\n",
      "loss: 0.361595  [  544/ 3200]\n",
      "loss: 0.029848  [  560/ 3200]\n",
      "loss: 0.264203  [  576/ 3200]\n",
      "loss: 0.154355  [  592/ 3200]\n",
      "loss: 0.057622  [  608/ 3200]\n",
      "loss: 0.175238  [  624/ 3200]\n",
      "loss: 0.276571  [  640/ 3200]\n",
      "loss: 0.245834  [  656/ 3200]\n",
      "loss: 0.288291  [  672/ 3200]\n",
      "loss: 0.443881  [  688/ 3200]\n",
      "loss: 0.234848  [  704/ 3200]\n",
      "loss: 0.275450  [  720/ 3200]\n",
      "loss: 0.086458  [  736/ 3200]\n",
      "loss: 0.103608  [  752/ 3200]\n",
      "loss: 0.039820  [  768/ 3200]\n",
      "loss: 0.404847  [  784/ 3200]\n",
      "loss: 0.071169  [  800/ 3200]\n",
      "loss: 0.041767  [  816/ 3200]\n",
      "loss: 0.129677  [  832/ 3200]\n",
      "loss: 0.398493  [  848/ 3200]\n",
      "loss: 0.043394  [  864/ 3200]\n",
      "loss: 0.073126  [  880/ 3200]\n",
      "loss: 0.235910  [  896/ 3200]\n",
      "loss: 0.473915  [  912/ 3200]\n",
      "loss: 0.062748  [  928/ 3200]\n",
      "loss: 0.199063  [  944/ 3200]\n",
      "loss: 0.372016  [  960/ 3200]\n",
      "loss: 0.257058  [  976/ 3200]\n",
      "loss: 0.174639  [  992/ 3200]\n",
      "loss: 0.137169  [ 1008/ 3200]\n",
      "loss: 0.186717  [ 1024/ 3200]\n",
      "loss: 0.154915  [ 1040/ 3200]\n",
      "loss: 0.263061  [ 1056/ 3200]\n",
      "loss: 0.175267  [ 1072/ 3200]\n",
      "loss: 0.230916  [ 1088/ 3200]\n",
      "loss: 0.046205  [ 1104/ 3200]\n",
      "loss: 0.214174  [ 1120/ 3200]\n",
      "loss: 0.364349  [ 1136/ 3200]\n",
      "loss: 0.125386  [ 1152/ 3200]\n",
      "loss: 0.320152  [ 1168/ 3200]\n",
      "loss: 0.394976  [ 1184/ 3200]\n",
      "loss: 1.058801  [ 1200/ 3200]\n",
      "loss: 0.449469  [ 1216/ 3200]\n",
      "loss: 0.238732  [ 1232/ 3200]\n",
      "loss: 0.106469  [ 1248/ 3200]\n",
      "loss: 0.124347  [ 1264/ 3200]\n",
      "loss: 0.340754  [ 1280/ 3200]\n",
      "loss: 0.343822  [ 1296/ 3200]\n",
      "loss: 0.143337  [ 1312/ 3200]\n",
      "loss: 0.102165  [ 1328/ 3200]\n",
      "loss: 0.195128  [ 1344/ 3200]\n",
      "loss: 0.062902  [ 1360/ 3200]\n",
      "loss: 0.027740  [ 1376/ 3200]\n",
      "loss: 0.174841  [ 1392/ 3200]\n",
      "loss: 0.482500  [ 1408/ 3200]\n",
      "loss: 0.044597  [ 1424/ 3200]\n",
      "loss: 0.177491  [ 1440/ 3200]\n",
      "loss: 0.387015  [ 1456/ 3200]\n",
      "loss: 0.119272  [ 1472/ 3200]\n",
      "loss: 0.192286  [ 1488/ 3200]\n",
      "loss: 0.174953  [ 1504/ 3200]\n",
      "loss: 0.196634  [ 1520/ 3200]\n",
      "loss: 0.089644  [ 1536/ 3200]\n",
      "loss: 0.274765  [ 1552/ 3200]\n",
      "loss: 0.116325  [ 1568/ 3200]\n",
      "loss: 0.162032  [ 1584/ 3200]\n",
      "loss: 0.104545  [ 1600/ 3200]\n",
      "loss: 0.090687  [ 1616/ 3200]\n",
      "loss: 0.188896  [ 1632/ 3200]\n",
      "loss: 0.159902  [ 1648/ 3200]\n",
      "loss: 0.145848  [ 1664/ 3200]\n",
      "loss: 0.153111  [ 1680/ 3200]\n",
      "loss: 0.111995  [ 1696/ 3200]\n",
      "loss: 0.203325  [ 1712/ 3200]\n",
      "loss: 0.205540  [ 1728/ 3200]\n",
      "loss: 0.259378  [ 1744/ 3200]\n",
      "loss: 0.366574  [ 1760/ 3200]\n",
      "loss: 0.136584  [ 1776/ 3200]\n",
      "loss: 0.311190  [ 1792/ 3200]\n",
      "loss: 0.566710  [ 1808/ 3200]\n",
      "loss: 0.546384  [ 1824/ 3200]\n",
      "loss: 0.158393  [ 1840/ 3200]\n",
      "loss: 0.180569  [ 1856/ 3200]\n",
      "loss: 0.027499  [ 1872/ 3200]\n",
      "loss: 0.612207  [ 1888/ 3200]\n",
      "loss: 0.326884  [ 1904/ 3200]\n",
      "loss: 0.087485  [ 1920/ 3200]\n",
      "loss: 0.094455  [ 1936/ 3200]\n",
      "loss: 0.117199  [ 1952/ 3200]\n",
      "loss: 0.149370  [ 1968/ 3200]\n",
      "loss: 0.230253  [ 1984/ 3200]\n",
      "loss: 0.444629  [ 2000/ 3200]\n",
      "loss: 0.203566  [ 2016/ 3200]\n",
      "loss: 0.134461  [ 2032/ 3200]\n",
      "loss: 0.244009  [ 2048/ 3200]\n",
      "loss: 0.051680  [ 2064/ 3200]\n",
      "loss: 0.258046  [ 2080/ 3200]\n",
      "loss: 0.247258  [ 2096/ 3200]\n",
      "loss: 0.126090  [ 2112/ 3200]\n",
      "loss: 0.232269  [ 2128/ 3200]\n",
      "loss: 0.209771  [ 2144/ 3200]\n",
      "loss: 0.105487  [ 2160/ 3200]\n",
      "loss: 0.081070  [ 2176/ 3200]\n",
      "loss: 0.086347  [ 2192/ 3200]\n",
      "loss: 0.026861  [ 2208/ 3200]\n",
      "loss: 0.110882  [ 2224/ 3200]\n",
      "loss: 0.107553  [ 2240/ 3200]\n",
      "loss: 0.033387  [ 2256/ 3200]\n",
      "loss: 0.248704  [ 2272/ 3200]\n",
      "loss: 0.140432  [ 2288/ 3200]\n",
      "loss: 0.253700  [ 2304/ 3200]\n",
      "loss: 0.442162  [ 2320/ 3200]\n",
      "loss: 0.119885  [ 2336/ 3200]\n",
      "loss: 0.184701  [ 2352/ 3200]\n",
      "loss: 0.076422  [ 2368/ 3200]\n",
      "loss: 0.141680  [ 2384/ 3200]\n",
      "loss: 0.093105  [ 2400/ 3200]\n",
      "loss: 0.214589  [ 2416/ 3200]\n",
      "loss: 0.574115  [ 2432/ 3200]\n",
      "loss: 0.200718  [ 2448/ 3200]\n",
      "loss: 0.153445  [ 2464/ 3200]\n",
      "loss: 0.125684  [ 2480/ 3200]\n",
      "loss: 0.169329  [ 2496/ 3200]\n",
      "loss: 0.069707  [ 2512/ 3200]\n",
      "loss: 0.144780  [ 2528/ 3200]\n",
      "loss: 0.248836  [ 2544/ 3200]\n",
      "loss: 0.091280  [ 2560/ 3200]\n",
      "loss: 0.104826  [ 2576/ 3200]\n",
      "loss: 0.265555  [ 2592/ 3200]\n",
      "loss: 0.513171  [ 2608/ 3200]\n",
      "loss: 0.337506  [ 2624/ 3200]\n",
      "loss: 0.343027  [ 2640/ 3200]\n",
      "loss: 0.077700  [ 2656/ 3200]\n",
      "loss: 0.199059  [ 2672/ 3200]\n",
      "loss: 0.060913  [ 2688/ 3200]\n",
      "loss: 0.221672  [ 2704/ 3200]\n",
      "loss: 0.127249  [ 2720/ 3200]\n",
      "loss: 0.311881  [ 2736/ 3200]\n",
      "loss: 0.285208  [ 2752/ 3200]\n",
      "loss: 0.042362  [ 2768/ 3200]\n",
      "loss: 0.539011  [ 2784/ 3200]\n",
      "loss: 0.374318  [ 2800/ 3200]\n",
      "loss: 0.165426  [ 2816/ 3200]\n",
      "loss: 0.147432  [ 2832/ 3200]\n",
      "loss: 0.101760  [ 2848/ 3200]\n",
      "loss: 0.147596  [ 2864/ 3200]\n",
      "loss: 0.061275  [ 2880/ 3200]\n",
      "loss: 0.127851  [ 2896/ 3200]\n",
      "loss: 0.066372  [ 2912/ 3200]\n",
      "loss: 0.171615  [ 2928/ 3200]\n",
      "loss: 0.287543  [ 2944/ 3200]\n",
      "loss: 0.402624  [ 2960/ 3200]\n",
      "loss: 0.328782  [ 2976/ 3200]\n",
      "loss: 0.089710  [ 2992/ 3200]\n",
      "loss: 0.210447  [ 3008/ 3200]\n",
      "loss: 0.157425  [ 3024/ 3200]\n",
      "loss: 0.192642  [ 3040/ 3200]\n",
      "loss: 0.103811  [ 3056/ 3200]\n",
      "loss: 0.174621  [ 3072/ 3200]\n",
      "loss: 0.031750  [ 3088/ 3200]\n",
      "loss: 0.271406  [ 3104/ 3200]\n",
      "loss: 0.221098  [ 3120/ 3200]\n",
      "loss: 0.474931  [ 3136/ 3200]\n",
      "loss: 0.280442  [ 3152/ 3200]\n",
      "loss: 0.198443  [ 3168/ 3200]\n",
      "loss: 0.168081  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.043114\n",
      "f1 macro averaged score: 0.787256\n",
      "Accuracy               : 78.8%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  17,   0,   8],\n",
      "        [ 23, 141,  21,  15],\n",
      "        [  1,  17, 172,  10],\n",
      "        [  9,  37,  12, 142]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1305e-02.\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.092382  [    0/ 3200]\n",
      "loss: 0.259820  [   16/ 3200]\n",
      "loss: 0.104671  [   32/ 3200]\n",
      "loss: 0.121015  [   48/ 3200]\n",
      "loss: 0.090895  [   64/ 3200]\n",
      "loss: 0.180965  [   80/ 3200]\n",
      "loss: 0.166631  [   96/ 3200]\n",
      "loss: 0.143778  [  112/ 3200]\n",
      "loss: 0.240934  [  128/ 3200]\n",
      "loss: 0.156518  [  144/ 3200]\n",
      "loss: 0.087614  [  160/ 3200]\n",
      "loss: 0.137888  [  176/ 3200]\n",
      "loss: 0.097551  [  192/ 3200]\n",
      "loss: 0.348567  [  208/ 3200]\n",
      "loss: 0.123975  [  224/ 3200]\n",
      "loss: 0.046588  [  240/ 3200]\n",
      "loss: 0.193133  [  256/ 3200]\n",
      "loss: 0.121407  [  272/ 3200]\n",
      "loss: 0.122442  [  288/ 3200]\n",
      "loss: 0.318097  [  304/ 3200]\n",
      "loss: 0.129486  [  320/ 3200]\n",
      "loss: 0.546078  [  336/ 3200]\n",
      "loss: 0.197388  [  352/ 3200]\n",
      "loss: 0.307527  [  368/ 3200]\n",
      "loss: 0.362867  [  384/ 3200]\n",
      "loss: 0.693953  [  400/ 3200]\n",
      "loss: 0.265649  [  416/ 3200]\n",
      "loss: 0.233378  [  432/ 3200]\n",
      "loss: 0.087527  [  448/ 3200]\n",
      "loss: 0.085182  [  464/ 3200]\n",
      "loss: 0.160243  [  480/ 3200]\n",
      "loss: 0.067503  [  496/ 3200]\n",
      "loss: 0.172757  [  512/ 3200]\n",
      "loss: 0.165279  [  528/ 3200]\n",
      "loss: 0.418689  [  544/ 3200]\n",
      "loss: 0.301488  [  560/ 3200]\n",
      "loss: 0.051027  [  576/ 3200]\n",
      "loss: 0.025807  [  592/ 3200]\n",
      "loss: 0.063773  [  608/ 3200]\n",
      "loss: 0.132964  [  624/ 3200]\n",
      "loss: 0.118238  [  640/ 3200]\n",
      "loss: 0.169422  [  656/ 3200]\n",
      "loss: 0.113608  [  672/ 3200]\n",
      "loss: 0.238390  [  688/ 3200]\n",
      "loss: 0.280977  [  704/ 3200]\n",
      "loss: 0.086025  [  720/ 3200]\n",
      "loss: 0.248096  [  736/ 3200]\n",
      "loss: 0.390032  [  752/ 3200]\n",
      "loss: 0.070446  [  768/ 3200]\n",
      "loss: 0.125149  [  784/ 3200]\n",
      "loss: 0.133674  [  800/ 3200]\n",
      "loss: 0.119309  [  816/ 3200]\n",
      "loss: 0.156852  [  832/ 3200]\n",
      "loss: 0.020354  [  848/ 3200]\n",
      "loss: 0.055921  [  864/ 3200]\n",
      "loss: 0.146205  [  880/ 3200]\n",
      "loss: 0.108278  [  896/ 3200]\n",
      "loss: 0.336950  [  912/ 3200]\n",
      "loss: 0.674356  [  928/ 3200]\n",
      "loss: 0.919097  [  944/ 3200]\n",
      "loss: 0.520232  [  960/ 3200]\n",
      "loss: 0.277755  [  976/ 3200]\n",
      "loss: 0.105638  [  992/ 3200]\n",
      "loss: 0.131044  [ 1008/ 3200]\n",
      "loss: 0.059482  [ 1024/ 3200]\n",
      "loss: 0.203092  [ 1040/ 3200]\n",
      "loss: 0.144632  [ 1056/ 3200]\n",
      "loss: 0.080622  [ 1072/ 3200]\n",
      "loss: 0.476984  [ 1088/ 3200]\n",
      "loss: 0.143746  [ 1104/ 3200]\n",
      "loss: 0.259241  [ 1120/ 3200]\n",
      "loss: 0.131865  [ 1136/ 3200]\n",
      "loss: 0.127367  [ 1152/ 3200]\n",
      "loss: 0.176529  [ 1168/ 3200]\n",
      "loss: 0.075393  [ 1184/ 3200]\n",
      "loss: 0.229599  [ 1200/ 3200]\n",
      "loss: 0.148312  [ 1216/ 3200]\n",
      "loss: 0.146879  [ 1232/ 3200]\n",
      "loss: 0.194454  [ 1248/ 3200]\n",
      "loss: 0.514958  [ 1264/ 3200]\n",
      "loss: 0.261912  [ 1280/ 3200]\n",
      "loss: 0.381542  [ 1296/ 3200]\n",
      "loss: 0.048713  [ 1312/ 3200]\n",
      "loss: 0.134000  [ 1328/ 3200]\n",
      "loss: 0.073100  [ 1344/ 3200]\n",
      "loss: 0.097886  [ 1360/ 3200]\n",
      "loss: 0.073806  [ 1376/ 3200]\n",
      "loss: 0.280033  [ 1392/ 3200]\n",
      "loss: 0.168718  [ 1408/ 3200]\n",
      "loss: 0.361088  [ 1424/ 3200]\n",
      "loss: 1.386146  [ 1440/ 3200]\n",
      "loss: 0.291898  [ 1456/ 3200]\n",
      "loss: 0.235345  [ 1472/ 3200]\n",
      "loss: 0.276098  [ 1488/ 3200]\n",
      "loss: 0.416703  [ 1504/ 3200]\n",
      "loss: 0.167235  [ 1520/ 3200]\n",
      "loss: 0.053665  [ 1536/ 3200]\n",
      "loss: 0.168063  [ 1552/ 3200]\n",
      "loss: 0.268441  [ 1568/ 3200]\n",
      "loss: 0.198686  [ 1584/ 3200]\n",
      "loss: 0.194421  [ 1600/ 3200]\n",
      "loss: 0.137416  [ 1616/ 3200]\n",
      "loss: 0.394000  [ 1632/ 3200]\n",
      "loss: 0.076522  [ 1648/ 3200]\n",
      "loss: 0.398353  [ 1664/ 3200]\n",
      "loss: 0.212530  [ 1680/ 3200]\n",
      "loss: 0.149232  [ 1696/ 3200]\n",
      "loss: 0.443479  [ 1712/ 3200]\n",
      "loss: 0.195532  [ 1728/ 3200]\n",
      "loss: 0.242372  [ 1744/ 3200]\n",
      "loss: 0.279434  [ 1760/ 3200]\n",
      "loss: 0.249403  [ 1776/ 3200]\n",
      "loss: 0.127313  [ 1792/ 3200]\n",
      "loss: 0.071703  [ 1808/ 3200]\n",
      "loss: 0.185482  [ 1824/ 3200]\n",
      "loss: 0.102283  [ 1840/ 3200]\n",
      "loss: 0.163212  [ 1856/ 3200]\n",
      "loss: 0.423052  [ 1872/ 3200]\n",
      "loss: 0.315523  [ 1888/ 3200]\n",
      "loss: 0.231818  [ 1904/ 3200]\n",
      "loss: 0.320066  [ 1920/ 3200]\n",
      "loss: 0.153534  [ 1936/ 3200]\n",
      "loss: 0.104404  [ 1952/ 3200]\n",
      "loss: 0.189611  [ 1968/ 3200]\n",
      "loss: 0.791777  [ 1984/ 3200]\n",
      "loss: 0.150683  [ 2000/ 3200]\n",
      "loss: 0.130770  [ 2016/ 3200]\n",
      "loss: 0.085429  [ 2032/ 3200]\n",
      "loss: 0.156279  [ 2048/ 3200]\n",
      "loss: 0.198625  [ 2064/ 3200]\n",
      "loss: 0.172687  [ 2080/ 3200]\n",
      "loss: 0.160813  [ 2096/ 3200]\n",
      "loss: 0.298276  [ 2112/ 3200]\n",
      "loss: 0.106386  [ 2128/ 3200]\n",
      "loss: 0.188070  [ 2144/ 3200]\n",
      "loss: 0.298013  [ 2160/ 3200]\n",
      "loss: 0.138105  [ 2176/ 3200]\n",
      "loss: 0.200046  [ 2192/ 3200]\n",
      "loss: 0.067277  [ 2208/ 3200]\n",
      "loss: 0.052510  [ 2224/ 3200]\n",
      "loss: 0.046829  [ 2240/ 3200]\n",
      "loss: 0.341426  [ 2256/ 3200]\n",
      "loss: 0.428604  [ 2272/ 3200]\n",
      "loss: 0.159786  [ 2288/ 3200]\n",
      "loss: 0.270275  [ 2304/ 3200]\n",
      "loss: 0.486597  [ 2320/ 3200]\n",
      "loss: 0.187798  [ 2336/ 3200]\n",
      "loss: 0.138479  [ 2352/ 3200]\n",
      "loss: 0.216688  [ 2368/ 3200]\n",
      "loss: 0.041289  [ 2384/ 3200]\n",
      "loss: 0.161264  [ 2400/ 3200]\n",
      "loss: 0.187086  [ 2416/ 3200]\n",
      "loss: 0.449376  [ 2432/ 3200]\n",
      "loss: 0.075521  [ 2448/ 3200]\n",
      "loss: 0.103633  [ 2464/ 3200]\n",
      "loss: 0.051141  [ 2480/ 3200]\n",
      "loss: 0.046672  [ 2496/ 3200]\n",
      "loss: 0.141745  [ 2512/ 3200]\n",
      "loss: 0.138259  [ 2528/ 3200]\n",
      "loss: 0.343287  [ 2544/ 3200]\n",
      "loss: 0.052385  [ 2560/ 3200]\n",
      "loss: 0.208549  [ 2576/ 3200]\n",
      "loss: 0.148345  [ 2592/ 3200]\n",
      "loss: 0.211130  [ 2608/ 3200]\n",
      "loss: 0.268160  [ 2624/ 3200]\n",
      "loss: 0.130998  [ 2640/ 3200]\n",
      "loss: 0.106545  [ 2656/ 3200]\n",
      "loss: 0.085059  [ 2672/ 3200]\n",
      "loss: 0.831229  [ 2688/ 3200]\n",
      "loss: 0.142531  [ 2704/ 3200]\n",
      "loss: 0.237293  [ 2720/ 3200]\n",
      "loss: 0.079473  [ 2736/ 3200]\n",
      "loss: 0.552219  [ 2752/ 3200]\n",
      "loss: 0.353896  [ 2768/ 3200]\n",
      "loss: 0.532173  [ 2784/ 3200]\n",
      "loss: 0.091990  [ 2800/ 3200]\n",
      "loss: 0.042409  [ 2816/ 3200]\n",
      "loss: 0.088916  [ 2832/ 3200]\n",
      "loss: 0.090023  [ 2848/ 3200]\n",
      "loss: 0.175944  [ 2864/ 3200]\n",
      "loss: 0.503146  [ 2880/ 3200]\n",
      "loss: 0.325441  [ 2896/ 3200]\n",
      "loss: 0.068262  [ 2912/ 3200]\n",
      "loss: 0.080489  [ 2928/ 3200]\n",
      "loss: 0.099125  [ 2944/ 3200]\n",
      "loss: 0.206979  [ 2960/ 3200]\n",
      "loss: 0.258893  [ 2976/ 3200]\n",
      "loss: 0.430596  [ 2992/ 3200]\n",
      "loss: 0.180540  [ 3008/ 3200]\n",
      "loss: 0.166266  [ 3024/ 3200]\n",
      "loss: 0.296272  [ 3040/ 3200]\n",
      "loss: 0.195912  [ 3056/ 3200]\n",
      "loss: 0.493602  [ 3072/ 3200]\n",
      "loss: 0.369744  [ 3088/ 3200]\n",
      "loss: 0.401226  [ 3104/ 3200]\n",
      "loss: 0.115737  [ 3120/ 3200]\n",
      "loss: 0.152472  [ 3136/ 3200]\n",
      "loss: 0.219841  [ 3152/ 3200]\n",
      "loss: 0.213750  [ 3168/ 3200]\n",
      "loss: 0.032454  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.044024\n",
      "f1 macro averaged score: 0.787658\n",
      "Accuracy               : 79.1%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  16,   0,  10],\n",
      "        [ 22, 119,  27,  32],\n",
      "        [  0,  13, 178,   9],\n",
      "        [  6,  20,  12, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1350e-02.\n",
      "\n",
      "Best epoch: 26 with f1 macro averaged score: 0.7996864318847656\n",
      "Test Error:\n",
      "Avg loss               : 0.046535\n",
      "f1 macro averaged score: 0.766720\n",
      "Accuracy               : 76.5%\n",
      "Confusion matrix       :\n",
      "tensor([[275,  21,   0,   1],\n",
      "        [  8, 192,  45,  79],\n",
      "        [  1,  21, 314,  20],\n",
      "        [ 21,  70,  36, 272]], device='cuda:0')\n",
      "CPU times: user 4min 21s, sys: 6.87 s, total: 4min 28s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_rate_0 = 0.002\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "# lr_lambda, step_size, gamma etc. argument values are taken from examples found in Pytorch Documentation pages for each scheduler\n",
    "def get_scheduler(index, optimizer):\n",
    "  if index == 0:\n",
    "    return \"LambdaLR\", LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: epoch // 30, verbose=True)\n",
    "  elif index == 1:\n",
    "    return \"MultiplicativeLR\", MultiplicativeLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95, verbose=True)\n",
    "  elif index == 2:\n",
    "    return \"StepLR\", StepLR(optimizer=optimizer, step_size=30, gamma=0.1, verbose=True)\n",
    "  elif index == 3:\n",
    "    return \"MultiStepLR\", MultiStepLR(optimizer=optimizer, milestones=[30,80], gamma=0.1, verbose=True)\n",
    "  elif index == 4:\n",
    "    return \"ConstantLR\", ConstantLR(optimizer=optimizer, factor=0.5, total_iters=4, verbose=True)\n",
    "  elif index == 5:\n",
    "    return \"LinearLR\", LinearLR(optimizer=optimizer, start_factor=0.5, total_iters=4, verbose=True)\n",
    "  elif index == 6:\n",
    "    return \"ExponentialLR\", ExponentialLR(optimizer=optimizer, gamma=0.1, verbose=True)\n",
    "  elif index == 7:\n",
    "    return \"PolynomialLR\", PolynomialLR(optimizer=optimizer, total_iters=4, power=2.0, verbose=True)\n",
    "  else:\n",
    "    return \"CyclicLR\", CyclicLR(optimizer=optimizer, base_lr=0.01, max_lr=0.1, cycle_momentum=False, verbose=True)\n",
    "\n",
    "f1_accuracy = []\n",
    "for i in range(0, 9):\n",
    "  torch_seed(0)\n",
    "  cnn_model = Net(ELU).to(device)\n",
    "  optimizer = Adagrad(params=cnn_model.parameters(), lr=learning_rate_0)\n",
    "  sch, scheduler = get_scheduler(i, optimizer)\n",
    "  print(\"  Scheduler:\", sch)\n",
    "  best_model, f1_per_epoch = validate_convolutional_neural_network(\n",
    "      epochs, optimizer, train_dataloader, val_dataloader, loss_function, cnn_model, True, scheduler\n",
    "      )\n",
    "  results = test_convolutional_neural_network(test_dataloader, loss_function, best_model)\n",
    "  f1_accuracy.append((sch, results[1], results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpnz_wBNKgvz",
    "outputId": "3f7b3348-3bae-4b0a-ad19-7c1cea4c2846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduler\t\tf1_macro_avg\tAccuracy\n",
      "MultiplicativeLR\t0.781785\t77.906977\n",
      "StepLR\t\t\t0.780637\t77.761628\n",
      "MultiStepLR\t\t0.780637\t77.761628\n",
      "CyclicLR\t\t0.766720\t76.526163\n",
      "LinearLR\t\t0.751379\t74.709302\n",
      "ConstantLR\t\t0.751224\t74.709302\n",
      "PolynomialLR\t\t0.726837\t72.529070\n",
      "ExponentialLR\t\t0.643599\t66.206395\n",
      "LambdaLR\t\t0.095294\t23.546512\n"
     ]
    }
   ],
   "source": [
    "f1_accuracy = sorted(f1_accuracy, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Scheduler\\t\\tf1_macro_avg\\tAccuracy\")\n",
    "for (scheduler, f1_macro_avg, accuracy) in f1_accuracy:\n",
    "  if len(scheduler) > 14: # for better visualization\n",
    "    print(f\"{scheduler}\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")\n",
    "    continue\n",
    "  if len(scheduler) > 7:  # for better visualization\n",
    "    print(f\"{scheduler}\\t\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")\n",
    "    continue\n",
    "  print(f\"{scheduler}\\t\\t\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icYh0TkSJKK2"
   },
   "source": [
    "The table above shows that the optimizer plays a big role in the development of the model, since it can really change the results during testing.\n",
    "\n",
    "The best activation function for our Convolutional Neural Network was ELU with almost $78\\%$ accuracy and the worst was Hardtanh with $23\\%$ accuracy.\n",
    "<br></br>\n",
    "\n",
    "We conclude that MultiplicativeLR increased accuracy from $77\\%$ to $78\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8HoXPWF48SI"
   },
   "source": [
    "### Step 4 - Batch Normalization\n",
    "\n",
    "Redefine the Convolutional Network Model by adding batch normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rupsv67c470v"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, activation_function):\n",
    "    super().__init__()\n",
    "    # convolutional layers\n",
    "    self.conv1_batch_norm = nn.BatchNorm2d(1)\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, padding=2)   # 1 channel   -> 16 channels\n",
    "    self.conv2_batch_norm = nn.BatchNorm2d(16)\n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2)  # 16 channels -> 32 channels\n",
    "    self.conv3_batch_norm = nn.BatchNorm2d(32)\n",
    "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)  # 32 channels -> 64 channels\n",
    "    self.conv4_batch_norm = nn.BatchNorm2d(64)\n",
    "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2) # 64 channels -> 128 channels\n",
    "\n",
    "    self.activation_function = activation_function()\n",
    "\n",
    "    # fully connected (dense) layers\n",
    "    self.dense1 = nn.Linear(1024, 1024) # input size = 1024 -> number of perceptrons in 1st hidden layer = 1024\n",
    "    self.dense2 = nn.Linear(1024, 256)  # number of perceptrons in 1st hidden layer = 1024 ->\n",
    "                                        # number of perceptrons in 2nd hidden layer = 256\n",
    "    self.dense3 = nn.Linear(256, 32)    # number of perceptrons in 2nd hidden layer = 256 ->\n",
    "                                        # number of perceptrons in 3rd hidden layer = 32\n",
    "    self.dense4 = nn.Linear(32, 4)      # number of perceptrons in 3rd hidden layer = 32 ->\n",
    "                                        # number of different labels = 4\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv2(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv3(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv4(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    x = torch.flatten(x, 1)\n",
    "\n",
    "    x = self.dense1(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = self.dense2(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = self.dense3(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = self.dense4(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw8uDNGsiPsZ"
   },
   "source": [
    "We validate our Convolutional Neural Network for $30$ epochs and test it.\n",
    "\n",
    "Our model uses:\n",
    "+ the Adagrad optimizer\n",
    "+ the ELU activation function\n",
    "+ the MultiplicativeLR scheduler\n",
    "\n",
    "as stated in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZg6wDCbKg_1",
    "outputId": "9a9cb317-93ec-49f8-b628-daadcbdbcaa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 0.275193  [ 1952/ 3200]\n",
      "loss: 0.344462  [ 1968/ 3200]\n",
      "loss: 0.173467  [ 1984/ 3200]\n",
      "loss: 0.247901  [ 2000/ 3200]\n",
      "loss: 0.256229  [ 2016/ 3200]\n",
      "loss: 0.467592  [ 2032/ 3200]\n",
      "loss: 0.224782  [ 2048/ 3200]\n",
      "loss: 0.198767  [ 2064/ 3200]\n",
      "loss: 0.155372  [ 2080/ 3200]\n",
      "loss: 0.457266  [ 2096/ 3200]\n",
      "loss: 0.215905  [ 2112/ 3200]\n",
      "loss: 0.474478  [ 2128/ 3200]\n",
      "loss: 0.415050  [ 2144/ 3200]\n",
      "loss: 0.268733  [ 2160/ 3200]\n",
      "loss: 0.387448  [ 2176/ 3200]\n",
      "loss: 0.218260  [ 2192/ 3200]\n",
      "loss: 0.345116  [ 2208/ 3200]\n",
      "loss: 0.239158  [ 2224/ 3200]\n",
      "loss: 0.501732  [ 2240/ 3200]\n",
      "loss: 0.771184  [ 2256/ 3200]\n",
      "loss: 1.107691  [ 2272/ 3200]\n",
      "loss: 0.499292  [ 2288/ 3200]\n",
      "loss: 0.295326  [ 2304/ 3200]\n",
      "loss: 0.543614  [ 2320/ 3200]\n",
      "loss: 0.475257  [ 2336/ 3200]\n",
      "loss: 0.452445  [ 2352/ 3200]\n",
      "loss: 0.432226  [ 2368/ 3200]\n",
      "loss: 0.312425  [ 2384/ 3200]\n",
      "loss: 0.606847  [ 2400/ 3200]\n",
      "loss: 0.578857  [ 2416/ 3200]\n",
      "loss: 0.252839  [ 2432/ 3200]\n",
      "loss: 0.423132  [ 2448/ 3200]\n",
      "loss: 0.148836  [ 2464/ 3200]\n",
      "loss: 0.387420  [ 2480/ 3200]\n",
      "loss: 0.355129  [ 2496/ 3200]\n",
      "loss: 0.344019  [ 2512/ 3200]\n",
      "loss: 0.327401  [ 2528/ 3200]\n",
      "loss: 0.413818  [ 2544/ 3200]\n",
      "loss: 0.544615  [ 2560/ 3200]\n",
      "loss: 0.726118  [ 2576/ 3200]\n",
      "loss: 0.592485  [ 2592/ 3200]\n",
      "loss: 0.316729  [ 2608/ 3200]\n",
      "loss: 0.398326  [ 2624/ 3200]\n",
      "loss: 0.089315  [ 2640/ 3200]\n",
      "loss: 0.459024  [ 2656/ 3200]\n",
      "loss: 0.299544  [ 2672/ 3200]\n",
      "loss: 0.389753  [ 2688/ 3200]\n",
      "loss: 0.289958  [ 2704/ 3200]\n",
      "loss: 0.149756  [ 2720/ 3200]\n",
      "loss: 0.293386  [ 2736/ 3200]\n",
      "loss: 0.466091  [ 2752/ 3200]\n",
      "loss: 0.478729  [ 2768/ 3200]\n",
      "loss: 0.110270  [ 2784/ 3200]\n",
      "loss: 0.548731  [ 2800/ 3200]\n",
      "loss: 0.285438  [ 2816/ 3200]\n",
      "loss: 0.146922  [ 2832/ 3200]\n",
      "loss: 0.260350  [ 2848/ 3200]\n",
      "loss: 0.859431  [ 2864/ 3200]\n",
      "loss: 0.753548  [ 2880/ 3200]\n",
      "loss: 0.270552  [ 2896/ 3200]\n",
      "loss: 0.359270  [ 2912/ 3200]\n",
      "loss: 0.529571  [ 2928/ 3200]\n",
      "loss: 0.121697  [ 2944/ 3200]\n",
      "loss: 0.195496  [ 2960/ 3200]\n",
      "loss: 0.495431  [ 2976/ 3200]\n",
      "loss: 0.343220  [ 2992/ 3200]\n",
      "loss: 0.715896  [ 3008/ 3200]\n",
      "loss: 0.423872  [ 3024/ 3200]\n",
      "loss: 0.470245  [ 3040/ 3200]\n",
      "loss: 0.450577  [ 3056/ 3200]\n",
      "loss: 0.480417  [ 3072/ 3200]\n",
      "loss: 0.218780  [ 3088/ 3200]\n",
      "loss: 0.087258  [ 3104/ 3200]\n",
      "loss: 0.506347  [ 3120/ 3200]\n",
      "loss: 0.289779  [ 3136/ 3200]\n",
      "loss: 0.482130  [ 3152/ 3200]\n",
      "loss: 0.294592  [ 3168/ 3200]\n",
      "loss: 0.507434  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034031\n",
      "f1 macro averaged score: 0.803109\n",
      "Accuracy               : 80.2%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  19,   1,   5],\n",
      "        [ 16, 143,  27,  14],\n",
      "        [  0,  21, 176,   3],\n",
      "        [  5,  30,  17, 148]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3967e-03.\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 0.667867  [    0/ 3200]\n",
      "loss: 0.401750  [   16/ 3200]\n",
      "loss: 0.493352  [   32/ 3200]\n",
      "loss: 0.254172  [   48/ 3200]\n",
      "loss: 0.222230  [   64/ 3200]\n",
      "loss: 0.402659  [   80/ 3200]\n",
      "loss: 0.263696  [   96/ 3200]\n",
      "loss: 0.198749  [  112/ 3200]\n",
      "loss: 0.256822  [  128/ 3200]\n",
      "loss: 0.519416  [  144/ 3200]\n",
      "loss: 0.283946  [  160/ 3200]\n",
      "loss: 0.347957  [  176/ 3200]\n",
      "loss: 0.617978  [  192/ 3200]\n",
      "loss: 0.440981  [  208/ 3200]\n",
      "loss: 0.222127  [  224/ 3200]\n",
      "loss: 0.247843  [  240/ 3200]\n",
      "loss: 0.684617  [  256/ 3200]\n",
      "loss: 0.237671  [  272/ 3200]\n",
      "loss: 0.314409  [  288/ 3200]\n",
      "loss: 0.214768  [  304/ 3200]\n",
      "loss: 0.690408  [  320/ 3200]\n",
      "loss: 0.312808  [  336/ 3200]\n",
      "loss: 0.570185  [  352/ 3200]\n",
      "loss: 0.315560  [  368/ 3200]\n",
      "loss: 0.297972  [  384/ 3200]\n",
      "loss: 0.520942  [  400/ 3200]\n",
      "loss: 0.364045  [  416/ 3200]\n",
      "loss: 0.657789  [  432/ 3200]\n",
      "loss: 0.302028  [  448/ 3200]\n",
      "loss: 0.516821  [  464/ 3200]\n",
      "loss: 0.232522  [  480/ 3200]\n",
      "loss: 0.243410  [  496/ 3200]\n",
      "loss: 0.627558  [  512/ 3200]\n",
      "loss: 0.274823  [  528/ 3200]\n",
      "loss: 0.659972  [  544/ 3200]\n",
      "loss: 0.527195  [  560/ 3200]\n",
      "loss: 0.383535  [  576/ 3200]\n",
      "loss: 0.334585  [  592/ 3200]\n",
      "loss: 0.266641  [  608/ 3200]\n",
      "loss: 0.239152  [  624/ 3200]\n",
      "loss: 0.270321  [  640/ 3200]\n",
      "loss: 0.624215  [  656/ 3200]\n",
      "loss: 0.551230  [  672/ 3200]\n",
      "loss: 0.508812  [  688/ 3200]\n",
      "loss: 0.239572  [  704/ 3200]\n",
      "loss: 0.578579  [  720/ 3200]\n",
      "loss: 0.177987  [  736/ 3200]\n",
      "loss: 0.286760  [  752/ 3200]\n",
      "loss: 0.401111  [  768/ 3200]\n",
      "loss: 0.368213  [  784/ 3200]\n",
      "loss: 0.122335  [  800/ 3200]\n",
      "loss: 0.520272  [  816/ 3200]\n",
      "loss: 0.163228  [  832/ 3200]\n",
      "loss: 0.177684  [  848/ 3200]\n",
      "loss: 0.667723  [  864/ 3200]\n",
      "loss: 0.443128  [  880/ 3200]\n",
      "loss: 0.181865  [  896/ 3200]\n",
      "loss: 0.181330  [  912/ 3200]\n",
      "loss: 0.540130  [  928/ 3200]\n",
      "loss: 0.268432  [  944/ 3200]\n",
      "loss: 0.386843  [  960/ 3200]\n",
      "loss: 0.209202  [  976/ 3200]\n",
      "loss: 0.229208  [  992/ 3200]\n",
      "loss: 0.481574  [ 1008/ 3200]\n",
      "loss: 0.627942  [ 1024/ 3200]\n",
      "loss: 0.463542  [ 1040/ 3200]\n",
      "loss: 0.235697  [ 1056/ 3200]\n",
      "loss: 0.440102  [ 1072/ 3200]\n",
      "loss: 0.364728  [ 1088/ 3200]\n",
      "loss: 0.568998  [ 1104/ 3200]\n",
      "loss: 0.156770  [ 1120/ 3200]\n",
      "loss: 0.201404  [ 1136/ 3200]\n",
      "loss: 0.274628  [ 1152/ 3200]\n",
      "loss: 0.715146  [ 1168/ 3200]\n",
      "loss: 0.536895  [ 1184/ 3200]\n",
      "loss: 0.501004  [ 1200/ 3200]\n",
      "loss: 0.436759  [ 1216/ 3200]\n",
      "loss: 0.179348  [ 1232/ 3200]\n",
      "loss: 0.273134  [ 1248/ 3200]\n",
      "loss: 0.188464  [ 1264/ 3200]\n",
      "loss: 0.592637  [ 1280/ 3200]\n",
      "loss: 0.392419  [ 1296/ 3200]\n",
      "loss: 0.348645  [ 1312/ 3200]\n",
      "loss: 0.358279  [ 1328/ 3200]\n",
      "loss: 0.322972  [ 1344/ 3200]\n",
      "loss: 0.202556  [ 1360/ 3200]\n",
      "loss: 0.334598  [ 1376/ 3200]\n",
      "loss: 0.170805  [ 1392/ 3200]\n",
      "loss: 0.454418  [ 1408/ 3200]\n",
      "loss: 0.508014  [ 1424/ 3200]\n",
      "loss: 0.747523  [ 1440/ 3200]\n",
      "loss: 0.520525  [ 1456/ 3200]\n",
      "loss: 0.099558  [ 1472/ 3200]\n",
      "loss: 0.290127  [ 1488/ 3200]\n",
      "loss: 0.183372  [ 1504/ 3200]\n",
      "loss: 0.601126  [ 1520/ 3200]\n",
      "loss: 0.668929  [ 1536/ 3200]\n",
      "loss: 0.444508  [ 1552/ 3200]\n",
      "loss: 0.185695  [ 1568/ 3200]\n",
      "loss: 0.370349  [ 1584/ 3200]\n",
      "loss: 0.398272  [ 1600/ 3200]\n",
      "loss: 0.158556  [ 1616/ 3200]\n",
      "loss: 0.217862  [ 1632/ 3200]\n",
      "loss: 0.224202  [ 1648/ 3200]\n",
      "loss: 0.727760  [ 1664/ 3200]\n",
      "loss: 0.349856  [ 1680/ 3200]\n",
      "loss: 0.605001  [ 1696/ 3200]\n",
      "loss: 0.281401  [ 1712/ 3200]\n",
      "loss: 0.219432  [ 1728/ 3200]\n",
      "loss: 0.154082  [ 1744/ 3200]\n",
      "loss: 0.226856  [ 1760/ 3200]\n",
      "loss: 0.538308  [ 1776/ 3200]\n",
      "loss: 0.198613  [ 1792/ 3200]\n",
      "loss: 0.250584  [ 1808/ 3200]\n",
      "loss: 0.517059  [ 1824/ 3200]\n",
      "loss: 0.306773  [ 1840/ 3200]\n",
      "loss: 0.315123  [ 1856/ 3200]\n",
      "loss: 0.425969  [ 1872/ 3200]\n",
      "loss: 0.213195  [ 1888/ 3200]\n",
      "loss: 0.326266  [ 1904/ 3200]\n",
      "loss: 0.209466  [ 1920/ 3200]\n",
      "loss: 0.403663  [ 1936/ 3200]\n",
      "loss: 0.128640  [ 1952/ 3200]\n",
      "loss: 0.317618  [ 1968/ 3200]\n",
      "loss: 0.301682  [ 1984/ 3200]\n",
      "loss: 0.230876  [ 2000/ 3200]\n",
      "loss: 0.363341  [ 2016/ 3200]\n",
      "loss: 0.347380  [ 2032/ 3200]\n",
      "loss: 0.595559  [ 2048/ 3200]\n",
      "loss: 0.275044  [ 2064/ 3200]\n",
      "loss: 0.287825  [ 2080/ 3200]\n",
      "loss: 0.599957  [ 2096/ 3200]\n",
      "loss: 0.347633  [ 2112/ 3200]\n",
      "loss: 0.502075  [ 2128/ 3200]\n",
      "loss: 0.381301  [ 2144/ 3200]\n",
      "loss: 0.467471  [ 2160/ 3200]\n",
      "loss: 0.208142  [ 2176/ 3200]\n",
      "loss: 0.460428  [ 2192/ 3200]\n",
      "loss: 0.140795  [ 2208/ 3200]\n",
      "loss: 0.359597  [ 2224/ 3200]\n",
      "loss: 0.380752  [ 2240/ 3200]\n",
      "loss: 0.293266  [ 2256/ 3200]\n",
      "loss: 0.411071  [ 2272/ 3200]\n",
      "loss: 0.764832  [ 2288/ 3200]\n",
      "loss: 0.403341  [ 2304/ 3200]\n",
      "loss: 0.436341  [ 2320/ 3200]\n",
      "loss: 0.464361  [ 2336/ 3200]\n",
      "loss: 0.313325  [ 2352/ 3200]\n",
      "loss: 0.495986  [ 2368/ 3200]\n",
      "loss: 0.520007  [ 2384/ 3200]\n",
      "loss: 0.428266  [ 2400/ 3200]\n",
      "loss: 0.238591  [ 2416/ 3200]\n",
      "loss: 0.604142  [ 2432/ 3200]\n",
      "loss: 0.328787  [ 2448/ 3200]\n",
      "loss: 0.486676  [ 2464/ 3200]\n",
      "loss: 0.219483  [ 2480/ 3200]\n",
      "loss: 0.228268  [ 2496/ 3200]\n",
      "loss: 0.473628  [ 2512/ 3200]\n",
      "loss: 0.203498  [ 2528/ 3200]\n",
      "loss: 0.247205  [ 2544/ 3200]\n",
      "loss: 0.227959  [ 2560/ 3200]\n",
      "loss: 0.537664  [ 2576/ 3200]\n",
      "loss: 0.328464  [ 2592/ 3200]\n",
      "loss: 0.472843  [ 2608/ 3200]\n",
      "loss: 0.370469  [ 2624/ 3200]\n",
      "loss: 0.245976  [ 2640/ 3200]\n",
      "loss: 0.175486  [ 2656/ 3200]\n",
      "loss: 0.217445  [ 2672/ 3200]\n",
      "loss: 0.245308  [ 2688/ 3200]\n",
      "loss: 0.604266  [ 2704/ 3200]\n",
      "loss: 0.592615  [ 2720/ 3200]\n",
      "loss: 0.248987  [ 2736/ 3200]\n",
      "loss: 0.373931  [ 2752/ 3200]\n",
      "loss: 0.275166  [ 2768/ 3200]\n",
      "loss: 0.300522  [ 2784/ 3200]\n",
      "loss: 0.149614  [ 2800/ 3200]\n",
      "loss: 0.307261  [ 2816/ 3200]\n",
      "loss: 0.307631  [ 2832/ 3200]\n",
      "loss: 0.696670  [ 2848/ 3200]\n",
      "loss: 0.203097  [ 2864/ 3200]\n",
      "loss: 0.510361  [ 2880/ 3200]\n",
      "loss: 0.384572  [ 2896/ 3200]\n",
      "loss: 0.746205  [ 2912/ 3200]\n",
      "loss: 0.430645  [ 2928/ 3200]\n",
      "loss: 0.369457  [ 2944/ 3200]\n",
      "loss: 0.403348  [ 2960/ 3200]\n",
      "loss: 0.157958  [ 2976/ 3200]\n",
      "loss: 0.288630  [ 2992/ 3200]\n",
      "loss: 0.201844  [ 3008/ 3200]\n",
      "loss: 0.062400  [ 3024/ 3200]\n",
      "loss: 0.838867  [ 3040/ 3200]\n",
      "loss: 0.298582  [ 3056/ 3200]\n",
      "loss: 0.168149  [ 3072/ 3200]\n",
      "loss: 0.114585  [ 3088/ 3200]\n",
      "loss: 0.270442  [ 3104/ 3200]\n",
      "loss: 0.619889  [ 3120/ 3200]\n",
      "loss: 0.265187  [ 3136/ 3200]\n",
      "loss: 0.438015  [ 3152/ 3200]\n",
      "loss: 0.319308  [ 3168/ 3200]\n",
      "loss: 0.443148  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035801\n",
      "f1 macro averaged score: 0.765417\n",
      "Accuracy               : 77.1%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  14,   0,  12],\n",
      "        [ 16, 104,  32,  48],\n",
      "        [  0,  14, 171,  15],\n",
      "        [  6,  10,  16, 168]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3268e-03.\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 0.287071  [    0/ 3200]\n",
      "loss: 0.442744  [   16/ 3200]\n",
      "loss: 0.396209  [   32/ 3200]\n",
      "loss: 0.404898  [   48/ 3200]\n",
      "loss: 0.261828  [   64/ 3200]\n",
      "loss: 0.375659  [   80/ 3200]\n",
      "loss: 0.441419  [   96/ 3200]\n",
      "loss: 0.126070  [  112/ 3200]\n",
      "loss: 0.233175  [  128/ 3200]\n",
      "loss: 0.435094  [  144/ 3200]\n",
      "loss: 0.525057  [  160/ 3200]\n",
      "loss: 0.496348  [  176/ 3200]\n",
      "loss: 0.447234  [  192/ 3200]\n",
      "loss: 0.202607  [  208/ 3200]\n",
      "loss: 0.335338  [  224/ 3200]\n",
      "loss: 0.283140  [  240/ 3200]\n",
      "loss: 0.191766  [  256/ 3200]\n",
      "loss: 0.290209  [  272/ 3200]\n",
      "loss: 0.428214  [  288/ 3200]\n",
      "loss: 0.276551  [  304/ 3200]\n",
      "loss: 0.549127  [  320/ 3200]\n",
      "loss: 0.370180  [  336/ 3200]\n",
      "loss: 0.420174  [  352/ 3200]\n",
      "loss: 0.375657  [  368/ 3200]\n",
      "loss: 0.094539  [  384/ 3200]\n",
      "loss: 0.369557  [  400/ 3200]\n",
      "loss: 0.417695  [  416/ 3200]\n",
      "loss: 0.072599  [  432/ 3200]\n",
      "loss: 0.349514  [  448/ 3200]\n",
      "loss: 0.253079  [  464/ 3200]\n",
      "loss: 0.308394  [  480/ 3200]\n",
      "loss: 0.156170  [  496/ 3200]\n",
      "loss: 0.199090  [  512/ 3200]\n",
      "loss: 0.313233  [  528/ 3200]\n",
      "loss: 0.270454  [  544/ 3200]\n",
      "loss: 0.388665  [  560/ 3200]\n",
      "loss: 0.366358  [  576/ 3200]\n",
      "loss: 0.274244  [  592/ 3200]\n",
      "loss: 0.361729  [  608/ 3200]\n",
      "loss: 0.649445  [  624/ 3200]\n",
      "loss: 0.421993  [  640/ 3200]\n",
      "loss: 0.153849  [  656/ 3200]\n",
      "loss: 0.313976  [  672/ 3200]\n",
      "loss: 0.194128  [  688/ 3200]\n",
      "loss: 0.610708  [  704/ 3200]\n",
      "loss: 0.577256  [  720/ 3200]\n",
      "loss: 0.437372  [  736/ 3200]\n",
      "loss: 0.394840  [  752/ 3200]\n",
      "loss: 0.284389  [  768/ 3200]\n",
      "loss: 0.320290  [  784/ 3200]\n",
      "loss: 0.430552  [  800/ 3200]\n",
      "loss: 0.368414  [  816/ 3200]\n",
      "loss: 0.229044  [  832/ 3200]\n",
      "loss: 0.150481  [  848/ 3200]\n",
      "loss: 0.423031  [  864/ 3200]\n",
      "loss: 0.211167  [  880/ 3200]\n",
      "loss: 0.164114  [  896/ 3200]\n",
      "loss: 0.460333  [  912/ 3200]\n",
      "loss: 0.178999  [  928/ 3200]\n",
      "loss: 0.257904  [  944/ 3200]\n",
      "loss: 0.435685  [  960/ 3200]\n",
      "loss: 0.231668  [  976/ 3200]\n",
      "loss: 0.552213  [  992/ 3200]\n",
      "loss: 0.432459  [ 1008/ 3200]\n",
      "loss: 0.403955  [ 1024/ 3200]\n",
      "loss: 0.387896  [ 1040/ 3200]\n",
      "loss: 0.434267  [ 1056/ 3200]\n",
      "loss: 0.513206  [ 1072/ 3200]\n",
      "loss: 0.757736  [ 1088/ 3200]\n",
      "loss: 0.217135  [ 1104/ 3200]\n",
      "loss: 0.339134  [ 1120/ 3200]\n",
      "loss: 0.269982  [ 1136/ 3200]\n",
      "loss: 0.308307  [ 1152/ 3200]\n",
      "loss: 0.321187  [ 1168/ 3200]\n",
      "loss: 0.305265  [ 1184/ 3200]\n",
      "loss: 0.281999  [ 1200/ 3200]\n",
      "loss: 0.590746  [ 1216/ 3200]\n",
      "loss: 0.454489  [ 1232/ 3200]\n",
      "loss: 0.366433  [ 1248/ 3200]\n",
      "loss: 0.430218  [ 1264/ 3200]\n",
      "loss: 0.272975  [ 1280/ 3200]\n",
      "loss: 0.277896  [ 1296/ 3200]\n",
      "loss: 0.322317  [ 1312/ 3200]\n",
      "loss: 0.387619  [ 1328/ 3200]\n",
      "loss: 0.406355  [ 1344/ 3200]\n",
      "loss: 0.423778  [ 1360/ 3200]\n",
      "loss: 0.346436  [ 1376/ 3200]\n",
      "loss: 0.226343  [ 1392/ 3200]\n",
      "loss: 0.763115  [ 1408/ 3200]\n",
      "loss: 0.227984  [ 1424/ 3200]\n",
      "loss: 0.720717  [ 1440/ 3200]\n",
      "loss: 0.389405  [ 1456/ 3200]\n",
      "loss: 0.631155  [ 1472/ 3200]\n",
      "loss: 0.723732  [ 1488/ 3200]\n",
      "loss: 0.136298  [ 1504/ 3200]\n",
      "loss: 0.078582  [ 1520/ 3200]\n",
      "loss: 0.275521  [ 1536/ 3200]\n",
      "loss: 0.451888  [ 1552/ 3200]\n",
      "loss: 0.232147  [ 1568/ 3200]\n",
      "loss: 0.122825  [ 1584/ 3200]\n",
      "loss: 0.291144  [ 1600/ 3200]\n",
      "loss: 0.422288  [ 1616/ 3200]\n",
      "loss: 0.299651  [ 1632/ 3200]\n",
      "loss: 0.266839  [ 1648/ 3200]\n",
      "loss: 0.313908  [ 1664/ 3200]\n",
      "loss: 0.339554  [ 1680/ 3200]\n",
      "loss: 0.184602  [ 1696/ 3200]\n",
      "loss: 0.295636  [ 1712/ 3200]\n",
      "loss: 0.284348  [ 1728/ 3200]\n",
      "loss: 0.427134  [ 1744/ 3200]\n",
      "loss: 0.243842  [ 1760/ 3200]\n",
      "loss: 0.271101  [ 1776/ 3200]\n",
      "loss: 0.544039  [ 1792/ 3200]\n",
      "loss: 0.311072  [ 1808/ 3200]\n",
      "loss: 0.098152  [ 1824/ 3200]\n",
      "loss: 0.380641  [ 1840/ 3200]\n",
      "loss: 0.448698  [ 1856/ 3200]\n",
      "loss: 0.249534  [ 1872/ 3200]\n",
      "loss: 0.172375  [ 1888/ 3200]\n",
      "loss: 0.304895  [ 1904/ 3200]\n",
      "loss: 0.207171  [ 1920/ 3200]\n",
      "loss: 0.212318  [ 1936/ 3200]\n",
      "loss: 0.461802  [ 1952/ 3200]\n",
      "loss: 0.222583  [ 1968/ 3200]\n",
      "loss: 0.354540  [ 1984/ 3200]\n",
      "loss: 0.253157  [ 2000/ 3200]\n",
      "loss: 0.182521  [ 2016/ 3200]\n",
      "loss: 0.244426  [ 2032/ 3200]\n",
      "loss: 0.230307  [ 2048/ 3200]\n",
      "loss: 0.215451  [ 2064/ 3200]\n",
      "loss: 0.306809  [ 2080/ 3200]\n",
      "loss: 0.088607  [ 2096/ 3200]\n",
      "loss: 0.145219  [ 2112/ 3200]\n",
      "loss: 0.175836  [ 2128/ 3200]\n",
      "loss: 0.134483  [ 2144/ 3200]\n",
      "loss: 0.707767  [ 2160/ 3200]\n",
      "loss: 0.217859  [ 2176/ 3200]\n",
      "loss: 0.125088  [ 2192/ 3200]\n",
      "loss: 0.232754  [ 2208/ 3200]\n",
      "loss: 0.513520  [ 2224/ 3200]\n",
      "loss: 0.161573  [ 2240/ 3200]\n",
      "loss: 0.341041  [ 2256/ 3200]\n",
      "loss: 0.315885  [ 2272/ 3200]\n",
      "loss: 0.472522  [ 2288/ 3200]\n",
      "loss: 0.511625  [ 2304/ 3200]\n",
      "loss: 0.531440  [ 2320/ 3200]\n",
      "loss: 0.235279  [ 2336/ 3200]\n",
      "loss: 0.330212  [ 2352/ 3200]\n",
      "loss: 0.518770  [ 2368/ 3200]\n",
      "loss: 0.198566  [ 2384/ 3200]\n",
      "loss: 0.279877  [ 2400/ 3200]\n",
      "loss: 0.231740  [ 2416/ 3200]\n",
      "loss: 0.297053  [ 2432/ 3200]\n",
      "loss: 0.177457  [ 2448/ 3200]\n",
      "loss: 0.170681  [ 2464/ 3200]\n",
      "loss: 0.308426  [ 2480/ 3200]\n",
      "loss: 0.135100  [ 2496/ 3200]\n",
      "loss: 0.303586  [ 2512/ 3200]\n",
      "loss: 0.349611  [ 2528/ 3200]\n",
      "loss: 0.181099  [ 2544/ 3200]\n",
      "loss: 0.243110  [ 2560/ 3200]\n",
      "loss: 0.190701  [ 2576/ 3200]\n",
      "loss: 0.384086  [ 2592/ 3200]\n",
      "loss: 0.421839  [ 2608/ 3200]\n",
      "loss: 0.439179  [ 2624/ 3200]\n",
      "loss: 0.395787  [ 2640/ 3200]\n",
      "loss: 0.179212  [ 2656/ 3200]\n",
      "loss: 0.323888  [ 2672/ 3200]\n",
      "loss: 0.379284  [ 2688/ 3200]\n",
      "loss: 0.102128  [ 2704/ 3200]\n",
      "loss: 0.563182  [ 2720/ 3200]\n",
      "loss: 0.566361  [ 2736/ 3200]\n",
      "loss: 0.321995  [ 2752/ 3200]\n",
      "loss: 0.741802  [ 2768/ 3200]\n",
      "loss: 0.560275  [ 2784/ 3200]\n",
      "loss: 0.147721  [ 2800/ 3200]\n",
      "loss: 0.216176  [ 2816/ 3200]\n",
      "loss: 0.469637  [ 2832/ 3200]\n",
      "loss: 0.537098  [ 2848/ 3200]\n",
      "loss: 0.314643  [ 2864/ 3200]\n",
      "loss: 0.377673  [ 2880/ 3200]\n",
      "loss: 0.347175  [ 2896/ 3200]\n",
      "loss: 0.381363  [ 2912/ 3200]\n",
      "loss: 0.175022  [ 2928/ 3200]\n",
      "loss: 0.421313  [ 2944/ 3200]\n",
      "loss: 0.144640  [ 2960/ 3200]\n",
      "loss: 0.721384  [ 2976/ 3200]\n",
      "loss: 0.395527  [ 2992/ 3200]\n",
      "loss: 0.288410  [ 3008/ 3200]\n",
      "loss: 0.175824  [ 3024/ 3200]\n",
      "loss: 0.234610  [ 3040/ 3200]\n",
      "loss: 0.387716  [ 3056/ 3200]\n",
      "loss: 0.403714  [ 3072/ 3200]\n",
      "loss: 0.136362  [ 3088/ 3200]\n",
      "loss: 0.105942  [ 3104/ 3200]\n",
      "loss: 0.288788  [ 3120/ 3200]\n",
      "loss: 0.558446  [ 3136/ 3200]\n",
      "loss: 0.347290  [ 3152/ 3200]\n",
      "loss: 0.302404  [ 3168/ 3200]\n",
      "loss: 0.504496  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.033833\n",
      "f1 macro averaged score: 0.799511\n",
      "Accuracy               : 80.1%\n",
      "Confusion matrix       :\n",
      "tensor([[179,  12,   1,   8],\n",
      "        [ 18, 133,  27,  22],\n",
      "        [  0,  19, 176,   5],\n",
      "        [  8,  23,  16, 153]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2605e-03.\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 0.185028  [    0/ 3200]\n",
      "loss: 0.290017  [   16/ 3200]\n",
      "loss: 0.170394  [   32/ 3200]\n",
      "loss: 0.746735  [   48/ 3200]\n",
      "loss: 0.722703  [   64/ 3200]\n",
      "loss: 0.370965  [   80/ 3200]\n",
      "loss: 0.208245  [   96/ 3200]\n",
      "loss: 0.229382  [  112/ 3200]\n",
      "loss: 0.150886  [  128/ 3200]\n",
      "loss: 0.281487  [  144/ 3200]\n",
      "loss: 0.181359  [  160/ 3200]\n",
      "loss: 0.346554  [  176/ 3200]\n",
      "loss: 0.365166  [  192/ 3200]\n",
      "loss: 0.278401  [  208/ 3200]\n",
      "loss: 0.379432  [  224/ 3200]\n",
      "loss: 0.301731  [  240/ 3200]\n",
      "loss: 0.307386  [  256/ 3200]\n",
      "loss: 0.686002  [  272/ 3200]\n",
      "loss: 0.317906  [  288/ 3200]\n",
      "loss: 0.456774  [  304/ 3200]\n",
      "loss: 0.269795  [  320/ 3200]\n",
      "loss: 0.293824  [  336/ 3200]\n",
      "loss: 0.530786  [  352/ 3200]\n",
      "loss: 0.166162  [  368/ 3200]\n",
      "loss: 0.158099  [  384/ 3200]\n",
      "loss: 0.222053  [  400/ 3200]\n",
      "loss: 0.178714  [  416/ 3200]\n",
      "loss: 0.337135  [  432/ 3200]\n",
      "loss: 0.152846  [  448/ 3200]\n",
      "loss: 0.383645  [  464/ 3200]\n",
      "loss: 0.235997  [  480/ 3200]\n",
      "loss: 0.510720  [  496/ 3200]\n",
      "loss: 0.201639  [  512/ 3200]\n",
      "loss: 0.543242  [  528/ 3200]\n",
      "loss: 0.231239  [  544/ 3200]\n",
      "loss: 0.299335  [  560/ 3200]\n",
      "loss: 0.520390  [  576/ 3200]\n",
      "loss: 0.172570  [  592/ 3200]\n",
      "loss: 0.398277  [  608/ 3200]\n",
      "loss: 0.100718  [  624/ 3200]\n",
      "loss: 0.335068  [  640/ 3200]\n",
      "loss: 0.270685  [  656/ 3200]\n",
      "loss: 0.321728  [  672/ 3200]\n",
      "loss: 0.215272  [  688/ 3200]\n",
      "loss: 0.526569  [  704/ 3200]\n",
      "loss: 0.359071  [  720/ 3200]\n",
      "loss: 0.341789  [  736/ 3200]\n",
      "loss: 0.349642  [  752/ 3200]\n",
      "loss: 0.604915  [  768/ 3200]\n",
      "loss: 0.268345  [  784/ 3200]\n",
      "loss: 0.539916  [  800/ 3200]\n",
      "loss: 0.643705  [  816/ 3200]\n",
      "loss: 0.250758  [  832/ 3200]\n",
      "loss: 0.396779  [  848/ 3200]\n",
      "loss: 0.372259  [  864/ 3200]\n",
      "loss: 0.359376  [  880/ 3200]\n",
      "loss: 0.212028  [  896/ 3200]\n",
      "loss: 0.289025  [  912/ 3200]\n",
      "loss: 0.191416  [  928/ 3200]\n",
      "loss: 0.254520  [  944/ 3200]\n",
      "loss: 0.263688  [  960/ 3200]\n",
      "loss: 0.136816  [  976/ 3200]\n",
      "loss: 0.836634  [  992/ 3200]\n",
      "loss: 0.162787  [ 1008/ 3200]\n",
      "loss: 0.161734  [ 1024/ 3200]\n",
      "loss: 0.434318  [ 1040/ 3200]\n",
      "loss: 0.069384  [ 1056/ 3200]\n",
      "loss: 0.271986  [ 1072/ 3200]\n",
      "loss: 0.628846  [ 1088/ 3200]\n",
      "loss: 0.113919  [ 1104/ 3200]\n",
      "loss: 0.284804  [ 1120/ 3200]\n",
      "loss: 0.273843  [ 1136/ 3200]\n",
      "loss: 0.381989  [ 1152/ 3200]\n",
      "loss: 0.343457  [ 1168/ 3200]\n",
      "loss: 0.208662  [ 1184/ 3200]\n",
      "loss: 0.204485  [ 1200/ 3200]\n",
      "loss: 0.197290  [ 1216/ 3200]\n",
      "loss: 0.283098  [ 1232/ 3200]\n",
      "loss: 0.461933  [ 1248/ 3200]\n",
      "loss: 0.243108  [ 1264/ 3200]\n",
      "loss: 0.256982  [ 1280/ 3200]\n",
      "loss: 0.195551  [ 1296/ 3200]\n",
      "loss: 0.259584  [ 1312/ 3200]\n",
      "loss: 0.106826  [ 1328/ 3200]\n",
      "loss: 0.201206  [ 1344/ 3200]\n",
      "loss: 0.207086  [ 1360/ 3200]\n",
      "loss: 0.129553  [ 1376/ 3200]\n",
      "loss: 0.243324  [ 1392/ 3200]\n",
      "loss: 0.178905  [ 1408/ 3200]\n",
      "loss: 0.248801  [ 1424/ 3200]\n",
      "loss: 0.095019  [ 1440/ 3200]\n",
      "loss: 0.177689  [ 1456/ 3200]\n",
      "loss: 0.229209  [ 1472/ 3200]\n",
      "loss: 0.327548  [ 1488/ 3200]\n",
      "loss: 0.057144  [ 1504/ 3200]\n",
      "loss: 0.198181  [ 1520/ 3200]\n",
      "loss: 0.617145  [ 1536/ 3200]\n",
      "loss: 0.358916  [ 1552/ 3200]\n",
      "loss: 0.399175  [ 1568/ 3200]\n",
      "loss: 0.246544  [ 1584/ 3200]\n",
      "loss: 0.293064  [ 1600/ 3200]\n",
      "loss: 0.278295  [ 1616/ 3200]\n",
      "loss: 0.200594  [ 1632/ 3200]\n",
      "loss: 0.250467  [ 1648/ 3200]\n",
      "loss: 0.266620  [ 1664/ 3200]\n",
      "loss: 0.562771  [ 1680/ 3200]\n",
      "loss: 0.230010  [ 1696/ 3200]\n",
      "loss: 0.162725  [ 1712/ 3200]\n",
      "loss: 0.383420  [ 1728/ 3200]\n",
      "loss: 0.617327  [ 1744/ 3200]\n",
      "loss: 0.321029  [ 1760/ 3200]\n",
      "loss: 0.603737  [ 1776/ 3200]\n",
      "loss: 0.098873  [ 1792/ 3200]\n",
      "loss: 0.509014  [ 1808/ 3200]\n",
      "loss: 0.182931  [ 1824/ 3200]\n",
      "loss: 0.200298  [ 1840/ 3200]\n",
      "loss: 0.148229  [ 1856/ 3200]\n",
      "loss: 0.242193  [ 1872/ 3200]\n",
      "loss: 0.209148  [ 1888/ 3200]\n",
      "loss: 0.382087  [ 1904/ 3200]\n",
      "loss: 0.312992  [ 1920/ 3200]\n",
      "loss: 0.382989  [ 1936/ 3200]\n",
      "loss: 0.351802  [ 1952/ 3200]\n",
      "loss: 0.249483  [ 1968/ 3200]\n",
      "loss: 0.591364  [ 1984/ 3200]\n",
      "loss: 0.130396  [ 2000/ 3200]\n",
      "loss: 0.282913  [ 2016/ 3200]\n",
      "loss: 0.228990  [ 2032/ 3200]\n",
      "loss: 0.184546  [ 2048/ 3200]\n",
      "loss: 0.071948  [ 2064/ 3200]\n",
      "loss: 0.473862  [ 2080/ 3200]\n",
      "loss: 0.106839  [ 2096/ 3200]\n",
      "loss: 0.232188  [ 2112/ 3200]\n",
      "loss: 0.355540  [ 2128/ 3200]\n",
      "loss: 0.191753  [ 2144/ 3200]\n",
      "loss: 0.329684  [ 2160/ 3200]\n",
      "loss: 0.224405  [ 2176/ 3200]\n",
      "loss: 0.130115  [ 2192/ 3200]\n",
      "loss: 0.217513  [ 2208/ 3200]\n",
      "loss: 0.550164  [ 2224/ 3200]\n",
      "loss: 0.263863  [ 2240/ 3200]\n",
      "loss: 0.659928  [ 2256/ 3200]\n",
      "loss: 0.063009  [ 2272/ 3200]\n",
      "loss: 0.407258  [ 2288/ 3200]\n",
      "loss: 0.481320  [ 2304/ 3200]\n",
      "loss: 0.460446  [ 2320/ 3200]\n",
      "loss: 0.290689  [ 2336/ 3200]\n",
      "loss: 0.342604  [ 2352/ 3200]\n",
      "loss: 0.142016  [ 2368/ 3200]\n",
      "loss: 0.142744  [ 2384/ 3200]\n",
      "loss: 0.260713  [ 2400/ 3200]\n",
      "loss: 0.092638  [ 2416/ 3200]\n",
      "loss: 0.140398  [ 2432/ 3200]\n",
      "loss: 0.381409  [ 2448/ 3200]\n",
      "loss: 0.593494  [ 2464/ 3200]\n",
      "loss: 0.369426  [ 2480/ 3200]\n",
      "loss: 0.280933  [ 2496/ 3200]\n",
      "loss: 0.344401  [ 2512/ 3200]\n",
      "loss: 0.228422  [ 2528/ 3200]\n",
      "loss: 0.290334  [ 2544/ 3200]\n",
      "loss: 0.432812  [ 2560/ 3200]\n",
      "loss: 0.146033  [ 2576/ 3200]\n",
      "loss: 0.143417  [ 2592/ 3200]\n",
      "loss: 0.239434  [ 2608/ 3200]\n",
      "loss: 0.233691  [ 2624/ 3200]\n",
      "loss: 0.269538  [ 2640/ 3200]\n",
      "loss: 0.206383  [ 2656/ 3200]\n",
      "loss: 0.245803  [ 2672/ 3200]\n",
      "loss: 0.300753  [ 2688/ 3200]\n",
      "loss: 0.210918  [ 2704/ 3200]\n",
      "loss: 0.197361  [ 2720/ 3200]\n",
      "loss: 0.499970  [ 2736/ 3200]\n",
      "loss: 0.246037  [ 2752/ 3200]\n",
      "loss: 0.296307  [ 2768/ 3200]\n",
      "loss: 0.432163  [ 2784/ 3200]\n",
      "loss: 0.189946  [ 2800/ 3200]\n",
      "loss: 0.391901  [ 2816/ 3200]\n",
      "loss: 0.232462  [ 2832/ 3200]\n",
      "loss: 0.131113  [ 2848/ 3200]\n",
      "loss: 0.250042  [ 2864/ 3200]\n",
      "loss: 0.245368  [ 2880/ 3200]\n",
      "loss: 0.228220  [ 2896/ 3200]\n",
      "loss: 0.368773  [ 2912/ 3200]\n",
      "loss: 0.425417  [ 2928/ 3200]\n",
      "loss: 0.416272  [ 2944/ 3200]\n",
      "loss: 0.320080  [ 2960/ 3200]\n",
      "loss: 0.202947  [ 2976/ 3200]\n",
      "loss: 0.228836  [ 2992/ 3200]\n",
      "loss: 0.169024  [ 3008/ 3200]\n",
      "loss: 0.137114  [ 3024/ 3200]\n",
      "loss: 0.205640  [ 3040/ 3200]\n",
      "loss: 0.093696  [ 3056/ 3200]\n",
      "loss: 0.141514  [ 3072/ 3200]\n",
      "loss: 0.592446  [ 3088/ 3200]\n",
      "loss: 0.418586  [ 3104/ 3200]\n",
      "loss: 0.562533  [ 3120/ 3200]\n",
      "loss: 0.126890  [ 3136/ 3200]\n",
      "loss: 0.155143  [ 3152/ 3200]\n",
      "loss: 0.124982  [ 3168/ 3200]\n",
      "loss: 0.387097  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035890\n",
      "f1 macro averaged score: 0.779057\n",
      "Accuracy               : 77.8%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  18,   0,   8],\n",
      "        [ 17, 142,  16,  25],\n",
      "        [  0,  36, 148,  16],\n",
      "        [  8,  24,  10, 158]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1975e-03.\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 0.173021  [    0/ 3200]\n",
      "loss: 0.474963  [   16/ 3200]\n",
      "loss: 0.189903  [   32/ 3200]\n",
      "loss: 0.647779  [   48/ 3200]\n",
      "loss: 0.553945  [   64/ 3200]\n",
      "loss: 0.394408  [   80/ 3200]\n",
      "loss: 0.237654  [   96/ 3200]\n",
      "loss: 0.381438  [  112/ 3200]\n",
      "loss: 0.226772  [  128/ 3200]\n",
      "loss: 0.155899  [  144/ 3200]\n",
      "loss: 0.211312  [  160/ 3200]\n",
      "loss: 0.200396  [  176/ 3200]\n",
      "loss: 0.294417  [  192/ 3200]\n",
      "loss: 0.116830  [  208/ 3200]\n",
      "loss: 0.202369  [  224/ 3200]\n",
      "loss: 0.160341  [  240/ 3200]\n",
      "loss: 0.092009  [  256/ 3200]\n",
      "loss: 0.192992  [  272/ 3200]\n",
      "loss: 0.202743  [  288/ 3200]\n",
      "loss: 0.513857  [  304/ 3200]\n",
      "loss: 0.708839  [  320/ 3200]\n",
      "loss: 0.662852  [  336/ 3200]\n",
      "loss: 0.341420  [  352/ 3200]\n",
      "loss: 0.454252  [  368/ 3200]\n",
      "loss: 0.291241  [  384/ 3200]\n",
      "loss: 0.151254  [  400/ 3200]\n",
      "loss: 0.423985  [  416/ 3200]\n",
      "loss: 0.166706  [  432/ 3200]\n",
      "loss: 0.439533  [  448/ 3200]\n",
      "loss: 0.433641  [  464/ 3200]\n",
      "loss: 0.107992  [  480/ 3200]\n",
      "loss: 0.281880  [  496/ 3200]\n",
      "loss: 0.414505  [  512/ 3200]\n",
      "loss: 0.394859  [  528/ 3200]\n",
      "loss: 0.242362  [  544/ 3200]\n",
      "loss: 0.250118  [  560/ 3200]\n",
      "loss: 0.102313  [  576/ 3200]\n",
      "loss: 0.121280  [  592/ 3200]\n",
      "loss: 0.157335  [  608/ 3200]\n",
      "loss: 0.467827  [  624/ 3200]\n",
      "loss: 0.541306  [  640/ 3200]\n",
      "loss: 0.497402  [  656/ 3200]\n",
      "loss: 0.208156  [  672/ 3200]\n",
      "loss: 0.440072  [  688/ 3200]\n",
      "loss: 0.190517  [  704/ 3200]\n",
      "loss: 0.500492  [  720/ 3200]\n",
      "loss: 0.315318  [  736/ 3200]\n",
      "loss: 0.404483  [  752/ 3200]\n",
      "loss: 0.117810  [  768/ 3200]\n",
      "loss: 0.137599  [  784/ 3200]\n",
      "loss: 0.165863  [  800/ 3200]\n",
      "loss: 0.214728  [  816/ 3200]\n",
      "loss: 0.261016  [  832/ 3200]\n",
      "loss: 0.315765  [  848/ 3200]\n",
      "loss: 0.198075  [  864/ 3200]\n",
      "loss: 0.346044  [  880/ 3200]\n",
      "loss: 0.182985  [  896/ 3200]\n",
      "loss: 0.403521  [  912/ 3200]\n",
      "loss: 0.228168  [  928/ 3200]\n",
      "loss: 0.268467  [  944/ 3200]\n",
      "loss: 0.158783  [  960/ 3200]\n",
      "loss: 0.171598  [  976/ 3200]\n",
      "loss: 0.143066  [  992/ 3200]\n",
      "loss: 0.262250  [ 1008/ 3200]\n",
      "loss: 0.104325  [ 1024/ 3200]\n",
      "loss: 0.296272  [ 1040/ 3200]\n",
      "loss: 0.182782  [ 1056/ 3200]\n",
      "loss: 0.191370  [ 1072/ 3200]\n",
      "loss: 0.134173  [ 1088/ 3200]\n",
      "loss: 0.370400  [ 1104/ 3200]\n",
      "loss: 0.477639  [ 1120/ 3200]\n",
      "loss: 0.051788  [ 1136/ 3200]\n",
      "loss: 0.261004  [ 1152/ 3200]\n",
      "loss: 0.161764  [ 1168/ 3200]\n",
      "loss: 0.208948  [ 1184/ 3200]\n",
      "loss: 0.344485  [ 1200/ 3200]\n",
      "loss: 0.106712  [ 1216/ 3200]\n",
      "loss: 0.160593  [ 1232/ 3200]\n",
      "loss: 0.096683  [ 1248/ 3200]\n",
      "loss: 0.151068  [ 1264/ 3200]\n",
      "loss: 0.146138  [ 1280/ 3200]\n",
      "loss: 0.131263  [ 1296/ 3200]\n",
      "loss: 0.148552  [ 1312/ 3200]\n",
      "loss: 0.185813  [ 1328/ 3200]\n",
      "loss: 0.029401  [ 1344/ 3200]\n",
      "loss: 0.446771  [ 1360/ 3200]\n",
      "loss: 0.350192  [ 1376/ 3200]\n",
      "loss: 0.418761  [ 1392/ 3200]\n",
      "loss: 0.542521  [ 1408/ 3200]\n",
      "loss: 0.181601  [ 1424/ 3200]\n",
      "loss: 0.221109  [ 1440/ 3200]\n",
      "loss: 0.671626  [ 1456/ 3200]\n",
      "loss: 0.069576  [ 1472/ 3200]\n",
      "loss: 0.256635  [ 1488/ 3200]\n",
      "loss: 0.173803  [ 1504/ 3200]\n",
      "loss: 0.144147  [ 1520/ 3200]\n",
      "loss: 0.317084  [ 1536/ 3200]\n",
      "loss: 0.134264  [ 1552/ 3200]\n",
      "loss: 0.133282  [ 1568/ 3200]\n",
      "loss: 0.227891  [ 1584/ 3200]\n",
      "loss: 0.425191  [ 1600/ 3200]\n",
      "loss: 0.583772  [ 1616/ 3200]\n",
      "loss: 0.210884  [ 1632/ 3200]\n",
      "loss: 0.147432  [ 1648/ 3200]\n",
      "loss: 0.282533  [ 1664/ 3200]\n",
      "loss: 0.252260  [ 1680/ 3200]\n",
      "loss: 0.230999  [ 1696/ 3200]\n",
      "loss: 0.152222  [ 1712/ 3200]\n",
      "loss: 0.351423  [ 1728/ 3200]\n",
      "loss: 0.362581  [ 1744/ 3200]\n",
      "loss: 0.231665  [ 1760/ 3200]\n",
      "loss: 0.326175  [ 1776/ 3200]\n",
      "loss: 0.102854  [ 1792/ 3200]\n",
      "loss: 0.163363  [ 1808/ 3200]\n",
      "loss: 0.204688  [ 1824/ 3200]\n",
      "loss: 0.402566  [ 1840/ 3200]\n",
      "loss: 0.067554  [ 1856/ 3200]\n",
      "loss: 0.157944  [ 1872/ 3200]\n",
      "loss: 0.321651  [ 1888/ 3200]\n",
      "loss: 0.503525  [ 1904/ 3200]\n",
      "loss: 0.141320  [ 1920/ 3200]\n",
      "loss: 0.436811  [ 1936/ 3200]\n",
      "loss: 0.261075  [ 1952/ 3200]\n",
      "loss: 0.083568  [ 1968/ 3200]\n",
      "loss: 0.097578  [ 1984/ 3200]\n",
      "loss: 0.415337  [ 2000/ 3200]\n",
      "loss: 0.092881  [ 2016/ 3200]\n",
      "loss: 0.189603  [ 2032/ 3200]\n",
      "loss: 0.360410  [ 2048/ 3200]\n",
      "loss: 0.552772  [ 2064/ 3200]\n",
      "loss: 0.517537  [ 2080/ 3200]\n",
      "loss: 0.372348  [ 2096/ 3200]\n",
      "loss: 0.483345  [ 2112/ 3200]\n",
      "loss: 0.144947  [ 2128/ 3200]\n",
      "loss: 0.138839  [ 2144/ 3200]\n",
      "loss: 0.693898  [ 2160/ 3200]\n",
      "loss: 0.533015  [ 2176/ 3200]\n",
      "loss: 0.550852  [ 2192/ 3200]\n",
      "loss: 0.253615  [ 2208/ 3200]\n",
      "loss: 0.080755  [ 2224/ 3200]\n",
      "loss: 0.114153  [ 2240/ 3200]\n",
      "loss: 0.295816  [ 2256/ 3200]\n",
      "loss: 0.513566  [ 2272/ 3200]\n",
      "loss: 0.064733  [ 2288/ 3200]\n",
      "loss: 0.121607  [ 2304/ 3200]\n",
      "loss: 0.096676  [ 2320/ 3200]\n",
      "loss: 0.379552  [ 2336/ 3200]\n",
      "loss: 0.314376  [ 2352/ 3200]\n",
      "loss: 0.110374  [ 2368/ 3200]\n",
      "loss: 0.233150  [ 2384/ 3200]\n",
      "loss: 0.181546  [ 2400/ 3200]\n",
      "loss: 0.278439  [ 2416/ 3200]\n",
      "loss: 0.419208  [ 2432/ 3200]\n",
      "loss: 0.182495  [ 2448/ 3200]\n",
      "loss: 0.281814  [ 2464/ 3200]\n",
      "loss: 0.121666  [ 2480/ 3200]\n",
      "loss: 0.252969  [ 2496/ 3200]\n",
      "loss: 0.254762  [ 2512/ 3200]\n",
      "loss: 0.223392  [ 2528/ 3200]\n",
      "loss: 0.140326  [ 2544/ 3200]\n",
      "loss: 0.140225  [ 2560/ 3200]\n",
      "loss: 0.305443  [ 2576/ 3200]\n",
      "loss: 0.333185  [ 2592/ 3200]\n",
      "loss: 0.228204  [ 2608/ 3200]\n",
      "loss: 0.146386  [ 2624/ 3200]\n",
      "loss: 0.336453  [ 2640/ 3200]\n",
      "loss: 0.553059  [ 2656/ 3200]\n",
      "loss: 0.448864  [ 2672/ 3200]\n",
      "loss: 0.281346  [ 2688/ 3200]\n",
      "loss: 0.281881  [ 2704/ 3200]\n",
      "loss: 0.057560  [ 2720/ 3200]\n",
      "loss: 0.189245  [ 2736/ 3200]\n",
      "loss: 0.371645  [ 2752/ 3200]\n",
      "loss: 0.195411  [ 2768/ 3200]\n",
      "loss: 0.145778  [ 2784/ 3200]\n",
      "loss: 0.157483  [ 2800/ 3200]\n",
      "loss: 0.107083  [ 2816/ 3200]\n",
      "loss: 0.302696  [ 2832/ 3200]\n",
      "loss: 0.235667  [ 2848/ 3200]\n",
      "loss: 0.252416  [ 2864/ 3200]\n",
      "loss: 0.334385  [ 2880/ 3200]\n",
      "loss: 0.198994  [ 2896/ 3200]\n",
      "loss: 0.131461  [ 2912/ 3200]\n",
      "loss: 0.165514  [ 2928/ 3200]\n",
      "loss: 0.061715  [ 2944/ 3200]\n",
      "loss: 0.290192  [ 2960/ 3200]\n",
      "loss: 0.231047  [ 2976/ 3200]\n",
      "loss: 0.142292  [ 2992/ 3200]\n",
      "loss: 0.165651  [ 3008/ 3200]\n",
      "loss: 0.173694  [ 3024/ 3200]\n",
      "loss: 0.092450  [ 3040/ 3200]\n",
      "loss: 0.200616  [ 3056/ 3200]\n",
      "loss: 0.405712  [ 3072/ 3200]\n",
      "loss: 0.419261  [ 3088/ 3200]\n",
      "loss: 0.442621  [ 3104/ 3200]\n",
      "loss: 0.223947  [ 3120/ 3200]\n",
      "loss: 0.160870  [ 3136/ 3200]\n",
      "loss: 0.490309  [ 3152/ 3200]\n",
      "loss: 0.415156  [ 3168/ 3200]\n",
      "loss: 0.315308  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036598\n",
      "f1 macro averaged score: 0.785722\n",
      "Accuracy               : 79.1%\n",
      "Confusion matrix       :\n",
      "tensor([[187,   2,   0,  11],\n",
      "        [ 22, 114,  25,  39],\n",
      "        [  0,  21, 169,  10],\n",
      "        [ 11,  12,  14, 163]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1376e-03.\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 0.232797  [    0/ 3200]\n",
      "loss: 0.346721  [   16/ 3200]\n",
      "loss: 0.284852  [   32/ 3200]\n",
      "loss: 0.105911  [   48/ 3200]\n",
      "loss: 0.238004  [   64/ 3200]\n",
      "loss: 0.486656  [   80/ 3200]\n",
      "loss: 0.261764  [   96/ 3200]\n",
      "loss: 0.281553  [  112/ 3200]\n",
      "loss: 0.194539  [  128/ 3200]\n",
      "loss: 0.311277  [  144/ 3200]\n",
      "loss: 0.291766  [  160/ 3200]\n",
      "loss: 0.191404  [  176/ 3200]\n",
      "loss: 0.290772  [  192/ 3200]\n",
      "loss: 0.463338  [  208/ 3200]\n",
      "loss: 0.065331  [  224/ 3200]\n",
      "loss: 0.271436  [  240/ 3200]\n",
      "loss: 0.430620  [  256/ 3200]\n",
      "loss: 0.279478  [  272/ 3200]\n",
      "loss: 0.244191  [  288/ 3200]\n",
      "loss: 0.342194  [  304/ 3200]\n",
      "loss: 0.381598  [  320/ 3200]\n",
      "loss: 0.180041  [  336/ 3200]\n",
      "loss: 0.170208  [  352/ 3200]\n",
      "loss: 0.238227  [  368/ 3200]\n",
      "loss: 0.095481  [  384/ 3200]\n",
      "loss: 0.123519  [  400/ 3200]\n",
      "loss: 0.108494  [  416/ 3200]\n",
      "loss: 0.054115  [  432/ 3200]\n",
      "loss: 0.057400  [  448/ 3200]\n",
      "loss: 0.062582  [  464/ 3200]\n",
      "loss: 1.078825  [  480/ 3200]\n",
      "loss: 0.060948  [  496/ 3200]\n",
      "loss: 0.491749  [  512/ 3200]\n",
      "loss: 0.085336  [  528/ 3200]\n",
      "loss: 0.092102  [  544/ 3200]\n",
      "loss: 0.110340  [  560/ 3200]\n",
      "loss: 0.285161  [  576/ 3200]\n",
      "loss: 0.263334  [  592/ 3200]\n",
      "loss: 0.231990  [  608/ 3200]\n",
      "loss: 0.289965  [  624/ 3200]\n",
      "loss: 0.102989  [  640/ 3200]\n",
      "loss: 0.499740  [  656/ 3200]\n",
      "loss: 0.285491  [  672/ 3200]\n",
      "loss: 0.381518  [  688/ 3200]\n",
      "loss: 0.118360  [  704/ 3200]\n",
      "loss: 0.316559  [  720/ 3200]\n",
      "loss: 0.298292  [  736/ 3200]\n",
      "loss: 0.343924  [  752/ 3200]\n",
      "loss: 0.204993  [  768/ 3200]\n",
      "loss: 0.278995  [  784/ 3200]\n",
      "loss: 0.063101  [  800/ 3200]\n",
      "loss: 0.216255  [  816/ 3200]\n",
      "loss: 0.163957  [  832/ 3200]\n",
      "loss: 0.172367  [  848/ 3200]\n",
      "loss: 0.392888  [  864/ 3200]\n",
      "loss: 0.385408  [  880/ 3200]\n",
      "loss: 0.102674  [  896/ 3200]\n",
      "loss: 0.158507  [  912/ 3200]\n",
      "loss: 0.164544  [  928/ 3200]\n",
      "loss: 0.257792  [  944/ 3200]\n",
      "loss: 0.190020  [  960/ 3200]\n",
      "loss: 0.223822  [  976/ 3200]\n",
      "loss: 0.398657  [  992/ 3200]\n",
      "loss: 0.453816  [ 1008/ 3200]\n",
      "loss: 0.303706  [ 1024/ 3200]\n",
      "loss: 0.126916  [ 1040/ 3200]\n",
      "loss: 0.318883  [ 1056/ 3200]\n",
      "loss: 0.205934  [ 1072/ 3200]\n",
      "loss: 0.196133  [ 1088/ 3200]\n",
      "loss: 0.466659  [ 1104/ 3200]\n",
      "loss: 0.106369  [ 1120/ 3200]\n",
      "loss: 0.354663  [ 1136/ 3200]\n",
      "loss: 0.174780  [ 1152/ 3200]\n",
      "loss: 0.158815  [ 1168/ 3200]\n",
      "loss: 0.484597  [ 1184/ 3200]\n",
      "loss: 0.095547  [ 1200/ 3200]\n",
      "loss: 0.585544  [ 1216/ 3200]\n",
      "loss: 0.142633  [ 1232/ 3200]\n",
      "loss: 0.229509  [ 1248/ 3200]\n",
      "loss: 0.130259  [ 1264/ 3200]\n",
      "loss: 0.195387  [ 1280/ 3200]\n",
      "loss: 0.072473  [ 1296/ 3200]\n",
      "loss: 0.247053  [ 1312/ 3200]\n",
      "loss: 0.097936  [ 1328/ 3200]\n",
      "loss: 0.202997  [ 1344/ 3200]\n",
      "loss: 0.076406  [ 1360/ 3200]\n",
      "loss: 0.356963  [ 1376/ 3200]\n",
      "loss: 0.234560  [ 1392/ 3200]\n",
      "loss: 0.266382  [ 1408/ 3200]\n",
      "loss: 0.155408  [ 1424/ 3200]\n",
      "loss: 0.174828  [ 1440/ 3200]\n",
      "loss: 0.128628  [ 1456/ 3200]\n",
      "loss: 0.128648  [ 1472/ 3200]\n",
      "loss: 0.164235  [ 1488/ 3200]\n",
      "loss: 0.312985  [ 1504/ 3200]\n",
      "loss: 0.295708  [ 1520/ 3200]\n",
      "loss: 0.284736  [ 1536/ 3200]\n",
      "loss: 0.157656  [ 1552/ 3200]\n",
      "loss: 0.487987  [ 1568/ 3200]\n",
      "loss: 0.189391  [ 1584/ 3200]\n",
      "loss: 0.398845  [ 1600/ 3200]\n",
      "loss: 0.205834  [ 1616/ 3200]\n",
      "loss: 0.178032  [ 1632/ 3200]\n",
      "loss: 0.136879  [ 1648/ 3200]\n",
      "loss: 0.129188  [ 1664/ 3200]\n",
      "loss: 0.447884  [ 1680/ 3200]\n",
      "loss: 0.443951  [ 1696/ 3200]\n",
      "loss: 0.196927  [ 1712/ 3200]\n",
      "loss: 0.385699  [ 1728/ 3200]\n",
      "loss: 0.086322  [ 1744/ 3200]\n",
      "loss: 0.092470  [ 1760/ 3200]\n",
      "loss: 0.077866  [ 1776/ 3200]\n",
      "loss: 0.151739  [ 1792/ 3200]\n",
      "loss: 0.209150  [ 1808/ 3200]\n",
      "loss: 0.219701  [ 1824/ 3200]\n",
      "loss: 0.106020  [ 1840/ 3200]\n",
      "loss: 0.278722  [ 1856/ 3200]\n",
      "loss: 0.072840  [ 1872/ 3200]\n",
      "loss: 0.165238  [ 1888/ 3200]\n",
      "loss: 0.334264  [ 1904/ 3200]\n",
      "loss: 0.118389  [ 1920/ 3200]\n",
      "loss: 0.065344  [ 1936/ 3200]\n",
      "loss: 0.218318  [ 1952/ 3200]\n",
      "loss: 0.202160  [ 1968/ 3200]\n",
      "loss: 0.177963  [ 1984/ 3200]\n",
      "loss: 0.190053  [ 2000/ 3200]\n",
      "loss: 0.239397  [ 2016/ 3200]\n",
      "loss: 0.215260  [ 2032/ 3200]\n",
      "loss: 0.249890  [ 2048/ 3200]\n",
      "loss: 0.204386  [ 2064/ 3200]\n",
      "loss: 0.211772  [ 2080/ 3200]\n",
      "loss: 0.256915  [ 2096/ 3200]\n",
      "loss: 0.241807  [ 2112/ 3200]\n",
      "loss: 0.431344  [ 2128/ 3200]\n",
      "loss: 0.455726  [ 2144/ 3200]\n",
      "loss: 0.141898  [ 2160/ 3200]\n",
      "loss: 0.106987  [ 2176/ 3200]\n",
      "loss: 0.251018  [ 2192/ 3200]\n",
      "loss: 0.146350  [ 2208/ 3200]\n",
      "loss: 0.151257  [ 2224/ 3200]\n",
      "loss: 0.270400  [ 2240/ 3200]\n",
      "loss: 0.412280  [ 2256/ 3200]\n",
      "loss: 0.203030  [ 2272/ 3200]\n",
      "loss: 0.125216  [ 2288/ 3200]\n",
      "loss: 0.571866  [ 2304/ 3200]\n",
      "loss: 0.273068  [ 2320/ 3200]\n",
      "loss: 0.189175  [ 2336/ 3200]\n",
      "loss: 0.075145  [ 2352/ 3200]\n",
      "loss: 0.226471  [ 2368/ 3200]\n",
      "loss: 0.166928  [ 2384/ 3200]\n",
      "loss: 0.262836  [ 2400/ 3200]\n",
      "loss: 0.411162  [ 2416/ 3200]\n",
      "loss: 0.322675  [ 2432/ 3200]\n",
      "loss: 0.387400  [ 2448/ 3200]\n",
      "loss: 0.067255  [ 2464/ 3200]\n",
      "loss: 0.074670  [ 2480/ 3200]\n",
      "loss: 0.259341  [ 2496/ 3200]\n",
      "loss: 0.180742  [ 2512/ 3200]\n",
      "loss: 0.080432  [ 2528/ 3200]\n",
      "loss: 0.358990  [ 2544/ 3200]\n",
      "loss: 0.127821  [ 2560/ 3200]\n",
      "loss: 0.132411  [ 2576/ 3200]\n",
      "loss: 0.171241  [ 2592/ 3200]\n",
      "loss: 0.221985  [ 2608/ 3200]\n",
      "loss: 0.144712  [ 2624/ 3200]\n",
      "loss: 0.091069  [ 2640/ 3200]\n",
      "loss: 0.226239  [ 2656/ 3200]\n",
      "loss: 0.059441  [ 2672/ 3200]\n",
      "loss: 0.212470  [ 2688/ 3200]\n",
      "loss: 0.486525  [ 2704/ 3200]\n",
      "loss: 0.183100  [ 2720/ 3200]\n",
      "loss: 0.056851  [ 2736/ 3200]\n",
      "loss: 0.407807  [ 2752/ 3200]\n",
      "loss: 0.037660  [ 2768/ 3200]\n",
      "loss: 0.184386  [ 2784/ 3200]\n",
      "loss: 0.230010  [ 2800/ 3200]\n",
      "loss: 0.370177  [ 2816/ 3200]\n",
      "loss: 0.113274  [ 2832/ 3200]\n",
      "loss: 0.397528  [ 2848/ 3200]\n",
      "loss: 0.277914  [ 2864/ 3200]\n",
      "loss: 0.152167  [ 2880/ 3200]\n",
      "loss: 0.202050  [ 2896/ 3200]\n",
      "loss: 0.075995  [ 2912/ 3200]\n",
      "loss: 0.330618  [ 2928/ 3200]\n",
      "loss: 0.345458  [ 2944/ 3200]\n",
      "loss: 0.433483  [ 2960/ 3200]\n",
      "loss: 0.241490  [ 2976/ 3200]\n",
      "loss: 0.206668  [ 2992/ 3200]\n",
      "loss: 0.347973  [ 3008/ 3200]\n",
      "loss: 0.286495  [ 3024/ 3200]\n",
      "loss: 0.249982  [ 3040/ 3200]\n",
      "loss: 0.152411  [ 3056/ 3200]\n",
      "loss: 0.160572  [ 3072/ 3200]\n",
      "loss: 0.179771  [ 3088/ 3200]\n",
      "loss: 0.374412  [ 3104/ 3200]\n",
      "loss: 0.331660  [ 3120/ 3200]\n",
      "loss: 0.249548  [ 3136/ 3200]\n",
      "loss: 0.083083  [ 3152/ 3200]\n",
      "loss: 0.391907  [ 3168/ 3200]\n",
      "loss: 0.159393  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.037828\n",
      "f1 macro averaged score: 0.773292\n",
      "Accuracy               : 77.9%\n",
      "Confusion matrix       :\n",
      "tensor([[184,   2,   0,  14],\n",
      "        [ 20, 108,  23,  49],\n",
      "        [  0,  20, 164,  16],\n",
      "        [  9,  10,  14, 167]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0807e-03.\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 0.195645  [    0/ 3200]\n",
      "loss: 0.190207  [   16/ 3200]\n",
      "loss: 0.108226  [   32/ 3200]\n",
      "loss: 0.249839  [   48/ 3200]\n",
      "loss: 0.158408  [   64/ 3200]\n",
      "loss: 0.157488  [   80/ 3200]\n",
      "loss: 0.333967  [   96/ 3200]\n",
      "loss: 0.207713  [  112/ 3200]\n",
      "loss: 0.225301  [  128/ 3200]\n",
      "loss: 0.097806  [  144/ 3200]\n",
      "loss: 0.151934  [  160/ 3200]\n",
      "loss: 0.267933  [  176/ 3200]\n",
      "loss: 0.088799  [  192/ 3200]\n",
      "loss: 0.230624  [  208/ 3200]\n",
      "loss: 0.134228  [  224/ 3200]\n",
      "loss: 0.112404  [  240/ 3200]\n",
      "loss: 0.429298  [  256/ 3200]\n",
      "loss: 0.040584  [  272/ 3200]\n",
      "loss: 0.076194  [  288/ 3200]\n",
      "loss: 0.146846  [  304/ 3200]\n",
      "loss: 0.420472  [  320/ 3200]\n",
      "loss: 0.366531  [  336/ 3200]\n",
      "loss: 0.173963  [  352/ 3200]\n",
      "loss: 0.107935  [  368/ 3200]\n",
      "loss: 0.359340  [  384/ 3200]\n",
      "loss: 0.279057  [  400/ 3200]\n",
      "loss: 0.139712  [  416/ 3200]\n",
      "loss: 0.241238  [  432/ 3200]\n",
      "loss: 0.142217  [  448/ 3200]\n",
      "loss: 0.155032  [  464/ 3200]\n",
      "loss: 0.052203  [  480/ 3200]\n",
      "loss: 0.213553  [  496/ 3200]\n",
      "loss: 0.139844  [  512/ 3200]\n",
      "loss: 0.267580  [  528/ 3200]\n",
      "loss: 0.120672  [  544/ 3200]\n",
      "loss: 0.206367  [  560/ 3200]\n",
      "loss: 0.172275  [  576/ 3200]\n",
      "loss: 0.155077  [  592/ 3200]\n",
      "loss: 0.281925  [  608/ 3200]\n",
      "loss: 0.162498  [  624/ 3200]\n",
      "loss: 0.341984  [  640/ 3200]\n",
      "loss: 0.260041  [  656/ 3200]\n",
      "loss: 0.257852  [  672/ 3200]\n",
      "loss: 0.330817  [  688/ 3200]\n",
      "loss: 0.153920  [  704/ 3200]\n",
      "loss: 0.336379  [  720/ 3200]\n",
      "loss: 0.318214  [  736/ 3200]\n",
      "loss: 0.121112  [  752/ 3200]\n",
      "loss: 0.260401  [  768/ 3200]\n",
      "loss: 0.224622  [  784/ 3200]\n",
      "loss: 0.137440  [  800/ 3200]\n",
      "loss: 0.148789  [  816/ 3200]\n",
      "loss: 0.225528  [  832/ 3200]\n",
      "loss: 0.129421  [  848/ 3200]\n",
      "loss: 0.073255  [  864/ 3200]\n",
      "loss: 0.265981  [  880/ 3200]\n",
      "loss: 0.073692  [  896/ 3200]\n",
      "loss: 0.073954  [  912/ 3200]\n",
      "loss: 0.386157  [  928/ 3200]\n",
      "loss: 0.241917  [  944/ 3200]\n",
      "loss: 0.349045  [  960/ 3200]\n",
      "loss: 0.101284  [  976/ 3200]\n",
      "loss: 0.220770  [  992/ 3200]\n",
      "loss: 0.216565  [ 1008/ 3200]\n",
      "loss: 0.221794  [ 1024/ 3200]\n",
      "loss: 0.179065  [ 1040/ 3200]\n",
      "loss: 0.260964  [ 1056/ 3200]\n",
      "loss: 0.324292  [ 1072/ 3200]\n",
      "loss: 0.374569  [ 1088/ 3200]\n",
      "loss: 0.310940  [ 1104/ 3200]\n",
      "loss: 0.341459  [ 1120/ 3200]\n",
      "loss: 0.182294  [ 1136/ 3200]\n",
      "loss: 0.174787  [ 1152/ 3200]\n",
      "loss: 0.329065  [ 1168/ 3200]\n",
      "loss: 0.236188  [ 1184/ 3200]\n",
      "loss: 0.333820  [ 1200/ 3200]\n",
      "loss: 0.227103  [ 1216/ 3200]\n",
      "loss: 0.212524  [ 1232/ 3200]\n",
      "loss: 0.155367  [ 1248/ 3200]\n",
      "loss: 0.152613  [ 1264/ 3200]\n",
      "loss: 0.208802  [ 1280/ 3200]\n",
      "loss: 0.255301  [ 1296/ 3200]\n",
      "loss: 0.214928  [ 1312/ 3200]\n",
      "loss: 0.137027  [ 1328/ 3200]\n",
      "loss: 0.048882  [ 1344/ 3200]\n",
      "loss: 0.106730  [ 1360/ 3200]\n",
      "loss: 0.216267  [ 1376/ 3200]\n",
      "loss: 0.167839  [ 1392/ 3200]\n",
      "loss: 0.214433  [ 1408/ 3200]\n",
      "loss: 0.193707  [ 1424/ 3200]\n",
      "loss: 0.262800  [ 1440/ 3200]\n",
      "loss: 0.073025  [ 1456/ 3200]\n",
      "loss: 0.111119  [ 1472/ 3200]\n",
      "loss: 0.305470  [ 1488/ 3200]\n",
      "loss: 0.228690  [ 1504/ 3200]\n",
      "loss: 0.075144  [ 1520/ 3200]\n",
      "loss: 0.137611  [ 1536/ 3200]\n",
      "loss: 0.221654  [ 1552/ 3200]\n",
      "loss: 0.518607  [ 1568/ 3200]\n",
      "loss: 0.126596  [ 1584/ 3200]\n",
      "loss: 0.136423  [ 1600/ 3200]\n",
      "loss: 0.340720  [ 1616/ 3200]\n",
      "loss: 0.086755  [ 1632/ 3200]\n",
      "loss: 0.044337  [ 1648/ 3200]\n",
      "loss: 0.064740  [ 1664/ 3200]\n",
      "loss: 0.244740  [ 1680/ 3200]\n",
      "loss: 0.109828  [ 1696/ 3200]\n",
      "loss: 0.038376  [ 1712/ 3200]\n",
      "loss: 0.157467  [ 1728/ 3200]\n",
      "loss: 0.424967  [ 1744/ 3200]\n",
      "loss: 0.119818  [ 1760/ 3200]\n",
      "loss: 0.278705  [ 1776/ 3200]\n",
      "loss: 0.232033  [ 1792/ 3200]\n",
      "loss: 0.117592  [ 1808/ 3200]\n",
      "loss: 0.279876  [ 1824/ 3200]\n",
      "loss: 0.277817  [ 1840/ 3200]\n",
      "loss: 0.202291  [ 1856/ 3200]\n",
      "loss: 0.048628  [ 1872/ 3200]\n",
      "loss: 0.111977  [ 1888/ 3200]\n",
      "loss: 0.142351  [ 1904/ 3200]\n",
      "loss: 0.088607  [ 1920/ 3200]\n",
      "loss: 0.041570  [ 1936/ 3200]\n",
      "loss: 0.140138  [ 1952/ 3200]\n",
      "loss: 0.104628  [ 1968/ 3200]\n",
      "loss: 0.428036  [ 1984/ 3200]\n",
      "loss: 0.190014  [ 2000/ 3200]\n",
      "loss: 0.068154  [ 2016/ 3200]\n",
      "loss: 0.113637  [ 2032/ 3200]\n",
      "loss: 0.123813  [ 2048/ 3200]\n",
      "loss: 0.207443  [ 2064/ 3200]\n",
      "loss: 0.305120  [ 2080/ 3200]\n",
      "loss: 0.438066  [ 2096/ 3200]\n",
      "loss: 0.219989  [ 2112/ 3200]\n",
      "loss: 0.151536  [ 2128/ 3200]\n",
      "loss: 0.032160  [ 2144/ 3200]\n",
      "loss: 0.437486  [ 2160/ 3200]\n",
      "loss: 0.218024  [ 2176/ 3200]\n",
      "loss: 0.081435  [ 2192/ 3200]\n",
      "loss: 0.141795  [ 2208/ 3200]\n",
      "loss: 0.282517  [ 2224/ 3200]\n",
      "loss: 0.260174  [ 2240/ 3200]\n",
      "loss: 0.253442  [ 2256/ 3200]\n",
      "loss: 0.052208  [ 2272/ 3200]\n",
      "loss: 0.076682  [ 2288/ 3200]\n",
      "loss: 0.103035  [ 2304/ 3200]\n",
      "loss: 0.270289  [ 2320/ 3200]\n",
      "loss: 0.129018  [ 2336/ 3200]\n",
      "loss: 0.150942  [ 2352/ 3200]\n",
      "loss: 0.229651  [ 2368/ 3200]\n",
      "loss: 0.339581  [ 2384/ 3200]\n",
      "loss: 0.309802  [ 2400/ 3200]\n",
      "loss: 0.166011  [ 2416/ 3200]\n",
      "loss: 0.142458  [ 2432/ 3200]\n",
      "loss: 0.338258  [ 2448/ 3200]\n",
      "loss: 0.190450  [ 2464/ 3200]\n",
      "loss: 0.216394  [ 2480/ 3200]\n",
      "loss: 0.080338  [ 2496/ 3200]\n",
      "loss: 0.109329  [ 2512/ 3200]\n",
      "loss: 0.050333  [ 2528/ 3200]\n",
      "loss: 0.124597  [ 2544/ 3200]\n",
      "loss: 0.113512  [ 2560/ 3200]\n",
      "loss: 0.162051  [ 2576/ 3200]\n",
      "loss: 0.149103  [ 2592/ 3200]\n",
      "loss: 0.094320  [ 2608/ 3200]\n",
      "loss: 0.322578  [ 2624/ 3200]\n",
      "loss: 0.533575  [ 2640/ 3200]\n",
      "loss: 0.135293  [ 2656/ 3200]\n",
      "loss: 0.251064  [ 2672/ 3200]\n",
      "loss: 0.406546  [ 2688/ 3200]\n",
      "loss: 0.409903  [ 2704/ 3200]\n",
      "loss: 0.179204  [ 2720/ 3200]\n",
      "loss: 0.411852  [ 2736/ 3200]\n",
      "loss: 0.479956  [ 2752/ 3200]\n",
      "loss: 0.342833  [ 2768/ 3200]\n",
      "loss: 0.049720  [ 2784/ 3200]\n",
      "loss: 0.181504  [ 2800/ 3200]\n",
      "loss: 0.180980  [ 2816/ 3200]\n",
      "loss: 0.118336  [ 2832/ 3200]\n",
      "loss: 0.082047  [ 2848/ 3200]\n",
      "loss: 0.293199  [ 2864/ 3200]\n",
      "loss: 0.117840  [ 2880/ 3200]\n",
      "loss: 0.317990  [ 2896/ 3200]\n",
      "loss: 0.086636  [ 2912/ 3200]\n",
      "loss: 0.139477  [ 2928/ 3200]\n",
      "loss: 0.500930  [ 2944/ 3200]\n",
      "loss: 0.372821  [ 2960/ 3200]\n",
      "loss: 0.072595  [ 2976/ 3200]\n",
      "loss: 0.207013  [ 2992/ 3200]\n",
      "loss: 0.242625  [ 3008/ 3200]\n",
      "loss: 0.140903  [ 3024/ 3200]\n",
      "loss: 0.063730  [ 3040/ 3200]\n",
      "loss: 0.120068  [ 3056/ 3200]\n",
      "loss: 0.139889  [ 3072/ 3200]\n",
      "loss: 0.446343  [ 3088/ 3200]\n",
      "loss: 0.239935  [ 3104/ 3200]\n",
      "loss: 0.170997  [ 3120/ 3200]\n",
      "loss: 0.150566  [ 3136/ 3200]\n",
      "loss: 0.138704  [ 3152/ 3200]\n",
      "loss: 0.339152  [ 3168/ 3200]\n",
      "loss: 0.661693  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038671\n",
      "f1 macro averaged score: 0.779831\n",
      "Accuracy               : 78.4%\n",
      "Confusion matrix       :\n",
      "tensor([[184,   2,   0,  14],\n",
      "        [ 20, 115,  20,  45],\n",
      "        [  0,  20, 164,  16],\n",
      "        [ 11,  13,  12, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0267e-03.\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 0.189144  [    0/ 3200]\n",
      "loss: 0.156775  [   16/ 3200]\n",
      "loss: 0.176737  [   32/ 3200]\n",
      "loss: 0.199702  [   48/ 3200]\n",
      "loss: 0.157939  [   64/ 3200]\n",
      "loss: 0.167559  [   80/ 3200]\n",
      "loss: 0.092558  [   96/ 3200]\n",
      "loss: 0.086185  [  112/ 3200]\n",
      "loss: 0.049494  [  128/ 3200]\n",
      "loss: 0.255149  [  144/ 3200]\n",
      "loss: 0.168981  [  160/ 3200]\n",
      "loss: 0.076336  [  176/ 3200]\n",
      "loss: 0.045457  [  192/ 3200]\n",
      "loss: 0.106094  [  208/ 3200]\n",
      "loss: 0.250231  [  224/ 3200]\n",
      "loss: 0.167103  [  240/ 3200]\n",
      "loss: 0.162844  [  256/ 3200]\n",
      "loss: 0.095262  [  272/ 3200]\n",
      "loss: 0.215739  [  288/ 3200]\n",
      "loss: 0.184558  [  304/ 3200]\n",
      "loss: 0.231478  [  320/ 3200]\n",
      "loss: 0.230108  [  336/ 3200]\n",
      "loss: 0.235815  [  352/ 3200]\n",
      "loss: 0.084228  [  368/ 3200]\n",
      "loss: 0.160872  [  384/ 3200]\n",
      "loss: 0.389087  [  400/ 3200]\n",
      "loss: 0.318132  [  416/ 3200]\n",
      "loss: 0.150535  [  432/ 3200]\n",
      "loss: 0.023313  [  448/ 3200]\n",
      "loss: 0.105802  [  464/ 3200]\n",
      "loss: 0.333492  [  480/ 3200]\n",
      "loss: 0.049876  [  496/ 3200]\n",
      "loss: 0.269404  [  512/ 3200]\n",
      "loss: 0.045647  [  528/ 3200]\n",
      "loss: 0.062786  [  544/ 3200]\n",
      "loss: 0.507843  [  560/ 3200]\n",
      "loss: 0.139007  [  576/ 3200]\n",
      "loss: 0.176950  [  592/ 3200]\n",
      "loss: 0.364281  [  608/ 3200]\n",
      "loss: 0.096727  [  624/ 3200]\n",
      "loss: 0.064905  [  640/ 3200]\n",
      "loss: 0.151293  [  656/ 3200]\n",
      "loss: 0.274462  [  672/ 3200]\n",
      "loss: 0.070996  [  688/ 3200]\n",
      "loss: 0.207159  [  704/ 3200]\n",
      "loss: 0.170882  [  720/ 3200]\n",
      "loss: 0.154647  [  736/ 3200]\n",
      "loss: 0.250113  [  752/ 3200]\n",
      "loss: 0.153139  [  768/ 3200]\n",
      "loss: 0.075482  [  784/ 3200]\n",
      "loss: 0.089396  [  800/ 3200]\n",
      "loss: 0.296840  [  816/ 3200]\n",
      "loss: 0.111711  [  832/ 3200]\n",
      "loss: 0.143317  [  848/ 3200]\n",
      "loss: 0.174916  [  864/ 3200]\n",
      "loss: 0.282541  [  880/ 3200]\n",
      "loss: 0.071991  [  896/ 3200]\n",
      "loss: 0.428887  [  912/ 3200]\n",
      "loss: 0.111119  [  928/ 3200]\n",
      "loss: 0.314599  [  944/ 3200]\n",
      "loss: 0.099157  [  960/ 3200]\n",
      "loss: 0.041126  [  976/ 3200]\n",
      "loss: 0.222134  [  992/ 3200]\n",
      "loss: 0.079599  [ 1008/ 3200]\n",
      "loss: 0.271039  [ 1024/ 3200]\n",
      "loss: 0.200334  [ 1040/ 3200]\n",
      "loss: 0.302331  [ 1056/ 3200]\n",
      "loss: 0.072025  [ 1072/ 3200]\n",
      "loss: 0.165468  [ 1088/ 3200]\n",
      "loss: 0.061364  [ 1104/ 3200]\n",
      "loss: 0.139820  [ 1120/ 3200]\n",
      "loss: 0.070483  [ 1136/ 3200]\n",
      "loss: 0.211731  [ 1152/ 3200]\n",
      "loss: 0.291968  [ 1168/ 3200]\n",
      "loss: 0.337376  [ 1184/ 3200]\n",
      "loss: 0.246335  [ 1200/ 3200]\n",
      "loss: 0.092007  [ 1216/ 3200]\n",
      "loss: 0.237089  [ 1232/ 3200]\n",
      "loss: 0.499706  [ 1248/ 3200]\n",
      "loss: 0.116076  [ 1264/ 3200]\n",
      "loss: 0.106141  [ 1280/ 3200]\n",
      "loss: 0.219846  [ 1296/ 3200]\n",
      "loss: 0.077232  [ 1312/ 3200]\n",
      "loss: 0.228643  [ 1328/ 3200]\n",
      "loss: 0.168196  [ 1344/ 3200]\n",
      "loss: 0.102309  [ 1360/ 3200]\n",
      "loss: 0.047944  [ 1376/ 3200]\n",
      "loss: 0.059793  [ 1392/ 3200]\n",
      "loss: 0.127109  [ 1408/ 3200]\n",
      "loss: 0.127950  [ 1424/ 3200]\n",
      "loss: 0.216124  [ 1440/ 3200]\n",
      "loss: 0.038463  [ 1456/ 3200]\n",
      "loss: 0.209238  [ 1472/ 3200]\n",
      "loss: 0.200781  [ 1488/ 3200]\n",
      "loss: 0.136108  [ 1504/ 3200]\n",
      "loss: 0.217921  [ 1520/ 3200]\n",
      "loss: 0.127600  [ 1536/ 3200]\n",
      "loss: 0.310981  [ 1552/ 3200]\n",
      "loss: 0.247731  [ 1568/ 3200]\n",
      "loss: 0.143510  [ 1584/ 3200]\n",
      "loss: 0.123025  [ 1600/ 3200]\n",
      "loss: 0.118732  [ 1616/ 3200]\n",
      "loss: 0.407448  [ 1632/ 3200]\n",
      "loss: 0.132480  [ 1648/ 3200]\n",
      "loss: 0.228206  [ 1664/ 3200]\n",
      "loss: 0.271117  [ 1680/ 3200]\n",
      "loss: 0.143672  [ 1696/ 3200]\n",
      "loss: 0.194838  [ 1712/ 3200]\n",
      "loss: 0.252444  [ 1728/ 3200]\n",
      "loss: 0.056914  [ 1744/ 3200]\n",
      "loss: 0.282191  [ 1760/ 3200]\n",
      "loss: 0.380883  [ 1776/ 3200]\n",
      "loss: 0.111092  [ 1792/ 3200]\n",
      "loss: 0.306814  [ 1808/ 3200]\n",
      "loss: 0.053958  [ 1824/ 3200]\n",
      "loss: 0.124523  [ 1840/ 3200]\n",
      "loss: 0.122763  [ 1856/ 3200]\n",
      "loss: 0.093466  [ 1872/ 3200]\n",
      "loss: 0.059426  [ 1888/ 3200]\n",
      "loss: 0.156046  [ 1904/ 3200]\n",
      "loss: 0.102375  [ 1920/ 3200]\n",
      "loss: 0.107683  [ 1936/ 3200]\n",
      "loss: 0.257574  [ 1952/ 3200]\n",
      "loss: 0.124357  [ 1968/ 3200]\n",
      "loss: 0.012907  [ 1984/ 3200]\n",
      "loss: 0.055748  [ 2000/ 3200]\n",
      "loss: 0.030427  [ 2016/ 3200]\n",
      "loss: 0.340785  [ 2032/ 3200]\n",
      "loss: 0.254630  [ 2048/ 3200]\n",
      "loss: 0.136412  [ 2064/ 3200]\n",
      "loss: 0.253088  [ 2080/ 3200]\n",
      "loss: 0.379028  [ 2096/ 3200]\n",
      "loss: 0.253457  [ 2112/ 3200]\n",
      "loss: 0.176346  [ 2128/ 3200]\n",
      "loss: 0.146494  [ 2144/ 3200]\n",
      "loss: 0.103354  [ 2160/ 3200]\n",
      "loss: 0.111962  [ 2176/ 3200]\n",
      "loss: 0.321006  [ 2192/ 3200]\n",
      "loss: 0.116640  [ 2208/ 3200]\n",
      "loss: 0.258892  [ 2224/ 3200]\n",
      "loss: 0.173658  [ 2240/ 3200]\n",
      "loss: 0.160320  [ 2256/ 3200]\n",
      "loss: 0.345128  [ 2272/ 3200]\n",
      "loss: 0.040026  [ 2288/ 3200]\n",
      "loss: 0.055575  [ 2304/ 3200]\n",
      "loss: 0.059647  [ 2320/ 3200]\n",
      "loss: 0.210436  [ 2336/ 3200]\n",
      "loss: 0.100347  [ 2352/ 3200]\n",
      "loss: 0.158247  [ 2368/ 3200]\n",
      "loss: 0.207755  [ 2384/ 3200]\n",
      "loss: 0.258439  [ 2400/ 3200]\n",
      "loss: 0.304794  [ 2416/ 3200]\n",
      "loss: 0.083613  [ 2432/ 3200]\n",
      "loss: 0.430213  [ 2448/ 3200]\n",
      "loss: 0.210756  [ 2464/ 3200]\n",
      "loss: 0.137877  [ 2480/ 3200]\n",
      "loss: 0.069612  [ 2496/ 3200]\n",
      "loss: 0.173936  [ 2512/ 3200]\n",
      "loss: 0.068839  [ 2528/ 3200]\n",
      "loss: 0.100954  [ 2544/ 3200]\n",
      "loss: 0.063355  [ 2560/ 3200]\n",
      "loss: 0.228223  [ 2576/ 3200]\n",
      "loss: 0.142072  [ 2592/ 3200]\n",
      "loss: 0.124363  [ 2608/ 3200]\n",
      "loss: 0.105405  [ 2624/ 3200]\n",
      "loss: 0.116129  [ 2640/ 3200]\n",
      "loss: 0.279191  [ 2656/ 3200]\n",
      "loss: 0.135109  [ 2672/ 3200]\n",
      "loss: 0.174125  [ 2688/ 3200]\n",
      "loss: 0.239629  [ 2704/ 3200]\n",
      "loss: 0.199149  [ 2720/ 3200]\n",
      "loss: 0.271992  [ 2736/ 3200]\n",
      "loss: 0.406703  [ 2752/ 3200]\n",
      "loss: 0.271067  [ 2768/ 3200]\n",
      "loss: 0.163931  [ 2784/ 3200]\n",
      "loss: 0.246104  [ 2800/ 3200]\n",
      "loss: 0.091833  [ 2816/ 3200]\n",
      "loss: 0.309801  [ 2832/ 3200]\n",
      "loss: 0.218244  [ 2848/ 3200]\n",
      "loss: 0.218967  [ 2864/ 3200]\n",
      "loss: 0.165103  [ 2880/ 3200]\n",
      "loss: 0.101778  [ 2896/ 3200]\n",
      "loss: 0.169588  [ 2912/ 3200]\n",
      "loss: 0.088347  [ 2928/ 3200]\n",
      "loss: 0.040751  [ 2944/ 3200]\n",
      "loss: 0.093626  [ 2960/ 3200]\n",
      "loss: 0.040927  [ 2976/ 3200]\n",
      "loss: 0.035394  [ 2992/ 3200]\n",
      "loss: 0.088275  [ 3008/ 3200]\n",
      "loss: 0.096010  [ 3024/ 3200]\n",
      "loss: 0.615160  [ 3040/ 3200]\n",
      "loss: 0.235629  [ 3056/ 3200]\n",
      "loss: 0.238499  [ 3072/ 3200]\n",
      "loss: 0.066415  [ 3088/ 3200]\n",
      "loss: 0.340628  [ 3104/ 3200]\n",
      "loss: 0.151339  [ 3120/ 3200]\n",
      "loss: 0.022666  [ 3136/ 3200]\n",
      "loss: 0.264062  [ 3152/ 3200]\n",
      "loss: 0.105153  [ 3168/ 3200]\n",
      "loss: 0.123755  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038754\n",
      "f1 macro averaged score: 0.781265\n",
      "Accuracy               : 78.5%\n",
      "Confusion matrix       :\n",
      "tensor([[183,   4,   0,  13],\n",
      "        [ 19, 118,  25,  38],\n",
      "        [  0,  22, 166,  12],\n",
      "        [  9,  15,  15, 161]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.7535e-04.\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.146697  [    0/ 3200]\n",
      "loss: 0.305373  [   16/ 3200]\n",
      "loss: 0.155897  [   32/ 3200]\n",
      "loss: 0.108558  [   48/ 3200]\n",
      "loss: 0.127197  [   64/ 3200]\n",
      "loss: 0.234192  [   80/ 3200]\n",
      "loss: 0.052525  [   96/ 3200]\n",
      "loss: 0.434243  [  112/ 3200]\n",
      "loss: 0.106837  [  128/ 3200]\n",
      "loss: 0.042416  [  144/ 3200]\n",
      "loss: 0.287218  [  160/ 3200]\n",
      "loss: 0.134806  [  176/ 3200]\n",
      "loss: 0.121888  [  192/ 3200]\n",
      "loss: 0.111351  [  208/ 3200]\n",
      "loss: 0.102970  [  224/ 3200]\n",
      "loss: 0.160420  [  240/ 3200]\n",
      "loss: 0.105460  [  256/ 3200]\n",
      "loss: 0.085008  [  272/ 3200]\n",
      "loss: 0.101105  [  288/ 3200]\n",
      "loss: 0.040307  [  304/ 3200]\n",
      "loss: 0.166968  [  320/ 3200]\n",
      "loss: 0.152148  [  336/ 3200]\n",
      "loss: 0.048473  [  352/ 3200]\n",
      "loss: 0.078166  [  368/ 3200]\n",
      "loss: 0.205949  [  384/ 3200]\n",
      "loss: 0.144615  [  400/ 3200]\n",
      "loss: 0.066101  [  416/ 3200]\n",
      "loss: 0.076285  [  432/ 3200]\n",
      "loss: 0.248838  [  448/ 3200]\n",
      "loss: 0.087615  [  464/ 3200]\n",
      "loss: 0.301273  [  480/ 3200]\n",
      "loss: 0.251729  [  496/ 3200]\n",
      "loss: 0.241391  [  512/ 3200]\n",
      "loss: 0.117883  [  528/ 3200]\n",
      "loss: 0.169841  [  544/ 3200]\n",
      "loss: 0.237111  [  560/ 3200]\n",
      "loss: 0.233116  [  576/ 3200]\n",
      "loss: 0.179485  [  592/ 3200]\n",
      "loss: 0.338416  [  608/ 3200]\n",
      "loss: 0.163217  [  624/ 3200]\n",
      "loss: 0.104603  [  640/ 3200]\n",
      "loss: 0.059652  [  656/ 3200]\n",
      "loss: 0.101049  [  672/ 3200]\n",
      "loss: 0.141766  [  688/ 3200]\n",
      "loss: 0.308217  [  704/ 3200]\n",
      "loss: 0.134238  [  720/ 3200]\n",
      "loss: 0.097030  [  736/ 3200]\n",
      "loss: 0.070433  [  752/ 3200]\n",
      "loss: 0.112189  [  768/ 3200]\n",
      "loss: 0.098510  [  784/ 3200]\n",
      "loss: 0.193078  [  800/ 3200]\n",
      "loss: 0.198016  [  816/ 3200]\n",
      "loss: 0.133501  [  832/ 3200]\n",
      "loss: 0.139653  [  848/ 3200]\n",
      "loss: 0.063511  [  864/ 3200]\n",
      "loss: 0.139377  [  880/ 3200]\n",
      "loss: 0.116809  [  896/ 3200]\n",
      "loss: 0.104531  [  912/ 3200]\n",
      "loss: 0.196915  [  928/ 3200]\n",
      "loss: 0.032347  [  944/ 3200]\n",
      "loss: 0.209192  [  960/ 3200]\n",
      "loss: 0.066628  [  976/ 3200]\n",
      "loss: 0.054608  [  992/ 3200]\n",
      "loss: 0.144737  [ 1008/ 3200]\n",
      "loss: 0.130759  [ 1024/ 3200]\n",
      "loss: 0.063686  [ 1040/ 3200]\n",
      "loss: 0.101783  [ 1056/ 3200]\n",
      "loss: 0.104433  [ 1072/ 3200]\n",
      "loss: 0.095647  [ 1088/ 3200]\n",
      "loss: 0.140907  [ 1104/ 3200]\n",
      "loss: 0.095595  [ 1120/ 3200]\n",
      "loss: 0.087355  [ 1136/ 3200]\n",
      "loss: 0.057158  [ 1152/ 3200]\n",
      "loss: 0.134412  [ 1168/ 3200]\n",
      "loss: 0.105882  [ 1184/ 3200]\n",
      "loss: 0.078253  [ 1200/ 3200]\n",
      "loss: 0.069092  [ 1216/ 3200]\n",
      "loss: 0.070379  [ 1232/ 3200]\n",
      "loss: 0.073743  [ 1248/ 3200]\n",
      "loss: 0.155821  [ 1264/ 3200]\n",
      "loss: 0.070255  [ 1280/ 3200]\n",
      "loss: 0.052527  [ 1296/ 3200]\n",
      "loss: 0.117601  [ 1312/ 3200]\n",
      "loss: 0.152595  [ 1328/ 3200]\n",
      "loss: 0.283817  [ 1344/ 3200]\n",
      "loss: 0.133458  [ 1360/ 3200]\n",
      "loss: 0.055082  [ 1376/ 3200]\n",
      "loss: 0.085291  [ 1392/ 3200]\n",
      "loss: 0.200862  [ 1408/ 3200]\n",
      "loss: 0.051020  [ 1424/ 3200]\n",
      "loss: 0.197826  [ 1440/ 3200]\n",
      "loss: 0.051841  [ 1456/ 3200]\n",
      "loss: 0.115767  [ 1472/ 3200]\n",
      "loss: 0.247768  [ 1488/ 3200]\n",
      "loss: 0.259121  [ 1504/ 3200]\n",
      "loss: 0.038131  [ 1520/ 3200]\n",
      "loss: 0.198540  [ 1536/ 3200]\n",
      "loss: 0.037988  [ 1552/ 3200]\n",
      "loss: 0.183418  [ 1568/ 3200]\n",
      "loss: 0.097201  [ 1584/ 3200]\n",
      "loss: 0.082796  [ 1600/ 3200]\n",
      "loss: 0.107328  [ 1616/ 3200]\n",
      "loss: 0.062034  [ 1632/ 3200]\n",
      "loss: 0.120609  [ 1648/ 3200]\n",
      "loss: 0.462991  [ 1664/ 3200]\n",
      "loss: 0.274154  [ 1680/ 3200]\n",
      "loss: 0.144361  [ 1696/ 3200]\n",
      "loss: 0.132181  [ 1712/ 3200]\n",
      "loss: 0.095735  [ 1728/ 3200]\n",
      "loss: 0.412950  [ 1744/ 3200]\n",
      "loss: 0.202519  [ 1760/ 3200]\n",
      "loss: 0.105912  [ 1776/ 3200]\n",
      "loss: 0.235479  [ 1792/ 3200]\n",
      "loss: 0.221973  [ 1808/ 3200]\n",
      "loss: 0.275071  [ 1824/ 3200]\n",
      "loss: 0.055435  [ 1840/ 3200]\n",
      "loss: 0.177929  [ 1856/ 3200]\n",
      "loss: 0.103645  [ 1872/ 3200]\n",
      "loss: 0.070376  [ 1888/ 3200]\n",
      "loss: 0.794884  [ 1904/ 3200]\n",
      "loss: 0.110820  [ 1920/ 3200]\n",
      "loss: 0.234030  [ 1936/ 3200]\n",
      "loss: 0.144420  [ 1952/ 3200]\n",
      "loss: 0.081593  [ 1968/ 3200]\n",
      "loss: 0.132540  [ 1984/ 3200]\n",
      "loss: 0.125267  [ 2000/ 3200]\n",
      "loss: 0.089192  [ 2016/ 3200]\n",
      "loss: 0.057393  [ 2032/ 3200]\n",
      "loss: 0.121289  [ 2048/ 3200]\n",
      "loss: 0.078232  [ 2064/ 3200]\n",
      "loss: 0.157231  [ 2080/ 3200]\n",
      "loss: 0.141119  [ 2096/ 3200]\n",
      "loss: 0.125025  [ 2112/ 3200]\n",
      "loss: 0.151135  [ 2128/ 3200]\n",
      "loss: 0.184240  [ 2144/ 3200]\n",
      "loss: 0.146059  [ 2160/ 3200]\n",
      "loss: 0.157293  [ 2176/ 3200]\n",
      "loss: 0.077075  [ 2192/ 3200]\n",
      "loss: 0.135371  [ 2208/ 3200]\n",
      "loss: 0.108483  [ 2224/ 3200]\n",
      "loss: 0.257255  [ 2240/ 3200]\n",
      "loss: 0.108971  [ 2256/ 3200]\n",
      "loss: 0.226996  [ 2272/ 3200]\n",
      "loss: 0.064605  [ 2288/ 3200]\n",
      "loss: 0.218886  [ 2304/ 3200]\n",
      "loss: 0.077880  [ 2320/ 3200]\n",
      "loss: 0.278668  [ 2336/ 3200]\n",
      "loss: 0.227737  [ 2352/ 3200]\n",
      "loss: 0.068633  [ 2368/ 3200]\n",
      "loss: 0.091274  [ 2384/ 3200]\n",
      "loss: 0.039022  [ 2400/ 3200]\n",
      "loss: 0.192056  [ 2416/ 3200]\n",
      "loss: 0.092085  [ 2432/ 3200]\n",
      "loss: 0.081203  [ 2448/ 3200]\n",
      "loss: 0.153068  [ 2464/ 3200]\n",
      "loss: 0.510063  [ 2480/ 3200]\n",
      "loss: 0.040655  [ 2496/ 3200]\n",
      "loss: 0.044087  [ 2512/ 3200]\n",
      "loss: 0.071100  [ 2528/ 3200]\n",
      "loss: 0.159819  [ 2544/ 3200]\n",
      "loss: 0.088720  [ 2560/ 3200]\n",
      "loss: 0.101037  [ 2576/ 3200]\n",
      "loss: 0.113166  [ 2592/ 3200]\n",
      "loss: 0.087564  [ 2608/ 3200]\n",
      "loss: 0.074196  [ 2624/ 3200]\n",
      "loss: 0.226563  [ 2640/ 3200]\n",
      "loss: 0.124403  [ 2656/ 3200]\n",
      "loss: 0.268200  [ 2672/ 3200]\n",
      "loss: 0.174737  [ 2688/ 3200]\n",
      "loss: 0.091300  [ 2704/ 3200]\n",
      "loss: 0.072814  [ 2720/ 3200]\n",
      "loss: 0.128486  [ 2736/ 3200]\n",
      "loss: 0.090552  [ 2752/ 3200]\n",
      "loss: 0.336258  [ 2768/ 3200]\n",
      "loss: 0.132819  [ 2784/ 3200]\n",
      "loss: 0.114043  [ 2800/ 3200]\n",
      "loss: 0.147015  [ 2816/ 3200]\n",
      "loss: 0.060598  [ 2832/ 3200]\n",
      "loss: 0.389053  [ 2848/ 3200]\n",
      "loss: 0.140913  [ 2864/ 3200]\n",
      "loss: 0.076329  [ 2880/ 3200]\n",
      "loss: 0.085177  [ 2896/ 3200]\n",
      "loss: 0.039837  [ 2912/ 3200]\n",
      "loss: 0.133883  [ 2928/ 3200]\n",
      "loss: 0.067717  [ 2944/ 3200]\n",
      "loss: 0.060193  [ 2960/ 3200]\n",
      "loss: 0.286073  [ 2976/ 3200]\n",
      "loss: 0.131082  [ 2992/ 3200]\n",
      "loss: 0.398999  [ 3008/ 3200]\n",
      "loss: 0.156119  [ 3024/ 3200]\n",
      "loss: 0.050920  [ 3040/ 3200]\n",
      "loss: 0.181286  [ 3056/ 3200]\n",
      "loss: 0.216588  [ 3072/ 3200]\n",
      "loss: 0.208995  [ 3088/ 3200]\n",
      "loss: 0.503175  [ 3104/ 3200]\n",
      "loss: 0.033092  [ 3120/ 3200]\n",
      "loss: 0.116441  [ 3136/ 3200]\n",
      "loss: 0.207919  [ 3152/ 3200]\n",
      "loss: 0.075036  [ 3168/ 3200]\n",
      "loss: 0.218975  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.043584\n",
      "f1 macro averaged score: 0.769992\n",
      "Accuracy               : 76.9%\n",
      "Confusion matrix       :\n",
      "tensor([[177,   8,   0,  15],\n",
      "        [ 15, 132,   8,  45],\n",
      "        [  0,  34, 139,  27],\n",
      "        [  7,  19,   7, 167]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.2658e-04.\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 0.116545  [    0/ 3200]\n",
      "loss: 0.091999  [   16/ 3200]\n",
      "loss: 0.080685  [   32/ 3200]\n",
      "loss: 0.148844  [   48/ 3200]\n",
      "loss: 0.047266  [   64/ 3200]\n",
      "loss: 0.383749  [   80/ 3200]\n",
      "loss: 0.323621  [   96/ 3200]\n",
      "loss: 0.120378  [  112/ 3200]\n",
      "loss: 0.113870  [  128/ 3200]\n",
      "loss: 0.059509  [  144/ 3200]\n",
      "loss: 0.101941  [  160/ 3200]\n",
      "loss: 0.116359  [  176/ 3200]\n",
      "loss: 0.067138  [  192/ 3200]\n",
      "loss: 0.092840  [  208/ 3200]\n",
      "loss: 0.123110  [  224/ 3200]\n",
      "loss: 0.076388  [  240/ 3200]\n",
      "loss: 0.114181  [  256/ 3200]\n",
      "loss: 0.126230  [  272/ 3200]\n",
      "loss: 0.086852  [  288/ 3200]\n",
      "loss: 0.182232  [  304/ 3200]\n",
      "loss: 0.075341  [  320/ 3200]\n",
      "loss: 0.293928  [  336/ 3200]\n",
      "loss: 0.256706  [  352/ 3200]\n",
      "loss: 0.154918  [  368/ 3200]\n",
      "loss: 0.057569  [  384/ 3200]\n",
      "loss: 0.174310  [  400/ 3200]\n",
      "loss: 0.209658  [  416/ 3200]\n",
      "loss: 0.065558  [  432/ 3200]\n",
      "loss: 0.096148  [  448/ 3200]\n",
      "loss: 0.343406  [  464/ 3200]\n",
      "loss: 0.349117  [  480/ 3200]\n",
      "loss: 0.091399  [  496/ 3200]\n",
      "loss: 0.158940  [  512/ 3200]\n",
      "loss: 0.090329  [  528/ 3200]\n",
      "loss: 0.102689  [  544/ 3200]\n",
      "loss: 0.155549  [  560/ 3200]\n",
      "loss: 0.079591  [  576/ 3200]\n",
      "loss: 0.199331  [  592/ 3200]\n",
      "loss: 0.048040  [  608/ 3200]\n",
      "loss: 0.333435  [  624/ 3200]\n",
      "loss: 0.012604  [  640/ 3200]\n",
      "loss: 0.216491  [  656/ 3200]\n",
      "loss: 0.081615  [  672/ 3200]\n",
      "loss: 0.086248  [  688/ 3200]\n",
      "loss: 0.025925  [  704/ 3200]\n",
      "loss: 0.105253  [  720/ 3200]\n",
      "loss: 0.040686  [  736/ 3200]\n",
      "loss: 0.161103  [  752/ 3200]\n",
      "loss: 0.064665  [  768/ 3200]\n",
      "loss: 0.104415  [  784/ 3200]\n",
      "loss: 0.074741  [  800/ 3200]\n",
      "loss: 0.176348  [  816/ 3200]\n",
      "loss: 0.056443  [  832/ 3200]\n",
      "loss: 0.211718  [  848/ 3200]\n",
      "loss: 0.055287  [  864/ 3200]\n",
      "loss: 0.049590  [  880/ 3200]\n",
      "loss: 0.169650  [  896/ 3200]\n",
      "loss: 0.200788  [  912/ 3200]\n",
      "loss: 0.193363  [  928/ 3200]\n",
      "loss: 0.168211  [  944/ 3200]\n",
      "loss: 0.102976  [  960/ 3200]\n",
      "loss: 0.050404  [  976/ 3200]\n",
      "loss: 0.052330  [  992/ 3200]\n",
      "loss: 0.054711  [ 1008/ 3200]\n",
      "loss: 0.074827  [ 1024/ 3200]\n",
      "loss: 0.027302  [ 1040/ 3200]\n",
      "loss: 0.193776  [ 1056/ 3200]\n",
      "loss: 0.194207  [ 1072/ 3200]\n",
      "loss: 0.101465  [ 1088/ 3200]\n",
      "loss: 0.105258  [ 1104/ 3200]\n",
      "loss: 0.248929  [ 1120/ 3200]\n",
      "loss: 0.210484  [ 1136/ 3200]\n",
      "loss: 0.299516  [ 1152/ 3200]\n",
      "loss: 0.202907  [ 1168/ 3200]\n",
      "loss: 0.147622  [ 1184/ 3200]\n",
      "loss: 0.236380  [ 1200/ 3200]\n",
      "loss: 0.196826  [ 1216/ 3200]\n",
      "loss: 0.127149  [ 1232/ 3200]\n",
      "loss: 0.102197  [ 1248/ 3200]\n",
      "loss: 0.040350  [ 1264/ 3200]\n",
      "loss: 0.097938  [ 1280/ 3200]\n",
      "loss: 0.063564  [ 1296/ 3200]\n",
      "loss: 0.119928  [ 1312/ 3200]\n",
      "loss: 0.198664  [ 1328/ 3200]\n",
      "loss: 0.091925  [ 1344/ 3200]\n",
      "loss: 0.073873  [ 1360/ 3200]\n",
      "loss: 0.054184  [ 1376/ 3200]\n",
      "loss: 0.167334  [ 1392/ 3200]\n",
      "loss: 0.052724  [ 1408/ 3200]\n",
      "loss: 0.134310  [ 1424/ 3200]\n",
      "loss: 0.227893  [ 1440/ 3200]\n",
      "loss: 0.385981  [ 1456/ 3200]\n",
      "loss: 0.073688  [ 1472/ 3200]\n",
      "loss: 0.507266  [ 1488/ 3200]\n",
      "loss: 0.102154  [ 1504/ 3200]\n",
      "loss: 0.150518  [ 1520/ 3200]\n",
      "loss: 0.083304  [ 1536/ 3200]\n",
      "loss: 0.096471  [ 1552/ 3200]\n",
      "loss: 0.196072  [ 1568/ 3200]\n",
      "loss: 0.053400  [ 1584/ 3200]\n",
      "loss: 0.073023  [ 1600/ 3200]\n",
      "loss: 0.163630  [ 1616/ 3200]\n",
      "loss: 0.159924  [ 1632/ 3200]\n",
      "loss: 0.031335  [ 1648/ 3200]\n",
      "loss: 0.066010  [ 1664/ 3200]\n",
      "loss: 0.054167  [ 1680/ 3200]\n",
      "loss: 0.055217  [ 1696/ 3200]\n",
      "loss: 0.197990  [ 1712/ 3200]\n",
      "loss: 0.114549  [ 1728/ 3200]\n",
      "loss: 0.405723  [ 1744/ 3200]\n",
      "loss: 0.084840  [ 1760/ 3200]\n",
      "loss: 0.088414  [ 1776/ 3200]\n",
      "loss: 0.063380  [ 1792/ 3200]\n",
      "loss: 0.097879  [ 1808/ 3200]\n",
      "loss: 0.121977  [ 1824/ 3200]\n",
      "loss: 0.107545  [ 1840/ 3200]\n",
      "loss: 0.166043  [ 1856/ 3200]\n",
      "loss: 0.083849  [ 1872/ 3200]\n",
      "loss: 0.039782  [ 1888/ 3200]\n",
      "loss: 0.062271  [ 1904/ 3200]\n",
      "loss: 0.053282  [ 1920/ 3200]\n",
      "loss: 0.113714  [ 1936/ 3200]\n",
      "loss: 0.094949  [ 1952/ 3200]\n",
      "loss: 0.177746  [ 1968/ 3200]\n",
      "loss: 0.130462  [ 1984/ 3200]\n",
      "loss: 0.072999  [ 2000/ 3200]\n",
      "loss: 0.055539  [ 2016/ 3200]\n",
      "loss: 0.162098  [ 2032/ 3200]\n",
      "loss: 0.106599  [ 2048/ 3200]\n",
      "loss: 0.181371  [ 2064/ 3200]\n",
      "loss: 0.069760  [ 2080/ 3200]\n",
      "loss: 0.155510  [ 2096/ 3200]\n",
      "loss: 0.052952  [ 2112/ 3200]\n",
      "loss: 0.075210  [ 2128/ 3200]\n",
      "loss: 0.039211  [ 2144/ 3200]\n",
      "loss: 0.056118  [ 2160/ 3200]\n",
      "loss: 0.079218  [ 2176/ 3200]\n",
      "loss: 0.119380  [ 2192/ 3200]\n",
      "loss: 0.078541  [ 2208/ 3200]\n",
      "loss: 0.113845  [ 2224/ 3200]\n",
      "loss: 0.093921  [ 2240/ 3200]\n",
      "loss: 0.039711  [ 2256/ 3200]\n",
      "loss: 0.167627  [ 2272/ 3200]\n",
      "loss: 0.106157  [ 2288/ 3200]\n",
      "loss: 0.061281  [ 2304/ 3200]\n",
      "loss: 0.093805  [ 2320/ 3200]\n",
      "loss: 0.083317  [ 2336/ 3200]\n",
      "loss: 0.109045  [ 2352/ 3200]\n",
      "loss: 0.165979  [ 2368/ 3200]\n",
      "loss: 0.135695  [ 2384/ 3200]\n",
      "loss: 0.092392  [ 2400/ 3200]\n",
      "loss: 0.105968  [ 2416/ 3200]\n",
      "loss: 0.174604  [ 2432/ 3200]\n",
      "loss: 0.147926  [ 2448/ 3200]\n",
      "loss: 0.070134  [ 2464/ 3200]\n",
      "loss: 0.063977  [ 2480/ 3200]\n",
      "loss: 0.033035  [ 2496/ 3200]\n",
      "loss: 0.487932  [ 2512/ 3200]\n",
      "loss: 0.089687  [ 2528/ 3200]\n",
      "loss: 0.068113  [ 2544/ 3200]\n",
      "loss: 0.171814  [ 2560/ 3200]\n",
      "loss: 0.224502  [ 2576/ 3200]\n",
      "loss: 0.052839  [ 2592/ 3200]\n",
      "loss: 0.020122  [ 2608/ 3200]\n",
      "loss: 0.021726  [ 2624/ 3200]\n",
      "loss: 0.102551  [ 2640/ 3200]\n",
      "loss: 0.238419  [ 2656/ 3200]\n",
      "loss: 0.238056  [ 2672/ 3200]\n",
      "loss: 0.299994  [ 2688/ 3200]\n",
      "loss: 0.262223  [ 2704/ 3200]\n",
      "loss: 0.098567  [ 2720/ 3200]\n",
      "loss: 0.117695  [ 2736/ 3200]\n",
      "loss: 0.115595  [ 2752/ 3200]\n",
      "loss: 0.049888  [ 2768/ 3200]\n",
      "loss: 0.082335  [ 2784/ 3200]\n",
      "loss: 0.146630  [ 2800/ 3200]\n",
      "loss: 0.041648  [ 2816/ 3200]\n",
      "loss: 0.082988  [ 2832/ 3200]\n",
      "loss: 0.274183  [ 2848/ 3200]\n",
      "loss: 0.073584  [ 2864/ 3200]\n",
      "loss: 0.076126  [ 2880/ 3200]\n",
      "loss: 0.069862  [ 2896/ 3200]\n",
      "loss: 0.163102  [ 2912/ 3200]\n",
      "loss: 0.119118  [ 2928/ 3200]\n",
      "loss: 0.047739  [ 2944/ 3200]\n",
      "loss: 0.215730  [ 2960/ 3200]\n",
      "loss: 0.102548  [ 2976/ 3200]\n",
      "loss: 0.120252  [ 2992/ 3200]\n",
      "loss: 0.034568  [ 3008/ 3200]\n",
      "loss: 0.034158  [ 3024/ 3200]\n",
      "loss: 0.075537  [ 3040/ 3200]\n",
      "loss: 0.035850  [ 3056/ 3200]\n",
      "loss: 0.122183  [ 3072/ 3200]\n",
      "loss: 0.129908  [ 3088/ 3200]\n",
      "loss: 0.257425  [ 3104/ 3200]\n",
      "loss: 0.363550  [ 3120/ 3200]\n",
      "loss: 0.138348  [ 3136/ 3200]\n",
      "loss: 0.090471  [ 3152/ 3200]\n",
      "loss: 0.083479  [ 3168/ 3200]\n",
      "loss: 0.129243  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.040387\n",
      "f1 macro averaged score: 0.790452\n",
      "Accuracy               : 79.0%\n",
      "Confusion matrix       :\n",
      "tensor([[180,   8,   0,  12],\n",
      "        [ 15, 145,  15,  25],\n",
      "        [  0,  33, 152,  15],\n",
      "        [  9,  23,  13, 155]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 8.8025e-04.\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 0.089816  [    0/ 3200]\n",
      "loss: 0.113928  [   16/ 3200]\n",
      "loss: 0.035574  [   32/ 3200]\n",
      "loss: 0.036629  [   48/ 3200]\n",
      "loss: 0.345382  [   64/ 3200]\n",
      "loss: 0.227675  [   80/ 3200]\n",
      "loss: 0.217272  [   96/ 3200]\n",
      "loss: 0.171135  [  112/ 3200]\n",
      "loss: 0.097035  [  128/ 3200]\n",
      "loss: 0.042046  [  144/ 3200]\n",
      "loss: 0.106469  [  160/ 3200]\n",
      "loss: 0.331116  [  176/ 3200]\n",
      "loss: 0.053986  [  192/ 3200]\n",
      "loss: 0.218806  [  208/ 3200]\n",
      "loss: 0.083993  [  224/ 3200]\n",
      "loss: 0.099241  [  240/ 3200]\n",
      "loss: 0.062622  [  256/ 3200]\n",
      "loss: 0.056483  [  272/ 3200]\n",
      "loss: 0.147639  [  288/ 3200]\n",
      "loss: 0.113385  [  304/ 3200]\n",
      "loss: 0.143540  [  320/ 3200]\n",
      "loss: 0.019271  [  336/ 3200]\n",
      "loss: 0.046170  [  352/ 3200]\n",
      "loss: 0.243756  [  368/ 3200]\n",
      "loss: 0.187033  [  384/ 3200]\n",
      "loss: 0.057256  [  400/ 3200]\n",
      "loss: 0.061574  [  416/ 3200]\n",
      "loss: 0.082017  [  432/ 3200]\n",
      "loss: 0.224575  [  448/ 3200]\n",
      "loss: 0.106858  [  464/ 3200]\n",
      "loss: 0.069605  [  480/ 3200]\n",
      "loss: 0.051156  [  496/ 3200]\n",
      "loss: 0.082449  [  512/ 3200]\n",
      "loss: 0.099367  [  528/ 3200]\n",
      "loss: 0.024274  [  544/ 3200]\n",
      "loss: 0.074703  [  560/ 3200]\n",
      "loss: 0.089017  [  576/ 3200]\n",
      "loss: 0.103896  [  592/ 3200]\n",
      "loss: 0.259378  [  608/ 3200]\n",
      "loss: 0.017504  [  624/ 3200]\n",
      "loss: 0.028210  [  640/ 3200]\n",
      "loss: 0.131102  [  656/ 3200]\n",
      "loss: 0.294929  [  672/ 3200]\n",
      "loss: 0.086087  [  688/ 3200]\n",
      "loss: 0.051507  [  704/ 3200]\n",
      "loss: 0.136888  [  720/ 3200]\n",
      "loss: 0.225296  [  736/ 3200]\n",
      "loss: 0.081821  [  752/ 3200]\n",
      "loss: 0.052934  [  768/ 3200]\n",
      "loss: 0.063919  [  784/ 3200]\n",
      "loss: 0.056535  [  800/ 3200]\n",
      "loss: 0.272519  [  816/ 3200]\n",
      "loss: 0.040931  [  832/ 3200]\n",
      "loss: 0.147498  [  848/ 3200]\n",
      "loss: 0.129826  [  864/ 3200]\n",
      "loss: 0.034923  [  880/ 3200]\n",
      "loss: 0.102310  [  896/ 3200]\n",
      "loss: 0.081750  [  912/ 3200]\n",
      "loss: 0.079029  [  928/ 3200]\n",
      "loss: 0.078017  [  944/ 3200]\n",
      "loss: 0.089443  [  960/ 3200]\n",
      "loss: 0.072500  [  976/ 3200]\n",
      "loss: 0.046830  [  992/ 3200]\n",
      "loss: 0.054386  [ 1008/ 3200]\n",
      "loss: 0.019996  [ 1024/ 3200]\n",
      "loss: 0.074526  [ 1040/ 3200]\n",
      "loss: 0.042040  [ 1056/ 3200]\n",
      "loss: 0.035294  [ 1072/ 3200]\n",
      "loss: 0.161650  [ 1088/ 3200]\n",
      "loss: 0.088937  [ 1104/ 3200]\n",
      "loss: 0.017932  [ 1120/ 3200]\n",
      "loss: 0.279162  [ 1136/ 3200]\n",
      "loss: 0.087347  [ 1152/ 3200]\n",
      "loss: 0.098690  [ 1168/ 3200]\n",
      "loss: 0.067785  [ 1184/ 3200]\n",
      "loss: 0.068811  [ 1200/ 3200]\n",
      "loss: 0.068702  [ 1216/ 3200]\n",
      "loss: 0.031876  [ 1232/ 3200]\n",
      "loss: 0.250947  [ 1248/ 3200]\n",
      "loss: 0.075090  [ 1264/ 3200]\n",
      "loss: 0.072281  [ 1280/ 3200]\n",
      "loss: 0.056974  [ 1296/ 3200]\n",
      "loss: 0.205672  [ 1312/ 3200]\n",
      "loss: 0.109392  [ 1328/ 3200]\n",
      "loss: 0.141362  [ 1344/ 3200]\n",
      "loss: 0.061269  [ 1360/ 3200]\n",
      "loss: 0.105408  [ 1376/ 3200]\n",
      "loss: 0.164136  [ 1392/ 3200]\n",
      "loss: 0.048624  [ 1408/ 3200]\n",
      "loss: 0.049476  [ 1424/ 3200]\n",
      "loss: 0.043181  [ 1440/ 3200]\n",
      "loss: 0.055098  [ 1456/ 3200]\n",
      "loss: 0.029027  [ 1472/ 3200]\n",
      "loss: 0.050973  [ 1488/ 3200]\n",
      "loss: 0.081176  [ 1504/ 3200]\n",
      "loss: 0.026453  [ 1520/ 3200]\n",
      "loss: 0.070530  [ 1536/ 3200]\n",
      "loss: 0.292928  [ 1552/ 3200]\n",
      "loss: 0.124698  [ 1568/ 3200]\n",
      "loss: 0.140936  [ 1584/ 3200]\n",
      "loss: 0.112404  [ 1600/ 3200]\n",
      "loss: 0.351914  [ 1616/ 3200]\n",
      "loss: 0.059768  [ 1632/ 3200]\n",
      "loss: 0.035820  [ 1648/ 3200]\n",
      "loss: 0.104311  [ 1664/ 3200]\n",
      "loss: 0.031029  [ 1680/ 3200]\n",
      "loss: 0.055657  [ 1696/ 3200]\n",
      "loss: 0.094551  [ 1712/ 3200]\n",
      "loss: 0.033065  [ 1728/ 3200]\n",
      "loss: 0.184418  [ 1744/ 3200]\n",
      "loss: 0.125470  [ 1760/ 3200]\n",
      "loss: 0.112405  [ 1776/ 3200]\n",
      "loss: 0.429785  [ 1792/ 3200]\n",
      "loss: 0.368780  [ 1808/ 3200]\n",
      "loss: 0.038010  [ 1824/ 3200]\n",
      "loss: 0.178687  [ 1840/ 3200]\n",
      "loss: 0.113650  [ 1856/ 3200]\n",
      "loss: 0.201738  [ 1872/ 3200]\n",
      "loss: 0.055565  [ 1888/ 3200]\n",
      "loss: 0.038599  [ 1904/ 3200]\n",
      "loss: 0.086660  [ 1920/ 3200]\n",
      "loss: 0.176983  [ 1936/ 3200]\n",
      "loss: 0.051523  [ 1952/ 3200]\n",
      "loss: 0.122942  [ 1968/ 3200]\n",
      "loss: 0.087877  [ 1984/ 3200]\n",
      "loss: 0.097951  [ 2000/ 3200]\n",
      "loss: 0.038186  [ 2016/ 3200]\n",
      "loss: 0.104949  [ 2032/ 3200]\n",
      "loss: 0.120496  [ 2048/ 3200]\n",
      "loss: 0.319431  [ 2064/ 3200]\n",
      "loss: 0.034733  [ 2080/ 3200]\n",
      "loss: 0.061331  [ 2096/ 3200]\n",
      "loss: 0.136096  [ 2112/ 3200]\n",
      "loss: 0.351391  [ 2128/ 3200]\n",
      "loss: 0.043782  [ 2144/ 3200]\n",
      "loss: 0.068621  [ 2160/ 3200]\n",
      "loss: 0.104020  [ 2176/ 3200]\n",
      "loss: 0.037382  [ 2192/ 3200]\n",
      "loss: 0.098458  [ 2208/ 3200]\n",
      "loss: 0.076568  [ 2224/ 3200]\n",
      "loss: 0.086682  [ 2240/ 3200]\n",
      "loss: 0.104328  [ 2256/ 3200]\n",
      "loss: 0.062241  [ 2272/ 3200]\n",
      "loss: 0.060336  [ 2288/ 3200]\n",
      "loss: 0.010129  [ 2304/ 3200]\n",
      "loss: 0.025547  [ 2320/ 3200]\n",
      "loss: 0.035299  [ 2336/ 3200]\n",
      "loss: 0.074673  [ 2352/ 3200]\n",
      "loss: 0.165399  [ 2368/ 3200]\n",
      "loss: 0.106337  [ 2384/ 3200]\n",
      "loss: 0.293799  [ 2400/ 3200]\n",
      "loss: 0.077020  [ 2416/ 3200]\n",
      "loss: 0.181069  [ 2432/ 3200]\n",
      "loss: 0.143747  [ 2448/ 3200]\n",
      "loss: 0.109170  [ 2464/ 3200]\n",
      "loss: 0.038133  [ 2480/ 3200]\n",
      "loss: 0.034664  [ 2496/ 3200]\n",
      "loss: 0.049969  [ 2512/ 3200]\n",
      "loss: 0.027681  [ 2528/ 3200]\n",
      "loss: 0.101299  [ 2544/ 3200]\n",
      "loss: 0.085680  [ 2560/ 3200]\n",
      "loss: 0.109141  [ 2576/ 3200]\n",
      "loss: 0.052391  [ 2592/ 3200]\n",
      "loss: 0.071336  [ 2608/ 3200]\n",
      "loss: 0.107449  [ 2624/ 3200]\n",
      "loss: 0.040465  [ 2640/ 3200]\n",
      "loss: 0.175565  [ 2656/ 3200]\n",
      "loss: 0.068572  [ 2672/ 3200]\n",
      "loss: 0.034264  [ 2688/ 3200]\n",
      "loss: 0.061092  [ 2704/ 3200]\n",
      "loss: 0.017449  [ 2720/ 3200]\n",
      "loss: 0.056266  [ 2736/ 3200]\n",
      "loss: 0.290907  [ 2752/ 3200]\n",
      "loss: 0.156320  [ 2768/ 3200]\n",
      "loss: 0.053635  [ 2784/ 3200]\n",
      "loss: 0.179221  [ 2800/ 3200]\n",
      "loss: 0.084545  [ 2816/ 3200]\n",
      "loss: 0.088159  [ 2832/ 3200]\n",
      "loss: 0.191625  [ 2848/ 3200]\n",
      "loss: 0.063708  [ 2864/ 3200]\n",
      "loss: 0.042728  [ 2880/ 3200]\n",
      "loss: 0.079856  [ 2896/ 3200]\n",
      "loss: 0.055525  [ 2912/ 3200]\n",
      "loss: 0.063845  [ 2928/ 3200]\n",
      "loss: 0.032037  [ 2944/ 3200]\n",
      "loss: 0.162420  [ 2960/ 3200]\n",
      "loss: 0.021008  [ 2976/ 3200]\n",
      "loss: 0.030101  [ 2992/ 3200]\n",
      "loss: 0.069230  [ 3008/ 3200]\n",
      "loss: 0.188146  [ 3024/ 3200]\n",
      "loss: 0.128975  [ 3040/ 3200]\n",
      "loss: 0.095554  [ 3056/ 3200]\n",
      "loss: 0.188480  [ 3072/ 3200]\n",
      "loss: 0.505144  [ 3088/ 3200]\n",
      "loss: 0.108301  [ 3104/ 3200]\n",
      "loss: 0.166654  [ 3120/ 3200]\n",
      "loss: 0.470183  [ 3136/ 3200]\n",
      "loss: 0.091993  [ 3152/ 3200]\n",
      "loss: 0.269558  [ 3168/ 3200]\n",
      "loss: 0.115109  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.041828\n",
      "f1 macro averaged score: 0.779148\n",
      "Accuracy               : 77.9%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  14,   0,  14],\n",
      "        [ 13, 132,  20,  35],\n",
      "        [  0,  26, 160,  14],\n",
      "        [  8,  20,  13, 159]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 8.3624e-04.\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 0.132532  [    0/ 3200]\n",
      "loss: 0.040417  [   16/ 3200]\n",
      "loss: 0.134723  [   32/ 3200]\n",
      "loss: 0.020187  [   48/ 3200]\n",
      "loss: 0.025521  [   64/ 3200]\n",
      "loss: 0.046301  [   80/ 3200]\n",
      "loss: 0.066660  [   96/ 3200]\n",
      "loss: 0.176156  [  112/ 3200]\n",
      "loss: 0.118380  [  128/ 3200]\n",
      "loss: 0.025673  [  144/ 3200]\n",
      "loss: 0.084168  [  160/ 3200]\n",
      "loss: 0.049167  [  176/ 3200]\n",
      "loss: 0.016941  [  192/ 3200]\n",
      "loss: 0.077443  [  208/ 3200]\n",
      "loss: 0.083996  [  224/ 3200]\n",
      "loss: 0.126397  [  240/ 3200]\n",
      "loss: 0.136389  [  256/ 3200]\n",
      "loss: 0.069507  [  272/ 3200]\n",
      "loss: 0.057997  [  288/ 3200]\n",
      "loss: 0.068545  [  304/ 3200]\n",
      "loss: 0.026386  [  320/ 3200]\n",
      "loss: 0.071630  [  336/ 3200]\n",
      "loss: 0.283082  [  352/ 3200]\n",
      "loss: 0.074166  [  368/ 3200]\n",
      "loss: 0.036378  [  384/ 3200]\n",
      "loss: 0.256056  [  400/ 3200]\n",
      "loss: 0.167110  [  416/ 3200]\n",
      "loss: 0.074835  [  432/ 3200]\n",
      "loss: 0.212642  [  448/ 3200]\n",
      "loss: 0.099809  [  464/ 3200]\n",
      "loss: 0.067328  [  480/ 3200]\n",
      "loss: 0.023876  [  496/ 3200]\n",
      "loss: 0.115686  [  512/ 3200]\n",
      "loss: 0.048189  [  528/ 3200]\n",
      "loss: 0.164197  [  544/ 3200]\n",
      "loss: 0.057921  [  560/ 3200]\n",
      "loss: 0.043239  [  576/ 3200]\n",
      "loss: 0.042773  [  592/ 3200]\n",
      "loss: 0.305860  [  608/ 3200]\n",
      "loss: 0.127241  [  624/ 3200]\n",
      "loss: 0.101485  [  640/ 3200]\n",
      "loss: 0.040786  [  656/ 3200]\n",
      "loss: 0.167779  [  672/ 3200]\n",
      "loss: 0.035432  [  688/ 3200]\n",
      "loss: 0.100040  [  704/ 3200]\n",
      "loss: 0.127612  [  720/ 3200]\n",
      "loss: 0.131782  [  736/ 3200]\n",
      "loss: 0.238041  [  752/ 3200]\n",
      "loss: 0.062103  [  768/ 3200]\n",
      "loss: 0.050727  [  784/ 3200]\n",
      "loss: 0.057793  [  800/ 3200]\n",
      "loss: 0.123092  [  816/ 3200]\n",
      "loss: 0.074921  [  832/ 3200]\n",
      "loss: 0.103911  [  848/ 3200]\n",
      "loss: 0.056215  [  864/ 3200]\n",
      "loss: 0.030338  [  880/ 3200]\n",
      "loss: 0.036793  [  896/ 3200]\n",
      "loss: 0.085417  [  912/ 3200]\n",
      "loss: 0.122986  [  928/ 3200]\n",
      "loss: 0.046280  [  944/ 3200]\n",
      "loss: 0.052250  [  960/ 3200]\n",
      "loss: 0.063403  [  976/ 3200]\n",
      "loss: 0.081152  [  992/ 3200]\n",
      "loss: 0.063577  [ 1008/ 3200]\n",
      "loss: 0.195507  [ 1024/ 3200]\n",
      "loss: 0.107699  [ 1040/ 3200]\n",
      "loss: 0.136567  [ 1056/ 3200]\n",
      "loss: 0.055192  [ 1072/ 3200]\n",
      "loss: 0.225718  [ 1088/ 3200]\n",
      "loss: 0.066974  [ 1104/ 3200]\n",
      "loss: 0.045004  [ 1120/ 3200]\n",
      "loss: 0.039908  [ 1136/ 3200]\n",
      "loss: 0.188086  [ 1152/ 3200]\n",
      "loss: 0.032261  [ 1168/ 3200]\n",
      "loss: 0.040239  [ 1184/ 3200]\n",
      "loss: 0.151845  [ 1200/ 3200]\n",
      "loss: 0.071970  [ 1216/ 3200]\n",
      "loss: 0.117714  [ 1232/ 3200]\n",
      "loss: 0.127388  [ 1248/ 3200]\n",
      "loss: 0.077420  [ 1264/ 3200]\n",
      "loss: 0.029068  [ 1280/ 3200]\n",
      "loss: 0.015769  [ 1296/ 3200]\n",
      "loss: 0.034312  [ 1312/ 3200]\n",
      "loss: 0.044660  [ 1328/ 3200]\n",
      "loss: 0.060115  [ 1344/ 3200]\n",
      "loss: 0.018970  [ 1360/ 3200]\n",
      "loss: 0.099946  [ 1376/ 3200]\n",
      "loss: 0.065074  [ 1392/ 3200]\n",
      "loss: 0.048757  [ 1408/ 3200]\n",
      "loss: 0.075578  [ 1424/ 3200]\n",
      "loss: 0.076861  [ 1440/ 3200]\n",
      "loss: 0.045938  [ 1456/ 3200]\n",
      "loss: 0.075048  [ 1472/ 3200]\n",
      "loss: 0.094619  [ 1488/ 3200]\n",
      "loss: 0.049601  [ 1504/ 3200]\n",
      "loss: 0.063087  [ 1520/ 3200]\n",
      "loss: 0.122665  [ 1536/ 3200]\n",
      "loss: 0.013736  [ 1552/ 3200]\n",
      "loss: 0.060466  [ 1568/ 3200]\n",
      "loss: 0.080505  [ 1584/ 3200]\n",
      "loss: 0.206430  [ 1600/ 3200]\n",
      "loss: 0.173138  [ 1616/ 3200]\n",
      "loss: 0.041908  [ 1632/ 3200]\n",
      "loss: 0.213108  [ 1648/ 3200]\n",
      "loss: 0.029936  [ 1664/ 3200]\n",
      "loss: 0.058542  [ 1680/ 3200]\n",
      "loss: 0.037699  [ 1696/ 3200]\n",
      "loss: 0.100824  [ 1712/ 3200]\n",
      "loss: 0.045802  [ 1728/ 3200]\n",
      "loss: 0.101145  [ 1744/ 3200]\n",
      "loss: 0.380436  [ 1760/ 3200]\n",
      "loss: 0.038028  [ 1776/ 3200]\n",
      "loss: 0.032465  [ 1792/ 3200]\n",
      "loss: 0.062846  [ 1808/ 3200]\n",
      "loss: 0.123695  [ 1824/ 3200]\n",
      "loss: 0.029076  [ 1840/ 3200]\n",
      "loss: 0.043017  [ 1856/ 3200]\n",
      "loss: 0.047923  [ 1872/ 3200]\n",
      "loss: 0.030833  [ 1888/ 3200]\n",
      "loss: 0.021835  [ 1904/ 3200]\n",
      "loss: 0.078248  [ 1920/ 3200]\n",
      "loss: 0.030104  [ 1936/ 3200]\n",
      "loss: 0.097137  [ 1952/ 3200]\n",
      "loss: 0.066469  [ 1968/ 3200]\n",
      "loss: 0.078057  [ 1984/ 3200]\n",
      "loss: 0.038699  [ 2000/ 3200]\n",
      "loss: 0.065708  [ 2016/ 3200]\n",
      "loss: 0.284077  [ 2032/ 3200]\n",
      "loss: 0.045075  [ 2048/ 3200]\n",
      "loss: 0.071298  [ 2064/ 3200]\n",
      "loss: 0.023652  [ 2080/ 3200]\n",
      "loss: 0.049530  [ 2096/ 3200]\n",
      "loss: 0.054841  [ 2112/ 3200]\n",
      "loss: 0.166367  [ 2128/ 3200]\n",
      "loss: 0.072004  [ 2144/ 3200]\n",
      "loss: 0.119145  [ 2160/ 3200]\n",
      "loss: 0.150754  [ 2176/ 3200]\n",
      "loss: 0.038546  [ 2192/ 3200]\n",
      "loss: 0.066887  [ 2208/ 3200]\n",
      "loss: 0.070371  [ 2224/ 3200]\n",
      "loss: 0.078977  [ 2240/ 3200]\n",
      "loss: 0.023726  [ 2256/ 3200]\n",
      "loss: 0.059540  [ 2272/ 3200]\n",
      "loss: 0.032501  [ 2288/ 3200]\n",
      "loss: 0.053836  [ 2304/ 3200]\n",
      "loss: 0.036166  [ 2320/ 3200]\n",
      "loss: 0.111793  [ 2336/ 3200]\n",
      "loss: 0.173323  [ 2352/ 3200]\n",
      "loss: 0.026083  [ 2368/ 3200]\n",
      "loss: 0.038107  [ 2384/ 3200]\n",
      "loss: 0.039764  [ 2400/ 3200]\n",
      "loss: 0.077572  [ 2416/ 3200]\n",
      "loss: 0.035707  [ 2432/ 3200]\n",
      "loss: 0.152983  [ 2448/ 3200]\n",
      "loss: 0.066230  [ 2464/ 3200]\n",
      "loss: 0.058657  [ 2480/ 3200]\n",
      "loss: 0.017512  [ 2496/ 3200]\n",
      "loss: 0.380371  [ 2512/ 3200]\n",
      "loss: 0.036559  [ 2528/ 3200]\n",
      "loss: 0.269562  [ 2544/ 3200]\n",
      "loss: 0.066792  [ 2560/ 3200]\n",
      "loss: 0.142543  [ 2576/ 3200]\n",
      "loss: 0.167724  [ 2592/ 3200]\n",
      "loss: 0.171194  [ 2608/ 3200]\n",
      "loss: 0.080251  [ 2624/ 3200]\n",
      "loss: 0.026141  [ 2640/ 3200]\n",
      "loss: 0.048299  [ 2656/ 3200]\n",
      "loss: 0.051764  [ 2672/ 3200]\n",
      "loss: 0.094043  [ 2688/ 3200]\n",
      "loss: 0.034012  [ 2704/ 3200]\n",
      "loss: 0.026128  [ 2720/ 3200]\n",
      "loss: 0.326641  [ 2736/ 3200]\n",
      "loss: 0.124512  [ 2752/ 3200]\n",
      "loss: 0.084992  [ 2768/ 3200]\n",
      "loss: 0.336806  [ 2784/ 3200]\n",
      "loss: 0.039051  [ 2800/ 3200]\n",
      "loss: 0.059694  [ 2816/ 3200]\n",
      "loss: 0.110980  [ 2832/ 3200]\n",
      "loss: 0.194185  [ 2848/ 3200]\n",
      "loss: 0.036546  [ 2864/ 3200]\n",
      "loss: 0.097531  [ 2880/ 3200]\n",
      "loss: 0.063175  [ 2896/ 3200]\n",
      "loss: 0.065011  [ 2912/ 3200]\n",
      "loss: 0.008365  [ 2928/ 3200]\n",
      "loss: 0.063536  [ 2944/ 3200]\n",
      "loss: 0.070226  [ 2960/ 3200]\n",
      "loss: 0.047074  [ 2976/ 3200]\n",
      "loss: 0.182069  [ 2992/ 3200]\n",
      "loss: 0.193821  [ 3008/ 3200]\n",
      "loss: 0.196452  [ 3024/ 3200]\n",
      "loss: 0.058174  [ 3040/ 3200]\n",
      "loss: 0.102377  [ 3056/ 3200]\n",
      "loss: 0.099496  [ 3072/ 3200]\n",
      "loss: 0.106471  [ 3088/ 3200]\n",
      "loss: 0.039986  [ 3104/ 3200]\n",
      "loss: 0.057964  [ 3120/ 3200]\n",
      "loss: 0.161266  [ 3136/ 3200]\n",
      "loss: 0.261306  [ 3152/ 3200]\n",
      "loss: 0.061689  [ 3168/ 3200]\n",
      "loss: 0.203886  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.045288\n",
      "f1 macro averaged score: 0.770622\n",
      "Accuracy               : 77.4%\n",
      "Confusion matrix       :\n",
      "tensor([[184,   2,   0,  14],\n",
      "        [ 19, 113,  16,  52],\n",
      "        [  0,  22, 154,  24],\n",
      "        [ 10,  11,  11, 168]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.9443e-04.\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 0.058040  [    0/ 3200]\n",
      "loss: 0.035871  [   16/ 3200]\n",
      "loss: 0.032103  [   32/ 3200]\n",
      "loss: 0.051566  [   48/ 3200]\n",
      "loss: 0.028990  [   64/ 3200]\n",
      "loss: 0.146791  [   80/ 3200]\n",
      "loss: 0.104986  [   96/ 3200]\n",
      "loss: 0.044208  [  112/ 3200]\n",
      "loss: 0.111375  [  128/ 3200]\n",
      "loss: 0.028926  [  144/ 3200]\n",
      "loss: 0.037348  [  160/ 3200]\n",
      "loss: 0.033499  [  176/ 3200]\n",
      "loss: 0.022989  [  192/ 3200]\n",
      "loss: 0.050529  [  208/ 3200]\n",
      "loss: 0.225278  [  224/ 3200]\n",
      "loss: 0.025449  [  240/ 3200]\n",
      "loss: 0.022924  [  256/ 3200]\n",
      "loss: 0.100368  [  272/ 3200]\n",
      "loss: 0.011809  [  288/ 3200]\n",
      "loss: 0.024016  [  304/ 3200]\n",
      "loss: 0.077893  [  320/ 3200]\n",
      "loss: 0.047703  [  336/ 3200]\n",
      "loss: 0.063348  [  352/ 3200]\n",
      "loss: 0.086489  [  368/ 3200]\n",
      "loss: 0.077455  [  384/ 3200]\n",
      "loss: 0.050908  [  400/ 3200]\n",
      "loss: 0.066233  [  416/ 3200]\n",
      "loss: 0.105710  [  432/ 3200]\n",
      "loss: 0.096647  [  448/ 3200]\n",
      "loss: 0.101131  [  464/ 3200]\n",
      "loss: 0.123499  [  480/ 3200]\n",
      "loss: 0.033793  [  496/ 3200]\n",
      "loss: 0.023733  [  512/ 3200]\n",
      "loss: 0.039529  [  528/ 3200]\n",
      "loss: 0.072599  [  544/ 3200]\n",
      "loss: 0.085669  [  560/ 3200]\n",
      "loss: 0.047882  [  576/ 3200]\n",
      "loss: 0.063537  [  592/ 3200]\n",
      "loss: 0.068240  [  608/ 3200]\n",
      "loss: 0.028798  [  624/ 3200]\n",
      "loss: 0.049071  [  640/ 3200]\n",
      "loss: 0.144958  [  656/ 3200]\n",
      "loss: 0.077309  [  672/ 3200]\n",
      "loss: 0.125438  [  688/ 3200]\n",
      "loss: 0.085482  [  704/ 3200]\n",
      "loss: 0.041484  [  720/ 3200]\n",
      "loss: 0.019947  [  736/ 3200]\n",
      "loss: 0.052540  [  752/ 3200]\n",
      "loss: 0.025238  [  768/ 3200]\n",
      "loss: 0.097562  [  784/ 3200]\n",
      "loss: 0.055049  [  800/ 3200]\n",
      "loss: 0.251158  [  816/ 3200]\n",
      "loss: 0.044338  [  832/ 3200]\n",
      "loss: 0.110852  [  848/ 3200]\n",
      "loss: 0.188299  [  864/ 3200]\n",
      "loss: 0.046626  [  880/ 3200]\n",
      "loss: 0.053655  [  896/ 3200]\n",
      "loss: 0.051251  [  912/ 3200]\n",
      "loss: 0.103862  [  928/ 3200]\n",
      "loss: 0.026007  [  944/ 3200]\n",
      "loss: 0.097140  [  960/ 3200]\n",
      "loss: 0.051675  [  976/ 3200]\n",
      "loss: 0.042171  [  992/ 3200]\n",
      "loss: 0.022734  [ 1008/ 3200]\n",
      "loss: 0.025631  [ 1024/ 3200]\n",
      "loss: 0.019570  [ 1040/ 3200]\n",
      "loss: 0.026532  [ 1056/ 3200]\n",
      "loss: 0.063912  [ 1072/ 3200]\n",
      "loss: 0.022750  [ 1088/ 3200]\n",
      "loss: 0.059078  [ 1104/ 3200]\n",
      "loss: 0.066937  [ 1120/ 3200]\n",
      "loss: 0.221424  [ 1136/ 3200]\n",
      "loss: 0.216228  [ 1152/ 3200]\n",
      "loss: 0.074442  [ 1168/ 3200]\n",
      "loss: 0.018184  [ 1184/ 3200]\n",
      "loss: 0.343243  [ 1200/ 3200]\n",
      "loss: 0.139811  [ 1216/ 3200]\n",
      "loss: 0.065173  [ 1232/ 3200]\n",
      "loss: 0.035323  [ 1248/ 3200]\n",
      "loss: 0.289534  [ 1264/ 3200]\n",
      "loss: 0.020347  [ 1280/ 3200]\n",
      "loss: 0.048114  [ 1296/ 3200]\n",
      "loss: 0.055001  [ 1312/ 3200]\n",
      "loss: 0.093734  [ 1328/ 3200]\n",
      "loss: 0.091586  [ 1344/ 3200]\n",
      "loss: 0.088238  [ 1360/ 3200]\n",
      "loss: 0.091516  [ 1376/ 3200]\n",
      "loss: 0.063255  [ 1392/ 3200]\n",
      "loss: 0.069456  [ 1408/ 3200]\n",
      "loss: 0.078032  [ 1424/ 3200]\n",
      "loss: 0.104574  [ 1440/ 3200]\n",
      "loss: 0.068430  [ 1456/ 3200]\n",
      "loss: 0.288408  [ 1472/ 3200]\n",
      "loss: 0.020330  [ 1488/ 3200]\n",
      "loss: 0.059941  [ 1504/ 3200]\n",
      "loss: 0.073323  [ 1520/ 3200]\n",
      "loss: 0.048154  [ 1536/ 3200]\n",
      "loss: 0.041030  [ 1552/ 3200]\n",
      "loss: 0.033693  [ 1568/ 3200]\n",
      "loss: 0.107328  [ 1584/ 3200]\n",
      "loss: 0.042143  [ 1600/ 3200]\n",
      "loss: 0.061297  [ 1616/ 3200]\n",
      "loss: 0.064273  [ 1632/ 3200]\n",
      "loss: 0.042104  [ 1648/ 3200]\n",
      "loss: 0.125349  [ 1664/ 3200]\n",
      "loss: 0.057084  [ 1680/ 3200]\n",
      "loss: 0.080522  [ 1696/ 3200]\n",
      "loss: 0.085317  [ 1712/ 3200]\n",
      "loss: 0.041689  [ 1728/ 3200]\n",
      "loss: 0.047130  [ 1744/ 3200]\n",
      "loss: 0.038172  [ 1760/ 3200]\n",
      "loss: 0.027001  [ 1776/ 3200]\n",
      "loss: 0.041910  [ 1792/ 3200]\n",
      "loss: 0.038861  [ 1808/ 3200]\n",
      "loss: 0.161964  [ 1824/ 3200]\n",
      "loss: 0.095971  [ 1840/ 3200]\n",
      "loss: 0.053899  [ 1856/ 3200]\n",
      "loss: 0.245934  [ 1872/ 3200]\n",
      "loss: 0.120960  [ 1888/ 3200]\n",
      "loss: 0.058453  [ 1904/ 3200]\n",
      "loss: 0.055611  [ 1920/ 3200]\n",
      "loss: 0.116961  [ 1936/ 3200]\n",
      "loss: 0.031241  [ 1952/ 3200]\n",
      "loss: 0.068152  [ 1968/ 3200]\n",
      "loss: 0.019726  [ 1984/ 3200]\n",
      "loss: 0.085109  [ 2000/ 3200]\n",
      "loss: 0.242818  [ 2016/ 3200]\n",
      "loss: 0.055107  [ 2032/ 3200]\n",
      "loss: 0.060464  [ 2048/ 3200]\n",
      "loss: 0.082277  [ 2064/ 3200]\n",
      "loss: 0.051786  [ 2080/ 3200]\n",
      "loss: 0.029729  [ 2096/ 3200]\n",
      "loss: 0.027460  [ 2112/ 3200]\n",
      "loss: 0.019099  [ 2128/ 3200]\n",
      "loss: 0.073029  [ 2144/ 3200]\n",
      "loss: 0.099499  [ 2160/ 3200]\n",
      "loss: 0.102969  [ 2176/ 3200]\n",
      "loss: 0.094150  [ 2192/ 3200]\n",
      "loss: 0.019761  [ 2208/ 3200]\n",
      "loss: 0.074514  [ 2224/ 3200]\n",
      "loss: 0.009740  [ 2240/ 3200]\n",
      "loss: 0.041279  [ 2256/ 3200]\n",
      "loss: 0.066869  [ 2272/ 3200]\n",
      "loss: 0.072794  [ 2288/ 3200]\n",
      "loss: 0.116022  [ 2304/ 3200]\n",
      "loss: 0.025570  [ 2320/ 3200]\n",
      "loss: 0.104255  [ 2336/ 3200]\n",
      "loss: 0.023233  [ 2352/ 3200]\n",
      "loss: 0.074768  [ 2368/ 3200]\n",
      "loss: 0.157582  [ 2384/ 3200]\n",
      "loss: 0.048792  [ 2400/ 3200]\n",
      "loss: 0.126497  [ 2416/ 3200]\n",
      "loss: 0.068212  [ 2432/ 3200]\n",
      "loss: 0.103582  [ 2448/ 3200]\n",
      "loss: 0.097496  [ 2464/ 3200]\n",
      "loss: 0.426314  [ 2480/ 3200]\n",
      "loss: 0.120695  [ 2496/ 3200]\n",
      "loss: 0.022970  [ 2512/ 3200]\n",
      "loss: 0.030633  [ 2528/ 3200]\n",
      "loss: 0.049651  [ 2544/ 3200]\n",
      "loss: 0.059129  [ 2560/ 3200]\n",
      "loss: 0.064765  [ 2576/ 3200]\n",
      "loss: 0.023330  [ 2592/ 3200]\n",
      "loss: 0.117517  [ 2608/ 3200]\n",
      "loss: 0.056625  [ 2624/ 3200]\n",
      "loss: 0.138765  [ 2640/ 3200]\n",
      "loss: 0.043547  [ 2656/ 3200]\n",
      "loss: 0.043138  [ 2672/ 3200]\n",
      "loss: 0.009540  [ 2688/ 3200]\n",
      "loss: 0.013378  [ 2704/ 3200]\n",
      "loss: 0.173831  [ 2720/ 3200]\n",
      "loss: 0.022792  [ 2736/ 3200]\n",
      "loss: 0.087492  [ 2752/ 3200]\n",
      "loss: 0.039283  [ 2768/ 3200]\n",
      "loss: 0.163578  [ 2784/ 3200]\n",
      "loss: 0.105120  [ 2800/ 3200]\n",
      "loss: 0.032205  [ 2816/ 3200]\n",
      "loss: 0.124892  [ 2832/ 3200]\n",
      "loss: 0.016142  [ 2848/ 3200]\n",
      "loss: 0.027823  [ 2864/ 3200]\n",
      "loss: 0.057773  [ 2880/ 3200]\n",
      "loss: 0.072639  [ 2896/ 3200]\n",
      "loss: 0.362611  [ 2912/ 3200]\n",
      "loss: 0.318932  [ 2928/ 3200]\n",
      "loss: 0.035913  [ 2944/ 3200]\n",
      "loss: 0.034793  [ 2960/ 3200]\n",
      "loss: 0.068882  [ 2976/ 3200]\n",
      "loss: 0.109173  [ 2992/ 3200]\n",
      "loss: 0.016988  [ 3008/ 3200]\n",
      "loss: 0.042039  [ 3024/ 3200]\n",
      "loss: 0.139101  [ 3040/ 3200]\n",
      "loss: 0.053424  [ 3056/ 3200]\n",
      "loss: 0.033639  [ 3072/ 3200]\n",
      "loss: 0.070477  [ 3088/ 3200]\n",
      "loss: 0.036814  [ 3104/ 3200]\n",
      "loss: 0.033206  [ 3120/ 3200]\n",
      "loss: 0.128924  [ 3136/ 3200]\n",
      "loss: 0.024148  [ 3152/ 3200]\n",
      "loss: 0.268779  [ 3168/ 3200]\n",
      "loss: 0.093365  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.046345\n",
      "f1 macro averaged score: 0.777920\n",
      "Accuracy               : 78.2%\n",
      "Confusion matrix       :\n",
      "tensor([[173,   9,   0,  18],\n",
      "        [ 15, 112,  31,  42],\n",
      "        [  0,  14, 177,   9],\n",
      "        [  8,  12,  16, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.5471e-04.\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 0.094334  [    0/ 3200]\n",
      "loss: 0.040869  [   16/ 3200]\n",
      "loss: 0.077478  [   32/ 3200]\n",
      "loss: 0.020438  [   48/ 3200]\n",
      "loss: 0.029774  [   64/ 3200]\n",
      "loss: 0.076544  [   80/ 3200]\n",
      "loss: 0.033290  [   96/ 3200]\n",
      "loss: 0.038104  [  112/ 3200]\n",
      "loss: 0.021778  [  128/ 3200]\n",
      "loss: 0.332294  [  144/ 3200]\n",
      "loss: 0.143425  [  160/ 3200]\n",
      "loss: 0.181270  [  176/ 3200]\n",
      "loss: 0.041875  [  192/ 3200]\n",
      "loss: 0.061889  [  208/ 3200]\n",
      "loss: 0.053406  [  224/ 3200]\n",
      "loss: 0.145357  [  240/ 3200]\n",
      "loss: 0.049911  [  256/ 3200]\n",
      "loss: 0.047762  [  272/ 3200]\n",
      "loss: 0.171745  [  288/ 3200]\n",
      "loss: 0.034579  [  304/ 3200]\n",
      "loss: 0.083767  [  320/ 3200]\n",
      "loss: 0.048716  [  336/ 3200]\n",
      "loss: 0.050727  [  352/ 3200]\n",
      "loss: 0.046703  [  368/ 3200]\n",
      "loss: 0.074892  [  384/ 3200]\n",
      "loss: 0.095477  [  400/ 3200]\n",
      "loss: 0.041096  [  416/ 3200]\n",
      "loss: 0.107254  [  432/ 3200]\n",
      "loss: 0.047096  [  448/ 3200]\n",
      "loss: 0.023679  [  464/ 3200]\n",
      "loss: 0.138216  [  480/ 3200]\n",
      "loss: 0.094399  [  496/ 3200]\n",
      "loss: 0.124085  [  512/ 3200]\n",
      "loss: 0.067741  [  528/ 3200]\n",
      "loss: 0.024924  [  544/ 3200]\n",
      "loss: 0.095993  [  560/ 3200]\n",
      "loss: 0.032995  [  576/ 3200]\n",
      "loss: 0.077696  [  592/ 3200]\n",
      "loss: 0.017360  [  608/ 3200]\n",
      "loss: 0.054130  [  624/ 3200]\n",
      "loss: 0.032976  [  640/ 3200]\n",
      "loss: 0.174972  [  656/ 3200]\n",
      "loss: 0.032216  [  672/ 3200]\n",
      "loss: 0.054856  [  688/ 3200]\n",
      "loss: 0.083068  [  704/ 3200]\n",
      "loss: 0.094289  [  720/ 3200]\n",
      "loss: 0.049920  [  736/ 3200]\n",
      "loss: 0.047595  [  752/ 3200]\n",
      "loss: 0.071486  [  768/ 3200]\n",
      "loss: 0.017173  [  784/ 3200]\n",
      "loss: 0.041624  [  800/ 3200]\n",
      "loss: 0.041815  [  816/ 3200]\n",
      "loss: 0.036685  [  832/ 3200]\n",
      "loss: 0.030733  [  848/ 3200]\n",
      "loss: 0.078791  [  864/ 3200]\n",
      "loss: 0.050584  [  880/ 3200]\n",
      "loss: 0.037397  [  896/ 3200]\n",
      "loss: 0.023714  [  912/ 3200]\n",
      "loss: 0.043851  [  928/ 3200]\n",
      "loss: 0.015157  [  944/ 3200]\n",
      "loss: 0.062593  [  960/ 3200]\n",
      "loss: 0.021757  [  976/ 3200]\n",
      "loss: 0.064058  [  992/ 3200]\n",
      "loss: 0.125312  [ 1008/ 3200]\n",
      "loss: 0.013181  [ 1024/ 3200]\n",
      "loss: 0.074697  [ 1040/ 3200]\n",
      "loss: 0.069841  [ 1056/ 3200]\n",
      "loss: 0.039766  [ 1072/ 3200]\n",
      "loss: 0.035968  [ 1088/ 3200]\n",
      "loss: 0.061729  [ 1104/ 3200]\n",
      "loss: 0.057330  [ 1120/ 3200]\n",
      "loss: 0.025934  [ 1136/ 3200]\n",
      "loss: 0.086647  [ 1152/ 3200]\n",
      "loss: 0.027240  [ 1168/ 3200]\n",
      "loss: 0.051799  [ 1184/ 3200]\n",
      "loss: 0.032790  [ 1200/ 3200]\n",
      "loss: 0.052896  [ 1216/ 3200]\n",
      "loss: 0.064178  [ 1232/ 3200]\n",
      "loss: 0.031773  [ 1248/ 3200]\n",
      "loss: 0.038516  [ 1264/ 3200]\n",
      "loss: 0.027388  [ 1280/ 3200]\n",
      "loss: 0.028233  [ 1296/ 3200]\n",
      "loss: 0.056369  [ 1312/ 3200]\n",
      "loss: 0.133867  [ 1328/ 3200]\n",
      "loss: 0.093203  [ 1344/ 3200]\n",
      "loss: 0.057057  [ 1360/ 3200]\n",
      "loss: 0.021886  [ 1376/ 3200]\n",
      "loss: 0.051662  [ 1392/ 3200]\n",
      "loss: 0.079109  [ 1408/ 3200]\n",
      "loss: 0.021137  [ 1424/ 3200]\n",
      "loss: 0.059952  [ 1440/ 3200]\n",
      "loss: 0.323673  [ 1456/ 3200]\n",
      "loss: 0.157560  [ 1472/ 3200]\n",
      "loss: 0.096100  [ 1488/ 3200]\n",
      "loss: 0.175843  [ 1504/ 3200]\n",
      "loss: 0.186588  [ 1520/ 3200]\n",
      "loss: 0.042287  [ 1536/ 3200]\n",
      "loss: 0.012036  [ 1552/ 3200]\n",
      "loss: 0.102326  [ 1568/ 3200]\n",
      "loss: 0.037448  [ 1584/ 3200]\n",
      "loss: 0.033612  [ 1600/ 3200]\n",
      "loss: 0.059733  [ 1616/ 3200]\n",
      "loss: 0.032111  [ 1632/ 3200]\n",
      "loss: 0.021080  [ 1648/ 3200]\n",
      "loss: 0.015231  [ 1664/ 3200]\n",
      "loss: 0.050464  [ 1680/ 3200]\n",
      "loss: 0.039815  [ 1696/ 3200]\n",
      "loss: 0.016576  [ 1712/ 3200]\n",
      "loss: 0.042578  [ 1728/ 3200]\n",
      "loss: 0.053321  [ 1744/ 3200]\n",
      "loss: 0.051499  [ 1760/ 3200]\n",
      "loss: 0.074907  [ 1776/ 3200]\n",
      "loss: 0.040150  [ 1792/ 3200]\n",
      "loss: 0.041111  [ 1808/ 3200]\n",
      "loss: 0.129599  [ 1824/ 3200]\n",
      "loss: 0.051596  [ 1840/ 3200]\n",
      "loss: 0.082577  [ 1856/ 3200]\n",
      "loss: 0.023172  [ 1872/ 3200]\n",
      "loss: 0.022722  [ 1888/ 3200]\n",
      "loss: 0.022193  [ 1904/ 3200]\n",
      "loss: 0.020198  [ 1920/ 3200]\n",
      "loss: 0.069515  [ 1936/ 3200]\n",
      "loss: 0.071014  [ 1952/ 3200]\n",
      "loss: 0.027644  [ 1968/ 3200]\n",
      "loss: 0.021008  [ 1984/ 3200]\n",
      "loss: 0.060488  [ 2000/ 3200]\n",
      "loss: 0.023589  [ 2016/ 3200]\n",
      "loss: 0.026631  [ 2032/ 3200]\n",
      "loss: 0.059508  [ 2048/ 3200]\n",
      "loss: 0.015472  [ 2064/ 3200]\n",
      "loss: 0.025233  [ 2080/ 3200]\n",
      "loss: 0.178269  [ 2096/ 3200]\n",
      "loss: 0.037212  [ 2112/ 3200]\n",
      "loss: 0.030270  [ 2128/ 3200]\n",
      "loss: 0.029428  [ 2144/ 3200]\n",
      "loss: 0.054027  [ 2160/ 3200]\n",
      "loss: 0.018510  [ 2176/ 3200]\n",
      "loss: 0.087607  [ 2192/ 3200]\n",
      "loss: 0.066094  [ 2208/ 3200]\n",
      "loss: 0.063805  [ 2224/ 3200]\n",
      "loss: 0.077656  [ 2240/ 3200]\n",
      "loss: 0.014404  [ 2256/ 3200]\n",
      "loss: 0.085130  [ 2272/ 3200]\n",
      "loss: 0.120005  [ 2288/ 3200]\n",
      "loss: 0.022773  [ 2304/ 3200]\n",
      "loss: 0.021357  [ 2320/ 3200]\n",
      "loss: 0.111641  [ 2336/ 3200]\n",
      "loss: 0.061074  [ 2352/ 3200]\n",
      "loss: 0.183296  [ 2368/ 3200]\n",
      "loss: 0.138517  [ 2384/ 3200]\n",
      "loss: 0.101727  [ 2400/ 3200]\n",
      "loss: 0.195294  [ 2416/ 3200]\n",
      "loss: 0.060611  [ 2432/ 3200]\n",
      "loss: 0.088428  [ 2448/ 3200]\n",
      "loss: 0.106722  [ 2464/ 3200]\n",
      "loss: 0.043493  [ 2480/ 3200]\n",
      "loss: 0.087162  [ 2496/ 3200]\n",
      "loss: 0.032370  [ 2512/ 3200]\n",
      "loss: 0.068429  [ 2528/ 3200]\n",
      "loss: 0.054479  [ 2544/ 3200]\n",
      "loss: 0.025643  [ 2560/ 3200]\n",
      "loss: 0.091674  [ 2576/ 3200]\n",
      "loss: 0.185639  [ 2592/ 3200]\n",
      "loss: 0.065722  [ 2608/ 3200]\n",
      "loss: 0.051083  [ 2624/ 3200]\n",
      "loss: 0.058431  [ 2640/ 3200]\n",
      "loss: 0.107349  [ 2656/ 3200]\n",
      "loss: 0.144166  [ 2672/ 3200]\n",
      "loss: 0.022005  [ 2688/ 3200]\n",
      "loss: 0.008389  [ 2704/ 3200]\n",
      "loss: 0.091995  [ 2720/ 3200]\n",
      "loss: 0.029074  [ 2736/ 3200]\n",
      "loss: 0.016823  [ 2752/ 3200]\n",
      "loss: 0.255908  [ 2768/ 3200]\n",
      "loss: 0.074036  [ 2784/ 3200]\n",
      "loss: 0.059864  [ 2800/ 3200]\n",
      "loss: 0.241386  [ 2816/ 3200]\n",
      "loss: 0.043377  [ 2832/ 3200]\n",
      "loss: 0.005799  [ 2848/ 3200]\n",
      "loss: 0.150439  [ 2864/ 3200]\n",
      "loss: 0.052724  [ 2880/ 3200]\n",
      "loss: 0.039067  [ 2896/ 3200]\n",
      "loss: 0.033158  [ 2912/ 3200]\n",
      "loss: 0.035237  [ 2928/ 3200]\n",
      "loss: 0.035944  [ 2944/ 3200]\n",
      "loss: 0.053246  [ 2960/ 3200]\n",
      "loss: 0.137175  [ 2976/ 3200]\n",
      "loss: 0.043886  [ 2992/ 3200]\n",
      "loss: 0.146310  [ 3008/ 3200]\n",
      "loss: 0.024610  [ 3024/ 3200]\n",
      "loss: 0.014233  [ 3040/ 3200]\n",
      "loss: 0.038913  [ 3056/ 3200]\n",
      "loss: 0.090178  [ 3072/ 3200]\n",
      "loss: 0.021248  [ 3088/ 3200]\n",
      "loss: 0.098310  [ 3104/ 3200]\n",
      "loss: 0.120442  [ 3120/ 3200]\n",
      "loss: 0.111510  [ 3136/ 3200]\n",
      "loss: 0.025901  [ 3152/ 3200]\n",
      "loss: 0.040220  [ 3168/ 3200]\n",
      "loss: 0.078257  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.049471\n",
      "f1 macro averaged score: 0.767294\n",
      "Accuracy               : 77.0%\n",
      "Confusion matrix       :\n",
      "tensor([[181,   3,   0,  16],\n",
      "        [ 17, 110,  17,  56],\n",
      "        [  0,  18, 155,  27],\n",
      "        [  9,  11,  10, 170]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.1697e-04.\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.037549  [    0/ 3200]\n",
      "loss: 0.048705  [   16/ 3200]\n",
      "loss: 0.022380  [   32/ 3200]\n",
      "loss: 0.063199  [   48/ 3200]\n",
      "loss: 0.055776  [   64/ 3200]\n",
      "loss: 0.053824  [   80/ 3200]\n",
      "loss: 0.017188  [   96/ 3200]\n",
      "loss: 0.025346  [  112/ 3200]\n",
      "loss: 0.043189  [  128/ 3200]\n",
      "loss: 0.034957  [  144/ 3200]\n",
      "loss: 0.129925  [  160/ 3200]\n",
      "loss: 0.081151  [  176/ 3200]\n",
      "loss: 0.064726  [  192/ 3200]\n",
      "loss: 0.018284  [  208/ 3200]\n",
      "loss: 0.045763  [  224/ 3200]\n",
      "loss: 0.112735  [  240/ 3200]\n",
      "loss: 0.070598  [  256/ 3200]\n",
      "loss: 0.084023  [  272/ 3200]\n",
      "loss: 0.100752  [  288/ 3200]\n",
      "loss: 0.047981  [  304/ 3200]\n",
      "loss: 0.268826  [  320/ 3200]\n",
      "loss: 0.159646  [  336/ 3200]\n",
      "loss: 0.021313  [  352/ 3200]\n",
      "loss: 0.033344  [  368/ 3200]\n",
      "loss: 0.043325  [  384/ 3200]\n",
      "loss: 0.045235  [  400/ 3200]\n",
      "loss: 0.049461  [  416/ 3200]\n",
      "loss: 0.060185  [  432/ 3200]\n",
      "loss: 0.019519  [  448/ 3200]\n",
      "loss: 0.037005  [  464/ 3200]\n",
      "loss: 0.020764  [  480/ 3200]\n",
      "loss: 0.095246  [  496/ 3200]\n",
      "loss: 0.054263  [  512/ 3200]\n",
      "loss: 0.027990  [  528/ 3200]\n",
      "loss: 0.031045  [  544/ 3200]\n",
      "loss: 0.065788  [  560/ 3200]\n",
      "loss: 0.043034  [  576/ 3200]\n",
      "loss: 0.052228  [  592/ 3200]\n",
      "loss: 0.032145  [  608/ 3200]\n",
      "loss: 0.013464  [  624/ 3200]\n",
      "loss: 0.007259  [  640/ 3200]\n",
      "loss: 0.118221  [  656/ 3200]\n",
      "loss: 0.030466  [  672/ 3200]\n",
      "loss: 0.034636  [  688/ 3200]\n",
      "loss: 0.022846  [  704/ 3200]\n",
      "loss: 0.081184  [  720/ 3200]\n",
      "loss: 0.067777  [  736/ 3200]\n",
      "loss: 0.138549  [  752/ 3200]\n",
      "loss: 0.032976  [  768/ 3200]\n",
      "loss: 0.159924  [  784/ 3200]\n",
      "loss: 0.035545  [  800/ 3200]\n",
      "loss: 0.160840  [  816/ 3200]\n",
      "loss: 0.016948  [  832/ 3200]\n",
      "loss: 0.064254  [  848/ 3200]\n",
      "loss: 0.120711  [  864/ 3200]\n",
      "loss: 0.064102  [  880/ 3200]\n",
      "loss: 0.040141  [  896/ 3200]\n",
      "loss: 0.023860  [  912/ 3200]\n",
      "loss: 0.122772  [  928/ 3200]\n",
      "loss: 0.030234  [  944/ 3200]\n",
      "loss: 0.177624  [  960/ 3200]\n",
      "loss: 0.025635  [  976/ 3200]\n",
      "loss: 0.049920  [  992/ 3200]\n",
      "loss: 0.121788  [ 1008/ 3200]\n",
      "loss: 0.010924  [ 1024/ 3200]\n",
      "loss: 0.040347  [ 1040/ 3200]\n",
      "loss: 0.019849  [ 1056/ 3200]\n",
      "loss: 0.067772  [ 1072/ 3200]\n",
      "loss: 0.021915  [ 1088/ 3200]\n",
      "loss: 0.062434  [ 1104/ 3200]\n",
      "loss: 0.032580  [ 1120/ 3200]\n",
      "loss: 0.026881  [ 1136/ 3200]\n",
      "loss: 0.075852  [ 1152/ 3200]\n",
      "loss: 0.087343  [ 1168/ 3200]\n",
      "loss: 0.090582  [ 1184/ 3200]\n",
      "loss: 0.084005  [ 1200/ 3200]\n",
      "loss: 0.039527  [ 1216/ 3200]\n",
      "loss: 0.008572  [ 1232/ 3200]\n",
      "loss: 0.069965  [ 1248/ 3200]\n",
      "loss: 0.054460  [ 1264/ 3200]\n",
      "loss: 0.008025  [ 1280/ 3200]\n",
      "loss: 0.023543  [ 1296/ 3200]\n",
      "loss: 0.045841  [ 1312/ 3200]\n",
      "loss: 0.033032  [ 1328/ 3200]\n",
      "loss: 0.047875  [ 1344/ 3200]\n",
      "loss: 0.052711  [ 1360/ 3200]\n",
      "loss: 0.081690  [ 1376/ 3200]\n",
      "loss: 0.027531  [ 1392/ 3200]\n",
      "loss: 0.063608  [ 1408/ 3200]\n",
      "loss: 0.022445  [ 1424/ 3200]\n",
      "loss: 0.035796  [ 1440/ 3200]\n",
      "loss: 0.056060  [ 1456/ 3200]\n",
      "loss: 0.170694  [ 1472/ 3200]\n",
      "loss: 0.034994  [ 1488/ 3200]\n",
      "loss: 0.025074  [ 1504/ 3200]\n",
      "loss: 0.017044  [ 1520/ 3200]\n",
      "loss: 0.039169  [ 1536/ 3200]\n",
      "loss: 0.078998  [ 1552/ 3200]\n",
      "loss: 0.017304  [ 1568/ 3200]\n",
      "loss: 0.017392  [ 1584/ 3200]\n",
      "loss: 0.086325  [ 1600/ 3200]\n",
      "loss: 0.030886  [ 1616/ 3200]\n",
      "loss: 0.011713  [ 1632/ 3200]\n",
      "loss: 0.055832  [ 1648/ 3200]\n",
      "loss: 0.022559  [ 1664/ 3200]\n",
      "loss: 0.286435  [ 1680/ 3200]\n",
      "loss: 0.071562  [ 1696/ 3200]\n",
      "loss: 0.115106  [ 1712/ 3200]\n",
      "loss: 0.022700  [ 1728/ 3200]\n",
      "loss: 0.039862  [ 1744/ 3200]\n",
      "loss: 0.103913  [ 1760/ 3200]\n",
      "loss: 0.036558  [ 1776/ 3200]\n",
      "loss: 0.142782  [ 1792/ 3200]\n",
      "loss: 0.037975  [ 1808/ 3200]\n",
      "loss: 0.012132  [ 1824/ 3200]\n",
      "loss: 0.157350  [ 1840/ 3200]\n",
      "loss: 0.045139  [ 1856/ 3200]\n",
      "loss: 0.033161  [ 1872/ 3200]\n",
      "loss: 0.070556  [ 1888/ 3200]\n",
      "loss: 0.021385  [ 1904/ 3200]\n",
      "loss: 0.023652  [ 1920/ 3200]\n",
      "loss: 0.098524  [ 1936/ 3200]\n",
      "loss: 0.062963  [ 1952/ 3200]\n",
      "loss: 0.016245  [ 1968/ 3200]\n",
      "loss: 0.088116  [ 1984/ 3200]\n",
      "loss: 0.065106  [ 2000/ 3200]\n",
      "loss: 0.068705  [ 2016/ 3200]\n",
      "loss: 0.066135  [ 2032/ 3200]\n",
      "loss: 0.031824  [ 2048/ 3200]\n",
      "loss: 0.047330  [ 2064/ 3200]\n",
      "loss: 0.054711  [ 2080/ 3200]\n",
      "loss: 0.041154  [ 2096/ 3200]\n",
      "loss: 0.038813  [ 2112/ 3200]\n",
      "loss: 0.059116  [ 2128/ 3200]\n",
      "loss: 0.037057  [ 2144/ 3200]\n",
      "loss: 0.043363  [ 2160/ 3200]\n",
      "loss: 0.029178  [ 2176/ 3200]\n",
      "loss: 0.051129  [ 2192/ 3200]\n",
      "loss: 0.018846  [ 2208/ 3200]\n",
      "loss: 0.065026  [ 2224/ 3200]\n",
      "loss: 0.095776  [ 2240/ 3200]\n",
      "loss: 0.016473  [ 2256/ 3200]\n",
      "loss: 0.027538  [ 2272/ 3200]\n",
      "loss: 0.065588  [ 2288/ 3200]\n",
      "loss: 0.045252  [ 2304/ 3200]\n",
      "loss: 0.019173  [ 2320/ 3200]\n",
      "loss: 0.051604  [ 2336/ 3200]\n",
      "loss: 0.003491  [ 2352/ 3200]\n",
      "loss: 0.059783  [ 2368/ 3200]\n",
      "loss: 0.122707  [ 2384/ 3200]\n",
      "loss: 0.088399  [ 2400/ 3200]\n",
      "loss: 0.032177  [ 2416/ 3200]\n",
      "loss: 0.074500  [ 2432/ 3200]\n",
      "loss: 0.045908  [ 2448/ 3200]\n",
      "loss: 0.048339  [ 2464/ 3200]\n",
      "loss: 0.075940  [ 2480/ 3200]\n",
      "loss: 0.418804  [ 2496/ 3200]\n",
      "loss: 0.032990  [ 2512/ 3200]\n",
      "loss: 0.012727  [ 2528/ 3200]\n",
      "loss: 0.072895  [ 2544/ 3200]\n",
      "loss: 0.078649  [ 2560/ 3200]\n",
      "loss: 0.076815  [ 2576/ 3200]\n",
      "loss: 0.044058  [ 2592/ 3200]\n",
      "loss: 0.116778  [ 2608/ 3200]\n",
      "loss: 0.033247  [ 2624/ 3200]\n",
      "loss: 0.047722  [ 2640/ 3200]\n",
      "loss: 0.084177  [ 2656/ 3200]\n",
      "loss: 0.010882  [ 2672/ 3200]\n",
      "loss: 0.163047  [ 2688/ 3200]\n",
      "loss: 0.136370  [ 2704/ 3200]\n",
      "loss: 0.021778  [ 2720/ 3200]\n",
      "loss: 0.007147  [ 2736/ 3200]\n",
      "loss: 0.013155  [ 2752/ 3200]\n",
      "loss: 0.070028  [ 2768/ 3200]\n",
      "loss: 0.058808  [ 2784/ 3200]\n",
      "loss: 0.030536  [ 2800/ 3200]\n",
      "loss: 0.027242  [ 2816/ 3200]\n",
      "loss: 0.087746  [ 2832/ 3200]\n",
      "loss: 0.030786  [ 2848/ 3200]\n",
      "loss: 0.043558  [ 2864/ 3200]\n",
      "loss: 0.045699  [ 2880/ 3200]\n",
      "loss: 0.077270  [ 2896/ 3200]\n",
      "loss: 0.021080  [ 2912/ 3200]\n",
      "loss: 0.063516  [ 2928/ 3200]\n",
      "loss: 0.029312  [ 2944/ 3200]\n",
      "loss: 0.041258  [ 2960/ 3200]\n",
      "loss: 0.051241  [ 2976/ 3200]\n",
      "loss: 0.061525  [ 2992/ 3200]\n",
      "loss: 0.036675  [ 3008/ 3200]\n",
      "loss: 0.119086  [ 3024/ 3200]\n",
      "loss: 0.038864  [ 3040/ 3200]\n",
      "loss: 0.062163  [ 3056/ 3200]\n",
      "loss: 0.011067  [ 3072/ 3200]\n",
      "loss: 0.101114  [ 3088/ 3200]\n",
      "loss: 0.022544  [ 3104/ 3200]\n",
      "loss: 0.060793  [ 3120/ 3200]\n",
      "loss: 0.060610  [ 3136/ 3200]\n",
      "loss: 0.031049  [ 3152/ 3200]\n",
      "loss: 0.036741  [ 3168/ 3200]\n",
      "loss: 0.039610  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.049657\n",
      "f1 macro averaged score: 0.764557\n",
      "Accuracy               : 76.6%\n",
      "Confusion matrix       :\n",
      "tensor([[180,   4,   0,  16],\n",
      "        [ 17, 115,  15,  53],\n",
      "        [  0,  25, 150,  25],\n",
      "        [  9,  13,  10, 168]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.8112e-04.\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 0.088807  [    0/ 3200]\n",
      "loss: 0.022374  [   16/ 3200]\n",
      "loss: 0.029370  [   32/ 3200]\n",
      "loss: 0.055620  [   48/ 3200]\n",
      "loss: 0.013055  [   64/ 3200]\n",
      "loss: 0.038673  [   80/ 3200]\n",
      "loss: 0.049903  [   96/ 3200]\n",
      "loss: 0.064951  [  112/ 3200]\n",
      "loss: 0.057538  [  128/ 3200]\n",
      "loss: 0.128246  [  144/ 3200]\n",
      "loss: 0.044130  [  160/ 3200]\n",
      "loss: 0.028783  [  176/ 3200]\n",
      "loss: 0.035490  [  192/ 3200]\n",
      "loss: 0.122008  [  208/ 3200]\n",
      "loss: 0.060087  [  224/ 3200]\n",
      "loss: 0.034222  [  240/ 3200]\n",
      "loss: 0.018722  [  256/ 3200]\n",
      "loss: 0.084845  [  272/ 3200]\n",
      "loss: 0.085226  [  288/ 3200]\n",
      "loss: 0.053326  [  304/ 3200]\n",
      "loss: 0.018157  [  320/ 3200]\n",
      "loss: 0.026250  [  336/ 3200]\n",
      "loss: 0.124959  [  352/ 3200]\n",
      "loss: 0.077137  [  368/ 3200]\n",
      "loss: 0.057415  [  384/ 3200]\n",
      "loss: 0.027777  [  400/ 3200]\n",
      "loss: 0.016903  [  416/ 3200]\n",
      "loss: 0.036037  [  432/ 3200]\n",
      "loss: 0.025281  [  448/ 3200]\n",
      "loss: 0.044530  [  464/ 3200]\n",
      "loss: 0.018455  [  480/ 3200]\n",
      "loss: 0.040090  [  496/ 3200]\n",
      "loss: 0.053089  [  512/ 3200]\n",
      "loss: 0.008134  [  528/ 3200]\n",
      "loss: 0.050848  [  544/ 3200]\n",
      "loss: 0.016743  [  560/ 3200]\n",
      "loss: 0.021716  [  576/ 3200]\n",
      "loss: 0.046974  [  592/ 3200]\n",
      "loss: 0.014867  [  608/ 3200]\n",
      "loss: 0.037159  [  624/ 3200]\n",
      "loss: 0.091669  [  640/ 3200]\n",
      "loss: 0.019626  [  656/ 3200]\n",
      "loss: 0.036659  [  672/ 3200]\n",
      "loss: 0.061192  [  688/ 3200]\n",
      "loss: 0.096528  [  704/ 3200]\n",
      "loss: 0.076201  [  720/ 3200]\n",
      "loss: 0.029299  [  736/ 3200]\n",
      "loss: 0.038349  [  752/ 3200]\n",
      "loss: 0.070501  [  768/ 3200]\n",
      "loss: 0.013628  [  784/ 3200]\n",
      "loss: 0.055523  [  800/ 3200]\n",
      "loss: 0.019488  [  816/ 3200]\n",
      "loss: 0.066349  [  832/ 3200]\n",
      "loss: 0.019275  [  848/ 3200]\n",
      "loss: 0.011201  [  864/ 3200]\n",
      "loss: 0.191061  [  880/ 3200]\n",
      "loss: 0.071511  [  896/ 3200]\n",
      "loss: 0.090125  [  912/ 3200]\n",
      "loss: 0.027210  [  928/ 3200]\n",
      "loss: 0.055182  [  944/ 3200]\n",
      "loss: 0.058997  [  960/ 3200]\n",
      "loss: 0.130190  [  976/ 3200]\n",
      "loss: 0.038497  [  992/ 3200]\n",
      "loss: 0.067472  [ 1008/ 3200]\n",
      "loss: 0.035687  [ 1024/ 3200]\n",
      "loss: 0.035556  [ 1040/ 3200]\n",
      "loss: 0.114161  [ 1056/ 3200]\n",
      "loss: 0.064448  [ 1072/ 3200]\n",
      "loss: 0.010901  [ 1088/ 3200]\n",
      "loss: 0.247201  [ 1104/ 3200]\n",
      "loss: 0.041687  [ 1120/ 3200]\n",
      "loss: 0.024507  [ 1136/ 3200]\n",
      "loss: 0.074255  [ 1152/ 3200]\n",
      "loss: 0.014765  [ 1168/ 3200]\n",
      "loss: 0.025302  [ 1184/ 3200]\n",
      "loss: 0.035713  [ 1200/ 3200]\n",
      "loss: 0.012803  [ 1216/ 3200]\n",
      "loss: 0.035268  [ 1232/ 3200]\n",
      "loss: 0.029166  [ 1248/ 3200]\n",
      "loss: 0.018958  [ 1264/ 3200]\n",
      "loss: 0.055753  [ 1280/ 3200]\n",
      "loss: 0.039720  [ 1296/ 3200]\n",
      "loss: 0.019305  [ 1312/ 3200]\n",
      "loss: 0.013387  [ 1328/ 3200]\n",
      "loss: 0.023064  [ 1344/ 3200]\n",
      "loss: 0.026083  [ 1360/ 3200]\n",
      "loss: 0.048440  [ 1376/ 3200]\n",
      "loss: 0.020923  [ 1392/ 3200]\n",
      "loss: 0.019157  [ 1408/ 3200]\n",
      "loss: 0.127997  [ 1424/ 3200]\n",
      "loss: 0.018076  [ 1440/ 3200]\n",
      "loss: 0.060254  [ 1456/ 3200]\n",
      "loss: 0.032280  [ 1472/ 3200]\n",
      "loss: 0.041140  [ 1488/ 3200]\n",
      "loss: 0.042669  [ 1504/ 3200]\n",
      "loss: 0.027414  [ 1520/ 3200]\n",
      "loss: 0.038620  [ 1536/ 3200]\n",
      "loss: 0.037734  [ 1552/ 3200]\n",
      "loss: 0.027634  [ 1568/ 3200]\n",
      "loss: 0.102343  [ 1584/ 3200]\n",
      "loss: 0.058071  [ 1600/ 3200]\n",
      "loss: 0.039009  [ 1616/ 3200]\n",
      "loss: 0.021940  [ 1632/ 3200]\n",
      "loss: 0.076768  [ 1648/ 3200]\n",
      "loss: 0.043014  [ 1664/ 3200]\n",
      "loss: 0.007630  [ 1680/ 3200]\n",
      "loss: 0.012738  [ 1696/ 3200]\n",
      "loss: 0.061897  [ 1712/ 3200]\n",
      "loss: 0.042345  [ 1728/ 3200]\n",
      "loss: 0.027765  [ 1744/ 3200]\n",
      "loss: 0.034378  [ 1760/ 3200]\n",
      "loss: 0.026997  [ 1776/ 3200]\n",
      "loss: 0.066976  [ 1792/ 3200]\n",
      "loss: 0.025019  [ 1808/ 3200]\n",
      "loss: 0.077350  [ 1824/ 3200]\n",
      "loss: 0.044802  [ 1840/ 3200]\n",
      "loss: 0.042308  [ 1856/ 3200]\n",
      "loss: 0.025041  [ 1872/ 3200]\n",
      "loss: 0.008802  [ 1888/ 3200]\n",
      "loss: 0.044086  [ 1904/ 3200]\n",
      "loss: 0.029605  [ 1920/ 3200]\n",
      "loss: 0.029226  [ 1936/ 3200]\n",
      "loss: 0.029947  [ 1952/ 3200]\n",
      "loss: 0.051855  [ 1968/ 3200]\n",
      "loss: 0.099838  [ 1984/ 3200]\n",
      "loss: 0.091836  [ 2000/ 3200]\n",
      "loss: 0.070872  [ 2016/ 3200]\n",
      "loss: 0.028581  [ 2032/ 3200]\n",
      "loss: 0.025006  [ 2048/ 3200]\n",
      "loss: 0.019498  [ 2064/ 3200]\n",
      "loss: 0.038525  [ 2080/ 3200]\n",
      "loss: 0.049615  [ 2096/ 3200]\n",
      "loss: 0.012414  [ 2112/ 3200]\n",
      "loss: 0.057889  [ 2128/ 3200]\n",
      "loss: 0.059435  [ 2144/ 3200]\n",
      "loss: 0.049339  [ 2160/ 3200]\n",
      "loss: 0.118378  [ 2176/ 3200]\n",
      "loss: 0.058711  [ 2192/ 3200]\n",
      "loss: 0.039902  [ 2208/ 3200]\n",
      "loss: 0.041291  [ 2224/ 3200]\n",
      "loss: 0.022606  [ 2240/ 3200]\n",
      "loss: 0.024873  [ 2256/ 3200]\n",
      "loss: 0.039070  [ 2272/ 3200]\n",
      "loss: 0.022211  [ 2288/ 3200]\n",
      "loss: 0.036127  [ 2304/ 3200]\n",
      "loss: 0.062877  [ 2320/ 3200]\n",
      "loss: 0.020385  [ 2336/ 3200]\n",
      "loss: 0.022522  [ 2352/ 3200]\n",
      "loss: 0.134234  [ 2368/ 3200]\n",
      "loss: 0.058560  [ 2384/ 3200]\n",
      "loss: 0.028842  [ 2400/ 3200]\n",
      "loss: 0.057130  [ 2416/ 3200]\n",
      "loss: 0.036192  [ 2432/ 3200]\n",
      "loss: 0.027674  [ 2448/ 3200]\n",
      "loss: 0.078705  [ 2464/ 3200]\n",
      "loss: 0.032974  [ 2480/ 3200]\n",
      "loss: 0.021796  [ 2496/ 3200]\n",
      "loss: 0.019363  [ 2512/ 3200]\n",
      "loss: 0.050011  [ 2528/ 3200]\n",
      "loss: 0.012560  [ 2544/ 3200]\n",
      "loss: 0.036347  [ 2560/ 3200]\n",
      "loss: 0.045548  [ 2576/ 3200]\n",
      "loss: 0.201018  [ 2592/ 3200]\n",
      "loss: 0.160720  [ 2608/ 3200]\n",
      "loss: 0.052158  [ 2624/ 3200]\n",
      "loss: 0.189743  [ 2640/ 3200]\n",
      "loss: 0.144426  [ 2656/ 3200]\n",
      "loss: 0.024557  [ 2672/ 3200]\n",
      "loss: 0.034918  [ 2688/ 3200]\n",
      "loss: 0.009658  [ 2704/ 3200]\n",
      "loss: 0.011020  [ 2720/ 3200]\n",
      "loss: 0.043385  [ 2736/ 3200]\n",
      "loss: 0.041780  [ 2752/ 3200]\n",
      "loss: 0.045406  [ 2768/ 3200]\n",
      "loss: 0.018010  [ 2784/ 3200]\n",
      "loss: 0.185880  [ 2800/ 3200]\n",
      "loss: 0.038832  [ 2816/ 3200]\n",
      "loss: 0.021442  [ 2832/ 3200]\n",
      "loss: 0.113317  [ 2848/ 3200]\n",
      "loss: 0.059897  [ 2864/ 3200]\n",
      "loss: 0.048329  [ 2880/ 3200]\n",
      "loss: 0.051716  [ 2896/ 3200]\n",
      "loss: 0.036668  [ 2912/ 3200]\n",
      "loss: 0.027026  [ 2928/ 3200]\n",
      "loss: 0.024604  [ 2944/ 3200]\n",
      "loss: 0.077709  [ 2960/ 3200]\n",
      "loss: 0.008568  [ 2976/ 3200]\n",
      "loss: 0.019175  [ 2992/ 3200]\n",
      "loss: 0.078050  [ 3008/ 3200]\n",
      "loss: 0.027579  [ 3024/ 3200]\n",
      "loss: 0.089256  [ 3040/ 3200]\n",
      "loss: 0.057159  [ 3056/ 3200]\n",
      "loss: 0.291687  [ 3072/ 3200]\n",
      "loss: 0.047053  [ 3088/ 3200]\n",
      "loss: 0.095207  [ 3104/ 3200]\n",
      "loss: 0.037793  [ 3120/ 3200]\n",
      "loss: 0.015199  [ 3136/ 3200]\n",
      "loss: 0.017950  [ 3152/ 3200]\n",
      "loss: 0.039615  [ 3168/ 3200]\n",
      "loss: 0.048497  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.046976\n",
      "f1 macro averaged score: 0.785159\n",
      "Accuracy               : 78.6%\n",
      "Confusion matrix       :\n",
      "tensor([[181,   5,   0,  14],\n",
      "        [ 17, 130,  18,  35],\n",
      "        [  0,  28, 156,  16],\n",
      "        [ 10,  17,  11, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.4707e-04.\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.133995  [    0/ 3200]\n",
      "loss: 0.066129  [   16/ 3200]\n",
      "loss: 0.022340  [   32/ 3200]\n",
      "loss: 0.069523  [   48/ 3200]\n",
      "loss: 0.032913  [   64/ 3200]\n",
      "loss: 0.013926  [   80/ 3200]\n",
      "loss: 0.036626  [   96/ 3200]\n",
      "loss: 0.031276  [  112/ 3200]\n",
      "loss: 0.020888  [  128/ 3200]\n",
      "loss: 0.145270  [  144/ 3200]\n",
      "loss: 0.052311  [  160/ 3200]\n",
      "loss: 0.020693  [  176/ 3200]\n",
      "loss: 0.046908  [  192/ 3200]\n",
      "loss: 0.049949  [  208/ 3200]\n",
      "loss: 0.054930  [  224/ 3200]\n",
      "loss: 0.034412  [  240/ 3200]\n",
      "loss: 0.015983  [  256/ 3200]\n",
      "loss: 0.037456  [  272/ 3200]\n",
      "loss: 0.032017  [  288/ 3200]\n",
      "loss: 0.030534  [  304/ 3200]\n",
      "loss: 0.029712  [  320/ 3200]\n",
      "loss: 0.083127  [  336/ 3200]\n",
      "loss: 0.031929  [  352/ 3200]\n",
      "loss: 0.161211  [  368/ 3200]\n",
      "loss: 0.058872  [  384/ 3200]\n",
      "loss: 0.092076  [  400/ 3200]\n",
      "loss: 0.035318  [  416/ 3200]\n",
      "loss: 0.054954  [  432/ 3200]\n",
      "loss: 0.027585  [  448/ 3200]\n",
      "loss: 0.048219  [  464/ 3200]\n",
      "loss: 0.030659  [  480/ 3200]\n",
      "loss: 0.023520  [  496/ 3200]\n",
      "loss: 0.025574  [  512/ 3200]\n",
      "loss: 0.015506  [  528/ 3200]\n",
      "loss: 0.062054  [  544/ 3200]\n",
      "loss: 0.057099  [  560/ 3200]\n",
      "loss: 0.014982  [  576/ 3200]\n",
      "loss: 0.041477  [  592/ 3200]\n",
      "loss: 0.027453  [  608/ 3200]\n",
      "loss: 0.015512  [  624/ 3200]\n",
      "loss: 0.032827  [  640/ 3200]\n",
      "loss: 0.134650  [  656/ 3200]\n",
      "loss: 0.084949  [  672/ 3200]\n",
      "loss: 0.017816  [  688/ 3200]\n",
      "loss: 0.055064  [  704/ 3200]\n",
      "loss: 0.014579  [  720/ 3200]\n",
      "loss: 0.044016  [  736/ 3200]\n",
      "loss: 0.043813  [  752/ 3200]\n",
      "loss: 0.037795  [  768/ 3200]\n",
      "loss: 0.048460  [  784/ 3200]\n",
      "loss: 0.022906  [  800/ 3200]\n",
      "loss: 0.072766  [  816/ 3200]\n",
      "loss: 0.023402  [  832/ 3200]\n",
      "loss: 0.009103  [  848/ 3200]\n",
      "loss: 0.024509  [  864/ 3200]\n",
      "loss: 0.034604  [  880/ 3200]\n",
      "loss: 0.081693  [  896/ 3200]\n",
      "loss: 0.046844  [  912/ 3200]\n",
      "loss: 0.012838  [  928/ 3200]\n",
      "loss: 0.048070  [  944/ 3200]\n",
      "loss: 0.010641  [  960/ 3200]\n",
      "loss: 0.235401  [  976/ 3200]\n",
      "loss: 0.034738  [  992/ 3200]\n",
      "loss: 0.019966  [ 1008/ 3200]\n",
      "loss: 0.010123  [ 1024/ 3200]\n",
      "loss: 0.025452  [ 1040/ 3200]\n",
      "loss: 0.026534  [ 1056/ 3200]\n",
      "loss: 0.021902  [ 1072/ 3200]\n",
      "loss: 0.049793  [ 1088/ 3200]\n",
      "loss: 0.041507  [ 1104/ 3200]\n",
      "loss: 0.011875  [ 1120/ 3200]\n",
      "loss: 0.101847  [ 1136/ 3200]\n",
      "loss: 0.020370  [ 1152/ 3200]\n",
      "loss: 0.033413  [ 1168/ 3200]\n",
      "loss: 0.032519  [ 1184/ 3200]\n",
      "loss: 0.028196  [ 1200/ 3200]\n",
      "loss: 0.035294  [ 1216/ 3200]\n",
      "loss: 0.021569  [ 1232/ 3200]\n",
      "loss: 0.021504  [ 1248/ 3200]\n",
      "loss: 0.026245  [ 1264/ 3200]\n",
      "loss: 0.061791  [ 1280/ 3200]\n",
      "loss: 0.040197  [ 1296/ 3200]\n",
      "loss: 0.025487  [ 1312/ 3200]\n",
      "loss: 0.029673  [ 1328/ 3200]\n",
      "loss: 0.005517  [ 1344/ 3200]\n",
      "loss: 0.038035  [ 1360/ 3200]\n",
      "loss: 0.024440  [ 1376/ 3200]\n",
      "loss: 0.058231  [ 1392/ 3200]\n",
      "loss: 0.274565  [ 1408/ 3200]\n",
      "loss: 0.040223  [ 1424/ 3200]\n",
      "loss: 0.090124  [ 1440/ 3200]\n",
      "loss: 0.075412  [ 1456/ 3200]\n",
      "loss: 0.024780  [ 1472/ 3200]\n",
      "loss: 0.024106  [ 1488/ 3200]\n",
      "loss: 0.156548  [ 1504/ 3200]\n",
      "loss: 0.038517  [ 1520/ 3200]\n",
      "loss: 0.009980  [ 1536/ 3200]\n",
      "loss: 0.069866  [ 1552/ 3200]\n",
      "loss: 0.028254  [ 1568/ 3200]\n",
      "loss: 0.027479  [ 1584/ 3200]\n",
      "loss: 0.034298  [ 1600/ 3200]\n",
      "loss: 0.075359  [ 1616/ 3200]\n",
      "loss: 0.013018  [ 1632/ 3200]\n",
      "loss: 0.064142  [ 1648/ 3200]\n",
      "loss: 0.043724  [ 1664/ 3200]\n",
      "loss: 0.091567  [ 1680/ 3200]\n",
      "loss: 0.019634  [ 1696/ 3200]\n",
      "loss: 0.042581  [ 1712/ 3200]\n",
      "loss: 0.024806  [ 1728/ 3200]\n",
      "loss: 0.068673  [ 1744/ 3200]\n",
      "loss: 0.077297  [ 1760/ 3200]\n",
      "loss: 0.035707  [ 1776/ 3200]\n",
      "loss: 0.033496  [ 1792/ 3200]\n",
      "loss: 0.041626  [ 1808/ 3200]\n",
      "loss: 0.026644  [ 1824/ 3200]\n",
      "loss: 0.045856  [ 1840/ 3200]\n",
      "loss: 0.038108  [ 1856/ 3200]\n",
      "loss: 0.032296  [ 1872/ 3200]\n",
      "loss: 0.013322  [ 1888/ 3200]\n",
      "loss: 0.053771  [ 1904/ 3200]\n",
      "loss: 0.015024  [ 1920/ 3200]\n",
      "loss: 0.023716  [ 1936/ 3200]\n",
      "loss: 0.031311  [ 1952/ 3200]\n",
      "loss: 0.008892  [ 1968/ 3200]\n",
      "loss: 0.086258  [ 1984/ 3200]\n",
      "loss: 0.088484  [ 2000/ 3200]\n",
      "loss: 0.018204  [ 2016/ 3200]\n",
      "loss: 0.011412  [ 2032/ 3200]\n",
      "loss: 0.112653  [ 2048/ 3200]\n",
      "loss: 0.022837  [ 2064/ 3200]\n",
      "loss: 0.011543  [ 2080/ 3200]\n",
      "loss: 0.093862  [ 2096/ 3200]\n",
      "loss: 0.114357  [ 2112/ 3200]\n",
      "loss: 0.031252  [ 2128/ 3200]\n",
      "loss: 0.084416  [ 2144/ 3200]\n",
      "loss: 0.031728  [ 2160/ 3200]\n",
      "loss: 0.024779  [ 2176/ 3200]\n",
      "loss: 0.022993  [ 2192/ 3200]\n",
      "loss: 0.103344  [ 2208/ 3200]\n",
      "loss: 0.053929  [ 2224/ 3200]\n",
      "loss: 0.057730  [ 2240/ 3200]\n",
      "loss: 0.042090  [ 2256/ 3200]\n",
      "loss: 0.054969  [ 2272/ 3200]\n",
      "loss: 0.043487  [ 2288/ 3200]\n",
      "loss: 0.044347  [ 2304/ 3200]\n",
      "loss: 0.011358  [ 2320/ 3200]\n",
      "loss: 0.048461  [ 2336/ 3200]\n",
      "loss: 0.020264  [ 2352/ 3200]\n",
      "loss: 0.036151  [ 2368/ 3200]\n",
      "loss: 0.026297  [ 2384/ 3200]\n",
      "loss: 0.041834  [ 2400/ 3200]\n",
      "loss: 0.008617  [ 2416/ 3200]\n",
      "loss: 0.012477  [ 2432/ 3200]\n",
      "loss: 0.051906  [ 2448/ 3200]\n",
      "loss: 0.036695  [ 2464/ 3200]\n",
      "loss: 0.055502  [ 2480/ 3200]\n",
      "loss: 0.073341  [ 2496/ 3200]\n",
      "loss: 0.006662  [ 2512/ 3200]\n",
      "loss: 0.069523  [ 2528/ 3200]\n",
      "loss: 0.015655  [ 2544/ 3200]\n",
      "loss: 0.030241  [ 2560/ 3200]\n",
      "loss: 0.050722  [ 2576/ 3200]\n",
      "loss: 0.016353  [ 2592/ 3200]\n",
      "loss: 0.012702  [ 2608/ 3200]\n",
      "loss: 0.021939  [ 2624/ 3200]\n",
      "loss: 0.007767  [ 2640/ 3200]\n",
      "loss: 0.093030  [ 2656/ 3200]\n",
      "loss: 0.020875  [ 2672/ 3200]\n",
      "loss: 0.014716  [ 2688/ 3200]\n",
      "loss: 0.020814  [ 2704/ 3200]\n",
      "loss: 0.042847  [ 2720/ 3200]\n",
      "loss: 0.050442  [ 2736/ 3200]\n",
      "loss: 0.052959  [ 2752/ 3200]\n",
      "loss: 0.061517  [ 2768/ 3200]\n",
      "loss: 0.008722  [ 2784/ 3200]\n",
      "loss: 0.089147  [ 2800/ 3200]\n",
      "loss: 0.018098  [ 2816/ 3200]\n",
      "loss: 0.068286  [ 2832/ 3200]\n",
      "loss: 0.067073  [ 2848/ 3200]\n",
      "loss: 0.019629  [ 2864/ 3200]\n",
      "loss: 0.026196  [ 2880/ 3200]\n",
      "loss: 0.014058  [ 2896/ 3200]\n",
      "loss: 0.039247  [ 2912/ 3200]\n",
      "loss: 0.041099  [ 2928/ 3200]\n",
      "loss: 0.007419  [ 2944/ 3200]\n",
      "loss: 0.045797  [ 2960/ 3200]\n",
      "loss: 0.061097  [ 2976/ 3200]\n",
      "loss: 0.068788  [ 2992/ 3200]\n",
      "loss: 0.036388  [ 3008/ 3200]\n",
      "loss: 0.037637  [ 3024/ 3200]\n",
      "loss: 0.054918  [ 3040/ 3200]\n",
      "loss: 0.021310  [ 3056/ 3200]\n",
      "loss: 0.027061  [ 3072/ 3200]\n",
      "loss: 0.058124  [ 3088/ 3200]\n",
      "loss: 0.017587  [ 3104/ 3200]\n",
      "loss: 0.012802  [ 3120/ 3200]\n",
      "loss: 0.026246  [ 3136/ 3200]\n",
      "loss: 0.077837  [ 3152/ 3200]\n",
      "loss: 0.041668  [ 3168/ 3200]\n",
      "loss: 0.136851  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.050864\n",
      "f1 macro averaged score: 0.766464\n",
      "Accuracy               : 76.8%\n",
      "Confusion matrix       :\n",
      "tensor([[178,   6,   0,  16],\n",
      "        [ 17, 120,  15,  48],\n",
      "        [  0,  27, 150,  23],\n",
      "        [ 10,  14,  10, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.1471e-04.\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.018051  [    0/ 3200]\n",
      "loss: 0.055896  [   16/ 3200]\n",
      "loss: 0.033394  [   32/ 3200]\n",
      "loss: 0.024447  [   48/ 3200]\n",
      "loss: 0.077296  [   64/ 3200]\n",
      "loss: 0.007431  [   80/ 3200]\n",
      "loss: 0.034159  [   96/ 3200]\n",
      "loss: 0.016844  [  112/ 3200]\n",
      "loss: 0.047384  [  128/ 3200]\n",
      "loss: 0.022904  [  144/ 3200]\n",
      "loss: 0.020363  [  160/ 3200]\n",
      "loss: 0.014409  [  176/ 3200]\n",
      "loss: 0.020647  [  192/ 3200]\n",
      "loss: 0.061419  [  208/ 3200]\n",
      "loss: 0.022414  [  224/ 3200]\n",
      "loss: 0.069147  [  240/ 3200]\n",
      "loss: 0.014082  [  256/ 3200]\n",
      "loss: 0.077666  [  272/ 3200]\n",
      "loss: 0.042718  [  288/ 3200]\n",
      "loss: 0.021614  [  304/ 3200]\n",
      "loss: 0.031936  [  320/ 3200]\n",
      "loss: 0.030625  [  336/ 3200]\n",
      "loss: 0.020959  [  352/ 3200]\n",
      "loss: 0.094458  [  368/ 3200]\n",
      "loss: 0.013722  [  384/ 3200]\n",
      "loss: 0.042194  [  400/ 3200]\n",
      "loss: 0.043367  [  416/ 3200]\n",
      "loss: 0.028630  [  432/ 3200]\n",
      "loss: 0.026190  [  448/ 3200]\n",
      "loss: 0.025764  [  464/ 3200]\n",
      "loss: 0.044843  [  480/ 3200]\n",
      "loss: 0.023500  [  496/ 3200]\n",
      "loss: 0.015200  [  512/ 3200]\n",
      "loss: 0.042666  [  528/ 3200]\n",
      "loss: 0.021895  [  544/ 3200]\n",
      "loss: 0.017922  [  560/ 3200]\n",
      "loss: 0.034804  [  576/ 3200]\n",
      "loss: 0.040921  [  592/ 3200]\n",
      "loss: 0.203964  [  608/ 3200]\n",
      "loss: 0.087130  [  624/ 3200]\n",
      "loss: 0.014082  [  640/ 3200]\n",
      "loss: 0.012814  [  656/ 3200]\n",
      "loss: 0.029200  [  672/ 3200]\n",
      "loss: 0.056553  [  688/ 3200]\n",
      "loss: 0.037042  [  704/ 3200]\n",
      "loss: 0.016873  [  720/ 3200]\n",
      "loss: 0.072181  [  736/ 3200]\n",
      "loss: 0.014471  [  752/ 3200]\n",
      "loss: 0.040793  [  768/ 3200]\n",
      "loss: 0.049487  [  784/ 3200]\n",
      "loss: 0.010108  [  800/ 3200]\n",
      "loss: 0.039288  [  816/ 3200]\n",
      "loss: 0.047889  [  832/ 3200]\n",
      "loss: 0.024835  [  848/ 3200]\n",
      "loss: 0.056847  [  864/ 3200]\n",
      "loss: 0.021820  [  880/ 3200]\n",
      "loss: 0.020536  [  896/ 3200]\n",
      "loss: 0.119195  [  912/ 3200]\n",
      "loss: 0.047818  [  928/ 3200]\n",
      "loss: 0.016619  [  944/ 3200]\n",
      "loss: 0.023729  [  960/ 3200]\n",
      "loss: 0.053447  [  976/ 3200]\n",
      "loss: 0.131354  [  992/ 3200]\n",
      "loss: 0.041925  [ 1008/ 3200]\n",
      "loss: 0.060361  [ 1024/ 3200]\n",
      "loss: 0.036155  [ 1040/ 3200]\n",
      "loss: 0.026379  [ 1056/ 3200]\n",
      "loss: 0.019421  [ 1072/ 3200]\n",
      "loss: 0.046900  [ 1088/ 3200]\n",
      "loss: 0.043082  [ 1104/ 3200]\n",
      "loss: 0.045543  [ 1120/ 3200]\n",
      "loss: 0.039156  [ 1136/ 3200]\n",
      "loss: 0.019821  [ 1152/ 3200]\n",
      "loss: 0.021908  [ 1168/ 3200]\n",
      "loss: 0.025225  [ 1184/ 3200]\n",
      "loss: 0.021732  [ 1200/ 3200]\n",
      "loss: 0.177884  [ 1216/ 3200]\n",
      "loss: 0.040061  [ 1232/ 3200]\n",
      "loss: 0.038452  [ 1248/ 3200]\n",
      "loss: 0.010758  [ 1264/ 3200]\n",
      "loss: 0.028663  [ 1280/ 3200]\n",
      "loss: 0.102418  [ 1296/ 3200]\n",
      "loss: 0.006453  [ 1312/ 3200]\n",
      "loss: 0.027563  [ 1328/ 3200]\n",
      "loss: 0.013318  [ 1344/ 3200]\n",
      "loss: 0.015443  [ 1360/ 3200]\n",
      "loss: 0.025093  [ 1376/ 3200]\n",
      "loss: 0.046109  [ 1392/ 3200]\n",
      "loss: 0.041292  [ 1408/ 3200]\n",
      "loss: 0.004940  [ 1424/ 3200]\n",
      "loss: 0.009884  [ 1440/ 3200]\n",
      "loss: 0.053287  [ 1456/ 3200]\n",
      "loss: 0.017956  [ 1472/ 3200]\n",
      "loss: 0.023946  [ 1488/ 3200]\n",
      "loss: 0.056305  [ 1504/ 3200]\n",
      "loss: 0.015745  [ 1520/ 3200]\n",
      "loss: 0.022072  [ 1536/ 3200]\n",
      "loss: 0.018401  [ 1552/ 3200]\n",
      "loss: 0.026905  [ 1568/ 3200]\n",
      "loss: 0.063624  [ 1584/ 3200]\n",
      "loss: 0.261832  [ 1600/ 3200]\n",
      "loss: 0.029746  [ 1616/ 3200]\n",
      "loss: 0.054505  [ 1632/ 3200]\n",
      "loss: 0.011215  [ 1648/ 3200]\n",
      "loss: 0.026834  [ 1664/ 3200]\n",
      "loss: 0.017639  [ 1680/ 3200]\n",
      "loss: 0.042255  [ 1696/ 3200]\n",
      "loss: 0.009652  [ 1712/ 3200]\n",
      "loss: 0.022080  [ 1728/ 3200]\n",
      "loss: 0.010664  [ 1744/ 3200]\n",
      "loss: 0.022656  [ 1760/ 3200]\n",
      "loss: 0.020720  [ 1776/ 3200]\n",
      "loss: 0.046746  [ 1792/ 3200]\n",
      "loss: 0.063266  [ 1808/ 3200]\n",
      "loss: 0.008537  [ 1824/ 3200]\n",
      "loss: 0.015021  [ 1840/ 3200]\n",
      "loss: 0.042991  [ 1856/ 3200]\n",
      "loss: 0.126203  [ 1872/ 3200]\n",
      "loss: 0.023070  [ 1888/ 3200]\n",
      "loss: 0.030292  [ 1904/ 3200]\n",
      "loss: 0.010154  [ 1920/ 3200]\n",
      "loss: 0.037457  [ 1936/ 3200]\n",
      "loss: 0.109500  [ 1952/ 3200]\n",
      "loss: 0.155527  [ 1968/ 3200]\n",
      "loss: 0.016867  [ 1984/ 3200]\n",
      "loss: 0.056471  [ 2000/ 3200]\n",
      "loss: 0.097792  [ 2016/ 3200]\n",
      "loss: 0.035132  [ 2032/ 3200]\n",
      "loss: 0.028174  [ 2048/ 3200]\n",
      "loss: 0.044462  [ 2064/ 3200]\n",
      "loss: 0.004562  [ 2080/ 3200]\n",
      "loss: 0.024439  [ 2096/ 3200]\n",
      "loss: 0.076909  [ 2112/ 3200]\n",
      "loss: 0.015886  [ 2128/ 3200]\n",
      "loss: 0.123379  [ 2144/ 3200]\n",
      "loss: 0.009619  [ 2160/ 3200]\n",
      "loss: 0.042909  [ 2176/ 3200]\n",
      "loss: 0.031906  [ 2192/ 3200]\n",
      "loss: 0.017004  [ 2208/ 3200]\n",
      "loss: 0.044956  [ 2224/ 3200]\n",
      "loss: 0.033719  [ 2240/ 3200]\n",
      "loss: 0.013812  [ 2256/ 3200]\n",
      "loss: 0.016652  [ 2272/ 3200]\n",
      "loss: 0.049773  [ 2288/ 3200]\n",
      "loss: 0.023178  [ 2304/ 3200]\n",
      "loss: 0.014832  [ 2320/ 3200]\n",
      "loss: 0.021913  [ 2336/ 3200]\n",
      "loss: 0.011447  [ 2352/ 3200]\n",
      "loss: 0.022943  [ 2368/ 3200]\n",
      "loss: 0.063525  [ 2384/ 3200]\n",
      "loss: 0.098747  [ 2400/ 3200]\n",
      "loss: 0.029307  [ 2416/ 3200]\n",
      "loss: 0.029280  [ 2432/ 3200]\n",
      "loss: 0.018680  [ 2448/ 3200]\n",
      "loss: 0.029969  [ 2464/ 3200]\n",
      "loss: 0.012974  [ 2480/ 3200]\n",
      "loss: 0.019721  [ 2496/ 3200]\n",
      "loss: 0.033624  [ 2512/ 3200]\n",
      "loss: 0.055304  [ 2528/ 3200]\n",
      "loss: 0.068609  [ 2544/ 3200]\n",
      "loss: 0.039142  [ 2560/ 3200]\n",
      "loss: 0.036419  [ 2576/ 3200]\n",
      "loss: 0.024403  [ 2592/ 3200]\n",
      "loss: 0.032834  [ 2608/ 3200]\n",
      "loss: 0.035701  [ 2624/ 3200]\n",
      "loss: 0.029211  [ 2640/ 3200]\n",
      "loss: 0.043443  [ 2656/ 3200]\n",
      "loss: 0.018752  [ 2672/ 3200]\n",
      "loss: 0.013103  [ 2688/ 3200]\n",
      "loss: 0.021234  [ 2704/ 3200]\n",
      "loss: 0.020704  [ 2720/ 3200]\n",
      "loss: 0.029317  [ 2736/ 3200]\n",
      "loss: 0.028065  [ 2752/ 3200]\n",
      "loss: 0.039580  [ 2768/ 3200]\n",
      "loss: 0.009178  [ 2784/ 3200]\n",
      "loss: 0.024502  [ 2800/ 3200]\n",
      "loss: 0.018392  [ 2816/ 3200]\n",
      "loss: 0.023804  [ 2832/ 3200]\n",
      "loss: 0.006573  [ 2848/ 3200]\n",
      "loss: 0.032551  [ 2864/ 3200]\n",
      "loss: 0.028564  [ 2880/ 3200]\n",
      "loss: 0.013606  [ 2896/ 3200]\n",
      "loss: 0.047379  [ 2912/ 3200]\n",
      "loss: 0.066498  [ 2928/ 3200]\n",
      "loss: 0.165807  [ 2944/ 3200]\n",
      "loss: 0.017695  [ 2960/ 3200]\n",
      "loss: 0.035185  [ 2976/ 3200]\n",
      "loss: 0.029866  [ 2992/ 3200]\n",
      "loss: 0.021401  [ 3008/ 3200]\n",
      "loss: 0.078006  [ 3024/ 3200]\n",
      "loss: 0.008609  [ 3040/ 3200]\n",
      "loss: 0.023075  [ 3056/ 3200]\n",
      "loss: 0.037931  [ 3072/ 3200]\n",
      "loss: 0.012011  [ 3088/ 3200]\n",
      "loss: 0.019089  [ 3104/ 3200]\n",
      "loss: 0.043194  [ 3120/ 3200]\n",
      "loss: 0.044452  [ 3136/ 3200]\n",
      "loss: 0.035104  [ 3152/ 3200]\n",
      "loss: 0.028429  [ 3168/ 3200]\n",
      "loss: 0.076825  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.049814\n",
      "f1 macro averaged score: 0.772239\n",
      "Accuracy               : 77.2%\n",
      "Confusion matrix       :\n",
      "tensor([[176,   8,   0,  16],\n",
      "        [ 15, 127,  18,  40],\n",
      "        [  0,  25, 153,  22],\n",
      "        [ 10,  17,  11, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.8398e-04.\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.023781  [    0/ 3200]\n",
      "loss: 0.024398  [   16/ 3200]\n",
      "loss: 0.023354  [   32/ 3200]\n",
      "loss: 0.043760  [   48/ 3200]\n",
      "loss: 0.019639  [   64/ 3200]\n",
      "loss: 0.035286  [   80/ 3200]\n",
      "loss: 0.021931  [   96/ 3200]\n",
      "loss: 0.012982  [  112/ 3200]\n",
      "loss: 0.033022  [  128/ 3200]\n",
      "loss: 0.023460  [  144/ 3200]\n",
      "loss: 0.072198  [  160/ 3200]\n",
      "loss: 0.008320  [  176/ 3200]\n",
      "loss: 0.017327  [  192/ 3200]\n",
      "loss: 0.056840  [  208/ 3200]\n",
      "loss: 0.040943  [  224/ 3200]\n",
      "loss: 0.024436  [  240/ 3200]\n",
      "loss: 0.031036  [  256/ 3200]\n",
      "loss: 0.050352  [  272/ 3200]\n",
      "loss: 0.016840  [  288/ 3200]\n",
      "loss: 0.008141  [  304/ 3200]\n",
      "loss: 0.042942  [  320/ 3200]\n",
      "loss: 0.062397  [  336/ 3200]\n",
      "loss: 0.054959  [  352/ 3200]\n",
      "loss: 0.055296  [  368/ 3200]\n",
      "loss: 0.061730  [  384/ 3200]\n",
      "loss: 0.129244  [  400/ 3200]\n",
      "loss: 0.033067  [  416/ 3200]\n",
      "loss: 0.009662  [  432/ 3200]\n",
      "loss: 0.015261  [  448/ 3200]\n",
      "loss: 0.029968  [  464/ 3200]\n",
      "loss: 0.017389  [  480/ 3200]\n",
      "loss: 0.069878  [  496/ 3200]\n",
      "loss: 0.029991  [  512/ 3200]\n",
      "loss: 0.026168  [  528/ 3200]\n",
      "loss: 0.028908  [  544/ 3200]\n",
      "loss: 0.014989  [  560/ 3200]\n",
      "loss: 0.091272  [  576/ 3200]\n",
      "loss: 0.050155  [  592/ 3200]\n",
      "loss: 0.027066  [  608/ 3200]\n",
      "loss: 0.158044  [  624/ 3200]\n",
      "loss: 0.018117  [  640/ 3200]\n",
      "loss: 0.039959  [  656/ 3200]\n",
      "loss: 0.054857  [  672/ 3200]\n",
      "loss: 0.006257  [  688/ 3200]\n",
      "loss: 0.035338  [  704/ 3200]\n",
      "loss: 0.020747  [  720/ 3200]\n",
      "loss: 0.013371  [  736/ 3200]\n",
      "loss: 0.019468  [  752/ 3200]\n",
      "loss: 0.102713  [  768/ 3200]\n",
      "loss: 0.092749  [  784/ 3200]\n",
      "loss: 0.023573  [  800/ 3200]\n",
      "loss: 0.030657  [  816/ 3200]\n",
      "loss: 0.022004  [  832/ 3200]\n",
      "loss: 0.008434  [  848/ 3200]\n",
      "loss: 0.033404  [  864/ 3200]\n",
      "loss: 0.013150  [  880/ 3200]\n",
      "loss: 0.026378  [  896/ 3200]\n",
      "loss: 0.014355  [  912/ 3200]\n",
      "loss: 0.062573  [  928/ 3200]\n",
      "loss: 0.027950  [  944/ 3200]\n",
      "loss: 0.091763  [  960/ 3200]\n",
      "loss: 0.018708  [  976/ 3200]\n",
      "loss: 0.037481  [  992/ 3200]\n",
      "loss: 0.016956  [ 1008/ 3200]\n",
      "loss: 0.059960  [ 1024/ 3200]\n",
      "loss: 0.063932  [ 1040/ 3200]\n",
      "loss: 0.045033  [ 1056/ 3200]\n",
      "loss: 0.027705  [ 1072/ 3200]\n",
      "loss: 0.093894  [ 1088/ 3200]\n",
      "loss: 0.025139  [ 1104/ 3200]\n",
      "loss: 0.015042  [ 1120/ 3200]\n",
      "loss: 0.044520  [ 1136/ 3200]\n",
      "loss: 0.008334  [ 1152/ 3200]\n",
      "loss: 0.019519  [ 1168/ 3200]\n",
      "loss: 0.014813  [ 1184/ 3200]\n",
      "loss: 0.025616  [ 1200/ 3200]\n",
      "loss: 0.028580  [ 1216/ 3200]\n",
      "loss: 0.018448  [ 1232/ 3200]\n",
      "loss: 0.016641  [ 1248/ 3200]\n",
      "loss: 0.144678  [ 1264/ 3200]\n",
      "loss: 0.055219  [ 1280/ 3200]\n",
      "loss: 0.035232  [ 1296/ 3200]\n",
      "loss: 0.011121  [ 1312/ 3200]\n",
      "loss: 0.014982  [ 1328/ 3200]\n",
      "loss: 0.020509  [ 1344/ 3200]\n",
      "loss: 0.011383  [ 1360/ 3200]\n",
      "loss: 0.006865  [ 1376/ 3200]\n",
      "loss: 0.019323  [ 1392/ 3200]\n",
      "loss: 0.036133  [ 1408/ 3200]\n",
      "loss: 0.039582  [ 1424/ 3200]\n",
      "loss: 0.030719  [ 1440/ 3200]\n",
      "loss: 0.015404  [ 1456/ 3200]\n",
      "loss: 0.035205  [ 1472/ 3200]\n",
      "loss: 0.013378  [ 1488/ 3200]\n",
      "loss: 0.014806  [ 1504/ 3200]\n",
      "loss: 0.026400  [ 1520/ 3200]\n",
      "loss: 0.022403  [ 1536/ 3200]\n",
      "loss: 0.025599  [ 1552/ 3200]\n",
      "loss: 0.024407  [ 1568/ 3200]\n",
      "loss: 0.010112  [ 1584/ 3200]\n",
      "loss: 0.007141  [ 1600/ 3200]\n",
      "loss: 0.028509  [ 1616/ 3200]\n",
      "loss: 0.009971  [ 1632/ 3200]\n",
      "loss: 0.008002  [ 1648/ 3200]\n",
      "loss: 0.095481  [ 1664/ 3200]\n",
      "loss: 0.023455  [ 1680/ 3200]\n",
      "loss: 0.010158  [ 1696/ 3200]\n",
      "loss: 0.028650  [ 1712/ 3200]\n",
      "loss: 0.039389  [ 1728/ 3200]\n",
      "loss: 0.101688  [ 1744/ 3200]\n",
      "loss: 0.025048  [ 1760/ 3200]\n",
      "loss: 0.019204  [ 1776/ 3200]\n",
      "loss: 0.018794  [ 1792/ 3200]\n",
      "loss: 0.046818  [ 1808/ 3200]\n",
      "loss: 0.046711  [ 1824/ 3200]\n",
      "loss: 0.029257  [ 1840/ 3200]\n",
      "loss: 0.027216  [ 1856/ 3200]\n",
      "loss: 0.055215  [ 1872/ 3200]\n",
      "loss: 0.022431  [ 1888/ 3200]\n",
      "loss: 0.033271  [ 1904/ 3200]\n",
      "loss: 0.092412  [ 1920/ 3200]\n",
      "loss: 0.013192  [ 1936/ 3200]\n",
      "loss: 0.028165  [ 1952/ 3200]\n",
      "loss: 0.032271  [ 1968/ 3200]\n",
      "loss: 0.043155  [ 1984/ 3200]\n",
      "loss: 0.018890  [ 2000/ 3200]\n",
      "loss: 0.055885  [ 2016/ 3200]\n",
      "loss: 0.062398  [ 2032/ 3200]\n",
      "loss: 0.019419  [ 2048/ 3200]\n",
      "loss: 0.040343  [ 2064/ 3200]\n",
      "loss: 0.016923  [ 2080/ 3200]\n",
      "loss: 0.078362  [ 2096/ 3200]\n",
      "loss: 0.048151  [ 2112/ 3200]\n",
      "loss: 0.011278  [ 2128/ 3200]\n",
      "loss: 0.015684  [ 2144/ 3200]\n",
      "loss: 0.032123  [ 2160/ 3200]\n",
      "loss: 0.034233  [ 2176/ 3200]\n",
      "loss: 0.036271  [ 2192/ 3200]\n",
      "loss: 0.014345  [ 2208/ 3200]\n",
      "loss: 0.015427  [ 2224/ 3200]\n",
      "loss: 0.015129  [ 2240/ 3200]\n",
      "loss: 0.007324  [ 2256/ 3200]\n",
      "loss: 0.016284  [ 2272/ 3200]\n",
      "loss: 0.018131  [ 2288/ 3200]\n",
      "loss: 0.016582  [ 2304/ 3200]\n",
      "loss: 0.022123  [ 2320/ 3200]\n",
      "loss: 0.027729  [ 2336/ 3200]\n",
      "loss: 0.017956  [ 2352/ 3200]\n",
      "loss: 0.035573  [ 2368/ 3200]\n",
      "loss: 0.045682  [ 2384/ 3200]\n",
      "loss: 0.031413  [ 2400/ 3200]\n",
      "loss: 0.073939  [ 2416/ 3200]\n",
      "loss: 0.053498  [ 2432/ 3200]\n",
      "loss: 0.010375  [ 2448/ 3200]\n",
      "loss: 0.021022  [ 2464/ 3200]\n",
      "loss: 0.035870  [ 2480/ 3200]\n",
      "loss: 0.039827  [ 2496/ 3200]\n",
      "loss: 0.028843  [ 2512/ 3200]\n",
      "loss: 0.039998  [ 2528/ 3200]\n",
      "loss: 0.206185  [ 2544/ 3200]\n",
      "loss: 0.015137  [ 2560/ 3200]\n",
      "loss: 0.004479  [ 2576/ 3200]\n",
      "loss: 0.037670  [ 2592/ 3200]\n",
      "loss: 0.053031  [ 2608/ 3200]\n",
      "loss: 0.003886  [ 2624/ 3200]\n",
      "loss: 0.081062  [ 2640/ 3200]\n",
      "loss: 0.031667  [ 2656/ 3200]\n",
      "loss: 0.039876  [ 2672/ 3200]\n",
      "loss: 0.044377  [ 2688/ 3200]\n",
      "loss: 0.015825  [ 2704/ 3200]\n",
      "loss: 0.024561  [ 2720/ 3200]\n",
      "loss: 0.024487  [ 2736/ 3200]\n",
      "loss: 0.014267  [ 2752/ 3200]\n",
      "loss: 0.010330  [ 2768/ 3200]\n",
      "loss: 0.049204  [ 2784/ 3200]\n",
      "loss: 0.030093  [ 2800/ 3200]\n",
      "loss: 0.014564  [ 2816/ 3200]\n",
      "loss: 0.006585  [ 2832/ 3200]\n",
      "loss: 0.030509  [ 2848/ 3200]\n",
      "loss: 0.013685  [ 2864/ 3200]\n",
      "loss: 0.008344  [ 2880/ 3200]\n",
      "loss: 0.051720  [ 2896/ 3200]\n",
      "loss: 0.044555  [ 2912/ 3200]\n",
      "loss: 0.023855  [ 2928/ 3200]\n",
      "loss: 0.016929  [ 2944/ 3200]\n",
      "loss: 0.020375  [ 2960/ 3200]\n",
      "loss: 0.037635  [ 2976/ 3200]\n",
      "loss: 0.023906  [ 2992/ 3200]\n",
      "loss: 0.290931  [ 3008/ 3200]\n",
      "loss: 0.069167  [ 3024/ 3200]\n",
      "loss: 0.027947  [ 3040/ 3200]\n",
      "loss: 0.013279  [ 3056/ 3200]\n",
      "loss: 0.016914  [ 3072/ 3200]\n",
      "loss: 0.013362  [ 3088/ 3200]\n",
      "loss: 0.012961  [ 3104/ 3200]\n",
      "loss: 0.010617  [ 3120/ 3200]\n",
      "loss: 0.015953  [ 3136/ 3200]\n",
      "loss: 0.030502  [ 3152/ 3200]\n",
      "loss: 0.059277  [ 3168/ 3200]\n",
      "loss: 0.023096  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.050033\n",
      "f1 macro averaged score: 0.780347\n",
      "Accuracy               : 78.1%\n",
      "Confusion matrix       :\n",
      "tensor([[180,   6,   0,  14],\n",
      "        [ 16, 128,  18,  38],\n",
      "        [  0,  25, 155,  20],\n",
      "        [ 10,  17,  11, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.5478e-04.\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.021022  [    0/ 3200]\n",
      "loss: 0.031764  [   16/ 3200]\n",
      "loss: 0.026335  [   32/ 3200]\n",
      "loss: 0.019858  [   48/ 3200]\n",
      "loss: 0.105995  [   64/ 3200]\n",
      "loss: 0.076134  [   80/ 3200]\n",
      "loss: 0.058037  [   96/ 3200]\n",
      "loss: 0.054937  [  112/ 3200]\n",
      "loss: 0.013823  [  128/ 3200]\n",
      "loss: 0.032740  [  144/ 3200]\n",
      "loss: 0.047359  [  160/ 3200]\n",
      "loss: 0.025357  [  176/ 3200]\n",
      "loss: 0.011958  [  192/ 3200]\n",
      "loss: 0.015054  [  208/ 3200]\n",
      "loss: 0.026233  [  224/ 3200]\n",
      "loss: 0.018619  [  240/ 3200]\n",
      "loss: 0.019971  [  256/ 3200]\n",
      "loss: 0.014924  [  272/ 3200]\n",
      "loss: 0.026705  [  288/ 3200]\n",
      "loss: 0.031394  [  304/ 3200]\n",
      "loss: 0.027655  [  320/ 3200]\n",
      "loss: 0.013510  [  336/ 3200]\n",
      "loss: 0.064734  [  352/ 3200]\n",
      "loss: 0.061260  [  368/ 3200]\n",
      "loss: 0.049413  [  384/ 3200]\n",
      "loss: 0.021681  [  400/ 3200]\n",
      "loss: 0.019130  [  416/ 3200]\n",
      "loss: 0.031147  [  432/ 3200]\n",
      "loss: 0.025816  [  448/ 3200]\n",
      "loss: 0.022670  [  464/ 3200]\n",
      "loss: 0.025590  [  480/ 3200]\n",
      "loss: 0.039260  [  496/ 3200]\n",
      "loss: 0.022114  [  512/ 3200]\n",
      "loss: 0.019788  [  528/ 3200]\n",
      "loss: 0.034568  [  544/ 3200]\n",
      "loss: 0.011154  [  560/ 3200]\n",
      "loss: 0.043700  [  576/ 3200]\n",
      "loss: 0.018197  [  592/ 3200]\n",
      "loss: 0.025572  [  608/ 3200]\n",
      "loss: 0.010207  [  624/ 3200]\n",
      "loss: 0.012819  [  640/ 3200]\n",
      "loss: 0.044695  [  656/ 3200]\n",
      "loss: 0.010104  [  672/ 3200]\n",
      "loss: 0.040332  [  688/ 3200]\n",
      "loss: 0.021933  [  704/ 3200]\n",
      "loss: 0.015026  [  720/ 3200]\n",
      "loss: 0.061104  [  736/ 3200]\n",
      "loss: 0.019140  [  752/ 3200]\n",
      "loss: 0.005304  [  768/ 3200]\n",
      "loss: 0.015629  [  784/ 3200]\n",
      "loss: 0.027307  [  800/ 3200]\n",
      "loss: 0.013389  [  816/ 3200]\n",
      "loss: 0.047496  [  832/ 3200]\n",
      "loss: 0.070328  [  848/ 3200]\n",
      "loss: 0.024127  [  864/ 3200]\n",
      "loss: 0.180360  [  880/ 3200]\n",
      "loss: 0.039601  [  896/ 3200]\n",
      "loss: 0.017111  [  912/ 3200]\n",
      "loss: 0.016966  [  928/ 3200]\n",
      "loss: 0.089358  [  944/ 3200]\n",
      "loss: 0.008472  [  960/ 3200]\n",
      "loss: 0.028397  [  976/ 3200]\n",
      "loss: 0.021934  [  992/ 3200]\n",
      "loss: 0.029330  [ 1008/ 3200]\n",
      "loss: 0.012244  [ 1024/ 3200]\n",
      "loss: 0.030528  [ 1040/ 3200]\n",
      "loss: 0.053860  [ 1056/ 3200]\n",
      "loss: 0.024142  [ 1072/ 3200]\n",
      "loss: 0.053680  [ 1088/ 3200]\n",
      "loss: 0.020127  [ 1104/ 3200]\n",
      "loss: 0.021202  [ 1120/ 3200]\n",
      "loss: 0.051007  [ 1136/ 3200]\n",
      "loss: 0.018707  [ 1152/ 3200]\n",
      "loss: 0.050707  [ 1168/ 3200]\n",
      "loss: 0.017533  [ 1184/ 3200]\n",
      "loss: 0.025085  [ 1200/ 3200]\n",
      "loss: 0.016035  [ 1216/ 3200]\n",
      "loss: 0.007055  [ 1232/ 3200]\n",
      "loss: 0.037856  [ 1248/ 3200]\n",
      "loss: 0.046287  [ 1264/ 3200]\n",
      "loss: 0.016003  [ 1280/ 3200]\n",
      "loss: 0.022893  [ 1296/ 3200]\n",
      "loss: 0.008245  [ 1312/ 3200]\n",
      "loss: 0.029718  [ 1328/ 3200]\n",
      "loss: 0.028387  [ 1344/ 3200]\n",
      "loss: 0.033853  [ 1360/ 3200]\n",
      "loss: 0.018797  [ 1376/ 3200]\n",
      "loss: 0.027180  [ 1392/ 3200]\n",
      "loss: 0.038746  [ 1408/ 3200]\n",
      "loss: 0.017445  [ 1424/ 3200]\n",
      "loss: 0.017208  [ 1440/ 3200]\n",
      "loss: 0.027204  [ 1456/ 3200]\n",
      "loss: 0.021657  [ 1472/ 3200]\n",
      "loss: 0.016871  [ 1488/ 3200]\n",
      "loss: 0.010078  [ 1504/ 3200]\n",
      "loss: 0.037433  [ 1520/ 3200]\n",
      "loss: 0.038825  [ 1536/ 3200]\n",
      "loss: 0.024776  [ 1552/ 3200]\n",
      "loss: 0.023617  [ 1568/ 3200]\n",
      "loss: 0.037707  [ 1584/ 3200]\n",
      "loss: 0.028152  [ 1600/ 3200]\n",
      "loss: 0.015780  [ 1616/ 3200]\n",
      "loss: 0.030001  [ 1632/ 3200]\n",
      "loss: 0.011697  [ 1648/ 3200]\n",
      "loss: 0.018657  [ 1664/ 3200]\n",
      "loss: 0.076963  [ 1680/ 3200]\n",
      "loss: 0.039366  [ 1696/ 3200]\n",
      "loss: 0.018522  [ 1712/ 3200]\n",
      "loss: 0.059141  [ 1728/ 3200]\n",
      "loss: 0.020981  [ 1744/ 3200]\n",
      "loss: 0.028276  [ 1760/ 3200]\n",
      "loss: 0.018710  [ 1776/ 3200]\n",
      "loss: 0.015071  [ 1792/ 3200]\n",
      "loss: 0.053046  [ 1808/ 3200]\n",
      "loss: 0.023527  [ 1824/ 3200]\n",
      "loss: 0.036390  [ 1840/ 3200]\n",
      "loss: 0.014985  [ 1856/ 3200]\n",
      "loss: 0.121032  [ 1872/ 3200]\n",
      "loss: 0.005154  [ 1888/ 3200]\n",
      "loss: 0.068032  [ 1904/ 3200]\n",
      "loss: 0.018471  [ 1920/ 3200]\n",
      "loss: 0.016733  [ 1936/ 3200]\n",
      "loss: 0.058853  [ 1952/ 3200]\n",
      "loss: 0.012167  [ 1968/ 3200]\n",
      "loss: 0.061681  [ 1984/ 3200]\n",
      "loss: 0.018684  [ 2000/ 3200]\n",
      "loss: 0.056818  [ 2016/ 3200]\n",
      "loss: 0.022693  [ 2032/ 3200]\n",
      "loss: 0.018947  [ 2048/ 3200]\n",
      "loss: 0.019234  [ 2064/ 3200]\n",
      "loss: 0.032080  [ 2080/ 3200]\n",
      "loss: 0.019552  [ 2096/ 3200]\n",
      "loss: 0.016430  [ 2112/ 3200]\n",
      "loss: 0.017355  [ 2128/ 3200]\n",
      "loss: 0.028508  [ 2144/ 3200]\n",
      "loss: 0.012012  [ 2160/ 3200]\n",
      "loss: 0.021184  [ 2176/ 3200]\n",
      "loss: 0.125095  [ 2192/ 3200]\n",
      "loss: 0.010230  [ 2208/ 3200]\n",
      "loss: 0.061076  [ 2224/ 3200]\n",
      "loss: 0.032666  [ 2240/ 3200]\n",
      "loss: 0.026756  [ 2256/ 3200]\n",
      "loss: 0.005332  [ 2272/ 3200]\n",
      "loss: 0.009903  [ 2288/ 3200]\n",
      "loss: 0.011566  [ 2304/ 3200]\n",
      "loss: 0.014015  [ 2320/ 3200]\n",
      "loss: 0.026438  [ 2336/ 3200]\n",
      "loss: 0.106365  [ 2352/ 3200]\n",
      "loss: 0.023900  [ 2368/ 3200]\n",
      "loss: 0.017584  [ 2384/ 3200]\n",
      "loss: 0.012038  [ 2400/ 3200]\n",
      "loss: 0.019502  [ 2416/ 3200]\n",
      "loss: 0.053541  [ 2432/ 3200]\n",
      "loss: 0.037857  [ 2448/ 3200]\n",
      "loss: 0.005098  [ 2464/ 3200]\n",
      "loss: 0.012395  [ 2480/ 3200]\n",
      "loss: 0.012630  [ 2496/ 3200]\n",
      "loss: 0.007853  [ 2512/ 3200]\n",
      "loss: 0.018050  [ 2528/ 3200]\n",
      "loss: 0.018569  [ 2544/ 3200]\n",
      "loss: 0.021087  [ 2560/ 3200]\n",
      "loss: 0.047040  [ 2576/ 3200]\n",
      "loss: 0.027919  [ 2592/ 3200]\n",
      "loss: 0.040630  [ 2608/ 3200]\n",
      "loss: 0.028179  [ 2624/ 3200]\n",
      "loss: 0.030726  [ 2640/ 3200]\n",
      "loss: 0.017655  [ 2656/ 3200]\n",
      "loss: 0.080732  [ 2672/ 3200]\n",
      "loss: 0.074640  [ 2688/ 3200]\n",
      "loss: 0.023067  [ 2704/ 3200]\n",
      "loss: 0.021809  [ 2720/ 3200]\n",
      "loss: 0.033884  [ 2736/ 3200]\n",
      "loss: 0.006481  [ 2752/ 3200]\n",
      "loss: 0.034404  [ 2768/ 3200]\n",
      "loss: 0.007288  [ 2784/ 3200]\n",
      "loss: 0.013547  [ 2800/ 3200]\n",
      "loss: 0.045717  [ 2816/ 3200]\n",
      "loss: 0.022827  [ 2832/ 3200]\n",
      "loss: 0.027939  [ 2848/ 3200]\n",
      "loss: 0.020453  [ 2864/ 3200]\n",
      "loss: 0.017987  [ 2880/ 3200]\n",
      "loss: 0.071454  [ 2896/ 3200]\n",
      "loss: 0.056589  [ 2912/ 3200]\n",
      "loss: 0.033857  [ 2928/ 3200]\n",
      "loss: 0.016636  [ 2944/ 3200]\n",
      "loss: 0.013254  [ 2960/ 3200]\n",
      "loss: 0.009425  [ 2976/ 3200]\n",
      "loss: 0.023679  [ 2992/ 3200]\n",
      "loss: 0.027357  [ 3008/ 3200]\n",
      "loss: 0.010398  [ 3024/ 3200]\n",
      "loss: 0.015813  [ 3040/ 3200]\n",
      "loss: 0.042157  [ 3056/ 3200]\n",
      "loss: 0.012935  [ 3072/ 3200]\n",
      "loss: 0.023714  [ 3088/ 3200]\n",
      "loss: 0.007431  [ 3104/ 3200]\n",
      "loss: 0.036234  [ 3120/ 3200]\n",
      "loss: 0.076769  [ 3136/ 3200]\n",
      "loss: 0.035673  [ 3152/ 3200]\n",
      "loss: 0.046690  [ 3168/ 3200]\n",
      "loss: 0.036006  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.050943\n",
      "f1 macro averaged score: 0.782234\n",
      "Accuracy               : 78.4%\n",
      "Confusion matrix       :\n",
      "tensor([[181,   5,   0,  14],\n",
      "        [ 16, 126,  20,  38],\n",
      "        [  0,  24, 158,  18],\n",
      "        [ 10,  16,  12, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.2704e-04.\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 0.019797  [    0/ 3200]\n",
      "loss: 0.071001  [   16/ 3200]\n",
      "loss: 0.043875  [   32/ 3200]\n",
      "loss: 0.007289  [   48/ 3200]\n",
      "loss: 0.011628  [   64/ 3200]\n",
      "loss: 0.008843  [   80/ 3200]\n",
      "loss: 0.018902  [   96/ 3200]\n",
      "loss: 0.048762  [  112/ 3200]\n",
      "loss: 0.016830  [  128/ 3200]\n",
      "loss: 0.032676  [  144/ 3200]\n",
      "loss: 0.023963  [  160/ 3200]\n",
      "loss: 0.022556  [  176/ 3200]\n",
      "loss: 0.043878  [  192/ 3200]\n",
      "loss: 0.017567  [  208/ 3200]\n",
      "loss: 0.026794  [  224/ 3200]\n",
      "loss: 0.075676  [  240/ 3200]\n",
      "loss: 0.014227  [  256/ 3200]\n",
      "loss: 0.044479  [  272/ 3200]\n",
      "loss: 0.022176  [  288/ 3200]\n",
      "loss: 0.026692  [  304/ 3200]\n",
      "loss: 0.007031  [  320/ 3200]\n",
      "loss: 0.018004  [  336/ 3200]\n",
      "loss: 0.014726  [  352/ 3200]\n",
      "loss: 0.025439  [  368/ 3200]\n",
      "loss: 0.020796  [  384/ 3200]\n",
      "loss: 0.014712  [  400/ 3200]\n",
      "loss: 0.081170  [  416/ 3200]\n",
      "loss: 0.021671  [  432/ 3200]\n",
      "loss: 0.035596  [  448/ 3200]\n",
      "loss: 0.020846  [  464/ 3200]\n",
      "loss: 0.026862  [  480/ 3200]\n",
      "loss: 0.014957  [  496/ 3200]\n",
      "loss: 0.015212  [  512/ 3200]\n",
      "loss: 0.032696  [  528/ 3200]\n",
      "loss: 0.015045  [  544/ 3200]\n",
      "loss: 0.011826  [  560/ 3200]\n",
      "loss: 0.038212  [  576/ 3200]\n",
      "loss: 0.019247  [  592/ 3200]\n",
      "loss: 0.023076  [  608/ 3200]\n",
      "loss: 0.027274  [  624/ 3200]\n",
      "loss: 0.014142  [  640/ 3200]\n",
      "loss: 0.102452  [  656/ 3200]\n",
      "loss: 0.018014  [  672/ 3200]\n",
      "loss: 0.018903  [  688/ 3200]\n",
      "loss: 0.041343  [  704/ 3200]\n",
      "loss: 0.009243  [  720/ 3200]\n",
      "loss: 0.006144  [  736/ 3200]\n",
      "loss: 0.036156  [  752/ 3200]\n",
      "loss: 0.013873  [  768/ 3200]\n",
      "loss: 0.005597  [  784/ 3200]\n",
      "loss: 0.019155  [  800/ 3200]\n",
      "loss: 0.011203  [  816/ 3200]\n",
      "loss: 0.026083  [  832/ 3200]\n",
      "loss: 0.015746  [  848/ 3200]\n",
      "loss: 0.021421  [  864/ 3200]\n",
      "loss: 0.060881  [  880/ 3200]\n",
      "loss: 0.018427  [  896/ 3200]\n",
      "loss: 0.023509  [  912/ 3200]\n",
      "loss: 0.046647  [  928/ 3200]\n",
      "loss: 0.018213  [  944/ 3200]\n",
      "loss: 0.005791  [  960/ 3200]\n",
      "loss: 0.007770  [  976/ 3200]\n",
      "loss: 0.022884  [  992/ 3200]\n",
      "loss: 0.017631  [ 1008/ 3200]\n",
      "loss: 0.048362  [ 1024/ 3200]\n",
      "loss: 0.014052  [ 1040/ 3200]\n",
      "loss: 0.063701  [ 1056/ 3200]\n",
      "loss: 0.034196  [ 1072/ 3200]\n",
      "loss: 0.010974  [ 1088/ 3200]\n",
      "loss: 0.024151  [ 1104/ 3200]\n",
      "loss: 0.011310  [ 1120/ 3200]\n",
      "loss: 0.019271  [ 1136/ 3200]\n",
      "loss: 0.004726  [ 1152/ 3200]\n",
      "loss: 0.013238  [ 1168/ 3200]\n",
      "loss: 0.036330  [ 1184/ 3200]\n",
      "loss: 0.035691  [ 1200/ 3200]\n",
      "loss: 0.008910  [ 1216/ 3200]\n",
      "loss: 0.018752  [ 1232/ 3200]\n",
      "loss: 0.048694  [ 1248/ 3200]\n",
      "loss: 0.035897  [ 1264/ 3200]\n",
      "loss: 0.009435  [ 1280/ 3200]\n",
      "loss: 0.009372  [ 1296/ 3200]\n",
      "loss: 0.017879  [ 1312/ 3200]\n",
      "loss: 0.046746  [ 1328/ 3200]\n",
      "loss: 0.024382  [ 1344/ 3200]\n",
      "loss: 0.036799  [ 1360/ 3200]\n",
      "loss: 0.031786  [ 1376/ 3200]\n",
      "loss: 0.031396  [ 1392/ 3200]\n",
      "loss: 0.027291  [ 1408/ 3200]\n",
      "loss: 0.020224  [ 1424/ 3200]\n",
      "loss: 0.028655  [ 1440/ 3200]\n",
      "loss: 0.049282  [ 1456/ 3200]\n",
      "loss: 0.014119  [ 1472/ 3200]\n",
      "loss: 0.018487  [ 1488/ 3200]\n",
      "loss: 0.024149  [ 1504/ 3200]\n",
      "loss: 0.051759  [ 1520/ 3200]\n",
      "loss: 0.015632  [ 1536/ 3200]\n",
      "loss: 0.053964  [ 1552/ 3200]\n",
      "loss: 0.023741  [ 1568/ 3200]\n",
      "loss: 0.024207  [ 1584/ 3200]\n",
      "loss: 0.008088  [ 1600/ 3200]\n",
      "loss: 0.031128  [ 1616/ 3200]\n",
      "loss: 0.006327  [ 1632/ 3200]\n",
      "loss: 0.019845  [ 1648/ 3200]\n",
      "loss: 0.011913  [ 1664/ 3200]\n",
      "loss: 0.007953  [ 1680/ 3200]\n",
      "loss: 0.054984  [ 1696/ 3200]\n",
      "loss: 0.084927  [ 1712/ 3200]\n",
      "loss: 0.004713  [ 1728/ 3200]\n",
      "loss: 0.033894  [ 1744/ 3200]\n",
      "loss: 0.015458  [ 1760/ 3200]\n",
      "loss: 0.006333  [ 1776/ 3200]\n",
      "loss: 0.042991  [ 1792/ 3200]\n",
      "loss: 0.020371  [ 1808/ 3200]\n",
      "loss: 0.015106  [ 1824/ 3200]\n",
      "loss: 0.110087  [ 1840/ 3200]\n",
      "loss: 0.069836  [ 1856/ 3200]\n",
      "loss: 0.086980  [ 1872/ 3200]\n",
      "loss: 0.004936  [ 1888/ 3200]\n",
      "loss: 0.021626  [ 1904/ 3200]\n",
      "loss: 0.029934  [ 1920/ 3200]\n",
      "loss: 0.013317  [ 1936/ 3200]\n",
      "loss: 0.031340  [ 1952/ 3200]\n",
      "loss: 0.022335  [ 1968/ 3200]\n",
      "loss: 0.019523  [ 1984/ 3200]\n",
      "loss: 0.038559  [ 2000/ 3200]\n",
      "loss: 0.006014  [ 2016/ 3200]\n",
      "loss: 0.005786  [ 2032/ 3200]\n",
      "loss: 0.016670  [ 2048/ 3200]\n",
      "loss: 0.017376  [ 2064/ 3200]\n",
      "loss: 0.018094  [ 2080/ 3200]\n",
      "loss: 0.042203  [ 2096/ 3200]\n",
      "loss: 0.022632  [ 2112/ 3200]\n",
      "loss: 0.007101  [ 2128/ 3200]\n",
      "loss: 0.014255  [ 2144/ 3200]\n",
      "loss: 0.033987  [ 2160/ 3200]\n",
      "loss: 0.043960  [ 2176/ 3200]\n",
      "loss: 0.013080  [ 2192/ 3200]\n",
      "loss: 0.028172  [ 2208/ 3200]\n",
      "loss: 0.020078  [ 2224/ 3200]\n",
      "loss: 0.037832  [ 2240/ 3200]\n",
      "loss: 0.012968  [ 2256/ 3200]\n",
      "loss: 0.024775  [ 2272/ 3200]\n",
      "loss: 0.028209  [ 2288/ 3200]\n",
      "loss: 0.119167  [ 2304/ 3200]\n",
      "loss: 0.060829  [ 2320/ 3200]\n",
      "loss: 0.047405  [ 2336/ 3200]\n",
      "loss: 0.022890  [ 2352/ 3200]\n",
      "loss: 0.012870  [ 2368/ 3200]\n",
      "loss: 0.012671  [ 2384/ 3200]\n",
      "loss: 0.015902  [ 2400/ 3200]\n",
      "loss: 0.007392  [ 2416/ 3200]\n",
      "loss: 0.058402  [ 2432/ 3200]\n",
      "loss: 0.059125  [ 2448/ 3200]\n",
      "loss: 0.007014  [ 2464/ 3200]\n",
      "loss: 0.021477  [ 2480/ 3200]\n",
      "loss: 0.045239  [ 2496/ 3200]\n",
      "loss: 0.029862  [ 2512/ 3200]\n",
      "loss: 0.011276  [ 2528/ 3200]\n",
      "loss: 0.016707  [ 2544/ 3200]\n",
      "loss: 0.045671  [ 2560/ 3200]\n",
      "loss: 0.002589  [ 2576/ 3200]\n",
      "loss: 0.026834  [ 2592/ 3200]\n",
      "loss: 0.008579  [ 2608/ 3200]\n",
      "loss: 0.008523  [ 2624/ 3200]\n",
      "loss: 0.013672  [ 2640/ 3200]\n",
      "loss: 0.031148  [ 2656/ 3200]\n",
      "loss: 0.016913  [ 2672/ 3200]\n",
      "loss: 0.026531  [ 2688/ 3200]\n",
      "loss: 0.055845  [ 2704/ 3200]\n",
      "loss: 0.064330  [ 2720/ 3200]\n",
      "loss: 0.008737  [ 2736/ 3200]\n",
      "loss: 0.010950  [ 2752/ 3200]\n",
      "loss: 0.018871  [ 2768/ 3200]\n",
      "loss: 0.016003  [ 2784/ 3200]\n",
      "loss: 0.027293  [ 2800/ 3200]\n",
      "loss: 0.010414  [ 2816/ 3200]\n",
      "loss: 0.036832  [ 2832/ 3200]\n",
      "loss: 0.037904  [ 2848/ 3200]\n",
      "loss: 0.007246  [ 2864/ 3200]\n",
      "loss: 0.028853  [ 2880/ 3200]\n",
      "loss: 0.011795  [ 2896/ 3200]\n",
      "loss: 0.024295  [ 2912/ 3200]\n",
      "loss: 0.031829  [ 2928/ 3200]\n",
      "loss: 0.241540  [ 2944/ 3200]\n",
      "loss: 0.038301  [ 2960/ 3200]\n",
      "loss: 0.021528  [ 2976/ 3200]\n",
      "loss: 0.017056  [ 2992/ 3200]\n",
      "loss: 0.086435  [ 3008/ 3200]\n",
      "loss: 0.021108  [ 3024/ 3200]\n",
      "loss: 0.020467  [ 3040/ 3200]\n",
      "loss: 0.042664  [ 3056/ 3200]\n",
      "loss: 0.038250  [ 3072/ 3200]\n",
      "loss: 0.010753  [ 3088/ 3200]\n",
      "loss: 0.028228  [ 3104/ 3200]\n",
      "loss: 0.023046  [ 3120/ 3200]\n",
      "loss: 0.062354  [ 3136/ 3200]\n",
      "loss: 0.014711  [ 3152/ 3200]\n",
      "loss: 0.010292  [ 3168/ 3200]\n",
      "loss: 0.014530  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.051835\n",
      "f1 macro averaged score: 0.780138\n",
      "Accuracy               : 78.1%\n",
      "Confusion matrix       :\n",
      "tensor([[181,   5,   0,  14],\n",
      "        [ 17, 128,  18,  37],\n",
      "        [  0,  26, 154,  20],\n",
      "        [ 10,  17,  11, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.0069e-04.\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.019164  [    0/ 3200]\n",
      "loss: 0.045409  [   16/ 3200]\n",
      "loss: 0.016972  [   32/ 3200]\n",
      "loss: 0.051477  [   48/ 3200]\n",
      "loss: 0.012171  [   64/ 3200]\n",
      "loss: 0.058416  [   80/ 3200]\n",
      "loss: 0.025550  [   96/ 3200]\n",
      "loss: 0.028964  [  112/ 3200]\n",
      "loss: 0.042318  [  128/ 3200]\n",
      "loss: 0.008519  [  144/ 3200]\n",
      "loss: 0.027450  [  160/ 3200]\n",
      "loss: 0.015561  [  176/ 3200]\n",
      "loss: 0.020380  [  192/ 3200]\n",
      "loss: 0.010738  [  208/ 3200]\n",
      "loss: 0.025049  [  224/ 3200]\n",
      "loss: 0.015954  [  240/ 3200]\n",
      "loss: 0.015596  [  256/ 3200]\n",
      "loss: 0.028953  [  272/ 3200]\n",
      "loss: 0.010088  [  288/ 3200]\n",
      "loss: 0.041697  [  304/ 3200]\n",
      "loss: 0.013086  [  320/ 3200]\n",
      "loss: 0.008800  [  336/ 3200]\n",
      "loss: 0.003646  [  352/ 3200]\n",
      "loss: 0.111107  [  368/ 3200]\n",
      "loss: 0.032416  [  384/ 3200]\n",
      "loss: 0.057277  [  400/ 3200]\n",
      "loss: 0.062388  [  416/ 3200]\n",
      "loss: 0.034266  [  432/ 3200]\n",
      "loss: 0.034458  [  448/ 3200]\n",
      "loss: 0.008444  [  464/ 3200]\n",
      "loss: 0.006804  [  480/ 3200]\n",
      "loss: 0.027304  [  496/ 3200]\n",
      "loss: 0.022424  [  512/ 3200]\n",
      "loss: 0.005146  [  528/ 3200]\n",
      "loss: 0.010122  [  544/ 3200]\n",
      "loss: 0.009480  [  560/ 3200]\n",
      "loss: 0.079160  [  576/ 3200]\n",
      "loss: 0.008916  [  592/ 3200]\n",
      "loss: 0.006041  [  608/ 3200]\n",
      "loss: 0.017071  [  624/ 3200]\n",
      "loss: 0.030644  [  640/ 3200]\n",
      "loss: 0.015048  [  656/ 3200]\n",
      "loss: 0.028690  [  672/ 3200]\n",
      "loss: 0.019773  [  688/ 3200]\n",
      "loss: 0.015078  [  704/ 3200]\n",
      "loss: 0.008402  [  720/ 3200]\n",
      "loss: 0.044570  [  736/ 3200]\n",
      "loss: 0.005511  [  752/ 3200]\n",
      "loss: 0.016212  [  768/ 3200]\n",
      "loss: 0.015317  [  784/ 3200]\n",
      "loss: 0.072556  [  800/ 3200]\n",
      "loss: 0.029153  [  816/ 3200]\n",
      "loss: 0.014418  [  832/ 3200]\n",
      "loss: 0.015135  [  848/ 3200]\n",
      "loss: 0.017336  [  864/ 3200]\n",
      "loss: 0.042792  [  880/ 3200]\n",
      "loss: 0.009600  [  896/ 3200]\n",
      "loss: 0.017833  [  912/ 3200]\n",
      "loss: 0.013013  [  928/ 3200]\n",
      "loss: 0.077294  [  944/ 3200]\n",
      "loss: 0.040357  [  960/ 3200]\n",
      "loss: 0.025800  [  976/ 3200]\n",
      "loss: 0.110485  [  992/ 3200]\n",
      "loss: 0.013010  [ 1008/ 3200]\n",
      "loss: 0.025605  [ 1024/ 3200]\n",
      "loss: 0.021289  [ 1040/ 3200]\n",
      "loss: 0.022510  [ 1056/ 3200]\n",
      "loss: 0.013463  [ 1072/ 3200]\n",
      "loss: 0.040845  [ 1088/ 3200]\n",
      "loss: 0.008352  [ 1104/ 3200]\n",
      "loss: 0.012757  [ 1120/ 3200]\n",
      "loss: 0.016727  [ 1136/ 3200]\n",
      "loss: 0.034405  [ 1152/ 3200]\n",
      "loss: 0.020490  [ 1168/ 3200]\n",
      "loss: 0.036832  [ 1184/ 3200]\n",
      "loss: 0.032838  [ 1200/ 3200]\n",
      "loss: 0.007918  [ 1216/ 3200]\n",
      "loss: 0.039533  [ 1232/ 3200]\n",
      "loss: 0.011672  [ 1248/ 3200]\n",
      "loss: 0.032216  [ 1264/ 3200]\n",
      "loss: 0.026733  [ 1280/ 3200]\n",
      "loss: 0.022067  [ 1296/ 3200]\n",
      "loss: 0.018483  [ 1312/ 3200]\n",
      "loss: 0.056009  [ 1328/ 3200]\n",
      "loss: 0.009830  [ 1344/ 3200]\n",
      "loss: 0.009051  [ 1360/ 3200]\n",
      "loss: 0.013209  [ 1376/ 3200]\n",
      "loss: 0.007740  [ 1392/ 3200]\n",
      "loss: 0.015293  [ 1408/ 3200]\n",
      "loss: 0.029569  [ 1424/ 3200]\n",
      "loss: 0.011857  [ 1440/ 3200]\n",
      "loss: 0.029406  [ 1456/ 3200]\n",
      "loss: 0.029844  [ 1472/ 3200]\n",
      "loss: 0.012549  [ 1488/ 3200]\n",
      "loss: 0.018508  [ 1504/ 3200]\n",
      "loss: 0.025352  [ 1520/ 3200]\n",
      "loss: 0.035163  [ 1536/ 3200]\n",
      "loss: 0.036301  [ 1552/ 3200]\n",
      "loss: 0.017977  [ 1568/ 3200]\n",
      "loss: 0.013974  [ 1584/ 3200]\n",
      "loss: 0.016487  [ 1600/ 3200]\n",
      "loss: 0.012148  [ 1616/ 3200]\n",
      "loss: 0.016026  [ 1632/ 3200]\n",
      "loss: 0.018941  [ 1648/ 3200]\n",
      "loss: 0.021530  [ 1664/ 3200]\n",
      "loss: 0.009918  [ 1680/ 3200]\n",
      "loss: 0.223762  [ 1696/ 3200]\n",
      "loss: 0.032689  [ 1712/ 3200]\n",
      "loss: 0.014235  [ 1728/ 3200]\n",
      "loss: 0.035781  [ 1744/ 3200]\n",
      "loss: 0.025750  [ 1760/ 3200]\n",
      "loss: 0.026913  [ 1776/ 3200]\n",
      "loss: 0.024231  [ 1792/ 3200]\n",
      "loss: 0.014002  [ 1808/ 3200]\n",
      "loss: 0.039766  [ 1824/ 3200]\n",
      "loss: 0.027721  [ 1840/ 3200]\n",
      "loss: 0.008824  [ 1856/ 3200]\n",
      "loss: 0.022647  [ 1872/ 3200]\n",
      "loss: 0.032026  [ 1888/ 3200]\n",
      "loss: 0.044388  [ 1904/ 3200]\n",
      "loss: 0.009077  [ 1920/ 3200]\n",
      "loss: 0.015904  [ 1936/ 3200]\n",
      "loss: 0.019580  [ 1952/ 3200]\n",
      "loss: 0.016079  [ 1968/ 3200]\n",
      "loss: 0.013643  [ 1984/ 3200]\n",
      "loss: 0.037949  [ 2000/ 3200]\n",
      "loss: 0.040957  [ 2016/ 3200]\n",
      "loss: 0.014878  [ 2032/ 3200]\n",
      "loss: 0.009328  [ 2048/ 3200]\n",
      "loss: 0.024635  [ 2064/ 3200]\n",
      "loss: 0.027924  [ 2080/ 3200]\n",
      "loss: 0.012744  [ 2096/ 3200]\n",
      "loss: 0.019591  [ 2112/ 3200]\n",
      "loss: 0.020060  [ 2128/ 3200]\n",
      "loss: 0.018680  [ 2144/ 3200]\n",
      "loss: 0.026999  [ 2160/ 3200]\n",
      "loss: 0.026838  [ 2176/ 3200]\n",
      "loss: 0.040066  [ 2192/ 3200]\n",
      "loss: 0.006532  [ 2208/ 3200]\n",
      "loss: 0.010013  [ 2224/ 3200]\n",
      "loss: 0.028425  [ 2240/ 3200]\n",
      "loss: 0.015434  [ 2256/ 3200]\n",
      "loss: 0.012458  [ 2272/ 3200]\n",
      "loss: 0.006314  [ 2288/ 3200]\n",
      "loss: 0.009479  [ 2304/ 3200]\n",
      "loss: 0.066329  [ 2320/ 3200]\n",
      "loss: 0.032244  [ 2336/ 3200]\n",
      "loss: 0.011052  [ 2352/ 3200]\n",
      "loss: 0.118369  [ 2368/ 3200]\n",
      "loss: 0.023937  [ 2384/ 3200]\n",
      "loss: 0.010368  [ 2400/ 3200]\n",
      "loss: 0.004887  [ 2416/ 3200]\n",
      "loss: 0.029543  [ 2432/ 3200]\n",
      "loss: 0.001281  [ 2448/ 3200]\n",
      "loss: 0.030268  [ 2464/ 3200]\n",
      "loss: 0.089407  [ 2480/ 3200]\n",
      "loss: 0.034512  [ 2496/ 3200]\n",
      "loss: 0.030564  [ 2512/ 3200]\n",
      "loss: 0.017876  [ 2528/ 3200]\n",
      "loss: 0.007275  [ 2544/ 3200]\n",
      "loss: 0.036645  [ 2560/ 3200]\n",
      "loss: 0.013248  [ 2576/ 3200]\n",
      "loss: 0.021693  [ 2592/ 3200]\n",
      "loss: 0.034708  [ 2608/ 3200]\n",
      "loss: 0.055938  [ 2624/ 3200]\n",
      "loss: 0.042017  [ 2640/ 3200]\n",
      "loss: 0.015416  [ 2656/ 3200]\n",
      "loss: 0.011083  [ 2672/ 3200]\n",
      "loss: 0.013713  [ 2688/ 3200]\n",
      "loss: 0.043561  [ 2704/ 3200]\n",
      "loss: 0.008324  [ 2720/ 3200]\n",
      "loss: 0.012028  [ 2736/ 3200]\n",
      "loss: 0.062876  [ 2752/ 3200]\n",
      "loss: 0.013194  [ 2768/ 3200]\n",
      "loss: 0.039721  [ 2784/ 3200]\n",
      "loss: 0.007532  [ 2800/ 3200]\n",
      "loss: 0.051305  [ 2816/ 3200]\n",
      "loss: 0.024452  [ 2832/ 3200]\n",
      "loss: 0.008167  [ 2848/ 3200]\n",
      "loss: 0.017951  [ 2864/ 3200]\n",
      "loss: 0.010004  [ 2880/ 3200]\n",
      "loss: 0.039504  [ 2896/ 3200]\n",
      "loss: 0.025770  [ 2912/ 3200]\n",
      "loss: 0.006054  [ 2928/ 3200]\n",
      "loss: 0.014660  [ 2944/ 3200]\n",
      "loss: 0.023489  [ 2960/ 3200]\n",
      "loss: 0.027626  [ 2976/ 3200]\n",
      "loss: 0.026599  [ 2992/ 3200]\n",
      "loss: 0.040081  [ 3008/ 3200]\n",
      "loss: 0.014931  [ 3024/ 3200]\n",
      "loss: 0.037091  [ 3040/ 3200]\n",
      "loss: 0.024187  [ 3056/ 3200]\n",
      "loss: 0.024096  [ 3072/ 3200]\n",
      "loss: 0.006270  [ 3088/ 3200]\n",
      "loss: 0.018322  [ 3104/ 3200]\n",
      "loss: 0.010764  [ 3120/ 3200]\n",
      "loss: 0.050405  [ 3136/ 3200]\n",
      "loss: 0.028741  [ 3152/ 3200]\n",
      "loss: 0.014565  [ 3168/ 3200]\n",
      "loss: 0.004783  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.053505\n",
      "f1 macro averaged score: 0.773992\n",
      "Accuracy               : 77.6%\n",
      "Confusion matrix       :\n",
      "tensor([[179,   5,   0,  16],\n",
      "        [ 16, 118,  22,  44],\n",
      "        [  0,  21, 159,  20],\n",
      "        [ 10,  14,  11, 165]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.7565e-04.\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 0.022081  [    0/ 3200]\n",
      "loss: 0.020310  [   16/ 3200]\n",
      "loss: 0.011190  [   32/ 3200]\n",
      "loss: 0.021957  [   48/ 3200]\n",
      "loss: 0.024692  [   64/ 3200]\n",
      "loss: 0.006906  [   80/ 3200]\n",
      "loss: 0.021651  [   96/ 3200]\n",
      "loss: 0.014734  [  112/ 3200]\n",
      "loss: 0.032470  [  128/ 3200]\n",
      "loss: 0.030684  [  144/ 3200]\n",
      "loss: 0.053812  [  160/ 3200]\n",
      "loss: 0.048885  [  176/ 3200]\n",
      "loss: 0.011932  [  192/ 3200]\n",
      "loss: 0.020427  [  208/ 3200]\n",
      "loss: 0.010629  [  224/ 3200]\n",
      "loss: 0.024424  [  240/ 3200]\n",
      "loss: 0.033139  [  256/ 3200]\n",
      "loss: 0.004912  [  272/ 3200]\n",
      "loss: 0.043901  [  288/ 3200]\n",
      "loss: 0.022836  [  304/ 3200]\n",
      "loss: 0.013408  [  320/ 3200]\n",
      "loss: 0.009789  [  336/ 3200]\n",
      "loss: 0.005517  [  352/ 3200]\n",
      "loss: 0.009697  [  368/ 3200]\n",
      "loss: 0.004302  [  384/ 3200]\n",
      "loss: 0.071761  [  400/ 3200]\n",
      "loss: 0.103293  [  416/ 3200]\n",
      "loss: 0.025894  [  432/ 3200]\n",
      "loss: 0.014991  [  448/ 3200]\n",
      "loss: 0.006681  [  464/ 3200]\n",
      "loss: 0.043855  [  480/ 3200]\n",
      "loss: 0.064604  [  496/ 3200]\n",
      "loss: 0.011691  [  512/ 3200]\n",
      "loss: 0.076279  [  528/ 3200]\n",
      "loss: 0.018757  [  544/ 3200]\n",
      "loss: 0.007299  [  560/ 3200]\n",
      "loss: 0.022450  [  576/ 3200]\n",
      "loss: 0.022137  [  592/ 3200]\n",
      "loss: 0.015176  [  608/ 3200]\n",
      "loss: 0.009300  [  624/ 3200]\n",
      "loss: 0.013344  [  640/ 3200]\n",
      "loss: 0.012158  [  656/ 3200]\n",
      "loss: 0.047607  [  672/ 3200]\n",
      "loss: 0.026040  [  688/ 3200]\n",
      "loss: 0.025726  [  704/ 3200]\n",
      "loss: 0.036638  [  720/ 3200]\n",
      "loss: 0.015641  [  736/ 3200]\n",
      "loss: 0.005165  [  752/ 3200]\n",
      "loss: 0.004907  [  768/ 3200]\n",
      "loss: 0.006429  [  784/ 3200]\n",
      "loss: 0.016908  [  800/ 3200]\n",
      "loss: 0.009652  [  816/ 3200]\n",
      "loss: 0.015635  [  832/ 3200]\n",
      "loss: 0.037927  [  848/ 3200]\n",
      "loss: 0.007248  [  864/ 3200]\n",
      "loss: 0.005167  [  880/ 3200]\n",
      "loss: 0.019182  [  896/ 3200]\n",
      "loss: 0.014678  [  912/ 3200]\n",
      "loss: 0.014894  [  928/ 3200]\n",
      "loss: 0.005250  [  944/ 3200]\n",
      "loss: 0.077749  [  960/ 3200]\n",
      "loss: 0.014650  [  976/ 3200]\n",
      "loss: 0.036648  [  992/ 3200]\n",
      "loss: 0.008089  [ 1008/ 3200]\n",
      "loss: 0.037142  [ 1024/ 3200]\n",
      "loss: 0.022638  [ 1040/ 3200]\n",
      "loss: 0.043351  [ 1056/ 3200]\n",
      "loss: 0.032819  [ 1072/ 3200]\n",
      "loss: 0.012619  [ 1088/ 3200]\n",
      "loss: 0.021226  [ 1104/ 3200]\n",
      "loss: 0.022799  [ 1120/ 3200]\n",
      "loss: 0.041680  [ 1136/ 3200]\n",
      "loss: 0.012829  [ 1152/ 3200]\n",
      "loss: 0.009688  [ 1168/ 3200]\n",
      "loss: 0.030123  [ 1184/ 3200]\n",
      "loss: 0.042605  [ 1200/ 3200]\n",
      "loss: 0.021037  [ 1216/ 3200]\n",
      "loss: 0.049822  [ 1232/ 3200]\n",
      "loss: 0.007628  [ 1248/ 3200]\n",
      "loss: 0.003696  [ 1264/ 3200]\n",
      "loss: 0.025758  [ 1280/ 3200]\n",
      "loss: 0.020741  [ 1296/ 3200]\n",
      "loss: 0.023883  [ 1312/ 3200]\n",
      "loss: 0.024903  [ 1328/ 3200]\n",
      "loss: 0.016100  [ 1344/ 3200]\n",
      "loss: 0.014390  [ 1360/ 3200]\n",
      "loss: 0.005118  [ 1376/ 3200]\n",
      "loss: 0.019460  [ 1392/ 3200]\n",
      "loss: 0.022628  [ 1408/ 3200]\n",
      "loss: 0.012408  [ 1424/ 3200]\n",
      "loss: 0.018284  [ 1440/ 3200]\n",
      "loss: 0.013776  [ 1456/ 3200]\n",
      "loss: 0.035967  [ 1472/ 3200]\n",
      "loss: 0.016762  [ 1488/ 3200]\n",
      "loss: 0.016657  [ 1504/ 3200]\n",
      "loss: 0.040411  [ 1520/ 3200]\n",
      "loss: 0.025205  [ 1536/ 3200]\n",
      "loss: 0.031874  [ 1552/ 3200]\n",
      "loss: 0.018849  [ 1568/ 3200]\n",
      "loss: 0.019439  [ 1584/ 3200]\n",
      "loss: 0.016548  [ 1600/ 3200]\n",
      "loss: 0.009934  [ 1616/ 3200]\n",
      "loss: 0.011300  [ 1632/ 3200]\n",
      "loss: 0.045695  [ 1648/ 3200]\n",
      "loss: 0.019836  [ 1664/ 3200]\n",
      "loss: 0.015973  [ 1680/ 3200]\n",
      "loss: 0.031952  [ 1696/ 3200]\n",
      "loss: 0.018432  [ 1712/ 3200]\n",
      "loss: 0.016880  [ 1728/ 3200]\n",
      "loss: 0.013186  [ 1744/ 3200]\n",
      "loss: 0.010788  [ 1760/ 3200]\n",
      "loss: 0.010395  [ 1776/ 3200]\n",
      "loss: 0.023991  [ 1792/ 3200]\n",
      "loss: 0.007992  [ 1808/ 3200]\n",
      "loss: 0.030387  [ 1824/ 3200]\n",
      "loss: 0.020682  [ 1840/ 3200]\n",
      "loss: 0.006427  [ 1856/ 3200]\n",
      "loss: 0.006072  [ 1872/ 3200]\n",
      "loss: 0.228451  [ 1888/ 3200]\n",
      "loss: 0.044453  [ 1904/ 3200]\n",
      "loss: 0.010404  [ 1920/ 3200]\n",
      "loss: 0.019173  [ 1936/ 3200]\n",
      "loss: 0.008256  [ 1952/ 3200]\n",
      "loss: 0.015320  [ 1968/ 3200]\n",
      "loss: 0.040766  [ 1984/ 3200]\n",
      "loss: 0.017345  [ 2000/ 3200]\n",
      "loss: 0.021874  [ 2016/ 3200]\n",
      "loss: 0.021826  [ 2032/ 3200]\n",
      "loss: 0.033622  [ 2048/ 3200]\n",
      "loss: 0.010727  [ 2064/ 3200]\n",
      "loss: 0.024421  [ 2080/ 3200]\n",
      "loss: 0.029332  [ 2096/ 3200]\n",
      "loss: 0.013239  [ 2112/ 3200]\n",
      "loss: 0.041744  [ 2128/ 3200]\n",
      "loss: 0.019891  [ 2144/ 3200]\n",
      "loss: 0.033219  [ 2160/ 3200]\n",
      "loss: 0.012374  [ 2176/ 3200]\n",
      "loss: 0.064719  [ 2192/ 3200]\n",
      "loss: 0.004121  [ 2208/ 3200]\n",
      "loss: 0.016338  [ 2224/ 3200]\n",
      "loss: 0.018849  [ 2240/ 3200]\n",
      "loss: 0.010576  [ 2256/ 3200]\n",
      "loss: 0.035551  [ 2272/ 3200]\n",
      "loss: 0.005640  [ 2288/ 3200]\n",
      "loss: 0.017531  [ 2304/ 3200]\n",
      "loss: 0.024647  [ 2320/ 3200]\n",
      "loss: 0.011573  [ 2336/ 3200]\n",
      "loss: 0.024780  [ 2352/ 3200]\n",
      "loss: 0.017775  [ 2368/ 3200]\n",
      "loss: 0.014596  [ 2384/ 3200]\n",
      "loss: 0.017818  [ 2400/ 3200]\n",
      "loss: 0.036767  [ 2416/ 3200]\n",
      "loss: 0.096823  [ 2432/ 3200]\n",
      "loss: 0.010052  [ 2448/ 3200]\n",
      "loss: 0.020397  [ 2464/ 3200]\n",
      "loss: 0.019711  [ 2480/ 3200]\n",
      "loss: 0.007942  [ 2496/ 3200]\n",
      "loss: 0.012993  [ 2512/ 3200]\n",
      "loss: 0.014732  [ 2528/ 3200]\n",
      "loss: 0.010182  [ 2544/ 3200]\n",
      "loss: 0.030147  [ 2560/ 3200]\n",
      "loss: 0.014007  [ 2576/ 3200]\n",
      "loss: 0.009917  [ 2592/ 3200]\n",
      "loss: 0.012434  [ 2608/ 3200]\n",
      "loss: 0.106153  [ 2624/ 3200]\n",
      "loss: 0.024498  [ 2640/ 3200]\n",
      "loss: 0.013236  [ 2656/ 3200]\n",
      "loss: 0.018616  [ 2672/ 3200]\n",
      "loss: 0.008187  [ 2688/ 3200]\n",
      "loss: 0.067918  [ 2704/ 3200]\n",
      "loss: 0.021284  [ 2720/ 3200]\n",
      "loss: 0.017946  [ 2736/ 3200]\n",
      "loss: 0.025037  [ 2752/ 3200]\n",
      "loss: 0.009216  [ 2768/ 3200]\n",
      "loss: 0.029500  [ 2784/ 3200]\n",
      "loss: 0.002995  [ 2800/ 3200]\n",
      "loss: 0.027868  [ 2816/ 3200]\n",
      "loss: 0.008177  [ 2832/ 3200]\n",
      "loss: 0.019906  [ 2848/ 3200]\n",
      "loss: 0.025587  [ 2864/ 3200]\n",
      "loss: 0.010167  [ 2880/ 3200]\n",
      "loss: 0.020720  [ 2896/ 3200]\n",
      "loss: 0.024224  [ 2912/ 3200]\n",
      "loss: 0.020968  [ 2928/ 3200]\n",
      "loss: 0.023479  [ 2944/ 3200]\n",
      "loss: 0.016575  [ 2960/ 3200]\n",
      "loss: 0.053669  [ 2976/ 3200]\n",
      "loss: 0.013561  [ 2992/ 3200]\n",
      "loss: 0.019928  [ 3008/ 3200]\n",
      "loss: 0.013369  [ 3024/ 3200]\n",
      "loss: 0.015170  [ 3040/ 3200]\n",
      "loss: 0.007159  [ 3056/ 3200]\n",
      "loss: 0.016988  [ 3072/ 3200]\n",
      "loss: 0.011048  [ 3088/ 3200]\n",
      "loss: 0.022246  [ 3104/ 3200]\n",
      "loss: 0.011715  [ 3120/ 3200]\n",
      "loss: 0.039608  [ 3136/ 3200]\n",
      "loss: 0.028589  [ 3152/ 3200]\n",
      "loss: 0.028676  [ 3168/ 3200]\n",
      "loss: 0.029456  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.052731\n",
      "f1 macro averaged score: 0.782827\n",
      "Accuracy               : 78.5%\n",
      "Confusion matrix       :\n",
      "tensor([[181,   5,   0,  14],\n",
      "        [ 17, 124,  23,  36],\n",
      "        [  0,  23, 161,  16],\n",
      "        [ 10,  16,  12, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.5187e-04.\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.011791  [    0/ 3200]\n",
      "loss: 0.016851  [   16/ 3200]\n",
      "loss: 0.006486  [   32/ 3200]\n",
      "loss: 0.010755  [   48/ 3200]\n",
      "loss: 0.011713  [   64/ 3200]\n",
      "loss: 0.025199  [   80/ 3200]\n",
      "loss: 0.015370  [   96/ 3200]\n",
      "loss: 0.015610  [  112/ 3200]\n",
      "loss: 0.015315  [  128/ 3200]\n",
      "loss: 0.024125  [  144/ 3200]\n",
      "loss: 0.011512  [  160/ 3200]\n",
      "loss: 0.025440  [  176/ 3200]\n",
      "loss: 0.005659  [  192/ 3200]\n",
      "loss: 0.153904  [  208/ 3200]\n",
      "loss: 0.037431  [  224/ 3200]\n",
      "loss: 0.008469  [  240/ 3200]\n",
      "loss: 0.006289  [  256/ 3200]\n",
      "loss: 0.010054  [  272/ 3200]\n",
      "loss: 0.011808  [  288/ 3200]\n",
      "loss: 0.023661  [  304/ 3200]\n",
      "loss: 0.012249  [  320/ 3200]\n",
      "loss: 0.037162  [  336/ 3200]\n",
      "loss: 0.016418  [  352/ 3200]\n",
      "loss: 0.022185  [  368/ 3200]\n",
      "loss: 0.037482  [  384/ 3200]\n",
      "loss: 0.035828  [  400/ 3200]\n",
      "loss: 0.038022  [  416/ 3200]\n",
      "loss: 0.018616  [  432/ 3200]\n",
      "loss: 0.038881  [  448/ 3200]\n",
      "loss: 0.027722  [  464/ 3200]\n",
      "loss: 0.021889  [  480/ 3200]\n",
      "loss: 0.010712  [  496/ 3200]\n",
      "loss: 0.038684  [  512/ 3200]\n",
      "loss: 0.016114  [  528/ 3200]\n",
      "loss: 0.020080  [  544/ 3200]\n",
      "loss: 0.013279  [  560/ 3200]\n",
      "loss: 0.004954  [  576/ 3200]\n",
      "loss: 0.007514  [  592/ 3200]\n",
      "loss: 0.010981  [  608/ 3200]\n",
      "loss: 0.016617  [  624/ 3200]\n",
      "loss: 0.051910  [  640/ 3200]\n",
      "loss: 0.016965  [  656/ 3200]\n",
      "loss: 0.010480  [  672/ 3200]\n",
      "loss: 0.013209  [  688/ 3200]\n",
      "loss: 0.036993  [  704/ 3200]\n",
      "loss: 0.014398  [  720/ 3200]\n",
      "loss: 0.018055  [  736/ 3200]\n",
      "loss: 0.036030  [  752/ 3200]\n",
      "loss: 0.006419  [  768/ 3200]\n",
      "loss: 0.021341  [  784/ 3200]\n",
      "loss: 0.017131  [  800/ 3200]\n",
      "loss: 0.012582  [  816/ 3200]\n",
      "loss: 0.016508  [  832/ 3200]\n",
      "loss: 0.010738  [  848/ 3200]\n",
      "loss: 0.014685  [  864/ 3200]\n",
      "loss: 0.013256  [  880/ 3200]\n",
      "loss: 0.001185  [  896/ 3200]\n",
      "loss: 0.027939  [  912/ 3200]\n",
      "loss: 0.018783  [  928/ 3200]\n",
      "loss: 0.049579  [  944/ 3200]\n",
      "loss: 0.041837  [  960/ 3200]\n",
      "loss: 0.014100  [  976/ 3200]\n",
      "loss: 0.037415  [  992/ 3200]\n",
      "loss: 0.025388  [ 1008/ 3200]\n",
      "loss: 0.026535  [ 1024/ 3200]\n",
      "loss: 0.025444  [ 1040/ 3200]\n",
      "loss: 0.012263  [ 1056/ 3200]\n",
      "loss: 0.020002  [ 1072/ 3200]\n",
      "loss: 0.018015  [ 1088/ 3200]\n",
      "loss: 0.016542  [ 1104/ 3200]\n",
      "loss: 0.015929  [ 1120/ 3200]\n",
      "loss: 0.013123  [ 1136/ 3200]\n",
      "loss: 0.023174  [ 1152/ 3200]\n",
      "loss: 0.023538  [ 1168/ 3200]\n",
      "loss: 0.021765  [ 1184/ 3200]\n",
      "loss: 0.005769  [ 1200/ 3200]\n",
      "loss: 0.022498  [ 1216/ 3200]\n",
      "loss: 0.038763  [ 1232/ 3200]\n",
      "loss: 0.039494  [ 1248/ 3200]\n",
      "loss: 0.011421  [ 1264/ 3200]\n",
      "loss: 0.016261  [ 1280/ 3200]\n",
      "loss: 0.023316  [ 1296/ 3200]\n",
      "loss: 0.010798  [ 1312/ 3200]\n",
      "loss: 0.013374  [ 1328/ 3200]\n",
      "loss: 0.015770  [ 1344/ 3200]\n",
      "loss: 0.027010  [ 1360/ 3200]\n",
      "loss: 0.017132  [ 1376/ 3200]\n",
      "loss: 0.040387  [ 1392/ 3200]\n",
      "loss: 0.021141  [ 1408/ 3200]\n",
      "loss: 0.016840  [ 1424/ 3200]\n",
      "loss: 0.009869  [ 1440/ 3200]\n",
      "loss: 0.018983  [ 1456/ 3200]\n",
      "loss: 0.004098  [ 1472/ 3200]\n",
      "loss: 0.011803  [ 1488/ 3200]\n",
      "loss: 0.048545  [ 1504/ 3200]\n",
      "loss: 0.003698  [ 1520/ 3200]\n",
      "loss: 0.013334  [ 1536/ 3200]\n",
      "loss: 0.017659  [ 1552/ 3200]\n",
      "loss: 0.038561  [ 1568/ 3200]\n",
      "loss: 0.008635  [ 1584/ 3200]\n",
      "loss: 0.020980  [ 1600/ 3200]\n",
      "loss: 0.010828  [ 1616/ 3200]\n",
      "loss: 0.060039  [ 1632/ 3200]\n",
      "loss: 0.012543  [ 1648/ 3200]\n",
      "loss: 0.033390  [ 1664/ 3200]\n",
      "loss: 0.019806  [ 1680/ 3200]\n",
      "loss: 0.017429  [ 1696/ 3200]\n",
      "loss: 0.050372  [ 1712/ 3200]\n",
      "loss: 0.006990  [ 1728/ 3200]\n",
      "loss: 0.028436  [ 1744/ 3200]\n",
      "loss: 0.044275  [ 1760/ 3200]\n",
      "loss: 0.016381  [ 1776/ 3200]\n",
      "loss: 0.029154  [ 1792/ 3200]\n",
      "loss: 0.017442  [ 1808/ 3200]\n",
      "loss: 0.015844  [ 1824/ 3200]\n",
      "loss: 0.008397  [ 1840/ 3200]\n",
      "loss: 0.013034  [ 1856/ 3200]\n",
      "loss: 0.048591  [ 1872/ 3200]\n",
      "loss: 0.005803  [ 1888/ 3200]\n",
      "loss: 0.039135  [ 1904/ 3200]\n",
      "loss: 0.027001  [ 1920/ 3200]\n",
      "loss: 0.010315  [ 1936/ 3200]\n",
      "loss: 0.016157  [ 1952/ 3200]\n",
      "loss: 0.028850  [ 1968/ 3200]\n",
      "loss: 0.081379  [ 1984/ 3200]\n",
      "loss: 0.036578  [ 2000/ 3200]\n",
      "loss: 0.010708  [ 2016/ 3200]\n",
      "loss: 0.015606  [ 2032/ 3200]\n",
      "loss: 0.011271  [ 2048/ 3200]\n",
      "loss: 0.039578  [ 2064/ 3200]\n",
      "loss: 0.009316  [ 2080/ 3200]\n",
      "loss: 0.015153  [ 2096/ 3200]\n",
      "loss: 0.015355  [ 2112/ 3200]\n",
      "loss: 0.010252  [ 2128/ 3200]\n",
      "loss: 0.033440  [ 2144/ 3200]\n",
      "loss: 0.016687  [ 2160/ 3200]\n",
      "loss: 0.017233  [ 2176/ 3200]\n",
      "loss: 0.010233  [ 2192/ 3200]\n",
      "loss: 0.015701  [ 2208/ 3200]\n",
      "loss: 0.016404  [ 2224/ 3200]\n",
      "loss: 0.004010  [ 2240/ 3200]\n",
      "loss: 0.041030  [ 2256/ 3200]\n",
      "loss: 0.042741  [ 2272/ 3200]\n",
      "loss: 0.022109  [ 2288/ 3200]\n",
      "loss: 0.023569  [ 2304/ 3200]\n",
      "loss: 0.013884  [ 2320/ 3200]\n",
      "loss: 0.008084  [ 2336/ 3200]\n",
      "loss: 0.018013  [ 2352/ 3200]\n",
      "loss: 0.032932  [ 2368/ 3200]\n",
      "loss: 0.007957  [ 2384/ 3200]\n",
      "loss: 0.039532  [ 2400/ 3200]\n",
      "loss: 0.020168  [ 2416/ 3200]\n",
      "loss: 0.039746  [ 2432/ 3200]\n",
      "loss: 0.015727  [ 2448/ 3200]\n",
      "loss: 0.009533  [ 2464/ 3200]\n",
      "loss: 0.009703  [ 2480/ 3200]\n",
      "loss: 0.050232  [ 2496/ 3200]\n",
      "loss: 0.008868  [ 2512/ 3200]\n",
      "loss: 0.013902  [ 2528/ 3200]\n",
      "loss: 0.044701  [ 2544/ 3200]\n",
      "loss: 0.008040  [ 2560/ 3200]\n",
      "loss: 0.008625  [ 2576/ 3200]\n",
      "loss: 0.014711  [ 2592/ 3200]\n",
      "loss: 0.022723  [ 2608/ 3200]\n",
      "loss: 0.035500  [ 2624/ 3200]\n",
      "loss: 0.005384  [ 2640/ 3200]\n",
      "loss: 0.020578  [ 2656/ 3200]\n",
      "loss: 0.010835  [ 2672/ 3200]\n",
      "loss: 0.063992  [ 2688/ 3200]\n",
      "loss: 0.013963  [ 2704/ 3200]\n",
      "loss: 0.024907  [ 2720/ 3200]\n",
      "loss: 0.012243  [ 2736/ 3200]\n",
      "loss: 0.109707  [ 2752/ 3200]\n",
      "loss: 0.022128  [ 2768/ 3200]\n",
      "loss: 0.019551  [ 2784/ 3200]\n",
      "loss: 0.008431  [ 2800/ 3200]\n",
      "loss: 0.012643  [ 2816/ 3200]\n",
      "loss: 0.019932  [ 2832/ 3200]\n",
      "loss: 0.008693  [ 2848/ 3200]\n",
      "loss: 0.008652  [ 2864/ 3200]\n",
      "loss: 0.026931  [ 2880/ 3200]\n",
      "loss: 0.050347  [ 2896/ 3200]\n",
      "loss: 0.006440  [ 2912/ 3200]\n",
      "loss: 0.007008  [ 2928/ 3200]\n",
      "loss: 0.019836  [ 2944/ 3200]\n",
      "loss: 0.027228  [ 2960/ 3200]\n",
      "loss: 0.025646  [ 2976/ 3200]\n",
      "loss: 0.022276  [ 2992/ 3200]\n",
      "loss: 0.017793  [ 3008/ 3200]\n",
      "loss: 0.016629  [ 3024/ 3200]\n",
      "loss: 0.021739  [ 3040/ 3200]\n",
      "loss: 0.027818  [ 3056/ 3200]\n",
      "loss: 0.020173  [ 3072/ 3200]\n",
      "loss: 0.005604  [ 3088/ 3200]\n",
      "loss: 0.014652  [ 3104/ 3200]\n",
      "loss: 0.018775  [ 3120/ 3200]\n",
      "loss: 0.013801  [ 3136/ 3200]\n",
      "loss: 0.012902  [ 3152/ 3200]\n",
      "loss: 0.024414  [ 3168/ 3200]\n",
      "loss: 0.006528  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.053981\n",
      "f1 macro averaged score: 0.779144\n",
      "Accuracy               : 78.1%\n",
      "Confusion matrix       :\n",
      "tensor([[179,   6,   0,  15],\n",
      "        [ 16, 121,  22,  41],\n",
      "        [  0,  22, 160,  18],\n",
      "        [ 10,  14,  11, 165]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.2928e-04.\n",
      "\n",
      "Best epoch: 7 with f1 macro averaged score: 0.8031092286109924\n",
      "Test Error:\n",
      "Avg loss               : 0.061541\n",
      "f1 macro averaged score: 0.781785\n",
      "Accuracy               : 77.9%\n",
      "Confusion matrix       :\n",
      "tensor([[280,   8,   3,   6],\n",
      "        [ 14, 203,  37,  70],\n",
      "        [  3,  31, 299,  23],\n",
      "        [ 10,  67,  32, 290]], device='cuda:0')\n",
      "CPU times: user 30.4 s, sys: 1.39 s, total: 31.7 s\n",
      "Wall time: 37.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch_seed(0)\n",
    "cnn_model = Net(ELU).to(device)\n",
    "\n",
    "learning_rate = 0.002\n",
    "optimizer = Adagrad(params=cnn_model.parameters(), lr=learning_rate)\n",
    "scheduler = MultiplicativeLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95, verbose=True)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "best_model, f1_per_epoch = validate_convolutional_neural_network(\n",
    "      epochs, optimizer, train_dataloader, val_dataloader, loss_function, cnn_model, True, scheduler\n",
    "      )\n",
    "results = test_convolutional_neural_network(test_dataloader, loss_function, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcj98K4nit9R"
   },
   "source": [
    "The accuracy stayed the same, around almost $78\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqDf2k2GMZHF"
   },
   "source": [
    "### Step 5 - Regularization\n",
    "\n",
    "i) Weight decay\n",
    "\n",
    "The following function validates a Convolutional Neural Network for different values of weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tSH7EMG-twC"
   },
   "outputs": [],
   "source": [
    "def validate_weight_decay(weight_decay_list, epochs):\n",
    "  f1_accuracy = []\n",
    "  for weight_decay in weight_decay_list:\n",
    "    torch_seed(0)\n",
    "    cnn_model = Net(ELU).to(device)\n",
    "    optimizer = Adagrad(params=cnn_model.parameters(), lr=learning_rate_0, weight_decay=weight_decay)\n",
    "    scheduler = MultiplicativeLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95, verbose=True)\n",
    "    print(\"  Weight decay:\", weight_decay)\n",
    "    best_model, f1_per_epoch = validate_convolutional_neural_network(\n",
    "        epochs, optimizer, train_dataloader, val_dataloader, loss_function, cnn_model, True, scheduler\n",
    "        )\n",
    "    results = test_convolutional_neural_network(test_dataloader, loss_function, best_model)\n",
    "    f1_accuracy.append((weight_decay, results[1], results[2]))\n",
    "  return f1_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUmT5zKajBBT"
   },
   "source": [
    "We validate our Convolutional Neural Network for $30$ and $60$ epochs and test it.\n",
    "\n",
    "Our model uses:\n",
    "+ the Adagrad optimizer\n",
    "+ the ELU activation function\n",
    "+ the MultiplicativeLR scheduler\n",
    "+ batch normalization\n",
    "\n",
    "as stated in the previous steps.\n",
    "\n",
    "The weight decay values used are the following: $0$, $10^{-5}$, $10^{-4}$, $10^{-3}$, $10^{-2}$, $10^{-1}$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-S0BlF-R_AwK",
    "outputId": "ccb3e313-7df3-4413-9008-13786592e7e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 1.384530  [ 1952/ 3200]\n",
      "loss: 1.388224  [ 1968/ 3200]\n",
      "loss: 1.388724  [ 1984/ 3200]\n",
      "loss: 1.381238  [ 2000/ 3200]\n",
      "loss: 1.381717  [ 2016/ 3200]\n",
      "loss: 1.385049  [ 2032/ 3200]\n",
      "loss: 1.385666  [ 2048/ 3200]\n",
      "loss: 1.389686  [ 2064/ 3200]\n",
      "loss: 1.382332  [ 2080/ 3200]\n",
      "loss: 1.382218  [ 2096/ 3200]\n",
      "loss: 1.386395  [ 2112/ 3200]\n",
      "loss: 1.390460  [ 2128/ 3200]\n",
      "loss: 1.385414  [ 2144/ 3200]\n",
      "loss: 1.387857  [ 2160/ 3200]\n",
      "loss: 1.384935  [ 2176/ 3200]\n",
      "loss: 1.390938  [ 2192/ 3200]\n",
      "loss: 1.385278  [ 2208/ 3200]\n",
      "loss: 1.388610  [ 2224/ 3200]\n",
      "loss: 1.390091  [ 2240/ 3200]\n",
      "loss: 1.383955  [ 2256/ 3200]\n",
      "loss: 1.387470  [ 2272/ 3200]\n",
      "loss: 1.378660  [ 2288/ 3200]\n",
      "loss: 1.390938  [ 2304/ 3200]\n",
      "loss: 1.385437  [ 2320/ 3200]\n",
      "loss: 1.390049  [ 2336/ 3200]\n",
      "loss: 1.388608  [ 2352/ 3200]\n",
      "loss: 1.388722  [ 2368/ 3200]\n",
      "loss: 1.387239  [ 2384/ 3200]\n",
      "loss: 1.387286  [ 2400/ 3200]\n",
      "loss: 1.389587  [ 2416/ 3200]\n",
      "loss: 1.386874  [ 2432/ 3200]\n",
      "loss: 1.389494  [ 2448/ 3200]\n",
      "loss: 1.386395  [ 2464/ 3200]\n",
      "loss: 1.389106  [ 2480/ 3200]\n",
      "loss: 1.385799  [ 2496/ 3200]\n",
      "loss: 1.388219  [ 2512/ 3200]\n",
      "loss: 1.384550  [ 2528/ 3200]\n",
      "loss: 1.386535  [ 2544/ 3200]\n",
      "loss: 1.384434  [ 2560/ 3200]\n",
      "loss: 1.386011  [ 2576/ 3200]\n",
      "loss: 1.387855  [ 2592/ 3200]\n",
      "loss: 1.384890  [ 2608/ 3200]\n",
      "loss: 1.381477  [ 2624/ 3200]\n",
      "loss: 1.386873  [ 2640/ 3200]\n",
      "loss: 1.382224  [ 2656/ 3200]\n",
      "loss: 1.387235  [ 2672/ 3200]\n",
      "loss: 1.381385  [ 2688/ 3200]\n",
      "loss: 1.384161  [ 2704/ 3200]\n",
      "loss: 1.387881  [ 2720/ 3200]\n",
      "loss: 1.383205  [ 2736/ 3200]\n",
      "loss: 1.384459  [ 2752/ 3200]\n",
      "loss: 1.395694  [ 2768/ 3200]\n",
      "loss: 1.388356  [ 2784/ 3200]\n",
      "loss: 1.388968  [ 2800/ 3200]\n",
      "loss: 1.384098  [ 2816/ 3200]\n",
      "loss: 1.380883  [ 2832/ 3200]\n",
      "loss: 1.389724  [ 2848/ 3200]\n",
      "loss: 1.380175  [ 2864/ 3200]\n",
      "loss: 1.384665  [ 2880/ 3200]\n",
      "loss: 1.383684  [ 2896/ 3200]\n",
      "loss: 1.386008  [ 2912/ 3200]\n",
      "loss: 1.383821  [ 2928/ 3200]\n",
      "loss: 1.384436  [ 2944/ 3200]\n",
      "loss: 1.385529  [ 2960/ 3200]\n",
      "loss: 1.386897  [ 2976/ 3200]\n",
      "loss: 1.391886  [ 2992/ 3200]\n",
      "loss: 1.387650  [ 3008/ 3200]\n",
      "loss: 1.386486  [ 3024/ 3200]\n",
      "loss: 1.380292  [ 3040/ 3200]\n",
      "loss: 1.377464  [ 3056/ 3200]\n",
      "loss: 1.389676  [ 3072/ 3200]\n",
      "loss: 1.386757  [ 3088/ 3200]\n",
      "loss: 1.386757  [ 3104/ 3200]\n",
      "loss: 1.385142  [ 3120/ 3200]\n",
      "loss: 1.387738  [ 3136/ 3200]\n",
      "loss: 1.380133  [ 3152/ 3200]\n",
      "loss: 1.388242  [ 3168/ 3200]\n",
      "loss: 1.394117  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086650\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.9978e-04.\n",
      "\n",
      "Epoch: 38\n",
      "-----------------------------\n",
      "loss: 1.388214  [    0/ 3200]\n",
      "loss: 1.385832  [   16/ 3200]\n",
      "loss: 1.383595  [   32/ 3200]\n",
      "loss: 1.376096  [   48/ 3200]\n",
      "loss: 1.385675  [   64/ 3200]\n",
      "loss: 1.396503  [   80/ 3200]\n",
      "loss: 1.387736  [   96/ 3200]\n",
      "loss: 1.384578  [  112/ 3200]\n",
      "loss: 1.388301  [  128/ 3200]\n",
      "loss: 1.392020  [  144/ 3200]\n",
      "loss: 1.386004  [  160/ 3200]\n",
      "loss: 1.386004  [  176/ 3200]\n",
      "loss: 1.387735  [  192/ 3200]\n",
      "loss: 1.385920  [  208/ 3200]\n",
      "loss: 1.384105  [  224/ 3200]\n",
      "loss: 1.389726  [  240/ 3200]\n",
      "loss: 1.383960  [  256/ 3200]\n",
      "loss: 1.387764  [  272/ 3200]\n",
      "loss: 1.384102  [  288/ 3200]\n",
      "loss: 1.386756  [  304/ 3200]\n",
      "loss: 1.385054  [  320/ 3200]\n",
      "loss: 1.383686  [  336/ 3200]\n",
      "loss: 1.383685  [  352/ 3200]\n",
      "loss: 1.386898  [  368/ 3200]\n",
      "loss: 1.383209  [  384/ 3200]\n",
      "loss: 1.389635  [  400/ 3200]\n",
      "loss: 1.389556  [  416/ 3200]\n",
      "loss: 1.387259  [  432/ 3200]\n",
      "loss: 1.382098  [  448/ 3200]\n",
      "loss: 1.385165  [  464/ 3200]\n",
      "loss: 1.384965  [  480/ 3200]\n",
      "loss: 1.381230  [  496/ 3200]\n",
      "loss: 1.383460  [  512/ 3200]\n",
      "loss: 1.382141  [  528/ 3200]\n",
      "loss: 1.383571  [  544/ 3200]\n",
      "loss: 1.387737  [  560/ 3200]\n",
      "loss: 1.386305  [  576/ 3200]\n",
      "loss: 1.387375  [  592/ 3200]\n",
      "loss: 1.383711  [  608/ 3200]\n",
      "loss: 1.386509  [  624/ 3200]\n",
      "loss: 1.383595  [  640/ 3200]\n",
      "loss: 1.384551  [  656/ 3200]\n",
      "loss: 1.384301  [  672/ 3200]\n",
      "loss: 1.390150  [  688/ 3200]\n",
      "loss: 1.387850  [  704/ 3200]\n",
      "loss: 1.389971  [  720/ 3200]\n",
      "loss: 1.387260  [  736/ 3200]\n",
      "loss: 1.385781  [  752/ 3200]\n",
      "loss: 1.391513  [  768/ 3200]\n",
      "loss: 1.389128  [  784/ 3200]\n",
      "loss: 1.386782  [  800/ 3200]\n",
      "loss: 1.382710  [  816/ 3200]\n",
      "loss: 1.380026  [  832/ 3200]\n",
      "loss: 1.388261  [  848/ 3200]\n",
      "loss: 1.389715  [  864/ 3200]\n",
      "loss: 1.386760  [  880/ 3200]\n",
      "loss: 1.381369  [  896/ 3200]\n",
      "loss: 1.391878  [  912/ 3200]\n",
      "loss: 1.384075  [  928/ 3200]\n",
      "loss: 1.384689  [  944/ 3200]\n",
      "loss: 1.386395  [  960/ 3200]\n",
      "loss: 1.394105  [  976/ 3200]\n",
      "loss: 1.388691  [  992/ 3200]\n",
      "loss: 1.387168  [ 1008/ 3200]\n",
      "loss: 1.386509  [ 1024/ 3200]\n",
      "loss: 1.385031  [ 1040/ 3200]\n",
      "loss: 1.382872  [ 1056/ 3200]\n",
      "loss: 1.390076  [ 1072/ 3200]\n",
      "loss: 1.381147  [ 1088/ 3200]\n",
      "loss: 1.386009  [ 1104/ 3200]\n",
      "loss: 1.380280  [ 1120/ 3200]\n",
      "loss: 1.384667  [ 1136/ 3200]\n",
      "loss: 1.384917  [ 1152/ 3200]\n",
      "loss: 1.376980  [ 1168/ 3200]\n",
      "loss: 1.390055  [ 1184/ 3200]\n",
      "loss: 1.386279  [ 1200/ 3200]\n",
      "loss: 1.385440  [ 1216/ 3200]\n",
      "loss: 1.385171  [ 1232/ 3200]\n",
      "loss: 1.392259  [ 1248/ 3200]\n",
      "loss: 1.390555  [ 1264/ 3200]\n",
      "loss: 1.383237  [ 1280/ 3200]\n",
      "loss: 1.383598  [ 1296/ 3200]\n",
      "loss: 1.386895  [ 1312/ 3200]\n",
      "loss: 1.384960  [ 1328/ 3200]\n",
      "loss: 1.387373  [ 1344/ 3200]\n",
      "loss: 1.380279  [ 1360/ 3200]\n",
      "loss: 1.380031  [ 1376/ 3200]\n",
      "loss: 1.383732  [ 1392/ 3200]\n",
      "loss: 1.389559  [ 1408/ 3200]\n",
      "loss: 1.386874  [ 1424/ 3200]\n",
      "loss: 1.386662  [ 1440/ 3200]\n",
      "loss: 1.383213  [ 1456/ 3200]\n",
      "loss: 1.384917  [ 1472/ 3200]\n",
      "loss: 1.382713  [ 1488/ 3200]\n",
      "loss: 1.386033  [ 1504/ 3200]\n",
      "loss: 1.383329  [ 1520/ 3200]\n",
      "loss: 1.383711  [ 1536/ 3200]\n",
      "loss: 1.389309  [ 1552/ 3200]\n",
      "loss: 1.386280  [ 1568/ 3200]\n",
      "loss: 1.389671  [ 1584/ 3200]\n",
      "loss: 1.383826  [ 1600/ 3200]\n",
      "loss: 1.386374  [ 1616/ 3200]\n",
      "loss: 1.388236  [ 1632/ 3200]\n",
      "loss: 1.386008  [ 1648/ 3200]\n",
      "loss: 1.393237  [ 1664/ 3200]\n",
      "loss: 1.380395  [ 1680/ 3200]\n",
      "loss: 1.386985  [ 1696/ 3200]\n",
      "loss: 1.386871  [ 1712/ 3200]\n",
      "loss: 1.381759  [ 1728/ 3200]\n",
      "loss: 1.376621  [ 1744/ 3200]\n",
      "loss: 1.389805  [ 1760/ 3200]\n",
      "loss: 1.385918  [ 1776/ 3200]\n",
      "loss: 1.381757  [ 1792/ 3200]\n",
      "loss: 1.383347  [ 1808/ 3200]\n",
      "loss: 1.394106  [ 1824/ 3200]\n",
      "loss: 1.390442  [ 1840/ 3200]\n",
      "loss: 1.386781  [ 1856/ 3200]\n",
      "loss: 1.388213  [ 1872/ 3200]\n",
      "loss: 1.385804  [ 1888/ 3200]\n",
      "loss: 1.392870  [ 1904/ 3200]\n",
      "loss: 1.388733  [ 1920/ 3200]\n",
      "loss: 1.384170  [ 1936/ 3200]\n",
      "loss: 1.391301  [ 1952/ 3200]\n",
      "loss: 1.383579  [ 1968/ 3200]\n",
      "loss: 1.389573  [ 1984/ 3200]\n",
      "loss: 1.383693  [ 2000/ 3200]\n",
      "loss: 1.388348  [ 2016/ 3200]\n",
      "loss: 1.392480  [ 2032/ 3200]\n",
      "loss: 1.384942  [ 2048/ 3200]\n",
      "loss: 1.383693  [ 2064/ 3200]\n",
      "loss: 1.394204  [ 2080/ 3200]\n",
      "loss: 1.385191  [ 2096/ 3200]\n",
      "loss: 1.393815  [ 2112/ 3200]\n",
      "loss: 1.389162  [ 2128/ 3200]\n",
      "loss: 1.386006  [ 2144/ 3200]\n",
      "loss: 1.387368  [ 2160/ 3200]\n",
      "loss: 1.388731  [ 2176/ 3200]\n",
      "loss: 1.386869  [ 2192/ 3200]\n",
      "loss: 1.386005  [ 2208/ 3200]\n",
      "loss: 1.383583  [ 2224/ 3200]\n",
      "loss: 1.391883  [ 2240/ 3200]\n",
      "loss: 1.382134  [ 2256/ 3200]\n",
      "loss: 1.386505  [ 2272/ 3200]\n",
      "loss: 1.389460  [ 2288/ 3200]\n",
      "loss: 1.382831  [ 2304/ 3200]\n",
      "loss: 1.382718  [ 2320/ 3200]\n",
      "loss: 1.379541  [ 2336/ 3200]\n",
      "loss: 1.386394  [ 2352/ 3200]\n",
      "loss: 1.384946  [ 2368/ 3200]\n",
      "loss: 1.379903  [ 2384/ 3200]\n",
      "loss: 1.382270  [ 2400/ 3200]\n",
      "loss: 1.385446  [ 2416/ 3200]\n",
      "loss: 1.381656  [ 2432/ 3200]\n",
      "loss: 1.387733  [ 2448/ 3200]\n",
      "loss: 1.386869  [ 2464/ 3200]\n",
      "loss: 1.388735  [ 2480/ 3200]\n",
      "loss: 1.387646  [ 2496/ 3200]\n",
      "loss: 1.387732  [ 2512/ 3200]\n",
      "loss: 1.383580  [ 2528/ 3200]\n",
      "loss: 1.390045  [ 2544/ 3200]\n",
      "loss: 1.386869  [ 2560/ 3200]\n",
      "loss: 1.390184  [ 2576/ 3200]\n",
      "loss: 1.386420  [ 2592/ 3200]\n",
      "loss: 1.392356  [ 2608/ 3200]\n",
      "loss: 1.393829  [ 2624/ 3200]\n",
      "loss: 1.386894  [ 2640/ 3200]\n",
      "loss: 1.379936  [ 2656/ 3200]\n",
      "loss: 1.386869  [ 2672/ 3200]\n",
      "loss: 1.393128  [ 2688/ 3200]\n",
      "loss: 1.389679  [ 2704/ 3200]\n",
      "loss: 1.388704  [ 2720/ 3200]\n",
      "loss: 1.392349  [ 2736/ 3200]\n",
      "loss: 1.388142  [ 2752/ 3200]\n",
      "loss: 1.382113  [ 2768/ 3200]\n",
      "loss: 1.387229  [ 2784/ 3200]\n",
      "loss: 1.388818  [ 2800/ 3200]\n",
      "loss: 1.381389  [ 2816/ 3200]\n",
      "loss: 1.386534  [ 2832/ 3200]\n",
      "loss: 1.389065  [ 2848/ 3200]\n",
      "loss: 1.390537  [ 2864/ 3200]\n",
      "loss: 1.388341  [ 2880/ 3200]\n",
      "loss: 1.380916  [ 2896/ 3200]\n",
      "loss: 1.383112  [ 2912/ 3200]\n",
      "loss: 1.391285  [ 2928/ 3200]\n",
      "loss: 1.390036  [ 2944/ 3200]\n",
      "loss: 1.388001  [ 2960/ 3200]\n",
      "loss: 1.392254  [ 2976/ 3200]\n",
      "loss: 1.388726  [ 2992/ 3200]\n",
      "loss: 1.390150  [ 3008/ 3200]\n",
      "loss: 1.383228  [ 3024/ 3200]\n",
      "loss: 1.385150  [ 3040/ 3200]\n",
      "loss: 1.389560  [ 3056/ 3200]\n",
      "loss: 1.392226  [ 3072/ 3200]\n",
      "loss: 1.377144  [ 3088/ 3200]\n",
      "loss: 1.386867  [ 3104/ 3200]\n",
      "loss: 1.382728  [ 3120/ 3200]\n",
      "loss: 1.386779  [ 3136/ 3200]\n",
      "loss: 1.388226  [ 3152/ 3200]\n",
      "loss: 1.384318  [ 3168/ 3200]\n",
      "loss: 1.388700  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086650\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.8479e-04.\n",
      "\n",
      "Epoch: 39\n",
      "-----------------------------\n",
      "loss: 1.387252  [    0/ 3200]\n",
      "loss: 1.385920  [   16/ 3200]\n",
      "loss: 1.389699  [   32/ 3200]\n",
      "loss: 1.388585  [   48/ 3200]\n",
      "loss: 1.387865  [   64/ 3200]\n",
      "loss: 1.386256  [   80/ 3200]\n",
      "loss: 1.387137  [   96/ 3200]\n",
      "loss: 1.383115  [  112/ 3200]\n",
      "loss: 1.383114  [  128/ 3200]\n",
      "loss: 1.384920  [  144/ 3200]\n",
      "loss: 1.387251  [  160/ 3200]\n",
      "loss: 1.384562  [  176/ 3200]\n",
      "loss: 1.396355  [  192/ 3200]\n",
      "loss: 1.391358  [  208/ 3200]\n",
      "loss: 1.389199  [  224/ 3200]\n",
      "loss: 1.394517  [  240/ 3200]\n",
      "loss: 1.380430  [  256/ 3200]\n",
      "loss: 1.390144  [  272/ 3200]\n",
      "loss: 1.386275  [  288/ 3200]\n",
      "loss: 1.386512  [  304/ 3200]\n",
      "loss: 1.388814  [  320/ 3200]\n",
      "loss: 1.392799  [  336/ 3200]\n",
      "loss: 1.388695  [  352/ 3200]\n",
      "loss: 1.383118  [  368/ 3200]\n",
      "loss: 1.381819  [  384/ 3200]\n",
      "loss: 1.383028  [  400/ 3200]\n",
      "loss: 1.380933  [  416/ 3200]\n",
      "loss: 1.383498  [  432/ 3200]\n",
      "loss: 1.387749  [  448/ 3200]\n",
      "loss: 1.384684  [  464/ 3200]\n",
      "loss: 1.386012  [  480/ 3200]\n",
      "loss: 1.380813  [  496/ 3200]\n",
      "loss: 1.383115  [  512/ 3200]\n",
      "loss: 1.390026  [  528/ 3200]\n",
      "loss: 1.381814  [  544/ 3200]\n",
      "loss: 1.391147  [  560/ 3200]\n",
      "loss: 1.383590  [  576/ 3200]\n",
      "loss: 1.384851  [  592/ 3200]\n",
      "loss: 1.383969  [  608/ 3200]\n",
      "loss: 1.390051  [  624/ 3200]\n",
      "loss: 1.394536  [  640/ 3200]\n",
      "loss: 1.384211  [  656/ 3200]\n",
      "loss: 1.382262  [  672/ 3200]\n",
      "loss: 1.382737  [  688/ 3200]\n",
      "loss: 1.384686  [  704/ 3200]\n",
      "loss: 1.387488  [  720/ 3200]\n",
      "loss: 1.389456  [  736/ 3200]\n",
      "loss: 1.387842  [  752/ 3200]\n",
      "loss: 1.383734  [  768/ 3200]\n",
      "loss: 1.391401  [  784/ 3200]\n",
      "loss: 1.388220  [  800/ 3200]\n",
      "loss: 1.384825  [  816/ 3200]\n",
      "loss: 1.389648  [  832/ 3200]\n",
      "loss: 1.387246  [  848/ 3200]\n",
      "loss: 1.389094  [  864/ 3200]\n",
      "loss: 1.385540  [  880/ 3200]\n",
      "loss: 1.384688  [  896/ 3200]\n",
      "loss: 1.389647  [  912/ 3200]\n",
      "loss: 1.389450  [  928/ 3200]\n",
      "loss: 1.384944  [  944/ 3200]\n",
      "loss: 1.385896  [  960/ 3200]\n",
      "loss: 1.383360  [  976/ 3200]\n",
      "loss: 1.382266  [  992/ 3200]\n",
      "loss: 1.387366  [ 1008/ 3200]\n",
      "loss: 1.389547  [ 1024/ 3200]\n",
      "loss: 1.386253  [ 1040/ 3200]\n",
      "loss: 1.380320  [ 1056/ 3200]\n",
      "loss: 1.390144  [ 1072/ 3200]\n",
      "loss: 1.386514  [ 1088/ 3200]\n",
      "loss: 1.387743  [ 1104/ 3200]\n",
      "loss: 1.381769  [ 1120/ 3200]\n",
      "loss: 1.384069  [ 1136/ 3200]\n",
      "loss: 1.391945  [ 1152/ 3200]\n",
      "loss: 1.391467  [ 1168/ 3200]\n",
      "loss: 1.392199  [ 1184/ 3200]\n",
      "loss: 1.388692  [ 1200/ 3200]\n",
      "loss: 1.385446  [ 1216/ 3200]\n",
      "loss: 1.382364  [ 1232/ 3200]\n",
      "loss: 1.383741  [ 1248/ 3200]\n",
      "loss: 1.387365  [ 1264/ 3200]\n",
      "loss: 1.385540  [ 1280/ 3200]\n",
      "loss: 1.390044  [ 1296/ 3200]\n",
      "loss: 1.390871  [ 1312/ 3200]\n",
      "loss: 1.387838  [ 1328/ 3200]\n",
      "loss: 1.382296  [ 1344/ 3200]\n",
      "loss: 1.384712  [ 1360/ 3200]\n",
      "loss: 1.383359  [ 1376/ 3200]\n",
      "loss: 1.389925  [ 1392/ 3200]\n",
      "loss: 1.385066  [ 1408/ 3200]\n",
      "loss: 1.390159  [ 1424/ 3200]\n",
      "loss: 1.387742  [ 1440/ 3200]\n",
      "loss: 1.388952  [ 1456/ 3200]\n",
      "loss: 1.388690  [ 1472/ 3200]\n",
      "loss: 1.385540  [ 1488/ 3200]\n",
      "loss: 1.387720  [ 1504/ 3200]\n",
      "loss: 1.388215  [ 1520/ 3200]\n",
      "loss: 1.389542  [ 1536/ 3200]\n",
      "loss: 1.380808  [ 1552/ 3200]\n",
      "loss: 1.381325  [ 1568/ 3200]\n",
      "loss: 1.381917  [ 1584/ 3200]\n",
      "loss: 1.387957  [ 1600/ 3200]\n",
      "loss: 1.383856  [ 1616/ 3200]\n",
      "loss: 1.387246  [ 1632/ 3200]\n",
      "loss: 1.388353  [ 1648/ 3200]\n",
      "loss: 1.382255  [ 1664/ 3200]\n",
      "loss: 1.382273  [ 1680/ 3200]\n",
      "loss: 1.389028  [ 1696/ 3200]\n",
      "loss: 1.383220  [ 1712/ 3200]\n",
      "loss: 1.393640  [ 1728/ 3200]\n",
      "loss: 1.385827  [ 1744/ 3200]\n",
      "loss: 1.382864  [ 1760/ 3200]\n",
      "loss: 1.385896  [ 1776/ 3200]\n",
      "loss: 1.382863  [ 1792/ 3200]\n",
      "loss: 1.386012  [ 1808/ 3200]\n",
      "loss: 1.381800  [ 1824/ 3200]\n",
      "loss: 1.384950  [ 1840/ 3200]\n",
      "loss: 1.383242  [ 1856/ 3200]\n",
      "loss: 1.390514  [ 1872/ 3200]\n",
      "loss: 1.391839  [ 1888/ 3200]\n",
      "loss: 1.375974  [ 1904/ 3200]\n",
      "loss: 1.379146  [ 1920/ 3200]\n",
      "loss: 1.384568  [ 1936/ 3200]\n",
      "loss: 1.389071  [ 1952/ 3200]\n",
      "loss: 1.386393  [ 1968/ 3200]\n",
      "loss: 1.385423  [ 1984/ 3200]\n",
      "loss: 1.383715  [ 2000/ 3200]\n",
      "loss: 1.380447  [ 2016/ 3200]\n",
      "loss: 1.387007  [ 2032/ 3200]\n",
      "loss: 1.385805  [ 2048/ 3200]\n",
      "loss: 1.385423  [ 2064/ 3200]\n",
      "loss: 1.388193  [ 2080/ 3200]\n",
      "loss: 1.391343  [ 2096/ 3200]\n",
      "loss: 1.389660  [ 2112/ 3200]\n",
      "loss: 1.392783  [ 2128/ 3200]\n",
      "loss: 1.392219  [ 2144/ 3200]\n",
      "loss: 1.380451  [ 2160/ 3200]\n",
      "loss: 1.384595  [ 2176/ 3200]\n",
      "loss: 1.383741  [ 2192/ 3200]\n",
      "loss: 1.381445  [ 2208/ 3200]\n",
      "loss: 1.382273  [ 2224/ 3200]\n",
      "loss: 1.388424  [ 2240/ 3200]\n",
      "loss: 1.383000  [ 2256/ 3200]\n",
      "loss: 1.388054  [ 2272/ 3200]\n",
      "loss: 1.384185  [ 2288/ 3200]\n",
      "loss: 1.389072  [ 2304/ 3200]\n",
      "loss: 1.386750  [ 2320/ 3200]\n",
      "loss: 1.383242  [ 2336/ 3200]\n",
      "loss: 1.386863  [ 2352/ 3200]\n",
      "loss: 1.391724  [ 2368/ 3200]\n",
      "loss: 1.385538  [ 2384/ 3200]\n",
      "loss: 1.388216  [ 2400/ 3200]\n",
      "loss: 1.388275  [ 2416/ 3200]\n",
      "loss: 1.385652  [ 2432/ 3200]\n",
      "loss: 1.382031  [ 2448/ 3200]\n",
      "loss: 1.382744  [ 2464/ 3200]\n",
      "loss: 1.390868  [ 2480/ 3200]\n",
      "loss: 1.383213  [ 2496/ 3200]\n",
      "loss: 1.386892  [ 2512/ 3200]\n",
      "loss: 1.384985  [ 2528/ 3200]\n",
      "loss: 1.379482  [ 2544/ 3200]\n",
      "loss: 1.383985  [ 2560/ 3200]\n",
      "loss: 1.387248  [ 2576/ 3200]\n",
      "loss: 1.388573  [ 2592/ 3200]\n",
      "loss: 1.386862  [ 2608/ 3200]\n",
      "loss: 1.382274  [ 2624/ 3200]\n",
      "loss: 1.385537  [ 2640/ 3200]\n",
      "loss: 1.386507  [ 2656/ 3200]\n",
      "loss: 1.391836  [ 2672/ 3200]\n",
      "loss: 1.382630  [ 2688/ 3200]\n",
      "loss: 1.384485  [ 2704/ 3200]\n",
      "loss: 1.376945  [ 2720/ 3200]\n",
      "loss: 1.389543  [ 2736/ 3200]\n",
      "loss: 1.386507  [ 2752/ 3200]\n",
      "loss: 1.395372  [ 2768/ 3200]\n",
      "loss: 1.391010  [ 2784/ 3200]\n",
      "loss: 1.389540  [ 2800/ 3200]\n",
      "loss: 1.383829  [ 2816/ 3200]\n",
      "loss: 1.394042  [ 2832/ 3200]\n",
      "loss: 1.383602  [ 2848/ 3200]\n",
      "loss: 1.387860  [ 2864/ 3200]\n",
      "loss: 1.382747  [ 2880/ 3200]\n",
      "loss: 1.393299  [ 2896/ 3200]\n",
      "loss: 1.383133  [ 2912/ 3200]\n",
      "loss: 1.387830  [ 2928/ 3200]\n",
      "loss: 1.386421  [ 2944/ 3200]\n",
      "loss: 1.386008  [ 2960/ 3200]\n",
      "loss: 1.387247  [ 2976/ 3200]\n",
      "loss: 1.384457  [ 2992/ 3200]\n",
      "loss: 1.383859  [ 3008/ 3200]\n",
      "loss: 1.385922  [ 3024/ 3200]\n",
      "loss: 1.385810  [ 3040/ 3200]\n",
      "loss: 1.391360  [ 3056/ 3200]\n",
      "loss: 1.386035  [ 3072/ 3200]\n",
      "loss: 1.382663  [ 3088/ 3200]\n",
      "loss: 1.388709  [ 3104/ 3200]\n",
      "loss: 1.391000  [ 3120/ 3200]\n",
      "loss: 1.389871  [ 3136/ 3200]\n",
      "loss: 1.386863  [ 3152/ 3200]\n",
      "loss: 1.386392  [ 3168/ 3200]\n",
      "loss: 1.387002  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.7055e-04.\n",
      "\n",
      "Epoch: 40\n",
      "-----------------------------\n",
      "loss: 1.388212  [    0/ 3200]\n",
      "loss: 1.388795  [   16/ 3200]\n",
      "loss: 1.384485  [   32/ 3200]\n",
      "loss: 1.388300  [   48/ 3200]\n",
      "loss: 1.387383  [   64/ 3200]\n",
      "loss: 1.391554  [   80/ 3200]\n",
      "loss: 1.384684  [   96/ 3200]\n",
      "loss: 1.387851  [  112/ 3200]\n",
      "loss: 1.380467  [  128/ 3200]\n",
      "loss: 1.382284  [  144/ 3200]\n",
      "loss: 1.385452  [  160/ 3200]\n",
      "loss: 1.387246  [  176/ 3200]\n",
      "loss: 1.386863  [  192/ 3200]\n",
      "loss: 1.392294  [  208/ 3200]\n",
      "loss: 1.391463  [  224/ 3200]\n",
      "loss: 1.382756  [  240/ 3200]\n",
      "loss: 1.384189  [  256/ 3200]\n",
      "loss: 1.384708  [  272/ 3200]\n",
      "loss: 1.390414  [  288/ 3200]\n",
      "loss: 1.377241  [  304/ 3200]\n",
      "loss: 1.392294  [  320/ 3200]\n",
      "loss: 1.393663  [  336/ 3200]\n",
      "loss: 1.382758  [  352/ 3200]\n",
      "loss: 1.391351  [  368/ 3200]\n",
      "loss: 1.384324  [  384/ 3200]\n",
      "loss: 1.389041  [  400/ 3200]\n",
      "loss: 1.391928  [  416/ 3200]\n",
      "loss: 1.387354  [  432/ 3200]\n",
      "loss: 1.386392  [  448/ 3200]\n",
      "loss: 1.384938  [  464/ 3200]\n",
      "loss: 1.390494  [  480/ 3200]\n",
      "loss: 1.385755  [  496/ 3200]\n",
      "loss: 1.386521  [  512/ 3200]\n",
      "loss: 1.388398  [  528/ 3200]\n",
      "loss: 1.391431  [  544/ 3200]\n",
      "loss: 1.390000  [  560/ 3200]\n",
      "loss: 1.393712  [  576/ 3200]\n",
      "loss: 1.391426  [  592/ 3200]\n",
      "loss: 1.387351  [  608/ 3200]\n",
      "loss: 1.392200  [  624/ 3200]\n",
      "loss: 1.380973  [  640/ 3200]\n",
      "loss: 1.388674  [  656/ 3200]\n",
      "loss: 1.378199  [  672/ 3200]\n",
      "loss: 1.383617  [  688/ 3200]\n",
      "loss: 1.390957  [  704/ 3200]\n",
      "loss: 1.386312  [  720/ 3200]\n",
      "loss: 1.384577  [  736/ 3200]\n",
      "loss: 1.389141  [  752/ 3200]\n",
      "loss: 1.391915  [  768/ 3200]\n",
      "loss: 1.387351  [  784/ 3200]\n",
      "loss: 1.383747  [  800/ 3200]\n",
      "loss: 1.383643  [  816/ 3200]\n",
      "loss: 1.385068  [  832/ 3200]\n",
      "loss: 1.384110  [  848/ 3200]\n",
      "loss: 1.387270  [  864/ 3200]\n",
      "loss: 1.386778  [  880/ 3200]\n",
      "loss: 1.378201  [  896/ 3200]\n",
      "loss: 1.387350  [  912/ 3200]\n",
      "loss: 1.387862  [  928/ 3200]\n",
      "loss: 1.390872  [  944/ 3200]\n",
      "loss: 1.384682  [  960/ 3200]\n",
      "loss: 1.388956  [  976/ 3200]\n",
      "loss: 1.387246  [  992/ 3200]\n",
      "loss: 1.388203  [ 1008/ 3200]\n",
      "loss: 1.387629  [ 1024/ 3200]\n",
      "loss: 1.382038  [ 1040/ 3200]\n",
      "loss: 1.388099  [ 1056/ 3200]\n",
      "loss: 1.377248  [ 1072/ 3200]\n",
      "loss: 1.390120  [ 1088/ 3200]\n",
      "loss: 1.378572  [ 1104/ 3200]\n",
      "loss: 1.383361  [ 1120/ 3200]\n",
      "loss: 1.391792  [ 1136/ 3200]\n",
      "loss: 1.386287  [ 1152/ 3200]\n",
      "loss: 1.381809  [ 1168/ 3200]\n",
      "loss: 1.390466  [ 1184/ 3200]\n",
      "loss: 1.386007  [ 1200/ 3200]\n",
      "loss: 1.393025  [ 1216/ 3200]\n",
      "loss: 1.386391  [ 1232/ 3200]\n",
      "loss: 1.384111  [ 1248/ 3200]\n",
      "loss: 1.384942  [ 1264/ 3200]\n",
      "loss: 1.385454  [ 1280/ 3200]\n",
      "loss: 1.385347  [ 1296/ 3200]\n",
      "loss: 1.381721  [ 1312/ 3200]\n",
      "loss: 1.386500  [ 1328/ 3200]\n",
      "loss: 1.382659  [ 1344/ 3200]\n",
      "loss: 1.385539  [ 1360/ 3200]\n",
      "loss: 1.384579  [ 1376/ 3200]\n",
      "loss: 1.388182  [ 1392/ 3200]\n",
      "loss: 1.389994  [ 1408/ 3200]\n",
      "loss: 1.388289  [ 1424/ 3200]\n",
      "loss: 1.392297  [ 1440/ 3200]\n",
      "loss: 1.387266  [ 1456/ 3200]\n",
      "loss: 1.387135  [ 1472/ 3200]\n",
      "loss: 1.386838  [ 1488/ 3200]\n",
      "loss: 1.387649  [ 1504/ 3200]\n",
      "loss: 1.380620  [ 1520/ 3200]\n",
      "loss: 1.383042  [ 1536/ 3200]\n",
      "loss: 1.384940  [ 1552/ 3200]\n",
      "loss: 1.384690  [ 1568/ 3200]\n",
      "loss: 1.386479  [ 1584/ 3200]\n",
      "loss: 1.382682  [ 1600/ 3200]\n",
      "loss: 1.386368  [ 1616/ 3200]\n",
      "loss: 1.387572  [ 1632/ 3200]\n",
      "loss: 1.379551  [ 1648/ 3200]\n",
      "loss: 1.384110  [ 1664/ 3200]\n",
      "loss: 1.389524  [ 1680/ 3200]\n",
      "loss: 1.384688  [ 1696/ 3200]\n",
      "loss: 1.393702  [ 1712/ 3200]\n",
      "loss: 1.385899  [ 1728/ 3200]\n",
      "loss: 1.384219  [ 1744/ 3200]\n",
      "loss: 1.386391  [ 1760/ 3200]\n",
      "loss: 1.391335  [ 1776/ 3200]\n",
      "loss: 1.390759  [ 1792/ 3200]\n",
      "loss: 1.390013  [ 1808/ 3200]\n",
      "loss: 1.392673  [ 1824/ 3200]\n",
      "loss: 1.392180  [ 1840/ 3200]\n",
      "loss: 1.383132  [ 1856/ 3200]\n",
      "loss: 1.386772  [ 1872/ 3200]\n",
      "loss: 1.388200  [ 1888/ 3200]\n",
      "loss: 1.389739  [ 1904/ 3200]\n",
      "loss: 1.388419  [ 1920/ 3200]\n",
      "loss: 1.377369  [ 1936/ 3200]\n",
      "loss: 1.382325  [ 1952/ 3200]\n",
      "loss: 1.385433  [ 1968/ 3200]\n",
      "loss: 1.384386  [ 1984/ 3200]\n",
      "loss: 1.388940  [ 2000/ 3200]\n",
      "loss: 1.387441  [ 2016/ 3200]\n",
      "loss: 1.383644  [ 2032/ 3200]\n",
      "loss: 1.383133  [ 2048/ 3200]\n",
      "loss: 1.382794  [ 2064/ 3200]\n",
      "loss: 1.386881  [ 2080/ 3200]\n",
      "loss: 1.386971  [ 2096/ 3200]\n",
      "loss: 1.384942  [ 2112/ 3200]\n",
      "loss: 1.388200  [ 2128/ 3200]\n",
      "loss: 1.383864  [ 2144/ 3200]\n",
      "loss: 1.387839  [ 2160/ 3200]\n",
      "loss: 1.385902  [ 2176/ 3200]\n",
      "loss: 1.381835  [ 2192/ 3200]\n",
      "loss: 1.388689  [ 2208/ 3200]\n",
      "loss: 1.395865  [ 2224/ 3200]\n",
      "loss: 1.390837  [ 2240/ 3200]\n",
      "loss: 1.387118  [ 2256/ 3200]\n",
      "loss: 1.384133  [ 2272/ 3200]\n",
      "loss: 1.383644  [ 2288/ 3200]\n",
      "loss: 1.390458  [ 2304/ 3200]\n",
      "loss: 1.381695  [ 2320/ 3200]\n",
      "loss: 1.385541  [ 2336/ 3200]\n",
      "loss: 1.383606  [ 2352/ 3200]\n",
      "loss: 1.388199  [ 2368/ 3200]\n",
      "loss: 1.384006  [ 2384/ 3200]\n",
      "loss: 1.389027  [ 2400/ 3200]\n",
      "loss: 1.380499  [ 2416/ 3200]\n",
      "loss: 1.385432  [ 2432/ 3200]\n",
      "loss: 1.386281  [ 2448/ 3200]\n",
      "loss: 1.382663  [ 2464/ 3200]\n",
      "loss: 1.385543  [ 2480/ 3200]\n",
      "loss: 1.392262  [ 2496/ 3200]\n",
      "loss: 1.383599  [ 2512/ 3200]\n",
      "loss: 1.393134  [ 2528/ 3200]\n",
      "loss: 1.384963  [ 2544/ 3200]\n",
      "loss: 1.394536  [ 2560/ 3200]\n",
      "loss: 1.380499  [ 2576/ 3200]\n",
      "loss: 1.394447  [ 2592/ 3200]\n",
      "loss: 1.386746  [ 2608/ 3200]\n",
      "loss: 1.381840  [ 2624/ 3200]\n",
      "loss: 1.383268  [ 2640/ 3200]\n",
      "loss: 1.391815  [ 2656/ 3200]\n",
      "loss: 1.383623  [ 2672/ 3200]\n",
      "loss: 1.388553  [ 2688/ 3200]\n",
      "loss: 1.380880  [ 2704/ 3200]\n",
      "loss: 1.386884  [ 2720/ 3200]\n",
      "loss: 1.379096  [ 2736/ 3200]\n",
      "loss: 1.381107  [ 2752/ 3200]\n",
      "loss: 1.379093  [ 2768/ 3200]\n",
      "loss: 1.379068  [ 2784/ 3200]\n",
      "loss: 1.380966  [ 2800/ 3200]\n",
      "loss: 1.386860  [ 2816/ 3200]\n",
      "loss: 1.397665  [ 2832/ 3200]\n",
      "loss: 1.388199  [ 2848/ 3200]\n",
      "loss: 1.389161  [ 2864/ 3200]\n",
      "loss: 1.383497  [ 2880/ 3200]\n",
      "loss: 1.388643  [ 2896/ 3200]\n",
      "loss: 1.383154  [ 2912/ 3200]\n",
      "loss: 1.385430  [ 2928/ 3200]\n",
      "loss: 1.388553  [ 2944/ 3200]\n",
      "loss: 1.379537  [ 2960/ 3200]\n",
      "loss: 1.390476  [ 2976/ 3200]\n",
      "loss: 1.383788  [ 2992/ 3200]\n",
      "loss: 1.378715  [ 3008/ 3200]\n",
      "loss: 1.383760  [ 3024/ 3200]\n",
      "loss: 1.382798  [ 3040/ 3200]\n",
      "loss: 1.391210  [ 3056/ 3200]\n",
      "loss: 1.391299  [ 3072/ 3200]\n",
      "loss: 1.378338  [ 3088/ 3200]\n",
      "loss: 1.387352  [ 3104/ 3200]\n",
      "loss: 1.390362  [ 3120/ 3200]\n",
      "loss: 1.384722  [ 3136/ 3200]\n",
      "loss: 1.384674  [ 3152/ 3200]\n",
      "loss: 1.384720  [ 3168/ 3200]\n",
      "loss: 1.391412  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.5702e-04.\n",
      "\n",
      "Epoch: 41\n",
      "-----------------------------\n",
      "loss: 1.383735  [    0/ 3200]\n",
      "loss: 1.386035  [   16/ 3200]\n",
      "loss: 1.392861  [   32/ 3200]\n",
      "loss: 1.389982  [   48/ 3200]\n",
      "loss: 1.385653  [   64/ 3200]\n",
      "loss: 1.389538  [   80/ 3200]\n",
      "loss: 1.387840  [   96/ 3200]\n",
      "loss: 1.385542  [  112/ 3200]\n",
      "loss: 1.388066  [  128/ 3200]\n",
      "loss: 1.381462  [  144/ 3200]\n",
      "loss: 1.388579  [  160/ 3200]\n",
      "loss: 1.383071  [  176/ 3200]\n",
      "loss: 1.388197  [  192/ 3200]\n",
      "loss: 1.385901  [  208/ 3200]\n",
      "loss: 1.378967  [  224/ 3200]\n",
      "loss: 1.385810  [  240/ 3200]\n",
      "loss: 1.385544  [  256/ 3200]\n",
      "loss: 1.386368  [  272/ 3200]\n",
      "loss: 1.385544  [  288/ 3200]\n",
      "loss: 1.384093  [  304/ 3200]\n",
      "loss: 1.392167  [  320/ 3200]\n",
      "loss: 1.385455  [  336/ 3200]\n",
      "loss: 1.385544  [  352/ 3200]\n",
      "loss: 1.393948  [  368/ 3200]\n",
      "loss: 1.384471  [  384/ 3200]\n",
      "loss: 1.384495  [  400/ 3200]\n",
      "loss: 1.386391  [  416/ 3200]\n",
      "loss: 1.391316  [  432/ 3200]\n",
      "loss: 1.390116  [  448/ 3200]\n",
      "loss: 1.384117  [  464/ 3200]\n",
      "loss: 1.391314  [  480/ 3200]\n",
      "loss: 1.390114  [  496/ 3200]\n",
      "loss: 1.384232  [  512/ 3200]\n",
      "loss: 1.381000  [  528/ 3200]\n",
      "loss: 1.381844  [  544/ 3200]\n",
      "loss: 1.384586  [  560/ 3200]\n",
      "loss: 1.387215  [  576/ 3200]\n",
      "loss: 1.386745  [  592/ 3200]\n",
      "loss: 1.388664  [  608/ 3200]\n",
      "loss: 1.385099  [  624/ 3200]\n",
      "loss: 1.387818  [  640/ 3200]\n",
      "loss: 1.386504  [  656/ 3200]\n",
      "loss: 1.383385  [  672/ 3200]\n",
      "loss: 1.383251  [  688/ 3200]\n",
      "loss: 1.387105  [  704/ 3200]\n",
      "loss: 1.387238  [  720/ 3200]\n",
      "loss: 1.378237  [  736/ 3200]\n",
      "loss: 1.381823  [  752/ 3200]\n",
      "loss: 1.388575  [  768/ 3200]\n",
      "loss: 1.386391  [  784/ 3200]\n",
      "loss: 1.379683  [  800/ 3200]\n",
      "loss: 1.388174  [  816/ 3200]\n",
      "loss: 1.384117  [  832/ 3200]\n",
      "loss: 1.385544  [  848/ 3200]\n",
      "loss: 1.386502  [  864/ 3200]\n",
      "loss: 1.388863  [  880/ 3200]\n",
      "loss: 1.382335  [  896/ 3200]\n",
      "loss: 1.388554  [  912/ 3200]\n",
      "loss: 1.389643  [  928/ 3200]\n",
      "loss: 1.384696  [  944/ 3200]\n",
      "loss: 1.380041  [  960/ 3200]\n",
      "loss: 1.385902  [  976/ 3200]\n",
      "loss: 1.383629  [  992/ 3200]\n",
      "loss: 1.386967  [ 1008/ 3200]\n",
      "loss: 1.389978  [ 1024/ 3200]\n",
      "loss: 1.387791  [ 1040/ 3200]\n",
      "loss: 1.385518  [ 1056/ 3200]\n",
      "loss: 1.384610  [ 1072/ 3200]\n",
      "loss: 1.390850  [ 1088/ 3200]\n",
      "loss: 1.391805  [ 1104/ 3200]\n",
      "loss: 1.385053  [ 1120/ 3200]\n",
      "loss: 1.389486  [ 1136/ 3200]\n",
      "loss: 1.383762  [ 1152/ 3200]\n",
      "loss: 1.388661  [ 1168/ 3200]\n",
      "loss: 1.386366  [ 1184/ 3200]\n",
      "loss: 1.387729  [ 1200/ 3200]\n",
      "loss: 1.389151  [ 1216/ 3200]\n",
      "loss: 1.390465  [ 1232/ 3200]\n",
      "loss: 1.385185  [ 1248/ 3200]\n",
      "loss: 1.386856  [ 1264/ 3200]\n",
      "loss: 1.389149  [ 1280/ 3200]\n",
      "loss: 1.386749  [ 1296/ 3200]\n",
      "loss: 1.385673  [ 1312/ 3200]\n",
      "loss: 1.387705  [ 1328/ 3200]\n",
      "loss: 1.389125  [ 1344/ 3200]\n",
      "loss: 1.388193  [ 1360/ 3200]\n",
      "loss: 1.385183  [ 1376/ 3200]\n",
      "loss: 1.383273  [ 1392/ 3200]\n",
      "loss: 1.376468  [ 1408/ 3200]\n",
      "loss: 1.389974  [ 1424/ 3200]\n",
      "loss: 1.383633  [ 1440/ 3200]\n",
      "loss: 1.391883  [ 1456/ 3200]\n",
      "loss: 1.390717  [ 1472/ 3200]\n",
      "loss: 1.385542  [ 1488/ 3200]\n",
      "loss: 1.385819  [ 1504/ 3200]\n",
      "loss: 1.390354  [ 1520/ 3200]\n",
      "loss: 1.386856  [ 1536/ 3200]\n",
      "loss: 1.387452  [ 1552/ 3200]\n",
      "loss: 1.390460  [ 1568/ 3200]\n",
      "loss: 1.385460  [ 1584/ 3200]\n",
      "loss: 1.384123  [ 1600/ 3200]\n",
      "loss: 1.389146  [ 1616/ 3200]\n",
      "loss: 1.383657  [ 1632/ 3200]\n",
      "loss: 1.385183  [ 1648/ 3200]\n",
      "loss: 1.393192  [ 1664/ 3200]\n",
      "loss: 1.386390  [ 1680/ 3200]\n",
      "loss: 1.382810  [ 1696/ 3200]\n",
      "loss: 1.388657  [ 1712/ 3200]\n",
      "loss: 1.384589  [ 1728/ 3200]\n",
      "loss: 1.394969  [ 1744/ 3200]\n",
      "loss: 1.382324  [ 1760/ 3200]\n",
      "loss: 1.387343  [ 1776/ 3200]\n",
      "loss: 1.390352  [ 1792/ 3200]\n",
      "loss: 1.381289  [ 1808/ 3200]\n",
      "loss: 1.390456  [ 1824/ 3200]\n",
      "loss: 1.388571  [ 1840/ 3200]\n",
      "loss: 1.386390  [ 1856/ 3200]\n",
      "loss: 1.387236  [ 1872/ 3200]\n",
      "loss: 1.389608  [ 1888/ 3200]\n",
      "loss: 1.387788  [ 1904/ 3200]\n",
      "loss: 1.386855  [ 1920/ 3200]\n",
      "loss: 1.383279  [ 1936/ 3200]\n",
      "loss: 1.385438  [ 1952/ 3200]\n",
      "loss: 1.386772  [ 1968/ 3200]\n",
      "loss: 1.383279  [ 1984/ 3200]\n",
      "loss: 1.384253  [ 2000/ 3200]\n",
      "loss: 1.386751  [ 2016/ 3200]\n",
      "loss: 1.385056  [ 2032/ 3200]\n",
      "loss: 1.379214  [ 2048/ 3200]\n",
      "loss: 1.388295  [ 2064/ 3200]\n",
      "loss: 1.385077  [ 2080/ 3200]\n",
      "loss: 1.382535  [ 2096/ 3200]\n",
      "loss: 1.386028  [ 2112/ 3200]\n",
      "loss: 1.379130  [ 2128/ 3200]\n",
      "loss: 1.389058  [ 2144/ 3200]\n",
      "loss: 1.383639  [ 2160/ 3200]\n",
      "loss: 1.384953  [ 2176/ 3200]\n",
      "loss: 1.387238  [ 2192/ 3200]\n",
      "loss: 1.383639  [ 2208/ 3200]\n",
      "loss: 1.383743  [ 2224/ 3200]\n",
      "loss: 1.384611  [ 2240/ 3200]\n",
      "loss: 1.386009  [ 2256/ 3200]\n",
      "loss: 1.385800  [ 2272/ 3200]\n",
      "loss: 1.384124  [ 2288/ 3200]\n",
      "loss: 1.388001  [ 2304/ 3200]\n",
      "loss: 1.391682  [ 2320/ 3200]\n",
      "loss: 1.390922  [ 2336/ 3200]\n",
      "loss: 1.387703  [ 2352/ 3200]\n",
      "loss: 1.389015  [ 2368/ 3200]\n",
      "loss: 1.389500  [ 2384/ 3200]\n",
      "loss: 1.390812  [ 2400/ 3200]\n",
      "loss: 1.385923  [ 2416/ 3200]\n",
      "loss: 1.389228  [ 2432/ 3200]\n",
      "loss: 1.386963  [ 2448/ 3200]\n",
      "loss: 1.390345  [ 2464/ 3200]\n",
      "loss: 1.378755  [ 2480/ 3200]\n",
      "loss: 1.388208  [ 2496/ 3200]\n",
      "loss: 1.387682  [ 2512/ 3200]\n",
      "loss: 1.388653  [ 2528/ 3200]\n",
      "loss: 1.384127  [ 2544/ 3200]\n",
      "loss: 1.388060  [ 2560/ 3200]\n",
      "loss: 1.389227  [ 2576/ 3200]\n",
      "loss: 1.391210  [ 2592/ 3200]\n",
      "loss: 1.387321  [ 2608/ 3200]\n",
      "loss: 1.386390  [ 2624/ 3200]\n",
      "loss: 1.384127  [ 2640/ 3200]\n",
      "loss: 1.385458  [ 2656/ 3200]\n",
      "loss: 1.386390  [ 2672/ 3200]\n",
      "loss: 1.387234  [ 2688/ 3200]\n",
      "loss: 1.384236  [ 2704/ 3200]\n",
      "loss: 1.387788  [ 2720/ 3200]\n",
      "loss: 1.384485  [ 2736/ 3200]\n",
      "loss: 1.375166  [ 2752/ 3200]\n",
      "loss: 1.380662  [ 2768/ 3200]\n",
      "loss: 1.381040  [ 2784/ 3200]\n",
      "loss: 1.386370  [ 2800/ 3200]\n",
      "loss: 1.387787  [ 2816/ 3200]\n",
      "loss: 1.377909  [ 2832/ 3200]\n",
      "loss: 1.388082  [ 2848/ 3200]\n",
      "loss: 1.385566  [ 2864/ 3200]\n",
      "loss: 1.383174  [ 2880/ 3200]\n",
      "loss: 1.390810  [ 2896/ 3200]\n",
      "loss: 1.376112  [ 2912/ 3200]\n",
      "loss: 1.389141  [ 2928/ 3200]\n",
      "loss: 1.389965  [ 2944/ 3200]\n",
      "loss: 1.390451  [ 2960/ 3200]\n",
      "loss: 1.386283  [ 2976/ 3200]\n",
      "loss: 1.386304  [ 2992/ 3200]\n",
      "loss: 1.386748  [ 3008/ 3200]\n",
      "loss: 1.387591  [ 3024/ 3200]\n",
      "loss: 1.389853  [ 3040/ 3200]\n",
      "loss: 1.390449  [ 3056/ 3200]\n",
      "loss: 1.384971  [ 3072/ 3200]\n",
      "loss: 1.391736  [ 3088/ 3200]\n",
      "loss: 1.383262  [ 3104/ 3200]\n",
      "loss: 1.382796  [ 3120/ 3200]\n",
      "loss: 1.387123  [ 3136/ 3200]\n",
      "loss: 1.385682  [ 3152/ 3200]\n",
      "loss: 1.387099  [ 3168/ 3200]\n",
      "loss: 1.388211  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.4417e-04.\n",
      "\n",
      "Epoch: 42\n",
      "-----------------------------\n",
      "loss: 1.385436  [    0/ 3200]\n",
      "loss: 1.391514  [   16/ 3200]\n",
      "loss: 1.389250  [   32/ 3200]\n",
      "loss: 1.389516  [   48/ 3200]\n",
      "loss: 1.387321  [   64/ 3200]\n",
      "loss: 1.385437  [   80/ 3200]\n",
      "loss: 1.389138  [   96/ 3200]\n",
      "loss: 1.388540  [  112/ 3200]\n",
      "loss: 1.390910  [  128/ 3200]\n",
      "loss: 1.384130  [  144/ 3200]\n",
      "loss: 1.387342  [  160/ 3200]\n",
      "loss: 1.381848  [  176/ 3200]\n",
      "loss: 1.389427  [  192/ 3200]\n",
      "loss: 1.386876  [  208/ 3200]\n",
      "loss: 1.386279  [  224/ 3200]\n",
      "loss: 1.393170  [  240/ 3200]\n",
      "loss: 1.386055  [  256/ 3200]\n",
      "loss: 1.390423  [  272/ 3200]\n",
      "loss: 1.391263  [  288/ 3200]\n",
      "loss: 1.386258  [  304/ 3200]\n",
      "loss: 1.387717  [  320/ 3200]\n",
      "loss: 1.389510  [  336/ 3200]\n",
      "loss: 1.384506  [  352/ 3200]\n",
      "loss: 1.391280  [  368/ 3200]\n",
      "loss: 1.379129  [  384/ 3200]\n",
      "loss: 1.387210  [  400/ 3200]\n",
      "loss: 1.386650  [  416/ 3200]\n",
      "loss: 1.387715  [  432/ 3200]\n",
      "loss: 1.386275  [  448/ 3200]\n",
      "loss: 1.384950  [  464/ 3200]\n",
      "loss: 1.388763  [  480/ 3200]\n",
      "loss: 1.381034  [  496/ 3200]\n",
      "loss: 1.385550  [  512/ 3200]\n",
      "loss: 1.380547  [  528/ 3200]\n",
      "loss: 1.391373  [  544/ 3200]\n",
      "loss: 1.382452  [  560/ 3200]\n",
      "loss: 1.378401  [  576/ 3200]\n",
      "loss: 1.383403  [  592/ 3200]\n",
      "loss: 1.384506  [  608/ 3200]\n",
      "loss: 1.385811  [  624/ 3200]\n",
      "loss: 1.381384  [  640/ 3200]\n",
      "loss: 1.382337  [  656/ 3200]\n",
      "loss: 1.383403  [  672/ 3200]\n",
      "loss: 1.387342  [  688/ 3200]\n",
      "loss: 1.389976  [  704/ 3200]\n",
      "loss: 1.392793  [  720/ 3200]\n",
      "loss: 1.381781  [  736/ 3200]\n",
      "loss: 1.390441  [  752/ 3200]\n",
      "loss: 1.383664  [  768/ 3200]\n",
      "loss: 1.394827  [  784/ 3200]\n",
      "loss: 1.385437  [  800/ 3200]\n",
      "loss: 1.384018  [  816/ 3200]\n",
      "loss: 1.384597  [  832/ 3200]\n",
      "loss: 1.387208  [  848/ 3200]\n",
      "loss: 1.389601  [  864/ 3200]\n",
      "loss: 1.386389  [  880/ 3200]\n",
      "loss: 1.387321  [  896/ 3200]\n",
      "loss: 1.386389  [  912/ 3200]\n",
      "loss: 1.385924  [  928/ 3200]\n",
      "loss: 1.386741  [  944/ 3200]\n",
      "loss: 1.383293  [  960/ 3200]\n",
      "loss: 1.389600  [  976/ 3200]\n",
      "loss: 1.381501  [  992/ 3200]\n",
      "loss: 1.384245  [ 1008/ 3200]\n",
      "loss: 1.387694  [ 1024/ 3200]\n",
      "loss: 1.387829  [ 1040/ 3200]\n",
      "loss: 1.390813  [ 1056/ 3200]\n",
      "loss: 1.390438  [ 1072/ 3200]\n",
      "loss: 1.390811  [ 1088/ 3200]\n",
      "loss: 1.384246  [ 1104/ 3200]\n",
      "loss: 1.386482  [ 1120/ 3200]\n",
      "loss: 1.384112  [ 1136/ 3200]\n",
      "loss: 1.391832  [ 1152/ 3200]\n",
      "loss: 1.388091  [ 1168/ 3200]\n",
      "loss: 1.384246  [ 1184/ 3200]\n",
      "loss: 1.384861  [ 1200/ 3200]\n",
      "loss: 1.379135  [ 1216/ 3200]\n",
      "loss: 1.387920  [ 1232/ 3200]\n",
      "loss: 1.383759  [ 1248/ 3200]\n",
      "loss: 1.383293  [ 1264/ 3200]\n",
      "loss: 1.389132  [ 1280/ 3200]\n",
      "loss: 1.382475  [ 1296/ 3200]\n",
      "loss: 1.382342  [ 1312/ 3200]\n",
      "loss: 1.393068  [ 1328/ 3200]\n",
      "loss: 1.385307  [ 1344/ 3200]\n",
      "loss: 1.385924  [ 1360/ 3200]\n",
      "loss: 1.392581  [ 1376/ 3200]\n",
      "loss: 1.389930  [ 1392/ 3200]\n",
      "loss: 1.390059  [ 1408/ 3200]\n",
      "loss: 1.390788  [ 1424/ 3200]\n",
      "loss: 1.379251  [ 1440/ 3200]\n",
      "loss: 1.388465  [ 1456/ 3200]\n",
      "loss: 1.391272  [ 1472/ 3200]\n",
      "loss: 1.387824  [ 1488/ 3200]\n",
      "loss: 1.382457  [ 1504/ 3200]\n",
      "loss: 1.390059  [ 1520/ 3200]\n",
      "loss: 1.382720  [ 1536/ 3200]\n",
      "loss: 1.386145  [ 1552/ 3200]\n",
      "loss: 1.382811  [ 1568/ 3200]\n",
      "loss: 1.385440  [ 1584/ 3200]\n",
      "loss: 1.385905  [ 1600/ 3200]\n",
      "loss: 1.388643  [ 1616/ 3200]\n",
      "loss: 1.383670  [ 1632/ 3200]\n",
      "loss: 1.386763  [ 1648/ 3200]\n",
      "loss: 1.382254  [ 1664/ 3200]\n",
      "loss: 1.382009  [ 1680/ 3200]\n",
      "loss: 1.382718  [ 1696/ 3200]\n",
      "loss: 1.384728  [ 1712/ 3200]\n",
      "loss: 1.385582  [ 1728/ 3200]\n",
      "loss: 1.380916  [ 1744/ 3200]\n",
      "loss: 1.384229  [ 1760/ 3200]\n",
      "loss: 1.391383  [ 1776/ 3200]\n",
      "loss: 1.384243  [ 1792/ 3200]\n",
      "loss: 1.384616  [ 1808/ 3200]\n",
      "loss: 1.385922  [ 1824/ 3200]\n",
      "loss: 1.388672  [ 1840/ 3200]\n",
      "loss: 1.390323  [ 1856/ 3200]\n",
      "loss: 1.382359  [ 1872/ 3200]\n",
      "loss: 1.382936  [ 1888/ 3200]\n",
      "loss: 1.388558  [ 1904/ 3200]\n",
      "loss: 1.379253  [ 1920/ 3200]\n",
      "loss: 1.382707  [ 1936/ 3200]\n",
      "loss: 1.393616  [ 1952/ 3200]\n",
      "loss: 1.380930  [ 1968/ 3200]\n",
      "loss: 1.384133  [ 1984/ 3200]\n",
      "loss: 1.381409  [ 2000/ 3200]\n",
      "loss: 1.386019  [ 2016/ 3200]\n",
      "loss: 1.381877  [ 2032/ 3200]\n",
      "loss: 1.383762  [ 2048/ 3200]\n",
      "loss: 1.386030  [ 2064/ 3200]\n",
      "loss: 1.395196  [ 2080/ 3200]\n",
      "loss: 1.389015  [ 2096/ 3200]\n",
      "loss: 1.384961  [ 2112/ 3200]\n",
      "loss: 1.383763  [ 2128/ 3200]\n",
      "loss: 1.388274  [ 2144/ 3200]\n",
      "loss: 1.384243  [ 2160/ 3200]\n",
      "loss: 1.391739  [ 2176/ 3200]\n",
      "loss: 1.384602  [ 2192/ 3200]\n",
      "loss: 1.387337  [ 2208/ 3200]\n",
      "loss: 1.377716  [ 2224/ 3200]\n",
      "loss: 1.385812  [ 2240/ 3200]\n",
      "loss: 1.392316  [ 2256/ 3200]\n",
      "loss: 1.382704  [ 2272/ 3200]\n",
      "loss: 1.388769  [ 2288/ 3200]\n",
      "loss: 1.384602  [ 2304/ 3200]\n",
      "loss: 1.384120  [ 2320/ 3200]\n",
      "loss: 1.384711  [ 2336/ 3200]\n",
      "loss: 1.388068  [ 2352/ 3200]\n",
      "loss: 1.386841  [ 2368/ 3200]\n",
      "loss: 1.386839  [ 2384/ 3200]\n",
      "loss: 1.384974  [ 2400/ 3200]\n",
      "loss: 1.379609  [ 2416/ 3200]\n",
      "loss: 1.387228  [ 2432/ 3200]\n",
      "loss: 1.383167  [ 2448/ 3200]\n",
      "loss: 1.392203  [ 2464/ 3200]\n",
      "loss: 1.384974  [ 2480/ 3200]\n",
      "loss: 1.390786  [ 2496/ 3200]\n",
      "loss: 1.387339  [ 2512/ 3200]\n",
      "loss: 1.392797  [ 2528/ 3200]\n",
      "loss: 1.391642  [ 2544/ 3200]\n",
      "loss: 1.387339  [ 2560/ 3200]\n",
      "loss: 1.394823  [ 2576/ 3200]\n",
      "loss: 1.390782  [ 2592/ 3200]\n",
      "loss: 1.387338  [ 2608/ 3200]\n",
      "loss: 1.385198  [ 2624/ 3200]\n",
      "loss: 1.385905  [ 2640/ 3200]\n",
      "loss: 1.381885  [ 2656/ 3200]\n",
      "loss: 1.389943  [ 2672/ 3200]\n",
      "loss: 1.385551  [ 2688/ 3200]\n",
      "loss: 1.386873  [ 2704/ 3200]\n",
      "loss: 1.381048  [ 2720/ 3200]\n",
      "loss: 1.389944  [ 2736/ 3200]\n",
      "loss: 1.392195  [ 2752/ 3200]\n",
      "loss: 1.384864  [ 2768/ 3200]\n",
      "loss: 1.381049  [ 2784/ 3200]\n",
      "loss: 1.385551  [ 2800/ 3200]\n",
      "loss: 1.386761  [ 2816/ 3200]\n",
      "loss: 1.389830  [ 2832/ 3200]\n",
      "loss: 1.386036  [ 2848/ 3200]\n",
      "loss: 1.387578  [ 2864/ 3200]\n",
      "loss: 1.389941  [ 2880/ 3200]\n",
      "loss: 1.385666  [ 2896/ 3200]\n",
      "loss: 1.385087  [ 2912/ 3200]\n",
      "loss: 1.385552  [ 2928/ 3200]\n",
      "loss: 1.387671  [ 2944/ 3200]\n",
      "loss: 1.389960  [ 2960/ 3200]\n",
      "loss: 1.381517  [ 2976/ 3200]\n",
      "loss: 1.382464  [ 2992/ 3200]\n",
      "loss: 1.382817  [ 3008/ 3200]\n",
      "loss: 1.384491  [ 3024/ 3200]\n",
      "loss: 1.389567  [ 3040/ 3200]\n",
      "loss: 1.386389  [ 3056/ 3200]\n",
      "loss: 1.390073  [ 3072/ 3200]\n",
      "loss: 1.382927  [ 3088/ 3200]\n",
      "loss: 1.387315  [ 3104/ 3200]\n",
      "loss: 1.381029  [ 3120/ 3200]\n",
      "loss: 1.393426  [ 3136/ 3200]\n",
      "loss: 1.384625  [ 3152/ 3200]\n",
      "loss: 1.383191  [ 3168/ 3200]\n",
      "loss: 1.390514  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.3196e-04.\n",
      "\n",
      "Epoch: 43\n",
      "-----------------------------\n",
      "loss: 1.383654  [    0/ 3200]\n",
      "loss: 1.388749  [   16/ 3200]\n",
      "loss: 1.387712  [   32/ 3200]\n",
      "loss: 1.387910  [   48/ 3200]\n",
      "loss: 1.391724  [   64/ 3200]\n",
      "loss: 1.385066  [   80/ 3200]\n",
      "loss: 1.385001  [   96/ 3200]\n",
      "loss: 1.379159  [  112/ 3200]\n",
      "loss: 1.387799  [  128/ 3200]\n",
      "loss: 1.380591  [  144/ 3200]\n",
      "loss: 1.389851  [  160/ 3200]\n",
      "loss: 1.387690  [  176/ 3200]\n",
      "loss: 1.390797  [  192/ 3200]\n",
      "loss: 1.386299  [  208/ 3200]\n",
      "loss: 1.386297  [  224/ 3200]\n",
      "loss: 1.389957  [  240/ 3200]\n",
      "loss: 1.384735  [  256/ 3200]\n",
      "loss: 1.393116  [  272/ 3200]\n",
      "loss: 1.390531  [  288/ 3200]\n",
      "loss: 1.386369  [  304/ 3200]\n",
      "loss: 1.389249  [  320/ 3200]\n",
      "loss: 1.383768  [  336/ 3200]\n",
      "loss: 1.381894  [  352/ 3200]\n",
      "loss: 1.393149  [  368/ 3200]\n",
      "loss: 1.383551  [  384/ 3200]\n",
      "loss: 1.379867  [  400/ 3200]\n",
      "loss: 1.376544  [  416/ 3200]\n",
      "loss: 1.389455  [  432/ 3200]\n",
      "loss: 1.388655  [  448/ 3200]\n",
      "loss: 1.385196  [  464/ 3200]\n",
      "loss: 1.381430  [  480/ 3200]\n",
      "loss: 1.389474  [  496/ 3200]\n",
      "loss: 1.386870  [  512/ 3200]\n",
      "loss: 1.382840  [  528/ 3200]\n",
      "loss: 1.385941  [  544/ 3200]\n",
      "loss: 1.388636  [  560/ 3200]\n",
      "loss: 1.381769  [  576/ 3200]\n",
      "loss: 1.385196  [  592/ 3200]\n",
      "loss: 1.388063  [  608/ 3200]\n",
      "loss: 1.384032  [  624/ 3200]\n",
      "loss: 1.385661  [  640/ 3200]\n",
      "loss: 1.386141  [  656/ 3200]\n",
      "loss: 1.393488  [  672/ 3200]\n",
      "loss: 1.383288  [  688/ 3200]\n",
      "loss: 1.385816  [  704/ 3200]\n",
      "loss: 1.380112  [  720/ 3200]\n",
      "loss: 1.390063  [  736/ 3200]\n",
      "loss: 1.387815  [  752/ 3200]\n",
      "loss: 1.384606  [  768/ 3200]\n",
      "loss: 1.383769  [  784/ 3200]\n",
      "loss: 1.386853  [  800/ 3200]\n",
      "loss: 1.381414  [  816/ 3200]\n",
      "loss: 1.389473  [  832/ 3200]\n",
      "loss: 1.383196  [  848/ 3200]\n",
      "loss: 1.381430  [  864/ 3200]\n",
      "loss: 1.386961  [  880/ 3200]\n",
      "loss: 1.385658  [  896/ 3200]\n",
      "loss: 1.388545  [  912/ 3200]\n",
      "loss: 1.383569  [  928/ 3200]\n",
      "loss: 1.387598  [  944/ 3200]\n",
      "loss: 1.383676  [  960/ 3200]\n",
      "loss: 1.383675  [  976/ 3200]\n",
      "loss: 1.388156  [  992/ 3200]\n",
      "loss: 1.377507  [ 1008/ 3200]\n",
      "loss: 1.387814  [ 1024/ 3200]\n",
      "loss: 1.388279  [ 1040/ 3200]\n",
      "loss: 1.384141  [ 1056/ 3200]\n",
      "loss: 1.393132  [ 1072/ 3200]\n",
      "loss: 1.389007  [ 1088/ 3200]\n",
      "loss: 1.386746  [ 1104/ 3200]\n",
      "loss: 1.390896  [ 1120/ 3200]\n",
      "loss: 1.385086  [ 1136/ 3200]\n",
      "loss: 1.382361  [ 1152/ 3200]\n",
      "loss: 1.385802  [ 1168/ 3200]\n",
      "loss: 1.384978  [ 1184/ 3200]\n",
      "loss: 1.384729  [ 1200/ 3200]\n",
      "loss: 1.386388  [ 1216/ 3200]\n",
      "loss: 1.386734  [ 1232/ 3200]\n",
      "loss: 1.388731  [ 1248/ 3200]\n",
      "loss: 1.390308  [ 1264/ 3200]\n",
      "loss: 1.387798  [ 1280/ 3200]\n",
      "loss: 1.379279  [ 1296/ 3200]\n",
      "loss: 1.379386  [ 1312/ 3200]\n",
      "loss: 1.392556  [ 1328/ 3200]\n",
      "loss: 1.387690  [ 1344/ 3200]\n",
      "loss: 1.388278  [ 1360/ 3200]\n",
      "loss: 1.390043  [ 1376/ 3200]\n",
      "loss: 1.387782  [ 1392/ 3200]\n",
      "loss: 1.385551  [ 1408/ 3200]\n",
      "loss: 1.381525  [ 1424/ 3200]\n",
      "loss: 1.392767  [ 1440/ 3200]\n",
      "loss: 1.392089  [ 1456/ 3200]\n",
      "loss: 1.389683  [ 1472/ 3200]\n",
      "loss: 1.384054  [ 1488/ 3200]\n",
      "loss: 1.388153  [ 1504/ 3200]\n",
      "loss: 1.387135  [ 1520/ 3200]\n",
      "loss: 1.391219  [ 1536/ 3200]\n",
      "loss: 1.386388  [ 1552/ 3200]\n",
      "loss: 1.388632  [ 1568/ 3200]\n",
      "loss: 1.386836  [ 1584/ 3200]\n",
      "loss: 1.384625  [ 1600/ 3200]\n",
      "loss: 1.393688  [ 1616/ 3200]\n",
      "loss: 1.387794  [ 1632/ 3200]\n",
      "loss: 1.388045  [ 1648/ 3200]\n",
      "loss: 1.388718  [ 1664/ 3200]\n",
      "loss: 1.384146  [ 1680/ 3200]\n",
      "loss: 1.390429  [ 1696/ 3200]\n",
      "loss: 1.386014  [ 1712/ 3200]\n",
      "loss: 1.384041  [ 1728/ 3200]\n",
      "loss: 1.390391  [ 1744/ 3200]\n",
      "loss: 1.391333  [ 1760/ 3200]\n",
      "loss: 1.383204  [ 1776/ 3200]\n",
      "loss: 1.388148  [ 1792/ 3200]\n",
      "loss: 1.388755  [ 1808/ 3200]\n",
      "loss: 1.383686  [ 1824/ 3200]\n",
      "loss: 1.387792  [ 1840/ 3200]\n",
      "loss: 1.384733  [ 1856/ 3200]\n",
      "loss: 1.389570  [ 1872/ 3200]\n",
      "loss: 1.392081  [ 1888/ 3200]\n",
      "loss: 1.392523  [ 1904/ 3200]\n",
      "loss: 1.382265  [ 1920/ 3200]\n",
      "loss: 1.388541  [ 1936/ 3200]\n",
      "loss: 1.389357  [ 1952/ 3200]\n",
      "loss: 1.387792  [ 1968/ 3200]\n",
      "loss: 1.389943  [ 1984/ 3200]\n",
      "loss: 1.388165  [ 2000/ 3200]\n",
      "loss: 1.389444  [ 2016/ 3200]\n",
      "loss: 1.386014  [ 2032/ 3200]\n",
      "loss: 1.388539  [ 2048/ 3200]\n",
      "loss: 1.386014  [ 2064/ 3200]\n",
      "loss: 1.381911  [ 2080/ 3200]\n",
      "loss: 1.386868  [ 2096/ 3200]\n",
      "loss: 1.387098  [ 2112/ 3200]\n",
      "loss: 1.386407  [ 2128/ 3200]\n",
      "loss: 1.382835  [ 2144/ 3200]\n",
      "loss: 1.385305  [ 2160/ 3200]\n",
      "loss: 1.389088  [ 2176/ 3200]\n",
      "loss: 1.385341  [ 2192/ 3200]\n",
      "loss: 1.385465  [ 2208/ 3200]\n",
      "loss: 1.386033  [ 2224/ 3200]\n",
      "loss: 1.383776  [ 2240/ 3200]\n",
      "loss: 1.385571  [ 2256/ 3200]\n",
      "loss: 1.384238  [ 2272/ 3200]\n",
      "loss: 1.386867  [ 2288/ 3200]\n",
      "loss: 1.387580  [ 2304/ 3200]\n",
      "loss: 1.389673  [ 2320/ 3200]\n",
      "loss: 1.385073  [ 2336/ 3200]\n",
      "loss: 1.381451  [ 2352/ 3200]\n",
      "loss: 1.381432  [ 2368/ 3200]\n",
      "loss: 1.378732  [ 2384/ 3200]\n",
      "loss: 1.380507  [ 2400/ 3200]\n",
      "loss: 1.384042  [ 2416/ 3200]\n",
      "loss: 1.387330  [ 2432/ 3200]\n",
      "loss: 1.383882  [ 2448/ 3200]\n",
      "loss: 1.384166  [ 2464/ 3200]\n",
      "loss: 1.383775  [ 2480/ 3200]\n",
      "loss: 1.389089  [ 2496/ 3200]\n",
      "loss: 1.380700  [ 2512/ 3200]\n",
      "loss: 1.384025  [ 2528/ 3200]\n",
      "loss: 1.385321  [ 2544/ 3200]\n",
      "loss: 1.385887  [ 2560/ 3200]\n",
      "loss: 1.389570  [ 2576/ 3200]\n",
      "loss: 1.380238  [ 2592/ 3200]\n",
      "loss: 1.376948  [ 2608/ 3200]\n",
      "loss: 1.388252  [ 2624/ 3200]\n",
      "loss: 1.384148  [ 2640/ 3200]\n",
      "loss: 1.382957  [ 2656/ 3200]\n",
      "loss: 1.387685  [ 2672/ 3200]\n",
      "loss: 1.389109  [ 2688/ 3200]\n",
      "loss: 1.383312  [ 2704/ 3200]\n",
      "loss: 1.381888  [ 2720/ 3200]\n",
      "loss: 1.385551  [ 2736/ 3200]\n",
      "loss: 1.387685  [ 2752/ 3200]\n",
      "loss: 1.387791  [ 2768/ 3200]\n",
      "loss: 1.387225  [ 2784/ 3200]\n",
      "loss: 1.384127  [ 2800/ 3200]\n",
      "loss: 1.388733  [ 2816/ 3200]\n",
      "loss: 1.393942  [ 2832/ 3200]\n",
      "loss: 1.383939  [ 2848/ 3200]\n",
      "loss: 1.383206  [ 2864/ 3200]\n",
      "loss: 1.392998  [ 2880/ 3200]\n",
      "loss: 1.386388  [ 2896/ 3200]\n",
      "loss: 1.391970  [ 2912/ 3200]\n",
      "loss: 1.383230  [ 2928/ 3200]\n",
      "loss: 1.387814  [ 2944/ 3200]\n",
      "loss: 1.389547  [ 2960/ 3200]\n",
      "loss: 1.383690  [ 2976/ 3200]\n",
      "loss: 1.386740  [ 2992/ 3200]\n",
      "loss: 1.393477  [ 3008/ 3200]\n",
      "loss: 1.389085  [ 3024/ 3200]\n",
      "loss: 1.383560  [ 3040/ 3200]\n",
      "loss: 1.382856  [ 3056/ 3200]\n",
      "loss: 1.387791  [ 3072/ 3200]\n",
      "loss: 1.389000  [ 3088/ 3200]\n",
      "loss: 1.380244  [ 3104/ 3200]\n",
      "loss: 1.387791  [ 3120/ 3200]\n",
      "loss: 1.387575  [ 3136/ 3200]\n",
      "loss: 1.388165  [ 3152/ 3200]\n",
      "loss: 1.382288  [ 3168/ 3200]\n",
      "loss: 1.392927  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.2037e-04.\n",
      "\n",
      "Epoch: 44\n",
      "-----------------------------\n",
      "loss: 1.384150  [    0/ 3200]\n",
      "loss: 1.386650  [   16/ 3200]\n",
      "loss: 1.383207  [   32/ 3200]\n",
      "loss: 1.387702  [   48/ 3200]\n",
      "loss: 1.387682  [   64/ 3200]\n",
      "loss: 1.387661  [   80/ 3200]\n",
      "loss: 1.388232  [   96/ 3200]\n",
      "loss: 1.390051  [  112/ 3200]\n",
      "loss: 1.384589  [  128/ 3200]\n",
      "loss: 1.386871  [  144/ 3200]\n",
      "loss: 1.393469  [  160/ 3200]\n",
      "loss: 1.387791  [  176/ 3200]\n",
      "loss: 1.380711  [  192/ 3200]\n",
      "loss: 1.386277  [  208/ 3200]\n",
      "loss: 1.383802  [  224/ 3200]\n",
      "loss: 1.391803  [  240/ 3200]\n",
      "loss: 1.386760  [  256/ 3200]\n",
      "loss: 1.381301  [  272/ 3200]\n",
      "loss: 1.385817  [  288/ 3200]\n",
      "loss: 1.389456  [  304/ 3200]\n",
      "loss: 1.382399  [  320/ 3200]\n",
      "loss: 1.382970  [  336/ 3200]\n",
      "loss: 1.386038  [  352/ 3200]\n",
      "loss: 1.387220  [  368/ 3200]\n",
      "loss: 1.382859  [  384/ 3200]\n",
      "loss: 1.387681  [  400/ 3200]\n",
      "loss: 1.386959  [  416/ 3200]\n",
      "loss: 1.387681  [  432/ 3200]\n",
      "loss: 1.386498  [  448/ 3200]\n",
      "loss: 1.390376  [  464/ 3200]\n",
      "loss: 1.384722  [  480/ 3200]\n",
      "loss: 1.389478  [  496/ 3200]\n",
      "loss: 1.390397  [  512/ 3200]\n",
      "loss: 1.379574  [  528/ 3200]\n",
      "loss: 1.386366  [  544/ 3200]\n",
      "loss: 1.383692  [  560/ 3200]\n",
      "loss: 1.388140  [  576/ 3200]\n",
      "loss: 1.393791  [  592/ 3200]\n",
      "loss: 1.387591  [  608/ 3200]\n",
      "loss: 1.386759  [  624/ 3200]\n",
      "loss: 1.387679  [  640/ 3200]\n",
      "loss: 1.389016  [  656/ 3200]\n",
      "loss: 1.388139  [  672/ 3200]\n",
      "loss: 1.389104  [  688/ 3200]\n",
      "loss: 1.383322  [  704/ 3200]\n",
      "loss: 1.381527  [  720/ 3200]\n",
      "loss: 1.386870  [  736/ 3200]\n",
      "loss: 1.388970  [  752/ 3200]\n",
      "loss: 1.388621  [  768/ 3200]\n",
      "loss: 1.383322  [  784/ 3200]\n",
      "loss: 1.384860  [  800/ 3200]\n",
      "loss: 1.387812  [  816/ 3200]\n",
      "loss: 1.385556  [  832/ 3200]\n",
      "loss: 1.385074  [  848/ 3200]\n",
      "loss: 1.387569  [  864/ 3200]\n",
      "loss: 1.386498  [  880/ 3200]\n",
      "loss: 1.385928  [  896/ 3200]\n",
      "loss: 1.384242  [  912/ 3200]\n",
      "loss: 1.383322  [  928/ 3200]\n",
      "loss: 1.387811  [  944/ 3200]\n",
      "loss: 1.382840  [  960/ 3200]\n",
      "loss: 1.387197  [  976/ 3200]\n",
      "loss: 1.377058  [  992/ 3200]\n",
      "loss: 1.379772  [ 1008/ 3200]\n",
      "loss: 1.388596  [ 1024/ 3200]\n",
      "loss: 1.387356  [ 1040/ 3200]\n",
      "loss: 1.384637  [ 1056/ 3200]\n",
      "loss: 1.387329  [ 1072/ 3200]\n",
      "loss: 1.390745  [ 1088/ 3200]\n",
      "loss: 1.386737  [ 1104/ 3200]\n",
      "loss: 1.389343  [ 1120/ 3200]\n",
      "loss: 1.383646  [ 1136/ 3200]\n",
      "loss: 1.391880  [ 1152/ 3200]\n",
      "loss: 1.383670  [ 1168/ 3200]\n",
      "loss: 1.382618  [ 1184/ 3200]\n",
      "loss: 1.388356  [ 1200/ 3200]\n",
      "loss: 1.384722  [ 1216/ 3200]\n",
      "loss: 1.387192  [ 1232/ 3200]\n",
      "loss: 1.388619  [ 1248/ 3200]\n",
      "loss: 1.384612  [ 1264/ 3200]\n",
      "loss: 1.384265  [ 1280/ 3200]\n",
      "loss: 1.384155  [ 1296/ 3200]\n",
      "loss: 1.384264  [ 1312/ 3200]\n",
      "loss: 1.388510  [ 1328/ 3200]\n",
      "loss: 1.383698  [ 1344/ 3200]\n",
      "loss: 1.388729  [ 1360/ 3200]\n",
      "loss: 1.382271  [ 1376/ 3200]\n",
      "loss: 1.381437  [ 1392/ 3200]\n",
      "loss: 1.392171  [ 1408/ 3200]\n",
      "loss: 1.391684  [ 1424/ 3200]\n",
      "loss: 1.386526  [ 1440/ 3200]\n",
      "loss: 1.386844  [ 1456/ 3200]\n",
      "loss: 1.393001  [ 1472/ 3200]\n",
      "loss: 1.389369  [ 1488/ 3200]\n",
      "loss: 1.389935  [ 1504/ 3200]\n",
      "loss: 1.383105  [ 1520/ 3200]\n",
      "loss: 1.388881  [ 1536/ 3200]\n",
      "loss: 1.378405  [ 1552/ 3200]\n",
      "loss: 1.389934  [ 1568/ 3200]\n",
      "loss: 1.379916  [ 1584/ 3200]\n",
      "loss: 1.384987  [ 1600/ 3200]\n",
      "loss: 1.389536  [ 1616/ 3200]\n",
      "loss: 1.382730  [ 1632/ 3200]\n",
      "loss: 1.384666  [ 1648/ 3200]\n",
      "loss: 1.389450  [ 1664/ 3200]\n",
      "loss: 1.384639  [ 1680/ 3200]\n",
      "loss: 1.378837  [ 1696/ 3200]\n",
      "loss: 1.391905  [ 1712/ 3200]\n",
      "loss: 1.390391  [ 1728/ 3200]\n",
      "loss: 1.386498  [ 1744/ 3200]\n",
      "loss: 1.390366  [ 1760/ 3200]\n",
      "loss: 1.383783  [ 1776/ 3200]\n",
      "loss: 1.387676  [ 1792/ 3200]\n",
      "loss: 1.380611  [ 1808/ 3200]\n",
      "loss: 1.387329  [ 1824/ 3200]\n",
      "loss: 1.384962  [ 1840/ 3200]\n",
      "loss: 1.383325  [ 1856/ 3200]\n",
      "loss: 1.387786  [ 1872/ 3200]\n",
      "loss: 1.390848  [ 1888/ 3200]\n",
      "loss: 1.387219  [ 1904/ 3200]\n",
      "loss: 1.384641  [ 1920/ 3200]\n",
      "loss: 1.383699  [ 1936/ 3200]\n",
      "loss: 1.384157  [ 1952/ 3200]\n",
      "loss: 1.388160  [ 1968/ 3200]\n",
      "loss: 1.390957  [ 1984/ 3200]\n",
      "loss: 1.387219  [ 2000/ 3200]\n",
      "loss: 1.387109  [ 2016/ 3200]\n",
      "loss: 1.388269  [ 2032/ 3200]\n",
      "loss: 1.393534  [ 2048/ 3200]\n",
      "loss: 1.387192  [ 2064/ 3200]\n",
      "loss: 1.382012  [ 2080/ 3200]\n",
      "loss: 1.390763  [ 2096/ 3200]\n",
      "loss: 1.386761  [ 2112/ 3200]\n",
      "loss: 1.382980  [ 2128/ 3200]\n",
      "loss: 1.381098  [ 2144/ 3200]\n",
      "loss: 1.384750  [ 2160/ 3200]\n",
      "loss: 1.379916  [ 2176/ 3200]\n",
      "loss: 1.385929  [ 2192/ 3200]\n",
      "loss: 1.385447  [ 2208/ 3200]\n",
      "loss: 1.389557  [ 2224/ 3200]\n",
      "loss: 1.384640  [ 2240/ 3200]\n",
      "loss: 1.388242  [ 2256/ 3200]\n",
      "loss: 1.383783  [ 2272/ 3200]\n",
      "loss: 1.387326  [ 2288/ 3200]\n",
      "loss: 1.381554  [ 2304/ 3200]\n",
      "loss: 1.394307  [ 2320/ 3200]\n",
      "loss: 1.379891  [ 2336/ 3200]\n",
      "loss: 1.386363  [ 2352/ 3200]\n",
      "loss: 1.385905  [ 2368/ 3200]\n",
      "loss: 1.387433  [ 2384/ 3200]\n",
      "loss: 1.390306  [ 2400/ 3200]\n",
      "loss: 1.380992  [ 2416/ 3200]\n",
      "loss: 1.386951  [ 2432/ 3200]\n",
      "loss: 1.388697  [ 2448/ 3200]\n",
      "loss: 1.386387  [ 2464/ 3200]\n",
      "loss: 1.384134  [ 2480/ 3200]\n",
      "loss: 1.388992  [ 2496/ 3200]\n",
      "loss: 1.390306  [ 2512/ 3200]\n",
      "loss: 1.387701  [ 2528/ 3200]\n",
      "loss: 1.383245  [ 2544/ 3200]\n",
      "loss: 1.388134  [ 2560/ 3200]\n",
      "loss: 1.386387  [ 2576/ 3200]\n",
      "loss: 1.386999  [ 2592/ 3200]\n",
      "loss: 1.378285  [ 2608/ 3200]\n",
      "loss: 1.386303  [ 2624/ 3200]\n",
      "loss: 1.387112  [ 2640/ 3200]\n",
      "loss: 1.386014  [ 2656/ 3200]\n",
      "loss: 1.387699  [ 2672/ 3200]\n",
      "loss: 1.382281  [ 2688/ 3200]\n",
      "loss: 1.382977  [ 2704/ 3200]\n",
      "loss: 1.389883  [ 2720/ 3200]\n",
      "loss: 1.392133  [ 2736/ 3200]\n",
      "loss: 1.384159  [ 2752/ 3200]\n",
      "loss: 1.379680  [ 2768/ 3200]\n",
      "loss: 1.386363  [ 2784/ 3200]\n",
      "loss: 1.387434  [ 2800/ 3200]\n",
      "loss: 1.385531  [ 2816/ 3200]\n",
      "loss: 1.385448  [ 2832/ 3200]\n",
      "loss: 1.389204  [ 2848/ 3200]\n",
      "loss: 1.388076  [ 2864/ 3200]\n",
      "loss: 1.385579  [ 2880/ 3200]\n",
      "loss: 1.385365  [ 2896/ 3200]\n",
      "loss: 1.387325  [ 2912/ 3200]\n",
      "loss: 1.388988  [ 2928/ 3200]\n",
      "loss: 1.389072  [ 2944/ 3200]\n",
      "loss: 1.389507  [ 2960/ 3200]\n",
      "loss: 1.384617  [ 2976/ 3200]\n",
      "loss: 1.384510  [ 2992/ 3200]\n",
      "loss: 1.388181  [ 3008/ 3200]\n",
      "loss: 1.382414  [ 3024/ 3200]\n",
      "loss: 1.387110  [ 3040/ 3200]\n",
      "loss: 1.388398  [ 3056/ 3200]\n",
      "loss: 1.389901  [ 3072/ 3200]\n",
      "loss: 1.387721  [ 3088/ 3200]\n",
      "loss: 1.388155  [ 3104/ 3200]\n",
      "loss: 1.385557  [ 3120/ 3200]\n",
      "loss: 1.396035  [ 3136/ 3200]\n",
      "loss: 1.389005  [ 3152/ 3200]\n",
      "loss: 1.381935  [ 3168/ 3200]\n",
      "loss: 1.384729  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.0935e-04.\n",
      "\n",
      "Epoch: 45\n",
      "-----------------------------\n",
      "loss: 1.392976  [    0/ 3200]\n",
      "loss: 1.385449  [   16/ 3200]\n",
      "loss: 1.383791  [   32/ 3200]\n",
      "loss: 1.382394  [   48/ 3200]\n",
      "loss: 1.387216  [   64/ 3200]\n",
      "loss: 1.389640  [   80/ 3200]\n",
      "loss: 1.390379  [   96/ 3200]\n",
      "loss: 1.380540  [  112/ 3200]\n",
      "loss: 1.382744  [  128/ 3200]\n",
      "loss: 1.388132  [  144/ 3200]\n",
      "loss: 1.384488  [  160/ 3200]\n",
      "loss: 1.383199  [  176/ 3200]\n",
      "loss: 1.388241  [  192/ 3200]\n",
      "loss: 1.390837  [  208/ 3200]\n",
      "loss: 1.393385  [  224/ 3200]\n",
      "loss: 1.386954  [  240/ 3200]\n",
      "loss: 1.385448  [  256/ 3200]\n",
      "loss: 1.383790  [  272/ 3200]\n",
      "loss: 1.388155  [  288/ 3200]\n",
      "loss: 1.385448  [  304/ 3200]\n",
      "loss: 1.392629  [  320/ 3200]\n",
      "loss: 1.383224  [  336/ 3200]\n",
      "loss: 1.386150  [  352/ 3200]\n",
      "loss: 1.383765  [  368/ 3200]\n",
      "loss: 1.389357  [  384/ 3200]\n",
      "loss: 1.389549  [  400/ 3200]\n",
      "loss: 1.386039  [  416/ 3200]\n",
      "loss: 1.388128  [  432/ 3200]\n",
      "loss: 1.392574  [  448/ 3200]\n",
      "loss: 1.388500  [  464/ 3200]\n",
      "loss: 1.376294  [  480/ 3200]\n",
      "loss: 1.387325  [  496/ 3200]\n",
      "loss: 1.382396  [  512/ 3200]\n",
      "loss: 1.390641  [  528/ 3200]\n",
      "loss: 1.387807  [  544/ 3200]\n",
      "loss: 1.381112  [  560/ 3200]\n",
      "loss: 1.382903  [  576/ 3200]\n",
      "loss: 1.382505  [  592/ 3200]\n",
      "loss: 1.388610  [  608/ 3200]\n",
      "loss: 1.387961  [  624/ 3200]\n",
      "loss: 1.384163  [  640/ 3200]\n",
      "loss: 1.388633  [  656/ 3200]\n",
      "loss: 1.386867  [  672/ 3200]\n",
      "loss: 1.387650  [  688/ 3200]\n",
      "loss: 1.388477  [  704/ 3200]\n",
      "loss: 1.385820  [  720/ 3200]\n",
      "loss: 1.385929  [  736/ 3200]\n",
      "loss: 1.390833  [  752/ 3200]\n",
      "loss: 1.387214  [  768/ 3200]\n",
      "loss: 1.385795  [  784/ 3200]\n",
      "loss: 1.387670  [  800/ 3200]\n",
      "loss: 1.387213  [  816/ 3200]\n",
      "loss: 1.384140  [  832/ 3200]\n",
      "loss: 1.384276  [  848/ 3200]\n",
      "loss: 1.386362  [  864/ 3200]\n",
      "loss: 1.379237  [  880/ 3200]\n",
      "loss: 1.382656  [  896/ 3200]\n",
      "loss: 1.383336  [  912/ 3200]\n",
      "loss: 1.386524  [  928/ 3200]\n",
      "loss: 1.375246  [  944/ 3200]\n",
      "loss: 1.386757  [  960/ 3200]\n",
      "loss: 1.382509  [  976/ 3200]\n",
      "loss: 1.386499  [  992/ 3200]\n",
      "loss: 1.389438  [ 1008/ 3200]\n",
      "loss: 1.387102  [ 1024/ 3200]\n",
      "loss: 1.384620  [ 1040/ 3200]\n",
      "loss: 1.388153  [ 1056/ 3200]\n",
      "loss: 1.390463  [ 1072/ 3200]\n",
      "loss: 1.384301  [ 1088/ 3200]\n",
      "loss: 1.381458  [ 1104/ 3200]\n",
      "loss: 1.384508  [ 1120/ 3200]\n",
      "loss: 1.390351  [ 1136/ 3200]\n",
      "loss: 1.378175  [ 1152/ 3200]\n",
      "loss: 1.386732  [ 1168/ 3200]\n",
      "loss: 1.382879  [ 1184/ 3200]\n",
      "loss: 1.385421  [ 1200/ 3200]\n",
      "loss: 1.388954  [ 1216/ 3200]\n",
      "loss: 1.384732  [ 1232/ 3200]\n",
      "loss: 1.387241  [ 1248/ 3200]\n",
      "loss: 1.382509  [ 1264/ 3200]\n",
      "loss: 1.383224  [ 1280/ 3200]\n",
      "loss: 1.387558  [ 1296/ 3200]\n",
      "loss: 1.390237  [ 1312/ 3200]\n",
      "loss: 1.382993  [ 1328/ 3200]\n",
      "loss: 1.385930  [ 1344/ 3200]\n",
      "loss: 1.385560  [ 1360/ 3200]\n",
      "loss: 1.386247  [ 1376/ 3200]\n",
      "loss: 1.386983  [ 1392/ 3200]\n",
      "loss: 1.380061  [ 1408/ 3200]\n",
      "loss: 1.384163  [ 1424/ 3200]\n",
      "loss: 1.386473  [ 1440/ 3200]\n",
      "loss: 1.386301  [ 1456/ 3200]\n",
      "loss: 1.386016  [ 1472/ 3200]\n",
      "loss: 1.385474  [ 1488/ 3200]\n",
      "loss: 1.388953  [ 1504/ 3200]\n",
      "loss: 1.388610  [ 1520/ 3200]\n",
      "loss: 1.386729  [ 1536/ 3200]\n",
      "loss: 1.386871  [ 1552/ 3200]\n",
      "loss: 1.388952  [ 1568/ 3200]\n",
      "loss: 1.388581  [ 1584/ 3200]\n",
      "loss: 1.382312  [ 1600/ 3200]\n",
      "loss: 1.389435  [ 1616/ 3200]\n",
      "loss: 1.382996  [ 1632/ 3200]\n",
      "loss: 1.386072  [ 1648/ 3200]\n",
      "loss: 1.385560  [ 1664/ 3200]\n",
      "loss: 1.391229  [ 1680/ 3200]\n",
      "loss: 1.382994  [ 1696/ 3200]\n",
      "loss: 1.383819  [ 1712/ 3200]\n",
      "loss: 1.386499  [ 1728/ 3200]\n",
      "loss: 1.391634  [ 1744/ 3200]\n",
      "loss: 1.386843  [ 1760/ 3200]\n",
      "loss: 1.383312  [ 1776/ 3200]\n",
      "loss: 1.381115  [ 1792/ 3200]\n",
      "loss: 1.391546  [ 1808/ 3200]\n",
      "loss: 1.387696  [ 1824/ 3200]\n",
      "loss: 1.387669  [ 1840/ 3200]\n",
      "loss: 1.381486  [ 1856/ 3200]\n",
      "loss: 1.385329  [ 1872/ 3200]\n",
      "loss: 1.383819  [ 1888/ 3200]\n",
      "loss: 1.388497  [ 1904/ 3200]\n",
      "loss: 1.385078  [ 1920/ 3200]\n",
      "loss: 1.391226  [ 1936/ 3200]\n",
      "loss: 1.386016  [ 1952/ 3200]\n",
      "loss: 1.390373  [ 1968/ 3200]\n",
      "loss: 1.383931  [ 1984/ 3200]\n",
      "loss: 1.386954  [ 2000/ 3200]\n",
      "loss: 1.390482  [ 2016/ 3200]\n",
      "loss: 1.384646  [ 2032/ 3200]\n",
      "loss: 1.391090  [ 2048/ 3200]\n",
      "loss: 1.378787  [ 2064/ 3200]\n",
      "loss: 1.389568  [ 2080/ 3200]\n",
      "loss: 1.385125  [ 2096/ 3200]\n",
      "loss: 1.376196  [ 2112/ 3200]\n",
      "loss: 1.382313  [ 2128/ 3200]\n",
      "loss: 1.386386  [ 2144/ 3200]\n",
      "loss: 1.384164  [ 2160/ 3200]\n",
      "loss: 1.391288  [ 2176/ 3200]\n",
      "loss: 1.385449  [ 2192/ 3200]\n",
      "loss: 1.391398  [ 2208/ 3200]\n",
      "loss: 1.389434  [ 2224/ 3200]\n",
      "loss: 1.383339  [ 2240/ 3200]\n",
      "loss: 1.388237  [ 2256/ 3200]\n",
      "loss: 1.384993  [ 2272/ 3200]\n",
      "loss: 1.392961  [ 2288/ 3200]\n",
      "loss: 1.391196  [ 2304/ 3200]\n",
      "loss: 1.388127  [ 2320/ 3200]\n",
      "loss: 1.386474  [ 2336/ 3200]\n",
      "loss: 1.385930  [ 2352/ 3200]\n",
      "loss: 1.388172  [ 2368/ 3200]\n",
      "loss: 1.385081  [ 2384/ 3200]\n",
      "loss: 1.387519  [ 2400/ 3200]\n",
      "loss: 1.382992  [ 2416/ 3200]\n",
      "loss: 1.390346  [ 2432/ 3200]\n",
      "loss: 1.378422  [ 2448/ 3200]\n",
      "loss: 1.387322  [ 2464/ 3200]\n",
      "loss: 1.384515  [ 2480/ 3200]\n",
      "loss: 1.390260  [ 2496/ 3200]\n",
      "loss: 1.391195  [ 2512/ 3200]\n",
      "loss: 1.385103  [ 2528/ 3200]\n",
      "loss: 1.393043  [ 2544/ 3200]\n",
      "loss: 1.388713  [ 2560/ 3200]\n",
      "loss: 1.387692  [ 2576/ 3200]\n",
      "loss: 1.390606  [ 2592/ 3200]\n",
      "loss: 1.388472  [ 2608/ 3200]\n",
      "loss: 1.384757  [ 2624/ 3200]\n",
      "loss: 1.381015  [ 2640/ 3200]\n",
      "loss: 1.383256  [ 2656/ 3200]\n",
      "loss: 1.388257  [ 2672/ 3200]\n",
      "loss: 1.391671  [ 2688/ 3200]\n",
      "loss: 1.386843  [ 2704/ 3200]\n",
      "loss: 1.386386  [ 2720/ 3200]\n",
      "loss: 1.386734  [ 2736/ 3200]\n",
      "loss: 1.382060  [ 2752/ 3200]\n",
      "loss: 1.387343  [ 2768/ 3200]\n",
      "loss: 1.383363  [ 2784/ 3200]\n",
      "loss: 1.387320  [ 2800/ 3200]\n",
      "loss: 1.386775  [ 2816/ 3200]\n",
      "loss: 1.379732  [ 2832/ 3200]\n",
      "loss: 1.390473  [ 2848/ 3200]\n",
      "loss: 1.383342  [ 2864/ 3200]\n",
      "loss: 1.392493  [ 2880/ 3200]\n",
      "loss: 1.386736  [ 2896/ 3200]\n",
      "loss: 1.393865  [ 2912/ 3200]\n",
      "loss: 1.390911  [ 2928/ 3200]\n",
      "loss: 1.388126  [ 2944/ 3200]\n",
      "loss: 1.389624  [ 2960/ 3200]\n",
      "loss: 1.388864  [ 2976/ 3200]\n",
      "loss: 1.387319  [ 2992/ 3200]\n",
      "loss: 1.385123  [ 3008/ 3200]\n",
      "loss: 1.385453  [ 3024/ 3200]\n",
      "loss: 1.387211  [ 3040/ 3200]\n",
      "loss: 1.386367  [ 3056/ 3200]\n",
      "loss: 1.387776  [ 3072/ 3200]\n",
      "loss: 1.385909  [ 3088/ 3200]\n",
      "loss: 1.382888  [ 3104/ 3200]\n",
      "loss: 1.381954  [ 3120/ 3200]\n",
      "loss: 1.388037  [ 3136/ 3200]\n",
      "loss: 1.385929  [ 3152/ 3200]\n",
      "loss: 1.386494  [ 3168/ 3200]\n",
      "loss: 1.388950  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.9888e-04.\n",
      "\n",
      "Epoch: 46\n",
      "-----------------------------\n",
      "loss: 1.382171  [    0/ 3200]\n",
      "loss: 1.389166  [   16/ 3200]\n",
      "loss: 1.386735  [   32/ 3200]\n",
      "loss: 1.389903  [   48/ 3200]\n",
      "loss: 1.384647  [   64/ 3200]\n",
      "loss: 1.389058  [   80/ 3200]\n",
      "loss: 1.388125  [   96/ 3200]\n",
      "loss: 1.383929  [  112/ 3200]\n",
      "loss: 1.388620  [  128/ 3200]\n",
      "loss: 1.386035  [  144/ 3200]\n",
      "loss: 1.385578  [  160/ 3200]\n",
      "loss: 1.382870  [  176/ 3200]\n",
      "loss: 1.384277  [  192/ 3200]\n",
      "loss: 1.392838  [  208/ 3200]\n",
      "loss: 1.381606  [  224/ 3200]\n",
      "loss: 1.389795  [  240/ 3200]\n",
      "loss: 1.382871  [  256/ 3200]\n",
      "loss: 1.386949  [  272/ 3200]\n",
      "loss: 1.389971  [  288/ 3200]\n",
      "loss: 1.386034  [  304/ 3200]\n",
      "loss: 1.380570  [  320/ 3200]\n",
      "loss: 1.384523  [  336/ 3200]\n",
      "loss: 1.389883  [  352/ 3200]\n",
      "loss: 1.388513  [  368/ 3200]\n",
      "loss: 1.389883  [  384/ 3200]\n",
      "loss: 1.393116  [  400/ 3200]\n",
      "loss: 1.384998  [  416/ 3200]\n",
      "loss: 1.385560  [  432/ 3200]\n",
      "loss: 1.383697  [  448/ 3200]\n",
      "loss: 1.389036  [  464/ 3200]\n",
      "loss: 1.390271  [  480/ 3200]\n",
      "loss: 1.386598  [  496/ 3200]\n",
      "loss: 1.384543  [  512/ 3200]\n",
      "loss: 1.386630  [  528/ 3200]\n",
      "loss: 1.383803  [  544/ 3200]\n",
      "loss: 1.388579  [  560/ 3200]\n",
      "loss: 1.380203  [  576/ 3200]\n",
      "loss: 1.382521  [  592/ 3200]\n",
      "loss: 1.389901  [  608/ 3200]\n",
      "loss: 1.390812  [  624/ 3200]\n",
      "loss: 1.385316  [  640/ 3200]\n",
      "loss: 1.384628  [  656/ 3200]\n",
      "loss: 1.388058  [  672/ 3200]\n",
      "loss: 1.386510  [  688/ 3200]\n",
      "loss: 1.383594  [  704/ 3200]\n",
      "loss: 1.386016  [  720/ 3200]\n",
      "loss: 1.386405  [  736/ 3200]\n",
      "loss: 1.382435  [  752/ 3200]\n",
      "loss: 1.388986  [  768/ 3200]\n",
      "loss: 1.384541  [  784/ 3200]\n",
      "loss: 1.377974  [  800/ 3200]\n",
      "loss: 1.386860  [  816/ 3200]\n",
      "loss: 1.385912  [  832/ 3200]\n",
      "loss: 1.387194  [  848/ 3200]\n",
      "loss: 1.386842  [  864/ 3200]\n",
      "loss: 1.374952  [  880/ 3200]\n",
      "loss: 1.388036  [  896/ 3200]\n",
      "loss: 1.387211  [  912/ 3200]\n",
      "loss: 1.386825  [  928/ 3200]\n",
      "loss: 1.392006  [  944/ 3200]\n",
      "loss: 1.380659  [  960/ 3200]\n",
      "loss: 1.386949  [  976/ 3200]\n",
      "loss: 1.386017  [  992/ 3200]\n",
      "loss: 1.386842  [ 1008/ 3200]\n",
      "loss: 1.383327  [ 1024/ 3200]\n",
      "loss: 1.386280  [ 1040/ 3200]\n",
      "loss: 1.386776  [ 1056/ 3200]\n",
      "loss: 1.376233  [ 1072/ 3200]\n",
      "loss: 1.386017  [ 1088/ 3200]\n",
      "loss: 1.389880  [ 1104/ 3200]\n",
      "loss: 1.388579  [ 1120/ 3200]\n",
      "loss: 1.388598  [ 1136/ 3200]\n",
      "loss: 1.385994  [ 1152/ 3200]\n",
      "loss: 1.387560  [ 1168/ 3200]\n",
      "loss: 1.379641  [ 1184/ 3200]\n",
      "loss: 1.388706  [ 1200/ 3200]\n",
      "loss: 1.391444  [ 1216/ 3200]\n",
      "loss: 1.389315  [ 1232/ 3200]\n",
      "loss: 1.381961  [ 1248/ 3200]\n",
      "loss: 1.386342  [ 1264/ 3200]\n",
      "loss: 1.389313  [ 1280/ 3200]\n",
      "loss: 1.382070  [ 1296/ 3200]\n",
      "loss: 1.391853  [ 1312/ 3200]\n",
      "loss: 1.390246  [ 1328/ 3200]\n",
      "loss: 1.386755  [ 1344/ 3200]\n",
      "loss: 1.392566  [ 1360/ 3200]\n",
      "loss: 1.393844  [ 1376/ 3200]\n",
      "loss: 1.382050  [ 1392/ 3200]\n",
      "loss: 1.388033  [ 1408/ 3200]\n",
      "loss: 1.385216  [ 1424/ 3200]\n",
      "loss: 1.382873  [ 1440/ 3200]\n",
      "loss: 1.382896  [ 1456/ 3200]\n",
      "loss: 1.389421  [ 1472/ 3200]\n",
      "loss: 1.386754  [ 1488/ 3200]\n",
      "loss: 1.384629  [ 1504/ 3200]\n",
      "loss: 1.384738  [ 1520/ 3200]\n",
      "loss: 1.386494  [ 1536/ 3200]\n",
      "loss: 1.388142  [ 1552/ 3200]\n",
      "loss: 1.385021  [ 1568/ 3200]\n",
      "loss: 1.387751  [ 1584/ 3200]\n",
      "loss: 1.387209  [ 1600/ 3200]\n",
      "loss: 1.384998  [ 1616/ 3200]\n",
      "loss: 1.390352  [ 1632/ 3200]\n",
      "loss: 1.389311  [ 1648/ 3200]\n",
      "loss: 1.386494  [ 1664/ 3200]\n",
      "loss: 1.389896  [ 1680/ 3200]\n",
      "loss: 1.391239  [ 1696/ 3200]\n",
      "loss: 1.385562  [ 1712/ 3200]\n",
      "loss: 1.384999  [ 1728/ 3200]\n",
      "loss: 1.383244  [ 1744/ 3200]\n",
      "loss: 1.383589  [ 1760/ 3200]\n",
      "loss: 1.378823  [ 1776/ 3200]\n",
      "loss: 1.382639  [ 1792/ 3200]\n",
      "loss: 1.386276  [ 1808/ 3200]\n",
      "loss: 1.388463  [ 1824/ 3200]\n",
      "loss: 1.386840  [ 1840/ 3200]\n",
      "loss: 1.385133  [ 1856/ 3200]\n",
      "loss: 1.388705  [ 1872/ 3200]\n",
      "loss: 1.391652  [ 1888/ 3200]\n",
      "loss: 1.384176  [ 1904/ 3200]\n",
      "loss: 1.394770  [ 1920/ 3200]\n",
      "loss: 1.386840  [ 1936/ 3200]\n",
      "loss: 1.383353  [ 1952/ 3200]\n",
      "loss: 1.388877  [ 1968/ 3200]\n",
      "loss: 1.386407  [ 1984/ 3200]\n",
      "loss: 1.387663  [ 2000/ 3200]\n",
      "loss: 1.380323  [ 2016/ 3200]\n",
      "loss: 1.385020  [ 2032/ 3200]\n",
      "loss: 1.386385  [ 2048/ 3200]\n",
      "loss: 1.385672  [ 2064/ 3200]\n",
      "loss: 1.386515  [ 2080/ 3200]\n",
      "loss: 1.382898  [ 2096/ 3200]\n",
      "loss: 1.385930  [ 2112/ 3200]\n",
      "loss: 1.384156  [ 2128/ 3200]\n",
      "loss: 1.387296  [ 2144/ 3200]\n",
      "loss: 1.380084  [ 2160/ 3200]\n",
      "loss: 1.388595  [ 2176/ 3200]\n",
      "loss: 1.383244  [ 2192/ 3200]\n",
      "loss: 1.390804  [ 2208/ 3200]\n",
      "loss: 1.387535  [ 2224/ 3200]\n",
      "loss: 1.393467  [ 2240/ 3200]\n",
      "loss: 1.388161  [ 2256/ 3200]\n",
      "loss: 1.391063  [ 2272/ 3200]\n",
      "loss: 1.390455  [ 2288/ 3200]\n",
      "loss: 1.381601  [ 2304/ 3200]\n",
      "loss: 1.382986  [ 2320/ 3200]\n",
      "loss: 1.387663  [ 2336/ 3200]\n",
      "loss: 1.387663  [ 2352/ 3200]\n",
      "loss: 1.385931  [ 2368/ 3200]\n",
      "loss: 1.390822  [ 2384/ 3200]\n",
      "loss: 1.387336  [ 2400/ 3200]\n",
      "loss: 1.386492  [ 2416/ 3200]\n",
      "loss: 1.393396  [ 2432/ 3200]\n",
      "loss: 1.379271  [ 2448/ 3200]\n",
      "loss: 1.386840  [ 2464/ 3200]\n",
      "loss: 1.386753  [ 2480/ 3200]\n",
      "loss: 1.382406  [ 2496/ 3200]\n",
      "loss: 1.388699  [ 2512/ 3200]\n",
      "loss: 1.386016  [ 2528/ 3200]\n",
      "loss: 1.382340  [ 2544/ 3200]\n",
      "loss: 1.387315  [ 2560/ 3200]\n",
      "loss: 1.380132  [ 2576/ 3200]\n",
      "loss: 1.384177  [ 2592/ 3200]\n",
      "loss: 1.388573  [ 2608/ 3200]\n",
      "loss: 1.386753  [ 2624/ 3200]\n",
      "loss: 1.383462  [ 2640/ 3200]\n",
      "loss: 1.391148  [ 2656/ 3200]\n",
      "loss: 1.380218  [ 2672/ 3200]\n",
      "loss: 1.381777  [ 2688/ 3200]\n",
      "loss: 1.383355  [ 2704/ 3200]\n",
      "loss: 1.390366  [ 2720/ 3200]\n",
      "loss: 1.384981  [ 2736/ 3200]\n",
      "loss: 1.382792  [ 2752/ 3200]\n",
      "loss: 1.389890  [ 2768/ 3200]\n",
      "loss: 1.386019  [ 2784/ 3200]\n",
      "loss: 1.386019  [ 2800/ 3200]\n",
      "loss: 1.388701  [ 2816/ 3200]\n",
      "loss: 1.387663  [ 2832/ 3200]\n",
      "loss: 1.386037  [ 2848/ 3200]\n",
      "loss: 1.383830  [ 2864/ 3200]\n",
      "loss: 1.388941  [ 2880/ 3200]\n",
      "loss: 1.385108  [ 2896/ 3200]\n",
      "loss: 1.387295  [ 2912/ 3200]\n",
      "loss: 1.388572  [ 2928/ 3200]\n",
      "loss: 1.381256  [ 2944/ 3200]\n",
      "loss: 1.386016  [ 2960/ 3200]\n",
      "loss: 1.390218  [ 2976/ 3200]\n",
      "loss: 1.392898  [ 2992/ 3200]\n",
      "loss: 1.383725  [ 3008/ 3200]\n",
      "loss: 1.391642  [ 3024/ 3200]\n",
      "loss: 1.385215  [ 3040/ 3200]\n",
      "loss: 1.387682  [ 3056/ 3200]\n",
      "loss: 1.385911  [ 3072/ 3200]\n",
      "loss: 1.385804  [ 3088/ 3200]\n",
      "loss: 1.387769  [ 3104/ 3200]\n",
      "loss: 1.387228  [ 3120/ 3200]\n",
      "loss: 1.386860  [ 3136/ 3200]\n",
      "loss: 1.387314  [ 3152/ 3200]\n",
      "loss: 1.384073  [ 3168/ 3200]\n",
      "loss: 1.386191  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.8894e-04.\n",
      "\n",
      "Epoch: 47\n",
      "-----------------------------\n",
      "loss: 1.381715  [    0/ 3200]\n",
      "loss: 1.388698  [   16/ 3200]\n",
      "loss: 1.389761  [   32/ 3200]\n",
      "loss: 1.392073  [   48/ 3200]\n",
      "loss: 1.385089  [   64/ 3200]\n",
      "loss: 1.384848  [   80/ 3200]\n",
      "loss: 1.383357  [   96/ 3200]\n",
      "loss: 1.384634  [  112/ 3200]\n",
      "loss: 1.385088  [  128/ 3200]\n",
      "loss: 1.380571  [  144/ 3200]\n",
      "loss: 1.385477  [  160/ 3200]\n",
      "loss: 1.391057  [  176/ 3200]\n",
      "loss: 1.387682  [  192/ 3200]\n",
      "loss: 1.381368  [  208/ 3200]\n",
      "loss: 1.390321  [  224/ 3200]\n",
      "loss: 1.386491  [  240/ 3200]\n",
      "loss: 1.381607  [  256/ 3200]\n",
      "loss: 1.386775  [  272/ 3200]\n",
      "loss: 1.393717  [  288/ 3200]\n",
      "loss: 1.387681  [  304/ 3200]\n",
      "loss: 1.386965  [  320/ 3200]\n",
      "loss: 1.381521  [  336/ 3200]\n",
      "loss: 1.383358  [  352/ 3200]\n",
      "loss: 1.378948  [  368/ 3200]\n",
      "loss: 1.384286  [  384/ 3200]\n",
      "loss: 1.382778  [  400/ 3200]\n",
      "loss: 1.382902  [  416/ 3200]\n",
      "loss: 1.383269  [  432/ 3200]\n",
      "loss: 1.387486  [  448/ 3200]\n",
      "loss: 1.391163  [  464/ 3200]\n",
      "loss: 1.386750  [  480/ 3200]\n",
      "loss: 1.388027  [  496/ 3200]\n",
      "loss: 1.384652  [  512/ 3200]\n",
      "loss: 1.378018  [  528/ 3200]\n",
      "loss: 1.384651  [  544/ 3200]\n",
      "loss: 1.382886  [  560/ 3200]\n",
      "loss: 1.386826  [  576/ 3200]\n",
      "loss: 1.388483  [  592/ 3200]\n",
      "loss: 1.392547  [  608/ 3200]\n",
      "loss: 1.391617  [  624/ 3200]\n",
      "loss: 1.386385  [  640/ 3200]\n",
      "loss: 1.385929  [  656/ 3200]\n",
      "loss: 1.381849  [  672/ 3200]\n",
      "loss: 1.384070  [  688/ 3200]\n",
      "loss: 1.385092  [  704/ 3200]\n",
      "loss: 1.382759  [  720/ 3200]\n",
      "loss: 1.384160  [  736/ 3200]\n",
      "loss: 1.382409  [  752/ 3200]\n",
      "loss: 1.386275  [  768/ 3200]\n",
      "loss: 1.381133  [  784/ 3200]\n",
      "loss: 1.389522  [  800/ 3200]\n",
      "loss: 1.388846  [  816/ 3200]\n",
      "loss: 1.382773  [  832/ 3200]\n",
      "loss: 1.386496  [  848/ 3200]\n",
      "loss: 1.386385  [  864/ 3200]\n",
      "loss: 1.391162  [  880/ 3200]\n",
      "loss: 1.382540  [  896/ 3200]\n",
      "loss: 1.384745  [  912/ 3200]\n",
      "loss: 1.378582  [  928/ 3200]\n",
      "loss: 1.389412  [  944/ 3200]\n",
      "loss: 1.391617  [  960/ 3200]\n",
      "loss: 1.383594  [  976/ 3200]\n",
      "loss: 1.388025  [  992/ 3200]\n",
      "loss: 1.386363  [ 1008/ 3200]\n",
      "loss: 1.386363  [ 1024/ 3200]\n",
      "loss: 1.380566  [ 1040/ 3200]\n",
      "loss: 1.392548  [ 1056/ 3200]\n",
      "loss: 1.388113  [ 1072/ 3200]\n",
      "loss: 1.386473  [ 1088/ 3200]\n",
      "loss: 1.382453  [ 1104/ 3200]\n",
      "loss: 1.387316  [ 1120/ 3200]\n",
      "loss: 1.390795  [ 1136/ 3200]\n",
      "loss: 1.386385  [ 1152/ 3200]\n",
      "loss: 1.386839  [ 1168/ 3200]\n",
      "loss: 1.388932  [ 1184/ 3200]\n",
      "loss: 1.383472  [ 1200/ 3200]\n",
      "loss: 1.385931  [ 1216/ 3200]\n",
      "loss: 1.380702  [ 1232/ 3200]\n",
      "loss: 1.385087  [ 1248/ 3200]\n",
      "loss: 1.383336  [ 1264/ 3200]\n",
      "loss: 1.384180  [ 1280/ 3200]\n",
      "loss: 1.388136  [ 1296/ 3200]\n",
      "loss: 1.390230  [ 1312/ 3200]\n",
      "loss: 1.388478  [ 1328/ 3200]\n",
      "loss: 1.389887  [ 1344/ 3200]\n",
      "loss: 1.384634  [ 1360/ 3200]\n",
      "loss: 1.382541  [ 1376/ 3200]\n",
      "loss: 1.387316  [ 1392/ 3200]\n",
      "loss: 1.387682  [ 1408/ 3200]\n",
      "loss: 1.388589  [ 1424/ 3200]\n",
      "loss: 1.389409  [ 1440/ 3200]\n",
      "loss: 1.387402  [ 1456/ 3200]\n",
      "loss: 1.386886  [ 1472/ 3200]\n",
      "loss: 1.382321  [ 1488/ 3200]\n",
      "loss: 1.382222  [ 1504/ 3200]\n",
      "loss: 1.390339  [ 1520/ 3200]\n",
      "loss: 1.387095  [ 1536/ 3200]\n",
      "loss: 1.386297  [ 1552/ 3200]\n",
      "loss: 1.383705  [ 1568/ 3200]\n",
      "loss: 1.385698  [ 1584/ 3200]\n",
      "loss: 1.384181  [ 1600/ 3200]\n",
      "loss: 1.385434  [ 1616/ 3200]\n",
      "loss: 1.390903  [ 1632/ 3200]\n",
      "loss: 1.385001  [ 1648/ 3200]\n",
      "loss: 1.389752  [ 1664/ 3200]\n",
      "loss: 1.388954  [ 1680/ 3200]\n",
      "loss: 1.384292  [ 1696/ 3200]\n",
      "loss: 1.386706  [ 1712/ 3200]\n",
      "loss: 1.390879  [ 1728/ 3200]\n",
      "loss: 1.386727  [ 1744/ 3200]\n",
      "loss: 1.386409  [ 1760/ 3200]\n",
      "loss: 1.386972  [ 1776/ 3200]\n",
      "loss: 1.382323  [ 1792/ 3200]\n",
      "loss: 1.390337  [ 1808/ 3200]\n",
      "loss: 1.387791  [ 1824/ 3200]\n",
      "loss: 1.386019  [ 1840/ 3200]\n",
      "loss: 1.386384  [ 1856/ 3200]\n",
      "loss: 1.387570  [ 1872/ 3200]\n",
      "loss: 1.392062  [ 1888/ 3200]\n",
      "loss: 1.388133  [ 1904/ 3200]\n",
      "loss: 1.390445  [ 1920/ 3200]\n",
      "loss: 1.390312  [ 1936/ 3200]\n",
      "loss: 1.388243  [ 1952/ 3200]\n",
      "loss: 1.392878  [ 1968/ 3200]\n",
      "loss: 1.383731  [ 1984/ 3200]\n",
      "loss: 1.392424  [ 2000/ 3200]\n",
      "loss: 1.389061  [ 2016/ 3200]\n",
      "loss: 1.385090  [ 2032/ 3200]\n",
      "loss: 1.388387  [ 2048/ 3200]\n",
      "loss: 1.388264  [ 2064/ 3200]\n",
      "loss: 1.386274  [ 2080/ 3200]\n",
      "loss: 1.392510  [ 2096/ 3200]\n",
      "loss: 1.386495  [ 2112/ 3200]\n",
      "loss: 1.387181  [ 2128/ 3200]\n",
      "loss: 1.385566  [ 2144/ 3200]\n",
      "loss: 1.387313  [ 2160/ 3200]\n",
      "loss: 1.384207  [ 2176/ 3200]\n",
      "loss: 1.383732  [ 2192/ 3200]\n",
      "loss: 1.388563  [ 2208/ 3200]\n",
      "loss: 1.380007  [ 2224/ 3200]\n",
      "loss: 1.385566  [ 2240/ 3200]\n",
      "loss: 1.387765  [ 2256/ 3200]\n",
      "loss: 1.389315  [ 2272/ 3200]\n",
      "loss: 1.389855  [ 2288/ 3200]\n",
      "loss: 1.385092  [ 2304/ 3200]\n",
      "loss: 1.383951  [ 2320/ 3200]\n",
      "loss: 1.385200  [ 2336/ 3200]\n",
      "loss: 1.387312  [ 2352/ 3200]\n",
      "loss: 1.378969  [ 2368/ 3200]\n",
      "loss: 1.390072  [ 2384/ 3200]\n",
      "loss: 1.387182  [ 2400/ 3200]\n",
      "loss: 1.385910  [ 2416/ 3200]\n",
      "loss: 1.383819  [ 2432/ 3200]\n",
      "loss: 1.384638  [ 2448/ 3200]\n",
      "loss: 1.388047  [ 2464/ 3200]\n",
      "loss: 1.381535  [ 2480/ 3200]\n",
      "loss: 1.387419  [ 2496/ 3200]\n",
      "loss: 1.390222  [ 2512/ 3200]\n",
      "loss: 1.385457  [ 2528/ 3200]\n",
      "loss: 1.388583  [ 2544/ 3200]\n",
      "loss: 1.381621  [ 2560/ 3200]\n",
      "loss: 1.384293  [ 2576/ 3200]\n",
      "loss: 1.392183  [ 2592/ 3200]\n",
      "loss: 1.391515  [ 2608/ 3200]\n",
      "loss: 1.388928  [ 2624/ 3200]\n",
      "loss: 1.385565  [ 2640/ 3200]\n",
      "loss: 1.389854  [ 2656/ 3200]\n",
      "loss: 1.387203  [ 2672/ 3200]\n",
      "loss: 1.386858  [ 2688/ 3200]\n",
      "loss: 1.385587  [ 2704/ 3200]\n",
      "loss: 1.385911  [ 2720/ 3200]\n",
      "loss: 1.384639  [ 2736/ 3200]\n",
      "loss: 1.393710  [ 2752/ 3200]\n",
      "loss: 1.386620  [ 2768/ 3200]\n",
      "loss: 1.386384  [ 2784/ 3200]\n",
      "loss: 1.388688  [ 2800/ 3200]\n",
      "loss: 1.389959  [ 2816/ 3200]\n",
      "loss: 1.385459  [ 2832/ 3200]\n",
      "loss: 1.389766  [ 2848/ 3200]\n",
      "loss: 1.390777  [ 2864/ 3200]\n",
      "loss: 1.383369  [ 2880/ 3200]\n",
      "loss: 1.384188  [ 2896/ 3200]\n",
      "loss: 1.381519  [ 2912/ 3200]\n",
      "loss: 1.382571  [ 2928/ 3200]\n",
      "loss: 1.394159  [ 2944/ 3200]\n",
      "loss: 1.383735  [ 2960/ 3200]\n",
      "loss: 1.383822  [ 2976/ 3200]\n",
      "loss: 1.384747  [ 2992/ 3200]\n",
      "loss: 1.375383  [ 3008/ 3200]\n",
      "loss: 1.385479  [ 3024/ 3200]\n",
      "loss: 1.387290  [ 3040/ 3200]\n",
      "loss: 1.391723  [ 3056/ 3200]\n",
      "loss: 1.390325  [ 3072/ 3200]\n",
      "loss: 1.390131  [ 3088/ 3200]\n",
      "loss: 1.384988  [ 3104/ 3200]\n",
      "loss: 1.381626  [ 3120/ 3200]\n",
      "loss: 1.385931  [ 3136/ 3200]\n",
      "loss: 1.381885  [ 3152/ 3200]\n",
      "loss: 1.384641  [ 3168/ 3200]\n",
      "loss: 1.386856  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.7949e-04.\n",
      "\n",
      "Epoch: 48\n",
      "-----------------------------\n",
      "loss: 1.391142  [    0/ 3200]\n",
      "loss: 1.385095  [   16/ 3200]\n",
      "loss: 1.384641  [   32/ 3200]\n",
      "loss: 1.381991  [   48/ 3200]\n",
      "loss: 1.386384  [   64/ 3200]\n",
      "loss: 1.386384  [   80/ 3200]\n",
      "loss: 1.385566  [   96/ 3200]\n",
      "loss: 1.381645  [  112/ 3200]\n",
      "loss: 1.385584  [  128/ 3200]\n",
      "loss: 1.382551  [  144/ 3200]\n",
      "loss: 1.385583  [  160/ 3200]\n",
      "loss: 1.380006  [  176/ 3200]\n",
      "loss: 1.383263  [  192/ 3200]\n",
      "loss: 1.383733  [  208/ 3200]\n",
      "loss: 1.392522  [  224/ 3200]\n",
      "loss: 1.389035  [  240/ 3200]\n",
      "loss: 1.387186  [  256/ 3200]\n",
      "loss: 1.392974  [  272/ 3200]\n",
      "loss: 1.386961  [  288/ 3200]\n",
      "loss: 1.386384  [  304/ 3200]\n",
      "loss: 1.385095  [  320/ 3200]\n",
      "loss: 1.386837  [  336/ 3200]\n",
      "loss: 1.385199  [  352/ 3200]\n",
      "loss: 1.388842  [  368/ 3200]\n",
      "loss: 1.389486  [  384/ 3200]\n",
      "loss: 1.387413  [  400/ 3200]\n",
      "loss: 1.388946  [  416/ 3200]\n",
      "loss: 1.384066  [  432/ 3200]\n",
      "loss: 1.385566  [  448/ 3200]\n",
      "loss: 1.385217  [  464/ 3200]\n",
      "loss: 1.386280  [  480/ 3200]\n",
      "loss: 1.388127  [  496/ 3200]\n",
      "loss: 1.387674  [  512/ 3200]\n",
      "loss: 1.392412  [  528/ 3200]\n",
      "loss: 1.386018  [  544/ 3200]\n",
      "loss: 1.387883  [  560/ 3200]\n",
      "loss: 1.381995  [  576/ 3200]\n",
      "loss: 1.387656  [  592/ 3200]\n",
      "loss: 1.384067  [  608/ 3200]\n",
      "loss: 1.393768  [  624/ 3200]\n",
      "loss: 1.388473  [  640/ 3200]\n",
      "loss: 1.385460  [  656/ 3200]\n",
      "loss: 1.384190  [  672/ 3200]\n",
      "loss: 1.384642  [  688/ 3200]\n",
      "loss: 1.388146  [  704/ 3200]\n",
      "loss: 1.386489  [  720/ 3200]\n",
      "loss: 1.391610  [  736/ 3200]\n",
      "loss: 1.381526  [  752/ 3200]\n",
      "loss: 1.388944  [  768/ 3200]\n",
      "loss: 1.386124  [  784/ 3200]\n",
      "loss: 1.387307  [  800/ 3200]\n",
      "loss: 1.382901  [  816/ 3200]\n",
      "loss: 1.384556  [  832/ 3200]\n",
      "loss: 1.389378  [  848/ 3200]\n",
      "loss: 1.383929  [  864/ 3200]\n",
      "loss: 1.384314  [  880/ 3200]\n",
      "loss: 1.387759  [  896/ 3200]\n",
      "loss: 1.387117  [  912/ 3200]\n",
      "loss: 1.387307  [  928/ 3200]\n",
      "loss: 1.386384  [  944/ 3200]\n",
      "loss: 1.388211  [  960/ 3200]\n",
      "loss: 1.386384  [  976/ 3200]\n",
      "loss: 1.386940  [  992/ 3200]\n",
      "loss: 1.385583  [ 1008/ 3200]\n",
      "loss: 1.379438  [ 1024/ 3200]\n",
      "loss: 1.385565  [ 1040/ 3200]\n",
      "loss: 1.382329  [ 1056/ 3200]\n",
      "loss: 1.385010  [ 1072/ 3200]\n",
      "loss: 1.382345  [ 1088/ 3200]\n",
      "loss: 1.388700  [ 1104/ 3200]\n",
      "loss: 1.390686  [ 1120/ 3200]\n",
      "loss: 1.385669  [ 1136/ 3200]\n",
      "loss: 1.389275  [ 1152/ 3200]\n",
      "loss: 1.388944  [ 1168/ 3200]\n",
      "loss: 1.384765  [ 1184/ 3200]\n",
      "loss: 1.387672  [ 1200/ 3200]\n",
      "loss: 1.381179  [ 1216/ 3200]\n",
      "loss: 1.387845  [ 1232/ 3200]\n",
      "loss: 1.383372  [ 1248/ 3200]\n",
      "loss: 1.386733  [ 1264/ 3200]\n",
      "loss: 1.383636  [ 1280/ 3200]\n",
      "loss: 1.389482  [ 1296/ 3200]\n",
      "loss: 1.380257  [ 1312/ 3200]\n",
      "loss: 1.381631  [ 1328/ 3200]\n",
      "loss: 1.387777  [ 1344/ 3200]\n",
      "loss: 1.382902  [ 1360/ 3200]\n",
      "loss: 1.383390  [ 1376/ 3200]\n",
      "loss: 1.387638  [ 1392/ 3200]\n",
      "loss: 1.389867  [ 1408/ 3200]\n",
      "loss: 1.379805  [ 1424/ 3200]\n",
      "loss: 1.386384  [ 1440/ 3200]\n",
      "loss: 1.388143  [ 1456/ 3200]\n",
      "loss: 1.386487  [ 1472/ 3200]\n",
      "loss: 1.387775  [ 1488/ 3200]\n",
      "loss: 1.383636  [ 1504/ 3200]\n",
      "loss: 1.383166  [ 1520/ 3200]\n",
      "loss: 1.384992  [ 1536/ 3200]\n",
      "loss: 1.383372  [ 1552/ 3200]\n",
      "loss: 1.384190  [ 1568/ 3200]\n",
      "loss: 1.385375  [ 1584/ 3200]\n",
      "loss: 1.386749  [ 1600/ 3200]\n",
      "loss: 1.381178  [ 1616/ 3200]\n",
      "loss: 1.392044  [ 1632/ 3200]\n",
      "loss: 1.384086  [ 1648/ 3200]\n",
      "loss: 1.395800  [ 1664/ 3200]\n",
      "loss: 1.382102  [ 1680/ 3200]\n",
      "loss: 1.383584  [ 1696/ 3200]\n",
      "loss: 1.386473  [ 1712/ 3200]\n",
      "loss: 1.389031  [ 1728/ 3200]\n",
      "loss: 1.387639  [ 1744/ 3200]\n",
      "loss: 1.386854  [ 1760/ 3200]\n",
      "loss: 1.382799  [ 1776/ 3200]\n",
      "loss: 1.387655  [ 1792/ 3200]\n",
      "loss: 1.386384  [ 1808/ 3200]\n",
      "loss: 1.386837  [ 1824/ 3200]\n",
      "loss: 1.388020  [ 1840/ 3200]\n",
      "loss: 1.384086  [ 1856/ 3200]\n",
      "loss: 1.386384  [ 1872/ 3200]\n",
      "loss: 1.387047  [ 1888/ 3200]\n",
      "loss: 1.386384  [ 1904/ 3200]\n",
      "loss: 1.386384  [ 1920/ 3200]\n",
      "loss: 1.391604  [ 1936/ 3200]\n",
      "loss: 1.388560  [ 1952/ 3200]\n",
      "loss: 1.387672  [ 1968/ 3200]\n",
      "loss: 1.383844  [ 1984/ 3200]\n",
      "loss: 1.382817  [ 2000/ 3200]\n",
      "loss: 1.389482  [ 2016/ 3200]\n",
      "loss: 1.384992  [ 2032/ 3200]\n",
      "loss: 1.385462  [ 2048/ 3200]\n",
      "loss: 1.386731  [ 2064/ 3200]\n",
      "loss: 1.384749  [ 2080/ 3200]\n",
      "loss: 1.388124  [ 2096/ 3200]\n",
      "loss: 1.388019  [ 2112/ 3200]\n",
      "loss: 1.386854  [ 2128/ 3200]\n",
      "loss: 1.386401  [ 2144/ 3200]\n",
      "loss: 1.388593  [ 2160/ 3200]\n",
      "loss: 1.389393  [ 2176/ 3200]\n",
      "loss: 1.386748  [ 2192/ 3200]\n",
      "loss: 1.382348  [ 2208/ 3200]\n",
      "loss: 1.382453  [ 2224/ 3200]\n",
      "loss: 1.390784  [ 2240/ 3200]\n",
      "loss: 1.388939  [ 2256/ 3200]\n",
      "loss: 1.386036  [ 2272/ 3200]\n",
      "loss: 1.384298  [ 2288/ 3200]\n",
      "loss: 1.389149  [ 2304/ 3200]\n",
      "loss: 1.387550  [ 2320/ 3200]\n",
      "loss: 1.393306  [ 2336/ 3200]\n",
      "loss: 1.387759  [ 2352/ 3200]\n",
      "loss: 1.387095  [ 2368/ 3200]\n",
      "loss: 1.389028  [ 2384/ 3200]\n",
      "loss: 1.383724  [ 2400/ 3200]\n",
      "loss: 1.386731  [ 2416/ 3200]\n",
      "loss: 1.381097  [ 2432/ 3200]\n",
      "loss: 1.387290  [ 2448/ 3200]\n",
      "loss: 1.384087  [ 2464/ 3200]\n",
      "loss: 1.385461  [ 2480/ 3200]\n",
      "loss: 1.389390  [ 2496/ 3200]\n",
      "loss: 1.386384  [ 2512/ 3200]\n",
      "loss: 1.389283  [ 2528/ 3200]\n",
      "loss: 1.383270  [ 2544/ 3200]\n",
      "loss: 1.386367  [ 2560/ 3200]\n",
      "loss: 1.387759  [ 2576/ 3200]\n",
      "loss: 1.390675  [ 2592/ 3200]\n",
      "loss: 1.387759  [ 2608/ 3200]\n",
      "loss: 1.382563  [ 2624/ 3200]\n",
      "loss: 1.385931  [ 2640/ 3200]\n",
      "loss: 1.391473  [ 2656/ 3200]\n",
      "loss: 1.388919  [ 2672/ 3200]\n",
      "loss: 1.388013  [ 2688/ 3200]\n",
      "loss: 1.381534  [ 2704/ 3200]\n",
      "loss: 1.389949  [ 2720/ 3200]\n",
      "loss: 1.388682  [ 2736/ 3200]\n",
      "loss: 1.389025  [ 2752/ 3200]\n",
      "loss: 1.383379  [ 2768/ 3200]\n",
      "loss: 1.385568  [ 2784/ 3200]\n",
      "loss: 1.388121  [ 2800/ 3200]\n",
      "loss: 1.385461  [ 2816/ 3200]\n",
      "loss: 1.385099  [ 2832/ 3200]\n",
      "loss: 1.388465  [ 2848/ 3200]\n",
      "loss: 1.385098  [ 2864/ 3200]\n",
      "loss: 1.382005  [ 2880/ 3200]\n",
      "loss: 1.385442  [ 2896/ 3200]\n",
      "loss: 1.380483  [ 2912/ 3200]\n",
      "loss: 1.388829  [ 2928/ 3200]\n",
      "loss: 1.389044  [ 2944/ 3200]\n",
      "loss: 1.383743  [ 2960/ 3200]\n",
      "loss: 1.383035  [ 2976/ 3200]\n",
      "loss: 1.389495  [ 2992/ 3200]\n",
      "loss: 1.385806  [ 3008/ 3200]\n",
      "loss: 1.382820  [ 3024/ 3200]\n",
      "loss: 1.385569  [ 3040/ 3200]\n",
      "loss: 1.383742  [ 3056/ 3200]\n",
      "loss: 1.385676  [ 3072/ 3200]\n",
      "loss: 1.389751  [ 3088/ 3200]\n",
      "loss: 1.391107  [ 3104/ 3200]\n",
      "loss: 1.385314  [ 3120/ 3200]\n",
      "loss: 1.387413  [ 3136/ 3200]\n",
      "loss: 1.392392  [ 3152/ 3200]\n",
      "loss: 1.388590  [ 3168/ 3200]\n",
      "loss: 1.389387  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.7052e-04.\n",
      "\n",
      "Epoch: 49\n",
      "-----------------------------\n",
      "loss: 1.386490  [    0/ 3200]\n",
      "loss: 1.384771  [   16/ 3200]\n",
      "loss: 1.381538  [   32/ 3200]\n",
      "loss: 1.385445  [   48/ 3200]\n",
      "loss: 1.381644  [   64/ 3200]\n",
      "loss: 1.389475  [   80/ 3200]\n",
      "loss: 1.384627  [   96/ 3200]\n",
      "loss: 1.386384  [  112/ 3200]\n",
      "loss: 1.388552  [  128/ 3200]\n",
      "loss: 1.383725  [  144/ 3200]\n",
      "loss: 1.382565  [  160/ 3200]\n",
      "loss: 1.383486  [  176/ 3200]\n",
      "loss: 1.389472  [  192/ 3200]\n",
      "loss: 1.386750  [  208/ 3200]\n",
      "loss: 1.380376  [  224/ 3200]\n",
      "loss: 1.387755  [  240/ 3200]\n",
      "loss: 1.385096  [  256/ 3200]\n",
      "loss: 1.390204  [  272/ 3200]\n",
      "loss: 1.390696  [  288/ 3200]\n",
      "loss: 1.384196  [  304/ 3200]\n",
      "loss: 1.388486  [  320/ 3200]\n",
      "loss: 1.389751  [  336/ 3200]\n",
      "loss: 1.389280  [  352/ 3200]\n",
      "loss: 1.388226  [  368/ 3200]\n",
      "loss: 1.389942  [  384/ 3200]\n",
      "loss: 1.381539  [  400/ 3200]\n",
      "loss: 1.389041  [  416/ 3200]\n",
      "loss: 1.391678  [  432/ 3200]\n",
      "loss: 1.388801  [  448/ 3200]\n",
      "loss: 1.382115  [  464/ 3200]\n",
      "loss: 1.386279  [  480/ 3200]\n",
      "loss: 1.382566  [  496/ 3200]\n",
      "loss: 1.386298  [  512/ 3200]\n",
      "loss: 1.381177  [  528/ 3200]\n",
      "loss: 1.391122  [  544/ 3200]\n",
      "loss: 1.385723  [  560/ 3200]\n",
      "loss: 1.382912  [  576/ 3200]\n",
      "loss: 1.387995  [  592/ 3200]\n",
      "loss: 1.386384  [  608/ 3200]\n",
      "loss: 1.388464  [  624/ 3200]\n",
      "loss: 1.383940  [  640/ 3200]\n",
      "loss: 1.388570  [  656/ 3200]\n",
      "loss: 1.389921  [  672/ 3200]\n",
      "loss: 1.385098  [  688/ 3200]\n",
      "loss: 1.389834  [  704/ 3200]\n",
      "loss: 1.383277  [  720/ 3200]\n",
      "loss: 1.388119  [  736/ 3200]\n",
      "loss: 1.389855  [  752/ 3200]\n",
      "loss: 1.388098  [  768/ 3200]\n",
      "loss: 1.388569  [  784/ 3200]\n",
      "loss: 1.386039  [  800/ 3200]\n",
      "loss: 1.387648  [  816/ 3200]\n",
      "loss: 1.386362  [  832/ 3200]\n",
      "loss: 1.388224  [  848/ 3200]\n",
      "loss: 1.387304  [  864/ 3200]\n",
      "loss: 1.384992  [  880/ 3200]\n",
      "loss: 1.384670  [  896/ 3200]\n",
      "loss: 1.379569  [  912/ 3200]\n",
      "loss: 1.382933  [  928/ 3200]\n",
      "loss: 1.388590  [  944/ 3200]\n",
      "loss: 1.390199  [  960/ 3200]\n",
      "loss: 1.386363  [  976/ 3200]\n",
      "loss: 1.382463  [  992/ 3200]\n",
      "loss: 1.385934  [ 1008/ 3200]\n",
      "loss: 1.390325  [ 1024/ 3200]\n",
      "loss: 1.382568  [ 1040/ 3200]\n",
      "loss: 1.389383  [ 1056/ 3200]\n",
      "loss: 1.383749  [ 1072/ 3200]\n",
      "loss: 1.388933  [ 1088/ 3200]\n",
      "loss: 1.389833  [ 1104/ 3200]\n",
      "loss: 1.381074  [ 1120/ 3200]\n",
      "loss: 1.389403  [ 1136/ 3200]\n",
      "loss: 1.388032  [ 1152/ 3200]\n",
      "loss: 1.385482  [ 1168/ 3200]\n",
      "loss: 1.389939  [ 1184/ 3200]\n",
      "loss: 1.387754  [ 1200/ 3200]\n",
      "loss: 1.386125  [ 1216/ 3200]\n",
      "loss: 1.385914  [ 1232/ 3200]\n",
      "loss: 1.388567  [ 1248/ 3200]\n",
      "loss: 1.389832  [ 1264/ 3200]\n",
      "loss: 1.390110  [ 1280/ 3200]\n",
      "loss: 1.382227  [ 1296/ 3200]\n",
      "loss: 1.384199  [ 1312/ 3200]\n",
      "loss: 1.388012  [ 1328/ 3200]\n",
      "loss: 1.383491  [ 1344/ 3200]\n",
      "loss: 1.392379  [ 1360/ 3200]\n",
      "loss: 1.387092  [ 1376/ 3200]\n",
      "loss: 1.384181  [ 1392/ 3200]\n",
      "loss: 1.390300  [ 1408/ 3200]\n",
      "loss: 1.386814  [ 1424/ 3200]\n",
      "loss: 1.390300  [ 1440/ 3200]\n",
      "loss: 1.386747  [ 1456/ 3200]\n",
      "loss: 1.387216  [ 1472/ 3200]\n",
      "loss: 1.387772  [ 1488/ 3200]\n",
      "loss: 1.388460  [ 1504/ 3200]\n",
      "loss: 1.383856  [ 1520/ 3200]\n",
      "loss: 1.387390  [ 1536/ 3200]\n",
      "loss: 1.386259  [ 1552/ 3200]\n",
      "loss: 1.386364  [ 1568/ 3200]\n",
      "loss: 1.385933  [ 1584/ 3200]\n",
      "loss: 1.385933  [ 1600/ 3200]\n",
      "loss: 1.387666  [ 1616/ 3200]\n",
      "loss: 1.384563  [ 1632/ 3200]\n",
      "loss: 1.392030  [ 1648/ 3200]\n",
      "loss: 1.391923  [ 1664/ 3200]\n",
      "loss: 1.380287  [ 1680/ 3200]\n",
      "loss: 1.384201  [ 1696/ 3200]\n",
      "loss: 1.386490  [ 1712/ 3200]\n",
      "loss: 1.391923  [ 1728/ 3200]\n",
      "loss: 1.383282  [ 1744/ 3200]\n",
      "loss: 1.385589  [ 1760/ 3200]\n",
      "loss: 1.380287  [ 1776/ 3200]\n",
      "loss: 1.390190  [ 1792/ 3200]\n",
      "loss: 1.387303  [ 1808/ 3200]\n",
      "loss: 1.378087  [ 1824/ 3200]\n",
      "loss: 1.388204  [ 1840/ 3200]\n",
      "loss: 1.385677  [ 1856/ 3200]\n",
      "loss: 1.385589  [ 1872/ 3200]\n",
      "loss: 1.391472  [ 1888/ 3200]\n",
      "loss: 1.386851  [ 1904/ 3200]\n",
      "loss: 1.389033  [ 1920/ 3200]\n",
      "loss: 1.390084  [ 1936/ 3200]\n",
      "loss: 1.381421  [ 1952/ 3200]\n",
      "loss: 1.384201  [ 1968/ 3200]\n",
      "loss: 1.384668  [ 1984/ 3200]\n",
      "loss: 1.383734  [ 2000/ 3200]\n",
      "loss: 1.386744  [ 2016/ 3200]\n",
      "loss: 1.383373  [ 2032/ 3200]\n",
      "loss: 1.386489  [ 2048/ 3200]\n",
      "loss: 1.389016  [ 2064/ 3200]\n",
      "loss: 1.388548  [ 2080/ 3200]\n",
      "loss: 1.385014  [ 2096/ 3200]\n",
      "loss: 1.388927  [ 2112/ 3200]\n",
      "loss: 1.383389  [ 2128/ 3200]\n",
      "loss: 1.387646  [ 2144/ 3200]\n",
      "loss: 1.388007  [ 2160/ 3200]\n",
      "loss: 1.384996  [ 2176/ 3200]\n",
      "loss: 1.382002  [ 2192/ 3200]\n",
      "loss: 1.386347  [ 2208/ 3200]\n",
      "loss: 1.388076  [ 2224/ 3200]\n",
      "loss: 1.387772  [ 2240/ 3200]\n",
      "loss: 1.393846  [ 2256/ 3200]\n",
      "loss: 1.388027  [ 2272/ 3200]\n",
      "loss: 1.382940  [ 2288/ 3200]\n",
      "loss: 1.381190  [ 2304/ 3200]\n",
      "loss: 1.382021  [ 2320/ 3200]\n",
      "loss: 1.388222  [ 2336/ 3200]\n",
      "loss: 1.387176  [ 2352/ 3200]\n",
      "loss: 1.383283  [ 2368/ 3200]\n",
      "loss: 1.386940  [ 2384/ 3200]\n",
      "loss: 1.387645  [ 2400/ 3200]\n",
      "loss: 1.389590  [ 2416/ 3200]\n",
      "loss: 1.386726  [ 2432/ 3200]\n",
      "loss: 1.387771  [ 2448/ 3200]\n",
      "loss: 1.388926  [ 2464/ 3200]\n",
      "loss: 1.384565  [ 2480/ 3200]\n",
      "loss: 1.386509  [ 2496/ 3200]\n",
      "loss: 1.386489  [ 2512/ 3200]\n",
      "loss: 1.387751  [ 2528/ 3200]\n",
      "loss: 1.384547  [ 2544/ 3200]\n",
      "loss: 1.387283  [ 2560/ 3200]\n",
      "loss: 1.383715  [ 2576/ 3200]\n",
      "loss: 1.381210  [ 2592/ 3200]\n",
      "loss: 1.385444  [ 2608/ 3200]\n",
      "loss: 1.388906  [ 2624/ 3200]\n",
      "loss: 1.388478  [ 2640/ 3200]\n",
      "loss: 1.384652  [ 2656/ 3200]\n",
      "loss: 1.385592  [ 2672/ 3200]\n",
      "loss: 1.387858  [ 2688/ 3200]\n",
      "loss: 1.383754  [ 2704/ 3200]\n",
      "loss: 1.383284  [ 2720/ 3200]\n",
      "loss: 1.381316  [ 2736/ 3200]\n",
      "loss: 1.381916  [ 2752/ 3200]\n",
      "loss: 1.386020  [ 2768/ 3200]\n",
      "loss: 1.388457  [ 2784/ 3200]\n",
      "loss: 1.390274  [ 2800/ 3200]\n",
      "loss: 1.392005  [ 2816/ 3200]\n",
      "loss: 1.387195  [ 2832/ 3200]\n",
      "loss: 1.386147  [ 2848/ 3200]\n",
      "loss: 1.387281  [ 2864/ 3200]\n",
      "loss: 1.385913  [ 2880/ 3200]\n",
      "loss: 1.387217  [ 2896/ 3200]\n",
      "loss: 1.389608  [ 2912/ 3200]\n",
      "loss: 1.393284  [ 2928/ 3200]\n",
      "loss: 1.385121  [ 2944/ 3200]\n",
      "loss: 1.386039  [ 2960/ 3200]\n",
      "loss: 1.384290  [ 2976/ 3200]\n",
      "loss: 1.389290  [ 2992/ 3200]\n",
      "loss: 1.383391  [ 3008/ 3200]\n",
      "loss: 1.385380  [ 3024/ 3200]\n",
      "loss: 1.382731  [ 3040/ 3200]\n",
      "loss: 1.386832  [ 3056/ 3200]\n",
      "loss: 1.379376  [ 3072/ 3200]\n",
      "loss: 1.389374  [ 3088/ 3200]\n",
      "loss: 1.383861  [ 3104/ 3200]\n",
      "loss: 1.387282  [ 3120/ 3200]\n",
      "loss: 1.382493  [ 3136/ 3200]\n",
      "loss: 1.382854  [ 3152/ 3200]\n",
      "loss: 1.382941  [ 3168/ 3200]\n",
      "loss: 1.383859  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.6199e-04.\n",
      "\n",
      "Epoch: 50\n",
      "-----------------------------\n",
      "loss: 1.391087  [    0/ 3200]\n",
      "loss: 1.387663  [   16/ 3200]\n",
      "loss: 1.387751  [   32/ 3200]\n",
      "loss: 1.388218  [   48/ 3200]\n",
      "loss: 1.387044  [   64/ 3200]\n",
      "loss: 1.387300  [   80/ 3200]\n",
      "loss: 1.384758  [   96/ 3200]\n",
      "loss: 1.385950  [  112/ 3200]\n",
      "loss: 1.386745  [  128/ 3200]\n",
      "loss: 1.381558  [  144/ 3200]\n",
      "loss: 1.392664  [  160/ 3200]\n",
      "loss: 1.388474  [  176/ 3200]\n",
      "loss: 1.383392  [  192/ 3200]\n",
      "loss: 1.391191  [  208/ 3200]\n",
      "loss: 1.387749  [  224/ 3200]\n",
      "loss: 1.384758  [  240/ 3200]\n",
      "loss: 1.385121  [  256/ 3200]\n",
      "loss: 1.386866  [  272/ 3200]\n",
      "loss: 1.389374  [  288/ 3200]\n",
      "loss: 1.385917  [  304/ 3200]\n",
      "loss: 1.383635  [  320/ 3200]\n",
      "loss: 1.385208  [  336/ 3200]\n",
      "loss: 1.389131  [  352/ 3200]\n",
      "loss: 1.382579  [  368/ 3200]\n",
      "loss: 1.385017  [  384/ 3200]\n",
      "loss: 1.383045  [  400/ 3200]\n",
      "loss: 1.385933  [  416/ 3200]\n",
      "loss: 1.382010  [  432/ 3200]\n",
      "loss: 1.383635  [  448/ 3200]\n",
      "loss: 1.389391  [  464/ 3200]\n",
      "loss: 1.386849  [  480/ 3200]\n",
      "loss: 1.384204  [  496/ 3200]\n",
      "loss: 1.382926  [  512/ 3200]\n",
      "loss: 1.390724  [  528/ 3200]\n",
      "loss: 1.386832  [  544/ 3200]\n",
      "loss: 1.378465  [  560/ 3200]\n",
      "loss: 1.387748  [  576/ 3200]\n",
      "loss: 1.384550  [  592/ 3200]\n",
      "loss: 1.385380  [  608/ 3200]\n",
      "loss: 1.388094  [  624/ 3200]\n",
      "loss: 1.386037  [  640/ 3200]\n",
      "loss: 1.385103  [  656/ 3200]\n",
      "loss: 1.387195  [  672/ 3200]\n",
      "loss: 1.384758  [  688/ 3200]\n",
      "loss: 1.381213  [  704/ 3200]\n",
      "loss: 1.386953  [  720/ 3200]\n",
      "loss: 1.382026  [  736/ 3200]\n",
      "loss: 1.388095  [  752/ 3200]\n",
      "loss: 1.388682  [  768/ 3200]\n",
      "loss: 1.386849  [  784/ 3200]\n",
      "loss: 1.386383  [  800/ 3200]\n",
      "loss: 1.389271  [  816/ 3200]\n",
      "loss: 1.392814  [  832/ 3200]\n",
      "loss: 1.382010  [  848/ 3200]\n",
      "loss: 1.390272  [  864/ 3200]\n",
      "loss: 1.386486  [  880/ 3200]\n",
      "loss: 1.385552  [  896/ 3200]\n",
      "loss: 1.389634  [  912/ 3200]\n",
      "loss: 1.388560  [  928/ 3200]\n",
      "loss: 1.382132  [  944/ 3200]\n",
      "loss: 1.389009  [  960/ 3200]\n",
      "loss: 1.383842  [  976/ 3200]\n",
      "loss: 1.391895  [  992/ 3200]\n",
      "loss: 1.386038  [ 1008/ 3200]\n",
      "loss: 1.388008  [ 1024/ 3200]\n",
      "loss: 1.387110  [ 1040/ 3200]\n",
      "loss: 1.385589  [ 1056/ 3200]\n",
      "loss: 1.387281  [ 1072/ 3200]\n",
      "loss: 1.384102  [ 1088/ 3200]\n",
      "loss: 1.387195  [ 1104/ 3200]\n",
      "loss: 1.389353  [ 1120/ 3200]\n",
      "loss: 1.387748  [ 1136/ 3200]\n",
      "loss: 1.385208  [ 1152/ 3200]\n",
      "loss: 1.389923  [ 1168/ 3200]\n",
      "loss: 1.384674  [ 1184/ 3200]\n",
      "loss: 1.389371  [ 1200/ 3200]\n",
      "loss: 1.388092  [ 1216/ 3200]\n",
      "loss: 1.385018  [ 1232/ 3200]\n",
      "loss: 1.389370  [ 1248/ 3200]\n",
      "loss: 1.383376  [ 1264/ 3200]\n",
      "loss: 1.387298  [ 1280/ 3200]\n",
      "loss: 1.384310  [ 1296/ 3200]\n",
      "loss: 1.385019  [ 1312/ 3200]\n",
      "loss: 1.382946  [ 1328/ 3200]\n",
      "loss: 1.387279  [ 1344/ 3200]\n",
      "loss: 1.388662  [ 1360/ 3200]\n",
      "loss: 1.392359  [ 1376/ 3200]\n",
      "loss: 1.379148  [ 1392/ 3200]\n",
      "loss: 1.385467  [ 1408/ 3200]\n",
      "loss: 1.386486  [ 1424/ 3200]\n",
      "loss: 1.380751  [ 1440/ 3200]\n",
      "loss: 1.382134  [ 1456/ 3200]\n",
      "loss: 1.385570  [ 1472/ 3200]\n",
      "loss: 1.384591  [ 1488/ 3200]\n",
      "loss: 1.390716  [ 1504/ 3200]\n",
      "loss: 1.387682  [ 1520/ 3200]\n",
      "loss: 1.380752  [ 1536/ 3200]\n",
      "loss: 1.385589  [ 1552/ 3200]\n",
      "loss: 1.389473  [ 1568/ 3200]\n",
      "loss: 1.385588  [ 1584/ 3200]\n",
      "loss: 1.389371  [ 1600/ 3200]\n",
      "loss: 1.379270  [ 1616/ 3200]\n",
      "loss: 1.386485  [ 1632/ 3200]\n",
      "loss: 1.389819  [ 1648/ 3200]\n",
      "loss: 1.390631  [ 1664/ 3200]\n",
      "loss: 1.378941  [ 1680/ 3200]\n",
      "loss: 1.381097  [ 1696/ 3200]\n",
      "loss: 1.390099  [ 1712/ 3200]\n",
      "loss: 1.389130  [ 1728/ 3200]\n",
      "loss: 1.387280  [ 1744/ 3200]\n",
      "loss: 1.386176  [ 1760/ 3200]\n",
      "loss: 1.380771  [ 1776/ 3200]\n",
      "loss: 1.391546  [ 1792/ 3200]\n",
      "loss: 1.388110  [ 1808/ 3200]\n",
      "loss: 1.389025  [ 1824/ 3200]\n",
      "loss: 1.389387  [ 1840/ 3200]\n",
      "loss: 1.386486  [ 1856/ 3200]\n",
      "loss: 1.388109  [ 1872/ 3200]\n",
      "loss: 1.387660  [ 1888/ 3200]\n",
      "loss: 1.389905  [ 1904/ 3200]\n",
      "loss: 1.383759  [ 1920/ 3200]\n",
      "loss: 1.385485  [ 1936/ 3200]\n",
      "loss: 1.389921  [ 1952/ 3200]\n",
      "loss: 1.383293  [ 1968/ 3200]\n",
      "loss: 1.388903  [ 1984/ 3200]\n",
      "loss: 1.389264  [ 2000/ 3200]\n",
      "loss: 1.392440  [ 2016/ 3200]\n",
      "loss: 1.385467  [ 2032/ 3200]\n",
      "loss: 1.390645  [ 2048/ 3200]\n",
      "loss: 1.382378  [ 2064/ 3200]\n",
      "loss: 1.387660  [ 2080/ 3200]\n",
      "loss: 1.390626  [ 2096/ 3200]\n",
      "loss: 1.385467  [ 2112/ 3200]\n",
      "loss: 1.384657  [ 2128/ 3200]\n",
      "loss: 1.386400  [ 2144/ 3200]\n",
      "loss: 1.383398  [ 2160/ 3200]\n",
      "loss: 1.389040  [ 2176/ 3200]\n",
      "loss: 1.386382  [ 2192/ 3200]\n",
      "loss: 1.388556  [ 2208/ 3200]\n",
      "loss: 1.392006  [ 2224/ 3200]\n",
      "loss: 1.386503  [ 2240/ 3200]\n",
      "loss: 1.379381  [ 2256/ 3200]\n",
      "loss: 1.383608  [ 2272/ 3200]\n",
      "loss: 1.381569  [ 2288/ 3200]\n",
      "loss: 1.382933  [ 2304/ 3200]\n",
      "loss: 1.381224  [ 2320/ 3200]\n",
      "loss: 1.381568  [ 2336/ 3200]\n",
      "loss: 1.389816  [ 2352/ 3200]\n",
      "loss: 1.389127  [ 2368/ 3200]\n",
      "loss: 1.382932  [ 2384/ 3200]\n",
      "loss: 1.383846  [ 2400/ 3200]\n",
      "loss: 1.384208  [ 2416/ 3200]\n",
      "loss: 1.386727  [ 2432/ 3200]\n",
      "loss: 1.389281  [ 2448/ 3200]\n",
      "loss: 1.378239  [ 2464/ 3200]\n",
      "loss: 1.382016  [ 2480/ 3200]\n",
      "loss: 1.384312  [ 2496/ 3200]\n",
      "loss: 1.388901  [ 2512/ 3200]\n",
      "loss: 1.385123  [ 2528/ 3200]\n",
      "loss: 1.388574  [ 2544/ 3200]\n",
      "loss: 1.394614  [ 2560/ 3200]\n",
      "loss: 1.392093  [ 2576/ 3200]\n",
      "loss: 1.391988  [ 2592/ 3200]\n",
      "loss: 1.388193  [ 2608/ 3200]\n",
      "loss: 1.389003  [ 2624/ 3200]\n",
      "loss: 1.384313  [ 2640/ 3200]\n",
      "loss: 1.380778  [ 2656/ 3200]\n",
      "loss: 1.390831  [ 2672/ 3200]\n",
      "loss: 1.386709  [ 2688/ 3200]\n",
      "loss: 1.383295  [ 2704/ 3200]\n",
      "loss: 1.387297  [ 2720/ 3200]\n",
      "loss: 1.389263  [ 2736/ 3200]\n",
      "loss: 1.379398  [ 2752/ 3200]\n",
      "loss: 1.385831  [ 2768/ 3200]\n",
      "loss: 1.383055  [ 2784/ 3200]\n",
      "loss: 1.385124  [ 2800/ 3200]\n",
      "loss: 1.385020  [ 2816/ 3200]\n",
      "loss: 1.387556  [ 2832/ 3200]\n",
      "loss: 1.386382  [ 2848/ 3200]\n",
      "loss: 1.388555  [ 2864/ 3200]\n",
      "loss: 1.386039  [ 2880/ 3200]\n",
      "loss: 1.384209  [ 2896/ 3200]\n",
      "loss: 1.385364  [ 2912/ 3200]\n",
      "loss: 1.388125  [ 2928/ 3200]\n",
      "loss: 1.390728  [ 2944/ 3200]\n",
      "loss: 1.391538  [ 2960/ 3200]\n",
      "loss: 1.385020  [ 2976/ 3200]\n",
      "loss: 1.385124  [ 2992/ 3200]\n",
      "loss: 1.392242  [ 3008/ 3200]\n",
      "loss: 1.390023  [ 3024/ 3200]\n",
      "loss: 1.385572  [ 3040/ 3200]\n",
      "loss: 1.383849  [ 3056/ 3200]\n",
      "loss: 1.383400  [ 3072/ 3200]\n",
      "loss: 1.384934  [ 3088/ 3200]\n",
      "loss: 1.382365  [ 3104/ 3200]\n",
      "loss: 1.387553  [ 3120/ 3200]\n",
      "loss: 1.383638  [ 3136/ 3200]\n",
      "loss: 1.387640  [ 3152/ 3200]\n",
      "loss: 1.385485  [ 3168/ 3200]\n",
      "loss: 1.386506  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.5389e-04.\n",
      "\n",
      "Epoch: 51\n",
      "-----------------------------\n",
      "loss: 1.383295  [    0/ 3200]\n",
      "loss: 1.390279  [   16/ 3200]\n",
      "loss: 1.386382  [   32/ 3200]\n",
      "loss: 1.385107  [   48/ 3200]\n",
      "loss: 1.388212  [   64/ 3200]\n",
      "loss: 1.388106  [   80/ 3200]\n",
      "loss: 1.391070  [   96/ 3200]\n",
      "loss: 1.386400  [  112/ 3200]\n",
      "loss: 1.390620  [  128/ 3200]\n",
      "loss: 1.389125  [  144/ 3200]\n",
      "loss: 1.386487  [  160/ 3200]\n",
      "loss: 1.389346  [  176/ 3200]\n",
      "loss: 1.389107  [  192/ 3200]\n",
      "loss: 1.384764  [  208/ 3200]\n",
      "loss: 1.384659  [  224/ 3200]\n",
      "loss: 1.393582  [  240/ 3200]\n",
      "loss: 1.387762  [  256/ 3200]\n",
      "loss: 1.389018  [  272/ 3200]\n",
      "loss: 1.387295  [  288/ 3200]\n",
      "loss: 1.387174  [  304/ 3200]\n",
      "loss: 1.389379  [  320/ 3200]\n",
      "loss: 1.388552  [  336/ 3200]\n",
      "loss: 1.379872  [  352/ 3200]\n",
      "loss: 1.378958  [  368/ 3200]\n",
      "loss: 1.385916  [  384/ 3200]\n",
      "loss: 1.391189  [  400/ 3200]\n",
      "loss: 1.380319  [  416/ 3200]\n",
      "loss: 1.387464  [  432/ 3200]\n",
      "loss: 1.385557  [  448/ 3200]\n",
      "loss: 1.388447  [  464/ 3200]\n",
      "loss: 1.382955  [  480/ 3200]\n",
      "loss: 1.382938  [  496/ 3200]\n",
      "loss: 1.385916  [  512/ 3200]\n",
      "loss: 1.385934  [  528/ 3200]\n",
      "loss: 1.384871  [  544/ 3200]\n",
      "loss: 1.387191  [  560/ 3200]\n",
      "loss: 1.382612  [  576/ 3200]\n",
      "loss: 1.385917  [  592/ 3200]\n",
      "loss: 1.384572  [  608/ 3200]\n",
      "loss: 1.386039  [  624/ 3200]\n",
      "loss: 1.385590  [  640/ 3200]\n",
      "loss: 1.388104  [  656/ 3200]\n",
      "loss: 1.380303  [  672/ 3200]\n",
      "loss: 1.378701  [  688/ 3200]\n",
      "loss: 1.393704  [  704/ 3200]\n",
      "loss: 1.384869  [  720/ 3200]\n",
      "loss: 1.382937  [  736/ 3200]\n",
      "loss: 1.380061  [  752/ 3200]\n",
      "loss: 1.384418  [  768/ 3200]\n",
      "loss: 1.386037  [  784/ 3200]\n",
      "loss: 1.390723  [  800/ 3200]\n",
      "loss: 1.391120  [  816/ 3200]\n",
      "loss: 1.382954  [  832/ 3200]\n",
      "loss: 1.395323  [  848/ 3200]\n",
      "loss: 1.385588  [  864/ 3200]\n",
      "loss: 1.387279  [  880/ 3200]\n",
      "loss: 1.385934  [  896/ 3200]\n",
      "loss: 1.388088  [  912/ 3200]\n",
      "loss: 1.386625  [  928/ 3200]\n",
      "loss: 1.383851  [  944/ 3200]\n",
      "loss: 1.383058  [  960/ 3200]\n",
      "loss: 1.386814  [  976/ 3200]\n",
      "loss: 1.389447  [  992/ 3200]\n",
      "loss: 1.379511  [ 1008/ 3200]\n",
      "loss: 1.386829  [ 1024/ 3200]\n",
      "loss: 1.386727  [ 1040/ 3200]\n",
      "loss: 1.387657  [ 1056/ 3200]\n",
      "loss: 1.386382  [ 1072/ 3200]\n",
      "loss: 1.385487  [ 1088/ 3200]\n",
      "loss: 1.388363  [ 1104/ 3200]\n",
      "loss: 1.388569  [ 1120/ 3200]\n",
      "loss: 1.388671  [ 1136/ 3200]\n",
      "loss: 1.384315  [ 1152/ 3200]\n",
      "loss: 1.387553  [ 1168/ 3200]\n",
      "loss: 1.382506  [ 1184/ 3200]\n",
      "loss: 1.384661  [ 1200/ 3200]\n",
      "loss: 1.384197  [ 1216/ 3200]\n",
      "loss: 1.383506  [ 1232/ 3200]\n",
      "loss: 1.385847  [ 1248/ 3200]\n",
      "loss: 1.384110  [ 1264/ 3200]\n",
      "loss: 1.388552  [ 1280/ 3200]\n",
      "loss: 1.390258  [ 1296/ 3200]\n",
      "loss: 1.385918  [ 1312/ 3200]\n",
      "loss: 1.385934  [ 1328/ 3200]\n",
      "loss: 1.385846  [ 1344/ 3200]\n",
      "loss: 1.387625  [ 1360/ 3200]\n",
      "loss: 1.385110  [ 1376/ 3200]\n",
      "loss: 1.386294  [ 1392/ 3200]\n",
      "loss: 1.382940  [ 1408/ 3200]\n",
      "loss: 1.394148  [ 1424/ 3200]\n",
      "loss: 1.389824  [ 1440/ 3200]\n",
      "loss: 1.392337  [ 1456/ 3200]\n",
      "loss: 1.389703  [ 1472/ 3200]\n",
      "loss: 1.390255  [ 1488/ 3200]\n",
      "loss: 1.386742  [ 1504/ 3200]\n",
      "loss: 1.388445  [ 1520/ 3200]\n",
      "loss: 1.391167  [ 1536/ 3200]\n",
      "loss: 1.383063  [ 1552/ 3200]\n",
      "loss: 1.382942  [ 1568/ 3200]\n",
      "loss: 1.387775  [ 1584/ 3200]\n",
      "loss: 1.386053  [ 1600/ 3200]\n",
      "loss: 1.382239  [ 1616/ 3200]\n",
      "loss: 1.376437  [ 1632/ 3200]\n",
      "loss: 1.386727  [ 1648/ 3200]\n",
      "loss: 1.387279  [ 1664/ 3200]\n",
      "loss: 1.381685  [ 1680/ 3200]\n",
      "loss: 1.389256  [ 1696/ 3200]\n",
      "loss: 1.386278  [ 1712/ 3200]\n",
      "loss: 1.389463  [ 1728/ 3200]\n",
      "loss: 1.390702  [ 1744/ 3200]\n",
      "loss: 1.388823  [ 1760/ 3200]\n",
      "loss: 1.383766  [ 1776/ 3200]\n",
      "loss: 1.385470  [ 1792/ 3200]\n",
      "loss: 1.383527  [ 1808/ 3200]\n",
      "loss: 1.383870  [ 1824/ 3200]\n",
      "loss: 1.383750  [ 1840/ 3200]\n",
      "loss: 1.390630  [ 1856/ 3200]\n",
      "loss: 1.387997  [ 1872/ 3200]\n",
      "loss: 1.387997  [ 1888/ 3200]\n",
      "loss: 1.390718  [ 1904/ 3200]\n",
      "loss: 1.388086  [ 1920/ 3200]\n",
      "loss: 1.386292  [ 1936/ 3200]\n",
      "loss: 1.388908  [ 1952/ 3200]\n",
      "loss: 1.388550  [ 1968/ 3200]\n",
      "loss: 1.390970  [ 1984/ 3200]\n",
      "loss: 1.386159  [ 2000/ 3200]\n",
      "loss: 1.385785  [ 2016/ 3200]\n",
      "loss: 1.383049  [ 2032/ 3200]\n",
      "loss: 1.381806  [ 2048/ 3200]\n",
      "loss: 1.382150  [ 2064/ 3200]\n",
      "loss: 1.384663  [ 2080/ 3200]\n",
      "loss: 1.383303  [ 2096/ 3200]\n",
      "loss: 1.380583  [ 2112/ 3200]\n",
      "loss: 1.382853  [ 2128/ 3200]\n",
      "loss: 1.386831  [ 2144/ 3200]\n",
      "loss: 1.384214  [ 2160/ 3200]\n",
      "loss: 1.386636  [ 2176/ 3200]\n",
      "loss: 1.386395  [ 2192/ 3200]\n",
      "loss: 1.386369  [ 2208/ 3200]\n",
      "loss: 1.387521  [ 2224/ 3200]\n",
      "loss: 1.384571  [ 2240/ 3200]\n",
      "loss: 1.386831  [ 2256/ 3200]\n",
      "loss: 1.391419  [ 2272/ 3200]\n",
      "loss: 1.388999  [ 2288/ 3200]\n",
      "loss: 1.380342  [ 2304/ 3200]\n",
      "loss: 1.383407  [ 2320/ 3200]\n",
      "loss: 1.389356  [ 2336/ 3200]\n",
      "loss: 1.388999  [ 2352/ 3200]\n",
      "loss: 1.383408  [ 2368/ 3200]\n",
      "loss: 1.388312  [ 2384/ 3200]\n",
      "loss: 1.389356  [ 2400/ 3200]\n",
      "loss: 1.386592  [ 2416/ 3200]\n",
      "loss: 1.384663  [ 2432/ 3200]\n",
      "loss: 1.391524  [ 2448/ 3200]\n",
      "loss: 1.392238  [ 2464/ 3200]\n",
      "loss: 1.386818  [ 2480/ 3200]\n",
      "loss: 1.382602  [ 2496/ 3200]\n",
      "loss: 1.389355  [ 2512/ 3200]\n",
      "loss: 1.391508  [ 2528/ 3200]\n",
      "loss: 1.382497  [ 2544/ 3200]\n",
      "loss: 1.385919  [ 2560/ 3200]\n",
      "loss: 1.382034  [ 2576/ 3200]\n",
      "loss: 1.387173  [ 2592/ 3200]\n",
      "loss: 1.387399  [ 2608/ 3200]\n",
      "loss: 1.389729  [ 2624/ 3200]\n",
      "loss: 1.388085  [ 2640/ 3200]\n",
      "loss: 1.392417  [ 2656/ 3200]\n",
      "loss: 1.385454  [ 2672/ 3200]\n",
      "loss: 1.385022  [ 2688/ 3200]\n",
      "loss: 1.386276  [ 2704/ 3200]\n",
      "loss: 1.389444  [ 2720/ 3200]\n",
      "loss: 1.381602  [ 2736/ 3200]\n",
      "loss: 1.389118  [ 2752/ 3200]\n",
      "loss: 1.382962  [ 2768/ 3200]\n",
      "loss: 1.383409  [ 2784/ 3200]\n",
      "loss: 1.385934  [ 2800/ 3200]\n",
      "loss: 1.380226  [ 2816/ 3200]\n",
      "loss: 1.390624  [ 2832/ 3200]\n",
      "loss: 1.392074  [ 2848/ 3200]\n",
      "loss: 1.382962  [ 2864/ 3200]\n",
      "loss: 1.386951  [ 2880/ 3200]\n",
      "loss: 1.376897  [ 2896/ 3200]\n",
      "loss: 1.385560  [ 2912/ 3200]\n",
      "loss: 1.379525  [ 2928/ 3200]\n",
      "loss: 1.386724  [ 2944/ 3200]\n",
      "loss: 1.380779  [ 2960/ 3200]\n",
      "loss: 1.383198  [ 2976/ 3200]\n",
      "loss: 1.388671  [ 2992/ 3200]\n",
      "loss: 1.391537  [ 3008/ 3200]\n",
      "loss: 1.386039  [ 3024/ 3200]\n",
      "loss: 1.388891  [ 3040/ 3200]\n",
      "loss: 1.381797  [ 3056/ 3200]\n",
      "loss: 1.386277  [ 3072/ 3200]\n",
      "loss: 1.391968  [ 3088/ 3200]\n",
      "loss: 1.386276  [ 3104/ 3200]\n",
      "loss: 1.381349  [ 3120/ 3200]\n",
      "loss: 1.389818  [ 3136/ 3200]\n",
      "loss: 1.390713  [ 3152/ 3200]\n",
      "loss: 1.385918  [ 3168/ 3200]\n",
      "loss: 1.389443  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.4620e-04.\n",
      "\n",
      "Epoch: 52\n",
      "-----------------------------\n",
      "loss: 1.387293  [    0/ 3200]\n",
      "loss: 1.385144  [   16/ 3200]\n",
      "loss: 1.386740  [   32/ 3200]\n",
      "loss: 1.390607  [   48/ 3200]\n",
      "loss: 1.382499  [   64/ 3200]\n",
      "loss: 1.388083  [   80/ 3200]\n",
      "loss: 1.380317  [   96/ 3200]\n",
      "loss: 1.386829  [  112/ 3200]\n",
      "loss: 1.384216  [  128/ 3200]\n",
      "loss: 1.384558  [  144/ 3200]\n",
      "loss: 1.379421  [  160/ 3200]\n",
      "loss: 1.386040  [  176/ 3200]\n",
      "loss: 1.388636  [  192/ 3200]\n",
      "loss: 1.379526  [  208/ 3200]\n",
      "loss: 1.387995  [  224/ 3200]\n",
      "loss: 1.387294  [  240/ 3200]\n",
      "loss: 1.387294  [  256/ 3200]\n",
      "loss: 1.389354  [  272/ 3200]\n",
      "loss: 1.388923  [  288/ 3200]\n",
      "loss: 1.384216  [  304/ 3200]\n",
      "loss: 1.388083  [  320/ 3200]\n",
      "loss: 1.387635  [  336/ 3200]\n",
      "loss: 1.387276  [  352/ 3200]\n",
      "loss: 1.383516  [  368/ 3200]\n",
      "loss: 1.387618  [  384/ 3200]\n",
      "loss: 1.389353  [  400/ 3200]\n",
      "loss: 1.385129  [  416/ 3200]\n",
      "loss: 1.388528  [  432/ 3200]\n",
      "loss: 1.386847  [  448/ 3200]\n",
      "loss: 1.385575  [  464/ 3200]\n",
      "loss: 1.391499  [  480/ 3200]\n",
      "loss: 1.376094  [  496/ 3200]\n",
      "loss: 1.379888  [  512/ 3200]\n",
      "loss: 1.384323  [  528/ 3200]\n",
      "loss: 1.388546  [  544/ 3200]\n",
      "loss: 1.385129  [  560/ 3200]\n",
      "loss: 1.387530  [  576/ 3200]\n",
      "loss: 1.389439  [  592/ 3200]\n",
      "loss: 1.394233  [  608/ 3200]\n",
      "loss: 1.386382  [  624/ 3200]\n",
      "loss: 1.386257  [  640/ 3200]\n",
      "loss: 1.389267  [  656/ 3200]\n",
      "loss: 1.389438  [  672/ 3200]\n",
      "loss: 1.384684  [  688/ 3200]\n",
      "loss: 1.389797  [  704/ 3200]\n",
      "loss: 1.386256  [  720/ 3200]\n",
      "loss: 1.384918  [  736/ 3200]\n",
      "loss: 1.385576  [  752/ 3200]\n",
      "loss: 1.386042  [  768/ 3200]\n",
      "loss: 1.387187  [  784/ 3200]\n",
      "loss: 1.385596  [  800/ 3200]\n",
      "loss: 1.391088  [  816/ 3200]\n",
      "loss: 1.387100  [  832/ 3200]\n",
      "loss: 1.390709  [  848/ 3200]\n",
      "loss: 1.378728  [  864/ 3200]\n",
      "loss: 1.387758  [  880/ 3200]\n",
      "loss: 1.385129  [  896/ 3200]\n",
      "loss: 1.385470  [  912/ 3200]\n",
      "loss: 1.386828  [  928/ 3200]\n",
      "loss: 1.379979  [  944/ 3200]\n",
      "loss: 1.386401  [  960/ 3200]\n",
      "loss: 1.389438  [  976/ 3200]\n",
      "loss: 1.386741  [  992/ 3200]\n",
      "loss: 1.393590  [ 1008/ 3200]\n",
      "loss: 1.391048  [ 1024/ 3200]\n",
      "loss: 1.390367  [ 1040/ 3200]\n",
      "loss: 1.386382  [ 1056/ 3200]\n",
      "loss: 1.387739  [ 1072/ 3200]\n",
      "loss: 1.386487  [ 1088/ 3200]\n",
      "loss: 1.384324  [ 1104/ 3200]\n",
      "loss: 1.385830  [ 1120/ 3200]\n",
      "loss: 1.384219  [ 1136/ 3200]\n",
      "loss: 1.385576  [ 1152/ 3200]\n",
      "loss: 1.385366  [ 1168/ 3200]\n",
      "loss: 1.386846  [ 1184/ 3200]\n",
      "loss: 1.378624  [ 1200/ 3200]\n",
      "loss: 1.386828  [ 1216/ 3200]\n",
      "loss: 1.391512  [ 1232/ 3200]\n",
      "loss: 1.385111  [ 1248/ 3200]\n",
      "loss: 1.386827  [ 1264/ 3200]\n",
      "loss: 1.385130  [ 1280/ 3200]\n",
      "loss: 1.388098  [ 1296/ 3200]\n",
      "loss: 1.386022  [ 1312/ 3200]\n",
      "loss: 1.384238  [ 1328/ 3200]\n",
      "loss: 1.388544  [ 1344/ 3200]\n",
      "loss: 1.386145  [ 1360/ 3200]\n",
      "loss: 1.381593  [ 1376/ 3200]\n",
      "loss: 1.382967  [ 1392/ 3200]\n",
      "loss: 1.388526  [ 1408/ 3200]\n",
      "loss: 1.391066  [ 1424/ 3200]\n",
      "loss: 1.380911  [ 1440/ 3200]\n",
      "loss: 1.381697  [ 1456/ 3200]\n",
      "loss: 1.386723  [ 1472/ 3200]\n",
      "loss: 1.391152  [ 1488/ 3200]\n",
      "loss: 1.383983  [ 1504/ 3200]\n",
      "loss: 1.388201  [ 1520/ 3200]\n",
      "loss: 1.385026  [ 1536/ 3200]\n",
      "loss: 1.387169  [ 1552/ 3200]\n",
      "loss: 1.386638  [ 1568/ 3200]\n",
      "loss: 1.384306  [ 1584/ 3200]\n",
      "loss: 1.385471  [ 1600/ 3200]\n",
      "loss: 1.394479  [ 1616/ 3200]\n",
      "loss: 1.384305  [ 1632/ 3200]\n",
      "loss: 1.388969  [ 1648/ 3200]\n",
      "loss: 1.388097  [ 1664/ 3200]\n",
      "loss: 1.391064  [ 1680/ 3200]\n",
      "loss: 1.386847  [ 1696/ 3200]\n",
      "loss: 1.385451  [ 1712/ 3200]\n",
      "loss: 1.387737  [ 1728/ 3200]\n",
      "loss: 1.386742  [ 1744/ 3200]\n",
      "loss: 1.387972  [ 1760/ 3200]\n",
      "loss: 1.386721  [ 1776/ 3200]\n",
      "loss: 1.397440  [ 1792/ 3200]\n",
      "loss: 1.390596  [ 1808/ 3200]\n",
      "loss: 1.387736  [ 1824/ 3200]\n",
      "loss: 1.385026  [ 1840/ 3200]\n",
      "loss: 1.382633  [ 1856/ 3200]\n",
      "loss: 1.385237  [ 1872/ 3200]\n",
      "loss: 1.389771  [ 1888/ 3200]\n",
      "loss: 1.382951  [ 1904/ 3200]\n",
      "loss: 1.389896  [ 1920/ 3200]\n",
      "loss: 1.381596  [ 1936/ 3200]\n",
      "loss: 1.387313  [ 1952/ 3200]\n",
      "loss: 1.389242  [ 1968/ 3200]\n",
      "loss: 1.387060  [ 1984/ 3200]\n",
      "loss: 1.386297  [ 2000/ 3200]\n",
      "loss: 1.390722  [ 2016/ 3200]\n",
      "loss: 1.388096  [ 2032/ 3200]\n",
      "loss: 1.389006  [ 2048/ 3200]\n",
      "loss: 1.386487  [ 2064/ 3200]\n",
      "loss: 1.386826  [ 2080/ 3200]\n",
      "loss: 1.381703  [ 2096/ 3200]\n",
      "loss: 1.388096  [ 2112/ 3200]\n",
      "loss: 1.387291  [ 2128/ 3200]\n",
      "loss: 1.386591  [ 2144/ 3200]\n",
      "loss: 1.383757  [ 2160/ 3200]\n",
      "loss: 1.384222  [ 2176/ 3200]\n",
      "loss: 1.383417  [ 2192/ 3200]\n",
      "loss: 1.383077  [ 2208/ 3200]\n",
      "loss: 1.389242  [ 2224/ 3200]\n",
      "loss: 1.388116  [ 2240/ 3200]\n",
      "loss: 1.385151  [ 2256/ 3200]\n",
      "loss: 1.389809  [ 2272/ 3200]\n",
      "loss: 1.383758  [ 2288/ 3200]\n",
      "loss: 1.386703  [ 2304/ 3200]\n",
      "loss: 1.383862  [ 2320/ 3200]\n",
      "loss: 1.386846  [ 2336/ 3200]\n",
      "loss: 1.386296  [ 2352/ 3200]\n",
      "loss: 1.382044  [ 2368/ 3200]\n",
      "loss: 1.386466  [ 2384/ 3200]\n",
      "loss: 1.387840  [ 2400/ 3200]\n",
      "loss: 1.388901  [ 2416/ 3200]\n",
      "loss: 1.381579  [ 2432/ 3200]\n",
      "loss: 1.383777  [ 2448/ 3200]\n",
      "loss: 1.381153  [ 2464/ 3200]\n",
      "loss: 1.384202  [ 2480/ 3200]\n",
      "loss: 1.384222  [ 2496/ 3200]\n",
      "loss: 1.391060  [ 2512/ 3200]\n",
      "loss: 1.387991  [ 2528/ 3200]\n",
      "loss: 1.385831  [ 2544/ 3200]\n",
      "loss: 1.388646  [ 2560/ 3200]\n",
      "loss: 1.384561  [ 2576/ 3200]\n",
      "loss: 1.385936  [ 2592/ 3200]\n",
      "loss: 1.385471  [ 2608/ 3200]\n",
      "loss: 1.381957  [ 2624/ 3200]\n",
      "loss: 1.385026  [ 2640/ 3200]\n",
      "loss: 1.389790  [ 2656/ 3200]\n",
      "loss: 1.385132  [ 2672/ 3200]\n",
      "loss: 1.391524  [ 2688/ 3200]\n",
      "loss: 1.385132  [ 2704/ 3200]\n",
      "loss: 1.387185  [ 2720/ 3200]\n",
      "loss: 1.386381  [ 2736/ 3200]\n",
      "loss: 1.384329  [ 2752/ 3200]\n",
      "loss: 1.388201  [ 2768/ 3200]\n",
      "loss: 1.386381  [ 2784/ 3200]\n",
      "loss: 1.384203  [ 2800/ 3200]\n",
      "loss: 1.384793  [ 2816/ 3200]\n",
      "loss: 1.383332  [ 2832/ 3200]\n",
      "loss: 1.384328  [ 2848/ 3200]\n",
      "loss: 1.387396  [ 2864/ 3200]\n",
      "loss: 1.383331  [ 2880/ 3200]\n",
      "loss: 1.387414  [ 2896/ 3200]\n",
      "loss: 1.386468  [ 2912/ 3200]\n",
      "loss: 1.382972  [ 2928/ 3200]\n",
      "loss: 1.386503  [ 2944/ 3200]\n",
      "loss: 1.387735  [ 2960/ 3200]\n",
      "loss: 1.383881  [ 2976/ 3200]\n",
      "loss: 1.390699  [ 2992/ 3200]\n",
      "loss: 1.385680  [ 3008/ 3200]\n",
      "loss: 1.391503  [ 3024/ 3200]\n",
      "loss: 1.387563  [ 3040/ 3200]\n",
      "loss: 1.384119  [ 3056/ 3200]\n",
      "loss: 1.380693  [ 3072/ 3200]\n",
      "loss: 1.383418  [ 3088/ 3200]\n",
      "loss: 1.386723  [ 3104/ 3200]\n",
      "loss: 1.385831  [ 3120/ 3200]\n",
      "loss: 1.389344  [ 3136/ 3200]\n",
      "loss: 1.383776  [ 3152/ 3200]\n",
      "loss: 1.391860  [ 3168/ 3200]\n",
      "loss: 1.383524  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3889e-04.\n",
      "\n",
      "Epoch: 53\n",
      "-----------------------------\n",
      "loss: 1.386040  [    0/ 3200]\n",
      "loss: 1.388198  [   16/ 3200]\n",
      "loss: 1.384789  [   32/ 3200]\n",
      "loss: 1.387170  [   48/ 3200]\n",
      "loss: 1.381706  [   64/ 3200]\n",
      "loss: 1.386739  [   80/ 3200]\n",
      "loss: 1.382526  [   96/ 3200]\n",
      "loss: 1.387185  [  112/ 3200]\n",
      "loss: 1.385682  [  128/ 3200]\n",
      "loss: 1.387289  [  144/ 3200]\n",
      "loss: 1.386931  [  160/ 3200]\n",
      "loss: 1.388109  [  176/ 3200]\n",
      "loss: 1.388554  [  192/ 3200]\n",
      "loss: 1.389000  [  208/ 3200]\n",
      "loss: 1.390684  [  224/ 3200]\n",
      "loss: 1.386499  [  240/ 3200]\n",
      "loss: 1.385102  [  256/ 3200]\n",
      "loss: 1.387990  [  272/ 3200]\n",
      "loss: 1.390594  [  288/ 3200]\n",
      "loss: 1.381590  [  304/ 3200]\n",
      "loss: 1.391501  [  320/ 3200]\n",
      "loss: 1.388642  [  336/ 3200]\n",
      "loss: 1.387974  [  352/ 3200]\n",
      "loss: 1.383762  [  368/ 3200]\n",
      "loss: 1.390696  [  384/ 3200]\n",
      "loss: 1.390233  [  400/ 3200]\n",
      "loss: 1.387751  [  416/ 3200]\n",
      "loss: 1.380801  [  432/ 3200]\n",
      "loss: 1.384686  [  448/ 3200]\n",
      "loss: 1.387734  [  464/ 3200]\n",
      "loss: 1.386484  [  480/ 3200]\n",
      "loss: 1.389771  [  496/ 3200]\n",
      "loss: 1.386278  [  512/ 3200]\n",
      "loss: 1.386484  [  528/ 3200]\n",
      "loss: 1.389787  [  544/ 3200]\n",
      "loss: 1.387374  [  560/ 3200]\n",
      "loss: 1.386381  [  576/ 3200]\n",
      "loss: 1.385389  [  592/ 3200]\n",
      "loss: 1.380357  [  608/ 3200]\n",
      "loss: 1.393192  [  624/ 3200]\n",
      "loss: 1.390334  [  640/ 3200]\n",
      "loss: 1.385114  [  656/ 3200]\n",
      "loss: 1.382069  [  672/ 3200]\n",
      "loss: 1.384225  [  688/ 3200]\n",
      "loss: 1.384790  [  704/ 3200]\n",
      "loss: 1.382771  [  720/ 3200]\n",
      "loss: 1.382976  [  736/ 3200]\n",
      "loss: 1.387545  [  752/ 3200]\n",
      "loss: 1.391156  [  768/ 3200]\n",
      "loss: 1.385577  [  784/ 3200]\n",
      "loss: 1.388999  [  800/ 3200]\n",
      "loss: 1.386381  [  816/ 3200]\n",
      "loss: 1.388537  [  832/ 3200]\n",
      "loss: 1.382172  [  848/ 3200]\n",
      "loss: 1.383318  [  864/ 3200]\n",
      "loss: 1.383626  [  880/ 3200]\n",
      "loss: 1.390591  [  896/ 3200]\n",
      "loss: 1.387185  [  912/ 3200]\n",
      "loss: 1.385936  [  928/ 3200]\n",
      "loss: 1.386826  [  944/ 3200]\n",
      "loss: 1.388177  [  960/ 3200]\n",
      "loss: 1.383678  [  976/ 3200]\n",
      "loss: 1.384567  [  992/ 3200]\n",
      "loss: 1.386278  [ 1008/ 3200]\n",
      "loss: 1.385577  [ 1024/ 3200]\n",
      "loss: 1.388896  [ 1040/ 3200]\n",
      "loss: 1.383404  [ 1056/ 3200]\n",
      "loss: 1.385253  [ 1072/ 3200]\n",
      "loss: 1.387733  [ 1088/ 3200]\n",
      "loss: 1.389802  [ 1104/ 3200]\n",
      "loss: 1.383985  [ 1120/ 3200]\n",
      "loss: 1.388450  [ 1136/ 3200]\n",
      "loss: 1.385920  [ 1152/ 3200]\n",
      "loss: 1.386397  [ 1168/ 3200]\n",
      "loss: 1.381250  [ 1184/ 3200]\n",
      "loss: 1.385936  [ 1200/ 3200]\n",
      "loss: 1.388966  [ 1216/ 3200]\n",
      "loss: 1.385920  [ 1232/ 3200]\n",
      "loss: 1.388536  [ 1248/ 3200]\n",
      "loss: 1.386945  [ 1264/ 3200]\n",
      "loss: 1.386364  [ 1280/ 3200]\n",
      "loss: 1.381164  [ 1296/ 3200]\n",
      "loss: 1.385577  [ 1312/ 3200]\n",
      "loss: 1.385458  [ 1328/ 3200]\n",
      "loss: 1.385594  [ 1344/ 3200]\n",
      "loss: 1.390674  [ 1360/ 3200]\n",
      "loss: 1.385835  [ 1376/ 3200]\n",
      "loss: 1.385559  [ 1392/ 3200]\n",
      "loss: 1.387732  [ 1408/ 3200]\n",
      "loss: 1.385937  [ 1424/ 3200]\n",
      "loss: 1.383337  [ 1440/ 3200]\n",
      "loss: 1.385115  [ 1456/ 3200]\n",
      "loss: 1.383319  [ 1472/ 3200]\n",
      "loss: 1.387629  [ 1488/ 3200]\n",
      "loss: 1.379658  [ 1504/ 3200]\n",
      "loss: 1.391034  [ 1520/ 3200]\n",
      "loss: 1.388638  [ 1536/ 3200]\n",
      "loss: 1.381165  [ 1552/ 3200]\n",
      "loss: 1.388176  [ 1568/ 3200]\n",
      "loss: 1.383763  [ 1584/ 3200]\n",
      "loss: 1.384670  [ 1600/ 3200]\n",
      "loss: 1.390266  [ 1616/ 3200]\n",
      "loss: 1.383866  [ 1632/ 3200]\n",
      "loss: 1.389442  [ 1648/ 3200]\n",
      "loss: 1.376495  [ 1664/ 3200]\n",
      "loss: 1.388296  [ 1680/ 3200]\n",
      "loss: 1.384226  [ 1696/ 3200]\n",
      "loss: 1.381146  [ 1712/ 3200]\n",
      "loss: 1.389341  [ 1728/ 3200]\n",
      "loss: 1.385937  [ 1744/ 3200]\n",
      "loss: 1.385493  [ 1760/ 3200]\n",
      "loss: 1.383782  [ 1776/ 3200]\n",
      "loss: 1.382172  [ 1792/ 3200]\n",
      "loss: 1.386861  [ 1808/ 3200]\n",
      "loss: 1.387731  [ 1824/ 3200]\n",
      "loss: 1.382071  [ 1840/ 3200]\n",
      "loss: 1.387973  [ 1856/ 3200]\n",
      "loss: 1.390691  [ 1872/ 3200]\n",
      "loss: 1.390246  [ 1888/ 3200]\n",
      "loss: 1.388434  [ 1904/ 3200]\n",
      "loss: 1.383320  [ 1920/ 3200]\n",
      "loss: 1.390708  [ 1936/ 3200]\n",
      "loss: 1.384929  [ 1952/ 3200]\n",
      "loss: 1.385030  [ 1968/ 3200]\n",
      "loss: 1.385474  [ 1984/ 3200]\n",
      "loss: 1.384141  [ 2000/ 3200]\n",
      "loss: 1.379215  [ 2016/ 3200]\n",
      "loss: 1.384670  [ 2032/ 3200]\n",
      "loss: 1.383781  [ 2048/ 3200]\n",
      "loss: 1.388895  [ 2064/ 3200]\n",
      "loss: 1.387646  [ 2080/ 3200]\n",
      "loss: 1.388895  [ 2096/ 3200]\n",
      "loss: 1.387733  [ 2112/ 3200]\n",
      "loss: 1.384568  [ 2128/ 3200]\n",
      "loss: 1.385235  [ 2144/ 3200]\n",
      "loss: 1.384089  [ 2160/ 3200]\n",
      "loss: 1.385045  [ 2176/ 3200]\n",
      "loss: 1.389800  [ 2192/ 3200]\n",
      "loss: 1.388981  [ 2208/ 3200]\n",
      "loss: 1.389150  [ 2224/ 3200]\n",
      "loss: 1.389339  [ 2240/ 3200]\n",
      "loss: 1.392297  [ 2256/ 3200]\n",
      "loss: 1.389784  [ 2272/ 3200]\n",
      "loss: 1.389353  [ 2288/ 3200]\n",
      "loss: 1.389783  [ 2304/ 3200]\n",
      "loss: 1.389337  [ 2320/ 3200]\n",
      "loss: 1.386738  [ 2336/ 3200]\n",
      "loss: 1.380721  [ 2352/ 3200]\n",
      "loss: 1.387272  [ 2368/ 3200]\n",
      "loss: 1.383424  [ 2384/ 3200]\n",
      "loss: 1.387183  [ 2400/ 3200]\n",
      "loss: 1.383410  [ 2416/ 3200]\n",
      "loss: 1.382979  [ 2432/ 3200]\n",
      "loss: 1.382636  [ 2448/ 3200]\n",
      "loss: 1.387184  [ 2464/ 3200]\n",
      "loss: 1.384672  [ 2480/ 3200]\n",
      "loss: 1.391478  [ 2496/ 3200]\n",
      "loss: 1.387493  [ 2512/ 3200]\n",
      "loss: 1.385935  [ 2528/ 3200]\n",
      "loss: 1.387526  [ 2544/ 3200]\n",
      "loss: 1.387629  [ 2560/ 3200]\n",
      "loss: 1.390703  [ 2576/ 3200]\n",
      "loss: 1.378657  [ 2592/ 3200]\n",
      "loss: 1.390332  [ 2608/ 3200]\n",
      "loss: 1.390688  [ 2624/ 3200]\n",
      "loss: 1.386929  [ 2640/ 3200]\n",
      "loss: 1.387286  [ 2656/ 3200]\n",
      "loss: 1.383870  [ 2672/ 3200]\n",
      "loss: 1.382519  [ 2688/ 3200]\n",
      "loss: 1.385834  [ 2704/ 3200]\n",
      "loss: 1.388877  [ 2720/ 3200]\n",
      "loss: 1.379818  [ 2736/ 3200]\n",
      "loss: 1.379801  [ 2752/ 3200]\n",
      "loss: 1.384243  [ 2768/ 3200]\n",
      "loss: 1.388980  [ 2784/ 3200]\n",
      "loss: 1.387183  [ 2800/ 3200]\n",
      "loss: 1.389441  [ 2816/ 3200]\n",
      "loss: 1.389336  [ 2832/ 3200]\n",
      "loss: 1.388907  [ 2848/ 3200]\n",
      "loss: 1.385386  [ 2864/ 3200]\n",
      "loss: 1.380024  [ 2880/ 3200]\n",
      "loss: 1.380811  [ 2896/ 3200]\n",
      "loss: 1.384138  [ 2912/ 3200]\n",
      "loss: 1.391477  [ 2928/ 3200]\n",
      "loss: 1.387984  [ 2944/ 3200]\n",
      "loss: 1.392367  [ 2960/ 3200]\n",
      "loss: 1.384673  [ 2976/ 3200]\n",
      "loss: 1.384777  [ 2992/ 3200]\n",
      "loss: 1.386722  [ 3008/ 3200]\n",
      "loss: 1.386721  [ 3024/ 3200]\n",
      "loss: 1.383677  [ 3040/ 3200]\n",
      "loss: 1.388550  [ 3056/ 3200]\n",
      "loss: 1.388193  [ 3072/ 3200]\n",
      "loss: 1.384227  [ 3088/ 3200]\n",
      "loss: 1.391043  [ 3104/ 3200]\n",
      "loss: 1.395779  [ 3120/ 3200]\n",
      "loss: 1.384438  [ 3136/ 3200]\n",
      "loss: 1.388518  [ 3152/ 3200]\n",
      "loss: 1.383783  [ 3168/ 3200]\n",
      "loss: 1.387732  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3194e-04.\n",
      "\n",
      "Epoch: 54\n",
      "-----------------------------\n",
      "loss: 1.389439  [    0/ 3200]\n",
      "loss: 1.392304  [   16/ 3200]\n",
      "loss: 1.386024  [   32/ 3200]\n",
      "loss: 1.382092  [   48/ 3200]\n",
      "loss: 1.383768  [   64/ 3200]\n",
      "loss: 1.386381  [   80/ 3200]\n",
      "loss: 1.382521  [   96/ 3200]\n",
      "loss: 1.385935  [  112/ 3200]\n",
      "loss: 1.384228  [  128/ 3200]\n",
      "loss: 1.388533  [  144/ 3200]\n",
      "loss: 1.383322  [  160/ 3200]\n",
      "loss: 1.387627  [  176/ 3200]\n",
      "loss: 1.385343  [  192/ 3200]\n",
      "loss: 1.387286  [  208/ 3200]\n",
      "loss: 1.380933  [  224/ 3200]\n",
      "loss: 1.393625  [  240/ 3200]\n",
      "loss: 1.385816  [  256/ 3200]\n",
      "loss: 1.387612  [  272/ 3200]\n",
      "loss: 1.388653  [  288/ 3200]\n",
      "loss: 1.388620  [  304/ 3200]\n",
      "loss: 1.382061  [  320/ 3200]\n",
      "loss: 1.385030  [  336/ 3200]\n",
      "loss: 1.384316  [  352/ 3200]\n",
      "loss: 1.387643  [  368/ 3200]\n",
      "loss: 1.383323  [  384/ 3200]\n",
      "loss: 1.388532  [  400/ 3200]\n",
      "loss: 1.390700  [  416/ 3200]\n",
      "loss: 1.380491  [  432/ 3200]\n",
      "loss: 1.383888  [  448/ 3200]\n",
      "loss: 1.384229  [  464/ 3200]\n",
      "loss: 1.389422  [  480/ 3200]\n",
      "loss: 1.394796  [  496/ 3200]\n",
      "loss: 1.380832  [  512/ 3200]\n",
      "loss: 1.388874  [  528/ 3200]\n",
      "loss: 1.384927  [  544/ 3200]\n",
      "loss: 1.385119  [  560/ 3200]\n",
      "loss: 1.386825  [  576/ 3200]\n",
      "loss: 1.382522  [  592/ 3200]\n",
      "loss: 1.381721  [  608/ 3200]\n",
      "loss: 1.390580  [  624/ 3200]\n",
      "loss: 1.386858  [  640/ 3200]\n",
      "loss: 1.383324  [  656/ 3200]\n",
      "loss: 1.389213  [  672/ 3200]\n",
      "loss: 1.387182  [  688/ 3200]\n",
      "loss: 1.386930  [  704/ 3200]\n",
      "loss: 1.387198  [  720/ 3200]\n",
      "loss: 1.382863  [  736/ 3200]\n",
      "loss: 1.381172  [  752/ 3200]\n",
      "loss: 1.389689  [  768/ 3200]\n",
      "loss: 1.386486  [  784/ 3200]\n",
      "loss: 1.386825  [  800/ 3200]\n",
      "loss: 1.384480  [  816/ 3200]\n",
      "loss: 1.392730  [  832/ 3200]\n",
      "loss: 1.380832  [  848/ 3200]\n",
      "loss: 1.383089  [  864/ 3200]\n",
      "loss: 1.389333  [  880/ 3200]\n",
      "loss: 1.385135  [  896/ 3200]\n",
      "loss: 1.386041  [  912/ 3200]\n",
      "loss: 1.382523  [  928/ 3200]\n",
      "loss: 1.394987  [  944/ 3200]\n",
      "loss: 1.388977  [  960/ 3200]\n",
      "loss: 1.383890  [  976/ 3200]\n",
      "loss: 1.386736  [  992/ 3200]\n",
      "loss: 1.381738  [ 1008/ 3200]\n",
      "loss: 1.388887  [ 1024/ 3200]\n",
      "loss: 1.381173  [ 1040/ 3200]\n",
      "loss: 1.381382  [ 1056/ 3200]\n",
      "loss: 1.383874  [ 1072/ 3200]\n",
      "loss: 1.385594  [ 1088/ 3200]\n",
      "loss: 1.383992  [ 1104/ 3200]\n",
      "loss: 1.389451  [ 1120/ 3200]\n",
      "loss: 1.386367  [ 1136/ 3200]\n",
      "loss: 1.390120  [ 1152/ 3200]\n",
      "loss: 1.385683  [ 1168/ 3200]\n",
      "loss: 1.389689  [ 1184/ 3200]\n",
      "loss: 1.384793  [ 1200/ 3200]\n",
      "loss: 1.388546  [ 1216/ 3200]\n",
      "loss: 1.391245  [ 1232/ 3200]\n",
      "loss: 1.391127  [ 1248/ 3200]\n",
      "loss: 1.388531  [ 1264/ 3200]\n",
      "loss: 1.385490  [ 1280/ 3200]\n",
      "loss: 1.382970  [ 1296/ 3200]\n",
      "loss: 1.386812  [ 1312/ 3200]\n",
      "loss: 1.389318  [ 1328/ 3200]\n",
      "loss: 1.385491  [ 1344/ 3200]\n",
      "loss: 1.386722  [ 1360/ 3200]\n",
      "loss: 1.385832  [ 1376/ 3200]\n",
      "loss: 1.389687  [ 1392/ 3200]\n",
      "loss: 1.388976  [ 1408/ 3200]\n",
      "loss: 1.386380  [ 1424/ 3200]\n",
      "loss: 1.379116  [ 1440/ 3200]\n",
      "loss: 1.385372  [ 1456/ 3200]\n",
      "loss: 1.383089  [ 1472/ 3200]\n",
      "loss: 1.386381  [ 1488/ 3200]\n",
      "loss: 1.383785  [ 1504/ 3200]\n",
      "loss: 1.387744  [ 1520/ 3200]\n",
      "loss: 1.382184  [ 1536/ 3200]\n",
      "loss: 1.388990  [ 1552/ 3200]\n",
      "loss: 1.388189  [ 1568/ 3200]\n",
      "loss: 1.380821  [ 1584/ 3200]\n",
      "loss: 1.382080  [ 1600/ 3200]\n",
      "loss: 1.387969  [ 1616/ 3200]\n",
      "loss: 1.383533  [ 1632/ 3200]\n",
      "loss: 1.394078  [ 1648/ 3200]\n",
      "loss: 1.390933  [ 1664/ 3200]\n",
      "loss: 1.384572  [ 1680/ 3200]\n",
      "loss: 1.387270  [ 1696/ 3200]\n",
      "loss: 1.388530  [ 1712/ 3200]\n",
      "loss: 1.389227  [ 1728/ 3200]\n",
      "loss: 1.388975  [ 1744/ 3200]\n",
      "loss: 1.386055  [ 1760/ 3200]\n",
      "loss: 1.386143  [ 1776/ 3200]\n",
      "loss: 1.387167  [ 1792/ 3200]\n",
      "loss: 1.395319  [ 1808/ 3200]\n",
      "loss: 1.386825  [ 1824/ 3200]\n",
      "loss: 1.387996  [ 1840/ 3200]\n",
      "loss: 1.393272  [ 1856/ 3200]\n",
      "loss: 1.388084  [ 1872/ 3200]\n",
      "loss: 1.380468  [ 1888/ 3200]\n",
      "loss: 1.388085  [ 1904/ 3200]\n",
      "loss: 1.382083  [ 1920/ 3200]\n",
      "loss: 1.388988  [ 1936/ 3200]\n",
      "loss: 1.378216  [ 1952/ 3200]\n",
      "loss: 1.383787  [ 1968/ 3200]\n",
      "loss: 1.392353  [ 1984/ 3200]\n",
      "loss: 1.388188  [ 2000/ 3200]\n",
      "loss: 1.387284  [ 2016/ 3200]\n",
      "loss: 1.388958  [ 2032/ 3200]\n",
      "loss: 1.382544  [ 2048/ 3200]\n",
      "loss: 1.386809  [ 2064/ 3200]\n",
      "loss: 1.390217  [ 2080/ 3200]\n",
      "loss: 1.391937  [ 2096/ 3200]\n",
      "loss: 1.389686  [ 2112/ 3200]\n",
      "loss: 1.391476  [ 2128/ 3200]\n",
      "loss: 1.387268  [ 2144/ 3200]\n",
      "loss: 1.382442  [ 2160/ 3200]\n",
      "loss: 1.383329  [ 2176/ 3200]\n",
      "loss: 1.391032  [ 2192/ 3200]\n",
      "loss: 1.382617  [ 2208/ 3200]\n",
      "loss: 1.384589  [ 2224/ 3200]\n",
      "loss: 1.383329  [ 2240/ 3200]\n",
      "loss: 1.384232  [ 2256/ 3200]\n",
      "loss: 1.385136  [ 2272/ 3200]\n",
      "loss: 1.380841  [ 2288/ 3200]\n",
      "loss: 1.387520  [ 2304/ 3200]\n",
      "loss: 1.388100  [ 2320/ 3200]\n",
      "loss: 1.386040  [ 2336/ 3200]\n",
      "loss: 1.386380  [ 2352/ 3200]\n",
      "loss: 1.385685  [ 2368/ 3200]\n",
      "loss: 1.383876  [ 2384/ 3200]\n",
      "loss: 1.383773  [ 2400/ 3200]\n",
      "loss: 1.385389  [ 2416/ 3200]\n",
      "loss: 1.385492  [ 2432/ 3200]\n",
      "loss: 1.384128  [ 2448/ 3200]\n",
      "loss: 1.389343  [ 2464/ 3200]\n",
      "loss: 1.382188  [ 2480/ 3200]\n",
      "loss: 1.385595  [ 2496/ 3200]\n",
      "loss: 1.387639  [ 2512/ 3200]\n",
      "loss: 1.389787  [ 2528/ 3200]\n",
      "loss: 1.386380  [ 2544/ 3200]\n",
      "loss: 1.384232  [ 2560/ 3200]\n",
      "loss: 1.383328  [ 2576/ 3200]\n",
      "loss: 1.390127  [ 2592/ 3200]\n",
      "loss: 1.385031  [ 2608/ 3200]\n",
      "loss: 1.388620  [ 2624/ 3200]\n",
      "loss: 1.387729  [ 2640/ 3200]\n",
      "loss: 1.382620  [ 2656/ 3200]\n",
      "loss: 1.388438  [ 2672/ 3200]\n",
      "loss: 1.391920  [ 2688/ 3200]\n",
      "loss: 1.391933  [ 2704/ 3200]\n",
      "loss: 1.387374  [ 2720/ 3200]\n",
      "loss: 1.388527  [ 2736/ 3200]\n",
      "loss: 1.384129  [ 2752/ 3200]\n",
      "loss: 1.385566  [ 2768/ 3200]\n",
      "loss: 1.386735  [ 2784/ 3200]\n",
      "loss: 1.386276  [ 2800/ 3200]\n",
      "loss: 1.381731  [ 2816/ 3200]\n",
      "loss: 1.386631  [ 2832/ 3200]\n",
      "loss: 1.385477  [ 2848/ 3200]\n",
      "loss: 1.386500  [ 2864/ 3200]\n",
      "loss: 1.387728  [ 2880/ 3200]\n",
      "loss: 1.382989  [ 2896/ 3200]\n",
      "loss: 1.385567  [ 2912/ 3200]\n",
      "loss: 1.389222  [ 2928/ 3200]\n",
      "loss: 1.389181  [ 2944/ 3200]\n",
      "loss: 1.385137  [ 2960/ 3200]\n",
      "loss: 1.381287  [ 2976/ 3200]\n",
      "loss: 1.381731  [ 2992/ 3200]\n",
      "loss: 1.386721  [ 3008/ 3200]\n",
      "loss: 1.385581  [ 3024/ 3200]\n",
      "loss: 1.386720  [ 3040/ 3200]\n",
      "loss: 1.385921  [ 3056/ 3200]\n",
      "loss: 1.386380  [ 3072/ 3200]\n",
      "loss: 1.383434  [ 3088/ 3200]\n",
      "loss: 1.386809  [ 3104/ 3200]\n",
      "loss: 1.392733  [ 3120/ 3200]\n",
      "loss: 1.392732  [ 3136/ 3200]\n",
      "loss: 1.382651  [ 3152/ 3200]\n",
      "loss: 1.383315  [ 3168/ 3200]\n",
      "loss: 1.388187  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2534e-04.\n",
      "\n",
      "Epoch: 55\n",
      "-----------------------------\n",
      "loss: 1.388187  [    0/ 3200]\n",
      "loss: 1.388527  [   16/ 3200]\n",
      "loss: 1.388542  [   32/ 3200]\n",
      "loss: 1.387639  [   48/ 3200]\n",
      "loss: 1.385477  [   64/ 3200]\n",
      "loss: 1.394506  [   80/ 3200]\n",
      "loss: 1.386943  [   96/ 3200]\n",
      "loss: 1.386839  [  112/ 3200]\n",
      "loss: 1.387474  [  128/ 3200]\n",
      "loss: 1.383332  [  144/ 3200]\n",
      "loss: 1.390213  [  160/ 3200]\n",
      "loss: 1.387623  [  176/ 3200]\n",
      "loss: 1.386380  [  192/ 3200]\n",
      "loss: 1.387535  [  208/ 3200]\n",
      "loss: 1.390124  [  224/ 3200]\n",
      "loss: 1.385137  [  240/ 3200]\n",
      "loss: 1.389324  [  256/ 3200]\n",
      "loss: 1.390937  [  272/ 3200]\n",
      "loss: 1.390123  [  288/ 3200]\n",
      "loss: 1.385137  [  304/ 3200]\n",
      "loss: 1.388614  [  320/ 3200]\n",
      "loss: 1.390655  [  336/ 3200]\n",
      "loss: 1.385596  [  352/ 3200]\n",
      "loss: 1.383096  [  368/ 3200]\n",
      "loss: 1.389338  [  384/ 3200]\n",
      "loss: 1.391010  [  400/ 3200]\n",
      "loss: 1.390580  [  416/ 3200]\n",
      "loss: 1.386483  [  432/ 3200]\n",
      "loss: 1.389767  [  448/ 3200]\n",
      "loss: 1.385922  [  464/ 3200]\n",
      "loss: 1.388524  [  480/ 3200]\n",
      "loss: 1.388538  [  496/ 3200]\n",
      "loss: 1.380051  [  512/ 3200]\n",
      "loss: 1.387090  [  528/ 3200]\n",
      "loss: 1.387519  [  544/ 3200]\n",
      "loss: 1.381737  [  560/ 3200]\n",
      "loss: 1.385832  [  576/ 3200]\n",
      "loss: 1.389662  [  592/ 3200]\n",
      "loss: 1.383688  [  608/ 3200]\n",
      "loss: 1.384117  [  624/ 3200]\n",
      "loss: 1.381738  [  640/ 3200]\n",
      "loss: 1.381752  [  656/ 3200]\n",
      "loss: 1.388434  [  672/ 3200]\n",
      "loss: 1.385491  [  688/ 3200]\n",
      "loss: 1.387531  [  704/ 3200]\n",
      "loss: 1.388524  [  720/ 3200]\n",
      "loss: 1.383438  [  736/ 3200]\n",
      "loss: 1.385243  [  752/ 3200]\n",
      "loss: 1.386380  [  768/ 3200]\n",
      "loss: 1.382889  [  784/ 3200]\n",
      "loss: 1.388420  [  800/ 3200]\n",
      "loss: 1.394840  [  816/ 3200]\n",
      "loss: 1.393152  [  832/ 3200]\n",
      "loss: 1.392353  [  848/ 3200]\n",
      "loss: 1.379047  [  864/ 3200]\n",
      "loss: 1.386627  [  880/ 3200]\n",
      "loss: 1.384680  [  896/ 3200]\n",
      "loss: 1.384342  [  912/ 3200]\n",
      "loss: 1.387529  [  928/ 3200]\n",
      "loss: 1.383101  [  944/ 3200]\n",
      "loss: 1.387741  [  960/ 3200]\n",
      "loss: 1.384328  [  976/ 3200]\n",
      "loss: 1.387740  [  992/ 3200]\n",
      "loss: 1.380956  [ 1008/ 3200]\n",
      "loss: 1.385687  [ 1024/ 3200]\n",
      "loss: 1.382550  [ 1040/ 3200]\n",
      "loss: 1.387190  [ 1056/ 3200]\n",
      "loss: 1.388171  [ 1072/ 3200]\n",
      "loss: 1.387282  [ 1088/ 3200]\n",
      "loss: 1.389060  [ 1104/ 3200]\n",
      "loss: 1.377909  [ 1120/ 3200]\n",
      "loss: 1.389766  [ 1136/ 3200]\n",
      "loss: 1.383438  [ 1152/ 3200]\n",
      "loss: 1.386824  [ 1168/ 3200]\n",
      "loss: 1.388037  [ 1184/ 3200]\n",
      "loss: 1.384694  [ 1200/ 3200]\n",
      "loss: 1.385479  [ 1216/ 3200]\n",
      "loss: 1.384591  [ 1232/ 3200]\n",
      "loss: 1.388878  [ 1248/ 3200]\n",
      "loss: 1.382093  [ 1264/ 3200]\n",
      "loss: 1.390667  [ 1280/ 3200]\n",
      "loss: 1.389425  [ 1296/ 3200]\n",
      "loss: 1.387293  [ 1312/ 3200]\n",
      "loss: 1.391477  [ 1328/ 3200]\n",
      "loss: 1.385833  [ 1344/ 3200]\n",
      "loss: 1.384133  [ 1360/ 3200]\n",
      "loss: 1.388067  [ 1376/ 3200]\n",
      "loss: 1.387622  [ 1392/ 3200]\n",
      "loss: 1.382434  [ 1408/ 3200]\n",
      "loss: 1.381856  [ 1424/ 3200]\n",
      "loss: 1.384900  [ 1440/ 3200]\n",
      "loss: 1.384943  [ 1456/ 3200]\n",
      "loss: 1.388887  [ 1472/ 3200]\n",
      "loss: 1.380395  [ 1488/ 3200]\n",
      "loss: 1.381740  [ 1504/ 3200]\n",
      "loss: 1.383325  [ 1520/ 3200]\n",
      "loss: 1.386380  [ 1536/ 3200]\n",
      "loss: 1.388513  [ 1552/ 3200]\n",
      "loss: 1.390234  [ 1568/ 3200]\n",
      "loss: 1.384236  [ 1584/ 3200]\n",
      "loss: 1.389217  [ 1600/ 3200]\n",
      "loss: 1.382994  [ 1616/ 3200]\n",
      "loss: 1.389870  [ 1632/ 3200]\n",
      "loss: 1.384236  [ 1648/ 3200]\n",
      "loss: 1.388068  [ 1664/ 3200]\n",
      "loss: 1.388078  [ 1680/ 3200]\n",
      "loss: 1.382879  [ 1696/ 3200]\n",
      "loss: 1.387166  [ 1712/ 3200]\n",
      "loss: 1.382186  [ 1728/ 3200]\n",
      "loss: 1.386276  [ 1744/ 3200]\n",
      "loss: 1.383780  [ 1760/ 3200]\n",
      "loss: 1.386484  [ 1776/ 3200]\n",
      "loss: 1.389765  [ 1792/ 3200]\n",
      "loss: 1.381284  [ 1808/ 3200]\n",
      "loss: 1.388980  [ 1824/ 3200]\n",
      "loss: 1.383988  [ 1840/ 3200]\n",
      "loss: 1.383335  [ 1856/ 3200]\n",
      "loss: 1.385491  [ 1872/ 3200]\n",
      "loss: 1.382982  [ 1888/ 3200]\n",
      "loss: 1.388067  [ 1904/ 3200]\n",
      "loss: 1.386824  [ 1920/ 3200]\n",
      "loss: 1.392012  [ 1936/ 3200]\n",
      "loss: 1.384444  [ 1952/ 3200]\n",
      "loss: 1.390472  [ 1968/ 3200]\n",
      "loss: 1.378697  [ 1984/ 3200]\n",
      "loss: 1.380735  [ 2000/ 3200]\n",
      "loss: 1.390666  [ 2016/ 3200]\n",
      "loss: 1.383099  [ 2032/ 3200]\n",
      "loss: 1.385686  [ 2048/ 3200]\n",
      "loss: 1.396548  [ 2064/ 3200]\n",
      "loss: 1.390208  [ 2080/ 3200]\n",
      "loss: 1.390665  [ 2096/ 3200]\n",
      "loss: 1.388522  [ 2112/ 3200]\n",
      "loss: 1.381194  [ 2128/ 3200]\n",
      "loss: 1.387635  [ 2144/ 3200]\n",
      "loss: 1.392807  [ 2160/ 3200]\n",
      "loss: 1.383870  [ 2176/ 3200]\n",
      "loss: 1.388522  [ 2192/ 3200]\n",
      "loss: 1.386276  [ 2208/ 3200]\n",
      "loss: 1.383780  [ 2224/ 3200]\n",
      "loss: 1.388965  [ 2240/ 3200]\n",
      "loss: 1.385479  [ 2256/ 3200]\n",
      "loss: 1.385139  [ 2272/ 3200]\n",
      "loss: 1.385597  [ 2288/ 3200]\n",
      "loss: 1.390574  [ 2304/ 3200]\n",
      "loss: 1.385036  [ 2320/ 3200]\n",
      "loss: 1.383441  [ 2336/ 3200]\n",
      "loss: 1.388875  [ 2352/ 3200]\n",
      "loss: 1.379497  [ 2368/ 3200]\n",
      "loss: 1.381181  [ 2384/ 3200]\n",
      "loss: 1.385582  [ 2400/ 3200]\n",
      "loss: 1.383426  [ 2416/ 3200]\n",
      "loss: 1.392259  [ 2432/ 3200]\n",
      "loss: 1.383794  [ 2448/ 3200]\n",
      "loss: 1.384003  [ 2464/ 3200]\n",
      "loss: 1.380840  [ 2480/ 3200]\n",
      "loss: 1.384341  [ 2496/ 3200]\n",
      "loss: 1.386720  [ 2512/ 3200]\n",
      "loss: 1.386483  [ 2528/ 3200]\n",
      "loss: 1.383543  [ 2544/ 3200]\n",
      "loss: 1.379702  [ 2560/ 3200]\n",
      "loss: 1.386039  [ 2576/ 3200]\n",
      "loss: 1.388727  [ 2592/ 3200]\n",
      "loss: 1.386721  [ 2608/ 3200]\n",
      "loss: 1.378357  [ 2624/ 3200]\n",
      "loss: 1.385391  [ 2640/ 3200]\n",
      "loss: 1.390221  [ 2656/ 3200]\n",
      "loss: 1.391906  [ 2672/ 3200]\n",
      "loss: 1.385138  [ 2688/ 3200]\n",
      "loss: 1.390220  [ 2704/ 3200]\n",
      "loss: 1.389408  [ 2720/ 3200]\n",
      "loss: 1.388521  [ 2736/ 3200]\n",
      "loss: 1.383781  [ 2752/ 3200]\n",
      "loss: 1.389675  [ 2768/ 3200]\n",
      "loss: 1.390677  [ 2784/ 3200]\n",
      "loss: 1.387607  [ 2800/ 3200]\n",
      "loss: 1.382185  [ 2816/ 3200]\n",
      "loss: 1.393499  [ 2832/ 3200]\n",
      "loss: 1.387177  [ 2848/ 3200]\n",
      "loss: 1.386734  [ 2864/ 3200]\n",
      "loss: 1.385922  [ 2880/ 3200]\n",
      "loss: 1.391917  [ 2896/ 3200]\n",
      "loss: 1.385036  [ 2912/ 3200]\n",
      "loss: 1.384121  [ 2928/ 3200]\n",
      "loss: 1.385582  [ 2944/ 3200]\n",
      "loss: 1.379942  [ 2960/ 3200]\n",
      "loss: 1.391902  [ 2976/ 3200]\n",
      "loss: 1.385140  [ 2992/ 3200]\n",
      "loss: 1.388078  [ 3008/ 3200]\n",
      "loss: 1.378156  [ 3024/ 3200]\n",
      "loss: 1.391355  [ 3040/ 3200]\n",
      "loss: 1.388859  [ 3056/ 3200]\n",
      "loss: 1.385583  [ 3072/ 3200]\n",
      "loss: 1.387281  [ 3088/ 3200]\n",
      "loss: 1.381197  [ 3104/ 3200]\n",
      "loss: 1.382999  [ 3120/ 3200]\n",
      "loss: 1.387739  [ 3136/ 3200]\n",
      "loss: 1.385140  [ 3152/ 3200]\n",
      "loss: 1.382187  [ 3168/ 3200]\n",
      "loss: 1.385582  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1908e-04.\n",
      "\n",
      "Epoch: 56\n",
      "-----------------------------\n",
      "loss: 1.387739  [    0/ 3200]\n",
      "loss: 1.388166  [   16/ 3200]\n",
      "loss: 1.385597  [   32/ 3200]\n",
      "loss: 1.386734  [   48/ 3200]\n",
      "loss: 1.386734  [   64/ 3200]\n",
      "loss: 1.386823  [   80/ 3200]\n",
      "loss: 1.387398  [   96/ 3200]\n",
      "loss: 1.385494  [  112/ 3200]\n",
      "loss: 1.385820  [  128/ 3200]\n",
      "loss: 1.389775  [  144/ 3200]\n",
      "loss: 1.388063  [  160/ 3200]\n",
      "loss: 1.388520  [  176/ 3200]\n",
      "loss: 1.385139  [  192/ 3200]\n",
      "loss: 1.383782  [  208/ 3200]\n",
      "loss: 1.380755  [  224/ 3200]\n",
      "loss: 1.383899  [  240/ 3200]\n",
      "loss: 1.385819  [  256/ 3200]\n",
      "loss: 1.383456  [  272/ 3200]\n",
      "loss: 1.382201  [  288/ 3200]\n",
      "loss: 1.389525  [  304/ 3200]\n",
      "loss: 1.387723  [  320/ 3200]\n",
      "loss: 1.391472  [  336/ 3200]\n",
      "loss: 1.383326  [  352/ 3200]\n",
      "loss: 1.387634  [  368/ 3200]\n",
      "loss: 1.382985  [  384/ 3200]\n",
      "loss: 1.380298  [  400/ 3200]\n",
      "loss: 1.384682  [  416/ 3200]\n",
      "loss: 1.389304  [  432/ 3200]\n",
      "loss: 1.382645  [  448/ 3200]\n",
      "loss: 1.387723  [  464/ 3200]\n",
      "loss: 1.386837  [  480/ 3200]\n",
      "loss: 1.386262  [  496/ 3200]\n",
      "loss: 1.389317  [  512/ 3200]\n",
      "loss: 1.382999  [  528/ 3200]\n",
      "loss: 1.383782  [  544/ 3200]\n",
      "loss: 1.388506  [  560/ 3200]\n",
      "loss: 1.386837  [  576/ 3200]\n",
      "loss: 1.388417  [  592/ 3200]\n",
      "loss: 1.383561  [  608/ 3200]\n",
      "loss: 1.385125  [  624/ 3200]\n",
      "loss: 1.383693  [  640/ 3200]\n",
      "loss: 1.390586  [  656/ 3200]\n",
      "loss: 1.386733  [  672/ 3200]\n",
      "loss: 1.391118  [  688/ 3200]\n",
      "loss: 1.388963  [  704/ 3200]\n",
      "loss: 1.389316  [  720/ 3200]\n",
      "loss: 1.386733  [  736/ 3200]\n",
      "loss: 1.386366  [  752/ 3200]\n",
      "loss: 1.387280  [  768/ 3200]\n",
      "loss: 1.384932  [  784/ 3200]\n",
      "loss: 1.387267  [  800/ 3200]\n",
      "loss: 1.386823  [  816/ 3200]\n",
      "loss: 1.387280  [  832/ 3200]\n",
      "loss: 1.386719  [  848/ 3200]\n",
      "loss: 1.387647  [  864/ 3200]\n",
      "loss: 1.385674  [  880/ 3200]\n",
      "loss: 1.384697  [  896/ 3200]\n",
      "loss: 1.383887  [  912/ 3200]\n",
      "loss: 1.390659  [  928/ 3200]\n",
      "loss: 1.381200  [  944/ 3200]\n",
      "loss: 1.387723  [  960/ 3200]\n",
      "loss: 1.384579  [  976/ 3200]\n",
      "loss: 1.384579  [  992/ 3200]\n",
      "loss: 1.383339  [ 1008/ 3200]\n",
      "loss: 1.385922  [ 1024/ 3200]\n",
      "loss: 1.386927  [ 1040/ 3200]\n",
      "loss: 1.386379  [ 1056/ 3200]\n",
      "loss: 1.382543  [ 1072/ 3200]\n",
      "loss: 1.386026  [ 1088/ 3200]\n",
      "loss: 1.389862  [ 1104/ 3200]\n",
      "loss: 1.387161  [ 1120/ 3200]\n",
      "loss: 1.389773  [ 1136/ 3200]\n",
      "loss: 1.389654  [ 1152/ 3200]\n",
      "loss: 1.389315  [ 1168/ 3200]\n",
      "loss: 1.388519  [ 1184/ 3200]\n",
      "loss: 1.387842  [ 1200/ 3200]\n",
      "loss: 1.382440  [ 1216/ 3200]\n",
      "loss: 1.384241  [ 1232/ 3200]\n",
      "loss: 1.379843  [ 1248/ 3200]\n",
      "loss: 1.388061  [ 1264/ 3200]\n",
      "loss: 1.382559  [ 1280/ 3200]\n",
      "loss: 1.381658  [ 1296/ 3200]\n",
      "loss: 1.386026  [ 1312/ 3200]\n",
      "loss: 1.382558  [ 1328/ 3200]\n",
      "loss: 1.391559  [ 1344/ 3200]\n",
      "loss: 1.390216  [ 1360/ 3200]\n",
      "loss: 1.389773  [ 1376/ 3200]\n",
      "loss: 1.381305  [ 1392/ 3200]\n",
      "loss: 1.380758  [ 1408/ 3200]\n",
      "loss: 1.385140  [ 1424/ 3200]\n",
      "loss: 1.385126  [ 1440/ 3200]\n",
      "loss: 1.384136  [ 1456/ 3200]\n",
      "loss: 1.391365  [ 1472/ 3200]\n",
      "loss: 1.383692  [ 1488/ 3200]\n",
      "loss: 1.389406  [ 1504/ 3200]\n",
      "loss: 1.387528  [ 1520/ 3200]\n",
      "loss: 1.386365  [ 1536/ 3200]\n",
      "loss: 1.391806  [ 1552/ 3200]\n",
      "loss: 1.380053  [ 1568/ 3200]\n",
      "loss: 1.386132  [ 1584/ 3200]\n",
      "loss: 1.389757  [ 1600/ 3200]\n",
      "loss: 1.389314  [ 1616/ 3200]\n",
      "loss: 1.391911  [ 1632/ 3200]\n",
      "loss: 1.388886  [ 1648/ 3200]\n",
      "loss: 1.388961  [ 1664/ 3200]\n",
      "loss: 1.383446  [ 1680/ 3200]\n",
      "loss: 1.387632  [ 1696/ 3200]\n",
      "loss: 1.381292  [ 1712/ 3200]\n",
      "loss: 1.383460  [ 1728/ 3200]\n",
      "loss: 1.388947  [ 1744/ 3200]\n",
      "loss: 1.383783  [ 1760/ 3200]\n",
      "loss: 1.384241  [ 1776/ 3200]\n",
      "loss: 1.390019  [ 1792/ 3200]\n",
      "loss: 1.386042  [ 1808/ 3200]\n",
      "loss: 1.385922  [ 1824/ 3200]\n",
      "loss: 1.387371  [ 1840/ 3200]\n",
      "loss: 1.388961  [ 1856/ 3200]\n",
      "loss: 1.388428  [ 1872/ 3200]\n",
      "loss: 1.388075  [ 1888/ 3200]\n",
      "loss: 1.387294  [ 1904/ 3200]\n",
      "loss: 1.383108  [ 1920/ 3200]\n",
      "loss: 1.391099  [ 1936/ 3200]\n",
      "loss: 1.386275  [ 1952/ 3200]\n",
      "loss: 1.380069  [ 1968/ 3200]\n",
      "loss: 1.385141  [ 1984/ 3200]\n",
      "loss: 1.387175  [ 2000/ 3200]\n",
      "loss: 1.378479  [ 2016/ 3200]\n",
      "loss: 1.387722  [ 2032/ 3200]\n",
      "loss: 1.391099  [ 2048/ 3200]\n",
      "loss: 1.383446  [ 2064/ 3200]\n",
      "loss: 1.385155  [ 2080/ 3200]\n",
      "loss: 1.387722  [ 2096/ 3200]\n",
      "loss: 1.389300  [ 2112/ 3200]\n",
      "loss: 1.382104  [ 2128/ 3200]\n",
      "loss: 1.390642  [ 2144/ 3200]\n",
      "loss: 1.382989  [ 2160/ 3200]\n",
      "loss: 1.383784  [ 2176/ 3200]\n",
      "loss: 1.383799  [ 2192/ 3200]\n",
      "loss: 1.382768  [ 2208/ 3200]\n",
      "loss: 1.385923  [ 2224/ 3200]\n",
      "loss: 1.382885  [ 2240/ 3200]\n",
      "loss: 1.386291  [ 2256/ 3200]\n",
      "loss: 1.384449  [ 2272/ 3200]\n",
      "loss: 1.382560  [ 2288/ 3200]\n",
      "loss: 1.385494  [ 2304/ 3200]\n",
      "loss: 1.382545  [ 2320/ 3200]\n",
      "loss: 1.385037  [ 2336/ 3200]\n",
      "loss: 1.388532  [ 2352/ 3200]\n",
      "loss: 1.388608  [ 2368/ 3200]\n",
      "loss: 1.392262  [ 2384/ 3200]\n",
      "loss: 1.394932  [ 2400/ 3200]\n",
      "loss: 1.385584  [ 2416/ 3200]\n",
      "loss: 1.381854  [ 2432/ 3200]\n",
      "loss: 1.388607  [ 2448/ 3200]\n",
      "loss: 1.381205  [ 2464/ 3200]\n",
      "loss: 1.387264  [ 2480/ 3200]\n",
      "loss: 1.390213  [ 2496/ 3200]\n",
      "loss: 1.387721  [ 2512/ 3200]\n",
      "loss: 1.382547  [ 2528/ 3200]\n",
      "loss: 1.386026  [ 2544/ 3200]\n",
      "loss: 1.386189  [ 2560/ 3200]\n",
      "loss: 1.389756  [ 2576/ 3200]\n",
      "loss: 1.387824  [ 2592/ 3200]\n",
      "loss: 1.387190  [ 2608/ 3200]\n",
      "loss: 1.386822  [ 2624/ 3200]\n",
      "loss: 1.387529  [ 2640/ 3200]\n",
      "loss: 1.385834  [ 2656/ 3200]\n",
      "loss: 1.388884  [ 2672/ 3200]\n",
      "loss: 1.385154  [ 2688/ 3200]\n",
      "loss: 1.385481  [ 2704/ 3200]\n",
      "loss: 1.386913  [ 2720/ 3200]\n",
      "loss: 1.381649  [ 2736/ 3200]\n",
      "loss: 1.386026  [ 2752/ 3200]\n",
      "loss: 1.391361  [ 2768/ 3200]\n",
      "loss: 1.388060  [ 2784/ 3200]\n",
      "loss: 1.387618  [ 2800/ 3200]\n",
      "loss: 1.382977  [ 2816/ 3200]\n",
      "loss: 1.385141  [ 2832/ 3200]\n",
      "loss: 1.386808  [ 2848/ 3200]\n",
      "loss: 1.381987  [ 2864/ 3200]\n",
      "loss: 1.382547  [ 2880/ 3200]\n",
      "loss: 1.389062  [ 2896/ 3200]\n",
      "loss: 1.385584  [ 2912/ 3200]\n",
      "loss: 1.389431  [ 2928/ 3200]\n",
      "loss: 1.389224  [ 2944/ 3200]\n",
      "loss: 1.382886  [ 2960/ 3200]\n",
      "loss: 1.393143  [ 2976/ 3200]\n",
      "loss: 1.391006  [ 2992/ 3200]\n",
      "loss: 1.385480  [ 3008/ 3200]\n",
      "loss: 1.384700  [ 3024/ 3200]\n",
      "loss: 1.388605  [ 3040/ 3200]\n",
      "loss: 1.391801  [ 3056/ 3200]\n",
      "loss: 1.393141  [ 3072/ 3200]\n",
      "loss: 1.380428  [ 3088/ 3200]\n",
      "loss: 1.386027  [ 3104/ 3200]\n",
      "loss: 1.385819  [ 3120/ 3200]\n",
      "loss: 1.387264  [ 3136/ 3200]\n",
      "loss: 1.390417  [ 3152/ 3200]\n",
      "loss: 1.388515  [ 3168/ 3200]\n",
      "loss: 1.385142  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1312e-04.\n",
      "\n",
      "Epoch: 57\n",
      "-----------------------------\n",
      "loss: 1.387160  [    0/ 3200]\n",
      "loss: 1.385496  [   16/ 3200]\n",
      "loss: 1.386718  [   32/ 3200]\n",
      "loss: 1.381755  [   48/ 3200]\n",
      "loss: 1.387616  [   64/ 3200]\n",
      "loss: 1.379722  [   80/ 3200]\n",
      "loss: 1.385127  [   96/ 3200]\n",
      "loss: 1.386940  [  112/ 3200]\n",
      "loss: 1.388750  [  128/ 3200]\n",
      "loss: 1.391872  [  144/ 3200]\n",
      "loss: 1.387529  [  160/ 3200]\n",
      "loss: 1.391533  [  176/ 3200]\n",
      "loss: 1.387719  [  192/ 3200]\n",
      "loss: 1.387398  [  208/ 3200]\n",
      "loss: 1.386379  [  224/ 3200]\n",
      "loss: 1.388144  [  240/ 3200]\n",
      "loss: 1.385481  [  256/ 3200]\n",
      "loss: 1.388057  [  272/ 3200]\n",
      "loss: 1.387953  [  288/ 3200]\n",
      "loss: 1.389750  [  304/ 3200]\n",
      "loss: 1.386276  [  320/ 3200]\n",
      "loss: 1.384158  [  336/ 3200]\n",
      "loss: 1.380166  [  352/ 3200]\n",
      "loss: 1.388160  [  368/ 3200]\n",
      "loss: 1.382092  [  384/ 3200]\n",
      "loss: 1.386820  [  400/ 3200]\n",
      "loss: 1.385835  [  416/ 3200]\n",
      "loss: 1.388496  [  432/ 3200]\n",
      "loss: 1.383993  [  448/ 3200]\n",
      "loss: 1.386276  [  464/ 3200]\n",
      "loss: 1.388429  [  480/ 3200]\n",
      "loss: 1.386361  [  496/ 3200]\n",
      "loss: 1.385247  [  512/ 3200]\n",
      "loss: 1.389205  [  528/ 3200]\n",
      "loss: 1.387089  [  544/ 3200]\n",
      "loss: 1.387719  [  560/ 3200]\n",
      "loss: 1.386716  [  576/ 3200]\n",
      "loss: 1.384495  [  592/ 3200]\n",
      "loss: 1.386257  [  608/ 3200]\n",
      "loss: 1.387633  [  624/ 3200]\n",
      "loss: 1.389749  [  640/ 3200]\n",
      "loss: 1.386925  [  656/ 3200]\n",
      "loss: 1.385162  [  672/ 3200]\n",
      "loss: 1.387528  [  688/ 3200]\n",
      "loss: 1.385938  [  704/ 3200]\n",
      "loss: 1.384350  [  720/ 3200]\n",
      "loss: 1.386130  [  736/ 3200]\n",
      "loss: 1.390544  [  752/ 3200]\n",
      "loss: 1.389748  [  768/ 3200]\n",
      "loss: 1.384686  [  784/ 3200]\n",
      "loss: 1.388055  [  800/ 3200]\n",
      "loss: 1.386275  [  816/ 3200]\n",
      "loss: 1.388954  [  832/ 3200]\n",
      "loss: 1.390188  [  848/ 3200]\n",
      "loss: 1.379624  [  864/ 3200]\n",
      "loss: 1.386275  [  880/ 3200]\n",
      "loss: 1.388054  [  896/ 3200]\n",
      "loss: 1.388054  [  912/ 3200]\n",
      "loss: 1.390206  [  928/ 3200]\n",
      "loss: 1.386379  [  944/ 3200]\n",
      "loss: 1.383012  [  960/ 3200]\n",
      "loss: 1.395248  [  976/ 3200]\n",
      "loss: 1.385040  [  992/ 3200]\n",
      "loss: 1.385126  [ 1008/ 3200]\n",
      "loss: 1.387278  [ 1024/ 3200]\n",
      "loss: 1.385835  [ 1040/ 3200]\n",
      "loss: 1.389431  [ 1056/ 3200]\n",
      "loss: 1.386733  [ 1072/ 3200]\n",
      "loss: 1.388866  [ 1088/ 3200]\n",
      "loss: 1.386484  [ 1104/ 3200]\n",
      "loss: 1.389220  [ 1120/ 3200]\n",
      "loss: 1.390558  [ 1136/ 3200]\n",
      "loss: 1.384687  [ 1152/ 3200]\n",
      "loss: 1.382535  [ 1168/ 3200]\n",
      "loss: 1.388760  [ 1184/ 3200]\n",
      "loss: 1.388406  [ 1200/ 3200]\n",
      "loss: 1.386026  [ 1216/ 3200]\n",
      "loss: 1.381760  [ 1232/ 3200]\n",
      "loss: 1.377035  [ 1248/ 3200]\n",
      "loss: 1.390645  [ 1264/ 3200]\n",
      "loss: 1.385834  [ 1280/ 3200]\n",
      "loss: 1.379627  [ 1296/ 3200]\n",
      "loss: 1.388512  [ 1312/ 3200]\n",
      "loss: 1.385126  [ 1328/ 3200]\n",
      "loss: 1.386713  [ 1344/ 3200]\n",
      "loss: 1.385939  [ 1360/ 3200]\n",
      "loss: 1.388637  [ 1376/ 3200]\n",
      "loss: 1.392883  [ 1392/ 3200]\n",
      "loss: 1.384352  [ 1408/ 3200]\n",
      "loss: 1.386274  [ 1424/ 3200]\n",
      "loss: 1.384141  [ 1440/ 3200]\n",
      "loss: 1.385040  [ 1456/ 3200]\n",
      "loss: 1.386045  [ 1472/ 3200]\n",
      "loss: 1.388865  [ 1488/ 3200]\n",
      "loss: 1.381656  [ 1504/ 3200]\n",
      "loss: 1.383453  [ 1520/ 3200]\n",
      "loss: 1.384246  [ 1536/ 3200]\n",
      "loss: 1.389057  [ 1552/ 3200]\n",
      "loss: 1.386044  [ 1568/ 3200]\n",
      "loss: 1.388761  [ 1584/ 3200]\n",
      "loss: 1.381778  [ 1600/ 3200]\n",
      "loss: 1.387278  [ 1616/ 3200]\n",
      "loss: 1.384351  [ 1632/ 3200]\n",
      "loss: 1.390980  [ 1648/ 3200]\n",
      "loss: 1.388970  [ 1664/ 3200]\n",
      "loss: 1.386361  [ 1680/ 3200]\n",
      "loss: 1.389850  [ 1696/ 3200]\n",
      "loss: 1.385903  [ 1712/ 3200]\n",
      "loss: 1.385921  [ 1728/ 3200]\n",
      "loss: 1.384810  [ 1744/ 3200]\n",
      "loss: 1.389764  [ 1760/ 3200]\n",
      "loss: 1.386379  [ 1776/ 3200]\n",
      "loss: 1.386025  [ 1792/ 3200]\n",
      "loss: 1.388951  [ 1808/ 3200]\n",
      "loss: 1.384583  [ 1824/ 3200]\n",
      "loss: 1.390624  [ 1840/ 3200]\n",
      "loss: 1.381218  [ 1856/ 3200]\n",
      "loss: 1.385566  [ 1872/ 3200]\n",
      "loss: 1.385959  [ 1888/ 3200]\n",
      "loss: 1.389744  [ 1904/ 3200]\n",
      "loss: 1.383453  [ 1920/ 3200]\n",
      "loss: 1.387717  [ 1936/ 3200]\n",
      "loss: 1.383892  [ 1952/ 3200]\n",
      "loss: 1.383912  [ 1968/ 3200]\n",
      "loss: 1.386379  [ 1984/ 3200]\n",
      "loss: 1.391688  [ 2000/ 3200]\n",
      "loss: 1.383118  [ 2016/ 3200]\n",
      "loss: 1.386043  [ 2032/ 3200]\n",
      "loss: 1.383911  [ 2048/ 3200]\n",
      "loss: 1.384583  [ 2064/ 3200]\n",
      "loss: 1.390997  [ 2080/ 3200]\n",
      "loss: 1.386819  [ 2096/ 3200]\n",
      "loss: 1.387967  [ 2112/ 3200]\n",
      "loss: 1.382220  [ 2128/ 3200]\n",
      "loss: 1.380882  [ 2144/ 3200]\n",
      "loss: 1.383685  [ 2160/ 3200]\n",
      "loss: 1.383807  [ 2176/ 3200]\n",
      "loss: 1.387259  [ 2192/ 3200]\n",
      "loss: 1.386360  [ 2208/ 3200]\n",
      "loss: 1.384934  [ 2224/ 3200]\n",
      "loss: 1.385939  [ 2240/ 3200]\n",
      "loss: 1.385041  [ 2256/ 3200]\n",
      "loss: 1.385377  [ 2272/ 3200]\n",
      "loss: 1.387154  [ 2288/ 3200]\n",
      "loss: 1.383807  [ 2304/ 3200]\n",
      "loss: 1.387967  [ 2320/ 3200]\n",
      "loss: 1.388511  [ 2336/ 3200]\n",
      "loss: 1.382660  [ 2352/ 3200]\n",
      "loss: 1.386733  [ 2368/ 3200]\n",
      "loss: 1.393921  [ 2384/ 3200]\n",
      "loss: 1.391331  [ 2400/ 3200]\n",
      "loss: 1.385957  [ 2416/ 3200]\n",
      "loss: 1.386397  [ 2432/ 3200]\n",
      "loss: 1.381763  [ 2448/ 3200]\n",
      "loss: 1.385041  [ 2464/ 3200]\n",
      "loss: 1.386131  [ 2480/ 3200]\n",
      "loss: 1.385481  [ 2496/ 3200]\n",
      "loss: 1.383454  [ 2512/ 3200]\n",
      "loss: 1.390220  [ 2528/ 3200]\n",
      "loss: 1.381763  [ 2544/ 3200]\n",
      "loss: 1.389304  [ 2560/ 3200]\n",
      "loss: 1.385128  [ 2576/ 3200]\n",
      "loss: 1.386484  [ 2592/ 3200]\n",
      "loss: 1.379863  [ 2608/ 3200]\n",
      "loss: 1.382220  [ 2624/ 3200]\n",
      "loss: 1.380986  [ 2640/ 3200]\n",
      "loss: 1.385041  [ 2656/ 3200]\n",
      "loss: 1.393232  [ 2672/ 3200]\n",
      "loss: 1.385145  [ 2688/ 3200]\n",
      "loss: 1.385817  [ 2704/ 3200]\n",
      "loss: 1.382116  [ 2720/ 3200]\n",
      "loss: 1.387735  [ 2736/ 3200]\n",
      "loss: 1.392334  [ 2752/ 3200]\n",
      "loss: 1.387613  [ 2768/ 3200]\n",
      "loss: 1.386043  [ 2784/ 3200]\n",
      "loss: 1.384954  [ 2800/ 3200]\n",
      "loss: 1.384247  [ 2816/ 3200]\n",
      "loss: 1.389408  [ 2832/ 3200]\n",
      "loss: 1.386819  [ 2848/ 3200]\n",
      "loss: 1.386732  [ 2864/ 3200]\n",
      "loss: 1.386820  [ 2880/ 3200]\n",
      "loss: 1.383471  [ 2896/ 3200]\n",
      "loss: 1.388054  [ 2912/ 3200]\n",
      "loss: 1.387277  [ 2928/ 3200]\n",
      "loss: 1.386483  [ 2944/ 3200]\n",
      "loss: 1.394463  [ 2960/ 3200]\n",
      "loss: 1.379986  [ 2976/ 3200]\n",
      "loss: 1.385834  [ 2992/ 3200]\n",
      "loss: 1.384584  [ 3008/ 3200]\n",
      "loss: 1.394359  [ 3024/ 3200]\n",
      "loss: 1.384705  [ 3040/ 3200]\n",
      "loss: 1.387628  [ 3056/ 3200]\n",
      "loss: 1.381990  [ 3072/ 3200]\n",
      "loss: 1.388173  [ 3088/ 3200]\n",
      "loss: 1.387747  [ 3104/ 3200]\n",
      "loss: 1.387835  [ 3120/ 3200]\n",
      "loss: 1.383881  [ 3136/ 3200]\n",
      "loss: 1.386820  [ 3152/ 3200]\n",
      "loss: 1.381676  [ 3168/ 3200]\n",
      "loss: 1.384351  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0747e-04.\n",
      "\n",
      "Epoch: 58\n",
      "-----------------------------\n",
      "loss: 1.388951  [    0/ 3200]\n",
      "loss: 1.390567  [   16/ 3200]\n",
      "loss: 1.391522  [   32/ 3200]\n",
      "loss: 1.387628  [   48/ 3200]\n",
      "loss: 1.379649  [   64/ 3200]\n",
      "loss: 1.379649  [   80/ 3200]\n",
      "loss: 1.385938  [   96/ 3200]\n",
      "loss: 1.382104  [  112/ 3200]\n",
      "loss: 1.378385  [  128/ 3200]\n",
      "loss: 1.383454  [  144/ 3200]\n",
      "loss: 1.386643  [  160/ 3200]\n",
      "loss: 1.385130  [  176/ 3200]\n",
      "loss: 1.386731  [  192/ 3200]\n",
      "loss: 1.390186  [  208/ 3200]\n",
      "loss: 1.395607  [  224/ 3200]\n",
      "loss: 1.391344  [  240/ 3200]\n",
      "loss: 1.382455  [  256/ 3200]\n",
      "loss: 1.387276  [  272/ 3200]\n",
      "loss: 1.391875  [  288/ 3200]\n",
      "loss: 1.391784  [  304/ 3200]\n",
      "loss: 1.395238  [  320/ 3200]\n",
      "loss: 1.383352  [  336/ 3200]\n",
      "loss: 1.384690  [  352/ 3200]\n",
      "loss: 1.385601  [  368/ 3200]\n",
      "loss: 1.389302  [  384/ 3200]\n",
      "loss: 1.382132  [  400/ 3200]\n",
      "loss: 1.385951  [  416/ 3200]\n",
      "loss: 1.385495  [  432/ 3200]\n",
      "loss: 1.389743  [  448/ 3200]\n",
      "loss: 1.389301  [  464/ 3200]\n",
      "loss: 1.389067  [  480/ 3200]\n",
      "loss: 1.384106  [  496/ 3200]\n",
      "loss: 1.380534  [  512/ 3200]\n",
      "loss: 1.390978  [  528/ 3200]\n",
      "loss: 1.386495  [  544/ 3200]\n",
      "loss: 1.386717  [  560/ 3200]\n",
      "loss: 1.391522  [  576/ 3200]\n",
      "loss: 1.392755  [  592/ 3200]\n",
      "loss: 1.383251  [  608/ 3200]\n",
      "loss: 1.385496  [  624/ 3200]\n",
      "loss: 1.390976  [  640/ 3200]\n",
      "loss: 1.386392  [  656/ 3200]\n",
      "loss: 1.383353  [  672/ 3200]\n",
      "loss: 1.382885  [  688/ 3200]\n",
      "loss: 1.387964  [  704/ 3200]\n",
      "loss: 1.386263  [  720/ 3200]\n",
      "loss: 1.385924  [  736/ 3200]\n",
      "loss: 1.388949  [  752/ 3200]\n",
      "loss: 1.381679  [  768/ 3200]\n",
      "loss: 1.387716  [  784/ 3200]\n",
      "loss: 1.383457  [  800/ 3200]\n",
      "loss: 1.386027  [  816/ 3200]\n",
      "loss: 1.383457  [  832/ 3200]\n",
      "loss: 1.388611  [  848/ 3200]\n",
      "loss: 1.390976  [  864/ 3200]\n",
      "loss: 1.388860  [  880/ 3200]\n",
      "loss: 1.389390  [  896/ 3200]\n",
      "loss: 1.381225  [  912/ 3200]\n",
      "loss: 1.383809  [  928/ 3200]\n",
      "loss: 1.393469  [  944/ 3200]\n",
      "loss: 1.382990  [  960/ 3200]\n",
      "loss: 1.391870  [  976/ 3200]\n",
      "loss: 1.385938  [  992/ 3200]\n",
      "loss: 1.383795  [ 1008/ 3200]\n",
      "loss: 1.383354  [ 1024/ 3200]\n",
      "loss: 1.389404  [ 1040/ 3200]\n",
      "loss: 1.388081  [ 1056/ 3200]\n",
      "loss: 1.386482  [ 1072/ 3200]\n",
      "loss: 1.383017  [ 1088/ 3200]\n",
      "loss: 1.387274  [ 1104/ 3200]\n",
      "loss: 1.378408  [ 1120/ 3200]\n",
      "loss: 1.387612  [ 1136/ 3200]\n",
      "loss: 1.388948  [ 1152/ 3200]\n",
      "loss: 1.380888  [ 1168/ 3200]\n",
      "loss: 1.390561  [ 1184/ 3200]\n",
      "loss: 1.393896  [ 1200/ 3200]\n",
      "loss: 1.377851  [ 1216/ 3200]\n",
      "loss: 1.383471  [ 1232/ 3200]\n",
      "loss: 1.385599  [ 1248/ 3200]\n",
      "loss: 1.388169  [ 1264/ 3200]\n",
      "loss: 1.387377  [ 1280/ 3200]\n",
      "loss: 1.383911  [ 1296/ 3200]\n",
      "loss: 1.388054  [ 1312/ 3200]\n",
      "loss: 1.385132  [ 1328/ 3200]\n",
      "loss: 1.385145  [ 1344/ 3200]\n",
      "loss: 1.388769  [ 1360/ 3200]\n",
      "loss: 1.380537  [ 1376/ 3200]\n",
      "loss: 1.380536  [ 1392/ 3200]\n",
      "loss: 1.389651  [ 1408/ 3200]\n",
      "loss: 1.386808  [ 1424/ 3200]\n",
      "loss: 1.386923  [ 1440/ 3200]\n",
      "loss: 1.382447  [ 1456/ 3200]\n",
      "loss: 1.385235  [ 1472/ 3200]\n",
      "loss: 1.393531  [ 1488/ 3200]\n",
      "loss: 1.384236  [ 1504/ 3200]\n",
      "loss: 1.388052  [ 1520/ 3200]\n",
      "loss: 1.389050  [ 1536/ 3200]\n",
      "loss: 1.380434  [ 1552/ 3200]\n",
      "loss: 1.387729  [ 1568/ 3200]\n",
      "loss: 1.390650  [ 1584/ 3200]\n",
      "loss: 1.388521  [ 1600/ 3200]\n",
      "loss: 1.385247  [ 1616/ 3200]\n",
      "loss: 1.386819  [ 1632/ 3200]\n",
      "loss: 1.393115  [ 1648/ 3200]\n",
      "loss: 1.382462  [ 1664/ 3200]\n",
      "loss: 1.377853  [ 1680/ 3200]\n",
      "loss: 1.385131  [ 1696/ 3200]\n",
      "loss: 1.384251  [ 1712/ 3200]\n",
      "loss: 1.383370  [ 1728/ 3200]\n",
      "loss: 1.385484  [ 1744/ 3200]\n",
      "loss: 1.381668  [ 1760/ 3200]\n",
      "loss: 1.390547  [ 1776/ 3200]\n",
      "loss: 1.393997  [ 1792/ 3200]\n",
      "loss: 1.390973  [ 1808/ 3200]\n",
      "loss: 1.390532  [ 1824/ 3200]\n",
      "loss: 1.382813  [ 1840/ 3200]\n",
      "loss: 1.383899  [ 1856/ 3200]\n",
      "loss: 1.385249  [ 1872/ 3200]\n",
      "loss: 1.384251  [ 1888/ 3200]\n",
      "loss: 1.388507  [ 1904/ 3200]\n",
      "loss: 1.387626  [ 1920/ 3200]\n",
      "loss: 1.386027  [ 1936/ 3200]\n",
      "loss: 1.383018  [ 1952/ 3200]\n",
      "loss: 1.387728  [ 1968/ 3200]\n",
      "loss: 1.385836  [ 1984/ 3200]\n",
      "loss: 1.390194  [ 2000/ 3200]\n",
      "loss: 1.384808  [ 2016/ 3200]\n",
      "loss: 1.388947  [ 2032/ 3200]\n",
      "loss: 1.382666  [ 2048/ 3200]\n",
      "loss: 1.384251  [ 2064/ 3200]\n",
      "loss: 1.381771  [ 2080/ 3200]\n",
      "loss: 1.391088  [ 2096/ 3200]\n",
      "loss: 1.386921  [ 2112/ 3200]\n",
      "loss: 1.384705  [ 2128/ 3200]\n",
      "loss: 1.386819  [ 2144/ 3200]\n",
      "loss: 1.390634  [ 2160/ 3200]\n",
      "loss: 1.386263  [ 2176/ 3200]\n",
      "loss: 1.380993  [ 2192/ 3200]\n",
      "loss: 1.392761  [ 2208/ 3200]\n",
      "loss: 1.382767  [ 2224/ 3200]\n",
      "loss: 1.384265  [ 2240/ 3200]\n",
      "loss: 1.391338  [ 2256/ 3200]\n",
      "loss: 1.388404  [ 2272/ 3200]\n",
      "loss: 1.387612  [ 2288/ 3200]\n",
      "loss: 1.385938  [ 2304/ 3200]\n",
      "loss: 1.388417  [ 2320/ 3200]\n",
      "loss: 1.385586  [ 2336/ 3200]\n",
      "loss: 1.383459  [ 2352/ 3200]\n",
      "loss: 1.385159  [ 2368/ 3200]\n",
      "loss: 1.382124  [ 2384/ 3200]\n",
      "loss: 1.388506  [ 2400/ 3200]\n",
      "loss: 1.385396  [ 2416/ 3200]\n",
      "loss: 1.388608  [ 2432/ 3200]\n",
      "loss: 1.380540  [ 2448/ 3200]\n",
      "loss: 1.387713  [ 2464/ 3200]\n",
      "loss: 1.386832  [ 2480/ 3200]\n",
      "loss: 1.385032  [ 2496/ 3200]\n",
      "loss: 1.385938  [ 2512/ 3200]\n",
      "loss: 1.387713  [ 2528/ 3200]\n",
      "loss: 1.385925  [ 2544/ 3200]\n",
      "loss: 1.384705  [ 2560/ 3200]\n",
      "loss: 1.384705  [ 2576/ 3200]\n",
      "loss: 1.387624  [ 2592/ 3200]\n",
      "loss: 1.380551  [ 2608/ 3200]\n",
      "loss: 1.385145  [ 2624/ 3200]\n",
      "loss: 1.382916  [ 2640/ 3200]\n",
      "loss: 1.385836  [ 2656/ 3200]\n",
      "loss: 1.384805  [ 2672/ 3200]\n",
      "loss: 1.385937  [ 2688/ 3200]\n",
      "loss: 1.380778  [ 2704/ 3200]\n",
      "loss: 1.384138  [ 2720/ 3200]\n",
      "loss: 1.388053  [ 2736/ 3200]\n",
      "loss: 1.384251  [ 2752/ 3200]\n",
      "loss: 1.392309  [ 2768/ 3200]\n",
      "loss: 1.384603  [ 2784/ 3200]\n",
      "loss: 1.391335  [ 2800/ 3200]\n",
      "loss: 1.384149  [ 2816/ 3200]\n",
      "loss: 1.386809  [ 2832/ 3200]\n",
      "loss: 1.393552  [ 2848/ 3200]\n",
      "loss: 1.383007  [ 2864/ 3200]\n",
      "loss: 1.382668  [ 2880/ 3200]\n",
      "loss: 1.386819  [ 2896/ 3200]\n",
      "loss: 1.391866  [ 2912/ 3200]\n",
      "loss: 1.388167  [ 2928/ 3200]\n",
      "loss: 1.388607  [ 2944/ 3200]\n",
      "loss: 1.386921  [ 2960/ 3200]\n",
      "loss: 1.391865  [ 2976/ 3200]\n",
      "loss: 1.387814  [ 2992/ 3200]\n",
      "loss: 1.386039  [ 3008/ 3200]\n",
      "loss: 1.386706  [ 3024/ 3200]\n",
      "loss: 1.390618  [ 3040/ 3200]\n",
      "loss: 1.382025  [ 3056/ 3200]\n",
      "loss: 1.381787  [ 3072/ 3200]\n",
      "loss: 1.387624  [ 3088/ 3200]\n",
      "loss: 1.387497  [ 3104/ 3200]\n",
      "loss: 1.383345  [ 3120/ 3200]\n",
      "loss: 1.383785  [ 3136/ 3200]\n",
      "loss: 1.385397  [ 3152/ 3200]\n",
      "loss: 1.385836  [ 3168/ 3200]\n",
      "loss: 1.388389  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0209e-04.\n",
      "\n",
      "Epoch: 59\n",
      "-----------------------------\n",
      "loss: 1.385572  [    0/ 3200]\n",
      "loss: 1.388168  [   16/ 3200]\n",
      "loss: 1.386466  [   32/ 3200]\n",
      "loss: 1.384707  [   48/ 3200]\n",
      "loss: 1.383460  [   64/ 3200]\n",
      "loss: 1.386731  [   80/ 3200]\n",
      "loss: 1.380675  [   96/ 3200]\n",
      "loss: 1.389399  [  112/ 3200]\n",
      "loss: 1.380454  [  128/ 3200]\n",
      "loss: 1.383900  [  144/ 3200]\n",
      "loss: 1.388065  [  160/ 3200]\n",
      "loss: 1.391863  [  176/ 3200]\n",
      "loss: 1.393124  [  192/ 3200]\n",
      "loss: 1.388167  [  208/ 3200]\n",
      "loss: 1.390733  [  224/ 3200]\n",
      "loss: 1.387610  [  240/ 3200]\n",
      "loss: 1.386818  [  256/ 3200]\n",
      "loss: 1.378756  [  272/ 3200]\n",
      "loss: 1.388944  [  288/ 3200]\n",
      "loss: 1.385939  [  304/ 3200]\n",
      "loss: 1.385924  [  320/ 3200]\n",
      "loss: 1.384942  [  336/ 3200]\n",
      "loss: 1.381335  [  352/ 3200]\n",
      "loss: 1.382214  [  368/ 3200]\n",
      "loss: 1.382918  [  384/ 3200]\n",
      "loss: 1.385499  [  400/ 3200]\n",
      "loss: 1.387610  [  416/ 3200]\n",
      "loss: 1.388754  [  432/ 3200]\n",
      "loss: 1.383461  [  448/ 3200]\n",
      "loss: 1.391525  [  464/ 3200]\n",
      "loss: 1.387713  [  480/ 3200]\n",
      "loss: 1.386818  [  496/ 3200]\n",
      "loss: 1.389735  [  512/ 3200]\n",
      "loss: 1.388856  [  528/ 3200]\n",
      "loss: 1.386042  [  544/ 3200]\n",
      "loss: 1.392755  [  560/ 3200]\n",
      "loss: 1.385059  [  576/ 3200]\n",
      "loss: 1.387375  [  592/ 3200]\n",
      "loss: 1.383124  [  608/ 3200]\n",
      "loss: 1.384253  [  624/ 3200]\n",
      "loss: 1.387272  [  640/ 3200]\n",
      "loss: 1.384137  [  656/ 3200]\n",
      "loss: 1.383373  [  672/ 3200]\n",
      "loss: 1.386716  [  688/ 3200]\n",
      "loss: 1.384253  [  704/ 3200]\n",
      "loss: 1.379313  [  720/ 3200]\n",
      "loss: 1.392653  [  736/ 3200]\n",
      "loss: 1.382113  [  752/ 3200]\n",
      "loss: 1.387170  [  768/ 3200]\n",
      "loss: 1.385953  [  784/ 3200]\n",
      "loss: 1.383799  [  800/ 3200]\n",
      "loss: 1.383124  [  816/ 3200]\n",
      "loss: 1.383461  [  832/ 3200]\n",
      "loss: 1.385044  [  848/ 3200]\n",
      "loss: 1.389412  [  864/ 3200]\n",
      "loss: 1.381336  [  880/ 3200]\n",
      "loss: 1.390968  [  896/ 3200]\n",
      "loss: 1.388620  [  912/ 3200]\n",
      "loss: 1.381336  [  928/ 3200]\n",
      "loss: 1.389749  [  944/ 3200]\n",
      "loss: 1.388944  [  960/ 3200]\n",
      "loss: 1.384240  [  976/ 3200]\n",
      "loss: 1.388429  [  992/ 3200]\n",
      "loss: 1.389748  [ 1008/ 3200]\n",
      "loss: 1.390617  [ 1024/ 3200]\n",
      "loss: 1.385045  [ 1040/ 3200]\n",
      "loss: 1.390189  [ 1056/ 3200]\n",
      "loss: 1.390086  [ 1072/ 3200]\n",
      "loss: 1.385938  [ 1088/ 3200]\n",
      "loss: 1.387725  [ 1104/ 3200]\n",
      "loss: 1.379755  [ 1120/ 3200]\n",
      "loss: 1.388063  [ 1136/ 3200]\n",
      "loss: 1.386729  [ 1152/ 3200]\n",
      "loss: 1.387386  [ 1168/ 3200]\n",
      "loss: 1.386039  [ 1184/ 3200]\n",
      "loss: 1.390515  [ 1200/ 3200]\n",
      "loss: 1.387610  [ 1216/ 3200]\n",
      "loss: 1.393091  [ 1232/ 3200]\n",
      "loss: 1.393645  [ 1248/ 3200]\n",
      "loss: 1.381237  [ 1264/ 3200]\n",
      "loss: 1.388943  [ 1280/ 3200]\n",
      "loss: 1.381237  [ 1296/ 3200]\n",
      "loss: 1.379654  [ 1312/ 3200]\n",
      "loss: 1.383915  [ 1328/ 3200]\n",
      "loss: 1.386933  [ 1344/ 3200]\n",
      "loss: 1.388063  [ 1360/ 3200]\n",
      "loss: 1.386705  [ 1376/ 3200]\n",
      "loss: 1.391431  [ 1392/ 3200]\n",
      "loss: 1.382684  [ 1408/ 3200]\n",
      "loss: 1.388152  [ 1424/ 3200]\n",
      "loss: 1.387711  [ 1440/ 3200]\n",
      "loss: 1.384254  [ 1456/ 3200]\n",
      "loss: 1.388943  [ 1472/ 3200]\n",
      "loss: 1.387936  [ 1488/ 3200]\n",
      "loss: 1.385485  [ 1504/ 3200]\n",
      "loss: 1.389632  [ 1520/ 3200]\n",
      "loss: 1.384254  [ 1536/ 3200]\n",
      "loss: 1.380006  [ 1552/ 3200]\n",
      "loss: 1.389938  [ 1568/ 3200]\n",
      "loss: 1.385837  [ 1584/ 3200]\n",
      "loss: 1.392298  [ 1600/ 3200]\n",
      "loss: 1.386027  [ 1616/ 3200]\n",
      "loss: 1.388942  [ 1632/ 3200]\n",
      "loss: 1.383348  [ 1648/ 3200]\n",
      "loss: 1.388414  [ 1664/ 3200]\n",
      "loss: 1.384268  [ 1680/ 3200]\n",
      "loss: 1.379994  [ 1696/ 3200]\n",
      "loss: 1.386027  [ 1712/ 3200]\n",
      "loss: 1.384064  [ 1728/ 3200]\n",
      "loss: 1.383450  [ 1744/ 3200]\n",
      "loss: 1.387258  [ 1760/ 3200]\n",
      "loss: 1.384694  [ 1776/ 3200]\n",
      "loss: 1.386832  [ 1792/ 3200]\n",
      "loss: 1.382672  [ 1808/ 3200]\n",
      "loss: 1.387725  [ 1824/ 3200]\n",
      "loss: 1.382584  [ 1840/ 3200]\n",
      "loss: 1.380663  [ 1856/ 3200]\n",
      "loss: 1.383814  [ 1872/ 3200]\n",
      "loss: 1.384694  [ 1888/ 3200]\n",
      "loss: 1.389822  [ 1904/ 3200]\n",
      "loss: 1.386730  [ 1920/ 3200]\n",
      "loss: 1.382583  [ 1936/ 3200]\n",
      "loss: 1.380331  [ 1952/ 3200]\n",
      "loss: 1.387623  [ 1968/ 3200]\n",
      "loss: 1.387170  [ 1984/ 3200]\n",
      "loss: 1.383125  [ 2000/ 3200]\n",
      "loss: 1.385046  [ 2016/ 3200]\n",
      "loss: 1.392561  [ 2032/ 3200]\n",
      "loss: 1.390716  [ 2048/ 3200]\n",
      "loss: 1.384254  [ 2064/ 3200]\n",
      "loss: 1.391871  [ 2080/ 3200]\n",
      "loss: 1.383814  [ 2096/ 3200]\n",
      "loss: 1.389382  [ 2112/ 3200]\n",
      "loss: 1.384255  [ 2128/ 3200]\n",
      "loss: 1.378648  [ 2144/ 3200]\n",
      "loss: 1.386832  [ 2160/ 3200]\n",
      "loss: 1.378309  [ 2176/ 3200]\n",
      "loss: 1.390978  [ 2192/ 3200]\n",
      "loss: 1.389205  [ 2208/ 3200]\n",
      "loss: 1.387169  [ 2224/ 3200]\n",
      "loss: 1.386494  [ 2240/ 3200]\n",
      "loss: 1.384254  [ 2256/ 3200]\n",
      "loss: 1.389383  [ 2272/ 3200]\n",
      "loss: 1.382233  [ 2288/ 3200]\n",
      "loss: 1.391168  [ 2304/ 3200]\n",
      "loss: 1.380887  [ 2320/ 3200]\n",
      "loss: 1.385587  [ 2336/ 3200]\n",
      "loss: 1.386027  [ 2352/ 3200]\n",
      "loss: 1.386392  [ 2368/ 3200]\n",
      "loss: 1.390977  [ 2384/ 3200]\n",
      "loss: 1.386277  [ 2400/ 3200]\n",
      "loss: 1.384694  [ 2416/ 3200]\n",
      "loss: 1.383904  [ 2432/ 3200]\n",
      "loss: 1.387711  [ 2448/ 3200]\n",
      "loss: 1.387521  [ 2464/ 3200]\n",
      "loss: 1.383667  [ 2480/ 3200]\n",
      "loss: 1.388854  [ 2496/ 3200]\n",
      "loss: 1.381252  [ 2512/ 3200]\n",
      "loss: 1.391857  [ 2528/ 3200]\n",
      "loss: 1.384166  [ 2544/ 3200]\n",
      "loss: 1.385913  [ 2560/ 3200]\n",
      "loss: 1.384593  [ 2576/ 3200]\n",
      "loss: 1.385574  [ 2592/ 3200]\n",
      "loss: 1.387285  [ 2608/ 3200]\n",
      "loss: 1.384153  [ 2624/ 3200]\n",
      "loss: 1.386027  [ 2640/ 3200]\n",
      "loss: 1.386378  [ 2656/ 3200]\n",
      "loss: 1.387418  [ 2672/ 3200]\n",
      "loss: 1.391754  [ 2688/ 3200]\n",
      "loss: 1.387609  [ 2704/ 3200]\n",
      "loss: 1.387609  [ 2720/ 3200]\n",
      "loss: 1.382132  [ 2736/ 3200]\n",
      "loss: 1.385574  [ 2752/ 3200]\n",
      "loss: 1.376641  [ 2768/ 3200]\n",
      "loss: 1.387609  [ 2784/ 3200]\n",
      "loss: 1.380887  [ 2800/ 3200]\n",
      "loss: 1.389278  [ 2816/ 3200]\n",
      "loss: 1.384358  [ 2832/ 3200]\n",
      "loss: 1.386832  [ 2848/ 3200]\n",
      "loss: 1.386378  [ 2864/ 3200]\n",
      "loss: 1.395210  [ 2880/ 3200]\n",
      "loss: 1.393539  [ 2896/ 3200]\n",
      "loss: 1.387608  [ 2912/ 3200]\n",
      "loss: 1.389409  [ 2928/ 3200]\n",
      "loss: 1.387257  [ 2944/ 3200]\n",
      "loss: 1.382820  [ 2960/ 3200]\n",
      "loss: 1.389642  [ 2976/ 3200]\n",
      "loss: 1.387725  [ 2992/ 3200]\n",
      "loss: 1.393098  [ 3008/ 3200]\n",
      "loss: 1.391414  [ 3024/ 3200]\n",
      "loss: 1.386378  [ 3040/ 3200]\n",
      "loss: 1.391399  [ 3056/ 3200]\n",
      "loss: 1.387711  [ 3072/ 3200]\n",
      "loss: 1.384344  [ 3088/ 3200]\n",
      "loss: 1.384256  [ 3104/ 3200]\n",
      "loss: 1.387519  [ 3120/ 3200]\n",
      "loss: 1.383130  [ 3136/ 3200]\n",
      "loss: 1.386920  [ 3152/ 3200]\n",
      "loss: 1.385705  [ 3168/ 3200]\n",
      "loss: 1.393096  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.6989e-05.\n",
      "\n",
      "Epoch: 60\n",
      "-----------------------------\n",
      "loss: 1.387607  [    0/ 3200]\n",
      "loss: 1.383803  [   16/ 3200]\n",
      "loss: 1.383803  [   32/ 3200]\n",
      "loss: 1.385588  [   48/ 3200]\n",
      "loss: 1.379222  [   64/ 3200]\n",
      "loss: 1.388939  [   80/ 3200]\n",
      "loss: 1.385588  [   96/ 3200]\n",
      "loss: 1.391764  [  112/ 3200]\n",
      "loss: 1.390885  [  128/ 3200]\n",
      "loss: 1.387154  [  144/ 3200]\n",
      "loss: 1.383363  [  160/ 3200]\n",
      "loss: 1.387622  [  176/ 3200]\n",
      "loss: 1.389921  [  192/ 3200]\n",
      "loss: 1.383788  [  208/ 3200]\n",
      "loss: 1.386481  [  224/ 3200]\n",
      "loss: 1.383364  [  240/ 3200]\n",
      "loss: 1.378769  [  256/ 3200]\n",
      "loss: 1.387607  [  272/ 3200]\n",
      "loss: 1.387623  [  288/ 3200]\n",
      "loss: 1.381902  [  304/ 3200]\n",
      "loss: 1.385500  [  320/ 3200]\n",
      "loss: 1.385485  [  336/ 3200]\n",
      "loss: 1.390534  [  352/ 3200]\n",
      "loss: 1.386832  [  368/ 3200]\n",
      "loss: 1.388164  [  384/ 3200]\n",
      "loss: 1.388149  [  400/ 3200]\n",
      "loss: 1.388953  [  416/ 3200]\n",
      "loss: 1.384798  [  432/ 3200]\n",
      "loss: 1.383115  [  448/ 3200]\n",
      "loss: 1.393547  [  464/ 3200]\n",
      "loss: 1.383818  [  480/ 3200]\n",
      "loss: 1.384696  [  496/ 3200]\n",
      "loss: 1.385398  [  512/ 3200]\n",
      "loss: 1.385486  [  528/ 3200]\n",
      "loss: 1.386378  [  544/ 3200]\n",
      "loss: 1.384359  [  560/ 3200]\n",
      "loss: 1.387270  [  576/ 3200]\n",
      "loss: 1.389289  [  592/ 3200]\n",
      "loss: 1.383027  [  608/ 3200]\n",
      "loss: 1.391748  [  624/ 3200]\n",
      "loss: 1.377542  [  640/ 3200]\n",
      "loss: 1.386027  [  656/ 3200]\n",
      "loss: 1.389289  [  672/ 3200]\n",
      "loss: 1.386027  [  688/ 3200]\n",
      "loss: 1.392303  [  704/ 3200]\n",
      "loss: 1.389830  [  720/ 3200]\n",
      "loss: 1.385047  [  736/ 3200]\n",
      "loss: 1.391849  [  752/ 3200]\n",
      "loss: 1.387621  [  768/ 3200]\n",
      "loss: 1.384243  [  784/ 3200]\n",
      "loss: 1.386392  [  800/ 3200]\n",
      "loss: 1.387972  [  816/ 3200]\n",
      "loss: 1.385136  [  832/ 3200]\n",
      "loss: 1.386803  [  848/ 3200]\n",
      "loss: 1.388060  [  864/ 3200]\n",
      "loss: 1.386817  [  880/ 3200]\n",
      "loss: 1.380002  [  896/ 3200]\n",
      "loss: 1.384243  [  912/ 3200]\n",
      "loss: 1.383014  [  928/ 3200]\n",
      "loss: 1.384608  [  944/ 3200]\n",
      "loss: 1.385135  [  960/ 3200]\n",
      "loss: 1.386042  [  976/ 3200]\n",
      "loss: 1.387081  [  992/ 3200]\n",
      "loss: 1.389728  [ 1008/ 3200]\n",
      "loss: 1.391410  [ 1024/ 3200]\n",
      "loss: 1.385383  [ 1040/ 3200]\n",
      "loss: 1.385589  [ 1056/ 3200]\n",
      "loss: 1.379913  [ 1072/ 3200]\n",
      "loss: 1.383365  [ 1088/ 3200]\n",
      "loss: 1.383364  [ 1104/ 3200]\n",
      "loss: 1.378332  [ 1120/ 3200]\n",
      "loss: 1.385253  [ 1136/ 3200]\n",
      "loss: 1.383674  [ 1152/ 3200]\n",
      "loss: 1.386363  [ 1168/ 3200]\n",
      "loss: 1.383921  [ 1184/ 3200]\n",
      "loss: 1.383013  [ 1200/ 3200]\n",
      "loss: 1.383379  [ 1216/ 3200]\n",
      "loss: 1.384257  [ 1232/ 3200]\n",
      "loss: 1.385368  [ 1248/ 3200]\n",
      "loss: 1.390621  [ 1264/ 3200]\n",
      "loss: 1.385954  [ 1280/ 3200]\n",
      "loss: 1.384345  [ 1296/ 3200]\n",
      "loss: 1.385924  [ 1312/ 3200]\n",
      "loss: 1.389216  [ 1328/ 3200]\n",
      "loss: 1.392756  [ 1344/ 3200]\n",
      "loss: 1.382239  [ 1360/ 3200]\n",
      "loss: 1.383921  [ 1376/ 3200]\n",
      "loss: 1.389041  [ 1392/ 3200]\n",
      "loss: 1.391952  [ 1408/ 3200]\n",
      "loss: 1.383365  [ 1424/ 3200]\n",
      "loss: 1.385135  [ 1440/ 3200]\n",
      "loss: 1.385925  [ 1456/ 3200]\n",
      "loss: 1.389289  [ 1472/ 3200]\n",
      "loss: 1.381332  [ 1488/ 3200]\n",
      "loss: 1.384593  [ 1504/ 3200]\n",
      "loss: 1.378333  [ 1520/ 3200]\n",
      "loss: 1.387080  [ 1536/ 3200]\n",
      "loss: 1.387285  [ 1552/ 3200]\n",
      "loss: 1.390635  [ 1568/ 3200]\n",
      "loss: 1.383701  [ 1584/ 3200]\n",
      "loss: 1.389289  [ 1600/ 3200]\n",
      "loss: 1.391409  [ 1616/ 3200]\n",
      "loss: 1.383118  [ 1632/ 3200]\n",
      "loss: 1.382911  [ 1648/ 3200]\n",
      "loss: 1.385150  [ 1664/ 3200]\n",
      "loss: 1.385382  [ 1680/ 3200]\n",
      "loss: 1.389624  [ 1696/ 3200]\n",
      "loss: 1.388306  [ 1712/ 3200]\n",
      "loss: 1.386043  [ 1728/ 3200]\n",
      "loss: 1.390062  [ 1744/ 3200]\n",
      "loss: 1.390969  [ 1760/ 3200]\n",
      "loss: 1.390076  [ 1776/ 3200]\n",
      "loss: 1.391318  [ 1792/ 3200]\n",
      "loss: 1.381788  [ 1808/ 3200]\n",
      "loss: 1.383484  [ 1824/ 3200]\n",
      "loss: 1.384139  [ 1840/ 3200]\n",
      "loss: 1.388045  [ 1856/ 3200]\n",
      "loss: 1.389057  [ 1872/ 3200]\n",
      "loss: 1.384801  [ 1888/ 3200]\n",
      "loss: 1.389391  [ 1904/ 3200]\n",
      "loss: 1.385150  [ 1920/ 3200]\n",
      "loss: 1.384593  [ 1936/ 3200]\n",
      "loss: 1.384607  [ 1952/ 3200]\n",
      "loss: 1.388938  [ 1968/ 3200]\n",
      "loss: 1.386378  [ 1984/ 3200]\n",
      "loss: 1.387606  [ 2000/ 3200]\n",
      "loss: 1.388045  [ 2016/ 3200]\n",
      "loss: 1.389302  [ 2032/ 3200]\n",
      "loss: 1.383484  [ 2048/ 3200]\n",
      "loss: 1.383818  [ 2064/ 3200]\n",
      "loss: 1.385953  [ 2080/ 3200]\n",
      "loss: 1.389287  [ 2096/ 3200]\n",
      "loss: 1.383470  [ 2112/ 3200]\n",
      "loss: 1.390605  [ 2128/ 3200]\n",
      "loss: 1.388045  [ 2144/ 3200]\n",
      "loss: 1.387955  [ 2160/ 3200]\n",
      "loss: 1.386378  [ 2176/ 3200]\n",
      "loss: 1.384593  [ 2192/ 3200]\n",
      "loss: 1.388044  [ 2208/ 3200]\n",
      "loss: 1.388707  [ 2224/ 3200]\n",
      "loss: 1.388951  [ 2240/ 3200]\n",
      "loss: 1.389635  [ 2256/ 3200]\n",
      "loss: 1.381120  [ 2272/ 3200]\n",
      "loss: 1.389494  [ 2288/ 3200]\n",
      "loss: 1.384698  [ 2304/ 3200]\n",
      "loss: 1.390953  [ 2320/ 3200]\n",
      "loss: 1.390282  [ 2336/ 3200]\n",
      "loss: 1.383806  [ 2352/ 3200]\n",
      "loss: 1.389271  [ 2368/ 3200]\n",
      "loss: 1.386378  [ 2384/ 3200]\n",
      "loss: 1.389828  [ 2400/ 3200]\n",
      "loss: 1.386378  [ 2416/ 3200]\n",
      "loss: 1.388950  [ 2432/ 3200]\n",
      "loss: 1.377202  [ 2448/ 3200]\n",
      "loss: 1.380562  [ 2464/ 3200]\n",
      "loss: 1.387605  [ 2480/ 3200]\n",
      "loss: 1.384712  [ 2496/ 3200]\n",
      "loss: 1.383367  [ 2512/ 3200]\n",
      "loss: 1.383352  [ 2528/ 3200]\n",
      "loss: 1.385121  [ 2544/ 3200]\n",
      "loss: 1.382474  [ 2560/ 3200]\n",
      "loss: 1.384920  [ 2576/ 3200]\n",
      "loss: 1.382913  [ 2592/ 3200]\n",
      "loss: 1.387079  [ 2608/ 3200]\n",
      "loss: 1.387621  [ 2624/ 3200]\n",
      "loss: 1.391494  [ 2640/ 3200]\n",
      "loss: 1.393071  [ 2656/ 3200]\n",
      "loss: 1.386920  [ 2672/ 3200]\n",
      "loss: 1.386951  [ 2688/ 3200]\n",
      "loss: 1.390266  [ 2704/ 3200]\n",
      "loss: 1.388863  [ 2720/ 3200]\n",
      "loss: 1.389739  [ 2736/ 3200]\n",
      "loss: 1.384347  [ 2752/ 3200]\n",
      "loss: 1.388043  [ 2768/ 3200]\n",
      "loss: 1.385707  [ 2784/ 3200]\n",
      "loss: 1.385253  [ 2800/ 3200]\n",
      "loss: 1.391506  [ 2816/ 3200]\n",
      "loss: 1.387167  [ 2832/ 3200]\n",
      "loss: 1.387707  [ 2848/ 3200]\n",
      "loss: 1.391418  [ 2864/ 3200]\n",
      "loss: 1.383032  [ 2880/ 3200]\n",
      "loss: 1.389723  [ 2896/ 3200]\n",
      "loss: 1.379337  [ 2912/ 3200]\n",
      "loss: 1.388949  [ 2928/ 3200]\n",
      "loss: 1.382829  [ 2944/ 3200]\n",
      "loss: 1.388599  [ 2960/ 3200]\n",
      "loss: 1.383018  [ 2976/ 3200]\n",
      "loss: 1.378781  [ 2992/ 3200]\n",
      "loss: 1.387736  [ 3008/ 3200]\n",
      "loss: 1.388510  [ 3024/ 3200]\n",
      "loss: 1.385939  [ 3040/ 3200]\n",
      "loss: 1.389197  [ 3056/ 3200]\n",
      "loss: 1.392835  [ 3072/ 3200]\n",
      "loss: 1.390074  [ 3088/ 3200]\n",
      "loss: 1.383923  [ 3104/ 3200]\n",
      "loss: 1.382905  [ 3120/ 3200]\n",
      "loss: 1.391505  [ 3136/ 3200]\n",
      "loss: 1.381238  [ 3152/ 3200]\n",
      "loss: 1.393507  [ 3168/ 3200]\n",
      "loss: 1.382156  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.2140e-05.\n",
      "\n",
      "Best epoch: 1 with f1 macro averaged score: 0.10000000149011612\n",
      "Test Error:\n",
      "Avg loss               : 0.086704\n",
      "f1 macro averaged score: 0.095294\n",
      "Accuracy               : 23.5%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 297,   0,   0],\n",
      "        [  0, 324,   0,   0],\n",
      "        [  0, 356,   0,   0],\n",
      "        [  0, 399,   0,   0]], device='cuda:0')\n",
      "CPU times: user 10min 10s, sys: 16.7 s, total: 10min 27s\n",
      "Wall time: 10min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "weight_decay_list = [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "\n",
    "f1_accuracy_30 = validate_weight_decay(weight_decay_list, 30)\n",
    "f1_accuracy_60 = validate_weight_decay(weight_decay_list, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQb3PCy-_GxR",
    "outputId": "a35e63d9-8043-4696-81df-f67f374a422e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 epochs\n",
      "W Decay\tf1_macro_avg\tAccuracy\n",
      "0\t0.781785\t77.906977\n",
      "1e-05\t0.777850\t77.543605\n",
      "0.001\t0.775723\t77.398256\n",
      "0.0001\t0.773398\t77.034884\n",
      "0.01\t0.770161\t77.107558\n",
      "0.1\t0.153741\t28.488372\n",
      "1\t0.095294\t23.546512\n",
      "\n",
      "60 epochs\n",
      "W Decay\tf1_macro_avg\tAccuracy\n",
      "0.01\t0.783624\t78.052326\n",
      "0\t0.779698\t77.616279\n",
      "0.001\t0.777604\t77.470930\n",
      "1e-05\t0.772178\t76.816860\n",
      "0.0001\t0.764765\t76.017442\n",
      "0.1\t0.166500\t29.433140\n",
      "1\t0.095294\t23.546512\n"
     ]
    }
   ],
   "source": [
    "f1_accuracy_30 = sorted(f1_accuracy_30, key=lambda x: x[1], reverse=True)\n",
    "f1_accuracy_60 = sorted(f1_accuracy_60, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"30 epochs\")\n",
    "print(\"W Decay\\tf1_macro_avg\\tAccuracy\")\n",
    "for (weight_decay, f1_macro_avg, accuracy) in f1_accuracy_30:\n",
    "  print(f\"{weight_decay}\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")\n",
    "print()\n",
    "\n",
    "print(\"60 epochs\")\n",
    "print(\"W Decay\\tf1_macro_avg\\tAccuracy\")\n",
    "for (weight_decay, f1_macro_avg, accuracy) in f1_accuracy_60:\n",
    "  print(f\"{weight_decay}\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hM3Z0iqFLmd"
   },
   "source": [
    "When using only weight decay $0.01 = 1e-2$ for $60$ epochs, the accuracy exceeded $78\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r36cT0i0_VY6"
   },
   "source": [
    "ii) Dropout\n",
    "\n",
    "Redefine the Convolutional Neural Network, so that the dropout parameter is given as an argument during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "rhT4SNdiRgMk"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, activation_function, p):\n",
    "    super().__init__()\n",
    "    # convolutional layers\n",
    "    self.conv1_batch_norm = nn.BatchNorm2d(1)\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, padding=2)   # 1 channel   -> 16 channels\n",
    "    self.conv2_batch_norm = nn.BatchNorm2d(16)\n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2)  # 16 channels -> 32 channels\n",
    "    self.conv3_batch_norm = nn.BatchNorm2d(32)\n",
    "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)  # 32 channels -> 64 channels\n",
    "    self.conv4_batch_norm = nn.BatchNorm2d(64)\n",
    "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2) # 64 channels -> 128 channels\n",
    "\n",
    "    self.activation_function = activation_function()\n",
    "    self.dropout = nn.Dropout(p)\n",
    "\n",
    "    # fully connected (dense) layers\n",
    "    self.dense1 = nn.Linear(1024, 1024) # input size = 1024 -> number of perceptrons in 1st hidden layer = 1024\n",
    "    self.dense2 = nn.Linear(1024, 256)  # number of perceptrons in 1st hidden layer = 1024 ->\n",
    "                                        # number of perceptrons in 2nd hidden layer = 256\n",
    "    self.dense3 = nn.Linear(256, 32)    # number of perceptrons in 2nd hidden layer = 256 ->\n",
    "                                        # number of perceptrons in 3rd hidden layer = 32\n",
    "    self.dense4 = nn.Linear(32, 4)      # number of perceptrons in 3rd hidden layer = 32 ->\n",
    "                                        # number of different labels = 4\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv2(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv3(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    x = self.conv4(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    x = torch.flatten(x, 1)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "    x = self.dense1(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.dense2(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.dense3(x)\n",
    "    x = self.activation_function(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.dense4(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqXWH4cnjrGQ"
   },
   "source": [
    "The following function validates a Convolutional Neural Network for different values of dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5t2sUDo_g5L"
   },
   "outputs": [],
   "source": [
    "def validate_dropout(p_list, epochs):\n",
    "  f1_accuracy = []\n",
    "  for p in p_list:\n",
    "    torch_seed(0)\n",
    "    cnn_model = Net(ELU, p).to(device)\n",
    "    optimizer = Adagrad(params=cnn_model.parameters(), lr=learning_rate_0)\n",
    "    scheduler = MultiplicativeLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95, verbose=True)\n",
    "    print(\"  Dropout     :\", p)\n",
    "    best_model, f1_per_epoch = validate_convolutional_neural_network(\n",
    "        epochs, optimizer, train_dataloader, val_dataloader, loss_function, cnn_model, True, scheduler\n",
    "        )\n",
    "    results = test_convolutional_neural_network(test_dataloader, loss_function, best_model)\n",
    "    f1_accuracy.append((p, results[1], results[2]))\n",
    "  return f1_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQu7ee82kD3P"
   },
   "source": [
    "We validate our Convolutional Neural Network for $30$ and $60$ epochs and test it.\n",
    "\n",
    "Our model uses:\n",
    "+ the Adagrad optimizer\n",
    "+ the ELU activation function\n",
    "+ the MultiplicativeLR scheduler\n",
    "+ batch normalization\n",
    "\n",
    "as stated in the previous steps.\n",
    "\n",
    "The dropout values used are the following: $0.1$, $0.2$ and $0.3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lSxjQnLw_qI-",
    "outputId": "6bbd948c-fe29-4805-96a1-ee4565ed50f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 0.021861  [ 1952/ 3200]\n",
      "loss: 0.084081  [ 1968/ 3200]\n",
      "loss: 0.036613  [ 1984/ 3200]\n",
      "loss: 0.170455  [ 2000/ 3200]\n",
      "loss: 0.031585  [ 2016/ 3200]\n",
      "loss: 0.085423  [ 2032/ 3200]\n",
      "loss: 0.019172  [ 2048/ 3200]\n",
      "loss: 0.085060  [ 2064/ 3200]\n",
      "loss: 0.097388  [ 2080/ 3200]\n",
      "loss: 0.038620  [ 2096/ 3200]\n",
      "loss: 0.058712  [ 2112/ 3200]\n",
      "loss: 0.051209  [ 2128/ 3200]\n",
      "loss: 0.046714  [ 2144/ 3200]\n",
      "loss: 0.009988  [ 2160/ 3200]\n",
      "loss: 0.062783  [ 2176/ 3200]\n",
      "loss: 0.072234  [ 2192/ 3200]\n",
      "loss: 0.018341  [ 2208/ 3200]\n",
      "loss: 0.058367  [ 2224/ 3200]\n",
      "loss: 0.245929  [ 2240/ 3200]\n",
      "loss: 0.042253  [ 2256/ 3200]\n",
      "loss: 0.038929  [ 2272/ 3200]\n",
      "loss: 0.086000  [ 2288/ 3200]\n",
      "loss: 0.025298  [ 2304/ 3200]\n",
      "loss: 0.019068  [ 2320/ 3200]\n",
      "loss: 0.105643  [ 2336/ 3200]\n",
      "loss: 0.198647  [ 2352/ 3200]\n",
      "loss: 0.051258  [ 2368/ 3200]\n",
      "loss: 0.077262  [ 2384/ 3200]\n",
      "loss: 0.055280  [ 2400/ 3200]\n",
      "loss: 0.023678  [ 2416/ 3200]\n",
      "loss: 0.513010  [ 2432/ 3200]\n",
      "loss: 0.048682  [ 2448/ 3200]\n",
      "loss: 0.092624  [ 2464/ 3200]\n",
      "loss: 0.018532  [ 2480/ 3200]\n",
      "loss: 0.067519  [ 2496/ 3200]\n",
      "loss: 0.034395  [ 2512/ 3200]\n",
      "loss: 0.120653  [ 2528/ 3200]\n",
      "loss: 0.112482  [ 2544/ 3200]\n",
      "loss: 0.011823  [ 2560/ 3200]\n",
      "loss: 0.061002  [ 2576/ 3200]\n",
      "loss: 0.069394  [ 2592/ 3200]\n",
      "loss: 0.039477  [ 2608/ 3200]\n",
      "loss: 0.025620  [ 2624/ 3200]\n",
      "loss: 0.105295  [ 2640/ 3200]\n",
      "loss: 0.063160  [ 2656/ 3200]\n",
      "loss: 0.094623  [ 2672/ 3200]\n",
      "loss: 0.041271  [ 2688/ 3200]\n",
      "loss: 0.027302  [ 2704/ 3200]\n",
      "loss: 0.072042  [ 2720/ 3200]\n",
      "loss: 0.112702  [ 2736/ 3200]\n",
      "loss: 0.263459  [ 2752/ 3200]\n",
      "loss: 0.012693  [ 2768/ 3200]\n",
      "loss: 0.031173  [ 2784/ 3200]\n",
      "loss: 0.074596  [ 2800/ 3200]\n",
      "loss: 0.042468  [ 2816/ 3200]\n",
      "loss: 0.145244  [ 2832/ 3200]\n",
      "loss: 0.037268  [ 2848/ 3200]\n",
      "loss: 0.023506  [ 2864/ 3200]\n",
      "loss: 0.060712  [ 2880/ 3200]\n",
      "loss: 0.120381  [ 2896/ 3200]\n",
      "loss: 0.068821  [ 2912/ 3200]\n",
      "loss: 0.147324  [ 2928/ 3200]\n",
      "loss: 0.068123  [ 2944/ 3200]\n",
      "loss: 0.037319  [ 2960/ 3200]\n",
      "loss: 0.039910  [ 2976/ 3200]\n",
      "loss: 0.084042  [ 2992/ 3200]\n",
      "loss: 0.253931  [ 3008/ 3200]\n",
      "loss: 0.011796  [ 3024/ 3200]\n",
      "loss: 0.157531  [ 3040/ 3200]\n",
      "loss: 0.069109  [ 3056/ 3200]\n",
      "loss: 0.053893  [ 3072/ 3200]\n",
      "loss: 0.079721  [ 3088/ 3200]\n",
      "loss: 0.048546  [ 3104/ 3200]\n",
      "loss: 0.045799  [ 3120/ 3200]\n",
      "loss: 0.088555  [ 3136/ 3200]\n",
      "loss: 0.017773  [ 3152/ 3200]\n",
      "loss: 0.042620  [ 3168/ 3200]\n",
      "loss: 0.097128  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.048659\n",
      "f1 macro averaged score: 0.766811\n",
      "Accuracy               : 76.6%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  11,   0,  17],\n",
      "        [ 15, 126,  15,  44],\n",
      "        [  0,  26, 155,  19],\n",
      "        [  8,  19,  13, 160]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.9978e-04.\n",
      "\n",
      "Epoch: 38\n",
      "-----------------------------\n",
      "loss: 0.032249  [    0/ 3200]\n",
      "loss: 0.239280  [   16/ 3200]\n",
      "loss: 0.020971  [   32/ 3200]\n",
      "loss: 0.070074  [   48/ 3200]\n",
      "loss: 0.058382  [   64/ 3200]\n",
      "loss: 0.015739  [   80/ 3200]\n",
      "loss: 0.020669  [   96/ 3200]\n",
      "loss: 0.074204  [  112/ 3200]\n",
      "loss: 0.026381  [  128/ 3200]\n",
      "loss: 0.074551  [  144/ 3200]\n",
      "loss: 0.019919  [  160/ 3200]\n",
      "loss: 0.042344  [  176/ 3200]\n",
      "loss: 0.091975  [  192/ 3200]\n",
      "loss: 0.133801  [  208/ 3200]\n",
      "loss: 0.058180  [  224/ 3200]\n",
      "loss: 0.060004  [  240/ 3200]\n",
      "loss: 0.066256  [  256/ 3200]\n",
      "loss: 0.057752  [  272/ 3200]\n",
      "loss: 0.279128  [  288/ 3200]\n",
      "loss: 0.136947  [  304/ 3200]\n",
      "loss: 0.029974  [  320/ 3200]\n",
      "loss: 0.109310  [  336/ 3200]\n",
      "loss: 0.081035  [  352/ 3200]\n",
      "loss: 0.108422  [  368/ 3200]\n",
      "loss: 0.029884  [  384/ 3200]\n",
      "loss: 0.012502  [  400/ 3200]\n",
      "loss: 0.316616  [  416/ 3200]\n",
      "loss: 0.018152  [  432/ 3200]\n",
      "loss: 0.017758  [  448/ 3200]\n",
      "loss: 0.064753  [  464/ 3200]\n",
      "loss: 0.045446  [  480/ 3200]\n",
      "loss: 0.111905  [  496/ 3200]\n",
      "loss: 0.133474  [  512/ 3200]\n",
      "loss: 0.036923  [  528/ 3200]\n",
      "loss: 0.015592  [  544/ 3200]\n",
      "loss: 0.044076  [  560/ 3200]\n",
      "loss: 0.246843  [  576/ 3200]\n",
      "loss: 0.062587  [  592/ 3200]\n",
      "loss: 0.036864  [  608/ 3200]\n",
      "loss: 0.090483  [  624/ 3200]\n",
      "loss: 0.050274  [  640/ 3200]\n",
      "loss: 0.007390  [  656/ 3200]\n",
      "loss: 0.047250  [  672/ 3200]\n",
      "loss: 0.009814  [  688/ 3200]\n",
      "loss: 0.047065  [  704/ 3200]\n",
      "loss: 0.041340  [  720/ 3200]\n",
      "loss: 0.091310  [  736/ 3200]\n",
      "loss: 0.075261  [  752/ 3200]\n",
      "loss: 0.074943  [  768/ 3200]\n",
      "loss: 0.017914  [  784/ 3200]\n",
      "loss: 0.044619  [  800/ 3200]\n",
      "loss: 0.027848  [  816/ 3200]\n",
      "loss: 0.084499  [  832/ 3200]\n",
      "loss: 0.191046  [  848/ 3200]\n",
      "loss: 0.030822  [  864/ 3200]\n",
      "loss: 0.051044  [  880/ 3200]\n",
      "loss: 0.041695  [  896/ 3200]\n",
      "loss: 0.046163  [  912/ 3200]\n",
      "loss: 0.083955  [  928/ 3200]\n",
      "loss: 0.033601  [  944/ 3200]\n",
      "loss: 0.035824  [  960/ 3200]\n",
      "loss: 0.090649  [  976/ 3200]\n",
      "loss: 0.152069  [  992/ 3200]\n",
      "loss: 0.062727  [ 1008/ 3200]\n",
      "loss: 0.045817  [ 1024/ 3200]\n",
      "loss: 0.138939  [ 1040/ 3200]\n",
      "loss: 0.039263  [ 1056/ 3200]\n",
      "loss: 0.057711  [ 1072/ 3200]\n",
      "loss: 0.057077  [ 1088/ 3200]\n",
      "loss: 0.038351  [ 1104/ 3200]\n",
      "loss: 0.015379  [ 1120/ 3200]\n",
      "loss: 0.032986  [ 1136/ 3200]\n",
      "loss: 0.079254  [ 1152/ 3200]\n",
      "loss: 0.035900  [ 1168/ 3200]\n",
      "loss: 0.057072  [ 1184/ 3200]\n",
      "loss: 0.063946  [ 1200/ 3200]\n",
      "loss: 0.032432  [ 1216/ 3200]\n",
      "loss: 0.042833  [ 1232/ 3200]\n",
      "loss: 0.084220  [ 1248/ 3200]\n",
      "loss: 0.101170  [ 1264/ 3200]\n",
      "loss: 0.060343  [ 1280/ 3200]\n",
      "loss: 0.065683  [ 1296/ 3200]\n",
      "loss: 0.036955  [ 1312/ 3200]\n",
      "loss: 0.135825  [ 1328/ 3200]\n",
      "loss: 0.054696  [ 1344/ 3200]\n",
      "loss: 0.069226  [ 1360/ 3200]\n",
      "loss: 0.057082  [ 1376/ 3200]\n",
      "loss: 0.016538  [ 1392/ 3200]\n",
      "loss: 0.102761  [ 1408/ 3200]\n",
      "loss: 0.045925  [ 1424/ 3200]\n",
      "loss: 0.087817  [ 1440/ 3200]\n",
      "loss: 0.034127  [ 1456/ 3200]\n",
      "loss: 0.164314  [ 1472/ 3200]\n",
      "loss: 0.031601  [ 1488/ 3200]\n",
      "loss: 0.007992  [ 1504/ 3200]\n",
      "loss: 0.065326  [ 1520/ 3200]\n",
      "loss: 0.019918  [ 1536/ 3200]\n",
      "loss: 0.089616  [ 1552/ 3200]\n",
      "loss: 0.028810  [ 1568/ 3200]\n",
      "loss: 0.064488  [ 1584/ 3200]\n",
      "loss: 0.029721  [ 1600/ 3200]\n",
      "loss: 0.095169  [ 1616/ 3200]\n",
      "loss: 0.233399  [ 1632/ 3200]\n",
      "loss: 0.017060  [ 1648/ 3200]\n",
      "loss: 0.058655  [ 1664/ 3200]\n",
      "loss: 0.125566  [ 1680/ 3200]\n",
      "loss: 0.023744  [ 1696/ 3200]\n",
      "loss: 0.127325  [ 1712/ 3200]\n",
      "loss: 0.051011  [ 1728/ 3200]\n",
      "loss: 0.120458  [ 1744/ 3200]\n",
      "loss: 0.070808  [ 1760/ 3200]\n",
      "loss: 0.034867  [ 1776/ 3200]\n",
      "loss: 0.023739  [ 1792/ 3200]\n",
      "loss: 0.077212  [ 1808/ 3200]\n",
      "loss: 0.024812  [ 1824/ 3200]\n",
      "loss: 0.138738  [ 1840/ 3200]\n",
      "loss: 0.199264  [ 1856/ 3200]\n",
      "loss: 0.030489  [ 1872/ 3200]\n",
      "loss: 0.046970  [ 1888/ 3200]\n",
      "loss: 0.074795  [ 1904/ 3200]\n",
      "loss: 0.029549  [ 1920/ 3200]\n",
      "loss: 0.104548  [ 1936/ 3200]\n",
      "loss: 0.062709  [ 1952/ 3200]\n",
      "loss: 0.064962  [ 1968/ 3200]\n",
      "loss: 0.210462  [ 1984/ 3200]\n",
      "loss: 0.040841  [ 2000/ 3200]\n",
      "loss: 0.171386  [ 2016/ 3200]\n",
      "loss: 0.019738  [ 2032/ 3200]\n",
      "loss: 0.160606  [ 2048/ 3200]\n",
      "loss: 0.039241  [ 2064/ 3200]\n",
      "loss: 0.056239  [ 2080/ 3200]\n",
      "loss: 0.087852  [ 2096/ 3200]\n",
      "loss: 0.071291  [ 2112/ 3200]\n",
      "loss: 0.061664  [ 2128/ 3200]\n",
      "loss: 0.081579  [ 2144/ 3200]\n",
      "loss: 0.106160  [ 2160/ 3200]\n",
      "loss: 0.029166  [ 2176/ 3200]\n",
      "loss: 0.054978  [ 2192/ 3200]\n",
      "loss: 0.051474  [ 2208/ 3200]\n",
      "loss: 0.110409  [ 2224/ 3200]\n",
      "loss: 0.034557  [ 2240/ 3200]\n",
      "loss: 0.117578  [ 2256/ 3200]\n",
      "loss: 0.010100  [ 2272/ 3200]\n",
      "loss: 0.039291  [ 2288/ 3200]\n",
      "loss: 0.018412  [ 2304/ 3200]\n",
      "loss: 0.016293  [ 2320/ 3200]\n",
      "loss: 0.128280  [ 2336/ 3200]\n",
      "loss: 0.050280  [ 2352/ 3200]\n",
      "loss: 0.035979  [ 2368/ 3200]\n",
      "loss: 0.056749  [ 2384/ 3200]\n",
      "loss: 0.028469  [ 2400/ 3200]\n",
      "loss: 0.047202  [ 2416/ 3200]\n",
      "loss: 0.034674  [ 2432/ 3200]\n",
      "loss: 0.050284  [ 2448/ 3200]\n",
      "loss: 0.112928  [ 2464/ 3200]\n",
      "loss: 0.264468  [ 2480/ 3200]\n",
      "loss: 0.092514  [ 2496/ 3200]\n",
      "loss: 0.025579  [ 2512/ 3200]\n",
      "loss: 0.035276  [ 2528/ 3200]\n",
      "loss: 0.109414  [ 2544/ 3200]\n",
      "loss: 0.200886  [ 2560/ 3200]\n",
      "loss: 0.046840  [ 2576/ 3200]\n",
      "loss: 0.084083  [ 2592/ 3200]\n",
      "loss: 0.094676  [ 2608/ 3200]\n",
      "loss: 0.038160  [ 2624/ 3200]\n",
      "loss: 0.025934  [ 2640/ 3200]\n",
      "loss: 0.116558  [ 2656/ 3200]\n",
      "loss: 0.037371  [ 2672/ 3200]\n",
      "loss: 0.119316  [ 2688/ 3200]\n",
      "loss: 0.090489  [ 2704/ 3200]\n",
      "loss: 0.099476  [ 2720/ 3200]\n",
      "loss: 0.083036  [ 2736/ 3200]\n",
      "loss: 0.077078  [ 2752/ 3200]\n",
      "loss: 0.078491  [ 2768/ 3200]\n",
      "loss: 0.154669  [ 2784/ 3200]\n",
      "loss: 0.042572  [ 2800/ 3200]\n",
      "loss: 0.012664  [ 2816/ 3200]\n",
      "loss: 0.025746  [ 2832/ 3200]\n",
      "loss: 0.131859  [ 2848/ 3200]\n",
      "loss: 0.064062  [ 2864/ 3200]\n",
      "loss: 0.032480  [ 2880/ 3200]\n",
      "loss: 0.073981  [ 2896/ 3200]\n",
      "loss: 0.067687  [ 2912/ 3200]\n",
      "loss: 0.105192  [ 2928/ 3200]\n",
      "loss: 0.023585  [ 2944/ 3200]\n",
      "loss: 0.080703  [ 2960/ 3200]\n",
      "loss: 0.013718  [ 2976/ 3200]\n",
      "loss: 0.066771  [ 2992/ 3200]\n",
      "loss: 0.050117  [ 3008/ 3200]\n",
      "loss: 0.065637  [ 3024/ 3200]\n",
      "loss: 0.033515  [ 3040/ 3200]\n",
      "loss: 0.047396  [ 3056/ 3200]\n",
      "loss: 0.118844  [ 3072/ 3200]\n",
      "loss: 0.025092  [ 3088/ 3200]\n",
      "loss: 0.042168  [ 3104/ 3200]\n",
      "loss: 0.136930  [ 3120/ 3200]\n",
      "loss: 0.076799  [ 3136/ 3200]\n",
      "loss: 0.008883  [ 3152/ 3200]\n",
      "loss: 0.048405  [ 3168/ 3200]\n",
      "loss: 0.042010  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.045921\n",
      "f1 macro averaged score: 0.782886\n",
      "Accuracy               : 78.5%\n",
      "Confusion matrix       :\n",
      "tensor([[180,   7,   0,  13],\n",
      "        [ 16, 122,  21,  41],\n",
      "        [  0,  23, 163,  14],\n",
      "        [  9,  16,  12, 163]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.8479e-04.\n",
      "\n",
      "Epoch: 39\n",
      "-----------------------------\n",
      "loss: 0.030413  [    0/ 3200]\n",
      "loss: 0.035147  [   16/ 3200]\n",
      "loss: 0.053992  [   32/ 3200]\n",
      "loss: 0.055235  [   48/ 3200]\n",
      "loss: 0.007277  [   64/ 3200]\n",
      "loss: 0.027146  [   80/ 3200]\n",
      "loss: 0.067367  [   96/ 3200]\n",
      "loss: 0.036158  [  112/ 3200]\n",
      "loss: 0.064394  [  128/ 3200]\n",
      "loss: 0.051798  [  144/ 3200]\n",
      "loss: 0.015981  [  160/ 3200]\n",
      "loss: 0.081601  [  176/ 3200]\n",
      "loss: 0.046632  [  192/ 3200]\n",
      "loss: 0.041085  [  208/ 3200]\n",
      "loss: 0.031074  [  224/ 3200]\n",
      "loss: 0.031193  [  240/ 3200]\n",
      "loss: 0.079936  [  256/ 3200]\n",
      "loss: 0.041012  [  272/ 3200]\n",
      "loss: 0.172548  [  288/ 3200]\n",
      "loss: 0.046867  [  304/ 3200]\n",
      "loss: 0.104063  [  320/ 3200]\n",
      "loss: 0.079817  [  336/ 3200]\n",
      "loss: 0.032404  [  352/ 3200]\n",
      "loss: 0.101214  [  368/ 3200]\n",
      "loss: 0.255579  [  384/ 3200]\n",
      "loss: 0.109266  [  400/ 3200]\n",
      "loss: 0.011632  [  416/ 3200]\n",
      "loss: 0.055853  [  432/ 3200]\n",
      "loss: 0.114245  [  448/ 3200]\n",
      "loss: 0.042755  [  464/ 3200]\n",
      "loss: 0.047555  [  480/ 3200]\n",
      "loss: 0.087595  [  496/ 3200]\n",
      "loss: 0.038368  [  512/ 3200]\n",
      "loss: 0.207148  [  528/ 3200]\n",
      "loss: 0.045580  [  544/ 3200]\n",
      "loss: 0.074347  [  560/ 3200]\n",
      "loss: 0.141443  [  576/ 3200]\n",
      "loss: 0.076058  [  592/ 3200]\n",
      "loss: 0.045067  [  608/ 3200]\n",
      "loss: 0.195115  [  624/ 3200]\n",
      "loss: 0.167303  [  640/ 3200]\n",
      "loss: 0.039005  [  656/ 3200]\n",
      "loss: 0.016767  [  672/ 3200]\n",
      "loss: 0.037936  [  688/ 3200]\n",
      "loss: 0.045888  [  704/ 3200]\n",
      "loss: 0.049483  [  720/ 3200]\n",
      "loss: 0.059307  [  736/ 3200]\n",
      "loss: 0.066687  [  752/ 3200]\n",
      "loss: 0.227698  [  768/ 3200]\n",
      "loss: 0.009868  [  784/ 3200]\n",
      "loss: 0.080851  [  800/ 3200]\n",
      "loss: 0.042155  [  816/ 3200]\n",
      "loss: 0.036105  [  832/ 3200]\n",
      "loss: 0.084642  [  848/ 3200]\n",
      "loss: 0.107437  [  864/ 3200]\n",
      "loss: 0.024836  [  880/ 3200]\n",
      "loss: 0.056143  [  896/ 3200]\n",
      "loss: 0.151857  [  912/ 3200]\n",
      "loss: 0.104820  [  928/ 3200]\n",
      "loss: 0.037673  [  944/ 3200]\n",
      "loss: 0.039838  [  960/ 3200]\n",
      "loss: 0.015562  [  976/ 3200]\n",
      "loss: 0.036473  [  992/ 3200]\n",
      "loss: 0.268919  [ 1008/ 3200]\n",
      "loss: 0.089282  [ 1024/ 3200]\n",
      "loss: 0.041882  [ 1040/ 3200]\n",
      "loss: 0.046241  [ 1056/ 3200]\n",
      "loss: 0.006802  [ 1072/ 3200]\n",
      "loss: 0.056062  [ 1088/ 3200]\n",
      "loss: 0.074440  [ 1104/ 3200]\n",
      "loss: 0.214167  [ 1120/ 3200]\n",
      "loss: 0.013538  [ 1136/ 3200]\n",
      "loss: 0.074407  [ 1152/ 3200]\n",
      "loss: 0.067434  [ 1168/ 3200]\n",
      "loss: 0.056923  [ 1184/ 3200]\n",
      "loss: 0.112700  [ 1200/ 3200]\n",
      "loss: 0.297700  [ 1216/ 3200]\n",
      "loss: 0.011632  [ 1232/ 3200]\n",
      "loss: 0.020108  [ 1248/ 3200]\n",
      "loss: 0.120297  [ 1264/ 3200]\n",
      "loss: 0.099689  [ 1280/ 3200]\n",
      "loss: 0.185409  [ 1296/ 3200]\n",
      "loss: 0.084264  [ 1312/ 3200]\n",
      "loss: 0.063163  [ 1328/ 3200]\n",
      "loss: 0.096236  [ 1344/ 3200]\n",
      "loss: 0.030537  [ 1360/ 3200]\n",
      "loss: 0.042091  [ 1376/ 3200]\n",
      "loss: 0.022405  [ 1392/ 3200]\n",
      "loss: 0.116866  [ 1408/ 3200]\n",
      "loss: 0.037906  [ 1424/ 3200]\n",
      "loss: 0.196014  [ 1440/ 3200]\n",
      "loss: 0.045788  [ 1456/ 3200]\n",
      "loss: 0.015811  [ 1472/ 3200]\n",
      "loss: 0.016024  [ 1488/ 3200]\n",
      "loss: 0.077360  [ 1504/ 3200]\n",
      "loss: 0.144606  [ 1520/ 3200]\n",
      "loss: 0.019228  [ 1536/ 3200]\n",
      "loss: 0.099218  [ 1552/ 3200]\n",
      "loss: 0.037786  [ 1568/ 3200]\n",
      "loss: 0.042265  [ 1584/ 3200]\n",
      "loss: 0.164048  [ 1600/ 3200]\n",
      "loss: 0.041666  [ 1616/ 3200]\n",
      "loss: 0.042722  [ 1632/ 3200]\n",
      "loss: 0.056502  [ 1648/ 3200]\n",
      "loss: 0.065238  [ 1664/ 3200]\n",
      "loss: 0.063220  [ 1680/ 3200]\n",
      "loss: 0.033872  [ 1696/ 3200]\n",
      "loss: 0.059764  [ 1712/ 3200]\n",
      "loss: 0.051908  [ 1728/ 3200]\n",
      "loss: 0.059620  [ 1744/ 3200]\n",
      "loss: 0.038211  [ 1760/ 3200]\n",
      "loss: 0.047960  [ 1776/ 3200]\n",
      "loss: 0.073752  [ 1792/ 3200]\n",
      "loss: 0.111952  [ 1808/ 3200]\n",
      "loss: 0.113514  [ 1824/ 3200]\n",
      "loss: 0.098685  [ 1840/ 3200]\n",
      "loss: 0.098590  [ 1856/ 3200]\n",
      "loss: 0.025905  [ 1872/ 3200]\n",
      "loss: 0.081581  [ 1888/ 3200]\n",
      "loss: 0.032470  [ 1904/ 3200]\n",
      "loss: 0.032394  [ 1920/ 3200]\n",
      "loss: 0.090318  [ 1936/ 3200]\n",
      "loss: 0.237697  [ 1952/ 3200]\n",
      "loss: 0.054112  [ 1968/ 3200]\n",
      "loss: 0.033405  [ 1984/ 3200]\n",
      "loss: 0.023568  [ 2000/ 3200]\n",
      "loss: 0.032361  [ 2016/ 3200]\n",
      "loss: 0.025248  [ 2032/ 3200]\n",
      "loss: 0.078052  [ 2048/ 3200]\n",
      "loss: 0.073583  [ 2064/ 3200]\n",
      "loss: 0.045067  [ 2080/ 3200]\n",
      "loss: 0.018565  [ 2096/ 3200]\n",
      "loss: 0.087819  [ 2112/ 3200]\n",
      "loss: 0.040217  [ 2128/ 3200]\n",
      "loss: 0.174787  [ 2144/ 3200]\n",
      "loss: 0.142486  [ 2160/ 3200]\n",
      "loss: 0.035248  [ 2176/ 3200]\n",
      "loss: 0.364518  [ 2192/ 3200]\n",
      "loss: 0.008858  [ 2208/ 3200]\n",
      "loss: 0.111851  [ 2224/ 3200]\n",
      "loss: 0.075471  [ 2240/ 3200]\n",
      "loss: 0.021550  [ 2256/ 3200]\n",
      "loss: 0.045278  [ 2272/ 3200]\n",
      "loss: 0.069845  [ 2288/ 3200]\n",
      "loss: 0.069068  [ 2304/ 3200]\n",
      "loss: 0.051874  [ 2320/ 3200]\n",
      "loss: 0.038211  [ 2336/ 3200]\n",
      "loss: 0.418842  [ 2352/ 3200]\n",
      "loss: 0.081534  [ 2368/ 3200]\n",
      "loss: 0.091655  [ 2384/ 3200]\n",
      "loss: 0.039747  [ 2400/ 3200]\n",
      "loss: 0.046483  [ 2416/ 3200]\n",
      "loss: 0.053577  [ 2432/ 3200]\n",
      "loss: 0.043231  [ 2448/ 3200]\n",
      "loss: 0.058223  [ 2464/ 3200]\n",
      "loss: 0.051874  [ 2480/ 3200]\n",
      "loss: 0.036378  [ 2496/ 3200]\n",
      "loss: 0.017846  [ 2512/ 3200]\n",
      "loss: 0.089708  [ 2528/ 3200]\n",
      "loss: 0.035800  [ 2544/ 3200]\n",
      "loss: 0.031155  [ 2560/ 3200]\n",
      "loss: 0.081205  [ 2576/ 3200]\n",
      "loss: 0.079836  [ 2592/ 3200]\n",
      "loss: 0.049064  [ 2608/ 3200]\n",
      "loss: 0.045866  [ 2624/ 3200]\n",
      "loss: 0.053325  [ 2640/ 3200]\n",
      "loss: 0.232040  [ 2656/ 3200]\n",
      "loss: 0.068042  [ 2672/ 3200]\n",
      "loss: 0.072259  [ 2688/ 3200]\n",
      "loss: 0.026777  [ 2704/ 3200]\n",
      "loss: 0.103397  [ 2720/ 3200]\n",
      "loss: 0.044737  [ 2736/ 3200]\n",
      "loss: 0.043249  [ 2752/ 3200]\n",
      "loss: 0.040098  [ 2768/ 3200]\n",
      "loss: 0.064857  [ 2784/ 3200]\n",
      "loss: 0.067393  [ 2800/ 3200]\n",
      "loss: 0.049885  [ 2816/ 3200]\n",
      "loss: 0.029008  [ 2832/ 3200]\n",
      "loss: 0.084318  [ 2848/ 3200]\n",
      "loss: 0.026147  [ 2864/ 3200]\n",
      "loss: 0.105707  [ 2880/ 3200]\n",
      "loss: 0.080427  [ 2896/ 3200]\n",
      "loss: 0.205689  [ 2912/ 3200]\n",
      "loss: 0.084465  [ 2928/ 3200]\n",
      "loss: 0.076292  [ 2944/ 3200]\n",
      "loss: 0.026604  [ 2960/ 3200]\n",
      "loss: 0.041262  [ 2976/ 3200]\n",
      "loss: 0.029958  [ 2992/ 3200]\n",
      "loss: 0.026524  [ 3008/ 3200]\n",
      "loss: 0.141595  [ 3024/ 3200]\n",
      "loss: 0.056576  [ 3040/ 3200]\n",
      "loss: 0.124671  [ 3056/ 3200]\n",
      "loss: 0.168390  [ 3072/ 3200]\n",
      "loss: 0.100695  [ 3088/ 3200]\n",
      "loss: 0.104235  [ 3104/ 3200]\n",
      "loss: 0.037265  [ 3120/ 3200]\n",
      "loss: 0.040709  [ 3136/ 3200]\n",
      "loss: 0.183600  [ 3152/ 3200]\n",
      "loss: 0.084205  [ 3168/ 3200]\n",
      "loss: 0.083795  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.048667\n",
      "f1 macro averaged score: 0.787733\n",
      "Accuracy               : 79.0%\n",
      "Confusion matrix       :\n",
      "tensor([[177,  10,   1,  12],\n",
      "        [ 18, 119,  15,  48],\n",
      "        [  0,  15, 163,  22],\n",
      "        [  7,   8,  12, 173]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.7055e-04.\n",
      "\n",
      "Epoch: 40\n",
      "-----------------------------\n",
      "loss: 0.053511  [    0/ 3200]\n",
      "loss: 0.063756  [   16/ 3200]\n",
      "loss: 0.046595  [   32/ 3200]\n",
      "loss: 0.009591  [   48/ 3200]\n",
      "loss: 0.094923  [   64/ 3200]\n",
      "loss: 0.013185  [   80/ 3200]\n",
      "loss: 0.019817  [   96/ 3200]\n",
      "loss: 0.023105  [  112/ 3200]\n",
      "loss: 0.097950  [  128/ 3200]\n",
      "loss: 0.189998  [  144/ 3200]\n",
      "loss: 0.033280  [  160/ 3200]\n",
      "loss: 0.053853  [  176/ 3200]\n",
      "loss: 0.034143  [  192/ 3200]\n",
      "loss: 0.054214  [  208/ 3200]\n",
      "loss: 0.087473  [  224/ 3200]\n",
      "loss: 0.031173  [  240/ 3200]\n",
      "loss: 0.017557  [  256/ 3200]\n",
      "loss: 0.104245  [  272/ 3200]\n",
      "loss: 0.079700  [  288/ 3200]\n",
      "loss: 0.032545  [  304/ 3200]\n",
      "loss: 0.090683  [  320/ 3200]\n",
      "loss: 0.079783  [  336/ 3200]\n",
      "loss: 0.061496  [  352/ 3200]\n",
      "loss: 0.041359  [  368/ 3200]\n",
      "loss: 0.045983  [  384/ 3200]\n",
      "loss: 0.079780  [  400/ 3200]\n",
      "loss: 0.032317  [  416/ 3200]\n",
      "loss: 0.078649  [  432/ 3200]\n",
      "loss: 0.174509  [  448/ 3200]\n",
      "loss: 0.304728  [  464/ 3200]\n",
      "loss: 0.045478  [  480/ 3200]\n",
      "loss: 0.036757  [  496/ 3200]\n",
      "loss: 0.079220  [  512/ 3200]\n",
      "loss: 0.380993  [  528/ 3200]\n",
      "loss: 0.129657  [  544/ 3200]\n",
      "loss: 0.066897  [  560/ 3200]\n",
      "loss: 0.034258  [  576/ 3200]\n",
      "loss: 0.099037  [  592/ 3200]\n",
      "loss: 0.019904  [  608/ 3200]\n",
      "loss: 0.055378  [  624/ 3200]\n",
      "loss: 0.040900  [  640/ 3200]\n",
      "loss: 0.114581  [  656/ 3200]\n",
      "loss: 0.136147  [  672/ 3200]\n",
      "loss: 0.023070  [  688/ 3200]\n",
      "loss: 0.048159  [  704/ 3200]\n",
      "loss: 0.034424  [  720/ 3200]\n",
      "loss: 0.121546  [  736/ 3200]\n",
      "loss: 0.083599  [  752/ 3200]\n",
      "loss: 0.046373  [  768/ 3200]\n",
      "loss: 0.029715  [  784/ 3200]\n",
      "loss: 0.133180  [  800/ 3200]\n",
      "loss: 0.070583  [  816/ 3200]\n",
      "loss: 0.046153  [  832/ 3200]\n",
      "loss: 0.031370  [  848/ 3200]\n",
      "loss: 0.215202  [  864/ 3200]\n",
      "loss: 0.082466  [  880/ 3200]\n",
      "loss: 0.109967  [  896/ 3200]\n",
      "loss: 0.058703  [  912/ 3200]\n",
      "loss: 0.084363  [  928/ 3200]\n",
      "loss: 0.061055  [  944/ 3200]\n",
      "loss: 0.109758  [  960/ 3200]\n",
      "loss: 0.018310  [  976/ 3200]\n",
      "loss: 0.026312  [  992/ 3200]\n",
      "loss: 0.062797  [ 1008/ 3200]\n",
      "loss: 0.032376  [ 1024/ 3200]\n",
      "loss: 0.008377  [ 1040/ 3200]\n",
      "loss: 0.085023  [ 1056/ 3200]\n",
      "loss: 0.136885  [ 1072/ 3200]\n",
      "loss: 0.062703  [ 1088/ 3200]\n",
      "loss: 0.018849  [ 1104/ 3200]\n",
      "loss: 0.013074  [ 1120/ 3200]\n",
      "loss: 0.082157  [ 1136/ 3200]\n",
      "loss: 0.040307  [ 1152/ 3200]\n",
      "loss: 0.073319  [ 1168/ 3200]\n",
      "loss: 0.011076  [ 1184/ 3200]\n",
      "loss: 0.059130  [ 1200/ 3200]\n",
      "loss: 0.103653  [ 1216/ 3200]\n",
      "loss: 0.037213  [ 1232/ 3200]\n",
      "loss: 0.059424  [ 1248/ 3200]\n",
      "loss: 0.143516  [ 1264/ 3200]\n",
      "loss: 0.049090  [ 1280/ 3200]\n",
      "loss: 0.029857  [ 1296/ 3200]\n",
      "loss: 0.047952  [ 1312/ 3200]\n",
      "loss: 0.027464  [ 1328/ 3200]\n",
      "loss: 0.063903  [ 1344/ 3200]\n",
      "loss: 0.014007  [ 1360/ 3200]\n",
      "loss: 0.068032  [ 1376/ 3200]\n",
      "loss: 0.035936  [ 1392/ 3200]\n",
      "loss: 0.140048  [ 1408/ 3200]\n",
      "loss: 0.018047  [ 1424/ 3200]\n",
      "loss: 0.082457  [ 1440/ 3200]\n",
      "loss: 0.034412  [ 1456/ 3200]\n",
      "loss: 0.143867  [ 1472/ 3200]\n",
      "loss: 0.040624  [ 1488/ 3200]\n",
      "loss: 0.035061  [ 1504/ 3200]\n",
      "loss: 0.013767  [ 1520/ 3200]\n",
      "loss: 0.140168  [ 1536/ 3200]\n",
      "loss: 0.028666  [ 1552/ 3200]\n",
      "loss: 0.021380  [ 1568/ 3200]\n",
      "loss: 0.023076  [ 1584/ 3200]\n",
      "loss: 0.080761  [ 1600/ 3200]\n",
      "loss: 0.025889  [ 1616/ 3200]\n",
      "loss: 0.025991  [ 1632/ 3200]\n",
      "loss: 0.050170  [ 1648/ 3200]\n",
      "loss: 0.028690  [ 1664/ 3200]\n",
      "loss: 0.161649  [ 1680/ 3200]\n",
      "loss: 0.059779  [ 1696/ 3200]\n",
      "loss: 0.030699  [ 1712/ 3200]\n",
      "loss: 0.019804  [ 1728/ 3200]\n",
      "loss: 0.073769  [ 1744/ 3200]\n",
      "loss: 0.025396  [ 1760/ 3200]\n",
      "loss: 0.132970  [ 1776/ 3200]\n",
      "loss: 0.022993  [ 1792/ 3200]\n",
      "loss: 0.040113  [ 1808/ 3200]\n",
      "loss: 0.026425  [ 1824/ 3200]\n",
      "loss: 0.025066  [ 1840/ 3200]\n",
      "loss: 0.083317  [ 1856/ 3200]\n",
      "loss: 0.042197  [ 1872/ 3200]\n",
      "loss: 0.025891  [ 1888/ 3200]\n",
      "loss: 0.028361  [ 1904/ 3200]\n",
      "loss: 0.141424  [ 1920/ 3200]\n",
      "loss: 0.057110  [ 1936/ 3200]\n",
      "loss: 0.022551  [ 1952/ 3200]\n",
      "loss: 0.063991  [ 1968/ 3200]\n",
      "loss: 0.031577  [ 1984/ 3200]\n",
      "loss: 0.065503  [ 2000/ 3200]\n",
      "loss: 0.080004  [ 2016/ 3200]\n",
      "loss: 0.050421  [ 2032/ 3200]\n",
      "loss: 0.017072  [ 2048/ 3200]\n",
      "loss: 0.037238  [ 2064/ 3200]\n",
      "loss: 0.173915  [ 2080/ 3200]\n",
      "loss: 0.035595  [ 2096/ 3200]\n",
      "loss: 0.058880  [ 2112/ 3200]\n",
      "loss: 0.020306  [ 2128/ 3200]\n",
      "loss: 0.118273  [ 2144/ 3200]\n",
      "loss: 0.018015  [ 2160/ 3200]\n",
      "loss: 0.042692  [ 2176/ 3200]\n",
      "loss: 0.475825  [ 2192/ 3200]\n",
      "loss: 0.064415  [ 2208/ 3200]\n",
      "loss: 0.070010  [ 2224/ 3200]\n",
      "loss: 0.100906  [ 2240/ 3200]\n",
      "loss: 0.012843  [ 2256/ 3200]\n",
      "loss: 0.042695  [ 2272/ 3200]\n",
      "loss: 0.043069  [ 2288/ 3200]\n",
      "loss: 0.054123  [ 2304/ 3200]\n",
      "loss: 0.050916  [ 2320/ 3200]\n",
      "loss: 0.058176  [ 2336/ 3200]\n",
      "loss: 0.040291  [ 2352/ 3200]\n",
      "loss: 0.025101  [ 2368/ 3200]\n",
      "loss: 0.049539  [ 2384/ 3200]\n",
      "loss: 0.084265  [ 2400/ 3200]\n",
      "loss: 0.016085  [ 2416/ 3200]\n",
      "loss: 0.038712  [ 2432/ 3200]\n",
      "loss: 0.020764  [ 2448/ 3200]\n",
      "loss: 0.026569  [ 2464/ 3200]\n",
      "loss: 0.111863  [ 2480/ 3200]\n",
      "loss: 0.064537  [ 2496/ 3200]\n",
      "loss: 0.033566  [ 2512/ 3200]\n",
      "loss: 0.020217  [ 2528/ 3200]\n",
      "loss: 0.088539  [ 2544/ 3200]\n",
      "loss: 0.126407  [ 2560/ 3200]\n",
      "loss: 0.043057  [ 2576/ 3200]\n",
      "loss: 0.019374  [ 2592/ 3200]\n",
      "loss: 0.082110  [ 2608/ 3200]\n",
      "loss: 0.032077  [ 2624/ 3200]\n",
      "loss: 0.010690  [ 2640/ 3200]\n",
      "loss: 0.041610  [ 2656/ 3200]\n",
      "loss: 0.053523  [ 2672/ 3200]\n",
      "loss: 0.158712  [ 2688/ 3200]\n",
      "loss: 0.042067  [ 2704/ 3200]\n",
      "loss: 0.162575  [ 2720/ 3200]\n",
      "loss: 0.112578  [ 2736/ 3200]\n",
      "loss: 0.032002  [ 2752/ 3200]\n",
      "loss: 0.065807  [ 2768/ 3200]\n",
      "loss: 0.064789  [ 2784/ 3200]\n",
      "loss: 0.102690  [ 2800/ 3200]\n",
      "loss: 0.111021  [ 2816/ 3200]\n",
      "loss: 0.021250  [ 2832/ 3200]\n",
      "loss: 0.037400  [ 2848/ 3200]\n",
      "loss: 0.012748  [ 2864/ 3200]\n",
      "loss: 0.039157  [ 2880/ 3200]\n",
      "loss: 0.067925  [ 2896/ 3200]\n",
      "loss: 0.041035  [ 2912/ 3200]\n",
      "loss: 0.049082  [ 2928/ 3200]\n",
      "loss: 0.201350  [ 2944/ 3200]\n",
      "loss: 0.094263  [ 2960/ 3200]\n",
      "loss: 0.048587  [ 2976/ 3200]\n",
      "loss: 0.027073  [ 2992/ 3200]\n",
      "loss: 0.057107  [ 3008/ 3200]\n",
      "loss: 0.095097  [ 3024/ 3200]\n",
      "loss: 0.042974  [ 3040/ 3200]\n",
      "loss: 0.055101  [ 3056/ 3200]\n",
      "loss: 0.027064  [ 3072/ 3200]\n",
      "loss: 0.064746  [ 3088/ 3200]\n",
      "loss: 0.121005  [ 3104/ 3200]\n",
      "loss: 0.043433  [ 3120/ 3200]\n",
      "loss: 0.006556  [ 3136/ 3200]\n",
      "loss: 0.079377  [ 3152/ 3200]\n",
      "loss: 0.046155  [ 3168/ 3200]\n",
      "loss: 0.091919  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.048791\n",
      "f1 macro averaged score: 0.779726\n",
      "Accuracy               : 78.0%\n",
      "Confusion matrix       :\n",
      "tensor([[178,   8,   0,  14],\n",
      "        [ 15, 128,  15,  42],\n",
      "        [  0,  23, 158,  19],\n",
      "        [  9,  18,  13, 160]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.5702e-04.\n",
      "\n",
      "Epoch: 41\n",
      "-----------------------------\n",
      "loss: 0.021335  [    0/ 3200]\n",
      "loss: 0.032195  [   16/ 3200]\n",
      "loss: 0.073818  [   32/ 3200]\n",
      "loss: 0.035887  [   48/ 3200]\n",
      "loss: 0.082699  [   64/ 3200]\n",
      "loss: 0.099032  [   80/ 3200]\n",
      "loss: 0.043928  [   96/ 3200]\n",
      "loss: 0.038158  [  112/ 3200]\n",
      "loss: 0.008996  [  128/ 3200]\n",
      "loss: 0.062534  [  144/ 3200]\n",
      "loss: 0.195124  [  160/ 3200]\n",
      "loss: 0.068770  [  176/ 3200]\n",
      "loss: 0.048064  [  192/ 3200]\n",
      "loss: 0.049081  [  208/ 3200]\n",
      "loss: 0.108683  [  224/ 3200]\n",
      "loss: 0.121792  [  240/ 3200]\n",
      "loss: 0.016685  [  256/ 3200]\n",
      "loss: 0.055436  [  272/ 3200]\n",
      "loss: 0.032914  [  288/ 3200]\n",
      "loss: 0.064662  [  304/ 3200]\n",
      "loss: 0.157785  [  320/ 3200]\n",
      "loss: 0.022439  [  336/ 3200]\n",
      "loss: 0.060237  [  352/ 3200]\n",
      "loss: 0.034453  [  368/ 3200]\n",
      "loss: 0.053549  [  384/ 3200]\n",
      "loss: 0.039610  [  400/ 3200]\n",
      "loss: 0.062238  [  416/ 3200]\n",
      "loss: 0.053468  [  432/ 3200]\n",
      "loss: 0.137837  [  448/ 3200]\n",
      "loss: 0.025194  [  464/ 3200]\n",
      "loss: 0.090010  [  480/ 3200]\n",
      "loss: 0.017767  [  496/ 3200]\n",
      "loss: 0.185214  [  512/ 3200]\n",
      "loss: 0.034961  [  528/ 3200]\n",
      "loss: 0.066304  [  544/ 3200]\n",
      "loss: 0.032008  [  560/ 3200]\n",
      "loss: 0.051889  [  576/ 3200]\n",
      "loss: 0.043314  [  592/ 3200]\n",
      "loss: 0.043672  [  608/ 3200]\n",
      "loss: 0.012499  [  624/ 3200]\n",
      "loss: 0.023518  [  640/ 3200]\n",
      "loss: 0.054593  [  656/ 3200]\n",
      "loss: 0.079246  [  672/ 3200]\n",
      "loss: 0.240702  [  688/ 3200]\n",
      "loss: 0.004210  [  704/ 3200]\n",
      "loss: 0.025189  [  720/ 3200]\n",
      "loss: 0.083007  [  736/ 3200]\n",
      "loss: 0.075708  [  752/ 3200]\n",
      "loss: 0.075697  [  768/ 3200]\n",
      "loss: 0.022300  [  784/ 3200]\n",
      "loss: 0.026840  [  800/ 3200]\n",
      "loss: 0.062267  [  816/ 3200]\n",
      "loss: 0.207138  [  832/ 3200]\n",
      "loss: 0.251151  [  848/ 3200]\n",
      "loss: 0.205523  [  864/ 3200]\n",
      "loss: 0.081465  [  880/ 3200]\n",
      "loss: 0.139070  [  896/ 3200]\n",
      "loss: 0.047625  [  912/ 3200]\n",
      "loss: 0.365493  [  928/ 3200]\n",
      "loss: 0.107526  [  944/ 3200]\n",
      "loss: 0.078862  [  960/ 3200]\n",
      "loss: 0.081825  [  976/ 3200]\n",
      "loss: 0.101341  [  992/ 3200]\n",
      "loss: 0.044414  [ 1008/ 3200]\n",
      "loss: 0.049837  [ 1024/ 3200]\n",
      "loss: 0.063961  [ 1040/ 3200]\n",
      "loss: 0.074572  [ 1056/ 3200]\n",
      "loss: 0.024189  [ 1072/ 3200]\n",
      "loss: 0.144602  [ 1088/ 3200]\n",
      "loss: 0.024680  [ 1104/ 3200]\n",
      "loss: 0.056922  [ 1120/ 3200]\n",
      "loss: 0.153787  [ 1136/ 3200]\n",
      "loss: 0.129379  [ 1152/ 3200]\n",
      "loss: 0.034249  [ 1168/ 3200]\n",
      "loss: 0.043996  [ 1184/ 3200]\n",
      "loss: 0.070157  [ 1200/ 3200]\n",
      "loss: 0.059192  [ 1216/ 3200]\n",
      "loss: 0.024981  [ 1232/ 3200]\n",
      "loss: 0.072402  [ 1248/ 3200]\n",
      "loss: 0.145296  [ 1264/ 3200]\n",
      "loss: 0.024890  [ 1280/ 3200]\n",
      "loss: 0.019445  [ 1296/ 3200]\n",
      "loss: 0.031389  [ 1312/ 3200]\n",
      "loss: 0.051889  [ 1328/ 3200]\n",
      "loss: 0.022332  [ 1344/ 3200]\n",
      "loss: 0.171205  [ 1360/ 3200]\n",
      "loss: 0.031128  [ 1376/ 3200]\n",
      "loss: 0.401233  [ 1392/ 3200]\n",
      "loss: 0.103673  [ 1408/ 3200]\n",
      "loss: 0.116894  [ 1424/ 3200]\n",
      "loss: 0.071007  [ 1440/ 3200]\n",
      "loss: 0.036081  [ 1456/ 3200]\n",
      "loss: 0.027451  [ 1472/ 3200]\n",
      "loss: 0.027462  [ 1488/ 3200]\n",
      "loss: 0.041994  [ 1504/ 3200]\n",
      "loss: 0.032136  [ 1520/ 3200]\n",
      "loss: 0.073188  [ 1536/ 3200]\n",
      "loss: 0.015493  [ 1552/ 3200]\n",
      "loss: 0.093486  [ 1568/ 3200]\n",
      "loss: 0.056069  [ 1584/ 3200]\n",
      "loss: 0.092481  [ 1600/ 3200]\n",
      "loss: 0.045135  [ 1616/ 3200]\n",
      "loss: 0.107914  [ 1632/ 3200]\n",
      "loss: 0.115809  [ 1648/ 3200]\n",
      "loss: 0.020293  [ 1664/ 3200]\n",
      "loss: 0.188181  [ 1680/ 3200]\n",
      "loss: 0.207460  [ 1696/ 3200]\n",
      "loss: 0.036037  [ 1712/ 3200]\n",
      "loss: 0.036787  [ 1728/ 3200]\n",
      "loss: 0.021321  [ 1744/ 3200]\n",
      "loss: 0.025876  [ 1760/ 3200]\n",
      "loss: 0.125523  [ 1776/ 3200]\n",
      "loss: 0.146869  [ 1792/ 3200]\n",
      "loss: 0.046313  [ 1808/ 3200]\n",
      "loss: 0.055899  [ 1824/ 3200]\n",
      "loss: 0.020600  [ 1840/ 3200]\n",
      "loss: 0.013472  [ 1856/ 3200]\n",
      "loss: 0.110162  [ 1872/ 3200]\n",
      "loss: 0.095594  [ 1888/ 3200]\n",
      "loss: 0.015322  [ 1904/ 3200]\n",
      "loss: 0.106315  [ 1920/ 3200]\n",
      "loss: 0.121272  [ 1936/ 3200]\n",
      "loss: 0.133711  [ 1952/ 3200]\n",
      "loss: 0.047806  [ 1968/ 3200]\n",
      "loss: 0.040369  [ 1984/ 3200]\n",
      "loss: 0.046254  [ 2000/ 3200]\n",
      "loss: 0.084140  [ 2016/ 3200]\n",
      "loss: 0.041717  [ 2032/ 3200]\n",
      "loss: 0.072050  [ 2048/ 3200]\n",
      "loss: 0.090454  [ 2064/ 3200]\n",
      "loss: 0.084387  [ 2080/ 3200]\n",
      "loss: 0.036565  [ 2096/ 3200]\n",
      "loss: 0.202932  [ 2112/ 3200]\n",
      "loss: 0.090693  [ 2128/ 3200]\n",
      "loss: 0.039086  [ 2144/ 3200]\n",
      "loss: 0.024618  [ 2160/ 3200]\n",
      "loss: 0.141395  [ 2176/ 3200]\n",
      "loss: 0.046624  [ 2192/ 3200]\n",
      "loss: 0.100684  [ 2208/ 3200]\n",
      "loss: 0.091356  [ 2224/ 3200]\n",
      "loss: 0.073589  [ 2240/ 3200]\n",
      "loss: 0.059510  [ 2256/ 3200]\n",
      "loss: 0.029305  [ 2272/ 3200]\n",
      "loss: 0.050086  [ 2288/ 3200]\n",
      "loss: 0.077956  [ 2304/ 3200]\n",
      "loss: 0.071349  [ 2320/ 3200]\n",
      "loss: 0.159010  [ 2336/ 3200]\n",
      "loss: 0.070084  [ 2352/ 3200]\n",
      "loss: 0.130185  [ 2368/ 3200]\n",
      "loss: 0.058089  [ 2384/ 3200]\n",
      "loss: 0.081472  [ 2400/ 3200]\n",
      "loss: 0.029418  [ 2416/ 3200]\n",
      "loss: 0.036561  [ 2432/ 3200]\n",
      "loss: 0.011340  [ 2448/ 3200]\n",
      "loss: 0.019584  [ 2464/ 3200]\n",
      "loss: 0.087654  [ 2480/ 3200]\n",
      "loss: 0.057240  [ 2496/ 3200]\n",
      "loss: 0.117893  [ 2512/ 3200]\n",
      "loss: 0.186445  [ 2528/ 3200]\n",
      "loss: 0.125669  [ 2544/ 3200]\n",
      "loss: 0.054246  [ 2560/ 3200]\n",
      "loss: 0.047148  [ 2576/ 3200]\n",
      "loss: 0.131728  [ 2592/ 3200]\n",
      "loss: 0.017224  [ 2608/ 3200]\n",
      "loss: 0.125838  [ 2624/ 3200]\n",
      "loss: 0.013763  [ 2640/ 3200]\n",
      "loss: 0.033280  [ 2656/ 3200]\n",
      "loss: 0.039722  [ 2672/ 3200]\n",
      "loss: 0.284000  [ 2688/ 3200]\n",
      "loss: 0.023299  [ 2704/ 3200]\n",
      "loss: 0.058972  [ 2720/ 3200]\n",
      "loss: 0.025872  [ 2736/ 3200]\n",
      "loss: 0.072821  [ 2752/ 3200]\n",
      "loss: 0.094854  [ 2768/ 3200]\n",
      "loss: 0.020606  [ 2784/ 3200]\n",
      "loss: 0.022536  [ 2800/ 3200]\n",
      "loss: 0.014141  [ 2816/ 3200]\n",
      "loss: 0.094390  [ 2832/ 3200]\n",
      "loss: 0.075788  [ 2848/ 3200]\n",
      "loss: 0.071549  [ 2864/ 3200]\n",
      "loss: 0.052423  [ 2880/ 3200]\n",
      "loss: 0.038752  [ 2896/ 3200]\n",
      "loss: 0.032981  [ 2912/ 3200]\n",
      "loss: 0.022228  [ 2928/ 3200]\n",
      "loss: 0.054321  [ 2944/ 3200]\n",
      "loss: 0.056493  [ 2960/ 3200]\n",
      "loss: 0.098665  [ 2976/ 3200]\n",
      "loss: 0.112133  [ 2992/ 3200]\n",
      "loss: 0.101920  [ 3008/ 3200]\n",
      "loss: 0.016387  [ 3024/ 3200]\n",
      "loss: 0.058344  [ 3040/ 3200]\n",
      "loss: 0.118364  [ 3056/ 3200]\n",
      "loss: 0.089318  [ 3072/ 3200]\n",
      "loss: 0.093723  [ 3088/ 3200]\n",
      "loss: 0.018473  [ 3104/ 3200]\n",
      "loss: 0.020139  [ 3120/ 3200]\n",
      "loss: 0.204778  [ 3136/ 3200]\n",
      "loss: 0.058695  [ 3152/ 3200]\n",
      "loss: 0.046111  [ 3168/ 3200]\n",
      "loss: 0.035266  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.048362\n",
      "f1 macro averaged score: 0.772486\n",
      "Accuracy               : 77.4%\n",
      "Confusion matrix       :\n",
      "tensor([[175,   8,   0,  17],\n",
      "        [ 18, 120,  19,  43],\n",
      "        [  0,  26, 158,  16],\n",
      "        [  7,  18,   9, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.4417e-04.\n",
      "\n",
      "Epoch: 42\n",
      "-----------------------------\n",
      "loss: 0.012646  [    0/ 3200]\n",
      "loss: 0.126554  [   16/ 3200]\n",
      "loss: 0.155251  [   32/ 3200]\n",
      "loss: 0.122311  [   48/ 3200]\n",
      "loss: 0.021288  [   64/ 3200]\n",
      "loss: 0.185404  [   80/ 3200]\n",
      "loss: 0.025028  [   96/ 3200]\n",
      "loss: 0.094560  [  112/ 3200]\n",
      "loss: 0.036397  [  128/ 3200]\n",
      "loss: 0.054507  [  144/ 3200]\n",
      "loss: 0.053308  [  160/ 3200]\n",
      "loss: 0.145958  [  176/ 3200]\n",
      "loss: 0.044092  [  192/ 3200]\n",
      "loss: 0.039205  [  208/ 3200]\n",
      "loss: 0.091865  [  224/ 3200]\n",
      "loss: 0.038626  [  240/ 3200]\n",
      "loss: 0.042859  [  256/ 3200]\n",
      "loss: 0.122156  [  272/ 3200]\n",
      "loss: 0.048686  [  288/ 3200]\n",
      "loss: 0.042755  [  304/ 3200]\n",
      "loss: 0.028009  [  320/ 3200]\n",
      "loss: 0.098907  [  336/ 3200]\n",
      "loss: 0.175320  [  352/ 3200]\n",
      "loss: 0.080952  [  368/ 3200]\n",
      "loss: 0.085797  [  384/ 3200]\n",
      "loss: 0.026393  [  400/ 3200]\n",
      "loss: 0.029696  [  416/ 3200]\n",
      "loss: 0.021013  [  432/ 3200]\n",
      "loss: 0.244129  [  448/ 3200]\n",
      "loss: 0.058328  [  464/ 3200]\n",
      "loss: 0.028846  [  480/ 3200]\n",
      "loss: 0.044473  [  496/ 3200]\n",
      "loss: 0.014502  [  512/ 3200]\n",
      "loss: 0.039145  [  528/ 3200]\n",
      "loss: 0.049008  [  544/ 3200]\n",
      "loss: 0.040962  [  560/ 3200]\n",
      "loss: 0.033753  [  576/ 3200]\n",
      "loss: 0.022880  [  592/ 3200]\n",
      "loss: 0.037710  [  608/ 3200]\n",
      "loss: 0.041010  [  624/ 3200]\n",
      "loss: 0.200315  [  640/ 3200]\n",
      "loss: 0.031400  [  656/ 3200]\n",
      "loss: 0.034669  [  672/ 3200]\n",
      "loss: 0.038529  [  688/ 3200]\n",
      "loss: 0.029291  [  704/ 3200]\n",
      "loss: 0.064033  [  720/ 3200]\n",
      "loss: 0.031935  [  736/ 3200]\n",
      "loss: 0.031099  [  752/ 3200]\n",
      "loss: 0.067037  [  768/ 3200]\n",
      "loss: 0.025456  [  784/ 3200]\n",
      "loss: 0.082546  [  800/ 3200]\n",
      "loss: 0.022847  [  816/ 3200]\n",
      "loss: 0.066315  [  832/ 3200]\n",
      "loss: 0.035317  [  848/ 3200]\n",
      "loss: 0.037325  [  864/ 3200]\n",
      "loss: 0.080684  [  880/ 3200]\n",
      "loss: 0.024199  [  896/ 3200]\n",
      "loss: 0.099191  [  912/ 3200]\n",
      "loss: 0.080289  [  928/ 3200]\n",
      "loss: 0.085005  [  944/ 3200]\n",
      "loss: 0.029753  [  960/ 3200]\n",
      "loss: 0.041210  [  976/ 3200]\n",
      "loss: 0.144609  [  992/ 3200]\n",
      "loss: 0.022460  [ 1008/ 3200]\n",
      "loss: 0.039663  [ 1024/ 3200]\n",
      "loss: 0.022006  [ 1040/ 3200]\n",
      "loss: 0.041158  [ 1056/ 3200]\n",
      "loss: 0.138999  [ 1072/ 3200]\n",
      "loss: 0.042880  [ 1088/ 3200]\n",
      "loss: 0.020327  [ 1104/ 3200]\n",
      "loss: 0.024908  [ 1120/ 3200]\n",
      "loss: 0.020673  [ 1136/ 3200]\n",
      "loss: 0.028876  [ 1152/ 3200]\n",
      "loss: 0.068084  [ 1168/ 3200]\n",
      "loss: 0.022376  [ 1184/ 3200]\n",
      "loss: 0.037949  [ 1200/ 3200]\n",
      "loss: 0.043421  [ 1216/ 3200]\n",
      "loss: 0.088718  [ 1232/ 3200]\n",
      "loss: 0.045316  [ 1248/ 3200]\n",
      "loss: 0.033031  [ 1264/ 3200]\n",
      "loss: 0.030888  [ 1280/ 3200]\n",
      "loss: 0.019472  [ 1296/ 3200]\n",
      "loss: 0.063185  [ 1312/ 3200]\n",
      "loss: 0.091600  [ 1328/ 3200]\n",
      "loss: 0.042179  [ 1344/ 3200]\n",
      "loss: 0.041681  [ 1360/ 3200]\n",
      "loss: 0.065739  [ 1376/ 3200]\n",
      "loss: 0.050065  [ 1392/ 3200]\n",
      "loss: 0.024217  [ 1408/ 3200]\n",
      "loss: 0.036235  [ 1424/ 3200]\n",
      "loss: 0.050166  [ 1440/ 3200]\n",
      "loss: 0.079226  [ 1456/ 3200]\n",
      "loss: 0.136591  [ 1472/ 3200]\n",
      "loss: 0.054020  [ 1488/ 3200]\n",
      "loss: 0.045919  [ 1504/ 3200]\n",
      "loss: 0.062711  [ 1520/ 3200]\n",
      "loss: 0.059618  [ 1536/ 3200]\n",
      "loss: 0.075880  [ 1552/ 3200]\n",
      "loss: 0.084455  [ 1568/ 3200]\n",
      "loss: 0.028570  [ 1584/ 3200]\n",
      "loss: 0.200795  [ 1600/ 3200]\n",
      "loss: 0.057030  [ 1616/ 3200]\n",
      "loss: 0.051578  [ 1632/ 3200]\n",
      "loss: 0.031609  [ 1648/ 3200]\n",
      "loss: 0.033480  [ 1664/ 3200]\n",
      "loss: 0.116030  [ 1680/ 3200]\n",
      "loss: 0.037804  [ 1696/ 3200]\n",
      "loss: 0.026197  [ 1712/ 3200]\n",
      "loss: 0.047707  [ 1728/ 3200]\n",
      "loss: 0.080287  [ 1744/ 3200]\n",
      "loss: 0.509791  [ 1760/ 3200]\n",
      "loss: 0.068314  [ 1776/ 3200]\n",
      "loss: 0.032156  [ 1792/ 3200]\n",
      "loss: 0.018574  [ 1808/ 3200]\n",
      "loss: 0.041245  [ 1824/ 3200]\n",
      "loss: 0.006386  [ 1840/ 3200]\n",
      "loss: 0.121491  [ 1856/ 3200]\n",
      "loss: 0.080474  [ 1872/ 3200]\n",
      "loss: 0.030166  [ 1888/ 3200]\n",
      "loss: 0.025285  [ 1904/ 3200]\n",
      "loss: 0.023397  [ 1920/ 3200]\n",
      "loss: 0.079670  [ 1936/ 3200]\n",
      "loss: 0.182127  [ 1952/ 3200]\n",
      "loss: 0.026659  [ 1968/ 3200]\n",
      "loss: 0.043896  [ 1984/ 3200]\n",
      "loss: 0.053819  [ 2000/ 3200]\n",
      "loss: 0.010861  [ 2016/ 3200]\n",
      "loss: 0.038895  [ 2032/ 3200]\n",
      "loss: 0.012804  [ 2048/ 3200]\n",
      "loss: 0.022325  [ 2064/ 3200]\n",
      "loss: 0.190787  [ 2080/ 3200]\n",
      "loss: 0.025527  [ 2096/ 3200]\n",
      "loss: 0.093628  [ 2112/ 3200]\n",
      "loss: 0.032649  [ 2128/ 3200]\n",
      "loss: 0.080917  [ 2144/ 3200]\n",
      "loss: 0.055118  [ 2160/ 3200]\n",
      "loss: 0.077731  [ 2176/ 3200]\n",
      "loss: 0.099695  [ 2192/ 3200]\n",
      "loss: 0.033494  [ 2208/ 3200]\n",
      "loss: 0.069519  [ 2224/ 3200]\n",
      "loss: 0.042391  [ 2240/ 3200]\n",
      "loss: 0.015308  [ 2256/ 3200]\n",
      "loss: 0.034328  [ 2272/ 3200]\n",
      "loss: 0.073883  [ 2288/ 3200]\n",
      "loss: 0.065160  [ 2304/ 3200]\n",
      "loss: 0.035443  [ 2320/ 3200]\n",
      "loss: 0.340285  [ 2336/ 3200]\n",
      "loss: 0.122624  [ 2352/ 3200]\n",
      "loss: 0.009259  [ 2368/ 3200]\n",
      "loss: 0.060689  [ 2384/ 3200]\n",
      "loss: 0.027816  [ 2400/ 3200]\n",
      "loss: 0.032515  [ 2416/ 3200]\n",
      "loss: 0.087775  [ 2432/ 3200]\n",
      "loss: 0.096402  [ 2448/ 3200]\n",
      "loss: 0.068364  [ 2464/ 3200]\n",
      "loss: 0.036527  [ 2480/ 3200]\n",
      "loss: 0.076117  [ 2496/ 3200]\n",
      "loss: 0.053548  [ 2512/ 3200]\n",
      "loss: 0.015634  [ 2528/ 3200]\n",
      "loss: 0.063718  [ 2544/ 3200]\n",
      "loss: 0.167132  [ 2560/ 3200]\n",
      "loss: 0.026318  [ 2576/ 3200]\n",
      "loss: 0.041664  [ 2592/ 3200]\n",
      "loss: 0.064696  [ 2608/ 3200]\n",
      "loss: 0.132079  [ 2624/ 3200]\n",
      "loss: 0.013127  [ 2640/ 3200]\n",
      "loss: 0.098219  [ 2656/ 3200]\n",
      "loss: 0.074562  [ 2672/ 3200]\n",
      "loss: 0.022046  [ 2688/ 3200]\n",
      "loss: 0.037394  [ 2704/ 3200]\n",
      "loss: 0.063323  [ 2720/ 3200]\n",
      "loss: 0.107022  [ 2736/ 3200]\n",
      "loss: 0.080583  [ 2752/ 3200]\n",
      "loss: 0.121342  [ 2768/ 3200]\n",
      "loss: 0.371855  [ 2784/ 3200]\n",
      "loss: 0.023554  [ 2800/ 3200]\n",
      "loss: 0.047680  [ 2816/ 3200]\n",
      "loss: 0.122072  [ 2832/ 3200]\n",
      "loss: 0.038480  [ 2848/ 3200]\n",
      "loss: 0.055561  [ 2864/ 3200]\n",
      "loss: 0.225094  [ 2880/ 3200]\n",
      "loss: 0.274623  [ 2896/ 3200]\n",
      "loss: 0.061780  [ 2912/ 3200]\n",
      "loss: 0.024799  [ 2928/ 3200]\n",
      "loss: 0.030594  [ 2944/ 3200]\n",
      "loss: 0.115594  [ 2960/ 3200]\n",
      "loss: 0.102857  [ 2976/ 3200]\n",
      "loss: 0.026156  [ 2992/ 3200]\n",
      "loss: 0.397625  [ 3008/ 3200]\n",
      "loss: 0.075320  [ 3024/ 3200]\n",
      "loss: 0.027427  [ 3040/ 3200]\n",
      "loss: 0.157565  [ 3056/ 3200]\n",
      "loss: 0.013951  [ 3072/ 3200]\n",
      "loss: 0.013024  [ 3088/ 3200]\n",
      "loss: 0.038420  [ 3104/ 3200]\n",
      "loss: 0.027642  [ 3120/ 3200]\n",
      "loss: 0.061616  [ 3136/ 3200]\n",
      "loss: 0.025207  [ 3152/ 3200]\n",
      "loss: 0.043188  [ 3168/ 3200]\n",
      "loss: 0.031607  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.049759\n",
      "f1 macro averaged score: 0.773920\n",
      "Accuracy               : 77.4%\n",
      "Confusion matrix       :\n",
      "tensor([[173,  10,   1,  16],\n",
      "        [ 13, 128,  20,  39],\n",
      "        [  0,  18, 160,  22],\n",
      "        [  9,  21,  12, 158]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.3196e-04.\n",
      "\n",
      "Epoch: 43\n",
      "-----------------------------\n",
      "loss: 0.021410  [    0/ 3200]\n",
      "loss: 0.032153  [   16/ 3200]\n",
      "loss: 0.042020  [   32/ 3200]\n",
      "loss: 0.143787  [   48/ 3200]\n",
      "loss: 0.051441  [   64/ 3200]\n",
      "loss: 0.113369  [   80/ 3200]\n",
      "loss: 0.137807  [   96/ 3200]\n",
      "loss: 0.033489  [  112/ 3200]\n",
      "loss: 0.057519  [  128/ 3200]\n",
      "loss: 0.041610  [  144/ 3200]\n",
      "loss: 0.119035  [  160/ 3200]\n",
      "loss: 0.027754  [  176/ 3200]\n",
      "loss: 0.065411  [  192/ 3200]\n",
      "loss: 0.088880  [  208/ 3200]\n",
      "loss: 0.120479  [  224/ 3200]\n",
      "loss: 0.124408  [  240/ 3200]\n",
      "loss: 0.049944  [  256/ 3200]\n",
      "loss: 0.018934  [  272/ 3200]\n",
      "loss: 0.038461  [  288/ 3200]\n",
      "loss: 0.166068  [  304/ 3200]\n",
      "loss: 0.022508  [  320/ 3200]\n",
      "loss: 0.061095  [  336/ 3200]\n",
      "loss: 0.034942  [  352/ 3200]\n",
      "loss: 0.213550  [  368/ 3200]\n",
      "loss: 0.042947  [  384/ 3200]\n",
      "loss: 0.085265  [  400/ 3200]\n",
      "loss: 0.117379  [  416/ 3200]\n",
      "loss: 0.018078  [  432/ 3200]\n",
      "loss: 0.151945  [  448/ 3200]\n",
      "loss: 0.028868  [  464/ 3200]\n",
      "loss: 0.038644  [  480/ 3200]\n",
      "loss: 0.033347  [  496/ 3200]\n",
      "loss: 0.012008  [  512/ 3200]\n",
      "loss: 0.070224  [  528/ 3200]\n",
      "loss: 0.149520  [  544/ 3200]\n",
      "loss: 0.017995  [  560/ 3200]\n",
      "loss: 0.101712  [  576/ 3200]\n",
      "loss: 0.036821  [  592/ 3200]\n",
      "loss: 0.152588  [  608/ 3200]\n",
      "loss: 0.083532  [  624/ 3200]\n",
      "loss: 0.024937  [  640/ 3200]\n",
      "loss: 0.431270  [  656/ 3200]\n",
      "loss: 0.017958  [  672/ 3200]\n",
      "loss: 0.028166  [  688/ 3200]\n",
      "loss: 0.060941  [  704/ 3200]\n",
      "loss: 0.155261  [  720/ 3200]\n",
      "loss: 0.032292  [  736/ 3200]\n",
      "loss: 0.029460  [  752/ 3200]\n",
      "loss: 0.010852  [  768/ 3200]\n",
      "loss: 0.046863  [  784/ 3200]\n",
      "loss: 0.082607  [  800/ 3200]\n",
      "loss: 0.094145  [  816/ 3200]\n",
      "loss: 0.040412  [  832/ 3200]\n",
      "loss: 0.028366  [  848/ 3200]\n",
      "loss: 0.049288  [  864/ 3200]\n",
      "loss: 0.105811  [  880/ 3200]\n",
      "loss: 0.054108  [  896/ 3200]\n",
      "loss: 0.085493  [  912/ 3200]\n",
      "loss: 0.040639  [  928/ 3200]\n",
      "loss: 0.155322  [  944/ 3200]\n",
      "loss: 0.079894  [  960/ 3200]\n",
      "loss: 0.017401  [  976/ 3200]\n",
      "loss: 0.014632  [  992/ 3200]\n",
      "loss: 0.011144  [ 1008/ 3200]\n",
      "loss: 0.037851  [ 1024/ 3200]\n",
      "loss: 0.031225  [ 1040/ 3200]\n",
      "loss: 0.044179  [ 1056/ 3200]\n",
      "loss: 0.070937  [ 1072/ 3200]\n",
      "loss: 0.121238  [ 1088/ 3200]\n",
      "loss: 0.125532  [ 1104/ 3200]\n",
      "loss: 0.019916  [ 1120/ 3200]\n",
      "loss: 0.060970  [ 1136/ 3200]\n",
      "loss: 0.023170  [ 1152/ 3200]\n",
      "loss: 0.058928  [ 1168/ 3200]\n",
      "loss: 0.025754  [ 1184/ 3200]\n",
      "loss: 0.083342  [ 1200/ 3200]\n",
      "loss: 0.025953  [ 1216/ 3200]\n",
      "loss: 0.058710  [ 1232/ 3200]\n",
      "loss: 0.026560  [ 1248/ 3200]\n",
      "loss: 0.032878  [ 1264/ 3200]\n",
      "loss: 0.011703  [ 1280/ 3200]\n",
      "loss: 0.103253  [ 1296/ 3200]\n",
      "loss: 0.098826  [ 1312/ 3200]\n",
      "loss: 0.074657  [ 1328/ 3200]\n",
      "loss: 0.014345  [ 1344/ 3200]\n",
      "loss: 0.019768  [ 1360/ 3200]\n",
      "loss: 0.039275  [ 1376/ 3200]\n",
      "loss: 0.045073  [ 1392/ 3200]\n",
      "loss: 0.144229  [ 1408/ 3200]\n",
      "loss: 0.034081  [ 1424/ 3200]\n",
      "loss: 0.029296  [ 1440/ 3200]\n",
      "loss: 0.266075  [ 1456/ 3200]\n",
      "loss: 0.140315  [ 1472/ 3200]\n",
      "loss: 0.055086  [ 1488/ 3200]\n",
      "loss: 0.048371  [ 1504/ 3200]\n",
      "loss: 0.100269  [ 1520/ 3200]\n",
      "loss: 0.066510  [ 1536/ 3200]\n",
      "loss: 0.051495  [ 1552/ 3200]\n",
      "loss: 0.042620  [ 1568/ 3200]\n",
      "loss: 0.078334  [ 1584/ 3200]\n",
      "loss: 0.054714  [ 1600/ 3200]\n",
      "loss: 0.014867  [ 1616/ 3200]\n",
      "loss: 0.014050  [ 1632/ 3200]\n",
      "loss: 0.081695  [ 1648/ 3200]\n",
      "loss: 0.042283  [ 1664/ 3200]\n",
      "loss: 0.038388  [ 1680/ 3200]\n",
      "loss: 0.042056  [ 1696/ 3200]\n",
      "loss: 0.045082  [ 1712/ 3200]\n",
      "loss: 0.229562  [ 1728/ 3200]\n",
      "loss: 0.020833  [ 1744/ 3200]\n",
      "loss: 0.015511  [ 1760/ 3200]\n",
      "loss: 0.033451  [ 1776/ 3200]\n",
      "loss: 0.061449  [ 1792/ 3200]\n",
      "loss: 0.009228  [ 1808/ 3200]\n",
      "loss: 0.014102  [ 1824/ 3200]\n",
      "loss: 0.028398  [ 1840/ 3200]\n",
      "loss: 0.061733  [ 1856/ 3200]\n",
      "loss: 0.082594  [ 1872/ 3200]\n",
      "loss: 0.040916  [ 1888/ 3200]\n",
      "loss: 0.109150  [ 1904/ 3200]\n",
      "loss: 0.142182  [ 1920/ 3200]\n",
      "loss: 0.054165  [ 1936/ 3200]\n",
      "loss: 0.055232  [ 1952/ 3200]\n",
      "loss: 0.027854  [ 1968/ 3200]\n",
      "loss: 0.031844  [ 1984/ 3200]\n",
      "loss: 0.063491  [ 2000/ 3200]\n",
      "loss: 0.089192  [ 2016/ 3200]\n",
      "loss: 0.013467  [ 2032/ 3200]\n",
      "loss: 0.072275  [ 2048/ 3200]\n",
      "loss: 0.041778  [ 2064/ 3200]\n",
      "loss: 0.090132  [ 2080/ 3200]\n",
      "loss: 0.070544  [ 2096/ 3200]\n",
      "loss: 0.041375  [ 2112/ 3200]\n",
      "loss: 0.030521  [ 2128/ 3200]\n",
      "loss: 0.048553  [ 2144/ 3200]\n",
      "loss: 0.075894  [ 2160/ 3200]\n",
      "loss: 0.108670  [ 2176/ 3200]\n",
      "loss: 0.110843  [ 2192/ 3200]\n",
      "loss: 0.080923  [ 2208/ 3200]\n",
      "loss: 0.063147  [ 2224/ 3200]\n",
      "loss: 0.022283  [ 2240/ 3200]\n",
      "loss: 0.033354  [ 2256/ 3200]\n",
      "loss: 0.090183  [ 2272/ 3200]\n",
      "loss: 0.089949  [ 2288/ 3200]\n",
      "loss: 0.091033  [ 2304/ 3200]\n",
      "loss: 0.056134  [ 2320/ 3200]\n",
      "loss: 0.005419  [ 2336/ 3200]\n",
      "loss: 0.076659  [ 2352/ 3200]\n",
      "loss: 0.041760  [ 2368/ 3200]\n",
      "loss: 0.057965  [ 2384/ 3200]\n",
      "loss: 0.036343  [ 2400/ 3200]\n",
      "loss: 0.097806  [ 2416/ 3200]\n",
      "loss: 0.052960  [ 2432/ 3200]\n",
      "loss: 0.060177  [ 2448/ 3200]\n",
      "loss: 0.076958  [ 2464/ 3200]\n",
      "loss: 0.065353  [ 2480/ 3200]\n",
      "loss: 0.022239  [ 2496/ 3200]\n",
      "loss: 0.178562  [ 2512/ 3200]\n",
      "loss: 0.027026  [ 2528/ 3200]\n",
      "loss: 0.086697  [ 2544/ 3200]\n",
      "loss: 0.039382  [ 2560/ 3200]\n",
      "loss: 0.093309  [ 2576/ 3200]\n",
      "loss: 0.019562  [ 2592/ 3200]\n",
      "loss: 0.016794  [ 2608/ 3200]\n",
      "loss: 0.057939  [ 2624/ 3200]\n",
      "loss: 0.247348  [ 2640/ 3200]\n",
      "loss: 0.027315  [ 2656/ 3200]\n",
      "loss: 0.055141  [ 2672/ 3200]\n",
      "loss: 0.025093  [ 2688/ 3200]\n",
      "loss: 0.023643  [ 2704/ 3200]\n",
      "loss: 0.110852  [ 2720/ 3200]\n",
      "loss: 0.026107  [ 2736/ 3200]\n",
      "loss: 0.059854  [ 2752/ 3200]\n",
      "loss: 0.037111  [ 2768/ 3200]\n",
      "loss: 0.037003  [ 2784/ 3200]\n",
      "loss: 0.066283  [ 2800/ 3200]\n",
      "loss: 0.008285  [ 2816/ 3200]\n",
      "loss: 0.031972  [ 2832/ 3200]\n",
      "loss: 0.065764  [ 2848/ 3200]\n",
      "loss: 0.069231  [ 2864/ 3200]\n",
      "loss: 0.101097  [ 2880/ 3200]\n",
      "loss: 0.311591  [ 2896/ 3200]\n",
      "loss: 0.055960  [ 2912/ 3200]\n",
      "loss: 0.147423  [ 2928/ 3200]\n",
      "loss: 0.152961  [ 2944/ 3200]\n",
      "loss: 0.041212  [ 2960/ 3200]\n",
      "loss: 0.032519  [ 2976/ 3200]\n",
      "loss: 0.049293  [ 2992/ 3200]\n",
      "loss: 0.046096  [ 3008/ 3200]\n",
      "loss: 0.056847  [ 3024/ 3200]\n",
      "loss: 0.021971  [ 3040/ 3200]\n",
      "loss: 0.036677  [ 3056/ 3200]\n",
      "loss: 0.026023  [ 3072/ 3200]\n",
      "loss: 0.080405  [ 3088/ 3200]\n",
      "loss: 0.031551  [ 3104/ 3200]\n",
      "loss: 0.081012  [ 3120/ 3200]\n",
      "loss: 0.047602  [ 3136/ 3200]\n",
      "loss: 0.127340  [ 3152/ 3200]\n",
      "loss: 0.193099  [ 3168/ 3200]\n",
      "loss: 0.061077  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.047006\n",
      "f1 macro averaged score: 0.773968\n",
      "Accuracy               : 77.4%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  10,   0,  16],\n",
      "        [ 16, 130,  15,  39],\n",
      "        [  0,  27, 155,  18],\n",
      "        [ 10,  18,  12, 160]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.2037e-04.\n",
      "\n",
      "Epoch: 44\n",
      "-----------------------------\n",
      "loss: 0.025359  [    0/ 3200]\n",
      "loss: 0.047137  [   16/ 3200]\n",
      "loss: 0.089618  [   32/ 3200]\n",
      "loss: 0.055456  [   48/ 3200]\n",
      "loss: 0.153547  [   64/ 3200]\n",
      "loss: 0.065309  [   80/ 3200]\n",
      "loss: 0.077768  [   96/ 3200]\n",
      "loss: 0.024843  [  112/ 3200]\n",
      "loss: 0.040993  [  128/ 3200]\n",
      "loss: 0.113757  [  144/ 3200]\n",
      "loss: 0.140755  [  160/ 3200]\n",
      "loss: 0.188170  [  176/ 3200]\n",
      "loss: 0.121402  [  192/ 3200]\n",
      "loss: 0.008531  [  208/ 3200]\n",
      "loss: 0.033747  [  224/ 3200]\n",
      "loss: 0.024837  [  240/ 3200]\n",
      "loss: 0.077886  [  256/ 3200]\n",
      "loss: 0.026336  [  272/ 3200]\n",
      "loss: 0.101260  [  288/ 3200]\n",
      "loss: 0.026894  [  304/ 3200]\n",
      "loss: 0.057987  [  320/ 3200]\n",
      "loss: 0.049444  [  336/ 3200]\n",
      "loss: 0.086388  [  352/ 3200]\n",
      "loss: 0.054955  [  368/ 3200]\n",
      "loss: 0.031081  [  384/ 3200]\n",
      "loss: 0.081636  [  400/ 3200]\n",
      "loss: 0.007683  [  416/ 3200]\n",
      "loss: 0.032674  [  432/ 3200]\n",
      "loss: 0.080179  [  448/ 3200]\n",
      "loss: 0.010273  [  464/ 3200]\n",
      "loss: 0.017194  [  480/ 3200]\n",
      "loss: 0.067312  [  496/ 3200]\n",
      "loss: 0.020852  [  512/ 3200]\n",
      "loss: 0.056823  [  528/ 3200]\n",
      "loss: 0.065643  [  544/ 3200]\n",
      "loss: 0.023155  [  560/ 3200]\n",
      "loss: 0.106151  [  576/ 3200]\n",
      "loss: 0.086875  [  592/ 3200]\n",
      "loss: 0.053098  [  608/ 3200]\n",
      "loss: 0.043925  [  624/ 3200]\n",
      "loss: 0.368098  [  640/ 3200]\n",
      "loss: 0.170266  [  656/ 3200]\n",
      "loss: 0.042548  [  672/ 3200]\n",
      "loss: 0.029646  [  688/ 3200]\n",
      "loss: 0.023572  [  704/ 3200]\n",
      "loss: 0.043569  [  720/ 3200]\n",
      "loss: 0.011478  [  736/ 3200]\n",
      "loss: 0.026773  [  752/ 3200]\n",
      "loss: 0.026994  [  768/ 3200]\n",
      "loss: 0.022703  [  784/ 3200]\n",
      "loss: 0.090438  [  800/ 3200]\n",
      "loss: 0.046348  [  816/ 3200]\n",
      "loss: 0.057296  [  832/ 3200]\n",
      "loss: 0.052637  [  848/ 3200]\n",
      "loss: 0.083278  [  864/ 3200]\n",
      "loss: 0.036357  [  880/ 3200]\n",
      "loss: 0.062723  [  896/ 3200]\n",
      "loss: 0.090047  [  912/ 3200]\n",
      "loss: 0.053447  [  928/ 3200]\n",
      "loss: 0.031147  [  944/ 3200]\n",
      "loss: 0.094551  [  960/ 3200]\n",
      "loss: 0.093017  [  976/ 3200]\n",
      "loss: 0.032821  [  992/ 3200]\n",
      "loss: 0.017318  [ 1008/ 3200]\n",
      "loss: 0.075435  [ 1024/ 3200]\n",
      "loss: 0.066398  [ 1040/ 3200]\n",
      "loss: 0.047517  [ 1056/ 3200]\n",
      "loss: 0.150409  [ 1072/ 3200]\n",
      "loss: 0.094257  [ 1088/ 3200]\n",
      "loss: 0.127799  [ 1104/ 3200]\n",
      "loss: 0.058140  [ 1120/ 3200]\n",
      "loss: 0.092716  [ 1136/ 3200]\n",
      "loss: 0.007902  [ 1152/ 3200]\n",
      "loss: 0.083694  [ 1168/ 3200]\n",
      "loss: 0.044799  [ 1184/ 3200]\n",
      "loss: 0.023769  [ 1200/ 3200]\n",
      "loss: 0.037741  [ 1216/ 3200]\n",
      "loss: 0.053271  [ 1232/ 3200]\n",
      "loss: 0.033498  [ 1248/ 3200]\n",
      "loss: 0.120260  [ 1264/ 3200]\n",
      "loss: 0.028174  [ 1280/ 3200]\n",
      "loss: 0.029303  [ 1296/ 3200]\n",
      "loss: 0.238797  [ 1312/ 3200]\n",
      "loss: 0.014250  [ 1328/ 3200]\n",
      "loss: 0.192388  [ 1344/ 3200]\n",
      "loss: 0.033172  [ 1360/ 3200]\n",
      "loss: 0.006898  [ 1376/ 3200]\n",
      "loss: 0.025788  [ 1392/ 3200]\n",
      "loss: 0.136575  [ 1408/ 3200]\n",
      "loss: 0.067790  [ 1424/ 3200]\n",
      "loss: 0.080731  [ 1440/ 3200]\n",
      "loss: 0.094510  [ 1456/ 3200]\n",
      "loss: 0.025966  [ 1472/ 3200]\n",
      "loss: 0.195252  [ 1488/ 3200]\n",
      "loss: 0.127462  [ 1504/ 3200]\n",
      "loss: 0.080355  [ 1520/ 3200]\n",
      "loss: 0.251165  [ 1536/ 3200]\n",
      "loss: 0.033686  [ 1552/ 3200]\n",
      "loss: 0.025695  [ 1568/ 3200]\n",
      "loss: 0.061057  [ 1584/ 3200]\n",
      "loss: 0.087742  [ 1600/ 3200]\n",
      "loss: 0.072719  [ 1616/ 3200]\n",
      "loss: 0.110916  [ 1632/ 3200]\n",
      "loss: 0.074788  [ 1648/ 3200]\n",
      "loss: 0.144093  [ 1664/ 3200]\n",
      "loss: 0.031403  [ 1680/ 3200]\n",
      "loss: 0.086933  [ 1696/ 3200]\n",
      "loss: 0.134201  [ 1712/ 3200]\n",
      "loss: 0.029408  [ 1728/ 3200]\n",
      "loss: 0.058346  [ 1744/ 3200]\n",
      "loss: 0.025743  [ 1760/ 3200]\n",
      "loss: 0.040737  [ 1776/ 3200]\n",
      "loss: 0.053870  [ 1792/ 3200]\n",
      "loss: 0.073210  [ 1808/ 3200]\n",
      "loss: 0.035920  [ 1824/ 3200]\n",
      "loss: 0.105467  [ 1840/ 3200]\n",
      "loss: 0.019227  [ 1856/ 3200]\n",
      "loss: 0.077894  [ 1872/ 3200]\n",
      "loss: 0.063463  [ 1888/ 3200]\n",
      "loss: 0.086620  [ 1904/ 3200]\n",
      "loss: 0.134361  [ 1920/ 3200]\n",
      "loss: 0.102366  [ 1936/ 3200]\n",
      "loss: 0.110847  [ 1952/ 3200]\n",
      "loss: 0.043285  [ 1968/ 3200]\n",
      "loss: 0.056570  [ 1984/ 3200]\n",
      "loss: 0.038471  [ 2000/ 3200]\n",
      "loss: 0.038071  [ 2016/ 3200]\n",
      "loss: 0.025674  [ 2032/ 3200]\n",
      "loss: 0.022522  [ 2048/ 3200]\n",
      "loss: 0.023149  [ 2064/ 3200]\n",
      "loss: 0.182850  [ 2080/ 3200]\n",
      "loss: 0.060683  [ 2096/ 3200]\n",
      "loss: 0.186889  [ 2112/ 3200]\n",
      "loss: 0.016903  [ 2128/ 3200]\n",
      "loss: 0.072209  [ 2144/ 3200]\n",
      "loss: 0.108450  [ 2160/ 3200]\n",
      "loss: 0.017917  [ 2176/ 3200]\n",
      "loss: 0.109808  [ 2192/ 3200]\n",
      "loss: 0.062961  [ 2208/ 3200]\n",
      "loss: 0.010168  [ 2224/ 3200]\n",
      "loss: 0.050733  [ 2240/ 3200]\n",
      "loss: 0.033811  [ 2256/ 3200]\n",
      "loss: 0.045323  [ 2272/ 3200]\n",
      "loss: 0.029549  [ 2288/ 3200]\n",
      "loss: 0.100514  [ 2304/ 3200]\n",
      "loss: 0.029787  [ 2320/ 3200]\n",
      "loss: 0.047997  [ 2336/ 3200]\n",
      "loss: 0.038289  [ 2352/ 3200]\n",
      "loss: 0.068457  [ 2368/ 3200]\n",
      "loss: 0.042161  [ 2384/ 3200]\n",
      "loss: 0.018126  [ 2400/ 3200]\n",
      "loss: 0.101623  [ 2416/ 3200]\n",
      "loss: 0.161912  [ 2432/ 3200]\n",
      "loss: 0.012794  [ 2448/ 3200]\n",
      "loss: 0.034341  [ 2464/ 3200]\n",
      "loss: 0.144061  [ 2480/ 3200]\n",
      "loss: 0.059680  [ 2496/ 3200]\n",
      "loss: 0.111701  [ 2512/ 3200]\n",
      "loss: 0.084617  [ 2528/ 3200]\n",
      "loss: 0.044236  [ 2544/ 3200]\n",
      "loss: 0.114046  [ 2560/ 3200]\n",
      "loss: 0.020745  [ 2576/ 3200]\n",
      "loss: 0.044174  [ 2592/ 3200]\n",
      "loss: 0.093238  [ 2608/ 3200]\n",
      "loss: 0.063166  [ 2624/ 3200]\n",
      "loss: 0.031737  [ 2640/ 3200]\n",
      "loss: 0.077196  [ 2656/ 3200]\n",
      "loss: 0.039761  [ 2672/ 3200]\n",
      "loss: 0.087124  [ 2688/ 3200]\n",
      "loss: 0.303003  [ 2704/ 3200]\n",
      "loss: 0.049576  [ 2720/ 3200]\n",
      "loss: 0.012508  [ 2736/ 3200]\n",
      "loss: 0.094815  [ 2752/ 3200]\n",
      "loss: 0.037847  [ 2768/ 3200]\n",
      "loss: 0.050174  [ 2784/ 3200]\n",
      "loss: 0.033882  [ 2800/ 3200]\n",
      "loss: 0.220547  [ 2816/ 3200]\n",
      "loss: 0.024801  [ 2832/ 3200]\n",
      "loss: 0.033867  [ 2848/ 3200]\n",
      "loss: 0.038457  [ 2864/ 3200]\n",
      "loss: 0.130620  [ 2880/ 3200]\n",
      "loss: 0.015690  [ 2896/ 3200]\n",
      "loss: 0.068594  [ 2912/ 3200]\n",
      "loss: 0.212318  [ 2928/ 3200]\n",
      "loss: 0.031571  [ 2944/ 3200]\n",
      "loss: 0.019549  [ 2960/ 3200]\n",
      "loss: 0.157628  [ 2976/ 3200]\n",
      "loss: 0.041066  [ 2992/ 3200]\n",
      "loss: 0.028218  [ 3008/ 3200]\n",
      "loss: 0.131915  [ 3024/ 3200]\n",
      "loss: 0.066956  [ 3040/ 3200]\n",
      "loss: 0.284356  [ 3056/ 3200]\n",
      "loss: 0.022308  [ 3072/ 3200]\n",
      "loss: 0.028623  [ 3088/ 3200]\n",
      "loss: 0.062812  [ 3104/ 3200]\n",
      "loss: 0.042985  [ 3120/ 3200]\n",
      "loss: 0.053845  [ 3136/ 3200]\n",
      "loss: 0.048342  [ 3152/ 3200]\n",
      "loss: 0.086746  [ 3168/ 3200]\n",
      "loss: 0.049730  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.049786\n",
      "f1 macro averaged score: 0.777492\n",
      "Accuracy               : 77.9%\n",
      "Confusion matrix       :\n",
      "tensor([[178,   9,   0,  13],\n",
      "        [ 16, 122,  16,  46],\n",
      "        [  0,  21, 159,  20],\n",
      "        [ 10,  14,  12, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.0935e-04.\n",
      "\n",
      "Epoch: 45\n",
      "-----------------------------\n",
      "loss: 0.043696  [    0/ 3200]\n",
      "loss: 0.105701  [   16/ 3200]\n",
      "loss: 0.037799  [   32/ 3200]\n",
      "loss: 0.327625  [   48/ 3200]\n",
      "loss: 0.053221  [   64/ 3200]\n",
      "loss: 0.026437  [   80/ 3200]\n",
      "loss: 0.034860  [   96/ 3200]\n",
      "loss: 0.101095  [  112/ 3200]\n",
      "loss: 0.035869  [  128/ 3200]\n",
      "loss: 0.010549  [  144/ 3200]\n",
      "loss: 0.041023  [  160/ 3200]\n",
      "loss: 0.189955  [  176/ 3200]\n",
      "loss: 0.008389  [  192/ 3200]\n",
      "loss: 0.032139  [  208/ 3200]\n",
      "loss: 0.138268  [  224/ 3200]\n",
      "loss: 0.045921  [  240/ 3200]\n",
      "loss: 0.147620  [  256/ 3200]\n",
      "loss: 0.055359  [  272/ 3200]\n",
      "loss: 0.039453  [  288/ 3200]\n",
      "loss: 0.055513  [  304/ 3200]\n",
      "loss: 0.360864  [  320/ 3200]\n",
      "loss: 0.104767  [  336/ 3200]\n",
      "loss: 0.018460  [  352/ 3200]\n",
      "loss: 0.037174  [  368/ 3200]\n",
      "loss: 0.041832  [  384/ 3200]\n",
      "loss: 0.404313  [  400/ 3200]\n",
      "loss: 0.080995  [  416/ 3200]\n",
      "loss: 0.027078  [  432/ 3200]\n",
      "loss: 0.151669  [  448/ 3200]\n",
      "loss: 0.060581  [  464/ 3200]\n",
      "loss: 0.135689  [  480/ 3200]\n",
      "loss: 0.082957  [  496/ 3200]\n",
      "loss: 0.037603  [  512/ 3200]\n",
      "loss: 0.039623  [  528/ 3200]\n",
      "loss: 0.027252  [  544/ 3200]\n",
      "loss: 0.052370  [  560/ 3200]\n",
      "loss: 0.058145  [  576/ 3200]\n",
      "loss: 0.030077  [  592/ 3200]\n",
      "loss: 0.027359  [  608/ 3200]\n",
      "loss: 0.071562  [  624/ 3200]\n",
      "loss: 0.038219  [  640/ 3200]\n",
      "loss: 0.009291  [  656/ 3200]\n",
      "loss: 0.041580  [  672/ 3200]\n",
      "loss: 0.051880  [  688/ 3200]\n",
      "loss: 0.123062  [  704/ 3200]\n",
      "loss: 0.033689  [  720/ 3200]\n",
      "loss: 0.055567  [  736/ 3200]\n",
      "loss: 0.220843  [  752/ 3200]\n",
      "loss: 0.045733  [  768/ 3200]\n",
      "loss: 0.074951  [  784/ 3200]\n",
      "loss: 0.061407  [  800/ 3200]\n",
      "loss: 0.084240  [  816/ 3200]\n",
      "loss: 0.069091  [  832/ 3200]\n",
      "loss: 0.022377  [  848/ 3200]\n",
      "loss: 0.061044  [  864/ 3200]\n",
      "loss: 0.089329  [  880/ 3200]\n",
      "loss: 0.033508  [  896/ 3200]\n",
      "loss: 0.055992  [  912/ 3200]\n",
      "loss: 0.045018  [  928/ 3200]\n",
      "loss: 0.031690  [  944/ 3200]\n",
      "loss: 0.046445  [  960/ 3200]\n",
      "loss: 0.027789  [  976/ 3200]\n",
      "loss: 0.076660  [  992/ 3200]\n",
      "loss: 0.082010  [ 1008/ 3200]\n",
      "loss: 0.075060  [ 1024/ 3200]\n",
      "loss: 0.077643  [ 1040/ 3200]\n",
      "loss: 0.020980  [ 1056/ 3200]\n",
      "loss: 0.158612  [ 1072/ 3200]\n",
      "loss: 0.031496  [ 1088/ 3200]\n",
      "loss: 0.055472  [ 1104/ 3200]\n",
      "loss: 0.045320  [ 1120/ 3200]\n",
      "loss: 0.043449  [ 1136/ 3200]\n",
      "loss: 0.034971  [ 1152/ 3200]\n",
      "loss: 0.078001  [ 1168/ 3200]\n",
      "loss: 0.036177  [ 1184/ 3200]\n",
      "loss: 0.025745  [ 1200/ 3200]\n",
      "loss: 0.082244  [ 1216/ 3200]\n",
      "loss: 0.025121  [ 1232/ 3200]\n",
      "loss: 0.044462  [ 1248/ 3200]\n",
      "loss: 0.018817  [ 1264/ 3200]\n",
      "loss: 0.026967  [ 1280/ 3200]\n",
      "loss: 0.063603  [ 1296/ 3200]\n",
      "loss: 0.037823  [ 1312/ 3200]\n",
      "loss: 0.029169  [ 1328/ 3200]\n",
      "loss: 0.047503  [ 1344/ 3200]\n",
      "loss: 0.038603  [ 1360/ 3200]\n",
      "loss: 0.019033  [ 1376/ 3200]\n",
      "loss: 0.005271  [ 1392/ 3200]\n",
      "loss: 0.024715  [ 1408/ 3200]\n",
      "loss: 0.301960  [ 1424/ 3200]\n",
      "loss: 0.024041  [ 1440/ 3200]\n",
      "loss: 0.066395  [ 1456/ 3200]\n",
      "loss: 0.020498  [ 1472/ 3200]\n",
      "loss: 0.080318  [ 1488/ 3200]\n",
      "loss: 0.035070  [ 1504/ 3200]\n",
      "loss: 0.020663  [ 1520/ 3200]\n",
      "loss: 0.017793  [ 1536/ 3200]\n",
      "loss: 0.223237  [ 1552/ 3200]\n",
      "loss: 0.059501  [ 1568/ 3200]\n",
      "loss: 0.095305  [ 1584/ 3200]\n",
      "loss: 0.160189  [ 1600/ 3200]\n",
      "loss: 0.057562  [ 1616/ 3200]\n",
      "loss: 0.046992  [ 1632/ 3200]\n",
      "loss: 0.054154  [ 1648/ 3200]\n",
      "loss: 0.080513  [ 1664/ 3200]\n",
      "loss: 0.060836  [ 1680/ 3200]\n",
      "loss: 0.064082  [ 1696/ 3200]\n",
      "loss: 0.047096  [ 1712/ 3200]\n",
      "loss: 0.033944  [ 1728/ 3200]\n",
      "loss: 0.046996  [ 1744/ 3200]\n",
      "loss: 0.051368  [ 1760/ 3200]\n",
      "loss: 0.016348  [ 1776/ 3200]\n",
      "loss: 0.029930  [ 1792/ 3200]\n",
      "loss: 0.029879  [ 1808/ 3200]\n",
      "loss: 0.041266  [ 1824/ 3200]\n",
      "loss: 0.023717  [ 1840/ 3200]\n",
      "loss: 0.043793  [ 1856/ 3200]\n",
      "loss: 0.068487  [ 1872/ 3200]\n",
      "loss: 0.021918  [ 1888/ 3200]\n",
      "loss: 0.209469  [ 1904/ 3200]\n",
      "loss: 0.022744  [ 1920/ 3200]\n",
      "loss: 0.202647  [ 1936/ 3200]\n",
      "loss: 0.142506  [ 1952/ 3200]\n",
      "loss: 0.055139  [ 1968/ 3200]\n",
      "loss: 0.011805  [ 1984/ 3200]\n",
      "loss: 0.029649  [ 2000/ 3200]\n",
      "loss: 0.010620  [ 2016/ 3200]\n",
      "loss: 0.079531  [ 2032/ 3200]\n",
      "loss: 0.040869  [ 2048/ 3200]\n",
      "loss: 0.037014  [ 2064/ 3200]\n",
      "loss: 0.052306  [ 2080/ 3200]\n",
      "loss: 0.034732  [ 2096/ 3200]\n",
      "loss: 0.065356  [ 2112/ 3200]\n",
      "loss: 0.057553  [ 2128/ 3200]\n",
      "loss: 0.096073  [ 2144/ 3200]\n",
      "loss: 0.036822  [ 2160/ 3200]\n",
      "loss: 0.014715  [ 2176/ 3200]\n",
      "loss: 0.022123  [ 2192/ 3200]\n",
      "loss: 0.014760  [ 2208/ 3200]\n",
      "loss: 0.060052  [ 2224/ 3200]\n",
      "loss: 0.028826  [ 2240/ 3200]\n",
      "loss: 0.038147  [ 2256/ 3200]\n",
      "loss: 0.103638  [ 2272/ 3200]\n",
      "loss: 0.011192  [ 2288/ 3200]\n",
      "loss: 0.068623  [ 2304/ 3200]\n",
      "loss: 0.081547  [ 2320/ 3200]\n",
      "loss: 0.071802  [ 2336/ 3200]\n",
      "loss: 0.043671  [ 2352/ 3200]\n",
      "loss: 0.017544  [ 2368/ 3200]\n",
      "loss: 0.057176  [ 2384/ 3200]\n",
      "loss: 0.234223  [ 2400/ 3200]\n",
      "loss: 0.072830  [ 2416/ 3200]\n",
      "loss: 0.034001  [ 2432/ 3200]\n",
      "loss: 0.034056  [ 2448/ 3200]\n",
      "loss: 0.037199  [ 2464/ 3200]\n",
      "loss: 0.021969  [ 2480/ 3200]\n",
      "loss: 0.044581  [ 2496/ 3200]\n",
      "loss: 0.087892  [ 2512/ 3200]\n",
      "loss: 0.033828  [ 2528/ 3200]\n",
      "loss: 0.035976  [ 2544/ 3200]\n",
      "loss: 0.062834  [ 2560/ 3200]\n",
      "loss: 0.035691  [ 2576/ 3200]\n",
      "loss: 0.058386  [ 2592/ 3200]\n",
      "loss: 0.046934  [ 2608/ 3200]\n",
      "loss: 0.051143  [ 2624/ 3200]\n",
      "loss: 0.052493  [ 2640/ 3200]\n",
      "loss: 0.049220  [ 2656/ 3200]\n",
      "loss: 0.053616  [ 2672/ 3200]\n",
      "loss: 0.097635  [ 2688/ 3200]\n",
      "loss: 0.059653  [ 2704/ 3200]\n",
      "loss: 0.070120  [ 2720/ 3200]\n",
      "loss: 0.084056  [ 2736/ 3200]\n",
      "loss: 0.060885  [ 2752/ 3200]\n",
      "loss: 0.021060  [ 2768/ 3200]\n",
      "loss: 0.066250  [ 2784/ 3200]\n",
      "loss: 0.027843  [ 2800/ 3200]\n",
      "loss: 0.108374  [ 2816/ 3200]\n",
      "loss: 0.181229  [ 2832/ 3200]\n",
      "loss: 0.066845  [ 2848/ 3200]\n",
      "loss: 0.027962  [ 2864/ 3200]\n",
      "loss: 0.059379  [ 2880/ 3200]\n",
      "loss: 0.082540  [ 2896/ 3200]\n",
      "loss: 0.014832  [ 2912/ 3200]\n",
      "loss: 0.010631  [ 2928/ 3200]\n",
      "loss: 0.014330  [ 2944/ 3200]\n",
      "loss: 0.040235  [ 2960/ 3200]\n",
      "loss: 0.060504  [ 2976/ 3200]\n",
      "loss: 0.029766  [ 2992/ 3200]\n",
      "loss: 0.045239  [ 3008/ 3200]\n",
      "loss: 0.033964  [ 3024/ 3200]\n",
      "loss: 0.078438  [ 3040/ 3200]\n",
      "loss: 0.029029  [ 3056/ 3200]\n",
      "loss: 0.041346  [ 3072/ 3200]\n",
      "loss: 0.039592  [ 3088/ 3200]\n",
      "loss: 0.050821  [ 3104/ 3200]\n",
      "loss: 0.077479  [ 3120/ 3200]\n",
      "loss: 0.167345  [ 3136/ 3200]\n",
      "loss: 0.016250  [ 3152/ 3200]\n",
      "loss: 0.075037  [ 3168/ 3200]\n",
      "loss: 0.139400  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.046532\n",
      "f1 macro averaged score: 0.769163\n",
      "Accuracy               : 77.1%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  11,   0,  15],\n",
      "        [ 17, 117,  21,  45],\n",
      "        [  0,  19, 163,  18],\n",
      "        [  8,  14,  15, 163]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.9888e-04.\n",
      "\n",
      "Epoch: 46\n",
      "-----------------------------\n",
      "loss: 0.047789  [    0/ 3200]\n",
      "loss: 0.015104  [   16/ 3200]\n",
      "loss: 0.081473  [   32/ 3200]\n",
      "loss: 0.032275  [   48/ 3200]\n",
      "loss: 0.072316  [   64/ 3200]\n",
      "loss: 0.025806  [   80/ 3200]\n",
      "loss: 0.049484  [   96/ 3200]\n",
      "loss: 0.097256  [  112/ 3200]\n",
      "loss: 0.086070  [  128/ 3200]\n",
      "loss: 0.060282  [  144/ 3200]\n",
      "loss: 0.044932  [  160/ 3200]\n",
      "loss: 0.075706  [  176/ 3200]\n",
      "loss: 0.066280  [  192/ 3200]\n",
      "loss: 0.062135  [  208/ 3200]\n",
      "loss: 0.067289  [  224/ 3200]\n",
      "loss: 0.066252  [  240/ 3200]\n",
      "loss: 0.172613  [  256/ 3200]\n",
      "loss: 0.037971  [  272/ 3200]\n",
      "loss: 0.032415  [  288/ 3200]\n",
      "loss: 0.039202  [  304/ 3200]\n",
      "loss: 0.065289  [  320/ 3200]\n",
      "loss: 0.033490  [  336/ 3200]\n",
      "loss: 0.058822  [  352/ 3200]\n",
      "loss: 0.024158  [  368/ 3200]\n",
      "loss: 0.042918  [  384/ 3200]\n",
      "loss: 0.057326  [  400/ 3200]\n",
      "loss: 0.013250  [  416/ 3200]\n",
      "loss: 0.020601  [  432/ 3200]\n",
      "loss: 0.021763  [  448/ 3200]\n",
      "loss: 0.031370  [  464/ 3200]\n",
      "loss: 0.042622  [  480/ 3200]\n",
      "loss: 0.045103  [  496/ 3200]\n",
      "loss: 0.195242  [  512/ 3200]\n",
      "loss: 0.248012  [  528/ 3200]\n",
      "loss: 0.040784  [  544/ 3200]\n",
      "loss: 0.038207  [  560/ 3200]\n",
      "loss: 0.044526  [  576/ 3200]\n",
      "loss: 0.022611  [  592/ 3200]\n",
      "loss: 0.045587  [  608/ 3200]\n",
      "loss: 0.014135  [  624/ 3200]\n",
      "loss: 0.165939  [  640/ 3200]\n",
      "loss: 0.035461  [  656/ 3200]\n",
      "loss: 0.032273  [  672/ 3200]\n",
      "loss: 0.133583  [  688/ 3200]\n",
      "loss: 0.053771  [  704/ 3200]\n",
      "loss: 0.363804  [  720/ 3200]\n",
      "loss: 0.049095  [  736/ 3200]\n",
      "loss: 0.023634  [  752/ 3200]\n",
      "loss: 0.066761  [  768/ 3200]\n",
      "loss: 0.069245  [  784/ 3200]\n",
      "loss: 0.026483  [  800/ 3200]\n",
      "loss: 0.017358  [  816/ 3200]\n",
      "loss: 0.022816  [  832/ 3200]\n",
      "loss: 0.062021  [  848/ 3200]\n",
      "loss: 0.007852  [  864/ 3200]\n",
      "loss: 0.016751  [  880/ 3200]\n",
      "loss: 0.006733  [  896/ 3200]\n",
      "loss: 0.087814  [  912/ 3200]\n",
      "loss: 0.059853  [  928/ 3200]\n",
      "loss: 0.037196  [  944/ 3200]\n",
      "loss: 0.039565  [  960/ 3200]\n",
      "loss: 0.044294  [  976/ 3200]\n",
      "loss: 0.039617  [  992/ 3200]\n",
      "loss: 0.025918  [ 1008/ 3200]\n",
      "loss: 0.169992  [ 1024/ 3200]\n",
      "loss: 0.074187  [ 1040/ 3200]\n",
      "loss: 0.050432  [ 1056/ 3200]\n",
      "loss: 0.065989  [ 1072/ 3200]\n",
      "loss: 0.041264  [ 1088/ 3200]\n",
      "loss: 0.053531  [ 1104/ 3200]\n",
      "loss: 0.023415  [ 1120/ 3200]\n",
      "loss: 0.044988  [ 1136/ 3200]\n",
      "loss: 0.024672  [ 1152/ 3200]\n",
      "loss: 0.408746  [ 1168/ 3200]\n",
      "loss: 0.015912  [ 1184/ 3200]\n",
      "loss: 0.030435  [ 1200/ 3200]\n",
      "loss: 0.078629  [ 1216/ 3200]\n",
      "loss: 0.022737  [ 1232/ 3200]\n",
      "loss: 0.024318  [ 1248/ 3200]\n",
      "loss: 0.030986  [ 1264/ 3200]\n",
      "loss: 0.024068  [ 1280/ 3200]\n",
      "loss: 0.021341  [ 1296/ 3200]\n",
      "loss: 0.086114  [ 1312/ 3200]\n",
      "loss: 0.074208  [ 1328/ 3200]\n",
      "loss: 0.210681  [ 1344/ 3200]\n",
      "loss: 0.023897  [ 1360/ 3200]\n",
      "loss: 0.044863  [ 1376/ 3200]\n",
      "loss: 0.019602  [ 1392/ 3200]\n",
      "loss: 0.050736  [ 1408/ 3200]\n",
      "loss: 0.017505  [ 1424/ 3200]\n",
      "loss: 0.022812  [ 1440/ 3200]\n",
      "loss: 0.014947  [ 1456/ 3200]\n",
      "loss: 0.025961  [ 1472/ 3200]\n",
      "loss: 0.082535  [ 1488/ 3200]\n",
      "loss: 0.060975  [ 1504/ 3200]\n",
      "loss: 0.014251  [ 1520/ 3200]\n",
      "loss: 0.062861  [ 1536/ 3200]\n",
      "loss: 0.067322  [ 1552/ 3200]\n",
      "loss: 0.055973  [ 1568/ 3200]\n",
      "loss: 0.100499  [ 1584/ 3200]\n",
      "loss: 0.179350  [ 1600/ 3200]\n",
      "loss: 0.150207  [ 1616/ 3200]\n",
      "loss: 0.090975  [ 1632/ 3200]\n",
      "loss: 0.052351  [ 1648/ 3200]\n",
      "loss: 0.031757  [ 1664/ 3200]\n",
      "loss: 0.039804  [ 1680/ 3200]\n",
      "loss: 0.041095  [ 1696/ 3200]\n",
      "loss: 0.015752  [ 1712/ 3200]\n",
      "loss: 0.088597  [ 1728/ 3200]\n",
      "loss: 0.031278  [ 1744/ 3200]\n",
      "loss: 0.019446  [ 1760/ 3200]\n",
      "loss: 0.055961  [ 1776/ 3200]\n",
      "loss: 0.054821  [ 1792/ 3200]\n",
      "loss: 0.052864  [ 1808/ 3200]\n",
      "loss: 0.104278  [ 1824/ 3200]\n",
      "loss: 0.031140  [ 1840/ 3200]\n",
      "loss: 0.083657  [ 1856/ 3200]\n",
      "loss: 0.017605  [ 1872/ 3200]\n",
      "loss: 0.023221  [ 1888/ 3200]\n",
      "loss: 0.053527  [ 1904/ 3200]\n",
      "loss: 0.098703  [ 1920/ 3200]\n",
      "loss: 0.057949  [ 1936/ 3200]\n",
      "loss: 0.074650  [ 1952/ 3200]\n",
      "loss: 0.014078  [ 1968/ 3200]\n",
      "loss: 0.054658  [ 1984/ 3200]\n",
      "loss: 0.043175  [ 2000/ 3200]\n",
      "loss: 0.048494  [ 2016/ 3200]\n",
      "loss: 0.053900  [ 2032/ 3200]\n",
      "loss: 0.066432  [ 2048/ 3200]\n",
      "loss: 0.080450  [ 2064/ 3200]\n",
      "loss: 0.044527  [ 2080/ 3200]\n",
      "loss: 0.025133  [ 2096/ 3200]\n",
      "loss: 0.009732  [ 2112/ 3200]\n",
      "loss: 0.021981  [ 2128/ 3200]\n",
      "loss: 0.050846  [ 2144/ 3200]\n",
      "loss: 0.120460  [ 2160/ 3200]\n",
      "loss: 0.035973  [ 2176/ 3200]\n",
      "loss: 0.037761  [ 2192/ 3200]\n",
      "loss: 0.010026  [ 2208/ 3200]\n",
      "loss: 0.009662  [ 2224/ 3200]\n",
      "loss: 0.036442  [ 2240/ 3200]\n",
      "loss: 0.052088  [ 2256/ 3200]\n",
      "loss: 0.065582  [ 2272/ 3200]\n",
      "loss: 0.020942  [ 2288/ 3200]\n",
      "loss: 0.083357  [ 2304/ 3200]\n",
      "loss: 0.073591  [ 2320/ 3200]\n",
      "loss: 0.036903  [ 2336/ 3200]\n",
      "loss: 0.027042  [ 2352/ 3200]\n",
      "loss: 0.123256  [ 2368/ 3200]\n",
      "loss: 0.038409  [ 2384/ 3200]\n",
      "loss: 0.061961  [ 2400/ 3200]\n",
      "loss: 0.016451  [ 2416/ 3200]\n",
      "loss: 0.064219  [ 2432/ 3200]\n",
      "loss: 0.108406  [ 2448/ 3200]\n",
      "loss: 0.201782  [ 2464/ 3200]\n",
      "loss: 0.174786  [ 2480/ 3200]\n",
      "loss: 0.082221  [ 2496/ 3200]\n",
      "loss: 0.061629  [ 2512/ 3200]\n",
      "loss: 0.039638  [ 2528/ 3200]\n",
      "loss: 0.042513  [ 2544/ 3200]\n",
      "loss: 0.119589  [ 2560/ 3200]\n",
      "loss: 0.065886  [ 2576/ 3200]\n",
      "loss: 0.061382  [ 2592/ 3200]\n",
      "loss: 0.027650  [ 2608/ 3200]\n",
      "loss: 0.227651  [ 2624/ 3200]\n",
      "loss: 0.018803  [ 2640/ 3200]\n",
      "loss: 0.025113  [ 2656/ 3200]\n",
      "loss: 0.094543  [ 2672/ 3200]\n",
      "loss: 0.166129  [ 2688/ 3200]\n",
      "loss: 0.024754  [ 2704/ 3200]\n",
      "loss: 0.073948  [ 2720/ 3200]\n",
      "loss: 0.060075  [ 2736/ 3200]\n",
      "loss: 0.026946  [ 2752/ 3200]\n",
      "loss: 0.041815  [ 2768/ 3200]\n",
      "loss: 0.013450  [ 2784/ 3200]\n",
      "loss: 0.050252  [ 2800/ 3200]\n",
      "loss: 0.036453  [ 2816/ 3200]\n",
      "loss: 0.030469  [ 2832/ 3200]\n",
      "loss: 0.076950  [ 2848/ 3200]\n",
      "loss: 0.057881  [ 2864/ 3200]\n",
      "loss: 0.061814  [ 2880/ 3200]\n",
      "loss: 0.077621  [ 2896/ 3200]\n",
      "loss: 0.022788  [ 2912/ 3200]\n",
      "loss: 0.049031  [ 2928/ 3200]\n",
      "loss: 0.002881  [ 2944/ 3200]\n",
      "loss: 0.014679  [ 2960/ 3200]\n",
      "loss: 0.028394  [ 2976/ 3200]\n",
      "loss: 0.030226  [ 2992/ 3200]\n",
      "loss: 0.204996  [ 3008/ 3200]\n",
      "loss: 0.021775  [ 3024/ 3200]\n",
      "loss: 0.074052  [ 3040/ 3200]\n",
      "loss: 0.041288  [ 3056/ 3200]\n",
      "loss: 0.053034  [ 3072/ 3200]\n",
      "loss: 0.038392  [ 3088/ 3200]\n",
      "loss: 0.030539  [ 3104/ 3200]\n",
      "loss: 0.132144  [ 3120/ 3200]\n",
      "loss: 0.182729  [ 3136/ 3200]\n",
      "loss: 0.015549  [ 3152/ 3200]\n",
      "loss: 0.082948  [ 3168/ 3200]\n",
      "loss: 0.026933  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.048255\n",
      "f1 macro averaged score: 0.772162\n",
      "Accuracy               : 77.4%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  14,   0,  14],\n",
      "        [ 16, 121,  20,  43],\n",
      "        [  1,  20, 164,  15],\n",
      "        [ 10,  14,  14, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.8894e-04.\n",
      "\n",
      "Epoch: 47\n",
      "-----------------------------\n",
      "loss: 0.009846  [    0/ 3200]\n",
      "loss: 0.064074  [   16/ 3200]\n",
      "loss: 0.038064  [   32/ 3200]\n",
      "loss: 0.070379  [   48/ 3200]\n",
      "loss: 0.120997  [   64/ 3200]\n",
      "loss: 0.044172  [   80/ 3200]\n",
      "loss: 0.109660  [   96/ 3200]\n",
      "loss: 0.085871  [  112/ 3200]\n",
      "loss: 0.024756  [  128/ 3200]\n",
      "loss: 0.054823  [  144/ 3200]\n",
      "loss: 0.052443  [  160/ 3200]\n",
      "loss: 0.051090  [  176/ 3200]\n",
      "loss: 0.038801  [  192/ 3200]\n",
      "loss: 0.014123  [  208/ 3200]\n",
      "loss: 0.013232  [  224/ 3200]\n",
      "loss: 0.035569  [  240/ 3200]\n",
      "loss: 0.040884  [  256/ 3200]\n",
      "loss: 0.046643  [  272/ 3200]\n",
      "loss: 0.075396  [  288/ 3200]\n",
      "loss: 0.011299  [  304/ 3200]\n",
      "loss: 0.028702  [  320/ 3200]\n",
      "loss: 0.029047  [  336/ 3200]\n",
      "loss: 0.039480  [  352/ 3200]\n",
      "loss: 0.018958  [  368/ 3200]\n",
      "loss: 0.025667  [  384/ 3200]\n",
      "loss: 0.017653  [  400/ 3200]\n",
      "loss: 0.027816  [  416/ 3200]\n",
      "loss: 0.063871  [  432/ 3200]\n",
      "loss: 0.017285  [  448/ 3200]\n",
      "loss: 0.218542  [  464/ 3200]\n",
      "loss: 0.072125  [  480/ 3200]\n",
      "loss: 0.034233  [  496/ 3200]\n",
      "loss: 0.030065  [  512/ 3200]\n",
      "loss: 0.116767  [  528/ 3200]\n",
      "loss: 0.129729  [  544/ 3200]\n",
      "loss: 0.020715  [  560/ 3200]\n",
      "loss: 0.022576  [  576/ 3200]\n",
      "loss: 0.058822  [  592/ 3200]\n",
      "loss: 0.044186  [  608/ 3200]\n",
      "loss: 0.069484  [  624/ 3200]\n",
      "loss: 0.120088  [  640/ 3200]\n",
      "loss: 0.028641  [  656/ 3200]\n",
      "loss: 0.053574  [  672/ 3200]\n",
      "loss: 0.086902  [  688/ 3200]\n",
      "loss: 0.196892  [  704/ 3200]\n",
      "loss: 0.055649  [  720/ 3200]\n",
      "loss: 0.027685  [  736/ 3200]\n",
      "loss: 0.047732  [  752/ 3200]\n",
      "loss: 0.250517  [  768/ 3200]\n",
      "loss: 0.063645  [  784/ 3200]\n",
      "loss: 0.080504  [  800/ 3200]\n",
      "loss: 0.041890  [  816/ 3200]\n",
      "loss: 0.037993  [  832/ 3200]\n",
      "loss: 0.022442  [  848/ 3200]\n",
      "loss: 0.019596  [  864/ 3200]\n",
      "loss: 0.050680  [  880/ 3200]\n",
      "loss: 0.028567  [  896/ 3200]\n",
      "loss: 0.068106  [  912/ 3200]\n",
      "loss: 0.035092  [  928/ 3200]\n",
      "loss: 0.081081  [  944/ 3200]\n",
      "loss: 0.017574  [  960/ 3200]\n",
      "loss: 0.084627  [  976/ 3200]\n",
      "loss: 0.040108  [  992/ 3200]\n",
      "loss: 0.020280  [ 1008/ 3200]\n",
      "loss: 0.018911  [ 1024/ 3200]\n",
      "loss: 0.028781  [ 1040/ 3200]\n",
      "loss: 0.022527  [ 1056/ 3200]\n",
      "loss: 0.106835  [ 1072/ 3200]\n",
      "loss: 0.382488  [ 1088/ 3200]\n",
      "loss: 0.044948  [ 1104/ 3200]\n",
      "loss: 0.035160  [ 1120/ 3200]\n",
      "loss: 0.047140  [ 1136/ 3200]\n",
      "loss: 0.115686  [ 1152/ 3200]\n",
      "loss: 0.052862  [ 1168/ 3200]\n",
      "loss: 0.065881  [ 1184/ 3200]\n",
      "loss: 0.025237  [ 1200/ 3200]\n",
      "loss: 0.046177  [ 1216/ 3200]\n",
      "loss: 0.038007  [ 1232/ 3200]\n",
      "loss: 0.029759  [ 1248/ 3200]\n",
      "loss: 0.025183  [ 1264/ 3200]\n",
      "loss: 0.044673  [ 1280/ 3200]\n",
      "loss: 0.150130  [ 1296/ 3200]\n",
      "loss: 0.063217  [ 1312/ 3200]\n",
      "loss: 0.047118  [ 1328/ 3200]\n",
      "loss: 0.029114  [ 1344/ 3200]\n",
      "loss: 0.023820  [ 1360/ 3200]\n",
      "loss: 0.017555  [ 1376/ 3200]\n",
      "loss: 0.137592  [ 1392/ 3200]\n",
      "loss: 0.078284  [ 1408/ 3200]\n",
      "loss: 0.035857  [ 1424/ 3200]\n",
      "loss: 0.035616  [ 1440/ 3200]\n",
      "loss: 0.132292  [ 1456/ 3200]\n",
      "loss: 0.062025  [ 1472/ 3200]\n",
      "loss: 0.072054  [ 1488/ 3200]\n",
      "loss: 0.047023  [ 1504/ 3200]\n",
      "loss: 0.057261  [ 1520/ 3200]\n",
      "loss: 0.077616  [ 1536/ 3200]\n",
      "loss: 0.140945  [ 1552/ 3200]\n",
      "loss: 0.164997  [ 1568/ 3200]\n",
      "loss: 0.023393  [ 1584/ 3200]\n",
      "loss: 0.081079  [ 1600/ 3200]\n",
      "loss: 0.042879  [ 1616/ 3200]\n",
      "loss: 0.021728  [ 1632/ 3200]\n",
      "loss: 0.049133  [ 1648/ 3200]\n",
      "loss: 0.086692  [ 1664/ 3200]\n",
      "loss: 0.041665  [ 1680/ 3200]\n",
      "loss: 0.035495  [ 1696/ 3200]\n",
      "loss: 0.048656  [ 1712/ 3200]\n",
      "loss: 0.011292  [ 1728/ 3200]\n",
      "loss: 0.022413  [ 1744/ 3200]\n",
      "loss: 0.012139  [ 1760/ 3200]\n",
      "loss: 0.032876  [ 1776/ 3200]\n",
      "loss: 0.020622  [ 1792/ 3200]\n",
      "loss: 0.020289  [ 1808/ 3200]\n",
      "loss: 0.035515  [ 1824/ 3200]\n",
      "loss: 0.067541  [ 1840/ 3200]\n",
      "loss: 0.025755  [ 1856/ 3200]\n",
      "loss: 0.023927  [ 1872/ 3200]\n",
      "loss: 0.113287  [ 1888/ 3200]\n",
      "loss: 0.044869  [ 1904/ 3200]\n",
      "loss: 0.023154  [ 1920/ 3200]\n",
      "loss: 0.044869  [ 1936/ 3200]\n",
      "loss: 0.076352  [ 1952/ 3200]\n",
      "loss: 0.015028  [ 1968/ 3200]\n",
      "loss: 0.011308  [ 1984/ 3200]\n",
      "loss: 0.058051  [ 2000/ 3200]\n",
      "loss: 0.151194  [ 2016/ 3200]\n",
      "loss: 0.117693  [ 2032/ 3200]\n",
      "loss: 0.020580  [ 2048/ 3200]\n",
      "loss: 0.280526  [ 2064/ 3200]\n",
      "loss: 0.057828  [ 2080/ 3200]\n",
      "loss: 0.041372  [ 2096/ 3200]\n",
      "loss: 0.024665  [ 2112/ 3200]\n",
      "loss: 0.010342  [ 2128/ 3200]\n",
      "loss: 0.110547  [ 2144/ 3200]\n",
      "loss: 0.038886  [ 2160/ 3200]\n",
      "loss: 0.079248  [ 2176/ 3200]\n",
      "loss: 0.136960  [ 2192/ 3200]\n",
      "loss: 0.195073  [ 2208/ 3200]\n",
      "loss: 0.064147  [ 2224/ 3200]\n",
      "loss: 0.067895  [ 2240/ 3200]\n",
      "loss: 0.008275  [ 2256/ 3200]\n",
      "loss: 0.045142  [ 2272/ 3200]\n",
      "loss: 0.089350  [ 2288/ 3200]\n",
      "loss: 0.035465  [ 2304/ 3200]\n",
      "loss: 0.048778  [ 2320/ 3200]\n",
      "loss: 0.079604  [ 2336/ 3200]\n",
      "loss: 0.035395  [ 2352/ 3200]\n",
      "loss: 0.027207  [ 2368/ 3200]\n",
      "loss: 0.012216  [ 2384/ 3200]\n",
      "loss: 0.103909  [ 2400/ 3200]\n",
      "loss: 0.119491  [ 2416/ 3200]\n",
      "loss: 0.126730  [ 2432/ 3200]\n",
      "loss: 0.013930  [ 2448/ 3200]\n",
      "loss: 0.131472  [ 2464/ 3200]\n",
      "loss: 0.043324  [ 2480/ 3200]\n",
      "loss: 0.060296  [ 2496/ 3200]\n",
      "loss: 0.088967  [ 2512/ 3200]\n",
      "loss: 0.129117  [ 2528/ 3200]\n",
      "loss: 0.022073  [ 2544/ 3200]\n",
      "loss: 0.034968  [ 2560/ 3200]\n",
      "loss: 0.046793  [ 2576/ 3200]\n",
      "loss: 0.095497  [ 2592/ 3200]\n",
      "loss: 0.033234  [ 2608/ 3200]\n",
      "loss: 0.075117  [ 2624/ 3200]\n",
      "loss: 0.013394  [ 2640/ 3200]\n",
      "loss: 0.023396  [ 2656/ 3200]\n",
      "loss: 0.155199  [ 2672/ 3200]\n",
      "loss: 0.056716  [ 2688/ 3200]\n",
      "loss: 0.033351  [ 2704/ 3200]\n",
      "loss: 0.087998  [ 2720/ 3200]\n",
      "loss: 0.045709  [ 2736/ 3200]\n",
      "loss: 0.063873  [ 2752/ 3200]\n",
      "loss: 0.052301  [ 2768/ 3200]\n",
      "loss: 0.106227  [ 2784/ 3200]\n",
      "loss: 0.024512  [ 2800/ 3200]\n",
      "loss: 0.025184  [ 2816/ 3200]\n",
      "loss: 0.070334  [ 2832/ 3200]\n",
      "loss: 0.041891  [ 2848/ 3200]\n",
      "loss: 0.208717  [ 2864/ 3200]\n",
      "loss: 0.071303  [ 2880/ 3200]\n",
      "loss: 0.009407  [ 2896/ 3200]\n",
      "loss: 0.037461  [ 2912/ 3200]\n",
      "loss: 0.034890  [ 2928/ 3200]\n",
      "loss: 0.044797  [ 2944/ 3200]\n",
      "loss: 0.111688  [ 2960/ 3200]\n",
      "loss: 0.122153  [ 2976/ 3200]\n",
      "loss: 0.012960  [ 2992/ 3200]\n",
      "loss: 0.095086  [ 3008/ 3200]\n",
      "loss: 0.056853  [ 3024/ 3200]\n",
      "loss: 0.085668  [ 3040/ 3200]\n",
      "loss: 0.177183  [ 3056/ 3200]\n",
      "loss: 0.025968  [ 3072/ 3200]\n",
      "loss: 0.150673  [ 3088/ 3200]\n",
      "loss: 0.036992  [ 3104/ 3200]\n",
      "loss: 0.047890  [ 3120/ 3200]\n",
      "loss: 0.125763  [ 3136/ 3200]\n",
      "loss: 0.027317  [ 3152/ 3200]\n",
      "loss: 0.049418  [ 3168/ 3200]\n",
      "loss: 0.026138  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.048392\n",
      "f1 macro averaged score: 0.771100\n",
      "Accuracy               : 77.2%\n",
      "Confusion matrix       :\n",
      "tensor([[176,   8,   0,  16],\n",
      "        [ 18, 121,  20,  41],\n",
      "        [  1,  25, 157,  17],\n",
      "        [  6,  18,  12, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.7949e-04.\n",
      "\n",
      "Epoch: 48\n",
      "-----------------------------\n",
      "loss: 0.109918  [    0/ 3200]\n",
      "loss: 0.042234  [   16/ 3200]\n",
      "loss: 0.034325  [   32/ 3200]\n",
      "loss: 0.055310  [   48/ 3200]\n",
      "loss: 0.071283  [   64/ 3200]\n",
      "loss: 0.040641  [   80/ 3200]\n",
      "loss: 0.016864  [   96/ 3200]\n",
      "loss: 0.074309  [  112/ 3200]\n",
      "loss: 0.041689  [  128/ 3200]\n",
      "loss: 0.048425  [  144/ 3200]\n",
      "loss: 0.025975  [  160/ 3200]\n",
      "loss: 0.035707  [  176/ 3200]\n",
      "loss: 0.066856  [  192/ 3200]\n",
      "loss: 0.056247  [  208/ 3200]\n",
      "loss: 0.142156  [  224/ 3200]\n",
      "loss: 0.023540  [  240/ 3200]\n",
      "loss: 0.069083  [  256/ 3200]\n",
      "loss: 0.095933  [  272/ 3200]\n",
      "loss: 0.014198  [  288/ 3200]\n",
      "loss: 0.012004  [  304/ 3200]\n",
      "loss: 0.033151  [  320/ 3200]\n",
      "loss: 0.024703  [  336/ 3200]\n",
      "loss: 0.033042  [  352/ 3200]\n",
      "loss: 0.025218  [  368/ 3200]\n",
      "loss: 0.031205  [  384/ 3200]\n",
      "loss: 0.038005  [  400/ 3200]\n",
      "loss: 0.016087  [  416/ 3200]\n",
      "loss: 0.157209  [  432/ 3200]\n",
      "loss: 0.036001  [  448/ 3200]\n",
      "loss: 0.033869  [  464/ 3200]\n",
      "loss: 0.105774  [  480/ 3200]\n",
      "loss: 0.022424  [  496/ 3200]\n",
      "loss: 0.061660  [  512/ 3200]\n",
      "loss: 0.031530  [  528/ 3200]\n",
      "loss: 0.047737  [  544/ 3200]\n",
      "loss: 0.073378  [  560/ 3200]\n",
      "loss: 0.075492  [  576/ 3200]\n",
      "loss: 0.036196  [  592/ 3200]\n",
      "loss: 0.035966  [  608/ 3200]\n",
      "loss: 0.036768  [  624/ 3200]\n",
      "loss: 0.042784  [  640/ 3200]\n",
      "loss: 0.039570  [  656/ 3200]\n",
      "loss: 0.153254  [  672/ 3200]\n",
      "loss: 0.043707  [  688/ 3200]\n",
      "loss: 0.056334  [  704/ 3200]\n",
      "loss: 0.011598  [  720/ 3200]\n",
      "loss: 0.026877  [  736/ 3200]\n",
      "loss: 0.043698  [  752/ 3200]\n",
      "loss: 0.133751  [  768/ 3200]\n",
      "loss: 0.011880  [  784/ 3200]\n",
      "loss: 0.024675  [  800/ 3200]\n",
      "loss: 0.029131  [  816/ 3200]\n",
      "loss: 0.050805  [  832/ 3200]\n",
      "loss: 0.061502  [  848/ 3200]\n",
      "loss: 0.027599  [  864/ 3200]\n",
      "loss: 0.019082  [  880/ 3200]\n",
      "loss: 0.077493  [  896/ 3200]\n",
      "loss: 0.046339  [  912/ 3200]\n",
      "loss: 0.053786  [  928/ 3200]\n",
      "loss: 0.035825  [  944/ 3200]\n",
      "loss: 0.007896  [  960/ 3200]\n",
      "loss: 0.069407  [  976/ 3200]\n",
      "loss: 0.014608  [  992/ 3200]\n",
      "loss: 0.100135  [ 1008/ 3200]\n",
      "loss: 0.022419  [ 1024/ 3200]\n",
      "loss: 0.047383  [ 1040/ 3200]\n",
      "loss: 0.023310  [ 1056/ 3200]\n",
      "loss: 0.027644  [ 1072/ 3200]\n",
      "loss: 0.028816  [ 1088/ 3200]\n",
      "loss: 0.011288  [ 1104/ 3200]\n",
      "loss: 0.191970  [ 1120/ 3200]\n",
      "loss: 0.159126  [ 1136/ 3200]\n",
      "loss: 0.084455  [ 1152/ 3200]\n",
      "loss: 0.180397  [ 1168/ 3200]\n",
      "loss: 0.015772  [ 1184/ 3200]\n",
      "loss: 0.061736  [ 1200/ 3200]\n",
      "loss: 0.010145  [ 1216/ 3200]\n",
      "loss: 0.018181  [ 1232/ 3200]\n",
      "loss: 0.066567  [ 1248/ 3200]\n",
      "loss: 0.129915  [ 1264/ 3200]\n",
      "loss: 0.050137  [ 1280/ 3200]\n",
      "loss: 0.107721  [ 1296/ 3200]\n",
      "loss: 0.042425  [ 1312/ 3200]\n",
      "loss: 0.008748  [ 1328/ 3200]\n",
      "loss: 0.066602  [ 1344/ 3200]\n",
      "loss: 0.033436  [ 1360/ 3200]\n",
      "loss: 0.205170  [ 1376/ 3200]\n",
      "loss: 0.067979  [ 1392/ 3200]\n",
      "loss: 0.020151  [ 1408/ 3200]\n",
      "loss: 0.034195  [ 1424/ 3200]\n",
      "loss: 0.171431  [ 1440/ 3200]\n",
      "loss: 0.053227  [ 1456/ 3200]\n",
      "loss: 0.014904  [ 1472/ 3200]\n",
      "loss: 0.061374  [ 1488/ 3200]\n",
      "loss: 0.073491  [ 1504/ 3200]\n",
      "loss: 0.068371  [ 1520/ 3200]\n",
      "loss: 0.065807  [ 1536/ 3200]\n",
      "loss: 0.145395  [ 1552/ 3200]\n",
      "loss: 0.045594  [ 1568/ 3200]\n",
      "loss: 0.045737  [ 1584/ 3200]\n",
      "loss: 0.018995  [ 1600/ 3200]\n",
      "loss: 0.059744  [ 1616/ 3200]\n",
      "loss: 0.033351  [ 1632/ 3200]\n",
      "loss: 0.029087  [ 1648/ 3200]\n",
      "loss: 0.042992  [ 1664/ 3200]\n",
      "loss: 0.019313  [ 1680/ 3200]\n",
      "loss: 0.085565  [ 1696/ 3200]\n",
      "loss: 0.102006  [ 1712/ 3200]\n",
      "loss: 0.109093  [ 1728/ 3200]\n",
      "loss: 0.222689  [ 1744/ 3200]\n",
      "loss: 0.068645  [ 1760/ 3200]\n",
      "loss: 0.066870  [ 1776/ 3200]\n",
      "loss: 0.059286  [ 1792/ 3200]\n",
      "loss: 0.051447  [ 1808/ 3200]\n",
      "loss: 0.028919  [ 1824/ 3200]\n",
      "loss: 0.026720  [ 1840/ 3200]\n",
      "loss: 0.039467  [ 1856/ 3200]\n",
      "loss: 0.045752  [ 1872/ 3200]\n",
      "loss: 0.292577  [ 1888/ 3200]\n",
      "loss: 0.030591  [ 1904/ 3200]\n",
      "loss: 0.036173  [ 1920/ 3200]\n",
      "loss: 0.037906  [ 1936/ 3200]\n",
      "loss: 0.037961  [ 1952/ 3200]\n",
      "loss: 0.016322  [ 1968/ 3200]\n",
      "loss: 0.186886  [ 1984/ 3200]\n",
      "loss: 0.024089  [ 2000/ 3200]\n",
      "loss: 0.056707  [ 2016/ 3200]\n",
      "loss: 0.044174  [ 2032/ 3200]\n",
      "loss: 0.051247  [ 2048/ 3200]\n",
      "loss: 0.159205  [ 2064/ 3200]\n",
      "loss: 0.027388  [ 2080/ 3200]\n",
      "loss: 0.041762  [ 2096/ 3200]\n",
      "loss: 0.045624  [ 2112/ 3200]\n",
      "loss: 0.020040  [ 2128/ 3200]\n",
      "loss: 0.035110  [ 2144/ 3200]\n",
      "loss: 0.022822  [ 2160/ 3200]\n",
      "loss: 0.037208  [ 2176/ 3200]\n",
      "loss: 0.098131  [ 2192/ 3200]\n",
      "loss: 0.052216  [ 2208/ 3200]\n",
      "loss: 0.034253  [ 2224/ 3200]\n",
      "loss: 0.096984  [ 2240/ 3200]\n",
      "loss: 0.097925  [ 2256/ 3200]\n",
      "loss: 0.046699  [ 2272/ 3200]\n",
      "loss: 0.037303  [ 2288/ 3200]\n",
      "loss: 0.019418  [ 2304/ 3200]\n",
      "loss: 0.151181  [ 2320/ 3200]\n",
      "loss: 0.052142  [ 2336/ 3200]\n",
      "loss: 0.020866  [ 2352/ 3200]\n",
      "loss: 0.086408  [ 2368/ 3200]\n",
      "loss: 0.013945  [ 2384/ 3200]\n",
      "loss: 0.208912  [ 2400/ 3200]\n",
      "loss: 0.062068  [ 2416/ 3200]\n",
      "loss: 0.050760  [ 2432/ 3200]\n",
      "loss: 0.055342  [ 2448/ 3200]\n",
      "loss: 0.028596  [ 2464/ 3200]\n",
      "loss: 0.018037  [ 2480/ 3200]\n",
      "loss: 0.073109  [ 2496/ 3200]\n",
      "loss: 0.221902  [ 2512/ 3200]\n",
      "loss: 0.011838  [ 2528/ 3200]\n",
      "loss: 0.035391  [ 2544/ 3200]\n",
      "loss: 0.078795  [ 2560/ 3200]\n",
      "loss: 0.070338  [ 2576/ 3200]\n",
      "loss: 0.072681  [ 2592/ 3200]\n",
      "loss: 0.064110  [ 2608/ 3200]\n",
      "loss: 0.018836  [ 2624/ 3200]\n",
      "loss: 0.029295  [ 2640/ 3200]\n",
      "loss: 0.139813  [ 2656/ 3200]\n",
      "loss: 0.044614  [ 2672/ 3200]\n",
      "loss: 0.052846  [ 2688/ 3200]\n",
      "loss: 0.032151  [ 2704/ 3200]\n",
      "loss: 0.087034  [ 2720/ 3200]\n",
      "loss: 0.039545  [ 2736/ 3200]\n",
      "loss: 0.013883  [ 2752/ 3200]\n",
      "loss: 0.035220  [ 2768/ 3200]\n",
      "loss: 0.071746  [ 2784/ 3200]\n",
      "loss: 0.018368  [ 2800/ 3200]\n",
      "loss: 0.035712  [ 2816/ 3200]\n",
      "loss: 0.043124  [ 2832/ 3200]\n",
      "loss: 0.165341  [ 2848/ 3200]\n",
      "loss: 0.067402  [ 2864/ 3200]\n",
      "loss: 0.411665  [ 2880/ 3200]\n",
      "loss: 0.026644  [ 2896/ 3200]\n",
      "loss: 0.102938  [ 2912/ 3200]\n",
      "loss: 0.071566  [ 2928/ 3200]\n",
      "loss: 0.038823  [ 2944/ 3200]\n",
      "loss: 0.120027  [ 2960/ 3200]\n",
      "loss: 0.051460  [ 2976/ 3200]\n",
      "loss: 0.044187  [ 2992/ 3200]\n",
      "loss: 0.036575  [ 3008/ 3200]\n",
      "loss: 0.062393  [ 3024/ 3200]\n",
      "loss: 0.083360  [ 3040/ 3200]\n",
      "loss: 0.027958  [ 3056/ 3200]\n",
      "loss: 0.021781  [ 3072/ 3200]\n",
      "loss: 0.037790  [ 3088/ 3200]\n",
      "loss: 0.012330  [ 3104/ 3200]\n",
      "loss: 0.030275  [ 3120/ 3200]\n",
      "loss: 0.011223  [ 3136/ 3200]\n",
      "loss: 0.057868  [ 3152/ 3200]\n",
      "loss: 0.159171  [ 3168/ 3200]\n",
      "loss: 0.045633  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.049169\n",
      "f1 macro averaged score: 0.785620\n",
      "Accuracy               : 78.6%\n",
      "Confusion matrix       :\n",
      "tensor([[173,   9,   0,  18],\n",
      "        [ 14, 129,  20,  37],\n",
      "        [  0,  20, 163,  17],\n",
      "        [ 10,  13,  13, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.7052e-04.\n",
      "\n",
      "Epoch: 49\n",
      "-----------------------------\n",
      "loss: 0.018852  [    0/ 3200]\n",
      "loss: 0.038178  [   16/ 3200]\n",
      "loss: 0.085963  [   32/ 3200]\n",
      "loss: 0.086210  [   48/ 3200]\n",
      "loss: 0.110547  [   64/ 3200]\n",
      "loss: 0.071242  [   80/ 3200]\n",
      "loss: 0.036122  [   96/ 3200]\n",
      "loss: 0.014850  [  112/ 3200]\n",
      "loss: 0.014849  [  128/ 3200]\n",
      "loss: 0.054754  [  144/ 3200]\n",
      "loss: 0.061588  [  160/ 3200]\n",
      "loss: 0.024247  [  176/ 3200]\n",
      "loss: 0.048813  [  192/ 3200]\n",
      "loss: 0.080785  [  208/ 3200]\n",
      "loss: 0.079711  [  224/ 3200]\n",
      "loss: 0.029733  [  240/ 3200]\n",
      "loss: 0.056552  [  256/ 3200]\n",
      "loss: 0.054865  [  272/ 3200]\n",
      "loss: 0.126460  [  288/ 3200]\n",
      "loss: 0.050018  [  304/ 3200]\n",
      "loss: 0.110825  [  320/ 3200]\n",
      "loss: 0.018970  [  336/ 3200]\n",
      "loss: 0.011627  [  352/ 3200]\n",
      "loss: 0.035085  [  368/ 3200]\n",
      "loss: 0.034838  [  384/ 3200]\n",
      "loss: 0.075738  [  400/ 3200]\n",
      "loss: 0.177339  [  416/ 3200]\n",
      "loss: 0.028737  [  432/ 3200]\n",
      "loss: 0.031487  [  448/ 3200]\n",
      "loss: 0.134938  [  464/ 3200]\n",
      "loss: 0.010377  [  480/ 3200]\n",
      "loss: 0.044089  [  496/ 3200]\n",
      "loss: 0.078890  [  512/ 3200]\n",
      "loss: 0.052365  [  528/ 3200]\n",
      "loss: 0.031006  [  544/ 3200]\n",
      "loss: 0.073597  [  560/ 3200]\n",
      "loss: 0.014245  [  576/ 3200]\n",
      "loss: 0.123477  [  592/ 3200]\n",
      "loss: 0.025603  [  608/ 3200]\n",
      "loss: 0.027779  [  624/ 3200]\n",
      "loss: 0.045312  [  640/ 3200]\n",
      "loss: 0.018701  [  656/ 3200]\n",
      "loss: 0.175249  [  672/ 3200]\n",
      "loss: 0.055417  [  688/ 3200]\n",
      "loss: 0.070527  [  704/ 3200]\n",
      "loss: 0.034351  [  720/ 3200]\n",
      "loss: 0.080571  [  736/ 3200]\n",
      "loss: 0.444947  [  752/ 3200]\n",
      "loss: 0.014815  [  768/ 3200]\n",
      "loss: 0.051267  [  784/ 3200]\n",
      "loss: 0.014746  [  800/ 3200]\n",
      "loss: 0.034084  [  816/ 3200]\n",
      "loss: 0.025594  [  832/ 3200]\n",
      "loss: 0.063620  [  848/ 3200]\n",
      "loss: 0.081203  [  864/ 3200]\n",
      "loss: 0.056297  [  880/ 3200]\n",
      "loss: 0.044719  [  896/ 3200]\n",
      "loss: 0.024293  [  912/ 3200]\n",
      "loss: 0.107618  [  928/ 3200]\n",
      "loss: 0.084346  [  944/ 3200]\n",
      "loss: 0.197891  [  960/ 3200]\n",
      "loss: 0.019069  [  976/ 3200]\n",
      "loss: 0.108298  [  992/ 3200]\n",
      "loss: 0.082658  [ 1008/ 3200]\n",
      "loss: 0.038419  [ 1024/ 3200]\n",
      "loss: 0.006548  [ 1040/ 3200]\n",
      "loss: 0.054050  [ 1056/ 3200]\n",
      "loss: 0.113840  [ 1072/ 3200]\n",
      "loss: 0.105027  [ 1088/ 3200]\n",
      "loss: 0.035234  [ 1104/ 3200]\n",
      "loss: 0.032323  [ 1120/ 3200]\n",
      "loss: 0.008000  [ 1136/ 3200]\n",
      "loss: 0.042195  [ 1152/ 3200]\n",
      "loss: 0.045423  [ 1168/ 3200]\n",
      "loss: 0.073848  [ 1184/ 3200]\n",
      "loss: 0.029510  [ 1200/ 3200]\n",
      "loss: 0.026846  [ 1216/ 3200]\n",
      "loss: 0.051095  [ 1232/ 3200]\n",
      "loss: 0.087531  [ 1248/ 3200]\n",
      "loss: 0.074066  [ 1264/ 3200]\n",
      "loss: 0.125575  [ 1280/ 3200]\n",
      "loss: 0.044376  [ 1296/ 3200]\n",
      "loss: 0.071751  [ 1312/ 3200]\n",
      "loss: 0.095331  [ 1328/ 3200]\n",
      "loss: 0.014511  [ 1344/ 3200]\n",
      "loss: 0.056549  [ 1360/ 3200]\n",
      "loss: 0.123561  [ 1376/ 3200]\n",
      "loss: 0.175383  [ 1392/ 3200]\n",
      "loss: 0.128093  [ 1408/ 3200]\n",
      "loss: 0.078358  [ 1424/ 3200]\n",
      "loss: 0.121948  [ 1440/ 3200]\n",
      "loss: 0.022461  [ 1456/ 3200]\n",
      "loss: 0.028315  [ 1472/ 3200]\n",
      "loss: 0.031429  [ 1488/ 3200]\n",
      "loss: 0.035540  [ 1504/ 3200]\n",
      "loss: 0.024257  [ 1520/ 3200]\n",
      "loss: 0.056418  [ 1536/ 3200]\n",
      "loss: 0.033670  [ 1552/ 3200]\n",
      "loss: 0.050500  [ 1568/ 3200]\n",
      "loss: 0.213614  [ 1584/ 3200]\n",
      "loss: 0.058499  [ 1600/ 3200]\n",
      "loss: 0.066164  [ 1616/ 3200]\n",
      "loss: 0.034184  [ 1632/ 3200]\n",
      "loss: 0.073968  [ 1648/ 3200]\n",
      "loss: 0.061414  [ 1664/ 3200]\n",
      "loss: 0.164858  [ 1680/ 3200]\n",
      "loss: 0.044841  [ 1696/ 3200]\n",
      "loss: 0.016331  [ 1712/ 3200]\n",
      "loss: 0.020755  [ 1728/ 3200]\n",
      "loss: 0.117461  [ 1744/ 3200]\n",
      "loss: 0.081734  [ 1760/ 3200]\n",
      "loss: 0.085418  [ 1776/ 3200]\n",
      "loss: 0.076921  [ 1792/ 3200]\n",
      "loss: 0.006350  [ 1808/ 3200]\n",
      "loss: 0.040023  [ 1824/ 3200]\n",
      "loss: 0.010973  [ 1840/ 3200]\n",
      "loss: 0.008746  [ 1856/ 3200]\n",
      "loss: 0.084149  [ 1872/ 3200]\n",
      "loss: 0.068926  [ 1888/ 3200]\n",
      "loss: 0.039124  [ 1904/ 3200]\n",
      "loss: 0.106780  [ 1920/ 3200]\n",
      "loss: 0.049532  [ 1936/ 3200]\n",
      "loss: 0.039031  [ 1952/ 3200]\n",
      "loss: 0.128448  [ 1968/ 3200]\n",
      "loss: 0.175177  [ 1984/ 3200]\n",
      "loss: 0.022187  [ 2000/ 3200]\n",
      "loss: 0.054040  [ 2016/ 3200]\n",
      "loss: 0.055946  [ 2032/ 3200]\n",
      "loss: 0.066524  [ 2048/ 3200]\n",
      "loss: 0.068894  [ 2064/ 3200]\n",
      "loss: 0.095683  [ 2080/ 3200]\n",
      "loss: 0.086103  [ 2096/ 3200]\n",
      "loss: 0.012627  [ 2112/ 3200]\n",
      "loss: 0.037516  [ 2128/ 3200]\n",
      "loss: 0.039169  [ 2144/ 3200]\n",
      "loss: 0.063654  [ 2160/ 3200]\n",
      "loss: 0.044050  [ 2176/ 3200]\n",
      "loss: 0.013673  [ 2192/ 3200]\n",
      "loss: 0.039789  [ 2208/ 3200]\n",
      "loss: 0.015958  [ 2224/ 3200]\n",
      "loss: 0.089587  [ 2240/ 3200]\n",
      "loss: 0.023777  [ 2256/ 3200]\n",
      "loss: 0.205699  [ 2272/ 3200]\n",
      "loss: 0.022797  [ 2288/ 3200]\n",
      "loss: 0.120858  [ 2304/ 3200]\n",
      "loss: 0.203285  [ 2320/ 3200]\n",
      "loss: 0.052494  [ 2336/ 3200]\n",
      "loss: 0.031309  [ 2352/ 3200]\n",
      "loss: 0.021739  [ 2368/ 3200]\n",
      "loss: 0.103340  [ 2384/ 3200]\n",
      "loss: 0.062502  [ 2400/ 3200]\n",
      "loss: 0.034098  [ 2416/ 3200]\n",
      "loss: 0.099503  [ 2432/ 3200]\n",
      "loss: 0.025809  [ 2448/ 3200]\n",
      "loss: 0.028487  [ 2464/ 3200]\n",
      "loss: 0.040022  [ 2480/ 3200]\n",
      "loss: 0.110834  [ 2496/ 3200]\n",
      "loss: 0.009534  [ 2512/ 3200]\n",
      "loss: 0.172517  [ 2528/ 3200]\n",
      "loss: 0.041345  [ 2544/ 3200]\n",
      "loss: 0.026555  [ 2560/ 3200]\n",
      "loss: 0.015956  [ 2576/ 3200]\n",
      "loss: 0.041711  [ 2592/ 3200]\n",
      "loss: 0.080820  [ 2608/ 3200]\n",
      "loss: 0.078501  [ 2624/ 3200]\n",
      "loss: 0.090337  [ 2640/ 3200]\n",
      "loss: 0.063680  [ 2656/ 3200]\n",
      "loss: 0.043946  [ 2672/ 3200]\n",
      "loss: 0.030794  [ 2688/ 3200]\n",
      "loss: 0.029119  [ 2704/ 3200]\n",
      "loss: 0.019537  [ 2720/ 3200]\n",
      "loss: 0.021344  [ 2736/ 3200]\n",
      "loss: 0.096700  [ 2752/ 3200]\n",
      "loss: 0.007142  [ 2768/ 3200]\n",
      "loss: 0.243873  [ 2784/ 3200]\n",
      "loss: 0.009883  [ 2800/ 3200]\n",
      "loss: 0.129870  [ 2816/ 3200]\n",
      "loss: 0.013699  [ 2832/ 3200]\n",
      "loss: 0.073440  [ 2848/ 3200]\n",
      "loss: 0.064124  [ 2864/ 3200]\n",
      "loss: 0.047212  [ 2880/ 3200]\n",
      "loss: 0.022094  [ 2896/ 3200]\n",
      "loss: 0.131888  [ 2912/ 3200]\n",
      "loss: 0.167690  [ 2928/ 3200]\n",
      "loss: 0.072818  [ 2944/ 3200]\n",
      "loss: 0.024160  [ 2960/ 3200]\n",
      "loss: 0.110435  [ 2976/ 3200]\n",
      "loss: 0.034405  [ 2992/ 3200]\n",
      "loss: 0.207502  [ 3008/ 3200]\n",
      "loss: 0.115093  [ 3024/ 3200]\n",
      "loss: 0.076408  [ 3040/ 3200]\n",
      "loss: 0.142604  [ 3056/ 3200]\n",
      "loss: 0.049048  [ 3072/ 3200]\n",
      "loss: 0.056718  [ 3088/ 3200]\n",
      "loss: 0.034281  [ 3104/ 3200]\n",
      "loss: 0.039479  [ 3120/ 3200]\n",
      "loss: 0.031270  [ 3136/ 3200]\n",
      "loss: 0.043667  [ 3152/ 3200]\n",
      "loss: 0.171064  [ 3168/ 3200]\n",
      "loss: 0.084265  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.047896\n",
      "f1 macro averaged score: 0.769902\n",
      "Accuracy               : 77.0%\n",
      "Confusion matrix       :\n",
      "tensor([[171,  11,   0,  18],\n",
      "        [ 16, 121,  14,  49],\n",
      "        [  1,  21, 158,  20],\n",
      "        [  7,  16,  11, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.6199e-04.\n",
      "\n",
      "Epoch: 50\n",
      "-----------------------------\n",
      "loss: 0.023326  [    0/ 3200]\n",
      "loss: 0.113419  [   16/ 3200]\n",
      "loss: 0.018614  [   32/ 3200]\n",
      "loss: 0.015364  [   48/ 3200]\n",
      "loss: 0.358255  [   64/ 3200]\n",
      "loss: 0.033367  [   80/ 3200]\n",
      "loss: 0.022469  [   96/ 3200]\n",
      "loss: 0.046924  [  112/ 3200]\n",
      "loss: 0.016731  [  128/ 3200]\n",
      "loss: 0.022043  [  144/ 3200]\n",
      "loss: 0.022803  [  160/ 3200]\n",
      "loss: 0.115046  [  176/ 3200]\n",
      "loss: 0.044074  [  192/ 3200]\n",
      "loss: 0.047990  [  208/ 3200]\n",
      "loss: 0.068755  [  224/ 3200]\n",
      "loss: 0.029534  [  240/ 3200]\n",
      "loss: 0.054460  [  256/ 3200]\n",
      "loss: 0.032752  [  272/ 3200]\n",
      "loss: 0.044331  [  288/ 3200]\n",
      "loss: 0.053074  [  304/ 3200]\n",
      "loss: 0.026877  [  320/ 3200]\n",
      "loss: 0.014682  [  336/ 3200]\n",
      "loss: 0.013185  [  352/ 3200]\n",
      "loss: 0.047096  [  368/ 3200]\n",
      "loss: 0.349809  [  384/ 3200]\n",
      "loss: 0.042678  [  400/ 3200]\n",
      "loss: 0.025605  [  416/ 3200]\n",
      "loss: 0.046815  [  432/ 3200]\n",
      "loss: 0.150953  [  448/ 3200]\n",
      "loss: 0.038420  [  464/ 3200]\n",
      "loss: 0.092814  [  480/ 3200]\n",
      "loss: 0.039142  [  496/ 3200]\n",
      "loss: 0.071150  [  512/ 3200]\n",
      "loss: 0.019508  [  528/ 3200]\n",
      "loss: 0.034315  [  544/ 3200]\n",
      "loss: 0.116986  [  560/ 3200]\n",
      "loss: 0.015262  [  576/ 3200]\n",
      "loss: 0.016417  [  592/ 3200]\n",
      "loss: 0.024510  [  608/ 3200]\n",
      "loss: 0.087135  [  624/ 3200]\n",
      "loss: 0.025970  [  640/ 3200]\n",
      "loss: 0.019658  [  656/ 3200]\n",
      "loss: 0.053669  [  672/ 3200]\n",
      "loss: 0.060301  [  688/ 3200]\n",
      "loss: 0.027904  [  704/ 3200]\n",
      "loss: 0.047249  [  720/ 3200]\n",
      "loss: 0.020548  [  736/ 3200]\n",
      "loss: 0.031626  [  752/ 3200]\n",
      "loss: 0.037559  [  768/ 3200]\n",
      "loss: 0.022585  [  784/ 3200]\n",
      "loss: 0.030125  [  800/ 3200]\n",
      "loss: 0.038818  [  816/ 3200]\n",
      "loss: 0.063408  [  832/ 3200]\n",
      "loss: 0.020420  [  848/ 3200]\n",
      "loss: 0.027289  [  864/ 3200]\n",
      "loss: 0.033829  [  880/ 3200]\n",
      "loss: 0.027387  [  896/ 3200]\n",
      "loss: 0.086789  [  912/ 3200]\n",
      "loss: 0.033193  [  928/ 3200]\n",
      "loss: 0.024422  [  944/ 3200]\n",
      "loss: 0.029992  [  960/ 3200]\n",
      "loss: 0.029152  [  976/ 3200]\n",
      "loss: 0.038575  [  992/ 3200]\n",
      "loss: 0.021577  [ 1008/ 3200]\n",
      "loss: 0.029684  [ 1024/ 3200]\n",
      "loss: 0.050053  [ 1040/ 3200]\n",
      "loss: 0.061957  [ 1056/ 3200]\n",
      "loss: 0.028032  [ 1072/ 3200]\n",
      "loss: 0.123269  [ 1088/ 3200]\n",
      "loss: 0.019495  [ 1104/ 3200]\n",
      "loss: 0.028467  [ 1120/ 3200]\n",
      "loss: 0.023756  [ 1136/ 3200]\n",
      "loss: 0.021918  [ 1152/ 3200]\n",
      "loss: 0.115078  [ 1168/ 3200]\n",
      "loss: 0.026757  [ 1184/ 3200]\n",
      "loss: 0.055361  [ 1200/ 3200]\n",
      "loss: 0.041168  [ 1216/ 3200]\n",
      "loss: 0.064115  [ 1232/ 3200]\n",
      "loss: 0.214282  [ 1248/ 3200]\n",
      "loss: 0.136528  [ 1264/ 3200]\n",
      "loss: 0.010534  [ 1280/ 3200]\n",
      "loss: 0.016594  [ 1296/ 3200]\n",
      "loss: 0.028961  [ 1312/ 3200]\n",
      "loss: 0.050734  [ 1328/ 3200]\n",
      "loss: 0.042521  [ 1344/ 3200]\n",
      "loss: 0.084264  [ 1360/ 3200]\n",
      "loss: 0.123766  [ 1376/ 3200]\n",
      "loss: 0.148090  [ 1392/ 3200]\n",
      "loss: 0.027799  [ 1408/ 3200]\n",
      "loss: 0.026330  [ 1424/ 3200]\n",
      "loss: 0.019849  [ 1440/ 3200]\n",
      "loss: 0.058244  [ 1456/ 3200]\n",
      "loss: 0.057647  [ 1472/ 3200]\n",
      "loss: 0.039396  [ 1488/ 3200]\n",
      "loss: 0.060406  [ 1504/ 3200]\n",
      "loss: 0.026759  [ 1520/ 3200]\n",
      "loss: 0.020875  [ 1536/ 3200]\n",
      "loss: 0.029823  [ 1552/ 3200]\n",
      "loss: 0.125338  [ 1568/ 3200]\n",
      "loss: 0.020781  [ 1584/ 3200]\n",
      "loss: 0.010388  [ 1600/ 3200]\n",
      "loss: 0.033358  [ 1616/ 3200]\n",
      "loss: 0.024939  [ 1632/ 3200]\n",
      "loss: 0.055481  [ 1648/ 3200]\n",
      "loss: 0.076505  [ 1664/ 3200]\n",
      "loss: 0.068029  [ 1680/ 3200]\n",
      "loss: 0.051307  [ 1696/ 3200]\n",
      "loss: 0.065086  [ 1712/ 3200]\n",
      "loss: 0.099422  [ 1728/ 3200]\n",
      "loss: 0.018662  [ 1744/ 3200]\n",
      "loss: 0.017769  [ 1760/ 3200]\n",
      "loss: 0.017091  [ 1776/ 3200]\n",
      "loss: 0.016761  [ 1792/ 3200]\n",
      "loss: 0.018397  [ 1808/ 3200]\n",
      "loss: 0.040429  [ 1824/ 3200]\n",
      "loss: 0.118149  [ 1840/ 3200]\n",
      "loss: 0.024671  [ 1856/ 3200]\n",
      "loss: 0.044782  [ 1872/ 3200]\n",
      "loss: 0.025443  [ 1888/ 3200]\n",
      "loss: 0.107521  [ 1904/ 3200]\n",
      "loss: 0.037195  [ 1920/ 3200]\n",
      "loss: 0.056063  [ 1936/ 3200]\n",
      "loss: 0.017537  [ 1952/ 3200]\n",
      "loss: 0.039823  [ 1968/ 3200]\n",
      "loss: 0.068199  [ 1984/ 3200]\n",
      "loss: 0.045845  [ 2000/ 3200]\n",
      "loss: 0.100418  [ 2016/ 3200]\n",
      "loss: 0.012146  [ 2032/ 3200]\n",
      "loss: 0.064966  [ 2048/ 3200]\n",
      "loss: 0.064515  [ 2064/ 3200]\n",
      "loss: 0.046889  [ 2080/ 3200]\n",
      "loss: 0.034452  [ 2096/ 3200]\n",
      "loss: 0.065426  [ 2112/ 3200]\n",
      "loss: 0.166821  [ 2128/ 3200]\n",
      "loss: 0.036777  [ 2144/ 3200]\n",
      "loss: 0.055467  [ 2160/ 3200]\n",
      "loss: 0.023204  [ 2176/ 3200]\n",
      "loss: 0.052552  [ 2192/ 3200]\n",
      "loss: 0.019073  [ 2208/ 3200]\n",
      "loss: 0.068223  [ 2224/ 3200]\n",
      "loss: 0.022161  [ 2240/ 3200]\n",
      "loss: 0.037697  [ 2256/ 3200]\n",
      "loss: 0.027282  [ 2272/ 3200]\n",
      "loss: 0.098072  [ 2288/ 3200]\n",
      "loss: 0.044359  [ 2304/ 3200]\n",
      "loss: 0.035289  [ 2320/ 3200]\n",
      "loss: 0.022967  [ 2336/ 3200]\n",
      "loss: 0.143958  [ 2352/ 3200]\n",
      "loss: 0.033851  [ 2368/ 3200]\n",
      "loss: 0.196445  [ 2384/ 3200]\n",
      "loss: 0.131396  [ 2400/ 3200]\n",
      "loss: 0.097893  [ 2416/ 3200]\n",
      "loss: 0.025034  [ 2432/ 3200]\n",
      "loss: 0.083071  [ 2448/ 3200]\n",
      "loss: 0.053731  [ 2464/ 3200]\n",
      "loss: 0.089143  [ 2480/ 3200]\n",
      "loss: 0.231121  [ 2496/ 3200]\n",
      "loss: 0.111446  [ 2512/ 3200]\n",
      "loss: 0.041366  [ 2528/ 3200]\n",
      "loss: 0.011067  [ 2544/ 3200]\n",
      "loss: 0.011653  [ 2560/ 3200]\n",
      "loss: 0.046533  [ 2576/ 3200]\n",
      "loss: 0.148856  [ 2592/ 3200]\n",
      "loss: 0.104453  [ 2608/ 3200]\n",
      "loss: 0.021564  [ 2624/ 3200]\n",
      "loss: 0.084846  [ 2640/ 3200]\n",
      "loss: 0.054955  [ 2656/ 3200]\n",
      "loss: 0.028179  [ 2672/ 3200]\n",
      "loss: 0.136653  [ 2688/ 3200]\n",
      "loss: 0.066326  [ 2704/ 3200]\n",
      "loss: 0.049602  [ 2720/ 3200]\n",
      "loss: 0.064540  [ 2736/ 3200]\n",
      "loss: 0.050631  [ 2752/ 3200]\n",
      "loss: 0.036528  [ 2768/ 3200]\n",
      "loss: 0.021606  [ 2784/ 3200]\n",
      "loss: 0.118394  [ 2800/ 3200]\n",
      "loss: 0.046721  [ 2816/ 3200]\n",
      "loss: 0.096272  [ 2832/ 3200]\n",
      "loss: 0.035645  [ 2848/ 3200]\n",
      "loss: 0.059382  [ 2864/ 3200]\n",
      "loss: 0.027767  [ 2880/ 3200]\n",
      "loss: 0.068238  [ 2896/ 3200]\n",
      "loss: 0.046479  [ 2912/ 3200]\n",
      "loss: 0.048513  [ 2928/ 3200]\n",
      "loss: 0.074667  [ 2944/ 3200]\n",
      "loss: 0.043674  [ 2960/ 3200]\n",
      "loss: 0.113176  [ 2976/ 3200]\n",
      "loss: 0.021821  [ 2992/ 3200]\n",
      "loss: 0.028436  [ 3008/ 3200]\n",
      "loss: 0.020039  [ 3024/ 3200]\n",
      "loss: 0.034232  [ 3040/ 3200]\n",
      "loss: 0.048827  [ 3056/ 3200]\n",
      "loss: 0.022655  [ 3072/ 3200]\n",
      "loss: 0.097505  [ 3088/ 3200]\n",
      "loss: 0.114988  [ 3104/ 3200]\n",
      "loss: 0.041868  [ 3120/ 3200]\n",
      "loss: 0.099725  [ 3136/ 3200]\n",
      "loss: 0.048884  [ 3152/ 3200]\n",
      "loss: 0.045851  [ 3168/ 3200]\n",
      "loss: 0.020978  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.047170\n",
      "f1 macro averaged score: 0.781792\n",
      "Accuracy               : 78.2%\n",
      "Confusion matrix       :\n",
      "tensor([[174,   9,   0,  17],\n",
      "        [ 16, 127,  18,  39],\n",
      "        [  0,  21, 164,  15],\n",
      "        [  8,  17,  14, 161]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.5389e-04.\n",
      "\n",
      "Epoch: 51\n",
      "-----------------------------\n",
      "loss: 0.091372  [    0/ 3200]\n",
      "loss: 0.044256  [   16/ 3200]\n",
      "loss: 0.019968  [   32/ 3200]\n",
      "loss: 0.014314  [   48/ 3200]\n",
      "loss: 0.098925  [   64/ 3200]\n",
      "loss: 0.032079  [   80/ 3200]\n",
      "loss: 0.050618  [   96/ 3200]\n",
      "loss: 0.014675  [  112/ 3200]\n",
      "loss: 0.020251  [  128/ 3200]\n",
      "loss: 0.040216  [  144/ 3200]\n",
      "loss: 0.033451  [  160/ 3200]\n",
      "loss: 0.142738  [  176/ 3200]\n",
      "loss: 0.043040  [  192/ 3200]\n",
      "loss: 0.031974  [  208/ 3200]\n",
      "loss: 0.013070  [  224/ 3200]\n",
      "loss: 0.021402  [  240/ 3200]\n",
      "loss: 0.059438  [  256/ 3200]\n",
      "loss: 0.051643  [  272/ 3200]\n",
      "loss: 0.021385  [  288/ 3200]\n",
      "loss: 0.051196  [  304/ 3200]\n",
      "loss: 0.060089  [  320/ 3200]\n",
      "loss: 0.020810  [  336/ 3200]\n",
      "loss: 0.080896  [  352/ 3200]\n",
      "loss: 0.036132  [  368/ 3200]\n",
      "loss: 0.028969  [  384/ 3200]\n",
      "loss: 0.147980  [  400/ 3200]\n",
      "loss: 0.085076  [  416/ 3200]\n",
      "loss: 0.067030  [  432/ 3200]\n",
      "loss: 0.011468  [  448/ 3200]\n",
      "loss: 0.076897  [  464/ 3200]\n",
      "loss: 0.027626  [  480/ 3200]\n",
      "loss: 0.060902  [  496/ 3200]\n",
      "loss: 0.069522  [  512/ 3200]\n",
      "loss: 0.027953  [  528/ 3200]\n",
      "loss: 0.041083  [  544/ 3200]\n",
      "loss: 0.035226  [  560/ 3200]\n",
      "loss: 0.029940  [  576/ 3200]\n",
      "loss: 0.048377  [  592/ 3200]\n",
      "loss: 0.061868  [  608/ 3200]\n",
      "loss: 0.010691  [  624/ 3200]\n",
      "loss: 0.042435  [  640/ 3200]\n",
      "loss: 0.072578  [  656/ 3200]\n",
      "loss: 0.060340  [  672/ 3200]\n",
      "loss: 0.025542  [  688/ 3200]\n",
      "loss: 0.056809  [  704/ 3200]\n",
      "loss: 0.026886  [  720/ 3200]\n",
      "loss: 0.044136  [  736/ 3200]\n",
      "loss: 0.009713  [  752/ 3200]\n",
      "loss: 0.010907  [  768/ 3200]\n",
      "loss: 0.037844  [  784/ 3200]\n",
      "loss: 0.040969  [  800/ 3200]\n",
      "loss: 0.061173  [  816/ 3200]\n",
      "loss: 0.177062  [  832/ 3200]\n",
      "loss: 0.019958  [  848/ 3200]\n",
      "loss: 0.062777  [  864/ 3200]\n",
      "loss: 0.084056  [  880/ 3200]\n",
      "loss: 0.054803  [  896/ 3200]\n",
      "loss: 0.043698  [  912/ 3200]\n",
      "loss: 0.134700  [  928/ 3200]\n",
      "loss: 0.056568  [  944/ 3200]\n",
      "loss: 0.006990  [  960/ 3200]\n",
      "loss: 0.035933  [  976/ 3200]\n",
      "loss: 0.036340  [  992/ 3200]\n",
      "loss: 0.064069  [ 1008/ 3200]\n",
      "loss: 0.032441  [ 1024/ 3200]\n",
      "loss: 0.089139  [ 1040/ 3200]\n",
      "loss: 0.010059  [ 1056/ 3200]\n",
      "loss: 0.095208  [ 1072/ 3200]\n",
      "loss: 0.054574  [ 1088/ 3200]\n",
      "loss: 0.260547  [ 1104/ 3200]\n",
      "loss: 0.056851  [ 1120/ 3200]\n",
      "loss: 0.379523  [ 1136/ 3200]\n",
      "loss: 0.016300  [ 1152/ 3200]\n",
      "loss: 0.071949  [ 1168/ 3200]\n",
      "loss: 0.082961  [ 1184/ 3200]\n",
      "loss: 0.059197  [ 1200/ 3200]\n",
      "loss: 0.029438  [ 1216/ 3200]\n",
      "loss: 0.060403  [ 1232/ 3200]\n",
      "loss: 0.043730  [ 1248/ 3200]\n",
      "loss: 0.059538  [ 1264/ 3200]\n",
      "loss: 0.007678  [ 1280/ 3200]\n",
      "loss: 0.028205  [ 1296/ 3200]\n",
      "loss: 0.058917  [ 1312/ 3200]\n",
      "loss: 0.041519  [ 1328/ 3200]\n",
      "loss: 0.026765  [ 1344/ 3200]\n",
      "loss: 0.095929  [ 1360/ 3200]\n",
      "loss: 0.094411  [ 1376/ 3200]\n",
      "loss: 0.129188  [ 1392/ 3200]\n",
      "loss: 0.052982  [ 1408/ 3200]\n",
      "loss: 0.089202  [ 1424/ 3200]\n",
      "loss: 0.096984  [ 1440/ 3200]\n",
      "loss: 0.029713  [ 1456/ 3200]\n",
      "loss: 0.005161  [ 1472/ 3200]\n",
      "loss: 0.091202  [ 1488/ 3200]\n",
      "loss: 0.039952  [ 1504/ 3200]\n",
      "loss: 0.090804  [ 1520/ 3200]\n",
      "loss: 0.069735  [ 1536/ 3200]\n",
      "loss: 0.035384  [ 1552/ 3200]\n",
      "loss: 0.022714  [ 1568/ 3200]\n",
      "loss: 0.012016  [ 1584/ 3200]\n",
      "loss: 0.159972  [ 1600/ 3200]\n",
      "loss: 0.045427  [ 1616/ 3200]\n",
      "loss: 0.083748  [ 1632/ 3200]\n",
      "loss: 0.093384  [ 1648/ 3200]\n",
      "loss: 0.130165  [ 1664/ 3200]\n",
      "loss: 0.065517  [ 1680/ 3200]\n",
      "loss: 0.263156  [ 1696/ 3200]\n",
      "loss: 0.038880  [ 1712/ 3200]\n",
      "loss: 0.059571  [ 1728/ 3200]\n",
      "loss: 0.073404  [ 1744/ 3200]\n",
      "loss: 0.085314  [ 1760/ 3200]\n",
      "loss: 0.137995  [ 1776/ 3200]\n",
      "loss: 0.012417  [ 1792/ 3200]\n",
      "loss: 0.021221  [ 1808/ 3200]\n",
      "loss: 0.165607  [ 1824/ 3200]\n",
      "loss: 0.051133  [ 1840/ 3200]\n",
      "loss: 0.084469  [ 1856/ 3200]\n",
      "loss: 0.114113  [ 1872/ 3200]\n",
      "loss: 0.014686  [ 1888/ 3200]\n",
      "loss: 0.072481  [ 1904/ 3200]\n",
      "loss: 0.033314  [ 1920/ 3200]\n",
      "loss: 0.013630  [ 1936/ 3200]\n",
      "loss: 0.027025  [ 1952/ 3200]\n",
      "loss: 0.008320  [ 1968/ 3200]\n",
      "loss: 0.049221  [ 1984/ 3200]\n",
      "loss: 0.040135  [ 2000/ 3200]\n",
      "loss: 0.019372  [ 2016/ 3200]\n",
      "loss: 0.047698  [ 2032/ 3200]\n",
      "loss: 0.005963  [ 2048/ 3200]\n",
      "loss: 0.021456  [ 2064/ 3200]\n",
      "loss: 0.099877  [ 2080/ 3200]\n",
      "loss: 0.122172  [ 2096/ 3200]\n",
      "loss: 0.052063  [ 2112/ 3200]\n",
      "loss: 0.070021  [ 2128/ 3200]\n",
      "loss: 0.015693  [ 2144/ 3200]\n",
      "loss: 0.091718  [ 2160/ 3200]\n",
      "loss: 0.014491  [ 2176/ 3200]\n",
      "loss: 0.104944  [ 2192/ 3200]\n",
      "loss: 0.016133  [ 2208/ 3200]\n",
      "loss: 0.043013  [ 2224/ 3200]\n",
      "loss: 0.023126  [ 2240/ 3200]\n",
      "loss: 0.142882  [ 2256/ 3200]\n",
      "loss: 0.033016  [ 2272/ 3200]\n",
      "loss: 0.022281  [ 2288/ 3200]\n",
      "loss: 0.102598  [ 2304/ 3200]\n",
      "loss: 0.065768  [ 2320/ 3200]\n",
      "loss: 0.057424  [ 2336/ 3200]\n",
      "loss: 0.073690  [ 2352/ 3200]\n",
      "loss: 0.043399  [ 2368/ 3200]\n",
      "loss: 0.009382  [ 2384/ 3200]\n",
      "loss: 0.008203  [ 2400/ 3200]\n",
      "loss: 0.008375  [ 2416/ 3200]\n",
      "loss: 0.054797  [ 2432/ 3200]\n",
      "loss: 0.365513  [ 2448/ 3200]\n",
      "loss: 0.015168  [ 2464/ 3200]\n",
      "loss: 0.132442  [ 2480/ 3200]\n",
      "loss: 0.056654  [ 2496/ 3200]\n",
      "loss: 0.059192  [ 2512/ 3200]\n",
      "loss: 0.035578  [ 2528/ 3200]\n",
      "loss: 0.094514  [ 2544/ 3200]\n",
      "loss: 0.041410  [ 2560/ 3200]\n",
      "loss: 0.056860  [ 2576/ 3200]\n",
      "loss: 0.007022  [ 2592/ 3200]\n",
      "loss: 0.137672  [ 2608/ 3200]\n",
      "loss: 0.026004  [ 2624/ 3200]\n",
      "loss: 0.029799  [ 2640/ 3200]\n",
      "loss: 0.020223  [ 2656/ 3200]\n",
      "loss: 0.031010  [ 2672/ 3200]\n",
      "loss: 0.048878  [ 2688/ 3200]\n",
      "loss: 0.082957  [ 2704/ 3200]\n",
      "loss: 0.051172  [ 2720/ 3200]\n",
      "loss: 0.016767  [ 2736/ 3200]\n",
      "loss: 0.026665  [ 2752/ 3200]\n",
      "loss: 0.033018  [ 2768/ 3200]\n",
      "loss: 0.035848  [ 2784/ 3200]\n",
      "loss: 0.027357  [ 2800/ 3200]\n",
      "loss: 0.122665  [ 2816/ 3200]\n",
      "loss: 0.083395  [ 2832/ 3200]\n",
      "loss: 0.030934  [ 2848/ 3200]\n",
      "loss: 0.034144  [ 2864/ 3200]\n",
      "loss: 0.096392  [ 2880/ 3200]\n",
      "loss: 0.059772  [ 2896/ 3200]\n",
      "loss: 0.060226  [ 2912/ 3200]\n",
      "loss: 0.080903  [ 2928/ 3200]\n",
      "loss: 0.041815  [ 2944/ 3200]\n",
      "loss: 0.075929  [ 2960/ 3200]\n",
      "loss: 0.039678  [ 2976/ 3200]\n",
      "loss: 0.038447  [ 2992/ 3200]\n",
      "loss: 0.045804  [ 3008/ 3200]\n",
      "loss: 0.038791  [ 3024/ 3200]\n",
      "loss: 0.062161  [ 3040/ 3200]\n",
      "loss: 0.018782  [ 3056/ 3200]\n",
      "loss: 0.055222  [ 3072/ 3200]\n",
      "loss: 0.032787  [ 3088/ 3200]\n",
      "loss: 0.042946  [ 3104/ 3200]\n",
      "loss: 0.075323  [ 3120/ 3200]\n",
      "loss: 0.006924  [ 3136/ 3200]\n",
      "loss: 0.147739  [ 3152/ 3200]\n",
      "loss: 0.064218  [ 3168/ 3200]\n",
      "loss: 0.080137  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.050500\n",
      "f1 macro averaged score: 0.779514\n",
      "Accuracy               : 78.1%\n",
      "Confusion matrix       :\n",
      "tensor([[176,  10,   0,  14],\n",
      "        [ 15, 121,  20,  44],\n",
      "        [  0,  20, 165,  15],\n",
      "        [  9,  15,  13, 163]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.4620e-04.\n",
      "\n",
      "Epoch: 52\n",
      "-----------------------------\n",
      "loss: 0.026310  [    0/ 3200]\n",
      "loss: 0.058780  [   16/ 3200]\n",
      "loss: 0.076959  [   32/ 3200]\n",
      "loss: 0.052226  [   48/ 3200]\n",
      "loss: 0.163120  [   64/ 3200]\n",
      "loss: 0.111315  [   80/ 3200]\n",
      "loss: 0.033264  [   96/ 3200]\n",
      "loss: 0.019947  [  112/ 3200]\n",
      "loss: 0.119123  [  128/ 3200]\n",
      "loss: 0.043958  [  144/ 3200]\n",
      "loss: 0.043856  [  160/ 3200]\n",
      "loss: 0.036919  [  176/ 3200]\n",
      "loss: 0.175032  [  192/ 3200]\n",
      "loss: 0.019151  [  208/ 3200]\n",
      "loss: 0.161309  [  224/ 3200]\n",
      "loss: 0.193254  [  240/ 3200]\n",
      "loss: 0.031339  [  256/ 3200]\n",
      "loss: 0.030664  [  272/ 3200]\n",
      "loss: 0.020916  [  288/ 3200]\n",
      "loss: 0.050569  [  304/ 3200]\n",
      "loss: 0.072556  [  320/ 3200]\n",
      "loss: 0.046150  [  336/ 3200]\n",
      "loss: 0.056112  [  352/ 3200]\n",
      "loss: 0.039161  [  368/ 3200]\n",
      "loss: 0.022860  [  384/ 3200]\n",
      "loss: 0.020748  [  400/ 3200]\n",
      "loss: 0.042917  [  416/ 3200]\n",
      "loss: 0.061465  [  432/ 3200]\n",
      "loss: 0.009973  [  448/ 3200]\n",
      "loss: 0.152811  [  464/ 3200]\n",
      "loss: 0.042644  [  480/ 3200]\n",
      "loss: 0.113505  [  496/ 3200]\n",
      "loss: 0.127747  [  512/ 3200]\n",
      "loss: 0.048108  [  528/ 3200]\n",
      "loss: 0.011586  [  544/ 3200]\n",
      "loss: 0.025313  [  560/ 3200]\n",
      "loss: 0.025232  [  576/ 3200]\n",
      "loss: 0.062706  [  592/ 3200]\n",
      "loss: 0.068760  [  608/ 3200]\n",
      "loss: 0.031832  [  624/ 3200]\n",
      "loss: 0.060336  [  640/ 3200]\n",
      "loss: 0.019289  [  656/ 3200]\n",
      "loss: 0.008832  [  672/ 3200]\n",
      "loss: 0.060884  [  688/ 3200]\n",
      "loss: 0.057400  [  704/ 3200]\n",
      "loss: 0.025339  [  720/ 3200]\n",
      "loss: 0.047406  [  736/ 3200]\n",
      "loss: 0.041096  [  752/ 3200]\n",
      "loss: 0.043766  [  768/ 3200]\n",
      "loss: 0.059732  [  784/ 3200]\n",
      "loss: 0.058013  [  800/ 3200]\n",
      "loss: 0.019435  [  816/ 3200]\n",
      "loss: 0.134458  [  832/ 3200]\n",
      "loss: 0.082261  [  848/ 3200]\n",
      "loss: 0.048729  [  864/ 3200]\n",
      "loss: 0.054285  [  880/ 3200]\n",
      "loss: 0.097702  [  896/ 3200]\n",
      "loss: 0.008682  [  912/ 3200]\n",
      "loss: 0.045503  [  928/ 3200]\n",
      "loss: 0.064670  [  944/ 3200]\n",
      "loss: 0.042084  [  960/ 3200]\n",
      "loss: 0.032738  [  976/ 3200]\n",
      "loss: 0.220125  [  992/ 3200]\n",
      "loss: 0.053241  [ 1008/ 3200]\n",
      "loss: 0.037750  [ 1024/ 3200]\n",
      "loss: 0.029680  [ 1040/ 3200]\n",
      "loss: 0.020942  [ 1056/ 3200]\n",
      "loss: 0.078453  [ 1072/ 3200]\n",
      "loss: 0.042032  [ 1088/ 3200]\n",
      "loss: 0.004925  [ 1104/ 3200]\n",
      "loss: 0.049135  [ 1120/ 3200]\n",
      "loss: 0.040424  [ 1136/ 3200]\n",
      "loss: 0.037703  [ 1152/ 3200]\n",
      "loss: 0.096993  [ 1168/ 3200]\n",
      "loss: 0.029082  [ 1184/ 3200]\n",
      "loss: 0.032404  [ 1200/ 3200]\n",
      "loss: 0.031922  [ 1216/ 3200]\n",
      "loss: 0.078279  [ 1232/ 3200]\n",
      "loss: 0.011333  [ 1248/ 3200]\n",
      "loss: 0.024415  [ 1264/ 3200]\n",
      "loss: 0.063792  [ 1280/ 3200]\n",
      "loss: 0.121894  [ 1296/ 3200]\n",
      "loss: 0.015342  [ 1312/ 3200]\n",
      "loss: 0.018006  [ 1328/ 3200]\n",
      "loss: 0.042607  [ 1344/ 3200]\n",
      "loss: 0.032225  [ 1360/ 3200]\n",
      "loss: 0.062373  [ 1376/ 3200]\n",
      "loss: 0.101480  [ 1392/ 3200]\n",
      "loss: 0.014495  [ 1408/ 3200]\n",
      "loss: 0.053745  [ 1424/ 3200]\n",
      "loss: 0.039808  [ 1440/ 3200]\n",
      "loss: 0.031007  [ 1456/ 3200]\n",
      "loss: 0.118042  [ 1472/ 3200]\n",
      "loss: 0.026034  [ 1488/ 3200]\n",
      "loss: 0.022550  [ 1504/ 3200]\n",
      "loss: 0.008755  [ 1520/ 3200]\n",
      "loss: 0.044679  [ 1536/ 3200]\n",
      "loss: 0.044501  [ 1552/ 3200]\n",
      "loss: 0.026493  [ 1568/ 3200]\n",
      "loss: 0.056597  [ 1584/ 3200]\n",
      "loss: 0.053318  [ 1600/ 3200]\n",
      "loss: 0.078162  [ 1616/ 3200]\n",
      "loss: 0.043275  [ 1632/ 3200]\n",
      "loss: 0.076978  [ 1648/ 3200]\n",
      "loss: 0.085175  [ 1664/ 3200]\n",
      "loss: 0.037685  [ 1680/ 3200]\n",
      "loss: 0.029286  [ 1696/ 3200]\n",
      "loss: 0.052261  [ 1712/ 3200]\n",
      "loss: 0.024382  [ 1728/ 3200]\n",
      "loss: 0.113325  [ 1744/ 3200]\n",
      "loss: 0.021943  [ 1760/ 3200]\n",
      "loss: 0.031512  [ 1776/ 3200]\n",
      "loss: 0.016587  [ 1792/ 3200]\n",
      "loss: 0.035272  [ 1808/ 3200]\n",
      "loss: 0.102348  [ 1824/ 3200]\n",
      "loss: 0.057886  [ 1840/ 3200]\n",
      "loss: 0.026208  [ 1856/ 3200]\n",
      "loss: 0.022511  [ 1872/ 3200]\n",
      "loss: 0.151269  [ 1888/ 3200]\n",
      "loss: 0.166540  [ 1904/ 3200]\n",
      "loss: 0.033188  [ 1920/ 3200]\n",
      "loss: 0.064749  [ 1936/ 3200]\n",
      "loss: 0.046167  [ 1952/ 3200]\n",
      "loss: 0.052400  [ 1968/ 3200]\n",
      "loss: 0.111394  [ 1984/ 3200]\n",
      "loss: 0.109541  [ 2000/ 3200]\n",
      "loss: 0.035032  [ 2016/ 3200]\n",
      "loss: 0.047027  [ 2032/ 3200]\n",
      "loss: 0.018987  [ 2048/ 3200]\n",
      "loss: 0.085635  [ 2064/ 3200]\n",
      "loss: 0.053456  [ 2080/ 3200]\n",
      "loss: 0.022990  [ 2096/ 3200]\n",
      "loss: 0.068412  [ 2112/ 3200]\n",
      "loss: 0.052463  [ 2128/ 3200]\n",
      "loss: 0.044223  [ 2144/ 3200]\n",
      "loss: 0.079012  [ 2160/ 3200]\n",
      "loss: 0.025537  [ 2176/ 3200]\n",
      "loss: 0.018345  [ 2192/ 3200]\n",
      "loss: 0.017144  [ 2208/ 3200]\n",
      "loss: 0.194364  [ 2224/ 3200]\n",
      "loss: 0.009945  [ 2240/ 3200]\n",
      "loss: 0.048687  [ 2256/ 3200]\n",
      "loss: 0.107641  [ 2272/ 3200]\n",
      "loss: 0.027031  [ 2288/ 3200]\n",
      "loss: 0.018273  [ 2304/ 3200]\n",
      "loss: 0.063639  [ 2320/ 3200]\n",
      "loss: 0.230867  [ 2336/ 3200]\n",
      "loss: 0.041328  [ 2352/ 3200]\n",
      "loss: 0.102794  [ 2368/ 3200]\n",
      "loss: 0.019816  [ 2384/ 3200]\n",
      "loss: 0.020514  [ 2400/ 3200]\n",
      "loss: 0.033613  [ 2416/ 3200]\n",
      "loss: 0.067327  [ 2432/ 3200]\n",
      "loss: 0.033198  [ 2448/ 3200]\n",
      "loss: 0.040298  [ 2464/ 3200]\n",
      "loss: 0.500298  [ 2480/ 3200]\n",
      "loss: 0.013292  [ 2496/ 3200]\n",
      "loss: 0.100087  [ 2512/ 3200]\n",
      "loss: 0.174807  [ 2528/ 3200]\n",
      "loss: 0.107569  [ 2544/ 3200]\n",
      "loss: 0.051730  [ 2560/ 3200]\n",
      "loss: 0.047599  [ 2576/ 3200]\n",
      "loss: 0.042561  [ 2592/ 3200]\n",
      "loss: 0.057845  [ 2608/ 3200]\n",
      "loss: 0.018671  [ 2624/ 3200]\n",
      "loss: 0.028213  [ 2640/ 3200]\n",
      "loss: 0.012963  [ 2656/ 3200]\n",
      "loss: 0.058433  [ 2672/ 3200]\n",
      "loss: 0.022932  [ 2688/ 3200]\n",
      "loss: 0.068026  [ 2704/ 3200]\n",
      "loss: 0.038922  [ 2720/ 3200]\n",
      "loss: 0.044555  [ 2736/ 3200]\n",
      "loss: 0.057684  [ 2752/ 3200]\n",
      "loss: 0.048756  [ 2768/ 3200]\n",
      "loss: 0.103642  [ 2784/ 3200]\n",
      "loss: 0.240264  [ 2800/ 3200]\n",
      "loss: 0.040221  [ 2816/ 3200]\n",
      "loss: 0.097159  [ 2832/ 3200]\n",
      "loss: 0.086710  [ 2848/ 3200]\n",
      "loss: 0.035465  [ 2864/ 3200]\n",
      "loss: 0.071710  [ 2880/ 3200]\n",
      "loss: 0.030643  [ 2896/ 3200]\n",
      "loss: 0.037051  [ 2912/ 3200]\n",
      "loss: 0.021830  [ 2928/ 3200]\n",
      "loss: 0.015194  [ 2944/ 3200]\n",
      "loss: 0.027212  [ 2960/ 3200]\n",
      "loss: 0.018927  [ 2976/ 3200]\n",
      "loss: 0.021516  [ 2992/ 3200]\n",
      "loss: 0.017694  [ 3008/ 3200]\n",
      "loss: 0.126880  [ 3024/ 3200]\n",
      "loss: 0.047821  [ 3040/ 3200]\n",
      "loss: 0.044621  [ 3056/ 3200]\n",
      "loss: 0.077430  [ 3072/ 3200]\n",
      "loss: 0.027876  [ 3088/ 3200]\n",
      "loss: 0.077505  [ 3104/ 3200]\n",
      "loss: 0.049474  [ 3120/ 3200]\n",
      "loss: 0.050649  [ 3136/ 3200]\n",
      "loss: 0.036340  [ 3152/ 3200]\n",
      "loss: 0.074771  [ 3168/ 3200]\n",
      "loss: 0.048626  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.047189\n",
      "f1 macro averaged score: 0.778738\n",
      "Accuracy               : 78.0%\n",
      "Confusion matrix       :\n",
      "tensor([[176,  11,   0,  13],\n",
      "        [ 16, 125,  18,  41],\n",
      "        [  1,  25, 158,  16],\n",
      "        [ 10,  13,  12, 165]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3889e-04.\n",
      "\n",
      "Epoch: 53\n",
      "-----------------------------\n",
      "loss: 0.053904  [    0/ 3200]\n",
      "loss: 0.067663  [   16/ 3200]\n",
      "loss: 0.059238  [   32/ 3200]\n",
      "loss: 0.041736  [   48/ 3200]\n",
      "loss: 0.122816  [   64/ 3200]\n",
      "loss: 0.029383  [   80/ 3200]\n",
      "loss: 0.030933  [   96/ 3200]\n",
      "loss: 0.031449  [  112/ 3200]\n",
      "loss: 0.020139  [  128/ 3200]\n",
      "loss: 0.023213  [  144/ 3200]\n",
      "loss: 0.048338  [  160/ 3200]\n",
      "loss: 0.093776  [  176/ 3200]\n",
      "loss: 0.148306  [  192/ 3200]\n",
      "loss: 0.029910  [  208/ 3200]\n",
      "loss: 0.076385  [  224/ 3200]\n",
      "loss: 0.026561  [  240/ 3200]\n",
      "loss: 0.022969  [  256/ 3200]\n",
      "loss: 0.061990  [  272/ 3200]\n",
      "loss: 0.122205  [  288/ 3200]\n",
      "loss: 0.025774  [  304/ 3200]\n",
      "loss: 0.024223  [  320/ 3200]\n",
      "loss: 0.042897  [  336/ 3200]\n",
      "loss: 0.065601  [  352/ 3200]\n",
      "loss: 0.040298  [  368/ 3200]\n",
      "loss: 0.122228  [  384/ 3200]\n",
      "loss: 0.057006  [  400/ 3200]\n",
      "loss: 0.017661  [  416/ 3200]\n",
      "loss: 0.081667  [  432/ 3200]\n",
      "loss: 0.033168  [  448/ 3200]\n",
      "loss: 0.062082  [  464/ 3200]\n",
      "loss: 0.070303  [  480/ 3200]\n",
      "loss: 0.023377  [  496/ 3200]\n",
      "loss: 0.037555  [  512/ 3200]\n",
      "loss: 0.099989  [  528/ 3200]\n",
      "loss: 0.024873  [  544/ 3200]\n",
      "loss: 0.011046  [  560/ 3200]\n",
      "loss: 0.087728  [  576/ 3200]\n",
      "loss: 0.012440  [  592/ 3200]\n",
      "loss: 0.018568  [  608/ 3200]\n",
      "loss: 0.102551  [  624/ 3200]\n",
      "loss: 0.021574  [  640/ 3200]\n",
      "loss: 0.065808  [  656/ 3200]\n",
      "loss: 0.024063  [  672/ 3200]\n",
      "loss: 0.028577  [  688/ 3200]\n",
      "loss: 0.156344  [  704/ 3200]\n",
      "loss: 0.129881  [  720/ 3200]\n",
      "loss: 0.056145  [  736/ 3200]\n",
      "loss: 0.023887  [  752/ 3200]\n",
      "loss: 0.323528  [  768/ 3200]\n",
      "loss: 0.024213  [  784/ 3200]\n",
      "loss: 0.029562  [  800/ 3200]\n",
      "loss: 0.075398  [  816/ 3200]\n",
      "loss: 0.039555  [  832/ 3200]\n",
      "loss: 0.014547  [  848/ 3200]\n",
      "loss: 0.032647  [  864/ 3200]\n",
      "loss: 0.015231  [  880/ 3200]\n",
      "loss: 0.115688  [  896/ 3200]\n",
      "loss: 0.017529  [  912/ 3200]\n",
      "loss: 0.022607  [  928/ 3200]\n",
      "loss: 0.062684  [  944/ 3200]\n",
      "loss: 0.041911  [  960/ 3200]\n",
      "loss: 0.019328  [  976/ 3200]\n",
      "loss: 0.028917  [  992/ 3200]\n",
      "loss: 0.023544  [ 1008/ 3200]\n",
      "loss: 0.038393  [ 1024/ 3200]\n",
      "loss: 0.029115  [ 1040/ 3200]\n",
      "loss: 0.044740  [ 1056/ 3200]\n",
      "loss: 0.027104  [ 1072/ 3200]\n",
      "loss: 0.035445  [ 1088/ 3200]\n",
      "loss: 0.133043  [ 1104/ 3200]\n",
      "loss: 0.178239  [ 1120/ 3200]\n",
      "loss: 0.134455  [ 1136/ 3200]\n",
      "loss: 0.022742  [ 1152/ 3200]\n",
      "loss: 0.019593  [ 1168/ 3200]\n",
      "loss: 0.006456  [ 1184/ 3200]\n",
      "loss: 0.089418  [ 1200/ 3200]\n",
      "loss: 0.075473  [ 1216/ 3200]\n",
      "loss: 0.055611  [ 1232/ 3200]\n",
      "loss: 0.036704  [ 1248/ 3200]\n",
      "loss: 0.090038  [ 1264/ 3200]\n",
      "loss: 0.072723  [ 1280/ 3200]\n",
      "loss: 0.035524  [ 1296/ 3200]\n",
      "loss: 0.034649  [ 1312/ 3200]\n",
      "loss: 0.019743  [ 1328/ 3200]\n",
      "loss: 0.023784  [ 1344/ 3200]\n",
      "loss: 0.035220  [ 1360/ 3200]\n",
      "loss: 0.043840  [ 1376/ 3200]\n",
      "loss: 0.085003  [ 1392/ 3200]\n",
      "loss: 0.156156  [ 1408/ 3200]\n",
      "loss: 0.041900  [ 1424/ 3200]\n",
      "loss: 0.029188  [ 1440/ 3200]\n",
      "loss: 0.111236  [ 1456/ 3200]\n",
      "loss: 0.134609  [ 1472/ 3200]\n",
      "loss: 0.039357  [ 1488/ 3200]\n",
      "loss: 0.049598  [ 1504/ 3200]\n",
      "loss: 0.038448  [ 1520/ 3200]\n",
      "loss: 0.049185  [ 1536/ 3200]\n",
      "loss: 0.066103  [ 1552/ 3200]\n",
      "loss: 0.070735  [ 1568/ 3200]\n",
      "loss: 0.041900  [ 1584/ 3200]\n",
      "loss: 0.103112  [ 1600/ 3200]\n",
      "loss: 0.094919  [ 1616/ 3200]\n",
      "loss: 0.094131  [ 1632/ 3200]\n",
      "loss: 0.030210  [ 1648/ 3200]\n",
      "loss: 0.038425  [ 1664/ 3200]\n",
      "loss: 0.046799  [ 1680/ 3200]\n",
      "loss: 0.057785  [ 1696/ 3200]\n",
      "loss: 0.013646  [ 1712/ 3200]\n",
      "loss: 0.044070  [ 1728/ 3200]\n",
      "loss: 0.046236  [ 1744/ 3200]\n",
      "loss: 0.223162  [ 1760/ 3200]\n",
      "loss: 0.016195  [ 1776/ 3200]\n",
      "loss: 0.075378  [ 1792/ 3200]\n",
      "loss: 0.038621  [ 1808/ 3200]\n",
      "loss: 0.104510  [ 1824/ 3200]\n",
      "loss: 0.043804  [ 1840/ 3200]\n",
      "loss: 0.091102  [ 1856/ 3200]\n",
      "loss: 0.064649  [ 1872/ 3200]\n",
      "loss: 0.061972  [ 1888/ 3200]\n",
      "loss: 0.032344  [ 1904/ 3200]\n",
      "loss: 0.011929  [ 1920/ 3200]\n",
      "loss: 0.071438  [ 1936/ 3200]\n",
      "loss: 0.054167  [ 1952/ 3200]\n",
      "loss: 0.085074  [ 1968/ 3200]\n",
      "loss: 0.019125  [ 1984/ 3200]\n",
      "loss: 0.040889  [ 2000/ 3200]\n",
      "loss: 0.064595  [ 2016/ 3200]\n",
      "loss: 0.247189  [ 2032/ 3200]\n",
      "loss: 0.058767  [ 2048/ 3200]\n",
      "loss: 0.051467  [ 2064/ 3200]\n",
      "loss: 0.014899  [ 2080/ 3200]\n",
      "loss: 0.020431  [ 2096/ 3200]\n",
      "loss: 0.064367  [ 2112/ 3200]\n",
      "loss: 0.038994  [ 2128/ 3200]\n",
      "loss: 0.026042  [ 2144/ 3200]\n",
      "loss: 0.046962  [ 2160/ 3200]\n",
      "loss: 0.028052  [ 2176/ 3200]\n",
      "loss: 0.037728  [ 2192/ 3200]\n",
      "loss: 0.073385  [ 2208/ 3200]\n",
      "loss: 0.080482  [ 2224/ 3200]\n",
      "loss: 0.059538  [ 2240/ 3200]\n",
      "loss: 0.064756  [ 2256/ 3200]\n",
      "loss: 0.046383  [ 2272/ 3200]\n",
      "loss: 0.151608  [ 2288/ 3200]\n",
      "loss: 0.010218  [ 2304/ 3200]\n",
      "loss: 0.050099  [ 2320/ 3200]\n",
      "loss: 0.042473  [ 2336/ 3200]\n",
      "loss: 0.101162  [ 2352/ 3200]\n",
      "loss: 0.061563  [ 2368/ 3200]\n",
      "loss: 0.049005  [ 2384/ 3200]\n",
      "loss: 0.053131  [ 2400/ 3200]\n",
      "loss: 0.015398  [ 2416/ 3200]\n",
      "loss: 0.081346  [ 2432/ 3200]\n",
      "loss: 0.024401  [ 2448/ 3200]\n",
      "loss: 0.031038  [ 2464/ 3200]\n",
      "loss: 0.038298  [ 2480/ 3200]\n",
      "loss: 0.329739  [ 2496/ 3200]\n",
      "loss: 0.015409  [ 2512/ 3200]\n",
      "loss: 0.032794  [ 2528/ 3200]\n",
      "loss: 0.140248  [ 2544/ 3200]\n",
      "loss: 0.262826  [ 2560/ 3200]\n",
      "loss: 0.013782  [ 2576/ 3200]\n",
      "loss: 0.065490  [ 2592/ 3200]\n",
      "loss: 0.025326  [ 2608/ 3200]\n",
      "loss: 0.023277  [ 2624/ 3200]\n",
      "loss: 0.040260  [ 2640/ 3200]\n",
      "loss: 0.068934  [ 2656/ 3200]\n",
      "loss: 0.076634  [ 2672/ 3200]\n",
      "loss: 0.071383  [ 2688/ 3200]\n",
      "loss: 0.065308  [ 2704/ 3200]\n",
      "loss: 0.039256  [ 2720/ 3200]\n",
      "loss: 0.100671  [ 2736/ 3200]\n",
      "loss: 0.018825  [ 2752/ 3200]\n",
      "loss: 0.010186  [ 2768/ 3200]\n",
      "loss: 0.092609  [ 2784/ 3200]\n",
      "loss: 0.052419  [ 2800/ 3200]\n",
      "loss: 0.018538  [ 2816/ 3200]\n",
      "loss: 0.073483  [ 2832/ 3200]\n",
      "loss: 0.059117  [ 2848/ 3200]\n",
      "loss: 0.080421  [ 2864/ 3200]\n",
      "loss: 0.024094  [ 2880/ 3200]\n",
      "loss: 0.086411  [ 2896/ 3200]\n",
      "loss: 0.049410  [ 2912/ 3200]\n",
      "loss: 0.047098  [ 2928/ 3200]\n",
      "loss: 0.189810  [ 2944/ 3200]\n",
      "loss: 0.026739  [ 2960/ 3200]\n",
      "loss: 0.034596  [ 2976/ 3200]\n",
      "loss: 0.037885  [ 2992/ 3200]\n",
      "loss: 0.030449  [ 3008/ 3200]\n",
      "loss: 0.042780  [ 3024/ 3200]\n",
      "loss: 0.016243  [ 3040/ 3200]\n",
      "loss: 0.037295  [ 3056/ 3200]\n",
      "loss: 0.048176  [ 3072/ 3200]\n",
      "loss: 0.055419  [ 3088/ 3200]\n",
      "loss: 0.007589  [ 3104/ 3200]\n",
      "loss: 0.037108  [ 3120/ 3200]\n",
      "loss: 0.025022  [ 3136/ 3200]\n",
      "loss: 0.034834  [ 3152/ 3200]\n",
      "loss: 0.039172  [ 3168/ 3200]\n",
      "loss: 0.088879  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.046483\n",
      "f1 macro averaged score: 0.790040\n",
      "Accuracy               : 79.0%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  10,   0,  15],\n",
      "        [ 13, 133,  18,  36],\n",
      "        [  0,  21, 161,  18],\n",
      "        [  8,  17,  12, 163]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3194e-04.\n",
      "\n",
      "Epoch: 54\n",
      "-----------------------------\n",
      "loss: 0.022945  [    0/ 3200]\n",
      "loss: 0.023964  [   16/ 3200]\n",
      "loss: 0.106606  [   32/ 3200]\n",
      "loss: 0.063779  [   48/ 3200]\n",
      "loss: 0.026721  [   64/ 3200]\n",
      "loss: 0.050638  [   80/ 3200]\n",
      "loss: 0.065029  [   96/ 3200]\n",
      "loss: 0.019721  [  112/ 3200]\n",
      "loss: 0.090012  [  128/ 3200]\n",
      "loss: 0.013926  [  144/ 3200]\n",
      "loss: 0.049492  [  160/ 3200]\n",
      "loss: 0.026360  [  176/ 3200]\n",
      "loss: 0.167893  [  192/ 3200]\n",
      "loss: 0.027800  [  208/ 3200]\n",
      "loss: 0.119186  [  224/ 3200]\n",
      "loss: 0.033541  [  240/ 3200]\n",
      "loss: 0.038516  [  256/ 3200]\n",
      "loss: 0.075867  [  272/ 3200]\n",
      "loss: 0.019422  [  288/ 3200]\n",
      "loss: 0.011798  [  304/ 3200]\n",
      "loss: 0.023318  [  320/ 3200]\n",
      "loss: 0.016219  [  336/ 3200]\n",
      "loss: 0.123479  [  352/ 3200]\n",
      "loss: 0.111542  [  368/ 3200]\n",
      "loss: 0.122469  [  384/ 3200]\n",
      "loss: 0.042139  [  400/ 3200]\n",
      "loss: 0.031421  [  416/ 3200]\n",
      "loss: 0.024207  [  432/ 3200]\n",
      "loss: 0.029409  [  448/ 3200]\n",
      "loss: 0.081414  [  464/ 3200]\n",
      "loss: 0.021098  [  480/ 3200]\n",
      "loss: 0.036375  [  496/ 3200]\n",
      "loss: 0.055609  [  512/ 3200]\n",
      "loss: 0.020116  [  528/ 3200]\n",
      "loss: 0.020137  [  544/ 3200]\n",
      "loss: 0.052669  [  560/ 3200]\n",
      "loss: 0.032530  [  576/ 3200]\n",
      "loss: 0.071527  [  592/ 3200]\n",
      "loss: 0.134815  [  608/ 3200]\n",
      "loss: 0.023790  [  624/ 3200]\n",
      "loss: 0.048282  [  640/ 3200]\n",
      "loss: 0.069405  [  656/ 3200]\n",
      "loss: 0.026455  [  672/ 3200]\n",
      "loss: 0.012977  [  688/ 3200]\n",
      "loss: 0.074619  [  704/ 3200]\n",
      "loss: 0.053956  [  720/ 3200]\n",
      "loss: 0.030003  [  736/ 3200]\n",
      "loss: 0.063916  [  752/ 3200]\n",
      "loss: 0.025950  [  768/ 3200]\n",
      "loss: 0.027594  [  784/ 3200]\n",
      "loss: 0.039350  [  800/ 3200]\n",
      "loss: 0.052293  [  816/ 3200]\n",
      "loss: 0.035857  [  832/ 3200]\n",
      "loss: 0.022520  [  848/ 3200]\n",
      "loss: 0.208178  [  864/ 3200]\n",
      "loss: 0.047868  [  880/ 3200]\n",
      "loss: 0.039320  [  896/ 3200]\n",
      "loss: 0.035269  [  912/ 3200]\n",
      "loss: 0.078919  [  928/ 3200]\n",
      "loss: 0.039602  [  944/ 3200]\n",
      "loss: 0.033946  [  960/ 3200]\n",
      "loss: 0.056696  [  976/ 3200]\n",
      "loss: 0.053622  [  992/ 3200]\n",
      "loss: 0.041705  [ 1008/ 3200]\n",
      "loss: 0.161481  [ 1024/ 3200]\n",
      "loss: 0.030015  [ 1040/ 3200]\n",
      "loss: 0.171520  [ 1056/ 3200]\n",
      "loss: 0.051822  [ 1072/ 3200]\n",
      "loss: 0.072746  [ 1088/ 3200]\n",
      "loss: 0.097864  [ 1104/ 3200]\n",
      "loss: 0.079427  [ 1120/ 3200]\n",
      "loss: 0.048896  [ 1136/ 3200]\n",
      "loss: 0.019008  [ 1152/ 3200]\n",
      "loss: 0.017974  [ 1168/ 3200]\n",
      "loss: 0.065040  [ 1184/ 3200]\n",
      "loss: 0.082083  [ 1200/ 3200]\n",
      "loss: 0.177472  [ 1216/ 3200]\n",
      "loss: 0.114669  [ 1232/ 3200]\n",
      "loss: 0.086831  [ 1248/ 3200]\n",
      "loss: 0.023805  [ 1264/ 3200]\n",
      "loss: 0.039529  [ 1280/ 3200]\n",
      "loss: 0.023875  [ 1296/ 3200]\n",
      "loss: 0.008105  [ 1312/ 3200]\n",
      "loss: 0.048675  [ 1328/ 3200]\n",
      "loss: 0.099601  [ 1344/ 3200]\n",
      "loss: 0.056428  [ 1360/ 3200]\n",
      "loss: 0.022858  [ 1376/ 3200]\n",
      "loss: 0.056221  [ 1392/ 3200]\n",
      "loss: 0.168943  [ 1408/ 3200]\n",
      "loss: 0.089031  [ 1424/ 3200]\n",
      "loss: 0.046319  [ 1440/ 3200]\n",
      "loss: 0.136148  [ 1456/ 3200]\n",
      "loss: 0.068266  [ 1472/ 3200]\n",
      "loss: 0.018967  [ 1488/ 3200]\n",
      "loss: 0.046272  [ 1504/ 3200]\n",
      "loss: 0.109702  [ 1520/ 3200]\n",
      "loss: 0.027874  [ 1536/ 3200]\n",
      "loss: 0.074339  [ 1552/ 3200]\n",
      "loss: 0.017593  [ 1568/ 3200]\n",
      "loss: 0.061429  [ 1584/ 3200]\n",
      "loss: 0.019363  [ 1600/ 3200]\n",
      "loss: 0.035773  [ 1616/ 3200]\n",
      "loss: 0.039160  [ 1632/ 3200]\n",
      "loss: 0.009806  [ 1648/ 3200]\n",
      "loss: 0.067502  [ 1664/ 3200]\n",
      "loss: 0.206247  [ 1680/ 3200]\n",
      "loss: 0.025134  [ 1696/ 3200]\n",
      "loss: 0.020941  [ 1712/ 3200]\n",
      "loss: 0.047524  [ 1728/ 3200]\n",
      "loss: 0.051696  [ 1744/ 3200]\n",
      "loss: 0.023254  [ 1760/ 3200]\n",
      "loss: 0.012645  [ 1776/ 3200]\n",
      "loss: 0.036704  [ 1792/ 3200]\n",
      "loss: 0.039217  [ 1808/ 3200]\n",
      "loss: 0.019354  [ 1824/ 3200]\n",
      "loss: 0.059520  [ 1840/ 3200]\n",
      "loss: 0.114958  [ 1856/ 3200]\n",
      "loss: 0.085818  [ 1872/ 3200]\n",
      "loss: 0.025125  [ 1888/ 3200]\n",
      "loss: 0.036859  [ 1904/ 3200]\n",
      "loss: 0.055643  [ 1920/ 3200]\n",
      "loss: 0.027884  [ 1936/ 3200]\n",
      "loss: 0.050901  [ 1952/ 3200]\n",
      "loss: 0.095497  [ 1968/ 3200]\n",
      "loss: 0.025288  [ 1984/ 3200]\n",
      "loss: 0.022351  [ 2000/ 3200]\n",
      "loss: 0.024683  [ 2016/ 3200]\n",
      "loss: 0.006467  [ 2032/ 3200]\n",
      "loss: 0.017345  [ 2048/ 3200]\n",
      "loss: 0.048744  [ 2064/ 3200]\n",
      "loss: 0.065770  [ 2080/ 3200]\n",
      "loss: 0.084961  [ 2096/ 3200]\n",
      "loss: 0.087982  [ 2112/ 3200]\n",
      "loss: 0.078852  [ 2128/ 3200]\n",
      "loss: 0.079574  [ 2144/ 3200]\n",
      "loss: 0.039953  [ 2160/ 3200]\n",
      "loss: 0.076951  [ 2176/ 3200]\n",
      "loss: 0.087362  [ 2192/ 3200]\n",
      "loss: 0.059801  [ 2208/ 3200]\n",
      "loss: 0.038418  [ 2224/ 3200]\n",
      "loss: 0.031771  [ 2240/ 3200]\n",
      "loss: 0.039534  [ 2256/ 3200]\n",
      "loss: 0.018675  [ 2272/ 3200]\n",
      "loss: 0.045289  [ 2288/ 3200]\n",
      "loss: 0.050535  [ 2304/ 3200]\n",
      "loss: 0.063360  [ 2320/ 3200]\n",
      "loss: 0.018526  [ 2336/ 3200]\n",
      "loss: 0.052663  [ 2352/ 3200]\n",
      "loss: 0.026969  [ 2368/ 3200]\n",
      "loss: 0.014749  [ 2384/ 3200]\n",
      "loss: 0.031496  [ 2400/ 3200]\n",
      "loss: 0.035655  [ 2416/ 3200]\n",
      "loss: 0.089904  [ 2432/ 3200]\n",
      "loss: 0.042321  [ 2448/ 3200]\n",
      "loss: 0.134457  [ 2464/ 3200]\n",
      "loss: 0.018936  [ 2480/ 3200]\n",
      "loss: 0.155781  [ 2496/ 3200]\n",
      "loss: 0.061238  [ 2512/ 3200]\n",
      "loss: 0.048193  [ 2528/ 3200]\n",
      "loss: 0.042284  [ 2544/ 3200]\n",
      "loss: 0.041429  [ 2560/ 3200]\n",
      "loss: 0.060368  [ 2576/ 3200]\n",
      "loss: 0.103196  [ 2592/ 3200]\n",
      "loss: 0.023128  [ 2608/ 3200]\n",
      "loss: 0.185575  [ 2624/ 3200]\n",
      "loss: 0.010456  [ 2640/ 3200]\n",
      "loss: 0.029268  [ 2656/ 3200]\n",
      "loss: 0.050940  [ 2672/ 3200]\n",
      "loss: 0.041786  [ 2688/ 3200]\n",
      "loss: 0.023665  [ 2704/ 3200]\n",
      "loss: 0.025987  [ 2720/ 3200]\n",
      "loss: 0.056722  [ 2736/ 3200]\n",
      "loss: 0.052027  [ 2752/ 3200]\n",
      "loss: 0.036751  [ 2768/ 3200]\n",
      "loss: 0.023613  [ 2784/ 3200]\n",
      "loss: 0.082298  [ 2800/ 3200]\n",
      "loss: 0.042647  [ 2816/ 3200]\n",
      "loss: 0.084630  [ 2832/ 3200]\n",
      "loss: 0.020049  [ 2848/ 3200]\n",
      "loss: 0.130508  [ 2864/ 3200]\n",
      "loss: 0.045023  [ 2880/ 3200]\n",
      "loss: 0.039438  [ 2896/ 3200]\n",
      "loss: 0.029110  [ 2912/ 3200]\n",
      "loss: 0.053462  [ 2928/ 3200]\n",
      "loss: 0.026599  [ 2944/ 3200]\n",
      "loss: 0.010423  [ 2960/ 3200]\n",
      "loss: 0.035761  [ 2976/ 3200]\n",
      "loss: 0.030046  [ 2992/ 3200]\n",
      "loss: 0.067195  [ 3008/ 3200]\n",
      "loss: 0.025719  [ 3024/ 3200]\n",
      "loss: 0.051425  [ 3040/ 3200]\n",
      "loss: 0.102746  [ 3056/ 3200]\n",
      "loss: 0.026253  [ 3072/ 3200]\n",
      "loss: 0.083420  [ 3088/ 3200]\n",
      "loss: 0.031399  [ 3104/ 3200]\n",
      "loss: 0.143365  [ 3120/ 3200]\n",
      "loss: 0.032717  [ 3136/ 3200]\n",
      "loss: 0.023253  [ 3152/ 3200]\n",
      "loss: 0.036381  [ 3168/ 3200]\n",
      "loss: 0.021791  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.048761\n",
      "f1 macro averaged score: 0.767496\n",
      "Accuracy               : 76.8%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  13,   0,  13],\n",
      "        [ 17, 124,  14,  45],\n",
      "        [  0,  23, 160,  17],\n",
      "        [  9,  22,  13, 156]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2534e-04.\n",
      "\n",
      "Epoch: 55\n",
      "-----------------------------\n",
      "loss: 0.039618  [    0/ 3200]\n",
      "loss: 0.033013  [   16/ 3200]\n",
      "loss: 0.055840  [   32/ 3200]\n",
      "loss: 0.011006  [   48/ 3200]\n",
      "loss: 0.058071  [   64/ 3200]\n",
      "loss: 0.022579  [   80/ 3200]\n",
      "loss: 0.020156  [   96/ 3200]\n",
      "loss: 0.010094  [  112/ 3200]\n",
      "loss: 0.008520  [  128/ 3200]\n",
      "loss: 0.056448  [  144/ 3200]\n",
      "loss: 0.018379  [  160/ 3200]\n",
      "loss: 0.102588  [  176/ 3200]\n",
      "loss: 0.089461  [  192/ 3200]\n",
      "loss: 0.068371  [  208/ 3200]\n",
      "loss: 0.017559  [  224/ 3200]\n",
      "loss: 0.094996  [  240/ 3200]\n",
      "loss: 0.026922  [  256/ 3200]\n",
      "loss: 0.056964  [  272/ 3200]\n",
      "loss: 0.058529  [  288/ 3200]\n",
      "loss: 0.079396  [  304/ 3200]\n",
      "loss: 0.043941  [  320/ 3200]\n",
      "loss: 0.038076  [  336/ 3200]\n",
      "loss: 0.368485  [  352/ 3200]\n",
      "loss: 0.145458  [  368/ 3200]\n",
      "loss: 0.060979  [  384/ 3200]\n",
      "loss: 0.032248  [  400/ 3200]\n",
      "loss: 0.040352  [  416/ 3200]\n",
      "loss: 0.085712  [  432/ 3200]\n",
      "loss: 0.043517  [  448/ 3200]\n",
      "loss: 0.035562  [  464/ 3200]\n",
      "loss: 0.060117  [  480/ 3200]\n",
      "loss: 0.061108  [  496/ 3200]\n",
      "loss: 0.072638  [  512/ 3200]\n",
      "loss: 0.100904  [  528/ 3200]\n",
      "loss: 0.035880  [  544/ 3200]\n",
      "loss: 0.068892  [  560/ 3200]\n",
      "loss: 0.056176  [  576/ 3200]\n",
      "loss: 0.131277  [  592/ 3200]\n",
      "loss: 0.064141  [  608/ 3200]\n",
      "loss: 0.064179  [  624/ 3200]\n",
      "loss: 0.044302  [  640/ 3200]\n",
      "loss: 0.010711  [  656/ 3200]\n",
      "loss: 0.060053  [  672/ 3200]\n",
      "loss: 0.016559  [  688/ 3200]\n",
      "loss: 0.033378  [  704/ 3200]\n",
      "loss: 0.033607  [  720/ 3200]\n",
      "loss: 0.012359  [  736/ 3200]\n",
      "loss: 0.041029  [  752/ 3200]\n",
      "loss: 0.040861  [  768/ 3200]\n",
      "loss: 0.022277  [  784/ 3200]\n",
      "loss: 0.029640  [  800/ 3200]\n",
      "loss: 0.017727  [  816/ 3200]\n",
      "loss: 0.059027  [  832/ 3200]\n",
      "loss: 0.025099  [  848/ 3200]\n",
      "loss: 0.083035  [  864/ 3200]\n",
      "loss: 0.026979  [  880/ 3200]\n",
      "loss: 0.032331  [  896/ 3200]\n",
      "loss: 0.021198  [  912/ 3200]\n",
      "loss: 0.029099  [  928/ 3200]\n",
      "loss: 0.063921  [  944/ 3200]\n",
      "loss: 0.021727  [  960/ 3200]\n",
      "loss: 0.105906  [  976/ 3200]\n",
      "loss: 0.117480  [  992/ 3200]\n",
      "loss: 0.030840  [ 1008/ 3200]\n",
      "loss: 0.088813  [ 1024/ 3200]\n",
      "loss: 0.015547  [ 1040/ 3200]\n",
      "loss: 0.038970  [ 1056/ 3200]\n",
      "loss: 0.030705  [ 1072/ 3200]\n",
      "loss: 0.059876  [ 1088/ 3200]\n",
      "loss: 0.087747  [ 1104/ 3200]\n",
      "loss: 0.081927  [ 1120/ 3200]\n",
      "loss: 0.067644  [ 1136/ 3200]\n",
      "loss: 0.034503  [ 1152/ 3200]\n",
      "loss: 0.024777  [ 1168/ 3200]\n",
      "loss: 0.027239  [ 1184/ 3200]\n",
      "loss: 0.028105  [ 1200/ 3200]\n",
      "loss: 0.021024  [ 1216/ 3200]\n",
      "loss: 0.051105  [ 1232/ 3200]\n",
      "loss: 0.025371  [ 1248/ 3200]\n",
      "loss: 0.033815  [ 1264/ 3200]\n",
      "loss: 0.022867  [ 1280/ 3200]\n",
      "loss: 0.114064  [ 1296/ 3200]\n",
      "loss: 0.020494  [ 1312/ 3200]\n",
      "loss: 0.050722  [ 1328/ 3200]\n",
      "loss: 0.227354  [ 1344/ 3200]\n",
      "loss: 0.113157  [ 1360/ 3200]\n",
      "loss: 0.038048  [ 1376/ 3200]\n",
      "loss: 0.082438  [ 1392/ 3200]\n",
      "loss: 0.038745  [ 1408/ 3200]\n",
      "loss: 0.038536  [ 1424/ 3200]\n",
      "loss: 0.025448  [ 1440/ 3200]\n",
      "loss: 0.063613  [ 1456/ 3200]\n",
      "loss: 0.015948  [ 1472/ 3200]\n",
      "loss: 0.200066  [ 1488/ 3200]\n",
      "loss: 0.051222  [ 1504/ 3200]\n",
      "loss: 0.068490  [ 1520/ 3200]\n",
      "loss: 0.049545  [ 1536/ 3200]\n",
      "loss: 0.031002  [ 1552/ 3200]\n",
      "loss: 0.031921  [ 1568/ 3200]\n",
      "loss: 0.085343  [ 1584/ 3200]\n",
      "loss: 0.093816  [ 1600/ 3200]\n",
      "loss: 0.013738  [ 1616/ 3200]\n",
      "loss: 0.052606  [ 1632/ 3200]\n",
      "loss: 0.042079  [ 1648/ 3200]\n",
      "loss: 0.077526  [ 1664/ 3200]\n",
      "loss: 0.036176  [ 1680/ 3200]\n",
      "loss: 0.032811  [ 1696/ 3200]\n",
      "loss: 0.036880  [ 1712/ 3200]\n",
      "loss: 0.050277  [ 1728/ 3200]\n",
      "loss: 0.038612  [ 1744/ 3200]\n",
      "loss: 0.082809  [ 1760/ 3200]\n",
      "loss: 0.024301  [ 1776/ 3200]\n",
      "loss: 0.012361  [ 1792/ 3200]\n",
      "loss: 0.040268  [ 1808/ 3200]\n",
      "loss: 0.034469  [ 1824/ 3200]\n",
      "loss: 0.004223  [ 1840/ 3200]\n",
      "loss: 0.035385  [ 1856/ 3200]\n",
      "loss: 0.126615  [ 1872/ 3200]\n",
      "loss: 0.033356  [ 1888/ 3200]\n",
      "loss: 0.050481  [ 1904/ 3200]\n",
      "loss: 0.057914  [ 1920/ 3200]\n",
      "loss: 0.103862  [ 1936/ 3200]\n",
      "loss: 0.009114  [ 1952/ 3200]\n",
      "loss: 0.067335  [ 1968/ 3200]\n",
      "loss: 0.042341  [ 1984/ 3200]\n",
      "loss: 0.023649  [ 2000/ 3200]\n",
      "loss: 0.127981  [ 2016/ 3200]\n",
      "loss: 0.076145  [ 2032/ 3200]\n",
      "loss: 0.018892  [ 2048/ 3200]\n",
      "loss: 0.052894  [ 2064/ 3200]\n",
      "loss: 0.041303  [ 2080/ 3200]\n",
      "loss: 0.021518  [ 2096/ 3200]\n",
      "loss: 0.017398  [ 2112/ 3200]\n",
      "loss: 0.070566  [ 2128/ 3200]\n",
      "loss: 0.041253  [ 2144/ 3200]\n",
      "loss: 0.055010  [ 2160/ 3200]\n",
      "loss: 0.018557  [ 2176/ 3200]\n",
      "loss: 0.029921  [ 2192/ 3200]\n",
      "loss: 0.110760  [ 2208/ 3200]\n",
      "loss: 0.090982  [ 2224/ 3200]\n",
      "loss: 0.060030  [ 2240/ 3200]\n",
      "loss: 0.021950  [ 2256/ 3200]\n",
      "loss: 0.021710  [ 2272/ 3200]\n",
      "loss: 0.062930  [ 2288/ 3200]\n",
      "loss: 0.100212  [ 2304/ 3200]\n",
      "loss: 0.120543  [ 2320/ 3200]\n",
      "loss: 0.095512  [ 2336/ 3200]\n",
      "loss: 0.018570  [ 2352/ 3200]\n",
      "loss: 0.022805  [ 2368/ 3200]\n",
      "loss: 0.042498  [ 2384/ 3200]\n",
      "loss: 0.086037  [ 2400/ 3200]\n",
      "loss: 0.063759  [ 2416/ 3200]\n",
      "loss: 0.088833  [ 2432/ 3200]\n",
      "loss: 0.060206  [ 2448/ 3200]\n",
      "loss: 0.063092  [ 2464/ 3200]\n",
      "loss: 0.035675  [ 2480/ 3200]\n",
      "loss: 0.154667  [ 2496/ 3200]\n",
      "loss: 0.101730  [ 2512/ 3200]\n",
      "loss: 0.022782  [ 2528/ 3200]\n",
      "loss: 0.014670  [ 2544/ 3200]\n",
      "loss: 0.094203  [ 2560/ 3200]\n",
      "loss: 0.072233  [ 2576/ 3200]\n",
      "loss: 0.007211  [ 2592/ 3200]\n",
      "loss: 0.071840  [ 2608/ 3200]\n",
      "loss: 0.008884  [ 2624/ 3200]\n",
      "loss: 0.098493  [ 2640/ 3200]\n",
      "loss: 0.052290  [ 2656/ 3200]\n",
      "loss: 0.108377  [ 2672/ 3200]\n",
      "loss: 0.112420  [ 2688/ 3200]\n",
      "loss: 0.075595  [ 2704/ 3200]\n",
      "loss: 0.032586  [ 2720/ 3200]\n",
      "loss: 0.034898  [ 2736/ 3200]\n",
      "loss: 0.024784  [ 2752/ 3200]\n",
      "loss: 0.031245  [ 2768/ 3200]\n",
      "loss: 0.090362  [ 2784/ 3200]\n",
      "loss: 0.079447  [ 2800/ 3200]\n",
      "loss: 0.214344  [ 2816/ 3200]\n",
      "loss: 0.061563  [ 2832/ 3200]\n",
      "loss: 0.203718  [ 2848/ 3200]\n",
      "loss: 0.184074  [ 2864/ 3200]\n",
      "loss: 0.022044  [ 2880/ 3200]\n",
      "loss: 0.078304  [ 2896/ 3200]\n",
      "loss: 0.047609  [ 2912/ 3200]\n",
      "loss: 0.011944  [ 2928/ 3200]\n",
      "loss: 0.018945  [ 2944/ 3200]\n",
      "loss: 0.106327  [ 2960/ 3200]\n",
      "loss: 0.072819  [ 2976/ 3200]\n",
      "loss: 0.052912  [ 2992/ 3200]\n",
      "loss: 0.059456  [ 3008/ 3200]\n",
      "loss: 0.049513  [ 3024/ 3200]\n",
      "loss: 0.163585  [ 3040/ 3200]\n",
      "loss: 0.081074  [ 3056/ 3200]\n",
      "loss: 0.018833  [ 3072/ 3200]\n",
      "loss: 0.132754  [ 3088/ 3200]\n",
      "loss: 0.051695  [ 3104/ 3200]\n",
      "loss: 0.040630  [ 3120/ 3200]\n",
      "loss: 0.032332  [ 3136/ 3200]\n",
      "loss: 0.024162  [ 3152/ 3200]\n",
      "loss: 0.039165  [ 3168/ 3200]\n",
      "loss: 0.036896  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.048184\n",
      "f1 macro averaged score: 0.793141\n",
      "Accuracy               : 79.4%\n",
      "Confusion matrix       :\n",
      "tensor([[178,  11,   0,  11],\n",
      "        [ 15, 131,  15,  39],\n",
      "        [  0,  20, 161,  19],\n",
      "        [ 11,  14,  10, 165]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1908e-04.\n",
      "\n",
      "Epoch: 56\n",
      "-----------------------------\n",
      "loss: 0.009259  [    0/ 3200]\n",
      "loss: 0.016960  [   16/ 3200]\n",
      "loss: 0.005212  [   32/ 3200]\n",
      "loss: 0.027078  [   48/ 3200]\n",
      "loss: 0.022754  [   64/ 3200]\n",
      "loss: 0.037819  [   80/ 3200]\n",
      "loss: 0.037855  [   96/ 3200]\n",
      "loss: 0.007415  [  112/ 3200]\n",
      "loss: 0.036791  [  128/ 3200]\n",
      "loss: 0.042815  [  144/ 3200]\n",
      "loss: 0.057121  [  160/ 3200]\n",
      "loss: 0.054676  [  176/ 3200]\n",
      "loss: 0.037505  [  192/ 3200]\n",
      "loss: 0.063944  [  208/ 3200]\n",
      "loss: 0.019486  [  224/ 3200]\n",
      "loss: 0.027169  [  240/ 3200]\n",
      "loss: 0.100024  [  256/ 3200]\n",
      "loss: 0.045694  [  272/ 3200]\n",
      "loss: 0.029350  [  288/ 3200]\n",
      "loss: 0.030378  [  304/ 3200]\n",
      "loss: 0.238766  [  320/ 3200]\n",
      "loss: 0.032142  [  336/ 3200]\n",
      "loss: 0.019984  [  352/ 3200]\n",
      "loss: 0.016209  [  368/ 3200]\n",
      "loss: 0.225251  [  384/ 3200]\n",
      "loss: 0.017923  [  400/ 3200]\n",
      "loss: 0.052932  [  416/ 3200]\n",
      "loss: 0.170584  [  432/ 3200]\n",
      "loss: 0.285917  [  448/ 3200]\n",
      "loss: 0.045022  [  464/ 3200]\n",
      "loss: 0.029153  [  480/ 3200]\n",
      "loss: 0.062091  [  496/ 3200]\n",
      "loss: 0.035231  [  512/ 3200]\n",
      "loss: 0.051673  [  528/ 3200]\n",
      "loss: 0.012057  [  544/ 3200]\n",
      "loss: 0.058361  [  560/ 3200]\n",
      "loss: 0.100696  [  576/ 3200]\n",
      "loss: 0.032153  [  592/ 3200]\n",
      "loss: 0.059637  [  608/ 3200]\n",
      "loss: 0.031726  [  624/ 3200]\n",
      "loss: 0.057826  [  640/ 3200]\n",
      "loss: 0.130136  [  656/ 3200]\n",
      "loss: 0.019942  [  672/ 3200]\n",
      "loss: 0.047589  [  688/ 3200]\n",
      "loss: 0.036741  [  704/ 3200]\n",
      "loss: 0.024116  [  720/ 3200]\n",
      "loss: 0.069520  [  736/ 3200]\n",
      "loss: 0.025649  [  752/ 3200]\n",
      "loss: 0.014078  [  768/ 3200]\n",
      "loss: 0.120254  [  784/ 3200]\n",
      "loss: 0.059005  [  800/ 3200]\n",
      "loss: 0.016264  [  816/ 3200]\n",
      "loss: 0.149203  [  832/ 3200]\n",
      "loss: 0.058988  [  848/ 3200]\n",
      "loss: 0.047818  [  864/ 3200]\n",
      "loss: 0.047532  [  880/ 3200]\n",
      "loss: 0.050751  [  896/ 3200]\n",
      "loss: 0.035743  [  912/ 3200]\n",
      "loss: 0.133313  [  928/ 3200]\n",
      "loss: 0.011297  [  944/ 3200]\n",
      "loss: 0.025166  [  960/ 3200]\n",
      "loss: 0.087854  [  976/ 3200]\n",
      "loss: 0.026162  [  992/ 3200]\n",
      "loss: 0.024479  [ 1008/ 3200]\n",
      "loss: 0.070220  [ 1024/ 3200]\n",
      "loss: 0.023215  [ 1040/ 3200]\n",
      "loss: 0.023079  [ 1056/ 3200]\n",
      "loss: 0.019424  [ 1072/ 3200]\n",
      "loss: 0.018180  [ 1088/ 3200]\n",
      "loss: 0.106393  [ 1104/ 3200]\n",
      "loss: 0.058619  [ 1120/ 3200]\n",
      "loss: 0.046995  [ 1136/ 3200]\n",
      "loss: 0.054610  [ 1152/ 3200]\n",
      "loss: 0.058684  [ 1168/ 3200]\n",
      "loss: 0.027032  [ 1184/ 3200]\n",
      "loss: 0.007986  [ 1200/ 3200]\n",
      "loss: 0.006983  [ 1216/ 3200]\n",
      "loss: 0.064576  [ 1232/ 3200]\n",
      "loss: 0.038964  [ 1248/ 3200]\n",
      "loss: 0.055959  [ 1264/ 3200]\n",
      "loss: 0.063834  [ 1280/ 3200]\n",
      "loss: 0.031067  [ 1296/ 3200]\n",
      "loss: 0.028654  [ 1312/ 3200]\n",
      "loss: 0.043808  [ 1328/ 3200]\n",
      "loss: 0.164733  [ 1344/ 3200]\n",
      "loss: 0.052896  [ 1360/ 3200]\n",
      "loss: 0.069009  [ 1376/ 3200]\n",
      "loss: 0.117203  [ 1392/ 3200]\n",
      "loss: 0.058530  [ 1408/ 3200]\n",
      "loss: 0.093290  [ 1424/ 3200]\n",
      "loss: 0.122091  [ 1440/ 3200]\n",
      "loss: 0.042831  [ 1456/ 3200]\n",
      "loss: 0.041076  [ 1472/ 3200]\n",
      "loss: 0.025170  [ 1488/ 3200]\n",
      "loss: 0.008659  [ 1504/ 3200]\n",
      "loss: 0.053273  [ 1520/ 3200]\n",
      "loss: 0.013964  [ 1536/ 3200]\n",
      "loss: 0.049364  [ 1552/ 3200]\n",
      "loss: 0.069059  [ 1568/ 3200]\n",
      "loss: 0.040730  [ 1584/ 3200]\n",
      "loss: 0.119250  [ 1600/ 3200]\n",
      "loss: 0.021806  [ 1616/ 3200]\n",
      "loss: 0.043421  [ 1632/ 3200]\n",
      "loss: 0.043267  [ 1648/ 3200]\n",
      "loss: 0.010078  [ 1664/ 3200]\n",
      "loss: 0.021657  [ 1680/ 3200]\n",
      "loss: 0.017651  [ 1696/ 3200]\n",
      "loss: 0.063437  [ 1712/ 3200]\n",
      "loss: 0.036575  [ 1728/ 3200]\n",
      "loss: 0.068925  [ 1744/ 3200]\n",
      "loss: 0.028147  [ 1760/ 3200]\n",
      "loss: 0.052485  [ 1776/ 3200]\n",
      "loss: 0.053071  [ 1792/ 3200]\n",
      "loss: 0.145931  [ 1808/ 3200]\n",
      "loss: 0.086242  [ 1824/ 3200]\n",
      "loss: 0.017477  [ 1840/ 3200]\n",
      "loss: 0.137110  [ 1856/ 3200]\n",
      "loss: 0.031838  [ 1872/ 3200]\n",
      "loss: 0.032728  [ 1888/ 3200]\n",
      "loss: 0.032500  [ 1904/ 3200]\n",
      "loss: 0.086651  [ 1920/ 3200]\n",
      "loss: 0.048851  [ 1936/ 3200]\n",
      "loss: 0.025616  [ 1952/ 3200]\n",
      "loss: 0.041340  [ 1968/ 3200]\n",
      "loss: 0.037820  [ 1984/ 3200]\n",
      "loss: 0.007423  [ 2000/ 3200]\n",
      "loss: 0.021900  [ 2016/ 3200]\n",
      "loss: 0.019612  [ 2032/ 3200]\n",
      "loss: 0.069598  [ 2048/ 3200]\n",
      "loss: 0.036869  [ 2064/ 3200]\n",
      "loss: 0.038928  [ 2080/ 3200]\n",
      "loss: 0.026934  [ 2096/ 3200]\n",
      "loss: 0.026817  [ 2112/ 3200]\n",
      "loss: 0.054355  [ 2128/ 3200]\n",
      "loss: 0.155493  [ 2144/ 3200]\n",
      "loss: 0.024732  [ 2160/ 3200]\n",
      "loss: 0.046049  [ 2176/ 3200]\n",
      "loss: 0.031857  [ 2192/ 3200]\n",
      "loss: 0.207496  [ 2208/ 3200]\n",
      "loss: 0.018606  [ 2224/ 3200]\n",
      "loss: 0.027804  [ 2240/ 3200]\n",
      "loss: 0.074627  [ 2256/ 3200]\n",
      "loss: 0.034558  [ 2272/ 3200]\n",
      "loss: 0.036958  [ 2288/ 3200]\n",
      "loss: 0.011357  [ 2304/ 3200]\n",
      "loss: 0.120930  [ 2320/ 3200]\n",
      "loss: 0.112733  [ 2336/ 3200]\n",
      "loss: 0.072887  [ 2352/ 3200]\n",
      "loss: 0.019537  [ 2368/ 3200]\n",
      "loss: 0.053892  [ 2384/ 3200]\n",
      "loss: 0.056543  [ 2400/ 3200]\n",
      "loss: 0.058812  [ 2416/ 3200]\n",
      "loss: 0.094049  [ 2432/ 3200]\n",
      "loss: 0.030068  [ 2448/ 3200]\n",
      "loss: 0.029593  [ 2464/ 3200]\n",
      "loss: 0.056763  [ 2480/ 3200]\n",
      "loss: 0.028188  [ 2496/ 3200]\n",
      "loss: 0.031777  [ 2512/ 3200]\n",
      "loss: 0.044415  [ 2528/ 3200]\n",
      "loss: 0.076562  [ 2544/ 3200]\n",
      "loss: 0.157981  [ 2560/ 3200]\n",
      "loss: 0.039538  [ 2576/ 3200]\n",
      "loss: 0.045731  [ 2592/ 3200]\n",
      "loss: 0.079734  [ 2608/ 3200]\n",
      "loss: 0.021133  [ 2624/ 3200]\n",
      "loss: 0.071130  [ 2640/ 3200]\n",
      "loss: 0.044723  [ 2656/ 3200]\n",
      "loss: 0.047348  [ 2672/ 3200]\n",
      "loss: 0.012757  [ 2688/ 3200]\n",
      "loss: 0.125627  [ 2704/ 3200]\n",
      "loss: 0.022432  [ 2720/ 3200]\n",
      "loss: 0.034270  [ 2736/ 3200]\n",
      "loss: 0.017247  [ 2752/ 3200]\n",
      "loss: 0.118517  [ 2768/ 3200]\n",
      "loss: 0.037509  [ 2784/ 3200]\n",
      "loss: 0.043462  [ 2800/ 3200]\n",
      "loss: 0.016773  [ 2816/ 3200]\n",
      "loss: 0.022139  [ 2832/ 3200]\n",
      "loss: 0.064093  [ 2848/ 3200]\n",
      "loss: 0.071618  [ 2864/ 3200]\n",
      "loss: 0.035762  [ 2880/ 3200]\n",
      "loss: 0.048094  [ 2896/ 3200]\n",
      "loss: 0.020355  [ 2912/ 3200]\n",
      "loss: 0.058831  [ 2928/ 3200]\n",
      "loss: 0.019841  [ 2944/ 3200]\n",
      "loss: 0.023299  [ 2960/ 3200]\n",
      "loss: 0.044767  [ 2976/ 3200]\n",
      "loss: 0.053269  [ 2992/ 3200]\n",
      "loss: 0.023708  [ 3008/ 3200]\n",
      "loss: 0.065344  [ 3024/ 3200]\n",
      "loss: 0.029590  [ 3040/ 3200]\n",
      "loss: 0.009663  [ 3056/ 3200]\n",
      "loss: 0.044181  [ 3072/ 3200]\n",
      "loss: 0.082504  [ 3088/ 3200]\n",
      "loss: 0.028813  [ 3104/ 3200]\n",
      "loss: 0.027750  [ 3120/ 3200]\n",
      "loss: 0.034597  [ 3136/ 3200]\n",
      "loss: 0.029167  [ 3152/ 3200]\n",
      "loss: 0.160257  [ 3168/ 3200]\n",
      "loss: 0.052442  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.048430\n",
      "f1 macro averaged score: 0.797398\n",
      "Accuracy               : 79.8%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  10,   0,  15],\n",
      "        [ 16, 135,  15,  34],\n",
      "        [  0,  20, 164,  16],\n",
      "        [  8,  17,  11, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1312e-04.\n",
      "\n",
      "Epoch: 57\n",
      "-----------------------------\n",
      "loss: 0.016278  [    0/ 3200]\n",
      "loss: 0.057527  [   16/ 3200]\n",
      "loss: 0.013856  [   32/ 3200]\n",
      "loss: 0.021378  [   48/ 3200]\n",
      "loss: 0.032281  [   64/ 3200]\n",
      "loss: 0.026625  [   80/ 3200]\n",
      "loss: 0.014967  [   96/ 3200]\n",
      "loss: 0.050947  [  112/ 3200]\n",
      "loss: 0.032498  [  128/ 3200]\n",
      "loss: 0.035509  [  144/ 3200]\n",
      "loss: 0.219436  [  160/ 3200]\n",
      "loss: 0.031145  [  176/ 3200]\n",
      "loss: 0.026443  [  192/ 3200]\n",
      "loss: 0.062553  [  208/ 3200]\n",
      "loss: 0.022558  [  224/ 3200]\n",
      "loss: 0.060956  [  240/ 3200]\n",
      "loss: 0.019186  [  256/ 3200]\n",
      "loss: 0.016373  [  272/ 3200]\n",
      "loss: 0.101456  [  288/ 3200]\n",
      "loss: 0.071170  [  304/ 3200]\n",
      "loss: 0.060257  [  320/ 3200]\n",
      "loss: 0.011048  [  336/ 3200]\n",
      "loss: 0.041417  [  352/ 3200]\n",
      "loss: 0.148728  [  368/ 3200]\n",
      "loss: 0.030399  [  384/ 3200]\n",
      "loss: 0.131259  [  400/ 3200]\n",
      "loss: 0.009038  [  416/ 3200]\n",
      "loss: 0.032138  [  432/ 3200]\n",
      "loss: 0.024503  [  448/ 3200]\n",
      "loss: 0.113997  [  464/ 3200]\n",
      "loss: 0.026438  [  480/ 3200]\n",
      "loss: 0.173028  [  496/ 3200]\n",
      "loss: 0.034835  [  512/ 3200]\n",
      "loss: 0.040782  [  528/ 3200]\n",
      "loss: 0.078498  [  544/ 3200]\n",
      "loss: 0.209126  [  560/ 3200]\n",
      "loss: 0.069517  [  576/ 3200]\n",
      "loss: 0.028873  [  592/ 3200]\n",
      "loss: 0.019300  [  608/ 3200]\n",
      "loss: 0.043872  [  624/ 3200]\n",
      "loss: 0.074464  [  640/ 3200]\n",
      "loss: 0.221522  [  656/ 3200]\n",
      "loss: 0.043994  [  672/ 3200]\n",
      "loss: 0.038194  [  688/ 3200]\n",
      "loss: 0.105570  [  704/ 3200]\n",
      "loss: 0.041435  [  720/ 3200]\n",
      "loss: 0.016475  [  736/ 3200]\n",
      "loss: 0.074305  [  752/ 3200]\n",
      "loss: 0.048014  [  768/ 3200]\n",
      "loss: 0.024181  [  784/ 3200]\n",
      "loss: 0.040630  [  800/ 3200]\n",
      "loss: 0.161096  [  816/ 3200]\n",
      "loss: 0.009265  [  832/ 3200]\n",
      "loss: 0.019205  [  848/ 3200]\n",
      "loss: 0.143305  [  864/ 3200]\n",
      "loss: 0.272153  [  880/ 3200]\n",
      "loss: 0.025823  [  896/ 3200]\n",
      "loss: 0.021580  [  912/ 3200]\n",
      "loss: 0.007476  [  928/ 3200]\n",
      "loss: 0.067009  [  944/ 3200]\n",
      "loss: 0.035911  [  960/ 3200]\n",
      "loss: 0.016863  [  976/ 3200]\n",
      "loss: 0.017091  [  992/ 3200]\n",
      "loss: 0.044312  [ 1008/ 3200]\n",
      "loss: 0.009153  [ 1024/ 3200]\n",
      "loss: 0.063997  [ 1040/ 3200]\n",
      "loss: 0.115595  [ 1056/ 3200]\n",
      "loss: 0.032949  [ 1072/ 3200]\n",
      "loss: 0.083441  [ 1088/ 3200]\n",
      "loss: 0.050217  [ 1104/ 3200]\n",
      "loss: 0.090425  [ 1120/ 3200]\n",
      "loss: 0.038104  [ 1136/ 3200]\n",
      "loss: 0.064039  [ 1152/ 3200]\n",
      "loss: 0.055207  [ 1168/ 3200]\n",
      "loss: 0.167136  [ 1184/ 3200]\n",
      "loss: 0.077106  [ 1200/ 3200]\n",
      "loss: 0.019416  [ 1216/ 3200]\n",
      "loss: 0.065152  [ 1232/ 3200]\n",
      "loss: 0.022538  [ 1248/ 3200]\n",
      "loss: 0.012987  [ 1264/ 3200]\n",
      "loss: 0.152079  [ 1280/ 3200]\n",
      "loss: 0.040819  [ 1296/ 3200]\n",
      "loss: 0.026136  [ 1312/ 3200]\n",
      "loss: 0.050404  [ 1328/ 3200]\n",
      "loss: 0.084826  [ 1344/ 3200]\n",
      "loss: 0.014424  [ 1360/ 3200]\n",
      "loss: 0.041719  [ 1376/ 3200]\n",
      "loss: 0.015030  [ 1392/ 3200]\n",
      "loss: 0.059369  [ 1408/ 3200]\n",
      "loss: 0.076912  [ 1424/ 3200]\n",
      "loss: 0.018813  [ 1440/ 3200]\n",
      "loss: 0.065449  [ 1456/ 3200]\n",
      "loss: 0.030763  [ 1472/ 3200]\n",
      "loss: 0.019929  [ 1488/ 3200]\n",
      "loss: 0.030183  [ 1504/ 3200]\n",
      "loss: 0.019460  [ 1520/ 3200]\n",
      "loss: 0.030947  [ 1536/ 3200]\n",
      "loss: 0.028705  [ 1552/ 3200]\n",
      "loss: 0.059730  [ 1568/ 3200]\n",
      "loss: 0.074153  [ 1584/ 3200]\n",
      "loss: 0.165930  [ 1600/ 3200]\n",
      "loss: 0.013622  [ 1616/ 3200]\n",
      "loss: 0.025734  [ 1632/ 3200]\n",
      "loss: 0.013566  [ 1648/ 3200]\n",
      "loss: 0.073274  [ 1664/ 3200]\n",
      "loss: 0.024538  [ 1680/ 3200]\n",
      "loss: 0.069526  [ 1696/ 3200]\n",
      "loss: 0.036705  [ 1712/ 3200]\n",
      "loss: 0.052777  [ 1728/ 3200]\n",
      "loss: 0.051932  [ 1744/ 3200]\n",
      "loss: 0.029316  [ 1760/ 3200]\n",
      "loss: 0.008341  [ 1776/ 3200]\n",
      "loss: 0.051289  [ 1792/ 3200]\n",
      "loss: 0.031602  [ 1808/ 3200]\n",
      "loss: 0.034269  [ 1824/ 3200]\n",
      "loss: 0.064127  [ 1840/ 3200]\n",
      "loss: 0.051681  [ 1856/ 3200]\n",
      "loss: 0.036891  [ 1872/ 3200]\n",
      "loss: 0.066889  [ 1888/ 3200]\n",
      "loss: 0.076071  [ 1904/ 3200]\n",
      "loss: 0.043761  [ 1920/ 3200]\n",
      "loss: 0.091679  [ 1936/ 3200]\n",
      "loss: 0.022159  [ 1952/ 3200]\n",
      "loss: 0.024548  [ 1968/ 3200]\n",
      "loss: 0.051391  [ 1984/ 3200]\n",
      "loss: 0.045839  [ 2000/ 3200]\n",
      "loss: 0.044275  [ 2016/ 3200]\n",
      "loss: 0.050412  [ 2032/ 3200]\n",
      "loss: 0.381628  [ 2048/ 3200]\n",
      "loss: 0.023635  [ 2064/ 3200]\n",
      "loss: 0.015501  [ 2080/ 3200]\n",
      "loss: 0.035274  [ 2096/ 3200]\n",
      "loss: 0.097547  [ 2112/ 3200]\n",
      "loss: 0.213151  [ 2128/ 3200]\n",
      "loss: 0.022389  [ 2144/ 3200]\n",
      "loss: 0.024494  [ 2160/ 3200]\n",
      "loss: 0.029518  [ 2176/ 3200]\n",
      "loss: 0.027915  [ 2192/ 3200]\n",
      "loss: 0.009631  [ 2208/ 3200]\n",
      "loss: 0.042570  [ 2224/ 3200]\n",
      "loss: 0.016129  [ 2240/ 3200]\n",
      "loss: 0.030968  [ 2256/ 3200]\n",
      "loss: 0.112400  [ 2272/ 3200]\n",
      "loss: 0.082294  [ 2288/ 3200]\n",
      "loss: 0.092338  [ 2304/ 3200]\n",
      "loss: 0.031875  [ 2320/ 3200]\n",
      "loss: 0.195606  [ 2336/ 3200]\n",
      "loss: 0.070779  [ 2352/ 3200]\n",
      "loss: 0.037351  [ 2368/ 3200]\n",
      "loss: 0.027542  [ 2384/ 3200]\n",
      "loss: 0.041222  [ 2400/ 3200]\n",
      "loss: 0.025268  [ 2416/ 3200]\n",
      "loss: 0.010990  [ 2432/ 3200]\n",
      "loss: 0.094946  [ 2448/ 3200]\n",
      "loss: 0.040111  [ 2464/ 3200]\n",
      "loss: 0.009729  [ 2480/ 3200]\n",
      "loss: 0.041347  [ 2496/ 3200]\n",
      "loss: 0.080734  [ 2512/ 3200]\n",
      "loss: 0.020852  [ 2528/ 3200]\n",
      "loss: 0.016264  [ 2544/ 3200]\n",
      "loss: 0.038828  [ 2560/ 3200]\n",
      "loss: 0.144868  [ 2576/ 3200]\n",
      "loss: 0.075130  [ 2592/ 3200]\n",
      "loss: 0.048376  [ 2608/ 3200]\n",
      "loss: 0.021766  [ 2624/ 3200]\n",
      "loss: 0.029406  [ 2640/ 3200]\n",
      "loss: 0.025600  [ 2656/ 3200]\n",
      "loss: 0.081707  [ 2672/ 3200]\n",
      "loss: 0.041194  [ 2688/ 3200]\n",
      "loss: 0.215275  [ 2704/ 3200]\n",
      "loss: 0.024264  [ 2720/ 3200]\n",
      "loss: 0.027551  [ 2736/ 3200]\n",
      "loss: 0.105724  [ 2752/ 3200]\n",
      "loss: 0.104351  [ 2768/ 3200]\n",
      "loss: 0.039219  [ 2784/ 3200]\n",
      "loss: 0.115877  [ 2800/ 3200]\n",
      "loss: 0.028025  [ 2816/ 3200]\n",
      "loss: 0.008668  [ 2832/ 3200]\n",
      "loss: 0.055052  [ 2848/ 3200]\n",
      "loss: 0.005575  [ 2864/ 3200]\n",
      "loss: 0.061574  [ 2880/ 3200]\n",
      "loss: 0.025077  [ 2896/ 3200]\n",
      "loss: 0.050070  [ 2912/ 3200]\n",
      "loss: 0.016581  [ 2928/ 3200]\n",
      "loss: 0.014540  [ 2944/ 3200]\n",
      "loss: 0.010819  [ 2960/ 3200]\n",
      "loss: 0.104746  [ 2976/ 3200]\n",
      "loss: 0.055704  [ 2992/ 3200]\n",
      "loss: 0.012384  [ 3008/ 3200]\n",
      "loss: 0.110837  [ 3024/ 3200]\n",
      "loss: 0.030684  [ 3040/ 3200]\n",
      "loss: 0.046831  [ 3056/ 3200]\n",
      "loss: 0.032365  [ 3072/ 3200]\n",
      "loss: 0.049580  [ 3088/ 3200]\n",
      "loss: 0.011486  [ 3104/ 3200]\n",
      "loss: 0.002397  [ 3120/ 3200]\n",
      "loss: 0.027059  [ 3136/ 3200]\n",
      "loss: 0.041513  [ 3152/ 3200]\n",
      "loss: 0.076585  [ 3168/ 3200]\n",
      "loss: 0.015023  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.048903\n",
      "f1 macro averaged score: 0.785811\n",
      "Accuracy               : 78.6%\n",
      "Confusion matrix       :\n",
      "tensor([[175,   9,   0,  16],\n",
      "        [ 15, 126,  16,  43],\n",
      "        [  0,  19, 161,  20],\n",
      "        [  8,  15,  10, 167]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0747e-04.\n",
      "\n",
      "Epoch: 58\n",
      "-----------------------------\n",
      "loss: 0.140314  [    0/ 3200]\n",
      "loss: 0.031771  [   16/ 3200]\n",
      "loss: 0.081628  [   32/ 3200]\n",
      "loss: 0.013441  [   48/ 3200]\n",
      "loss: 0.032740  [   64/ 3200]\n",
      "loss: 0.022650  [   80/ 3200]\n",
      "loss: 0.012310  [   96/ 3200]\n",
      "loss: 0.085597  [  112/ 3200]\n",
      "loss: 0.052580  [  128/ 3200]\n",
      "loss: 0.006319  [  144/ 3200]\n",
      "loss: 0.013688  [  160/ 3200]\n",
      "loss: 0.037199  [  176/ 3200]\n",
      "loss: 0.049521  [  192/ 3200]\n",
      "loss: 0.091586  [  208/ 3200]\n",
      "loss: 0.028191  [  224/ 3200]\n",
      "loss: 0.024148  [  240/ 3200]\n",
      "loss: 0.037369  [  256/ 3200]\n",
      "loss: 0.029064  [  272/ 3200]\n",
      "loss: 0.146956  [  288/ 3200]\n",
      "loss: 0.020520  [  304/ 3200]\n",
      "loss: 0.057377  [  320/ 3200]\n",
      "loss: 0.064550  [  336/ 3200]\n",
      "loss: 0.094783  [  352/ 3200]\n",
      "loss: 0.026448  [  368/ 3200]\n",
      "loss: 0.048341  [  384/ 3200]\n",
      "loss: 0.021128  [  400/ 3200]\n",
      "loss: 0.062407  [  416/ 3200]\n",
      "loss: 0.080209  [  432/ 3200]\n",
      "loss: 0.036333  [  448/ 3200]\n",
      "loss: 0.015060  [  464/ 3200]\n",
      "loss: 0.087313  [  480/ 3200]\n",
      "loss: 0.035815  [  496/ 3200]\n",
      "loss: 0.051701  [  512/ 3200]\n",
      "loss: 0.072869  [  528/ 3200]\n",
      "loss: 0.014730  [  544/ 3200]\n",
      "loss: 0.041718  [  560/ 3200]\n",
      "loss: 0.083205  [  576/ 3200]\n",
      "loss: 0.008002  [  592/ 3200]\n",
      "loss: 0.071392  [  608/ 3200]\n",
      "loss: 0.061808  [  624/ 3200]\n",
      "loss: 0.017625  [  640/ 3200]\n",
      "loss: 0.041566  [  656/ 3200]\n",
      "loss: 0.049328  [  672/ 3200]\n",
      "loss: 0.028406  [  688/ 3200]\n",
      "loss: 0.042495  [  704/ 3200]\n",
      "loss: 0.022452  [  720/ 3200]\n",
      "loss: 0.027247  [  736/ 3200]\n",
      "loss: 0.055334  [  752/ 3200]\n",
      "loss: 0.026571  [  768/ 3200]\n",
      "loss: 0.043767  [  784/ 3200]\n",
      "loss: 0.013305  [  800/ 3200]\n",
      "loss: 0.024445  [  816/ 3200]\n",
      "loss: 0.007219  [  832/ 3200]\n",
      "loss: 0.009104  [  848/ 3200]\n",
      "loss: 0.018082  [  864/ 3200]\n",
      "loss: 0.035162  [  880/ 3200]\n",
      "loss: 0.016957  [  896/ 3200]\n",
      "loss: 0.061793  [  912/ 3200]\n",
      "loss: 0.019876  [  928/ 3200]\n",
      "loss: 0.034483  [  944/ 3200]\n",
      "loss: 0.027305  [  960/ 3200]\n",
      "loss: 0.129277  [  976/ 3200]\n",
      "loss: 0.013179  [  992/ 3200]\n",
      "loss: 0.068502  [ 1008/ 3200]\n",
      "loss: 0.051150  [ 1024/ 3200]\n",
      "loss: 0.085615  [ 1040/ 3200]\n",
      "loss: 0.310615  [ 1056/ 3200]\n",
      "loss: 0.061562  [ 1072/ 3200]\n",
      "loss: 0.244768  [ 1088/ 3200]\n",
      "loss: 0.011109  [ 1104/ 3200]\n",
      "loss: 0.028627  [ 1120/ 3200]\n",
      "loss: 0.030021  [ 1136/ 3200]\n",
      "loss: 0.022999  [ 1152/ 3200]\n",
      "loss: 0.079699  [ 1168/ 3200]\n",
      "loss: 0.082701  [ 1184/ 3200]\n",
      "loss: 0.037328  [ 1200/ 3200]\n",
      "loss: 0.072252  [ 1216/ 3200]\n",
      "loss: 0.187809  [ 1232/ 3200]\n",
      "loss: 0.210642  [ 1248/ 3200]\n",
      "loss: 0.091319  [ 1264/ 3200]\n",
      "loss: 0.034125  [ 1280/ 3200]\n",
      "loss: 0.048254  [ 1296/ 3200]\n",
      "loss: 0.035722  [ 1312/ 3200]\n",
      "loss: 0.032838  [ 1328/ 3200]\n",
      "loss: 0.053766  [ 1344/ 3200]\n",
      "loss: 0.088539  [ 1360/ 3200]\n",
      "loss: 0.052092  [ 1376/ 3200]\n",
      "loss: 0.052693  [ 1392/ 3200]\n",
      "loss: 0.163187  [ 1408/ 3200]\n",
      "loss: 0.014118  [ 1424/ 3200]\n",
      "loss: 0.012543  [ 1440/ 3200]\n",
      "loss: 0.017552  [ 1456/ 3200]\n",
      "loss: 0.016502  [ 1472/ 3200]\n",
      "loss: 0.043975  [ 1488/ 3200]\n",
      "loss: 0.023146  [ 1504/ 3200]\n",
      "loss: 0.035316  [ 1520/ 3200]\n",
      "loss: 0.012968  [ 1536/ 3200]\n",
      "loss: 0.071400  [ 1552/ 3200]\n",
      "loss: 0.062356  [ 1568/ 3200]\n",
      "loss: 0.019024  [ 1584/ 3200]\n",
      "loss: 0.023610  [ 1600/ 3200]\n",
      "loss: 0.030043  [ 1616/ 3200]\n",
      "loss: 0.043530  [ 1632/ 3200]\n",
      "loss: 0.031841  [ 1648/ 3200]\n",
      "loss: 0.043906  [ 1664/ 3200]\n",
      "loss: 0.050014  [ 1680/ 3200]\n",
      "loss: 0.026715  [ 1696/ 3200]\n",
      "loss: 0.050267  [ 1712/ 3200]\n",
      "loss: 0.014826  [ 1728/ 3200]\n",
      "loss: 0.075140  [ 1744/ 3200]\n",
      "loss: 0.043743  [ 1760/ 3200]\n",
      "loss: 0.095783  [ 1776/ 3200]\n",
      "loss: 0.099764  [ 1792/ 3200]\n",
      "loss: 0.018325  [ 1808/ 3200]\n",
      "loss: 0.049175  [ 1824/ 3200]\n",
      "loss: 0.025199  [ 1840/ 3200]\n",
      "loss: 0.027423  [ 1856/ 3200]\n",
      "loss: 0.027252  [ 1872/ 3200]\n",
      "loss: 0.146516  [ 1888/ 3200]\n",
      "loss: 0.057037  [ 1904/ 3200]\n",
      "loss: 0.021556  [ 1920/ 3200]\n",
      "loss: 0.031506  [ 1936/ 3200]\n",
      "loss: 0.017747  [ 1952/ 3200]\n",
      "loss: 0.056684  [ 1968/ 3200]\n",
      "loss: 0.029742  [ 1984/ 3200]\n",
      "loss: 0.016834  [ 2000/ 3200]\n",
      "loss: 0.017520  [ 2016/ 3200]\n",
      "loss: 0.020937  [ 2032/ 3200]\n",
      "loss: 0.018538  [ 2048/ 3200]\n",
      "loss: 0.021889  [ 2064/ 3200]\n",
      "loss: 0.017819  [ 2080/ 3200]\n",
      "loss: 0.075300  [ 2096/ 3200]\n",
      "loss: 0.014394  [ 2112/ 3200]\n",
      "loss: 0.064609  [ 2128/ 3200]\n",
      "loss: 0.017015  [ 2144/ 3200]\n",
      "loss: 0.114391  [ 2160/ 3200]\n",
      "loss: 0.035404  [ 2176/ 3200]\n",
      "loss: 0.008462  [ 2192/ 3200]\n",
      "loss: 0.064055  [ 2208/ 3200]\n",
      "loss: 0.022014  [ 2224/ 3200]\n",
      "loss: 0.017800  [ 2240/ 3200]\n",
      "loss: 0.087144  [ 2256/ 3200]\n",
      "loss: 0.055793  [ 2272/ 3200]\n",
      "loss: 0.034173  [ 2288/ 3200]\n",
      "loss: 0.017387  [ 2304/ 3200]\n",
      "loss: 0.075576  [ 2320/ 3200]\n",
      "loss: 0.036625  [ 2336/ 3200]\n",
      "loss: 0.066526  [ 2352/ 3200]\n",
      "loss: 0.016240  [ 2368/ 3200]\n",
      "loss: 0.038365  [ 2384/ 3200]\n",
      "loss: 0.029038  [ 2400/ 3200]\n",
      "loss: 0.058014  [ 2416/ 3200]\n",
      "loss: 0.047899  [ 2432/ 3200]\n",
      "loss: 0.074451  [ 2448/ 3200]\n",
      "loss: 0.101951  [ 2464/ 3200]\n",
      "loss: 0.077503  [ 2480/ 3200]\n",
      "loss: 0.025197  [ 2496/ 3200]\n",
      "loss: 0.048566  [ 2512/ 3200]\n",
      "loss: 0.011500  [ 2528/ 3200]\n",
      "loss: 0.010967  [ 2544/ 3200]\n",
      "loss: 0.055543  [ 2560/ 3200]\n",
      "loss: 0.103538  [ 2576/ 3200]\n",
      "loss: 0.037758  [ 2592/ 3200]\n",
      "loss: 0.035149  [ 2608/ 3200]\n",
      "loss: 0.006384  [ 2624/ 3200]\n",
      "loss: 0.067487  [ 2640/ 3200]\n",
      "loss: 0.038409  [ 2656/ 3200]\n",
      "loss: 0.028141  [ 2672/ 3200]\n",
      "loss: 0.032579  [ 2688/ 3200]\n",
      "loss: 0.164978  [ 2704/ 3200]\n",
      "loss: 0.307547  [ 2720/ 3200]\n",
      "loss: 0.117305  [ 2736/ 3200]\n",
      "loss: 0.070014  [ 2752/ 3200]\n",
      "loss: 0.046195  [ 2768/ 3200]\n",
      "loss: 0.046993  [ 2784/ 3200]\n",
      "loss: 0.040260  [ 2800/ 3200]\n",
      "loss: 0.053545  [ 2816/ 3200]\n",
      "loss: 0.053255  [ 2832/ 3200]\n",
      "loss: 0.131178  [ 2848/ 3200]\n",
      "loss: 0.064422  [ 2864/ 3200]\n",
      "loss: 0.021383  [ 2880/ 3200]\n",
      "loss: 0.136816  [ 2896/ 3200]\n",
      "loss: 0.021507  [ 2912/ 3200]\n",
      "loss: 0.016264  [ 2928/ 3200]\n",
      "loss: 0.078024  [ 2944/ 3200]\n",
      "loss: 0.035074  [ 2960/ 3200]\n",
      "loss: 0.067278  [ 2976/ 3200]\n",
      "loss: 0.066123  [ 2992/ 3200]\n",
      "loss: 0.160352  [ 3008/ 3200]\n",
      "loss: 0.043716  [ 3024/ 3200]\n",
      "loss: 0.048265  [ 3040/ 3200]\n",
      "loss: 0.080379  [ 3056/ 3200]\n",
      "loss: 0.072491  [ 3072/ 3200]\n",
      "loss: 0.015596  [ 3088/ 3200]\n",
      "loss: 0.018166  [ 3104/ 3200]\n",
      "loss: 0.035485  [ 3120/ 3200]\n",
      "loss: 0.029369  [ 3136/ 3200]\n",
      "loss: 0.082070  [ 3152/ 3200]\n",
      "loss: 0.051998  [ 3168/ 3200]\n",
      "loss: 0.030086  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.049427\n",
      "f1 macro averaged score: 0.775058\n",
      "Accuracy               : 77.5%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  11,   0,  17],\n",
      "        [ 17, 128,  15,  40],\n",
      "        [  0,  21, 161,  18],\n",
      "        [ 10,  19,  12, 159]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0209e-04.\n",
      "\n",
      "Epoch: 59\n",
      "-----------------------------\n",
      "loss: 0.047207  [    0/ 3200]\n",
      "loss: 0.042456  [   16/ 3200]\n",
      "loss: 0.020435  [   32/ 3200]\n",
      "loss: 0.106675  [   48/ 3200]\n",
      "loss: 0.168869  [   64/ 3200]\n",
      "loss: 0.257839  [   80/ 3200]\n",
      "loss: 0.014024  [   96/ 3200]\n",
      "loss: 0.020170  [  112/ 3200]\n",
      "loss: 0.011266  [  128/ 3200]\n",
      "loss: 0.021407  [  144/ 3200]\n",
      "loss: 0.030943  [  160/ 3200]\n",
      "loss: 0.017219  [  176/ 3200]\n",
      "loss: 0.023326  [  192/ 3200]\n",
      "loss: 0.060913  [  208/ 3200]\n",
      "loss: 0.008611  [  224/ 3200]\n",
      "loss: 0.056431  [  240/ 3200]\n",
      "loss: 0.034305  [  256/ 3200]\n",
      "loss: 0.030882  [  272/ 3200]\n",
      "loss: 0.052292  [  288/ 3200]\n",
      "loss: 0.020531  [  304/ 3200]\n",
      "loss: 0.015718  [  320/ 3200]\n",
      "loss: 0.019413  [  336/ 3200]\n",
      "loss: 0.019926  [  352/ 3200]\n",
      "loss: 0.041671  [  368/ 3200]\n",
      "loss: 0.051975  [  384/ 3200]\n",
      "loss: 0.067774  [  400/ 3200]\n",
      "loss: 0.051289  [  416/ 3200]\n",
      "loss: 0.024363  [  432/ 3200]\n",
      "loss: 0.082532  [  448/ 3200]\n",
      "loss: 0.023942  [  464/ 3200]\n",
      "loss: 0.037515  [  480/ 3200]\n",
      "loss: 0.037875  [  496/ 3200]\n",
      "loss: 0.037024  [  512/ 3200]\n",
      "loss: 0.011562  [  528/ 3200]\n",
      "loss: 0.040497  [  544/ 3200]\n",
      "loss: 0.051255  [  560/ 3200]\n",
      "loss: 0.027873  [  576/ 3200]\n",
      "loss: 0.014920  [  592/ 3200]\n",
      "loss: 0.036418  [  608/ 3200]\n",
      "loss: 0.032997  [  624/ 3200]\n",
      "loss: 0.076631  [  640/ 3200]\n",
      "loss: 0.088313  [  656/ 3200]\n",
      "loss: 0.019524  [  672/ 3200]\n",
      "loss: 0.052833  [  688/ 3200]\n",
      "loss: 0.047511  [  704/ 3200]\n",
      "loss: 0.050109  [  720/ 3200]\n",
      "loss: 0.015853  [  736/ 3200]\n",
      "loss: 0.080388  [  752/ 3200]\n",
      "loss: 0.018103  [  768/ 3200]\n",
      "loss: 0.047775  [  784/ 3200]\n",
      "loss: 0.030400  [  800/ 3200]\n",
      "loss: 0.154121  [  816/ 3200]\n",
      "loss: 0.055051  [  832/ 3200]\n",
      "loss: 0.089192  [  848/ 3200]\n",
      "loss: 0.015025  [  864/ 3200]\n",
      "loss: 0.066009  [  880/ 3200]\n",
      "loss: 0.014399  [  896/ 3200]\n",
      "loss: 0.051638  [  912/ 3200]\n",
      "loss: 0.034391  [  928/ 3200]\n",
      "loss: 0.022347  [  944/ 3200]\n",
      "loss: 0.078161  [  960/ 3200]\n",
      "loss: 0.044720  [  976/ 3200]\n",
      "loss: 0.061120  [  992/ 3200]\n",
      "loss: 0.018638  [ 1008/ 3200]\n",
      "loss: 0.028324  [ 1024/ 3200]\n",
      "loss: 0.058350  [ 1040/ 3200]\n",
      "loss: 0.135317  [ 1056/ 3200]\n",
      "loss: 0.019875  [ 1072/ 3200]\n",
      "loss: 0.015580  [ 1088/ 3200]\n",
      "loss: 0.045248  [ 1104/ 3200]\n",
      "loss: 0.037305  [ 1120/ 3200]\n",
      "loss: 0.057048  [ 1136/ 3200]\n",
      "loss: 0.034388  [ 1152/ 3200]\n",
      "loss: 0.064684  [ 1168/ 3200]\n",
      "loss: 0.050706  [ 1184/ 3200]\n",
      "loss: 0.025205  [ 1200/ 3200]\n",
      "loss: 0.059740  [ 1216/ 3200]\n",
      "loss: 0.089038  [ 1232/ 3200]\n",
      "loss: 0.007748  [ 1248/ 3200]\n",
      "loss: 0.014116  [ 1264/ 3200]\n",
      "loss: 0.047804  [ 1280/ 3200]\n",
      "loss: 0.069947  [ 1296/ 3200]\n",
      "loss: 0.199784  [ 1312/ 3200]\n",
      "loss: 0.039193  [ 1328/ 3200]\n",
      "loss: 0.035505  [ 1344/ 3200]\n",
      "loss: 0.035074  [ 1360/ 3200]\n",
      "loss: 0.051463  [ 1376/ 3200]\n",
      "loss: 0.031633  [ 1392/ 3200]\n",
      "loss: 0.053428  [ 1408/ 3200]\n",
      "loss: 0.014811  [ 1424/ 3200]\n",
      "loss: 0.016287  [ 1440/ 3200]\n",
      "loss: 0.071481  [ 1456/ 3200]\n",
      "loss: 0.024609  [ 1472/ 3200]\n",
      "loss: 0.085897  [ 1488/ 3200]\n",
      "loss: 0.056888  [ 1504/ 3200]\n",
      "loss: 0.021181  [ 1520/ 3200]\n",
      "loss: 0.055601  [ 1536/ 3200]\n",
      "loss: 0.044655  [ 1552/ 3200]\n",
      "loss: 0.057411  [ 1568/ 3200]\n",
      "loss: 0.036024  [ 1584/ 3200]\n",
      "loss: 0.031352  [ 1600/ 3200]\n",
      "loss: 0.027266  [ 1616/ 3200]\n",
      "loss: 0.012093  [ 1632/ 3200]\n",
      "loss: 0.138801  [ 1648/ 3200]\n",
      "loss: 0.069604  [ 1664/ 3200]\n",
      "loss: 0.043332  [ 1680/ 3200]\n",
      "loss: 0.169438  [ 1696/ 3200]\n",
      "loss: 0.027784  [ 1712/ 3200]\n",
      "loss: 0.043211  [ 1728/ 3200]\n",
      "loss: 0.075125  [ 1744/ 3200]\n",
      "loss: 0.057886  [ 1760/ 3200]\n",
      "loss: 0.018827  [ 1776/ 3200]\n",
      "loss: 0.042770  [ 1792/ 3200]\n",
      "loss: 0.022809  [ 1808/ 3200]\n",
      "loss: 0.059193  [ 1824/ 3200]\n",
      "loss: 0.010192  [ 1840/ 3200]\n",
      "loss: 0.071207  [ 1856/ 3200]\n",
      "loss: 0.060682  [ 1872/ 3200]\n",
      "loss: 0.041589  [ 1888/ 3200]\n",
      "loss: 0.067819  [ 1904/ 3200]\n",
      "loss: 0.120841  [ 1920/ 3200]\n",
      "loss: 0.095396  [ 1936/ 3200]\n",
      "loss: 0.074427  [ 1952/ 3200]\n",
      "loss: 0.024396  [ 1968/ 3200]\n",
      "loss: 0.014052  [ 1984/ 3200]\n",
      "loss: 0.056119  [ 2000/ 3200]\n",
      "loss: 0.063064  [ 2016/ 3200]\n",
      "loss: 0.008805  [ 2032/ 3200]\n",
      "loss: 0.037322  [ 2048/ 3200]\n",
      "loss: 0.028535  [ 2064/ 3200]\n",
      "loss: 0.062972  [ 2080/ 3200]\n",
      "loss: 0.015472  [ 2096/ 3200]\n",
      "loss: 0.056755  [ 2112/ 3200]\n",
      "loss: 0.028377  [ 2128/ 3200]\n",
      "loss: 0.050942  [ 2144/ 3200]\n",
      "loss: 0.168060  [ 2160/ 3200]\n",
      "loss: 0.118255  [ 2176/ 3200]\n",
      "loss: 0.232234  [ 2192/ 3200]\n",
      "loss: 0.038551  [ 2208/ 3200]\n",
      "loss: 0.157653  [ 2224/ 3200]\n",
      "loss: 0.057831  [ 2240/ 3200]\n",
      "loss: 0.140974  [ 2256/ 3200]\n",
      "loss: 0.076463  [ 2272/ 3200]\n",
      "loss: 0.037840  [ 2288/ 3200]\n",
      "loss: 0.065855  [ 2304/ 3200]\n",
      "loss: 0.016613  [ 2320/ 3200]\n",
      "loss: 0.031591  [ 2336/ 3200]\n",
      "loss: 0.033705  [ 2352/ 3200]\n",
      "loss: 0.071539  [ 2368/ 3200]\n",
      "loss: 0.053073  [ 2384/ 3200]\n",
      "loss: 0.098621  [ 2400/ 3200]\n",
      "loss: 0.064699  [ 2416/ 3200]\n",
      "loss: 0.028520  [ 2432/ 3200]\n",
      "loss: 0.087258  [ 2448/ 3200]\n",
      "loss: 0.069301  [ 2464/ 3200]\n",
      "loss: 0.012995  [ 2480/ 3200]\n",
      "loss: 0.092295  [ 2496/ 3200]\n",
      "loss: 0.067304  [ 2512/ 3200]\n",
      "loss: 0.159451  [ 2528/ 3200]\n",
      "loss: 0.081272  [ 2544/ 3200]\n",
      "loss: 0.044917  [ 2560/ 3200]\n",
      "loss: 0.077131  [ 2576/ 3200]\n",
      "loss: 0.051406  [ 2592/ 3200]\n",
      "loss: 0.041161  [ 2608/ 3200]\n",
      "loss: 0.018389  [ 2624/ 3200]\n",
      "loss: 0.008751  [ 2640/ 3200]\n",
      "loss: 0.020327  [ 2656/ 3200]\n",
      "loss: 0.014796  [ 2672/ 3200]\n",
      "loss: 0.047124  [ 2688/ 3200]\n",
      "loss: 0.029072  [ 2704/ 3200]\n",
      "loss: 0.069666  [ 2720/ 3200]\n",
      "loss: 0.071607  [ 2736/ 3200]\n",
      "loss: 0.009567  [ 2752/ 3200]\n",
      "loss: 0.013811  [ 2768/ 3200]\n",
      "loss: 0.025038  [ 2784/ 3200]\n",
      "loss: 0.044424  [ 2800/ 3200]\n",
      "loss: 0.062401  [ 2816/ 3200]\n",
      "loss: 0.054839  [ 2832/ 3200]\n",
      "loss: 0.019493  [ 2848/ 3200]\n",
      "loss: 0.029271  [ 2864/ 3200]\n",
      "loss: 0.018710  [ 2880/ 3200]\n",
      "loss: 0.068376  [ 2896/ 3200]\n",
      "loss: 0.137297  [ 2912/ 3200]\n",
      "loss: 0.147266  [ 2928/ 3200]\n",
      "loss: 0.074280  [ 2944/ 3200]\n",
      "loss: 0.079373  [ 2960/ 3200]\n",
      "loss: 0.031224  [ 2976/ 3200]\n",
      "loss: 0.010667  [ 2992/ 3200]\n",
      "loss: 0.115669  [ 3008/ 3200]\n",
      "loss: 0.059193  [ 3024/ 3200]\n",
      "loss: 0.030578  [ 3040/ 3200]\n",
      "loss: 0.061173  [ 3056/ 3200]\n",
      "loss: 0.064508  [ 3072/ 3200]\n",
      "loss: 0.056229  [ 3088/ 3200]\n",
      "loss: 0.036512  [ 3104/ 3200]\n",
      "loss: 0.126225  [ 3120/ 3200]\n",
      "loss: 0.243283  [ 3136/ 3200]\n",
      "loss: 0.051909  [ 3152/ 3200]\n",
      "loss: 0.025073  [ 3168/ 3200]\n",
      "loss: 0.031942  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.050760\n",
      "f1 macro averaged score: 0.773249\n",
      "Accuracy               : 77.4%\n",
      "Confusion matrix       :\n",
      "tensor([[176,  10,   0,  14],\n",
      "        [ 15, 124,  21,  40],\n",
      "        [  0,  23, 161,  16],\n",
      "        [  7,  24,  11, 158]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.6989e-05.\n",
      "\n",
      "Epoch: 60\n",
      "-----------------------------\n",
      "loss: 0.034253  [    0/ 3200]\n",
      "loss: 0.232758  [   16/ 3200]\n",
      "loss: 0.092999  [   32/ 3200]\n",
      "loss: 0.012295  [   48/ 3200]\n",
      "loss: 0.044220  [   64/ 3200]\n",
      "loss: 0.076229  [   80/ 3200]\n",
      "loss: 0.061726  [   96/ 3200]\n",
      "loss: 0.027948  [  112/ 3200]\n",
      "loss: 0.051589  [  128/ 3200]\n",
      "loss: 0.025315  [  144/ 3200]\n",
      "loss: 0.124053  [  160/ 3200]\n",
      "loss: 0.014120  [  176/ 3200]\n",
      "loss: 0.092093  [  192/ 3200]\n",
      "loss: 0.076765  [  208/ 3200]\n",
      "loss: 0.020489  [  224/ 3200]\n",
      "loss: 0.072888  [  240/ 3200]\n",
      "loss: 0.107374  [  256/ 3200]\n",
      "loss: 0.055146  [  272/ 3200]\n",
      "loss: 0.046200  [  288/ 3200]\n",
      "loss: 0.020678  [  304/ 3200]\n",
      "loss: 0.055260  [  320/ 3200]\n",
      "loss: 0.066744  [  336/ 3200]\n",
      "loss: 0.013290  [  352/ 3200]\n",
      "loss: 0.013080  [  368/ 3200]\n",
      "loss: 0.089259  [  384/ 3200]\n",
      "loss: 0.056918  [  400/ 3200]\n",
      "loss: 0.065153  [  416/ 3200]\n",
      "loss: 0.022029  [  432/ 3200]\n",
      "loss: 0.106814  [  448/ 3200]\n",
      "loss: 0.042312  [  464/ 3200]\n",
      "loss: 0.163079  [  480/ 3200]\n",
      "loss: 0.036427  [  496/ 3200]\n",
      "loss: 0.029738  [  512/ 3200]\n",
      "loss: 0.070511  [  528/ 3200]\n",
      "loss: 0.036791  [  544/ 3200]\n",
      "loss: 0.023155  [  560/ 3200]\n",
      "loss: 0.040484  [  576/ 3200]\n",
      "loss: 0.098960  [  592/ 3200]\n",
      "loss: 0.027594  [  608/ 3200]\n",
      "loss: 0.041318  [  624/ 3200]\n",
      "loss: 0.030949  [  640/ 3200]\n",
      "loss: 0.021956  [  656/ 3200]\n",
      "loss: 0.083153  [  672/ 3200]\n",
      "loss: 0.065966  [  688/ 3200]\n",
      "loss: 0.022326  [  704/ 3200]\n",
      "loss: 0.043371  [  720/ 3200]\n",
      "loss: 0.086717  [  736/ 3200]\n",
      "loss: 0.059492  [  752/ 3200]\n",
      "loss: 0.085327  [  768/ 3200]\n",
      "loss: 0.130576  [  784/ 3200]\n",
      "loss: 0.076116  [  800/ 3200]\n",
      "loss: 0.026434  [  816/ 3200]\n",
      "loss: 0.042006  [  832/ 3200]\n",
      "loss: 0.032777  [  848/ 3200]\n",
      "loss: 0.021957  [  864/ 3200]\n",
      "loss: 0.016385  [  880/ 3200]\n",
      "loss: 0.128354  [  896/ 3200]\n",
      "loss: 0.075041  [  912/ 3200]\n",
      "loss: 0.070484  [  928/ 3200]\n",
      "loss: 0.028562  [  944/ 3200]\n",
      "loss: 0.132441  [  960/ 3200]\n",
      "loss: 0.103780  [  976/ 3200]\n",
      "loss: 0.064621  [  992/ 3200]\n",
      "loss: 0.020630  [ 1008/ 3200]\n",
      "loss: 0.051268  [ 1024/ 3200]\n",
      "loss: 0.032110  [ 1040/ 3200]\n",
      "loss: 0.013726  [ 1056/ 3200]\n",
      "loss: 0.154052  [ 1072/ 3200]\n",
      "loss: 0.031296  [ 1088/ 3200]\n",
      "loss: 0.044263  [ 1104/ 3200]\n",
      "loss: 0.074459  [ 1120/ 3200]\n",
      "loss: 0.026421  [ 1136/ 3200]\n",
      "loss: 0.022847  [ 1152/ 3200]\n",
      "loss: 0.010205  [ 1168/ 3200]\n",
      "loss: 0.020634  [ 1184/ 3200]\n",
      "loss: 0.031252  [ 1200/ 3200]\n",
      "loss: 0.089652  [ 1216/ 3200]\n",
      "loss: 0.121987  [ 1232/ 3200]\n",
      "loss: 0.063390  [ 1248/ 3200]\n",
      "loss: 0.015650  [ 1264/ 3200]\n",
      "loss: 0.069445  [ 1280/ 3200]\n",
      "loss: 0.013294  [ 1296/ 3200]\n",
      "loss: 0.038127  [ 1312/ 3200]\n",
      "loss: 0.043157  [ 1328/ 3200]\n",
      "loss: 0.009093  [ 1344/ 3200]\n",
      "loss: 0.007870  [ 1360/ 3200]\n",
      "loss: 0.017832  [ 1376/ 3200]\n",
      "loss: 0.043560  [ 1392/ 3200]\n",
      "loss: 0.037729  [ 1408/ 3200]\n",
      "loss: 0.087625  [ 1424/ 3200]\n",
      "loss: 0.012086  [ 1440/ 3200]\n",
      "loss: 0.016123  [ 1456/ 3200]\n",
      "loss: 0.073901  [ 1472/ 3200]\n",
      "loss: 0.015006  [ 1488/ 3200]\n",
      "loss: 0.056136  [ 1504/ 3200]\n",
      "loss: 0.031516  [ 1520/ 3200]\n",
      "loss: 0.022663  [ 1536/ 3200]\n",
      "loss: 0.144407  [ 1552/ 3200]\n",
      "loss: 0.052736  [ 1568/ 3200]\n",
      "loss: 0.024489  [ 1584/ 3200]\n",
      "loss: 0.028820  [ 1600/ 3200]\n",
      "loss: 0.032928  [ 1616/ 3200]\n",
      "loss: 0.006847  [ 1632/ 3200]\n",
      "loss: 0.039683  [ 1648/ 3200]\n",
      "loss: 0.007886  [ 1664/ 3200]\n",
      "loss: 0.124435  [ 1680/ 3200]\n",
      "loss: 0.127200  [ 1696/ 3200]\n",
      "loss: 0.082681  [ 1712/ 3200]\n",
      "loss: 0.057600  [ 1728/ 3200]\n",
      "loss: 0.113816  [ 1744/ 3200]\n",
      "loss: 0.058599  [ 1760/ 3200]\n",
      "loss: 0.098346  [ 1776/ 3200]\n",
      "loss: 0.060232  [ 1792/ 3200]\n",
      "loss: 0.155694  [ 1808/ 3200]\n",
      "loss: 0.054725  [ 1824/ 3200]\n",
      "loss: 0.006935  [ 1840/ 3200]\n",
      "loss: 0.088774  [ 1856/ 3200]\n",
      "loss: 0.036662  [ 1872/ 3200]\n",
      "loss: 0.016500  [ 1888/ 3200]\n",
      "loss: 0.012251  [ 1904/ 3200]\n",
      "loss: 0.090978  [ 1920/ 3200]\n",
      "loss: 0.102290  [ 1936/ 3200]\n",
      "loss: 0.051461  [ 1952/ 3200]\n",
      "loss: 0.009469  [ 1968/ 3200]\n",
      "loss: 0.138332  [ 1984/ 3200]\n",
      "loss: 0.009154  [ 2000/ 3200]\n",
      "loss: 0.025205  [ 2016/ 3200]\n",
      "loss: 0.037884  [ 2032/ 3200]\n",
      "loss: 0.118218  [ 2048/ 3200]\n",
      "loss: 0.028657  [ 2064/ 3200]\n",
      "loss: 0.128996  [ 2080/ 3200]\n",
      "loss: 0.082035  [ 2096/ 3200]\n",
      "loss: 0.021443  [ 2112/ 3200]\n",
      "loss: 0.022298  [ 2128/ 3200]\n",
      "loss: 0.071699  [ 2144/ 3200]\n",
      "loss: 0.018220  [ 2160/ 3200]\n",
      "loss: 0.033483  [ 2176/ 3200]\n",
      "loss: 0.036171  [ 2192/ 3200]\n",
      "loss: 0.019439  [ 2208/ 3200]\n",
      "loss: 0.016407  [ 2224/ 3200]\n",
      "loss: 0.014120  [ 2240/ 3200]\n",
      "loss: 0.058974  [ 2256/ 3200]\n",
      "loss: 0.023158  [ 2272/ 3200]\n",
      "loss: 0.108357  [ 2288/ 3200]\n",
      "loss: 0.020759  [ 2304/ 3200]\n",
      "loss: 0.015944  [ 2320/ 3200]\n",
      "loss: 0.027652  [ 2336/ 3200]\n",
      "loss: 0.014460  [ 2352/ 3200]\n",
      "loss: 0.026804  [ 2368/ 3200]\n",
      "loss: 0.034141  [ 2384/ 3200]\n",
      "loss: 0.051483  [ 2400/ 3200]\n",
      "loss: 0.013181  [ 2416/ 3200]\n",
      "loss: 0.017436  [ 2432/ 3200]\n",
      "loss: 0.033851  [ 2448/ 3200]\n",
      "loss: 0.155753  [ 2464/ 3200]\n",
      "loss: 0.029391  [ 2480/ 3200]\n",
      "loss: 0.036903  [ 2496/ 3200]\n",
      "loss: 0.062114  [ 2512/ 3200]\n",
      "loss: 0.017190  [ 2528/ 3200]\n",
      "loss: 0.069198  [ 2544/ 3200]\n",
      "loss: 0.078272  [ 2560/ 3200]\n",
      "loss: 0.027189  [ 2576/ 3200]\n",
      "loss: 0.134639  [ 2592/ 3200]\n",
      "loss: 0.036970  [ 2608/ 3200]\n",
      "loss: 0.011234  [ 2624/ 3200]\n",
      "loss: 0.032206  [ 2640/ 3200]\n",
      "loss: 0.056551  [ 2656/ 3200]\n",
      "loss: 0.038259  [ 2672/ 3200]\n",
      "loss: 0.059696  [ 2688/ 3200]\n",
      "loss: 0.012212  [ 2704/ 3200]\n",
      "loss: 0.036081  [ 2720/ 3200]\n",
      "loss: 0.056496  [ 2736/ 3200]\n",
      "loss: 0.072933  [ 2752/ 3200]\n",
      "loss: 0.026786  [ 2768/ 3200]\n",
      "loss: 0.075873  [ 2784/ 3200]\n",
      "loss: 0.077018  [ 2800/ 3200]\n",
      "loss: 0.045130  [ 2816/ 3200]\n",
      "loss: 0.021628  [ 2832/ 3200]\n",
      "loss: 0.028644  [ 2848/ 3200]\n",
      "loss: 0.010742  [ 2864/ 3200]\n",
      "loss: 0.090160  [ 2880/ 3200]\n",
      "loss: 0.028381  [ 2896/ 3200]\n",
      "loss: 0.019111  [ 2912/ 3200]\n",
      "loss: 0.027935  [ 2928/ 3200]\n",
      "loss: 0.054615  [ 2944/ 3200]\n",
      "loss: 0.051194  [ 2960/ 3200]\n",
      "loss: 0.075365  [ 2976/ 3200]\n",
      "loss: 0.112069  [ 2992/ 3200]\n",
      "loss: 0.015634  [ 3008/ 3200]\n",
      "loss: 0.028556  [ 3024/ 3200]\n",
      "loss: 0.017003  [ 3040/ 3200]\n",
      "loss: 0.042367  [ 3056/ 3200]\n",
      "loss: 0.033640  [ 3072/ 3200]\n",
      "loss: 0.063321  [ 3088/ 3200]\n",
      "loss: 0.080094  [ 3104/ 3200]\n",
      "loss: 0.048120  [ 3120/ 3200]\n",
      "loss: 0.050275  [ 3136/ 3200]\n",
      "loss: 0.049731  [ 3152/ 3200]\n",
      "loss: 0.075231  [ 3168/ 3200]\n",
      "loss: 0.090931  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.049776\n",
      "f1 macro averaged score: 0.785776\n",
      "Accuracy               : 78.6%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  13,   0,  15],\n",
      "        [ 14, 128,  17,  41],\n",
      "        [  0,  23, 165,  12],\n",
      "        [ 10,  15,  11, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.2140e-05.\n",
      "\n",
      "Best epoch: 9 with f1 macro averaged score: 0.8073028326034546\n",
      "Test Error:\n",
      "Avg loss               : 0.053303\n",
      "f1 macro averaged score: 0.775903\n",
      "Accuracy               : 77.2%\n",
      "Confusion matrix       :\n",
      "tensor([[276,  14,   1,   6],\n",
      "        [ 10, 207,  31,  76],\n",
      "        [  5,  36, 290,  25],\n",
      "        [ 13,  67,  30, 289]], device='cuda:0')\n",
      "CPU times: user 4min 33s, sys: 6.89 s, total: 4min 39s\n",
      "Wall time: 4min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p_list = [0.1, 0.2, 0.3]\n",
    "\n",
    "f1_accuracy_30 = validate_dropout(p_list, 30)\n",
    "f1_accuracy_60 = validate_dropout(p_list, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_q8b1VX_0K6",
    "outputId": "6a3e5b49-11d3-4523-cdee-205f79ef47c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 epochs\n",
      "Dropout\tf1_macro_avg\tAccuracy\n",
      "0.3\t0.785990\t78.488372\n",
      "0.1\t0.785191\t78.343023\n",
      "0.2\t0.771558\t76.962209\n",
      "\n",
      "60 epochs\n",
      "Dropout\tf1_macro_avg\tAccuracy\n",
      "0.1\t0.783522\t77.834302\n",
      "0.2\t0.776003\t77.325581\n",
      "0.3\t0.775903\t77.180233\n"
     ]
    }
   ],
   "source": [
    "f1_accuracy_30 = sorted(f1_accuracy_30, key=lambda x: x[1], reverse=True)\n",
    "f1_accuracy_60 = sorted(f1_accuracy_60, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"30 epochs\")\n",
    "print(\"Dropout\\tf1_macro_avg\\tAccuracy\")\n",
    "for (dropout, f1_macro_avg, accuracy) in f1_accuracy_30:\n",
    "  print(f\"{dropout}\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")\n",
    "print()\n",
    "\n",
    "print(\"60 epochs\")\n",
    "print(\"Dropout\\tf1_macro_avg\\tAccuracy\")\n",
    "for (dropout, f1_macro_avg, accuracy) in f1_accuracy_60:\n",
    "  print(f\"{dropout}\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnRcjidEG8wm"
   },
   "source": [
    "When using only dropout $0.1$ for $30$ epochs, the accuracy is increased to almost $78.5\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuKNpf1_AAOS"
   },
   "source": [
    "i) Weight decay and ii) Dropout\n",
    "\n",
    "The following function validates a Convolutional Neural Network for different values of weight decay and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nfk80R62VSYc"
   },
   "outputs": [],
   "source": [
    "def validate_weight_decay_dropout(weight_decay_list, p_list, epochs):\n",
    "  f1_accuracy = []\n",
    "  for weight_decay in weight_decay_list:\n",
    "    for p in p_list:\n",
    "      torch_seed(0)\n",
    "      cnn_model = Net(ELU, p).to(device)\n",
    "      optimizer = Adagrad(params=cnn_model.parameters(), lr=learning_rate_0, weight_decay=weight_decay)\n",
    "      scheduler = MultiplicativeLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95, verbose=True)\n",
    "      print(\"  Weight decay:\", weight_decay)\n",
    "      print(\"  Dropout     :\", p)\n",
    "      best_model, f1_per_epoch = validate_convolutional_neural_network(\n",
    "          epochs, optimizer, train_dataloader, val_dataloader, loss_function, cnn_model, True, scheduler\n",
    "          )\n",
    "      results = test_convolutional_neural_network(test_dataloader, loss_function, best_model)\n",
    "      f1_accuracy.append((weight_decay, p, results[1], results[2]))\n",
    "  return f1_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1X2JUavJkzwL"
   },
   "source": [
    "We validate our Convolutional Neural Network for $30$ and $60$ epochs and test it.\n",
    "\n",
    "Our model uses:\n",
    "+ the Adagrad optimizer\n",
    "+ the ELU activation function\n",
    "+ the MultiplicativeLR scheduler\n",
    "+ batch normalization\n",
    "\n",
    "as stated in the previous steps.\n",
    "\n",
    "The weight decay and dropout values are the same as before, except the fact that now we are trying to find the optimal combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73Dv8L1CMcfg",
    "outputId": "facfab8f-e296-434a-9a2c-8e30c93633cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 1.384473  [ 1952/ 3200]\n",
      "loss: 1.388189  [ 1968/ 3200]\n",
      "loss: 1.388715  [ 1984/ 3200]\n",
      "loss: 1.381229  [ 2000/ 3200]\n",
      "loss: 1.381688  [ 2016/ 3200]\n",
      "loss: 1.385062  [ 2032/ 3200]\n",
      "loss: 1.385709  [ 2048/ 3200]\n",
      "loss: 1.389639  [ 2064/ 3200]\n",
      "loss: 1.382332  [ 2080/ 3200]\n",
      "loss: 1.382215  [ 2096/ 3200]\n",
      "loss: 1.386395  [ 2112/ 3200]\n",
      "loss: 1.390459  [ 2128/ 3200]\n",
      "loss: 1.385408  [ 2144/ 3200]\n",
      "loss: 1.387844  [ 2160/ 3200]\n",
      "loss: 1.384947  [ 2176/ 3200]\n",
      "loss: 1.390916  [ 2192/ 3200]\n",
      "loss: 1.385297  [ 2208/ 3200]\n",
      "loss: 1.388598  [ 2224/ 3200]\n",
      "loss: 1.390112  [ 2240/ 3200]\n",
      "loss: 1.383960  [ 2256/ 3200]\n",
      "loss: 1.387431  [ 2272/ 3200]\n",
      "loss: 1.378685  [ 2288/ 3200]\n",
      "loss: 1.390916  [ 2304/ 3200]\n",
      "loss: 1.385477  [ 2320/ 3200]\n",
      "loss: 1.389980  [ 2336/ 3200]\n",
      "loss: 1.388595  [ 2352/ 3200]\n",
      "loss: 1.388713  [ 2368/ 3200]\n",
      "loss: 1.387197  [ 2384/ 3200]\n",
      "loss: 1.387336  [ 2400/ 3200]\n",
      "loss: 1.389582  [ 2416/ 3200]\n",
      "loss: 1.386853  [ 2432/ 3200]\n",
      "loss: 1.389532  [ 2448/ 3200]\n",
      "loss: 1.386394  [ 2464/ 3200]\n",
      "loss: 1.389120  [ 2480/ 3200]\n",
      "loss: 1.385815  [ 2496/ 3200]\n",
      "loss: 1.388184  [ 2512/ 3200]\n",
      "loss: 1.384540  [ 2528/ 3200]\n",
      "loss: 1.386584  [ 2544/ 3200]\n",
      "loss: 1.384419  [ 2560/ 3200]\n",
      "loss: 1.385987  [ 2576/ 3200]\n",
      "loss: 1.387844  [ 2592/ 3200]\n",
      "loss: 1.384811  [ 2608/ 3200]\n",
      "loss: 1.381471  [ 2624/ 3200]\n",
      "loss: 1.386853  [ 2640/ 3200]\n",
      "loss: 1.382223  [ 2656/ 3200]\n",
      "loss: 1.387193  [ 2672/ 3200]\n",
      "loss: 1.381424  [ 2688/ 3200]\n",
      "loss: 1.384127  [ 2704/ 3200]\n",
      "loss: 1.387911  [ 2720/ 3200]\n",
      "loss: 1.383210  [ 2736/ 3200]\n",
      "loss: 1.384493  [ 2752/ 3200]\n",
      "loss: 1.395659  [ 2768/ 3200]\n",
      "loss: 1.388369  [ 2784/ 3200]\n",
      "loss: 1.388936  [ 2800/ 3200]\n",
      "loss: 1.384152  [ 2816/ 3200]\n",
      "loss: 1.380897  [ 2832/ 3200]\n",
      "loss: 1.389768  [ 2848/ 3200]\n",
      "loss: 1.380204  [ 2864/ 3200]\n",
      "loss: 1.384656  [ 2880/ 3200]\n",
      "loss: 1.383668  [ 2896/ 3200]\n",
      "loss: 1.385986  [ 2912/ 3200]\n",
      "loss: 1.383853  [ 2928/ 3200]\n",
      "loss: 1.384423  [ 2944/ 3200]\n",
      "loss: 1.385523  [ 2960/ 3200]\n",
      "loss: 1.386922  [ 2976/ 3200]\n",
      "loss: 1.391828  [ 2992/ 3200]\n",
      "loss: 1.387678  [ 3008/ 3200]\n",
      "loss: 1.386442  [ 3024/ 3200]\n",
      "loss: 1.380322  [ 3040/ 3200]\n",
      "loss: 1.377477  [ 3056/ 3200]\n",
      "loss: 1.389628  [ 3072/ 3200]\n",
      "loss: 1.386734  [ 3088/ 3200]\n",
      "loss: 1.386735  [ 3104/ 3200]\n",
      "loss: 1.385115  [ 3120/ 3200]\n",
      "loss: 1.387721  [ 3136/ 3200]\n",
      "loss: 1.380144  [ 3152/ 3200]\n",
      "loss: 1.388254  [ 3168/ 3200]\n",
      "loss: 1.394093  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086650\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.9978e-04.\n",
      "\n",
      "Epoch: 38\n",
      "-----------------------------\n",
      "loss: 1.388177  [    0/ 3200]\n",
      "loss: 1.385894  [   16/ 3200]\n",
      "loss: 1.383624  [   32/ 3200]\n",
      "loss: 1.376079  [   48/ 3200]\n",
      "loss: 1.385718  [   64/ 3200]\n",
      "loss: 1.396380  [   80/ 3200]\n",
      "loss: 1.387719  [   96/ 3200]\n",
      "loss: 1.384612  [  112/ 3200]\n",
      "loss: 1.388223  [  128/ 3200]\n",
      "loss: 1.392008  [  144/ 3200]\n",
      "loss: 1.385982  [  160/ 3200]\n",
      "loss: 1.385980  [  176/ 3200]\n",
      "loss: 1.387719  [  192/ 3200]\n",
      "loss: 1.385941  [  208/ 3200]\n",
      "loss: 1.384159  [  224/ 3200]\n",
      "loss: 1.389768  [  240/ 3200]\n",
      "loss: 1.383968  [  256/ 3200]\n",
      "loss: 1.387793  [  272/ 3200]\n",
      "loss: 1.384155  [  288/ 3200]\n",
      "loss: 1.386734  [  304/ 3200]\n",
      "loss: 1.385071  [  320/ 3200]\n",
      "loss: 1.383671  [  336/ 3200]\n",
      "loss: 1.383669  [  352/ 3200]\n",
      "loss: 1.386923  [  368/ 3200]\n",
      "loss: 1.383211  [  384/ 3200]\n",
      "loss: 1.389721  [  400/ 3200]\n",
      "loss: 1.389504  [  416/ 3200]\n",
      "loss: 1.387263  [  432/ 3200]\n",
      "loss: 1.382123  [  448/ 3200]\n",
      "loss: 1.385182  [  464/ 3200]\n",
      "loss: 1.385023  [  480/ 3200]\n",
      "loss: 1.381177  [  496/ 3200]\n",
      "loss: 1.383440  [  512/ 3200]\n",
      "loss: 1.382182  [  528/ 3200]\n",
      "loss: 1.383554  [  544/ 3200]\n",
      "loss: 1.387722  [  560/ 3200]\n",
      "loss: 1.386347  [  576/ 3200]\n",
      "loss: 1.387381  [  592/ 3200]\n",
      "loss: 1.383741  [  608/ 3200]\n",
      "loss: 1.386510  [  624/ 3200]\n",
      "loss: 1.383622  [  640/ 3200]\n",
      "loss: 1.384539  [  656/ 3200]\n",
      "loss: 1.384315  [  672/ 3200]\n",
      "loss: 1.390084  [  688/ 3200]\n",
      "loss: 1.387838  [  704/ 3200]\n",
      "loss: 1.389992  [  720/ 3200]\n",
      "loss: 1.387264  [  736/ 3200]\n",
      "loss: 1.385752  [  752/ 3200]\n",
      "loss: 1.391476  [  768/ 3200]\n",
      "loss: 1.389189  [  784/ 3200]\n",
      "loss: 1.386804  [  800/ 3200]\n",
      "loss: 1.382688  [  816/ 3200]\n",
      "loss: 1.380035  [  832/ 3200]\n",
      "loss: 1.388318  [  848/ 3200]\n",
      "loss: 1.389759  [  864/ 3200]\n",
      "loss: 1.386738  [  880/ 3200]\n",
      "loss: 1.381362  [  896/ 3200]\n",
      "loss: 1.391818  [  912/ 3200]\n",
      "loss: 1.384084  [  928/ 3200]\n",
      "loss: 1.384727  [  944/ 3200]\n",
      "loss: 1.386395  [  960/ 3200]\n",
      "loss: 1.394079  [  976/ 3200]\n",
      "loss: 1.388637  [  992/ 3200]\n",
      "loss: 1.387214  [ 1008/ 3200]\n",
      "loss: 1.386512  [ 1024/ 3200]\n",
      "loss: 1.385002  [ 1040/ 3200]\n",
      "loss: 1.382945  [ 1056/ 3200]\n",
      "loss: 1.390097  [ 1072/ 3200]\n",
      "loss: 1.381134  [ 1088/ 3200]\n",
      "loss: 1.385984  [ 1104/ 3200]\n",
      "loss: 1.380265  [ 1120/ 3200]\n",
      "loss: 1.384658  [ 1136/ 3200]\n",
      "loss: 1.384884  [ 1152/ 3200]\n",
      "loss: 1.376966  [ 1168/ 3200]\n",
      "loss: 1.390029  [ 1184/ 3200]\n",
      "loss: 1.386276  [ 1200/ 3200]\n",
      "loss: 1.385478  [ 1216/ 3200]\n",
      "loss: 1.385190  [ 1232/ 3200]\n",
      "loss: 1.392223  [ 1248/ 3200]\n",
      "loss: 1.390556  [ 1264/ 3200]\n",
      "loss: 1.383288  [ 1280/ 3200]\n",
      "loss: 1.383625  [ 1296/ 3200]\n",
      "loss: 1.386921  [ 1312/ 3200]\n",
      "loss: 1.385018  [ 1328/ 3200]\n",
      "loss: 1.387378  [ 1344/ 3200]\n",
      "loss: 1.380265  [ 1360/ 3200]\n",
      "loss: 1.380041  [ 1376/ 3200]\n",
      "loss: 1.383810  [ 1392/ 3200]\n",
      "loss: 1.389509  [ 1408/ 3200]\n",
      "loss: 1.386855  [ 1424/ 3200]\n",
      "loss: 1.386681  [ 1440/ 3200]\n",
      "loss: 1.383217  [ 1456/ 3200]\n",
      "loss: 1.384884  [ 1472/ 3200]\n",
      "loss: 1.382690  [ 1488/ 3200]\n",
      "loss: 1.386056  [ 1504/ 3200]\n",
      "loss: 1.383336  [ 1520/ 3200]\n",
      "loss: 1.383741  [ 1536/ 3200]\n",
      "loss: 1.389283  [ 1552/ 3200]\n",
      "loss: 1.386278  [ 1568/ 3200]\n",
      "loss: 1.389624  [ 1584/ 3200]\n",
      "loss: 1.383859  [ 1600/ 3200]\n",
      "loss: 1.386329  [ 1616/ 3200]\n",
      "loss: 1.388245  [ 1632/ 3200]\n",
      "loss: 1.385987  [ 1648/ 3200]\n",
      "loss: 1.393207  [ 1664/ 3200]\n",
      "loss: 1.380382  [ 1680/ 3200]\n",
      "loss: 1.386967  [ 1696/ 3200]\n",
      "loss: 1.386852  [ 1712/ 3200]\n",
      "loss: 1.381777  [ 1728/ 3200]\n",
      "loss: 1.376631  [ 1744/ 3200]\n",
      "loss: 1.389802  [ 1760/ 3200]\n",
      "loss: 1.385938  [ 1776/ 3200]\n",
      "loss: 1.381775  [ 1792/ 3200]\n",
      "loss: 1.383399  [ 1808/ 3200]\n",
      "loss: 1.394082  [ 1824/ 3200]\n",
      "loss: 1.390443  [ 1840/ 3200]\n",
      "loss: 1.386804  [ 1856/ 3200]\n",
      "loss: 1.388176  [ 1872/ 3200]\n",
      "loss: 1.385819  [ 1888/ 3200]\n",
      "loss: 1.392863  [ 1904/ 3200]\n",
      "loss: 1.388769  [ 1920/ 3200]\n",
      "loss: 1.384136  [ 1936/ 3200]\n",
      "loss: 1.391304  [ 1952/ 3200]\n",
      "loss: 1.383560  [ 1968/ 3200]\n",
      "loss: 1.389569  [ 1984/ 3200]\n",
      "loss: 1.383680  [ 2000/ 3200]\n",
      "loss: 1.388361  [ 2016/ 3200]\n",
      "loss: 1.392451  [ 2032/ 3200]\n",
      "loss: 1.384954  [ 2048/ 3200]\n",
      "loss: 1.383679  [ 2064/ 3200]\n",
      "loss: 1.394182  [ 2080/ 3200]\n",
      "loss: 1.385254  [ 2096/ 3200]\n",
      "loss: 1.393771  [ 2112/ 3200]\n",
      "loss: 1.389089  [ 2128/ 3200]\n",
      "loss: 1.385983  [ 2144/ 3200]\n",
      "loss: 1.387375  [ 2160/ 3200]\n",
      "loss: 1.388767  [ 2176/ 3200]\n",
      "loss: 1.386849  [ 2192/ 3200]\n",
      "loss: 1.385981  [ 2208/ 3200]\n",
      "loss: 1.383566  [ 2224/ 3200]\n",
      "loss: 1.391870  [ 2240/ 3200]\n",
      "loss: 1.382129  [ 2256/ 3200]\n",
      "loss: 1.386506  [ 2272/ 3200]\n",
      "loss: 1.389453  [ 2288/ 3200]\n",
      "loss: 1.382811  [ 2304/ 3200]\n",
      "loss: 1.382696  [ 2320/ 3200]\n",
      "loss: 1.379525  [ 2336/ 3200]\n",
      "loss: 1.386393  [ 2352/ 3200]\n",
      "loss: 1.384959  [ 2368/ 3200]\n",
      "loss: 1.379864  [ 2384/ 3200]\n",
      "loss: 1.382314  [ 2400/ 3200]\n",
      "loss: 1.385485  [ 2416/ 3200]\n",
      "loss: 1.381669  [ 2432/ 3200]\n",
      "loss: 1.387718  [ 2448/ 3200]\n",
      "loss: 1.386850  [ 2464/ 3200]\n",
      "loss: 1.388771  [ 2480/ 3200]\n",
      "loss: 1.387674  [ 2496/ 3200]\n",
      "loss: 1.387716  [ 2512/ 3200]\n",
      "loss: 1.383563  [ 2528/ 3200]\n",
      "loss: 1.390021  [ 2544/ 3200]\n",
      "loss: 1.386850  [ 2560/ 3200]\n",
      "loss: 1.390208  [ 2576/ 3200]\n",
      "loss: 1.386465  [ 2592/ 3200]\n",
      "loss: 1.392323  [ 2608/ 3200]\n",
      "loss: 1.393829  [ 2624/ 3200]\n",
      "loss: 1.386918  [ 2640/ 3200]\n",
      "loss: 1.379941  [ 2656/ 3200]\n",
      "loss: 1.386849  [ 2672/ 3200]\n",
      "loss: 1.393142  [ 2688/ 3200]\n",
      "loss: 1.389677  [ 2704/ 3200]\n",
      "loss: 1.388697  [ 2720/ 3200]\n",
      "loss: 1.392316  [ 2736/ 3200]\n",
      "loss: 1.388194  [ 2752/ 3200]\n",
      "loss: 1.382063  [ 2768/ 3200]\n",
      "loss: 1.387186  [ 2784/ 3200]\n",
      "loss: 1.388810  [ 2800/ 3200]\n",
      "loss: 1.381384  [ 2816/ 3200]\n",
      "loss: 1.386582  [ 2832/ 3200]\n",
      "loss: 1.389033  [ 2848/ 3200]\n",
      "loss: 1.390538  [ 2864/ 3200]\n",
      "loss: 1.388352  [ 2880/ 3200]\n",
      "loss: 1.380929  [ 2896/ 3200]\n",
      "loss: 1.383114  [ 2912/ 3200]\n",
      "loss: 1.391288  [ 2928/ 3200]\n",
      "loss: 1.390013  [ 2944/ 3200]\n",
      "loss: 1.388006  [ 2960/ 3200]\n",
      "loss: 1.392263  [ 2976/ 3200]\n",
      "loss: 1.388761  [ 2992/ 3200]\n",
      "loss: 1.390129  [ 3008/ 3200]\n",
      "loss: 1.383233  [ 3024/ 3200]\n",
      "loss: 1.385122  [ 3040/ 3200]\n",
      "loss: 1.389554  [ 3056/ 3200]\n",
      "loss: 1.392191  [ 3072/ 3200]\n",
      "loss: 1.377133  [ 3088/ 3200]\n",
      "loss: 1.386847  [ 3104/ 3200]\n",
      "loss: 1.382706  [ 3120/ 3200]\n",
      "loss: 1.386803  [ 3136/ 3200]\n",
      "loss: 1.388235  [ 3152/ 3200]\n",
      "loss: 1.384331  [ 3168/ 3200]\n",
      "loss: 1.388691  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086650\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.8479e-04.\n",
      "\n",
      "Epoch: 39\n",
      "-----------------------------\n",
      "loss: 1.387255  [    0/ 3200]\n",
      "loss: 1.385939  [   16/ 3200]\n",
      "loss: 1.389742  [   32/ 3200]\n",
      "loss: 1.388574  [   48/ 3200]\n",
      "loss: 1.387897  [   64/ 3200]\n",
      "loss: 1.386208  [   80/ 3200]\n",
      "loss: 1.387137  [   96/ 3200]\n",
      "loss: 1.383117  [  112/ 3200]\n",
      "loss: 1.383115  [  128/ 3200]\n",
      "loss: 1.384887  [  144/ 3200]\n",
      "loss: 1.387254  [  160/ 3200]\n",
      "loss: 1.384551  [  176/ 3200]\n",
      "loss: 1.396322  [  192/ 3200]\n",
      "loss: 1.391320  [  208/ 3200]\n",
      "loss: 1.389216  [  224/ 3200]\n",
      "loss: 1.394473  [  240/ 3200]\n",
      "loss: 1.380415  [  256/ 3200]\n",
      "loss: 1.390124  [  272/ 3200]\n",
      "loss: 1.386274  [  288/ 3200]\n",
      "loss: 1.386516  [  304/ 3200]\n",
      "loss: 1.388810  [  320/ 3200]\n",
      "loss: 1.392750  [  336/ 3200]\n",
      "loss: 1.388687  [  352/ 3200]\n",
      "loss: 1.383121  [  368/ 3200]\n",
      "loss: 1.381882  [  384/ 3200]\n",
      "loss: 1.383072  [  400/ 3200]\n",
      "loss: 1.380946  [  416/ 3200]\n",
      "loss: 1.383522  [  432/ 3200]\n",
      "loss: 1.387778  [  448/ 3200]\n",
      "loss: 1.384676  [  464/ 3200]\n",
      "loss: 1.385987  [  480/ 3200]\n",
      "loss: 1.380823  [  496/ 3200]\n",
      "loss: 1.383118  [  512/ 3200]\n",
      "loss: 1.390001  [  528/ 3200]\n",
      "loss: 1.381877  [  544/ 3200]\n",
      "loss: 1.391178  [  560/ 3200]\n",
      "loss: 1.383572  [  576/ 3200]\n",
      "loss: 1.384908  [  592/ 3200]\n",
      "loss: 1.383975  [  608/ 3200]\n",
      "loss: 1.390073  [  624/ 3200]\n",
      "loss: 1.394537  [  640/ 3200]\n",
      "loss: 1.384221  [  656/ 3200]\n",
      "loss: 1.382261  [  672/ 3200]\n",
      "loss: 1.382717  [  688/ 3200]\n",
      "loss: 1.384677  [  704/ 3200]\n",
      "loss: 1.387496  [  720/ 3200]\n",
      "loss: 1.389494  [  736/ 3200]\n",
      "loss: 1.387829  [  752/ 3200]\n",
      "loss: 1.383766  [  768/ 3200]\n",
      "loss: 1.391451  [  784/ 3200]\n",
      "loss: 1.388229  [  800/ 3200]\n",
      "loss: 1.384834  [  816/ 3200]\n",
      "loss: 1.389600  [  832/ 3200]\n",
      "loss: 1.387249  [  848/ 3200]\n",
      "loss: 1.389153  [  864/ 3200]\n",
      "loss: 1.385537  [  880/ 3200]\n",
      "loss: 1.384679  [  896/ 3200]\n",
      "loss: 1.389599  [  912/ 3200]\n",
      "loss: 1.389489  [  928/ 3200]\n",
      "loss: 1.384956  [  944/ 3200]\n",
      "loss: 1.385870  [  960/ 3200]\n",
      "loss: 1.383365  [  976/ 3200]\n",
      "loss: 1.382265  [  992/ 3200]\n",
      "loss: 1.387373  [ 1008/ 3200]\n",
      "loss: 1.389543  [ 1024/ 3200]\n",
      "loss: 1.386207  [ 1040/ 3200]\n",
      "loss: 1.380306  [ 1056/ 3200]\n",
      "loss: 1.390121  [ 1072/ 3200]\n",
      "loss: 1.386517  [ 1088/ 3200]\n",
      "loss: 1.387773  [ 1104/ 3200]\n",
      "loss: 1.381742  [ 1120/ 3200]\n",
      "loss: 1.384034  [ 1136/ 3200]\n",
      "loss: 1.391888  [ 1152/ 3200]\n",
      "loss: 1.391430  [ 1168/ 3200]\n",
      "loss: 1.392164  [ 1184/ 3200]\n",
      "loss: 1.388683  [ 1200/ 3200]\n",
      "loss: 1.385485  [ 1216/ 3200]\n",
      "loss: 1.382319  [ 1232/ 3200]\n",
      "loss: 1.383769  [ 1248/ 3200]\n",
      "loss: 1.387373  [ 1264/ 3200]\n",
      "loss: 1.385535  [ 1280/ 3200]\n",
      "loss: 1.390065  [ 1296/ 3200]\n",
      "loss: 1.390851  [ 1312/ 3200]\n",
      "loss: 1.387825  [ 1328/ 3200]\n",
      "loss: 1.382337  [ 1344/ 3200]\n",
      "loss: 1.384748  [ 1360/ 3200]\n",
      "loss: 1.383366  [ 1376/ 3200]\n",
      "loss: 1.389943  [ 1392/ 3200]\n",
      "loss: 1.385081  [ 1408/ 3200]\n",
      "loss: 1.390181  [ 1424/ 3200]\n",
      "loss: 1.387771  [ 1440/ 3200]\n",
      "loss: 1.388964  [ 1456/ 3200]\n",
      "loss: 1.388680  [ 1472/ 3200]\n",
      "loss: 1.385535  [ 1488/ 3200]\n",
      "loss: 1.387704  [ 1504/ 3200]\n",
      "loss: 1.388225  [ 1520/ 3200]\n",
      "loss: 1.389536  [ 1536/ 3200]\n",
      "loss: 1.380773  [ 1552/ 3200]\n",
      "loss: 1.381361  [ 1568/ 3200]\n",
      "loss: 1.381936  [ 1584/ 3200]\n",
      "loss: 1.387946  [ 1600/ 3200]\n",
      "loss: 1.383887  [ 1616/ 3200]\n",
      "loss: 1.387251  [ 1632/ 3200]\n",
      "loss: 1.388410  [ 1648/ 3200]\n",
      "loss: 1.382208  [ 1664/ 3200]\n",
      "loss: 1.382271  [ 1680/ 3200]\n",
      "loss: 1.388952  [ 1696/ 3200]\n",
      "loss: 1.383178  [ 1712/ 3200]\n",
      "loss: 1.393594  [ 1728/ 3200]\n",
      "loss: 1.385890  [ 1744/ 3200]\n",
      "loss: 1.382843  [ 1760/ 3200]\n",
      "loss: 1.385869  [ 1776/ 3200]\n",
      "loss: 1.382845  [ 1792/ 3200]\n",
      "loss: 1.385986  [ 1808/ 3200]\n",
      "loss: 1.381819  [ 1824/ 3200]\n",
      "loss: 1.384962  [ 1840/ 3200]\n",
      "loss: 1.383246  [ 1856/ 3200]\n",
      "loss: 1.390515  [ 1872/ 3200]\n",
      "loss: 1.391825  [ 1888/ 3200]\n",
      "loss: 1.375980  [ 1904/ 3200]\n",
      "loss: 1.379194  [ 1920/ 3200]\n",
      "loss: 1.384557  [ 1936/ 3200]\n",
      "loss: 1.389086  [ 1952/ 3200]\n",
      "loss: 1.386392  [ 1968/ 3200]\n",
      "loss: 1.385418  [ 1984/ 3200]\n",
      "loss: 1.383700  [ 2000/ 3200]\n",
      "loss: 1.380434  [ 2016/ 3200]\n",
      "loss: 1.387035  [ 2032/ 3200]\n",
      "loss: 1.385822  [ 2048/ 3200]\n",
      "loss: 1.385415  [ 2064/ 3200]\n",
      "loss: 1.388158  [ 2080/ 3200]\n",
      "loss: 1.391305  [ 2096/ 3200]\n",
      "loss: 1.389659  [ 2112/ 3200]\n",
      "loss: 1.392732  [ 2128/ 3200]\n",
      "loss: 1.392228  [ 2144/ 3200]\n",
      "loss: 1.380438  [ 2160/ 3200]\n",
      "loss: 1.384630  [ 2176/ 3200]\n",
      "loss: 1.383771  [ 2192/ 3200]\n",
      "loss: 1.381484  [ 2208/ 3200]\n",
      "loss: 1.382268  [ 2224/ 3200]\n",
      "loss: 1.388394  [ 2240/ 3200]\n",
      "loss: 1.383028  [ 2256/ 3200]\n",
      "loss: 1.387972  [ 2272/ 3200]\n",
      "loss: 1.384150  [ 2288/ 3200]\n",
      "loss: 1.389087  [ 2304/ 3200]\n",
      "loss: 1.386729  [ 2320/ 3200]\n",
      "loss: 1.383249  [ 2336/ 3200]\n",
      "loss: 1.386843  [ 2352/ 3200]\n",
      "loss: 1.391708  [ 2368/ 3200]\n",
      "loss: 1.385534  [ 2384/ 3200]\n",
      "loss: 1.388227  [ 2400/ 3200]\n",
      "loss: 1.388197  [ 2416/ 3200]\n",
      "loss: 1.385650  [ 2432/ 3200]\n",
      "loss: 1.382052  [ 2448/ 3200]\n",
      "loss: 1.382721  [ 2464/ 3200]\n",
      "loss: 1.390850  [ 2480/ 3200]\n",
      "loss: 1.383171  [ 2496/ 3200]\n",
      "loss: 1.386918  [ 2512/ 3200]\n",
      "loss: 1.385043  [ 2528/ 3200]\n",
      "loss: 1.379462  [ 2544/ 3200]\n",
      "loss: 1.383991  [ 2560/ 3200]\n",
      "loss: 1.387252  [ 2576/ 3200]\n",
      "loss: 1.388561  [ 2592/ 3200]\n",
      "loss: 1.386843  [ 2608/ 3200]\n",
      "loss: 1.382272  [ 2624/ 3200]\n",
      "loss: 1.385534  [ 2640/ 3200]\n",
      "loss: 1.386509  [ 2656/ 3200]\n",
      "loss: 1.391825  [ 2672/ 3200]\n",
      "loss: 1.382607  [ 2688/ 3200]\n",
      "loss: 1.384517  [ 2704/ 3200]\n",
      "loss: 1.376958  [ 2720/ 3200]\n",
      "loss: 1.389538  [ 2736/ 3200]\n",
      "loss: 1.386509  [ 2752/ 3200]\n",
      "loss: 1.395379  [ 2768/ 3200]\n",
      "loss: 1.391037  [ 2784/ 3200]\n",
      "loss: 1.389535  [ 2800/ 3200]\n",
      "loss: 1.383818  [ 2816/ 3200]\n",
      "loss: 1.394063  [ 2832/ 3200]\n",
      "loss: 1.383584  [ 2848/ 3200]\n",
      "loss: 1.387894  [ 2864/ 3200]\n",
      "loss: 1.382726  [ 2880/ 3200]\n",
      "loss: 1.393318  [ 2896/ 3200]\n",
      "loss: 1.383136  [ 2912/ 3200]\n",
      "loss: 1.387816  [ 2928/ 3200]\n",
      "loss: 1.386467  [ 2944/ 3200]\n",
      "loss: 1.385982  [ 2960/ 3200]\n",
      "loss: 1.387250  [ 2976/ 3200]\n",
      "loss: 1.384444  [ 2992/ 3200]\n",
      "loss: 1.383890  [ 3008/ 3200]\n",
      "loss: 1.385942  [ 3024/ 3200]\n",
      "loss: 1.385827  [ 3040/ 3200]\n",
      "loss: 1.391365  [ 3056/ 3200]\n",
      "loss: 1.386057  [ 3072/ 3200]\n",
      "loss: 1.382684  [ 3088/ 3200]\n",
      "loss: 1.388746  [ 3104/ 3200]\n",
      "loss: 1.391026  [ 3120/ 3200]\n",
      "loss: 1.389801  [ 3136/ 3200]\n",
      "loss: 1.386845  [ 3152/ 3200]\n",
      "loss: 1.386391  [ 3168/ 3200]\n",
      "loss: 1.387030  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.7055e-04.\n",
      "\n",
      "Epoch: 40\n",
      "-----------------------------\n",
      "loss: 1.388223  [    0/ 3200]\n",
      "loss: 1.388787  [   16/ 3200]\n",
      "loss: 1.384516  [   32/ 3200]\n",
      "loss: 1.388268  [   48/ 3200]\n",
      "loss: 1.387433  [   64/ 3200]\n",
      "loss: 1.391518  [   80/ 3200]\n",
      "loss: 1.384678  [   96/ 3200]\n",
      "loss: 1.387883  [  112/ 3200]\n",
      "loss: 1.380454  [  128/ 3200]\n",
      "loss: 1.382282  [  144/ 3200]\n",
      "loss: 1.385491  [  160/ 3200]\n",
      "loss: 1.387249  [  176/ 3200]\n",
      "loss: 1.386843  [  192/ 3200]\n",
      "loss: 1.392262  [  208/ 3200]\n",
      "loss: 1.391472  [  224/ 3200]\n",
      "loss: 1.382735  [  240/ 3200]\n",
      "loss: 1.384155  [  256/ 3200]\n",
      "loss: 1.384744  [  272/ 3200]\n",
      "loss: 1.390458  [  288/ 3200]\n",
      "loss: 1.377275  [  304/ 3200]\n",
      "loss: 1.392261  [  320/ 3200]\n",
      "loss: 1.393705  [  336/ 3200]\n",
      "loss: 1.382735  [  352/ 3200]\n",
      "loss: 1.391356  [  368/ 3200]\n",
      "loss: 1.384337  [  384/ 3200]\n",
      "loss: 1.389010  [  400/ 3200]\n",
      "loss: 1.391916  [  416/ 3200]\n",
      "loss: 1.387360  [  432/ 3200]\n",
      "loss: 1.386390  [  448/ 3200]\n",
      "loss: 1.384904  [  464/ 3200]\n",
      "loss: 1.390497  [  480/ 3200]\n",
      "loss: 1.385755  [  496/ 3200]\n",
      "loss: 1.386569  [  512/ 3200]\n",
      "loss: 1.388367  [  528/ 3200]\n",
      "loss: 1.391393  [  544/ 3200]\n",
      "loss: 1.389976  [  560/ 3200]\n",
      "loss: 1.393666  [  576/ 3200]\n",
      "loss: 1.391387  [  592/ 3200]\n",
      "loss: 1.387356  [  608/ 3200]\n",
      "loss: 1.392210  [  624/ 3200]\n",
      "loss: 1.380985  [  640/ 3200]\n",
      "loss: 1.388665  [  656/ 3200]\n",
      "loss: 1.378195  [  672/ 3200]\n",
      "loss: 1.383601  [  688/ 3200]\n",
      "loss: 1.390938  [  704/ 3200]\n",
      "loss: 1.386355  [  720/ 3200]\n",
      "loss: 1.384566  [  736/ 3200]\n",
      "loss: 1.389113  [  752/ 3200]\n",
      "loss: 1.391903  [  768/ 3200]\n",
      "loss: 1.387358  [  784/ 3200]\n",
      "loss: 1.383778  [  800/ 3200]\n",
      "loss: 1.383669  [  816/ 3200]\n",
      "loss: 1.385084  [  832/ 3200]\n",
      "loss: 1.384117  [  848/ 3200]\n",
      "loss: 1.387319  [  864/ 3200]\n",
      "loss: 1.386802  [  880/ 3200]\n",
      "loss: 1.378198  [  896/ 3200]\n",
      "loss: 1.387357  [  912/ 3200]\n",
      "loss: 1.387938  [  928/ 3200]\n",
      "loss: 1.390898  [  944/ 3200]\n",
      "loss: 1.384674  [  960/ 3200]\n",
      "loss: 1.388967  [  976/ 3200]\n",
      "loss: 1.387251  [  992/ 3200]\n",
      "loss: 1.388214  [ 1008/ 3200]\n",
      "loss: 1.387656  [ 1024/ 3200]\n",
      "loss: 1.382059  [ 1040/ 3200]\n",
      "loss: 1.388107  [ 1056/ 3200]\n",
      "loss: 1.377236  [ 1072/ 3200]\n",
      "loss: 1.390142  [ 1088/ 3200]\n",
      "loss: 1.378546  [ 1104/ 3200]\n",
      "loss: 1.383366  [ 1120/ 3200]\n",
      "loss: 1.391733  [ 1136/ 3200]\n",
      "loss: 1.386284  [ 1152/ 3200]\n",
      "loss: 1.381779  [ 1168/ 3200]\n",
      "loss: 1.390423  [ 1184/ 3200]\n",
      "loss: 1.385983  [ 1200/ 3200]\n",
      "loss: 1.392994  [ 1216/ 3200]\n",
      "loss: 1.386390  [ 1232/ 3200]\n",
      "loss: 1.384120  [ 1248/ 3200]\n",
      "loss: 1.384910  [ 1264/ 3200]\n",
      "loss: 1.385491  [ 1280/ 3200]\n",
      "loss: 1.385383  [ 1296/ 3200]\n",
      "loss: 1.381737  [ 1312/ 3200]\n",
      "loss: 1.386501  [ 1328/ 3200]\n",
      "loss: 1.382635  [ 1344/ 3200]\n",
      "loss: 1.385535  [ 1360/ 3200]\n",
      "loss: 1.384570  [ 1376/ 3200]\n",
      "loss: 1.388146  [ 1392/ 3200]\n",
      "loss: 1.389970  [ 1408/ 3200]\n",
      "loss: 1.388256  [ 1424/ 3200]\n",
      "loss: 1.392311  [ 1440/ 3200]\n",
      "loss: 1.387316  [ 1456/ 3200]\n",
      "loss: 1.387137  [ 1472/ 3200]\n",
      "loss: 1.386771  [ 1488/ 3200]\n",
      "loss: 1.387722  [ 1504/ 3200]\n",
      "loss: 1.380655  [ 1520/ 3200]\n",
      "loss: 1.383043  [ 1536/ 3200]\n",
      "loss: 1.384907  [ 1552/ 3200]\n",
      "loss: 1.384682  [ 1568/ 3200]\n",
      "loss: 1.386437  [ 1584/ 3200]\n",
      "loss: 1.382704  [ 1600/ 3200]\n",
      "loss: 1.386322  [ 1616/ 3200]\n",
      "loss: 1.387583  [ 1632/ 3200]\n",
      "loss: 1.379574  [ 1648/ 3200]\n",
      "loss: 1.384120  [ 1664/ 3200]\n",
      "loss: 1.389519  [ 1680/ 3200]\n",
      "loss: 1.384679  [ 1696/ 3200]\n",
      "loss: 1.393656  [ 1712/ 3200]\n",
      "loss: 1.385874  [ 1728/ 3200]\n",
      "loss: 1.384233  [ 1744/ 3200]\n",
      "loss: 1.386391  [ 1760/ 3200]\n",
      "loss: 1.391342  [ 1776/ 3200]\n",
      "loss: 1.390782  [ 1792/ 3200]\n",
      "loss: 1.390033  [ 1808/ 3200]\n",
      "loss: 1.392708  [ 1824/ 3200]\n",
      "loss: 1.392190  [ 1840/ 3200]\n",
      "loss: 1.383089  [ 1856/ 3200]\n",
      "loss: 1.386795  [ 1872/ 3200]\n",
      "loss: 1.388211  [ 1888/ 3200]\n",
      "loss: 1.389738  [ 1904/ 3200]\n",
      "loss: 1.388433  [ 1920/ 3200]\n",
      "loss: 1.377361  [ 1936/ 3200]\n",
      "loss: 1.382368  [ 1952/ 3200]\n",
      "loss: 1.385427  [ 1968/ 3200]\n",
      "loss: 1.384416  [ 1984/ 3200]\n",
      "loss: 1.388952  [ 2000/ 3200]\n",
      "loss: 1.387406  [ 2016/ 3200]\n",
      "loss: 1.383672  [ 2032/ 3200]\n",
      "loss: 1.383091  [ 2048/ 3200]\n",
      "loss: 1.382818  [ 2064/ 3200]\n",
      "loss: 1.386907  [ 2080/ 3200]\n",
      "loss: 1.386953  [ 2096/ 3200]\n",
      "loss: 1.384909  [ 2112/ 3200]\n",
      "loss: 1.388212  [ 2128/ 3200]\n",
      "loss: 1.383894  [ 2144/ 3200]\n",
      "loss: 1.387871  [ 2160/ 3200]\n",
      "loss: 1.385877  [ 2176/ 3200]\n",
      "loss: 1.381852  [ 2192/ 3200]\n",
      "loss: 1.388726  [ 2208/ 3200]\n",
      "loss: 1.395807  [ 2224/ 3200]\n",
      "loss: 1.390817  [ 2240/ 3200]\n",
      "loss: 1.387192  [ 2256/ 3200]\n",
      "loss: 1.384186  [ 2272/ 3200]\n",
      "loss: 1.383671  [ 2288/ 3200]\n",
      "loss: 1.390414  [ 2304/ 3200]\n",
      "loss: 1.381620  [ 2320/ 3200]\n",
      "loss: 1.385536  [ 2336/ 3200]\n",
      "loss: 1.383543  [ 2352/ 3200]\n",
      "loss: 1.388208  [ 2368/ 3200]\n",
      "loss: 1.384012  [ 2384/ 3200]\n",
      "loss: 1.388997  [ 2400/ 3200]\n",
      "loss: 1.380486  [ 2416/ 3200]\n",
      "loss: 1.385426  [ 2432/ 3200]\n",
      "loss: 1.386277  [ 2448/ 3200]\n",
      "loss: 1.382640  [ 2464/ 3200]\n",
      "loss: 1.385539  [ 2480/ 3200]\n",
      "loss: 1.392228  [ 2496/ 3200]\n",
      "loss: 1.383538  [ 2512/ 3200]\n",
      "loss: 1.393149  [ 2528/ 3200]\n",
      "loss: 1.384976  [ 2544/ 3200]\n",
      "loss: 1.394492  [ 2560/ 3200]\n",
      "loss: 1.380487  [ 2576/ 3200]\n",
      "loss: 1.394448  [ 2592/ 3200]\n",
      "loss: 1.386725  [ 2608/ 3200]\n",
      "loss: 1.381859  [ 2624/ 3200]\n",
      "loss: 1.383272  [ 2640/ 3200]\n",
      "loss: 1.391846  [ 2656/ 3200]\n",
      "loss: 1.383606  [ 2672/ 3200]\n",
      "loss: 1.388541  [ 2688/ 3200]\n",
      "loss: 1.380890  [ 2704/ 3200]\n",
      "loss: 1.386910  [ 2720/ 3200]\n",
      "loss: 1.379142  [ 2736/ 3200]\n",
      "loss: 1.381122  [ 2752/ 3200]\n",
      "loss: 1.379138  [ 2768/ 3200]\n",
      "loss: 1.379069  [ 2784/ 3200]\n",
      "loss: 1.380932  [ 2800/ 3200]\n",
      "loss: 1.386839  [ 2816/ 3200]\n",
      "loss: 1.397619  [ 2832/ 3200]\n",
      "loss: 1.388209  [ 2848/ 3200]\n",
      "loss: 1.389176  [ 2864/ 3200]\n",
      "loss: 1.383503  [ 2880/ 3200]\n",
      "loss: 1.388590  [ 2896/ 3200]\n",
      "loss: 1.383157  [ 2912/ 3200]\n",
      "loss: 1.385423  [ 2928/ 3200]\n",
      "loss: 1.388542  [ 2944/ 3200]\n",
      "loss: 1.379518  [ 2960/ 3200]\n",
      "loss: 1.390476  [ 2976/ 3200]\n",
      "loss: 1.383862  [ 2992/ 3200]\n",
      "loss: 1.378736  [ 3008/ 3200]\n",
      "loss: 1.383789  [ 3024/ 3200]\n",
      "loss: 1.382823  [ 3040/ 3200]\n",
      "loss: 1.391214  [ 3056/ 3200]\n",
      "loss: 1.391262  [ 3072/ 3200]\n",
      "loss: 1.378336  [ 3088/ 3200]\n",
      "loss: 1.387357  [ 3104/ 3200]\n",
      "loss: 1.390360  [ 3120/ 3200]\n",
      "loss: 1.384757  [ 3136/ 3200]\n",
      "loss: 1.384620  [ 3152/ 3200]\n",
      "loss: 1.384757  [ 3168/ 3200]\n",
      "loss: 1.391375  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.5702e-04.\n",
      "\n",
      "Epoch: 41\n",
      "-----------------------------\n",
      "loss: 1.383723  [    0/ 3200]\n",
      "loss: 1.386055  [   16/ 3200]\n",
      "loss: 1.392855  [   32/ 3200]\n",
      "loss: 1.389957  [   48/ 3200]\n",
      "loss: 1.385650  [   64/ 3200]\n",
      "loss: 1.389578  [   80/ 3200]\n",
      "loss: 1.387872  [   96/ 3200]\n",
      "loss: 1.385537  [  112/ 3200]\n",
      "loss: 1.388029  [  128/ 3200]\n",
      "loss: 1.381456  [  144/ 3200]\n",
      "loss: 1.388613  [  160/ 3200]\n",
      "loss: 1.383117  [  176/ 3200]\n",
      "loss: 1.388207  [  192/ 3200]\n",
      "loss: 1.385876  [  208/ 3200]\n",
      "loss: 1.378965  [  224/ 3200]\n",
      "loss: 1.385829  [  240/ 3200]\n",
      "loss: 1.385540  [  256/ 3200]\n",
      "loss: 1.386322  [  272/ 3200]\n",
      "loss: 1.385540  [  288/ 3200]\n",
      "loss: 1.384056  [  304/ 3200]\n",
      "loss: 1.392177  [  320/ 3200]\n",
      "loss: 1.385493  [  336/ 3200]\n",
      "loss: 1.385540  [  352/ 3200]\n",
      "loss: 1.393922  [  368/ 3200]\n",
      "loss: 1.384458  [  384/ 3200]\n",
      "loss: 1.384527  [  400/ 3200]\n",
      "loss: 1.386391  [  416/ 3200]\n",
      "loss: 1.391321  [  432/ 3200]\n",
      "loss: 1.390139  [  448/ 3200]\n",
      "loss: 1.384127  [  464/ 3200]\n",
      "loss: 1.391321  [  480/ 3200]\n",
      "loss: 1.390136  [  496/ 3200]\n",
      "loss: 1.384243  [  512/ 3200]\n",
      "loss: 1.381012  [  528/ 3200]\n",
      "loss: 1.381861  [  544/ 3200]\n",
      "loss: 1.384577  [  560/ 3200]\n",
      "loss: 1.387174  [  576/ 3200]\n",
      "loss: 1.386723  [  592/ 3200]\n",
      "loss: 1.388655  [  608/ 3200]\n",
      "loss: 1.385159  [  624/ 3200]\n",
      "loss: 1.387806  [  640/ 3200]\n",
      "loss: 1.386506  [  656/ 3200]\n",
      "loss: 1.383390  [  672/ 3200]\n",
      "loss: 1.383212  [  688/ 3200]\n",
      "loss: 1.387133  [  704/ 3200]\n",
      "loss: 1.387241  [  720/ 3200]\n",
      "loss: 1.378233  [  736/ 3200]\n",
      "loss: 1.381797  [  752/ 3200]\n",
      "loss: 1.388609  [  768/ 3200]\n",
      "loss: 1.386391  [  784/ 3200]\n",
      "loss: 1.379710  [  800/ 3200]\n",
      "loss: 1.388140  [  816/ 3200]\n",
      "loss: 1.384126  [  832/ 3200]\n",
      "loss: 1.385540  [  848/ 3200]\n",
      "loss: 1.386504  [  864/ 3200]\n",
      "loss: 1.388813  [  880/ 3200]\n",
      "loss: 1.382379  [  896/ 3200]\n",
      "loss: 1.388543  [  912/ 3200]\n",
      "loss: 1.389685  [  928/ 3200]\n",
      "loss: 1.384687  [  944/ 3200]\n",
      "loss: 1.380048  [  960/ 3200]\n",
      "loss: 1.385874  [  976/ 3200]\n",
      "loss: 1.383611  [  992/ 3200]\n",
      "loss: 1.386951  [ 1008/ 3200]\n",
      "loss: 1.389955  [ 1024/ 3200]\n",
      "loss: 1.387735  [ 1040/ 3200]\n",
      "loss: 1.385468  [ 1056/ 3200]\n",
      "loss: 1.384645  [ 1072/ 3200]\n",
      "loss: 1.390878  [ 1088/ 3200]\n",
      "loss: 1.391837  [ 1104/ 3200]\n",
      "loss: 1.385024  [ 1120/ 3200]\n",
      "loss: 1.389436  [ 1136/ 3200]\n",
      "loss: 1.383792  [ 1152/ 3200]\n",
      "loss: 1.388653  [ 1168/ 3200]\n",
      "loss: 1.386320  [ 1184/ 3200]\n",
      "loss: 1.387758  [ 1200/ 3200]\n",
      "loss: 1.389168  [ 1216/ 3200]\n",
      "loss: 1.390467  [ 1232/ 3200]\n",
      "loss: 1.385200  [ 1248/ 3200]\n",
      "loss: 1.386837  [ 1264/ 3200]\n",
      "loss: 1.389165  [ 1280/ 3200]\n",
      "loss: 1.386729  [ 1296/ 3200]\n",
      "loss: 1.385716  [ 1312/ 3200]\n",
      "loss: 1.387689  [ 1328/ 3200]\n",
      "loss: 1.389095  [ 1344/ 3200]\n",
      "loss: 1.388203  [ 1360/ 3200]\n",
      "loss: 1.385199  [ 1376/ 3200]\n",
      "loss: 1.383277  [ 1392/ 3200]\n",
      "loss: 1.376498  [ 1408/ 3200]\n",
      "loss: 1.389949  [ 1424/ 3200]\n",
      "loss: 1.383616  [ 1440/ 3200]\n",
      "loss: 1.391871  [ 1456/ 3200]\n",
      "loss: 1.390695  [ 1472/ 3200]\n",
      "loss: 1.385537  [ 1488/ 3200]\n",
      "loss: 1.385835  [ 1504/ 3200]\n",
      "loss: 1.390355  [ 1520/ 3200]\n",
      "loss: 1.386837  [ 1536/ 3200]\n",
      "loss: 1.387460  [ 1552/ 3200]\n",
      "loss: 1.390462  [ 1568/ 3200]\n",
      "loss: 1.385497  [ 1584/ 3200]\n",
      "loss: 1.384131  [ 1600/ 3200]\n",
      "loss: 1.389164  [ 1616/ 3200]\n",
      "loss: 1.383685  [ 1632/ 3200]\n",
      "loss: 1.385200  [ 1648/ 3200]\n",
      "loss: 1.393167  [ 1664/ 3200]\n",
      "loss: 1.386389  [ 1680/ 3200]\n",
      "loss: 1.382832  [ 1696/ 3200]\n",
      "loss: 1.388647  [ 1712/ 3200]\n",
      "loss: 1.384579  [ 1728/ 3200]\n",
      "loss: 1.394909  [ 1744/ 3200]\n",
      "loss: 1.382322  [ 1760/ 3200]\n",
      "loss: 1.387350  [ 1776/ 3200]\n",
      "loss: 1.390350  [ 1792/ 3200]\n",
      "loss: 1.381324  [ 1808/ 3200]\n",
      "loss: 1.390459  [ 1824/ 3200]\n",
      "loss: 1.388604  [ 1840/ 3200]\n",
      "loss: 1.386388  [ 1856/ 3200]\n",
      "loss: 1.387240  [ 1872/ 3200]\n",
      "loss: 1.389605  [ 1888/ 3200]\n",
      "loss: 1.387731  [ 1904/ 3200]\n",
      "loss: 1.386837  [ 1920/ 3200]\n",
      "loss: 1.383282  [ 1936/ 3200]\n",
      "loss: 1.385432  [ 1952/ 3200]\n",
      "loss: 1.386794  [ 1968/ 3200]\n",
      "loss: 1.383283  [ 1984/ 3200]\n",
      "loss: 1.384307  [ 2000/ 3200]\n",
      "loss: 1.386728  [ 2016/ 3200]\n",
      "loss: 1.385026  [ 2032/ 3200]\n",
      "loss: 1.379218  [ 2048/ 3200]\n",
      "loss: 1.388306  [ 2064/ 3200]\n",
      "loss: 1.385092  [ 2080/ 3200]\n",
      "loss: 1.382537  [ 2096/ 3200]\n",
      "loss: 1.386051  [ 2112/ 3200]\n",
      "loss: 1.379175  [ 2128/ 3200]\n",
      "loss: 1.389118  [ 2144/ 3200]\n",
      "loss: 1.383622  [ 2160/ 3200]\n",
      "loss: 1.384920  [ 2176/ 3200]\n",
      "loss: 1.387242  [ 2192/ 3200]\n",
      "loss: 1.383620  [ 2208/ 3200]\n",
      "loss: 1.383727  [ 2224/ 3200]\n",
      "loss: 1.384644  [ 2240/ 3200]\n",
      "loss: 1.385984  [ 2256/ 3200]\n",
      "loss: 1.385771  [ 2272/ 3200]\n",
      "loss: 1.384133  [ 2288/ 3200]\n",
      "loss: 1.388052  [ 2304/ 3200]\n",
      "loss: 1.391711  [ 2320/ 3200]\n",
      "loss: 1.390904  [ 2336/ 3200]\n",
      "loss: 1.387688  [ 2352/ 3200]\n",
      "loss: 1.388984  [ 2368/ 3200]\n",
      "loss: 1.389496  [ 2384/ 3200]\n",
      "loss: 1.390792  [ 2400/ 3200]\n",
      "loss: 1.385942  [ 2416/ 3200]\n",
      "loss: 1.389203  [ 2432/ 3200]\n",
      "loss: 1.386945  [ 2448/ 3200]\n",
      "loss: 1.390344  [ 2464/ 3200]\n",
      "loss: 1.378777  [ 2480/ 3200]\n",
      "loss: 1.388263  [ 2496/ 3200]\n",
      "loss: 1.387622  [ 2512/ 3200]\n",
      "loss: 1.388646  [ 2528/ 3200]\n",
      "loss: 1.384135  [ 2544/ 3200]\n",
      "loss: 1.388022  [ 2560/ 3200]\n",
      "loss: 1.389199  [ 2576/ 3200]\n",
      "loss: 1.391258  [ 2592/ 3200]\n",
      "loss: 1.387282  [ 2608/ 3200]\n",
      "loss: 1.386390  [ 2624/ 3200]\n",
      "loss: 1.384136  [ 2640/ 3200]\n",
      "loss: 1.385497  [ 2656/ 3200]\n",
      "loss: 1.386388  [ 2672/ 3200]\n",
      "loss: 1.387236  [ 2688/ 3200]\n",
      "loss: 1.384245  [ 2704/ 3200]\n",
      "loss: 1.387731  [ 2720/ 3200]\n",
      "loss: 1.384472  [ 2736/ 3200]\n",
      "loss: 1.375164  [ 2752/ 3200]\n",
      "loss: 1.380695  [ 2768/ 3200]\n",
      "loss: 1.381098  [ 2784/ 3200]\n",
      "loss: 1.386325  [ 2800/ 3200]\n",
      "loss: 1.387730  [ 2816/ 3200]\n",
      "loss: 1.377927  [ 2832/ 3200]\n",
      "loss: 1.388089  [ 2848/ 3200]\n",
      "loss: 1.385606  [ 2864/ 3200]\n",
      "loss: 1.383176  [ 2880/ 3200]\n",
      "loss: 1.390790  [ 2896/ 3200]\n",
      "loss: 1.376118  [ 2912/ 3200]\n",
      "loss: 1.389157  [ 2928/ 3200]\n",
      "loss: 1.389941  [ 2944/ 3200]\n",
      "loss: 1.390452  [ 2960/ 3200]\n",
      "loss: 1.386282  [ 2976/ 3200]\n",
      "loss: 1.386346  [ 2992/ 3200]\n",
      "loss: 1.386726  [ 3008/ 3200]\n",
      "loss: 1.387575  [ 3024/ 3200]\n",
      "loss: 1.389828  [ 3040/ 3200]\n",
      "loss: 1.390453  [ 3056/ 3200]\n",
      "loss: 1.384983  [ 3072/ 3200]\n",
      "loss: 1.391677  [ 3088/ 3200]\n",
      "loss: 1.383221  [ 3104/ 3200]\n",
      "loss: 1.382775  [ 3120/ 3200]\n",
      "loss: 1.387125  [ 3136/ 3200]\n",
      "loss: 1.385723  [ 3152/ 3200]\n",
      "loss: 1.387057  [ 3168/ 3200]\n",
      "loss: 1.388266  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.4417e-04.\n",
      "\n",
      "Epoch: 42\n",
      "-----------------------------\n",
      "loss: 1.385430  [    0/ 3200]\n",
      "loss: 1.391525  [   16/ 3200]\n",
      "loss: 1.389270  [   32/ 3200]\n",
      "loss: 1.389557  [   48/ 3200]\n",
      "loss: 1.387281  [   64/ 3200]\n",
      "loss: 1.385431  [   80/ 3200]\n",
      "loss: 1.389154  [   96/ 3200]\n",
      "loss: 1.388529  [  112/ 3200]\n",
      "loss: 1.390893  [  128/ 3200]\n",
      "loss: 1.384138  [  144/ 3200]\n",
      "loss: 1.387348  [  160/ 3200]\n",
      "loss: 1.381819  [  176/ 3200]\n",
      "loss: 1.389511  [  192/ 3200]\n",
      "loss: 1.386902  [  208/ 3200]\n",
      "loss: 1.386277  [  224/ 3200]\n",
      "loss: 1.393145  [  240/ 3200]\n",
      "loss: 1.386120  [  256/ 3200]\n",
      "loss: 1.390378  [  272/ 3200]\n",
      "loss: 1.391223  [  288/ 3200]\n",
      "loss: 1.386211  [  304/ 3200]\n",
      "loss: 1.387747  [  320/ 3200]\n",
      "loss: 1.389549  [  336/ 3200]\n",
      "loss: 1.384538  [  352/ 3200]\n",
      "loss: 1.391286  [  368/ 3200]\n",
      "loss: 1.379130  [  384/ 3200]\n",
      "loss: 1.387170  [  400/ 3200]\n",
      "loss: 1.386671  [  416/ 3200]\n",
      "loss: 1.387744  [  432/ 3200]\n",
      "loss: 1.386273  [  448/ 3200]\n",
      "loss: 1.384917  [  464/ 3200]\n",
      "loss: 1.388756  [  480/ 3200]\n",
      "loss: 1.381046  [  496/ 3200]\n",
      "loss: 1.385544  [  512/ 3200]\n",
      "loss: 1.380535  [  528/ 3200]\n",
      "loss: 1.391337  [  544/ 3200]\n",
      "loss: 1.382452  [  560/ 3200]\n",
      "loss: 1.378399  [  576/ 3200]\n",
      "loss: 1.383409  [  592/ 3200]\n",
      "loss: 1.384537  [  608/ 3200]\n",
      "loss: 1.385829  [  624/ 3200]\n",
      "loss: 1.381377  [  640/ 3200]\n",
      "loss: 1.382334  [  656/ 3200]\n",
      "loss: 1.383410  [  672/ 3200]\n",
      "loss: 1.387348  [  688/ 3200]\n",
      "loss: 1.389997  [  704/ 3200]\n",
      "loss: 1.392744  [  720/ 3200]\n",
      "loss: 1.381842  [  736/ 3200]\n",
      "loss: 1.390443  [  752/ 3200]\n",
      "loss: 1.383692  [  768/ 3200]\n",
      "loss: 1.394765  [  784/ 3200]\n",
      "loss: 1.385430  [  800/ 3200]\n",
      "loss: 1.384025  [  816/ 3200]\n",
      "loss: 1.384586  [  832/ 3200]\n",
      "loss: 1.387168  [  848/ 3200]\n",
      "loss: 1.389598  [  864/ 3200]\n",
      "loss: 1.386389  [  880/ 3200]\n",
      "loss: 1.387282  [  896/ 3200]\n",
      "loss: 1.386391  [  912/ 3200]\n",
      "loss: 1.385941  [  928/ 3200]\n",
      "loss: 1.386720  [  944/ 3200]\n",
      "loss: 1.383297  [  960/ 3200]\n",
      "loss: 1.389599  [  976/ 3200]\n",
      "loss: 1.381494  [  992/ 3200]\n",
      "loss: 1.384258  [ 1008/ 3200]\n",
      "loss: 1.387679  [ 1024/ 3200]\n",
      "loss: 1.387860  [ 1040/ 3200]\n",
      "loss: 1.390839  [ 1056/ 3200]\n",
      "loss: 1.390439  [ 1072/ 3200]\n",
      "loss: 1.390834  [ 1088/ 3200]\n",
      "loss: 1.384256  [ 1104/ 3200]\n",
      "loss: 1.386440  [ 1120/ 3200]\n",
      "loss: 1.384076  [ 1136/ 3200]\n",
      "loss: 1.391777  [ 1152/ 3200]\n",
      "loss: 1.388145  [ 1168/ 3200]\n",
      "loss: 1.384255  [ 1184/ 3200]\n",
      "loss: 1.384872  [ 1200/ 3200]\n",
      "loss: 1.379135  [ 1216/ 3200]\n",
      "loss: 1.387909  [ 1232/ 3200]\n",
      "loss: 1.383743  [ 1248/ 3200]\n",
      "loss: 1.383296  [ 1264/ 3200]\n",
      "loss: 1.389150  [ 1280/ 3200]\n",
      "loss: 1.382520  [ 1296/ 3200]\n",
      "loss: 1.382338  [ 1312/ 3200]\n",
      "loss: 1.393085  [ 1328/ 3200]\n",
      "loss: 1.385324  [ 1344/ 3200]\n",
      "loss: 1.385945  [ 1360/ 3200]\n",
      "loss: 1.392573  [ 1376/ 3200]\n",
      "loss: 1.389863  [ 1392/ 3200]\n",
      "loss: 1.390038  [ 1408/ 3200]\n",
      "loss: 1.390769  [ 1424/ 3200]\n",
      "loss: 1.379254  [ 1440/ 3200]\n",
      "loss: 1.388541  [ 1456/ 3200]\n",
      "loss: 1.391278  [ 1472/ 3200]\n",
      "loss: 1.387854  [ 1488/ 3200]\n",
      "loss: 1.382457  [ 1504/ 3200]\n",
      "loss: 1.390036  [ 1520/ 3200]\n",
      "loss: 1.382742  [ 1536/ 3200]\n",
      "loss: 1.386167  [ 1552/ 3200]\n",
      "loss: 1.382790  [ 1568/ 3200]\n",
      "loss: 1.385432  [ 1584/ 3200]\n",
      "loss: 1.385879  [ 1600/ 3200]\n",
      "loss: 1.388633  [ 1616/ 3200]\n",
      "loss: 1.383698  [ 1632/ 3200]\n",
      "loss: 1.386786  [ 1648/ 3200]\n",
      "loss: 1.382294  [ 1664/ 3200]\n",
      "loss: 1.382073  [ 1680/ 3200]\n",
      "loss: 1.382738  [ 1696/ 3200]\n",
      "loss: 1.384764  [ 1712/ 3200]\n",
      "loss: 1.385666  [ 1728/ 3200]\n",
      "loss: 1.380881  [ 1744/ 3200]\n",
      "loss: 1.384194  [ 1760/ 3200]\n",
      "loss: 1.391391  [ 1776/ 3200]\n",
      "loss: 1.384255  [ 1792/ 3200]\n",
      "loss: 1.384651  [ 1808/ 3200]\n",
      "loss: 1.385940  [ 1824/ 3200]\n",
      "loss: 1.388752  [ 1840/ 3200]\n",
      "loss: 1.390322  [ 1856/ 3200]\n",
      "loss: 1.382401  [ 1872/ 3200]\n",
      "loss: 1.382961  [ 1888/ 3200]\n",
      "loss: 1.388637  [ 1904/ 3200]\n",
      "loss: 1.379255  [ 1920/ 3200]\n",
      "loss: 1.382684  [ 1936/ 3200]\n",
      "loss: 1.393527  [ 1952/ 3200]\n",
      "loss: 1.380941  [ 1968/ 3200]\n",
      "loss: 1.384141  [ 1984/ 3200]\n",
      "loss: 1.381446  [ 2000/ 3200]\n",
      "loss: 1.385995  [ 2016/ 3200]\n",
      "loss: 1.381894  [ 2032/ 3200]\n",
      "loss: 1.383748  [ 2048/ 3200]\n",
      "loss: 1.386050  [ 2064/ 3200]\n",
      "loss: 1.395158  [ 2080/ 3200]\n",
      "loss: 1.389031  [ 2096/ 3200]\n",
      "loss: 1.384929  [ 2112/ 3200]\n",
      "loss: 1.383749  [ 2128/ 3200]\n",
      "loss: 1.388241  [ 2144/ 3200]\n",
      "loss: 1.384253  [ 2160/ 3200]\n",
      "loss: 1.391725  [ 2176/ 3200]\n",
      "loss: 1.384590  [ 2192/ 3200]\n",
      "loss: 1.387343  [ 2208/ 3200]\n",
      "loss: 1.377684  [ 2224/ 3200]\n",
      "loss: 1.385828  [ 2240/ 3200]\n",
      "loss: 1.392287  [ 2256/ 3200]\n",
      "loss: 1.382682  [ 2272/ 3200]\n",
      "loss: 1.388806  [ 2288/ 3200]\n",
      "loss: 1.384591  [ 2304/ 3200]\n",
      "loss: 1.384082  [ 2320/ 3200]\n",
      "loss: 1.384702  [ 2336/ 3200]\n",
      "loss: 1.388077  [ 2352/ 3200]\n",
      "loss: 1.386776  [ 2368/ 3200]\n",
      "loss: 1.386776  [ 2384/ 3200]\n",
      "loss: 1.384986  [ 2400/ 3200]\n",
      "loss: 1.379589  [ 2416/ 3200]\n",
      "loss: 1.387232  [ 2432/ 3200]\n",
      "loss: 1.383122  [ 2448/ 3200]\n",
      "loss: 1.392171  [ 2464/ 3200]\n",
      "loss: 1.384986  [ 2480/ 3200]\n",
      "loss: 1.390766  [ 2496/ 3200]\n",
      "loss: 1.387345  [ 2512/ 3200]\n",
      "loss: 1.392792  [ 2528/ 3200]\n",
      "loss: 1.391671  [ 2544/ 3200]\n",
      "loss: 1.387345  [ 2560/ 3200]\n",
      "loss: 1.394805  [ 2576/ 3200]\n",
      "loss: 1.390761  [ 2592/ 3200]\n",
      "loss: 1.387345  [ 2608/ 3200]\n",
      "loss: 1.385214  [ 2624/ 3200]\n",
      "loss: 1.385879  [ 2640/ 3200]\n",
      "loss: 1.381902  [ 2656/ 3200]\n",
      "loss: 1.389918  [ 2672/ 3200]\n",
      "loss: 1.385547  [ 2688/ 3200]\n",
      "loss: 1.386899  [ 2704/ 3200]\n",
      "loss: 1.381060  [ 2720/ 3200]\n",
      "loss: 1.389920  [ 2736/ 3200]\n",
      "loss: 1.392161  [ 2752/ 3200]\n",
      "loss: 1.384876  [ 2768/ 3200]\n",
      "loss: 1.381063  [ 2784/ 3200]\n",
      "loss: 1.385547  [ 2800/ 3200]\n",
      "loss: 1.386784  [ 2816/ 3200]\n",
      "loss: 1.389803  [ 2832/ 3200]\n",
      "loss: 1.386057  [ 2848/ 3200]\n",
      "loss: 1.387561  [ 2864/ 3200]\n",
      "loss: 1.389918  [ 2880/ 3200]\n",
      "loss: 1.385665  [ 2896/ 3200]\n",
      "loss: 1.385102  [ 2912/ 3200]\n",
      "loss: 1.385547  [ 2928/ 3200]\n",
      "loss: 1.387611  [ 2944/ 3200]\n",
      "loss: 1.389981  [ 2960/ 3200]\n",
      "loss: 1.381511  [ 2976/ 3200]\n",
      "loss: 1.382465  [ 2992/ 3200]\n",
      "loss: 1.382795  [ 3008/ 3200]\n",
      "loss: 1.384478  [ 3024/ 3200]\n",
      "loss: 1.389521  [ 3040/ 3200]\n",
      "loss: 1.386388  [ 3056/ 3200]\n",
      "loss: 1.390095  [ 3072/ 3200]\n",
      "loss: 1.382908  [ 3088/ 3200]\n",
      "loss: 1.387277  [ 3104/ 3200]\n",
      "loss: 1.380999  [ 3120/ 3200]\n",
      "loss: 1.393466  [ 3136/ 3200]\n",
      "loss: 1.384658  [ 3152/ 3200]\n",
      "loss: 1.383193  [ 3168/ 3200]\n",
      "loss: 1.390474  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.3196e-04.\n",
      "\n",
      "Epoch: 43\n",
      "-----------------------------\n",
      "loss: 1.383638  [    0/ 3200]\n",
      "loss: 1.388742  [   16/ 3200]\n",
      "loss: 1.387742  [   32/ 3200]\n",
      "loss: 1.387899  [   48/ 3200]\n",
      "loss: 1.391710  [   64/ 3200]\n",
      "loss: 1.385037  [   80/ 3200]\n",
      "loss: 1.385058  [   96/ 3200]\n",
      "loss: 1.379160  [  112/ 3200]\n",
      "loss: 1.387786  [  128/ 3200]\n",
      "loss: 1.380622  [  144/ 3200]\n",
      "loss: 1.389870  [  160/ 3200]\n",
      "loss: 1.387673  [  176/ 3200]\n",
      "loss: 1.390821  [  192/ 3200]\n",
      "loss: 1.386341  [  208/ 3200]\n",
      "loss: 1.386340  [  224/ 3200]\n",
      "loss: 1.389978  [  240/ 3200]\n",
      "loss: 1.384772  [  256/ 3200]\n",
      "loss: 1.393044  [  272/ 3200]\n",
      "loss: 1.390533  [  288/ 3200]\n",
      "loss: 1.386325  [  304/ 3200]\n",
      "loss: 1.389313  [  320/ 3200]\n",
      "loss: 1.383754  [  336/ 3200]\n",
      "loss: 1.381911  [  352/ 3200]\n",
      "loss: 1.393166  [  368/ 3200]\n",
      "loss: 1.383532  [  384/ 3200]\n",
      "loss: 1.379896  [  400/ 3200]\n",
      "loss: 1.376529  [  416/ 3200]\n",
      "loss: 1.389406  [  432/ 3200]\n",
      "loss: 1.388690  [  448/ 3200]\n",
      "loss: 1.385213  [  464/ 3200]\n",
      "loss: 1.381466  [  480/ 3200]\n",
      "loss: 1.389470  [  496/ 3200]\n",
      "loss: 1.386894  [  512/ 3200]\n",
      "loss: 1.382863  [  528/ 3200]\n",
      "loss: 1.386004  [  544/ 3200]\n",
      "loss: 1.388628  [  560/ 3200]\n",
      "loss: 1.381741  [  576/ 3200]\n",
      "loss: 1.385212  [  592/ 3200]\n",
      "loss: 1.388072  [  608/ 3200]\n",
      "loss: 1.384039  [  624/ 3200]\n",
      "loss: 1.385659  [  640/ 3200]\n",
      "loss: 1.386165  [  656/ 3200]\n",
      "loss: 1.393440  [  672/ 3200]\n",
      "loss: 1.383247  [  688/ 3200]\n",
      "loss: 1.385833  [  704/ 3200]\n",
      "loss: 1.380119  [  720/ 3200]\n",
      "loss: 1.390085  [  736/ 3200]\n",
      "loss: 1.387846  [  752/ 3200]\n",
      "loss: 1.384594  [  768/ 3200]\n",
      "loss: 1.383754  [  784/ 3200]\n",
      "loss: 1.386835  [  800/ 3200]\n",
      "loss: 1.381406  [  816/ 3200]\n",
      "loss: 1.389468  [  832/ 3200]\n",
      "loss: 1.383198  [  848/ 3200]\n",
      "loss: 1.381466  [  864/ 3200]\n",
      "loss: 1.386944  [  880/ 3200]\n",
      "loss: 1.385657  [  896/ 3200]\n",
      "loss: 1.388577  [  912/ 3200]\n",
      "loss: 1.383595  [  928/ 3200]\n",
      "loss: 1.387626  [  944/ 3200]\n",
      "loss: 1.383704  [  960/ 3200]\n",
      "loss: 1.383703  [  976/ 3200]\n",
      "loss: 1.388120  [  992/ 3200]\n",
      "loss: 1.377544  [ 1008/ 3200]\n",
      "loss: 1.387845  [ 1024/ 3200]\n",
      "loss: 1.388291  [ 1040/ 3200]\n",
      "loss: 1.384149  [ 1056/ 3200]\n",
      "loss: 1.393106  [ 1072/ 3200]\n",
      "loss: 1.389021  [ 1088/ 3200]\n",
      "loss: 1.386724  [ 1104/ 3200]\n",
      "loss: 1.390923  [ 1120/ 3200]\n",
      "loss: 1.385100  [ 1136/ 3200]\n",
      "loss: 1.382358  [ 1152/ 3200]\n",
      "loss: 1.385775  [ 1168/ 3200]\n",
      "loss: 1.384990  [ 1184/ 3200]\n",
      "loss: 1.384763  [ 1200/ 3200]\n",
      "loss: 1.386388  [ 1216/ 3200]\n",
      "loss: 1.386667  [ 1232/ 3200]\n",
      "loss: 1.388680  [ 1248/ 3200]\n",
      "loss: 1.390308  [ 1264/ 3200]\n",
      "loss: 1.387783  [ 1280/ 3200]\n",
      "loss: 1.379281  [ 1296/ 3200]\n",
      "loss: 1.379389  [ 1312/ 3200]\n",
      "loss: 1.392546  [ 1328/ 3200]\n",
      "loss: 1.387675  [ 1344/ 3200]\n",
      "loss: 1.388290  [ 1360/ 3200]\n",
      "loss: 1.390022  [ 1376/ 3200]\n",
      "loss: 1.387725  [ 1392/ 3200]\n",
      "loss: 1.385547  [ 1408/ 3200]\n",
      "loss: 1.381519  [ 1424/ 3200]\n",
      "loss: 1.392763  [ 1440/ 3200]\n",
      "loss: 1.392099  [ 1456/ 3200]\n",
      "loss: 1.389682  [ 1472/ 3200]\n",
      "loss: 1.384104  [ 1488/ 3200]\n",
      "loss: 1.388120  [ 1504/ 3200]\n",
      "loss: 1.387180  [ 1520/ 3200]\n",
      "loss: 1.391137  [ 1536/ 3200]\n",
      "loss: 1.386389  [ 1552/ 3200]\n",
      "loss: 1.388623  [ 1568/ 3200]\n",
      "loss: 1.386772  [ 1584/ 3200]\n",
      "loss: 1.384659  [ 1600/ 3200]\n",
      "loss: 1.393645  [ 1616/ 3200]\n",
      "loss: 1.387780  [ 1632/ 3200]\n",
      "loss: 1.388007  [ 1648/ 3200]\n",
      "loss: 1.388667  [ 1664/ 3200]\n",
      "loss: 1.384153  [ 1680/ 3200]\n",
      "loss: 1.390475  [ 1696/ 3200]\n",
      "loss: 1.385989  [ 1712/ 3200]\n",
      "loss: 1.384047  [ 1728/ 3200]\n",
      "loss: 1.390348  [ 1744/ 3200]\n",
      "loss: 1.391294  [ 1760/ 3200]\n",
      "loss: 1.383207  [ 1776/ 3200]\n",
      "loss: 1.388113  [ 1792/ 3200]\n",
      "loss: 1.388793  [ 1808/ 3200]\n",
      "loss: 1.383713  [ 1824/ 3200]\n",
      "loss: 1.387779  [ 1840/ 3200]\n",
      "loss: 1.384769  [ 1856/ 3200]\n",
      "loss: 1.389567  [ 1872/ 3200]\n",
      "loss: 1.392090  [ 1888/ 3200]\n",
      "loss: 1.392469  [ 1904/ 3200]\n",
      "loss: 1.382262  [ 1920/ 3200]\n",
      "loss: 1.388574  [ 1936/ 3200]\n",
      "loss: 1.389351  [ 1952/ 3200]\n",
      "loss: 1.387781  [ 1968/ 3200]\n",
      "loss: 1.389964  [ 1984/ 3200]\n",
      "loss: 1.388176  [ 2000/ 3200]\n",
      "loss: 1.389396  [ 2016/ 3200]\n",
      "loss: 1.385991  [ 2032/ 3200]\n",
      "loss: 1.388572  [ 2048/ 3200]\n",
      "loss: 1.385990  [ 2064/ 3200]\n",
      "loss: 1.381928  [ 2080/ 3200]\n",
      "loss: 1.386892  [ 2096/ 3200]\n",
      "loss: 1.387057  [ 2112/ 3200]\n",
      "loss: 1.386453  [ 2128/ 3200]\n",
      "loss: 1.382815  [ 2144/ 3200]\n",
      "loss: 1.385325  [ 2160/ 3200]\n",
      "loss: 1.389061  [ 2176/ 3200]\n",
      "loss: 1.385332  [ 2192/ 3200]\n",
      "loss: 1.385502  [ 2208/ 3200]\n",
      "loss: 1.386053  [ 2224/ 3200]\n",
      "loss: 1.383762  [ 2240/ 3200]\n",
      "loss: 1.385612  [ 2256/ 3200]\n",
      "loss: 1.384202  [ 2272/ 3200]\n",
      "loss: 1.386892  [ 2288/ 3200]\n",
      "loss: 1.387562  [ 2304/ 3200]\n",
      "loss: 1.389672  [ 2320/ 3200]\n",
      "loss: 1.385042  [ 2336/ 3200]\n",
      "loss: 1.381487  [ 2352/ 3200]\n",
      "loss: 1.381424  [ 2368/ 3200]\n",
      "loss: 1.378750  [ 2384/ 3200]\n",
      "loss: 1.380536  [ 2400/ 3200]\n",
      "loss: 1.384048  [ 2416/ 3200]\n",
      "loss: 1.387336  [ 2432/ 3200]\n",
      "loss: 1.383870  [ 2448/ 3200]\n",
      "loss: 1.384219  [ 2464/ 3200]\n",
      "loss: 1.383761  [ 2480/ 3200]\n",
      "loss: 1.389062  [ 2496/ 3200]\n",
      "loss: 1.380688  [ 2512/ 3200]\n",
      "loss: 1.383987  [ 2528/ 3200]\n",
      "loss: 1.385271  [ 2544/ 3200]\n",
      "loss: 1.385817  [ 2560/ 3200]\n",
      "loss: 1.389568  [ 2576/ 3200]\n",
      "loss: 1.380247  [ 2592/ 3200]\n",
      "loss: 1.376958  [ 2608/ 3200]\n",
      "loss: 1.388219  [ 2624/ 3200]\n",
      "loss: 1.384157  [ 2640/ 3200]\n",
      "loss: 1.382983  [ 2656/ 3200]\n",
      "loss: 1.387670  [ 2672/ 3200]\n",
      "loss: 1.389127  [ 2688/ 3200]\n",
      "loss: 1.383315  [ 2704/ 3200]\n",
      "loss: 1.381861  [ 2720/ 3200]\n",
      "loss: 1.385547  [ 2736/ 3200]\n",
      "loss: 1.387670  [ 2752/ 3200]\n",
      "loss: 1.387777  [ 2768/ 3200]\n",
      "loss: 1.387228  [ 2784/ 3200]\n",
      "loss: 1.384091  [ 2800/ 3200]\n",
      "loss: 1.388726  [ 2816/ 3200]\n",
      "loss: 1.393921  [ 2832/ 3200]\n",
      "loss: 1.383944  [ 2848/ 3200]\n",
      "loss: 1.383208  [ 2864/ 3200]\n",
      "loss: 1.392970  [ 2880/ 3200]\n",
      "loss: 1.386388  [ 2896/ 3200]\n",
      "loss: 1.391977  [ 2912/ 3200]\n",
      "loss: 1.383276  [ 2928/ 3200]\n",
      "loss: 1.387845  [ 2944/ 3200]\n",
      "loss: 1.389499  [ 2960/ 3200]\n",
      "loss: 1.383717  [ 2976/ 3200]\n",
      "loss: 1.386719  [ 2992/ 3200]\n",
      "loss: 1.393474  [ 3008/ 3200]\n",
      "loss: 1.389059  [ 3024/ 3200]\n",
      "loss: 1.383541  [ 3040/ 3200]\n",
      "loss: 1.382877  [ 3056/ 3200]\n",
      "loss: 1.387776  [ 3072/ 3200]\n",
      "loss: 1.389013  [ 3088/ 3200]\n",
      "loss: 1.380252  [ 3104/ 3200]\n",
      "loss: 1.387775  [ 3120/ 3200]\n",
      "loss: 1.387558  [ 3136/ 3200]\n",
      "loss: 1.388175  [ 3152/ 3200]\n",
      "loss: 1.382328  [ 3168/ 3200]\n",
      "loss: 1.392986  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.2037e-04.\n",
      "\n",
      "Epoch: 44\n",
      "-----------------------------\n",
      "loss: 1.384158  [    0/ 3200]\n",
      "loss: 1.386671  [   16/ 3200]\n",
      "loss: 1.383209  [   32/ 3200]\n",
      "loss: 1.387733  [   48/ 3200]\n",
      "loss: 1.387665  [   64/ 3200]\n",
      "loss: 1.387602  [   80/ 3200]\n",
      "loss: 1.388156  [   96/ 3200]\n",
      "loss: 1.390074  [  112/ 3200]\n",
      "loss: 1.384535  [  128/ 3200]\n",
      "loss: 1.386896  [  144/ 3200]\n",
      "loss: 1.393467  [  160/ 3200]\n",
      "loss: 1.387778  [  176/ 3200]\n",
      "loss: 1.380699  [  192/ 3200]\n",
      "loss: 1.386275  [  208/ 3200]\n",
      "loss: 1.383831  [  224/ 3200]\n",
      "loss: 1.391791  [  240/ 3200]\n",
      "loss: 1.386785  [  256/ 3200]\n",
      "loss: 1.381248  [  272/ 3200]\n",
      "loss: 1.385834  [  288/ 3200]\n",
      "loss: 1.389452  [  304/ 3200]\n",
      "loss: 1.382442  [  320/ 3200]\n",
      "loss: 1.382997  [  336/ 3200]\n",
      "loss: 1.386058  [  352/ 3200]\n",
      "loss: 1.387224  [  368/ 3200]\n",
      "loss: 1.382882  [  384/ 3200]\n",
      "loss: 1.387665  [  400/ 3200]\n",
      "loss: 1.386941  [  416/ 3200]\n",
      "loss: 1.387666  [  432/ 3200]\n",
      "loss: 1.386500  [  448/ 3200]\n",
      "loss: 1.390333  [  464/ 3200]\n",
      "loss: 1.384714  [  480/ 3200]\n",
      "loss: 1.389518  [  496/ 3200]\n",
      "loss: 1.390400  [  512/ 3200]\n",
      "loss: 1.379598  [  528/ 3200]\n",
      "loss: 1.386323  [  544/ 3200]\n",
      "loss: 1.383718  [  560/ 3200]\n",
      "loss: 1.388107  [  576/ 3200]\n",
      "loss: 1.393724  [  592/ 3200]\n",
      "loss: 1.387618  [  608/ 3200]\n",
      "loss: 1.386781  [  624/ 3200]\n",
      "loss: 1.387665  [  640/ 3200]\n",
      "loss: 1.389073  [  656/ 3200]\n",
      "loss: 1.388105  [  672/ 3200]\n",
      "loss: 1.389120  [  688/ 3200]\n",
      "loss: 1.383329  [  704/ 3200]\n",
      "loss: 1.381476  [  720/ 3200]\n",
      "loss: 1.386896  [  736/ 3200]\n",
      "loss: 1.388941  [  752/ 3200]\n",
      "loss: 1.388613  [  768/ 3200]\n",
      "loss: 1.383327  [  784/ 3200]\n",
      "loss: 1.384899  [  800/ 3200]\n",
      "loss: 1.387842  [  816/ 3200]\n",
      "loss: 1.385551  [  832/ 3200]\n",
      "loss: 1.385043  [  848/ 3200]\n",
      "loss: 1.387551  [  864/ 3200]\n",
      "loss: 1.386500  [  880/ 3200]\n",
      "loss: 1.385946  [  896/ 3200]\n",
      "loss: 1.384208  [  912/ 3200]\n",
      "loss: 1.383326  [  928/ 3200]\n",
      "loss: 1.387843  [  944/ 3200]\n",
      "loss: 1.382817  [  960/ 3200]\n",
      "loss: 1.387157  [  976/ 3200]\n",
      "loss: 1.377023  [  992/ 3200]\n",
      "loss: 1.379755  [ 1008/ 3200]\n",
      "loss: 1.388542  [ 1024/ 3200]\n",
      "loss: 1.387405  [ 1040/ 3200]\n",
      "loss: 1.384670  [ 1056/ 3200]\n",
      "loss: 1.387335  [ 1072/ 3200]\n",
      "loss: 1.390727  [ 1088/ 3200]\n",
      "loss: 1.386715  [ 1104/ 3200]\n",
      "loss: 1.389336  [ 1120/ 3200]\n",
      "loss: 1.383586  [ 1136/ 3200]\n",
      "loss: 1.391828  [ 1152/ 3200]\n",
      "loss: 1.383654  [ 1168/ 3200]\n",
      "loss: 1.382593  [ 1184/ 3200]\n",
      "loss: 1.388330  [ 1200/ 3200]\n",
      "loss: 1.384713  [ 1216/ 3200]\n",
      "loss: 1.387151  [ 1232/ 3200]\n",
      "loss: 1.388610  [ 1248/ 3200]\n",
      "loss: 1.384601  [ 1264/ 3200]\n",
      "loss: 1.384275  [ 1280/ 3200]\n",
      "loss: 1.384163  [ 1296/ 3200]\n",
      "loss: 1.384274  [ 1312/ 3200]\n",
      "loss: 1.388498  [ 1328/ 3200]\n",
      "loss: 1.383726  [ 1344/ 3200]\n",
      "loss: 1.388724  [ 1360/ 3200]\n",
      "loss: 1.382265  [ 1376/ 3200]\n",
      "loss: 1.381430  [ 1392/ 3200]\n",
      "loss: 1.392183  [ 1408/ 3200]\n",
      "loss: 1.391671  [ 1424/ 3200]\n",
      "loss: 1.386571  [ 1440/ 3200]\n",
      "loss: 1.386825  [ 1456/ 3200]\n",
      "loss: 1.393017  [ 1472/ 3200]\n",
      "loss: 1.389407  [ 1488/ 3200]\n",
      "loss: 1.389956  [ 1504/ 3200]\n",
      "loss: 1.383104  [ 1520/ 3200]\n",
      "loss: 1.388893  [ 1536/ 3200]\n",
      "loss: 1.378445  [ 1552/ 3200]\n",
      "loss: 1.389953  [ 1568/ 3200]\n",
      "loss: 1.379946  [ 1584/ 3200]\n",
      "loss: 1.384999  [ 1600/ 3200]\n",
      "loss: 1.389490  [ 1616/ 3200]\n",
      "loss: 1.382707  [ 1632/ 3200]\n",
      "loss: 1.384744  [ 1648/ 3200]\n",
      "loss: 1.389445  [ 1664/ 3200]\n",
      "loss: 1.384672  [ 1680/ 3200]\n",
      "loss: 1.378814  [ 1696/ 3200]\n",
      "loss: 1.391897  [ 1712/ 3200]\n",
      "loss: 1.390394  [ 1728/ 3200]\n",
      "loss: 1.386499  [ 1744/ 3200]\n",
      "loss: 1.390325  [ 1760/ 3200]\n",
      "loss: 1.383768  [ 1776/ 3200]\n",
      "loss: 1.387661  [ 1792/ 3200]\n",
      "loss: 1.380598  [ 1808/ 3200]\n",
      "loss: 1.387335  [ 1824/ 3200]\n",
      "loss: 1.384928  [ 1840/ 3200]\n",
      "loss: 1.383329  [ 1856/ 3200]\n",
      "loss: 1.387774  [ 1872/ 3200]\n",
      "loss: 1.390831  [ 1888/ 3200]\n",
      "loss: 1.387222  [ 1904/ 3200]\n",
      "loss: 1.384675  [ 1920/ 3200]\n",
      "loss: 1.383726  [ 1936/ 3200]\n",
      "loss: 1.384166  [ 1952/ 3200]\n",
      "loss: 1.388171  [ 1968/ 3200]\n",
      "loss: 1.390943  [ 1984/ 3200]\n",
      "loss: 1.387223  [ 2000/ 3200]\n",
      "loss: 1.387111  [ 2016/ 3200]\n",
      "loss: 1.388281  [ 2032/ 3200]\n",
      "loss: 1.393488  [ 2048/ 3200]\n",
      "loss: 1.387151  [ 2064/ 3200]\n",
      "loss: 1.381987  [ 2080/ 3200]\n",
      "loss: 1.390787  [ 2096/ 3200]\n",
      "loss: 1.386785  [ 2112/ 3200]\n",
      "loss: 1.383003  [ 2128/ 3200]\n",
      "loss: 1.381111  [ 2144/ 3200]\n",
      "loss: 1.384785  [ 2160/ 3200]\n",
      "loss: 1.379946  [ 2176/ 3200]\n",
      "loss: 1.385947  [ 2192/ 3200]\n",
      "loss: 1.385440  [ 2208/ 3200]\n",
      "loss: 1.389552  [ 2224/ 3200]\n",
      "loss: 1.384673  [ 2240/ 3200]\n",
      "loss: 1.388211  [ 2256/ 3200]\n",
      "loss: 1.383769  [ 2272/ 3200]\n",
      "loss: 1.387332  [ 2288/ 3200]\n",
      "loss: 1.381547  [ 2304/ 3200]\n",
      "loss: 1.394352  [ 2320/ 3200]\n",
      "loss: 1.379876  [ 2336/ 3200]\n",
      "loss: 1.386319  [ 2352/ 3200]\n",
      "loss: 1.385880  [ 2368/ 3200]\n",
      "loss: 1.387441  [ 2384/ 3200]\n",
      "loss: 1.390350  [ 2400/ 3200]\n",
      "loss: 1.381001  [ 2416/ 3200]\n",
      "loss: 1.386934  [ 2432/ 3200]\n",
      "loss: 1.388645  [ 2448/ 3200]\n",
      "loss: 1.386386  [ 2464/ 3200]\n",
      "loss: 1.384097  [ 2480/ 3200]\n",
      "loss: 1.389006  [ 2496/ 3200]\n",
      "loss: 1.390350  [ 2512/ 3200]\n",
      "loss: 1.387730  [ 2528/ 3200]\n",
      "loss: 1.383291  [ 2544/ 3200]\n",
      "loss: 1.388100  [ 2560/ 3200]\n",
      "loss: 1.386387  [ 2576/ 3200]\n",
      "loss: 1.387070  [ 2592/ 3200]\n",
      "loss: 1.378280  [ 2608/ 3200]\n",
      "loss: 1.386344  [ 2624/ 3200]\n",
      "loss: 1.387113  [ 2640/ 3200]\n",
      "loss: 1.385991  [ 2656/ 3200]\n",
      "loss: 1.387727  [ 2672/ 3200]\n",
      "loss: 1.382277  [ 2688/ 3200]\n",
      "loss: 1.383002  [ 2704/ 3200]\n",
      "loss: 1.389816  [ 2720/ 3200]\n",
      "loss: 1.392101  [ 2736/ 3200]\n",
      "loss: 1.384168  [ 2752/ 3200]\n",
      "loss: 1.379660  [ 2768/ 3200]\n",
      "loss: 1.386320  [ 2784/ 3200]\n",
      "loss: 1.387442  [ 2800/ 3200]\n",
      "loss: 1.385483  [ 2816/ 3200]\n",
      "loss: 1.385441  [ 2832/ 3200]\n",
      "loss: 1.389223  [ 2848/ 3200]\n",
      "loss: 1.388129  [ 2864/ 3200]\n",
      "loss: 1.385619  [ 2880/ 3200]\n",
      "loss: 1.385401  [ 2896/ 3200]\n",
      "loss: 1.387331  [ 2912/ 3200]\n",
      "loss: 1.389002  [ 2928/ 3200]\n",
      "loss: 1.389043  [ 2944/ 3200]\n",
      "loss: 1.389416  [ 2960/ 3200]\n",
      "loss: 1.384607  [ 2976/ 3200]\n",
      "loss: 1.384498  [ 2992/ 3200]\n",
      "loss: 1.388235  [ 3008/ 3200]\n",
      "loss: 1.382456  [ 3024/ 3200]\n",
      "loss: 1.387112  [ 3040/ 3200]\n",
      "loss: 1.388385  [ 3056/ 3200]\n",
      "loss: 1.389878  [ 3072/ 3200]\n",
      "loss: 1.387795  [ 3088/ 3200]\n",
      "loss: 1.388165  [ 3104/ 3200]\n",
      "loss: 1.385554  [ 3120/ 3200]\n",
      "loss: 1.396047  [ 3136/ 3200]\n",
      "loss: 1.389062  [ 3152/ 3200]\n",
      "loss: 1.381951  [ 3168/ 3200]\n",
      "loss: 1.384720  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.0935e-04.\n",
      "\n",
      "Epoch: 45\n",
      "-----------------------------\n",
      "loss: 1.392992  [    0/ 3200]\n",
      "loss: 1.385441  [   16/ 3200]\n",
      "loss: 1.383778  [   32/ 3200]\n",
      "loss: 1.382392  [   48/ 3200]\n",
      "loss: 1.387220  [   64/ 3200]\n",
      "loss: 1.389593  [   80/ 3200]\n",
      "loss: 1.390381  [   96/ 3200]\n",
      "loss: 1.380569  [  112/ 3200]\n",
      "loss: 1.382720  [  128/ 3200]\n",
      "loss: 1.388098  [  144/ 3200]\n",
      "loss: 1.384432  [  160/ 3200]\n",
      "loss: 1.383157  [  176/ 3200]\n",
      "loss: 1.388210  [  192/ 3200]\n",
      "loss: 1.390819  [  208/ 3200]\n",
      "loss: 1.393293  [  224/ 3200]\n",
      "loss: 1.386938  [  240/ 3200]\n",
      "loss: 1.385440  [  256/ 3200]\n",
      "loss: 1.383775  [  272/ 3200]\n",
      "loss: 1.388165  [  288/ 3200]\n",
      "loss: 1.385441  [  304/ 3200]\n",
      "loss: 1.392666  [  320/ 3200]\n",
      "loss: 1.383226  [  336/ 3200]\n",
      "loss: 1.386174  [  352/ 3200]\n",
      "loss: 1.383707  [  368/ 3200]\n",
      "loss: 1.389394  [  384/ 3200]\n",
      "loss: 1.389546  [  400/ 3200]\n",
      "loss: 1.386059  [  416/ 3200]\n",
      "loss: 1.388093  [  432/ 3200]\n",
      "loss: 1.392524  [  448/ 3200]\n",
      "loss: 1.388489  [  464/ 3200]\n",
      "loss: 1.376300  [  480/ 3200]\n",
      "loss: 1.387332  [  496/ 3200]\n",
      "loss: 1.382395  [  512/ 3200]\n",
      "loss: 1.390663  [  528/ 3200]\n",
      "loss: 1.387838  [  544/ 3200]\n",
      "loss: 1.381126  [  560/ 3200]\n",
      "loss: 1.382971  [  576/ 3200]\n",
      "loss: 1.382505  [  592/ 3200]\n",
      "loss: 1.388601  [  608/ 3200]\n",
      "loss: 1.388011  [  624/ 3200]\n",
      "loss: 1.384170  [  640/ 3200]\n",
      "loss: 1.388669  [  656/ 3200]\n",
      "loss: 1.386892  [  672/ 3200]\n",
      "loss: 1.387590  [  688/ 3200]\n",
      "loss: 1.388424  [  704/ 3200]\n",
      "loss: 1.385838  [  720/ 3200]\n",
      "loss: 1.385947  [  736/ 3200]\n",
      "loss: 1.390816  [  752/ 3200]\n",
      "loss: 1.387218  [  768/ 3200]\n",
      "loss: 1.385767  [  784/ 3200]\n",
      "loss: 1.387654  [  800/ 3200]\n",
      "loss: 1.387217  [  816/ 3200]\n",
      "loss: 1.384104  [  832/ 3200]\n",
      "loss: 1.384285  [  848/ 3200]\n",
      "loss: 1.386316  [  864/ 3200]\n",
      "loss: 1.379237  [  880/ 3200]\n",
      "loss: 1.382674  [  896/ 3200]\n",
      "loss: 1.383342  [  912/ 3200]\n",
      "loss: 1.386571  [  928/ 3200]\n",
      "loss: 1.375245  [  944/ 3200]\n",
      "loss: 1.386779  [  960/ 3200]\n",
      "loss: 1.382508  [  976/ 3200]\n",
      "loss: 1.386500  [  992/ 3200]\n",
      "loss: 1.389433  [ 1008/ 3200]\n",
      "loss: 1.387104  [ 1024/ 3200]\n",
      "loss: 1.384609  [ 1040/ 3200]\n",
      "loss: 1.388162  [ 1056/ 3200]\n",
      "loss: 1.390424  [ 1072/ 3200]\n",
      "loss: 1.384355  [ 1088/ 3200]\n",
      "loss: 1.381451  [ 1104/ 3200]\n",
      "loss: 1.384496  [ 1120/ 3200]\n",
      "loss: 1.390309  [ 1136/ 3200]\n",
      "loss: 1.378195  [ 1152/ 3200]\n",
      "loss: 1.386710  [ 1168/ 3200]\n",
      "loss: 1.382902  [ 1184/ 3200]\n",
      "loss: 1.385370  [ 1200/ 3200]\n",
      "loss: 1.388925  [ 1216/ 3200]\n",
      "loss: 1.384723  [ 1232/ 3200]\n",
      "loss: 1.387290  [ 1248/ 3200]\n",
      "loss: 1.382508  [ 1264/ 3200]\n",
      "loss: 1.383225  [ 1280/ 3200]\n",
      "loss: 1.387541  [ 1296/ 3200]\n",
      "loss: 1.390193  [ 1312/ 3200]\n",
      "loss: 1.383019  [ 1328/ 3200]\n",
      "loss: 1.385949  [ 1344/ 3200]\n",
      "loss: 1.385554  [ 1360/ 3200]\n",
      "loss: 1.386200  [ 1376/ 3200]\n",
      "loss: 1.387011  [ 1392/ 3200]\n",
      "loss: 1.380066  [ 1408/ 3200]\n",
      "loss: 1.384171  [ 1424/ 3200]\n",
      "loss: 1.386430  [ 1440/ 3200]\n",
      "loss: 1.386342  [ 1456/ 3200]\n",
      "loss: 1.385993  [ 1472/ 3200]\n",
      "loss: 1.385510  [ 1488/ 3200]\n",
      "loss: 1.388923  [ 1504/ 3200]\n",
      "loss: 1.388602  [ 1520/ 3200]\n",
      "loss: 1.386709  [ 1536/ 3200]\n",
      "loss: 1.386895  [ 1552/ 3200]\n",
      "loss: 1.388922  [ 1568/ 3200]\n",
      "loss: 1.388529  [ 1584/ 3200]\n",
      "loss: 1.382351  [ 1600/ 3200]\n",
      "loss: 1.389430  [ 1616/ 3200]\n",
      "loss: 1.383022  [ 1632/ 3200]\n",
      "loss: 1.386137  [ 1648/ 3200]\n",
      "loss: 1.385555  [ 1664/ 3200]\n",
      "loss: 1.391279  [ 1680/ 3200]\n",
      "loss: 1.383019  [ 1696/ 3200]\n",
      "loss: 1.383848  [ 1712/ 3200]\n",
      "loss: 1.386501  [ 1728/ 3200]\n",
      "loss: 1.391576  [ 1744/ 3200]\n",
      "loss: 1.386824  [ 1760/ 3200]\n",
      "loss: 1.383272  [ 1776/ 3200]\n",
      "loss: 1.381129  [ 1792/ 3200]\n",
      "loss: 1.391532  [ 1808/ 3200]\n",
      "loss: 1.387725  [ 1824/ 3200]\n",
      "loss: 1.387655  [ 1840/ 3200]\n",
      "loss: 1.381523  [ 1856/ 3200]\n",
      "loss: 1.385347  [ 1872/ 3200]\n",
      "loss: 1.383849  [ 1888/ 3200]\n",
      "loss: 1.388486  [ 1904/ 3200]\n",
      "loss: 1.385048  [ 1920/ 3200]\n",
      "loss: 1.391276  [ 1936/ 3200]\n",
      "loss: 1.385993  [ 1952/ 3200]\n",
      "loss: 1.390373  [ 1968/ 3200]\n",
      "loss: 1.383962  [ 1984/ 3200]\n",
      "loss: 1.386936  [ 2000/ 3200]\n",
      "loss: 1.390486  [ 2016/ 3200]\n",
      "loss: 1.384679  [ 2032/ 3200]\n",
      "loss: 1.391094  [ 2048/ 3200]\n",
      "loss: 1.378806  [ 2064/ 3200]\n",
      "loss: 1.389609  [ 2080/ 3200]\n",
      "loss: 1.385183  [ 2096/ 3200]\n",
      "loss: 1.376201  [ 2112/ 3200]\n",
      "loss: 1.382352  [ 2128/ 3200]\n",
      "loss: 1.386385  [ 2144/ 3200]\n",
      "loss: 1.384173  [ 2160/ 3200]\n",
      "loss: 1.391252  [ 2176/ 3200]\n",
      "loss: 1.385442  [ 2192/ 3200]\n",
      "loss: 1.391364  [ 2208/ 3200]\n",
      "loss: 1.389430  [ 2224/ 3200]\n",
      "loss: 1.383343  [ 2240/ 3200]\n",
      "loss: 1.388205  [ 2256/ 3200]\n",
      "loss: 1.385006  [ 2272/ 3200]\n",
      "loss: 1.392977  [ 2288/ 3200]\n",
      "loss: 1.391202  [ 2304/ 3200]\n",
      "loss: 1.388092  [ 2320/ 3200]\n",
      "loss: 1.386432  [ 2336/ 3200]\n",
      "loss: 1.385949  [ 2352/ 3200]\n",
      "loss: 1.388226  [ 2368/ 3200]\n",
      "loss: 1.385051  [ 2384/ 3200]\n",
      "loss: 1.387487  [ 2400/ 3200]\n",
      "loss: 1.383017  [ 2416/ 3200]\n",
      "loss: 1.390303  [ 2432/ 3200]\n",
      "loss: 1.378417  [ 2448/ 3200]\n",
      "loss: 1.387327  [ 2464/ 3200]\n",
      "loss: 1.384502  [ 2480/ 3200]\n",
      "loss: 1.390260  [ 2496/ 3200]\n",
      "loss: 1.391201  [ 2512/ 3200]\n",
      "loss: 1.385118  [ 2528/ 3200]\n",
      "loss: 1.393018  [ 2544/ 3200]\n",
      "loss: 1.388706  [ 2560/ 3200]\n",
      "loss: 1.387721  [ 2576/ 3200]\n",
      "loss: 1.390584  [ 2592/ 3200]\n",
      "loss: 1.388417  [ 2608/ 3200]\n",
      "loss: 1.384793  [ 2624/ 3200]\n",
      "loss: 1.381026  [ 2640/ 3200]\n",
      "loss: 1.383301  [ 2656/ 3200]\n",
      "loss: 1.388270  [ 2672/ 3200]\n",
      "loss: 1.391701  [ 2688/ 3200]\n",
      "loss: 1.386823  [ 2704/ 3200]\n",
      "loss: 1.386384  [ 2720/ 3200]\n",
      "loss: 1.386712  [ 2736/ 3200]\n",
      "loss: 1.382078  [ 2752/ 3200]\n",
      "loss: 1.387392  [ 2768/ 3200]\n",
      "loss: 1.383413  [ 2784/ 3200]\n",
      "loss: 1.387326  [ 2800/ 3200]\n",
      "loss: 1.386842  [ 2816/ 3200]\n",
      "loss: 1.379757  [ 2832/ 3200]\n",
      "loss: 1.390477  [ 2848/ 3200]\n",
      "loss: 1.383346  [ 2864/ 3200]\n",
      "loss: 1.392526  [ 2880/ 3200]\n",
      "loss: 1.386715  [ 2896/ 3200]\n",
      "loss: 1.393844  [ 2912/ 3200]\n",
      "loss: 1.390851  [ 2928/ 3200]\n",
      "loss: 1.388093  [ 2944/ 3200]\n",
      "loss: 1.389580  [ 2960/ 3200]\n",
      "loss: 1.388877  [ 2976/ 3200]\n",
      "loss: 1.387326  [ 2992/ 3200]\n",
      "loss: 1.385182  [ 3008/ 3200]\n",
      "loss: 1.385447  [ 3024/ 3200]\n",
      "loss: 1.387215  [ 3040/ 3200]\n",
      "loss: 1.386322  [ 3056/ 3200]\n",
      "loss: 1.387763  [ 3072/ 3200]\n",
      "loss: 1.385884  [ 3088/ 3200]\n",
      "loss: 1.382911  [ 3104/ 3200]\n",
      "loss: 1.381970  [ 3120/ 3200]\n",
      "loss: 1.388045  [ 3136/ 3200]\n",
      "loss: 1.385948  [ 3152/ 3200]\n",
      "loss: 1.386497  [ 3168/ 3200]\n",
      "loss: 1.388920  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.9888e-04.\n",
      "\n",
      "Epoch: 46\n",
      "-----------------------------\n",
      "loss: 1.382192  [    0/ 3200]\n",
      "loss: 1.389140  [   16/ 3200]\n",
      "loss: 1.386713  [   32/ 3200]\n",
      "loss: 1.389923  [   48/ 3200]\n",
      "loss: 1.384680  [   64/ 3200]\n",
      "loss: 1.389031  [   80/ 3200]\n",
      "loss: 1.388092  [   96/ 3200]\n",
      "loss: 1.383960  [  112/ 3200]\n",
      "loss: 1.388656  [  128/ 3200]\n",
      "loss: 1.386056  [  144/ 3200]\n",
      "loss: 1.385618  [  160/ 3200]\n",
      "loss: 1.382849  [  176/ 3200]\n",
      "loss: 1.384287  [  192/ 3200]\n",
      "loss: 1.392852  [  208/ 3200]\n",
      "loss: 1.381643  [  224/ 3200]\n",
      "loss: 1.389814  [  240/ 3200]\n",
      "loss: 1.382850  [  256/ 3200]\n",
      "loss: 1.386934  [  272/ 3200]\n",
      "loss: 1.389907  [  288/ 3200]\n",
      "loss: 1.386054  [  304/ 3200]\n",
      "loss: 1.380599  [  320/ 3200]\n",
      "loss: 1.384509  [  336/ 3200]\n",
      "loss: 1.389861  [  352/ 3200]\n",
      "loss: 1.388545  [  368/ 3200]\n",
      "loss: 1.389858  [  384/ 3200]\n",
      "loss: 1.393048  [  400/ 3200]\n",
      "loss: 1.385010  [  416/ 3200]\n",
      "loss: 1.385557  [  432/ 3200]\n",
      "loss: 1.383680  [  448/ 3200]\n",
      "loss: 1.388965  [  464/ 3200]\n",
      "loss: 1.390314  [  480/ 3200]\n",
      "loss: 1.386603  [  496/ 3200]\n",
      "loss: 1.384574  [  512/ 3200]\n",
      "loss: 1.386607  [  528/ 3200]\n",
      "loss: 1.383787  [  544/ 3200]\n",
      "loss: 1.388526  [  560/ 3200]\n",
      "loss: 1.380210  [  576/ 3200]\n",
      "loss: 1.382520  [  592/ 3200]\n",
      "loss: 1.389920  [  608/ 3200]\n",
      "loss: 1.390794  [  624/ 3200]\n",
      "loss: 1.385335  [  640/ 3200]\n",
      "loss: 1.384618  [  656/ 3200]\n",
      "loss: 1.388110  [  672/ 3200]\n",
      "loss: 1.386557  [  688/ 3200]\n",
      "loss: 1.383576  [  704/ 3200]\n",
      "loss: 1.385993  [  720/ 3200]\n",
      "loss: 1.386449  [  736/ 3200]\n",
      "loss: 1.382477  [  752/ 3200]\n",
      "loss: 1.389044  [  768/ 3200]\n",
      "loss: 1.384572  [  784/ 3200]\n",
      "loss: 1.377946  [  800/ 3200]\n",
      "loss: 1.386884  [  816/ 3200]\n",
      "loss: 1.385886  [  832/ 3200]\n",
      "loss: 1.387154  [  848/ 3200]\n",
      "loss: 1.386823  [  864/ 3200]\n",
      "loss: 1.374971  [  880/ 3200]\n",
      "loss: 1.388045  [  896/ 3200]\n",
      "loss: 1.387215  [  912/ 3200]\n",
      "loss: 1.386761  [  928/ 3200]\n",
      "loss: 1.392016  [  944/ 3200]\n",
      "loss: 1.380647  [  960/ 3200]\n",
      "loss: 1.386933  [  976/ 3200]\n",
      "loss: 1.385993  [  992/ 3200]\n",
      "loss: 1.386823  [ 1008/ 3200]\n",
      "loss: 1.383288  [ 1024/ 3200]\n",
      "loss: 1.386277  [ 1040/ 3200]\n",
      "loss: 1.386841  [ 1056/ 3200]\n",
      "loss: 1.376238  [ 1072/ 3200]\n",
      "loss: 1.385993  [ 1088/ 3200]\n",
      "loss: 1.389858  [ 1104/ 3200]\n",
      "loss: 1.388526  [ 1120/ 3200]\n",
      "loss: 1.388589  [ 1136/ 3200]\n",
      "loss: 1.385927  [ 1152/ 3200]\n",
      "loss: 1.387542  [ 1168/ 3200]\n",
      "loss: 1.379665  [ 1184/ 3200]\n",
      "loss: 1.388698  [ 1200/ 3200]\n",
      "loss: 1.391470  [ 1216/ 3200]\n",
      "loss: 1.389309  [ 1232/ 3200]\n",
      "loss: 1.381978  [ 1248/ 3200]\n",
      "loss: 1.386253  [ 1264/ 3200]\n",
      "loss: 1.389305  [ 1280/ 3200]\n",
      "loss: 1.382090  [ 1296/ 3200]\n",
      "loss: 1.391845  [ 1312/ 3200]\n",
      "loss: 1.390245  [ 1328/ 3200]\n",
      "loss: 1.386779  [ 1344/ 3200]\n",
      "loss: 1.392560  [ 1360/ 3200]\n",
      "loss: 1.393823  [ 1376/ 3200]\n",
      "loss: 1.382025  [ 1392/ 3200]\n",
      "loss: 1.388041  [ 1408/ 3200]\n",
      "loss: 1.385234  [ 1424/ 3200]\n",
      "loss: 1.382853  [ 1440/ 3200]\n",
      "loss: 1.382918  [ 1456/ 3200]\n",
      "loss: 1.389418  [ 1472/ 3200]\n",
      "loss: 1.386777  [ 1488/ 3200]\n",
      "loss: 1.384620  [ 1504/ 3200]\n",
      "loss: 1.384729  [ 1520/ 3200]\n",
      "loss: 1.386496  [ 1536/ 3200]\n",
      "loss: 1.388152  [ 1552/ 3200]\n",
      "loss: 1.385077  [ 1568/ 3200]\n",
      "loss: 1.387694  [ 1584/ 3200]\n",
      "loss: 1.387213  [ 1600/ 3200]\n",
      "loss: 1.385010  [ 1616/ 3200]\n",
      "loss: 1.390352  [ 1632/ 3200]\n",
      "loss: 1.389303  [ 1648/ 3200]\n",
      "loss: 1.386496  [ 1664/ 3200]\n",
      "loss: 1.389918  [ 1680/ 3200]\n",
      "loss: 1.391158  [ 1696/ 3200]\n",
      "loss: 1.385558  [ 1712/ 3200]\n",
      "loss: 1.385013  [ 1728/ 3200]\n",
      "loss: 1.383246  [ 1744/ 3200]\n",
      "loss: 1.383571  [ 1760/ 3200]\n",
      "loss: 1.378842  [ 1776/ 3200]\n",
      "loss: 1.382642  [ 1792/ 3200]\n",
      "loss: 1.386274  [ 1808/ 3200]\n",
      "loss: 1.388408  [ 1824/ 3200]\n",
      "loss: 1.386822  [ 1840/ 3200]\n",
      "loss: 1.385191  [ 1856/ 3200]\n",
      "loss: 1.388697  [ 1872/ 3200]\n",
      "loss: 1.391682  [ 1888/ 3200]\n",
      "loss: 1.384183  [ 1904/ 3200]\n",
      "loss: 1.394755  [ 1920/ 3200]\n",
      "loss: 1.386819  [ 1936/ 3200]\n",
      "loss: 1.383358  [ 1952/ 3200]\n",
      "loss: 1.388934  [ 1968/ 3200]\n",
      "loss: 1.386451  [ 1984/ 3200]\n",
      "loss: 1.387647  [ 2000/ 3200]\n",
      "loss: 1.380331  [ 2016/ 3200]\n",
      "loss: 1.385077  [ 2032/ 3200]\n",
      "loss: 1.386384  [ 2048/ 3200]\n",
      "loss: 1.385670  [ 2064/ 3200]\n",
      "loss: 1.386560  [ 2080/ 3200]\n",
      "loss: 1.382920  [ 2096/ 3200]\n",
      "loss: 1.385948  [ 2112/ 3200]\n",
      "loss: 1.384121  [ 2128/ 3200]\n",
      "loss: 1.387258  [ 2144/ 3200]\n",
      "loss: 1.380044  [ 2160/ 3200]\n",
      "loss: 1.388586  [ 2176/ 3200]\n",
      "loss: 1.383246  [ 2192/ 3200]\n",
      "loss: 1.390787  [ 2208/ 3200]\n",
      "loss: 1.387544  [ 2224/ 3200]\n",
      "loss: 1.393423  [ 2240/ 3200]\n",
      "loss: 1.388214  [ 2256/ 3200]\n",
      "loss: 1.391066  [ 2272/ 3200]\n",
      "loss: 1.390458  [ 2288/ 3200]\n",
      "loss: 1.381596  [ 2304/ 3200]\n",
      "loss: 1.382967  [ 2320/ 3200]\n",
      "loss: 1.387648  [ 2336/ 3200]\n",
      "loss: 1.387647  [ 2352/ 3200]\n",
      "loss: 1.385949  [ 2368/ 3200]\n",
      "loss: 1.390850  [ 2384/ 3200]\n",
      "loss: 1.387384  [ 2400/ 3200]\n",
      "loss: 1.386494  [ 2416/ 3200]\n",
      "loss: 1.393436  [ 2432/ 3200]\n",
      "loss: 1.379227  [ 2448/ 3200]\n",
      "loss: 1.386819  [ 2464/ 3200]\n",
      "loss: 1.386776  [ 2480/ 3200]\n",
      "loss: 1.382359  [ 2496/ 3200]\n",
      "loss: 1.388693  [ 2512/ 3200]\n",
      "loss: 1.385993  [ 2528/ 3200]\n",
      "loss: 1.382380  [ 2544/ 3200]\n",
      "loss: 1.387321  [ 2560/ 3200]\n",
      "loss: 1.380180  [ 2576/ 3200]\n",
      "loss: 1.384186  [ 2592/ 3200]\n",
      "loss: 1.388522  [ 2608/ 3200]\n",
      "loss: 1.386776  [ 2624/ 3200]\n",
      "loss: 1.383469  [ 2640/ 3200]\n",
      "loss: 1.391110  [ 2656/ 3200]\n",
      "loss: 1.380225  [ 2672/ 3200]\n",
      "loss: 1.381833  [ 2688/ 3200]\n",
      "loss: 1.383358  [ 2704/ 3200]\n",
      "loss: 1.390411  [ 2720/ 3200]\n",
      "loss: 1.384950  [ 2736/ 3200]\n",
      "loss: 1.382811  [ 2752/ 3200]\n",
      "loss: 1.389912  [ 2768/ 3200]\n",
      "loss: 1.385995  [ 2784/ 3200]\n",
      "loss: 1.385996  [ 2800/ 3200]\n",
      "loss: 1.388695  [ 2816/ 3200]\n",
      "loss: 1.387648  [ 2832/ 3200]\n",
      "loss: 1.386057  [ 2848/ 3200]\n",
      "loss: 1.383857  [ 2864/ 3200]\n",
      "loss: 1.388912  [ 2880/ 3200]\n",
      "loss: 1.385121  [ 2896/ 3200]\n",
      "loss: 1.387257  [ 2912/ 3200]\n",
      "loss: 1.388520  [ 2928/ 3200]\n",
      "loss: 1.381270  [ 2944/ 3200]\n",
      "loss: 1.385993  [ 2960/ 3200]\n",
      "loss: 1.390174  [ 2976/ 3200]\n",
      "loss: 1.392870  [ 2992/ 3200]\n",
      "loss: 1.383753  [ 3008/ 3200]\n",
      "loss: 1.391672  [ 3024/ 3200]\n",
      "loss: 1.385230  [ 3040/ 3200]\n",
      "loss: 1.387711  [ 3056/ 3200]\n",
      "loss: 1.385885  [ 3072/ 3200]\n",
      "loss: 1.385777  [ 3088/ 3200]\n",
      "loss: 1.387755  [ 3104/ 3200]\n",
      "loss: 1.387276  [ 3120/ 3200]\n",
      "loss: 1.386884  [ 3136/ 3200]\n",
      "loss: 1.387319  [ 3152/ 3200]\n",
      "loss: 1.384079  [ 3168/ 3200]\n",
      "loss: 1.386230  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.8894e-04.\n",
      "\n",
      "Epoch: 47\n",
      "-----------------------------\n",
      "loss: 1.381711  [    0/ 3200]\n",
      "loss: 1.388692  [   16/ 3200]\n",
      "loss: 1.389734  [   32/ 3200]\n",
      "loss: 1.392040  [   48/ 3200]\n",
      "loss: 1.385060  [   64/ 3200]\n",
      "loss: 1.384843  [   80/ 3200]\n",
      "loss: 1.383362  [   96/ 3200]\n",
      "loss: 1.384622  [  112/ 3200]\n",
      "loss: 1.385057  [  128/ 3200]\n",
      "loss: 1.380557  [  144/ 3200]\n",
      "loss: 1.385514  [  160/ 3200]\n",
      "loss: 1.391061  [  176/ 3200]\n",
      "loss: 1.387712  [  192/ 3200]\n",
      "loss: 1.381385  [  208/ 3200]\n",
      "loss: 1.390279  [  224/ 3200]\n",
      "loss: 1.386493  [  240/ 3200]\n",
      "loss: 1.381599  [  256/ 3200]\n",
      "loss: 1.386840  [  272/ 3200]\n",
      "loss: 1.393695  [  288/ 3200]\n",
      "loss: 1.387711  [  304/ 3200]\n",
      "loss: 1.386993  [  320/ 3200]\n",
      "loss: 1.381556  [  336/ 3200]\n",
      "loss: 1.383362  [  352/ 3200]\n",
      "loss: 1.378970  [  368/ 3200]\n",
      "loss: 1.384295  [  384/ 3200]\n",
      "loss: 1.382757  [  400/ 3200]\n",
      "loss: 1.382925  [  416/ 3200]\n",
      "loss: 1.383315  [  432/ 3200]\n",
      "loss: 1.387554  [  448/ 3200]\n",
      "loss: 1.391168  [  464/ 3200]\n",
      "loss: 1.386774  [  480/ 3200]\n",
      "loss: 1.388035  [  496/ 3200]\n",
      "loss: 1.384685  [  512/ 3200]\n",
      "loss: 1.378034  [  528/ 3200]\n",
      "loss: 1.384686  [  544/ 3200]\n",
      "loss: 1.382864  [  560/ 3200]\n",
      "loss: 1.386762  [  576/ 3200]\n",
      "loss: 1.388474  [  592/ 3200]\n",
      "loss: 1.392539  [  608/ 3200]\n",
      "loss: 1.391603  [  624/ 3200]\n",
      "loss: 1.386384  [  640/ 3200]\n",
      "loss: 1.385947  [  656/ 3200]\n",
      "loss: 1.381820  [  672/ 3200]\n",
      "loss: 1.384078  [  688/ 3200]\n",
      "loss: 1.385061  [  704/ 3200]\n",
      "loss: 1.382692  [  720/ 3200]\n",
      "loss: 1.384124  [  736/ 3200]\n",
      "loss: 1.382365  [  752/ 3200]\n",
      "loss: 1.386274  [  768/ 3200]\n",
      "loss: 1.381101  [  784/ 3200]\n",
      "loss: 1.389517  [  800/ 3200]\n",
      "loss: 1.388859  [  816/ 3200]\n",
      "loss: 1.382750  [  832/ 3200]\n",
      "loss: 1.386498  [  848/ 3200]\n",
      "loss: 1.386384  [  864/ 3200]\n",
      "loss: 1.391168  [  880/ 3200]\n",
      "loss: 1.382540  [  896/ 3200]\n",
      "loss: 1.384737  [  912/ 3200]\n",
      "loss: 1.378581  [  928/ 3200]\n",
      "loss: 1.389407  [  944/ 3200]\n",
      "loss: 1.391602  [  960/ 3200]\n",
      "loss: 1.383575  [  976/ 3200]\n",
      "loss: 1.388033  [  992/ 3200]\n",
      "loss: 1.386320  [ 1008/ 3200]\n",
      "loss: 1.386319  [ 1024/ 3200]\n",
      "loss: 1.380551  [ 1040/ 3200]\n",
      "loss: 1.392540  [ 1056/ 3200]\n",
      "loss: 1.388079  [ 1072/ 3200]\n",
      "loss: 1.386432  [ 1088/ 3200]\n",
      "loss: 1.382494  [ 1104/ 3200]\n",
      "loss: 1.387322  [ 1120/ 3200]\n",
      "loss: 1.390776  [ 1136/ 3200]\n",
      "loss: 1.386384  [ 1152/ 3200]\n",
      "loss: 1.386819  [ 1168/ 3200]\n",
      "loss: 1.388904  [ 1184/ 3200]\n",
      "loss: 1.383477  [ 1200/ 3200]\n",
      "loss: 1.385950  [ 1216/ 3200]\n",
      "loss: 1.380733  [ 1232/ 3200]\n",
      "loss: 1.385059  [ 1248/ 3200]\n",
      "loss: 1.383298  [ 1264/ 3200]\n",
      "loss: 1.384189  [ 1280/ 3200]\n",
      "loss: 1.388147  [ 1296/ 3200]\n",
      "loss: 1.390230  [ 1312/ 3200]\n",
      "loss: 1.388468  [ 1328/ 3200]\n",
      "loss: 1.389908  [ 1344/ 3200]\n",
      "loss: 1.384623  [ 1360/ 3200]\n",
      "loss: 1.382542  [ 1376/ 3200]\n",
      "loss: 1.387322  [ 1392/ 3200]\n",
      "loss: 1.387710  [ 1408/ 3200]\n",
      "loss: 1.388582  [ 1424/ 3200]\n",
      "loss: 1.389405  [ 1440/ 3200]\n",
      "loss: 1.387367  [ 1456/ 3200]\n",
      "loss: 1.386954  [ 1472/ 3200]\n",
      "loss: 1.382318  [ 1488/ 3200]\n",
      "loss: 1.382285  [ 1504/ 3200]\n",
      "loss: 1.390342  [ 1520/ 3200]\n",
      "loss: 1.387097  [ 1536/ 3200]\n",
      "loss: 1.386337  [ 1552/ 3200]\n",
      "loss: 1.383688  [ 1568/ 3200]\n",
      "loss: 1.385741  [ 1584/ 3200]\n",
      "loss: 1.384189  [ 1600/ 3200]\n",
      "loss: 1.385383  [ 1616/ 3200]\n",
      "loss: 1.390887  [ 1632/ 3200]\n",
      "loss: 1.385013  [ 1648/ 3200]\n",
      "loss: 1.389727  [ 1664/ 3200]\n",
      "loss: 1.388969  [ 1680/ 3200]\n",
      "loss: 1.384304  [ 1696/ 3200]\n",
      "loss: 1.386642  [ 1712/ 3200]\n",
      "loss: 1.390820  [ 1728/ 3200]\n",
      "loss: 1.386707  [ 1744/ 3200]\n",
      "loss: 1.386453  [ 1760/ 3200]\n",
      "loss: 1.386998  [ 1776/ 3200]\n",
      "loss: 1.382318  [ 1792/ 3200]\n",
      "loss: 1.390339  [ 1808/ 3200]\n",
      "loss: 1.387822  [ 1824/ 3200]\n",
      "loss: 1.385996  [ 1840/ 3200]\n",
      "loss: 1.386384  [ 1856/ 3200]\n",
      "loss: 1.387597  [ 1872/ 3200]\n",
      "loss: 1.392031  [ 1888/ 3200]\n",
      "loss: 1.388144  [ 1904/ 3200]\n",
      "loss: 1.390448  [ 1920/ 3200]\n",
      "loss: 1.390270  [ 1936/ 3200]\n",
      "loss: 1.388255  [ 1952/ 3200]\n",
      "loss: 1.392850  [ 1968/ 3200]\n",
      "loss: 1.383757  [ 1984/ 3200]\n",
      "loss: 1.392414  [ 2000/ 3200]\n",
      "loss: 1.389077  [ 2016/ 3200]\n",
      "loss: 1.385063  [ 2032/ 3200]\n",
      "loss: 1.388419  [ 2048/ 3200]\n",
      "loss: 1.388319  [ 2064/ 3200]\n",
      "loss: 1.386273  [ 2080/ 3200]\n",
      "loss: 1.392458  [ 2096/ 3200]\n",
      "loss: 1.386497  [ 2112/ 3200]\n",
      "loss: 1.387141  [ 2128/ 3200]\n",
      "loss: 1.385560  [ 2144/ 3200]\n",
      "loss: 1.387320  [ 2160/ 3200]\n",
      "loss: 1.384258  [ 2176/ 3200]\n",
      "loss: 1.383758  [ 2192/ 3200]\n",
      "loss: 1.388511  [ 2208/ 3200]\n",
      "loss: 1.380036  [ 2224/ 3200]\n",
      "loss: 1.385561  [ 2240/ 3200]\n",
      "loss: 1.387753  [ 2256/ 3200]\n",
      "loss: 1.389352  [ 2272/ 3200]\n",
      "loss: 1.389831  [ 2288/ 3200]\n",
      "loss: 1.385062  [ 2304/ 3200]\n",
      "loss: 1.383983  [ 2320/ 3200]\n",
      "loss: 1.385172  [ 2336/ 3200]\n",
      "loss: 1.387317  [ 2352/ 3200]\n",
      "loss: 1.378989  [ 2368/ 3200]\n",
      "loss: 1.390052  [ 2384/ 3200]\n",
      "loss: 1.387142  [ 2400/ 3200]\n",
      "loss: 1.385884  [ 2416/ 3200]\n",
      "loss: 1.383804  [ 2432/ 3200]\n",
      "loss: 1.384626  [ 2448/ 3200]\n",
      "loss: 1.388098  [ 2464/ 3200]\n",
      "loss: 1.381570  [ 2480/ 3200]\n",
      "loss: 1.387429  [ 2496/ 3200]\n",
      "loss: 1.390224  [ 2512/ 3200]\n",
      "loss: 1.385452  [ 2528/ 3200]\n",
      "loss: 1.388573  [ 2544/ 3200]\n",
      "loss: 1.381616  [ 2560/ 3200]\n",
      "loss: 1.384305  [ 2576/ 3200]\n",
      "loss: 1.392197  [ 2592/ 3200]\n",
      "loss: 1.391543  [ 2608/ 3200]\n",
      "loss: 1.388898  [ 2624/ 3200]\n",
      "loss: 1.385562  [ 2640/ 3200]\n",
      "loss: 1.389830  [ 2656/ 3200]\n",
      "loss: 1.387209  [ 2672/ 3200]\n",
      "loss: 1.386883  [ 2688/ 3200]\n",
      "loss: 1.385625  [ 2704/ 3200]\n",
      "loss: 1.385887  [ 2720/ 3200]\n",
      "loss: 1.384629  [ 2736/ 3200]\n",
      "loss: 1.393730  [ 2752/ 3200]\n",
      "loss: 1.386668  [ 2768/ 3200]\n",
      "loss: 1.386384  [ 2784/ 3200]\n",
      "loss: 1.388681  [ 2800/ 3200]\n",
      "loss: 1.389938  [ 2816/ 3200]\n",
      "loss: 1.385453  [ 2832/ 3200]\n",
      "loss: 1.389785  [ 2848/ 3200]\n",
      "loss: 1.390760  [ 2864/ 3200]\n",
      "loss: 1.383373  [ 2880/ 3200]\n",
      "loss: 1.384196  [ 2896/ 3200]\n",
      "loss: 1.381511  [ 2912/ 3200]\n",
      "loss: 1.382614  [ 2928/ 3200]\n",
      "loss: 1.394159  [ 2944/ 3200]\n",
      "loss: 1.383761  [ 2960/ 3200]\n",
      "loss: 1.383808  [ 2976/ 3200]\n",
      "loss: 1.384736  [ 2992/ 3200]\n",
      "loss: 1.375382  [ 3008/ 3200]\n",
      "loss: 1.385516  [ 3024/ 3200]\n",
      "loss: 1.387254  [ 3040/ 3200]\n",
      "loss: 1.391755  [ 3056/ 3200]\n",
      "loss: 1.390325  [ 3072/ 3200]\n",
      "loss: 1.390174  [ 3088/ 3200]\n",
      "loss: 1.384958  [ 3104/ 3200]\n",
      "loss: 1.381621  [ 3120/ 3200]\n",
      "loss: 1.385949  [ 3136/ 3200]\n",
      "loss: 1.381899  [ 3152/ 3200]\n",
      "loss: 1.384630  [ 3168/ 3200]\n",
      "loss: 1.386881  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.7949e-04.\n",
      "\n",
      "Epoch: 48\n",
      "-----------------------------\n",
      "loss: 1.391148  [    0/ 3200]\n",
      "loss: 1.385066  [   16/ 3200]\n",
      "loss: 1.384629  [   32/ 3200]\n",
      "loss: 1.382008  [   48/ 3200]\n",
      "loss: 1.386384  [   64/ 3200]\n",
      "loss: 1.386385  [   80/ 3200]\n",
      "loss: 1.385562  [   96/ 3200]\n",
      "loss: 1.381682  [  112/ 3200]\n",
      "loss: 1.385623  [  128/ 3200]\n",
      "loss: 1.382551  [  144/ 3200]\n",
      "loss: 1.385623  [  160/ 3200]\n",
      "loss: 1.380035  [  176/ 3200]\n",
      "loss: 1.383264  [  192/ 3200]\n",
      "loss: 1.383760  [  208/ 3200]\n",
      "loss: 1.392515  [  224/ 3200]\n",
      "loss: 1.389008  [  240/ 3200]\n",
      "loss: 1.387146  [  256/ 3200]\n",
      "loss: 1.392948  [  272/ 3200]\n",
      "loss: 1.386988  [  288/ 3200]\n",
      "loss: 1.386384  [  304/ 3200]\n",
      "loss: 1.385065  [  320/ 3200]\n",
      "loss: 1.386820  [  336/ 3200]\n",
      "loss: 1.385172  [  352/ 3200]\n",
      "loss: 1.388854  [  368/ 3200]\n",
      "loss: 1.389441  [  384/ 3200]\n",
      "loss: 1.387421  [  400/ 3200]\n",
      "loss: 1.388960  [  416/ 3200]\n",
      "loss: 1.384029  [  432/ 3200]\n",
      "loss: 1.385561  [  448/ 3200]\n",
      "loss: 1.385234  [  464/ 3200]\n",
      "loss: 1.386277  [  480/ 3200]\n",
      "loss: 1.388138  [  496/ 3200]\n",
      "loss: 1.387702  [  512/ 3200]\n",
      "loss: 1.392403  [  528/ 3200]\n",
      "loss: 1.385996  [  544/ 3200]\n",
      "loss: 1.387917  [  560/ 3200]\n",
      "loss: 1.382011  [  576/ 3200]\n",
      "loss: 1.387640  [  592/ 3200]\n",
      "loss: 1.384030  [  608/ 3200]\n",
      "loss: 1.393704  [  624/ 3200]\n",
      "loss: 1.388463  [  640/ 3200]\n",
      "loss: 1.385455  [  656/ 3200]\n",
      "loss: 1.384197  [  672/ 3200]\n",
      "loss: 1.384632  [  688/ 3200]\n",
      "loss: 1.388199  [  704/ 3200]\n",
      "loss: 1.386492  [  720/ 3200]\n",
      "loss: 1.391640  [  736/ 3200]\n",
      "loss: 1.381518  [  752/ 3200]\n",
      "loss: 1.388958  [  768/ 3200]\n",
      "loss: 1.386103  [  784/ 3200]\n",
      "loss: 1.387312  [  800/ 3200]\n",
      "loss: 1.382881  [  816/ 3200]\n",
      "loss: 1.384587  [  832/ 3200]\n",
      "loss: 1.389330  [  848/ 3200]\n",
      "loss: 1.383918  [  864/ 3200]\n",
      "loss: 1.384367  [  880/ 3200]\n",
      "loss: 1.387746  [  896/ 3200]\n",
      "loss: 1.387161  [  912/ 3200]\n",
      "loss: 1.387312  [  928/ 3200]\n",
      "loss: 1.386383  [  944/ 3200]\n",
      "loss: 1.388181  [  960/ 3200]\n",
      "loss: 1.386384  [  976/ 3200]\n",
      "loss: 1.386923  [  992/ 3200]\n",
      "loss: 1.385622  [ 1008/ 3200]\n",
      "loss: 1.379440  [ 1024/ 3200]\n",
      "loss: 1.385560  [ 1040/ 3200]\n",
      "loss: 1.382280  [ 1056/ 3200]\n",
      "loss: 1.385022  [ 1072/ 3200]\n",
      "loss: 1.382341  [ 1088/ 3200]\n",
      "loss: 1.388738  [ 1104/ 3200]\n",
      "loss: 1.390710  [ 1120/ 3200]\n",
      "loss: 1.385666  [ 1136/ 3200]\n",
      "loss: 1.389225  [ 1152/ 3200]\n",
      "loss: 1.388958  [ 1168/ 3200]\n",
      "loss: 1.384801  [ 1184/ 3200]\n",
      "loss: 1.387700  [ 1200/ 3200]\n",
      "loss: 1.381192  [ 1216/ 3200]\n",
      "loss: 1.387790  [ 1232/ 3200]\n",
      "loss: 1.383377  [ 1248/ 3200]\n",
      "loss: 1.386711  [ 1264/ 3200]\n",
      "loss: 1.383661  [ 1280/ 3200]\n",
      "loss: 1.389436  [ 1296/ 3200]\n",
      "loss: 1.380263  [ 1312/ 3200]\n",
      "loss: 1.381625  [ 1328/ 3200]\n",
      "loss: 1.387807  [ 1344/ 3200]\n",
      "loss: 1.382881  [ 1360/ 3200]\n",
      "loss: 1.383438  [ 1376/ 3200]\n",
      "loss: 1.387579  [ 1392/ 3200]\n",
      "loss: 1.389887  [ 1408/ 3200]\n",
      "loss: 1.379829  [ 1424/ 3200]\n",
      "loss: 1.386384  [ 1440/ 3200]\n",
      "loss: 1.388197  [ 1456/ 3200]\n",
      "loss: 1.386489  [ 1472/ 3200]\n",
      "loss: 1.387807  [ 1488/ 3200]\n",
      "loss: 1.383661  [ 1504/ 3200]\n",
      "loss: 1.383166  [ 1520/ 3200]\n",
      "loss: 1.384962  [ 1536/ 3200]\n",
      "loss: 1.383375  [ 1552/ 3200]\n",
      "loss: 1.384198  [ 1568/ 3200]\n",
      "loss: 1.385410  [ 1584/ 3200]\n",
      "loss: 1.386773  [ 1600/ 3200]\n",
      "loss: 1.381191  [ 1616/ 3200]\n",
      "loss: 1.392012  [ 1632/ 3200]\n",
      "loss: 1.384091  [ 1648/ 3200]\n",
      "loss: 1.395853  [ 1664/ 3200]\n",
      "loss: 1.382121  [ 1680/ 3200]\n",
      "loss: 1.383591  [ 1696/ 3200]\n",
      "loss: 1.386431  [ 1712/ 3200]\n",
      "loss: 1.389002  [ 1728/ 3200]\n",
      "loss: 1.387581  [ 1744/ 3200]\n",
      "loss: 1.386878  [ 1760/ 3200]\n",
      "loss: 1.382777  [ 1776/ 3200]\n",
      "loss: 1.387641  [ 1792/ 3200]\n",
      "loss: 1.386384  [ 1808/ 3200]\n",
      "loss: 1.386817  [ 1824/ 3200]\n",
      "loss: 1.388029  [ 1840/ 3200]\n",
      "loss: 1.384093  [ 1856/ 3200]\n",
      "loss: 1.386384  [ 1872/ 3200]\n",
      "loss: 1.387032  [ 1888/ 3200]\n",
      "loss: 1.386384  [ 1904/ 3200]\n",
      "loss: 1.386385  [ 1920/ 3200]\n",
      "loss: 1.391635  [ 1936/ 3200]\n",
      "loss: 1.388508  [ 1952/ 3200]\n",
      "loss: 1.387701  [ 1968/ 3200]\n",
      "loss: 1.383872  [ 1984/ 3200]\n",
      "loss: 1.382838  [ 2000/ 3200]\n",
      "loss: 1.389436  [ 2016/ 3200]\n",
      "loss: 1.384961  [ 2032/ 3200]\n",
      "loss: 1.385455  [ 2048/ 3200]\n",
      "loss: 1.386710  [ 2064/ 3200]\n",
      "loss: 1.384741  [ 2080/ 3200]\n",
      "loss: 1.388134  [ 2096/ 3200]\n",
      "loss: 1.388027  [ 2112/ 3200]\n",
      "loss: 1.386878  [ 2128/ 3200]\n",
      "loss: 1.386444  [ 2144/ 3200]\n",
      "loss: 1.388628  [ 2160/ 3200]\n",
      "loss: 1.389387  [ 2176/ 3200]\n",
      "loss: 1.386770  [ 2192/ 3200]\n",
      "loss: 1.382344  [ 2208/ 3200]\n",
      "loss: 1.382451  [ 2224/ 3200]\n",
      "loss: 1.390810  [ 2240/ 3200]\n",
      "loss: 1.388953  [ 2256/ 3200]\n",
      "loss: 1.386057  [ 2272/ 3200]\n",
      "loss: 1.384306  [ 2288/ 3200]\n",
      "loss: 1.389167  [ 2304/ 3200]\n",
      "loss: 1.387533  [ 2320/ 3200]\n",
      "loss: 1.393259  [ 2336/ 3200]\n",
      "loss: 1.387747  [ 2352/ 3200]\n",
      "loss: 1.387097  [ 2368/ 3200]\n",
      "loss: 1.389001  [ 2384/ 3200]\n",
      "loss: 1.383707  [ 2400/ 3200]\n",
      "loss: 1.386710  [ 2416/ 3200]\n",
      "loss: 1.381150  [ 2432/ 3200]\n",
      "loss: 1.387253  [ 2448/ 3200]\n",
      "loss: 1.384092  [ 2464/ 3200]\n",
      "loss: 1.385455  [ 2480/ 3200]\n",
      "loss: 1.389386  [ 2496/ 3200]\n",
      "loss: 1.386382  [ 2512/ 3200]\n",
      "loss: 1.389277  [ 2528/ 3200]\n",
      "loss: 1.383273  [ 2544/ 3200]\n",
      "loss: 1.386323  [ 2560/ 3200]\n",
      "loss: 1.387745  [ 2576/ 3200]\n",
      "loss: 1.390699  [ 2592/ 3200]\n",
      "loss: 1.387747  [ 2608/ 3200]\n",
      "loss: 1.382563  [ 2624/ 3200]\n",
      "loss: 1.385950  [ 2640/ 3200]\n",
      "loss: 1.391458  [ 2656/ 3200]\n",
      "loss: 1.388891  [ 2672/ 3200]\n",
      "loss: 1.388022  [ 2688/ 3200]\n",
      "loss: 1.381525  [ 2704/ 3200]\n",
      "loss: 1.389926  [ 2720/ 3200]\n",
      "loss: 1.388675  [ 2736/ 3200]\n",
      "loss: 1.388999  [ 2752/ 3200]\n",
      "loss: 1.383384  [ 2768/ 3200]\n",
      "loss: 1.385565  [ 2784/ 3200]\n",
      "loss: 1.388130  [ 2800/ 3200]\n",
      "loss: 1.385454  [ 2816/ 3200]\n",
      "loss: 1.385070  [ 2832/ 3200]\n",
      "loss: 1.388455  [ 2848/ 3200]\n",
      "loss: 1.385069  [ 2864/ 3200]\n",
      "loss: 1.382022  [ 2880/ 3200]\n",
      "loss: 1.385393  [ 2896/ 3200]\n",
      "loss: 1.380495  [ 2912/ 3200]\n",
      "loss: 1.388841  [ 2928/ 3200]\n",
      "loss: 1.389061  [ 2944/ 3200]\n",
      "loss: 1.383770  [ 2960/ 3200]\n",
      "loss: 1.383060  [ 2976/ 3200]\n",
      "loss: 1.389493  [ 2992/ 3200]\n",
      "loss: 1.385779  [ 3008/ 3200]\n",
      "loss: 1.382839  [ 3024/ 3200]\n",
      "loss: 1.385566  [ 3040/ 3200]\n",
      "loss: 1.383769  [ 3056/ 3200]\n",
      "loss: 1.385674  [ 3072/ 3200]\n",
      "loss: 1.389769  [ 3088/ 3200]\n",
      "loss: 1.391071  [ 3104/ 3200]\n",
      "loss: 1.385289  [ 3120/ 3200]\n",
      "loss: 1.387421  [ 3136/ 3200]\n",
      "loss: 1.392383  [ 3152/ 3200]\n",
      "loss: 1.388624  [ 3168/ 3200]\n",
      "loss: 1.389383  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.7052e-04.\n",
      "\n",
      "Epoch: 49\n",
      "-----------------------------\n",
      "loss: 1.386492  [    0/ 3200]\n",
      "loss: 1.384807  [   16/ 3200]\n",
      "loss: 1.381530  [   32/ 3200]\n",
      "loss: 1.385395  [   48/ 3200]\n",
      "loss: 1.381638  [   64/ 3200]\n",
      "loss: 1.389429  [   80/ 3200]\n",
      "loss: 1.384573  [   96/ 3200]\n",
      "loss: 1.386383  [  112/ 3200]\n",
      "loss: 1.388498  [  128/ 3200]\n",
      "loss: 1.383708  [  144/ 3200]\n",
      "loss: 1.382565  [  160/ 3200]\n",
      "loss: 1.383491  [  176/ 3200]\n",
      "loss: 1.389427  [  192/ 3200]\n",
      "loss: 1.386772  [  208/ 3200]\n",
      "loss: 1.380384  [  224/ 3200]\n",
      "loss: 1.387743  [  240/ 3200]\n",
      "loss: 1.385068  [  256/ 3200]\n",
      "loss: 1.390204  [  272/ 3200]\n",
      "loss: 1.390764  [  288/ 3200]\n",
      "loss: 1.384204  [  304/ 3200]\n",
      "loss: 1.388518  [  320/ 3200]\n",
      "loss: 1.389770  [  336/ 3200]\n",
      "loss: 1.389273  [  352/ 3200]\n",
      "loss: 1.388238  [  368/ 3200]\n",
      "loss: 1.389922  [  384/ 3200]\n",
      "loss: 1.381533  [  400/ 3200]\n",
      "loss: 1.389058  [  416/ 3200]\n",
      "loss: 1.391668  [  432/ 3200]\n",
      "loss: 1.388839  [  448/ 3200]\n",
      "loss: 1.382134  [  464/ 3200]\n",
      "loss: 1.386276  [  480/ 3200]\n",
      "loss: 1.382567  [  496/ 3200]\n",
      "loss: 1.386338  [  512/ 3200]\n",
      "loss: 1.381145  [  528/ 3200]\n",
      "loss: 1.391128  [  544/ 3200]\n",
      "loss: 1.385737  [  560/ 3200]\n",
      "loss: 1.382892  [  576/ 3200]\n",
      "loss: 1.387958  [  592/ 3200]\n",
      "loss: 1.386383  [  608/ 3200]\n",
      "loss: 1.388453  [  624/ 3200]\n",
      "loss: 1.383927  [  640/ 3200]\n",
      "loss: 1.388560  [  656/ 3200]\n",
      "loss: 1.389858  [  672/ 3200]\n",
      "loss: 1.385069  [  688/ 3200]\n",
      "loss: 1.389811  [  704/ 3200]\n",
      "loss: 1.383279  [  720/ 3200]\n",
      "loss: 1.388128  [  736/ 3200]\n",
      "loss: 1.389876  [  752/ 3200]\n",
      "loss: 1.388065  [  768/ 3200]\n",
      "loss: 1.388559  [  784/ 3200]\n",
      "loss: 1.386061  [  800/ 3200]\n",
      "loss: 1.387632  [  816/ 3200]\n",
      "loss: 1.386319  [  832/ 3200]\n",
      "loss: 1.388236  [  848/ 3200]\n",
      "loss: 1.387309  [  864/ 3200]\n",
      "loss: 1.384961  [  880/ 3200]\n",
      "loss: 1.384703  [  896/ 3200]\n",
      "loss: 1.379573  [  912/ 3200]\n",
      "loss: 1.382957  [  928/ 3200]\n",
      "loss: 1.388624  [  944/ 3200]\n",
      "loss: 1.390200  [  960/ 3200]\n",
      "loss: 1.386319  [  976/ 3200]\n",
      "loss: 1.382461  [  992/ 3200]\n",
      "loss: 1.385952  [ 1008/ 3200]\n",
      "loss: 1.390370  [ 1024/ 3200]\n",
      "loss: 1.382569  [ 1040/ 3200]\n",
      "loss: 1.389377  [ 1056/ 3200]\n",
      "loss: 1.383775  [ 1072/ 3200]\n",
      "loss: 1.388947  [ 1088/ 3200]\n",
      "loss: 1.389809  [ 1104/ 3200]\n",
      "loss: 1.381042  [ 1120/ 3200]\n",
      "loss: 1.389443  [ 1136/ 3200]\n",
      "loss: 1.388084  [ 1152/ 3200]\n",
      "loss: 1.385520  [ 1168/ 3200]\n",
      "loss: 1.389918  [ 1184/ 3200]\n",
      "loss: 1.387741  [ 1200/ 3200]\n",
      "loss: 1.386104  [ 1216/ 3200]\n",
      "loss: 1.385889  [ 1232/ 3200]\n",
      "loss: 1.388559  [ 1248/ 3200]\n",
      "loss: 1.389808  [ 1264/ 3200]\n",
      "loss: 1.390152  [ 1280/ 3200]\n",
      "loss: 1.382248  [ 1296/ 3200]\n",
      "loss: 1.384207  [ 1312/ 3200]\n",
      "loss: 1.388020  [ 1328/ 3200]\n",
      "loss: 1.383497  [ 1344/ 3200]\n",
      "loss: 1.392370  [ 1360/ 3200]\n",
      "loss: 1.387094  [ 1376/ 3200]\n",
      "loss: 1.384145  [ 1392/ 3200]\n",
      "loss: 1.390303  [ 1408/ 3200]\n",
      "loss: 1.386752  [ 1424/ 3200]\n",
      "loss: 1.390301  [ 1440/ 3200]\n",
      "loss: 1.386768  [ 1456/ 3200]\n",
      "loss: 1.387263  [ 1472/ 3200]\n",
      "loss: 1.387803  [ 1488/ 3200]\n",
      "loss: 1.388450  [ 1504/ 3200]\n",
      "loss: 1.383886  [ 1520/ 3200]\n",
      "loss: 1.387356  [ 1536/ 3200]\n",
      "loss: 1.386213  [ 1552/ 3200]\n",
      "loss: 1.386319  [ 1568/ 3200]\n",
      "loss: 1.385952  [ 1584/ 3200]\n",
      "loss: 1.385952  [ 1600/ 3200]\n",
      "loss: 1.387694  [ 1616/ 3200]\n",
      "loss: 1.384593  [ 1632/ 3200]\n",
      "loss: 1.392044  [ 1648/ 3200]\n",
      "loss: 1.391933  [ 1664/ 3200]\n",
      "loss: 1.380293  [ 1680/ 3200]\n",
      "loss: 1.384208  [ 1696/ 3200]\n",
      "loss: 1.386492  [ 1712/ 3200]\n",
      "loss: 1.391932  [ 1728/ 3200]\n",
      "loss: 1.383283  [ 1744/ 3200]\n",
      "loss: 1.385628  [ 1760/ 3200]\n",
      "loss: 1.380292  [ 1776/ 3200]\n",
      "loss: 1.390190  [ 1792/ 3200]\n",
      "loss: 1.387309  [ 1808/ 3200]\n",
      "loss: 1.378058  [ 1824/ 3200]\n",
      "loss: 1.388172  [ 1840/ 3200]\n",
      "loss: 1.385675  [ 1856/ 3200]\n",
      "loss: 1.385627  [ 1872/ 3200]\n",
      "loss: 1.391501  [ 1888/ 3200]\n",
      "loss: 1.386875  [ 1904/ 3200]\n",
      "loss: 1.389049  [ 1920/ 3200]\n",
      "loss: 1.390081  [ 1936/ 3200]\n",
      "loss: 1.381438  [ 1952/ 3200]\n",
      "loss: 1.384208  [ 1968/ 3200]\n",
      "loss: 1.384701  [ 1984/ 3200]\n",
      "loss: 1.383717  [ 2000/ 3200]\n",
      "loss: 1.386766  [ 2016/ 3200]\n",
      "loss: 1.383335  [ 2032/ 3200]\n",
      "loss: 1.386492  [ 2048/ 3200]\n",
      "loss: 1.388989  [ 2064/ 3200]\n",
      "loss: 1.388497  [ 2080/ 3200]\n",
      "loss: 1.385026  [ 2096/ 3200]\n",
      "loss: 1.388941  [ 2112/ 3200]\n",
      "loss: 1.383392  [ 2128/ 3200]\n",
      "loss: 1.387631  [ 2144/ 3200]\n",
      "loss: 1.388016  [ 2160/ 3200]\n",
      "loss: 1.384965  [ 2176/ 3200]\n",
      "loss: 1.381974  [ 2192/ 3200]\n",
      "loss: 1.386261  [ 2208/ 3200]\n",
      "loss: 1.388000  [ 2224/ 3200]\n",
      "loss: 1.387803  [ 2240/ 3200]\n",
      "loss: 1.393827  [ 2256/ 3200]\n",
      "loss: 1.388077  [ 2272/ 3200]\n",
      "loss: 1.382962  [ 2288/ 3200]\n",
      "loss: 1.381160  [ 2304/ 3200]\n",
      "loss: 1.382036  [ 2320/ 3200]\n",
      "loss: 1.388234  [ 2336/ 3200]\n",
      "loss: 1.387136  [ 2352/ 3200]\n",
      "loss: 1.383284  [ 2368/ 3200]\n",
      "loss: 1.386924  [ 2384/ 3200]\n",
      "loss: 1.387630  [ 2400/ 3200]\n",
      "loss: 1.389588  [ 2416/ 3200]\n",
      "loss: 1.386705  [ 2432/ 3200]\n",
      "loss: 1.387803  [ 2448/ 3200]\n",
      "loss: 1.388941  [ 2464/ 3200]\n",
      "loss: 1.384595  [ 2480/ 3200]\n",
      "loss: 1.386554  [ 2496/ 3200]\n",
      "loss: 1.386490  [ 2512/ 3200]\n",
      "loss: 1.387740  [ 2528/ 3200]\n",
      "loss: 1.384534  [ 2544/ 3200]\n",
      "loss: 1.387244  [ 2560/ 3200]\n",
      "loss: 1.383656  [ 2576/ 3200]\n",
      "loss: 1.381222  [ 2592/ 3200]\n",
      "loss: 1.385395  [ 2608/ 3200]\n",
      "loss: 1.388878  [ 2624/ 3200]\n",
      "loss: 1.388510  [ 2640/ 3200]\n",
      "loss: 1.384643  [ 2656/ 3200]\n",
      "loss: 1.385631  [ 2672/ 3200]\n",
      "loss: 1.387847  [ 2688/ 3200]\n",
      "loss: 1.383781  [ 2704/ 3200]\n",
      "loss: 1.383286  [ 2720/ 3200]\n",
      "loss: 1.381330  [ 2736/ 3200]\n",
      "loss: 1.381932  [ 2752/ 3200]\n",
      "loss: 1.385996  [ 2768/ 3200]\n",
      "loss: 1.388446  [ 2784/ 3200]\n",
      "loss: 1.390231  [ 2800/ 3200]\n",
      "loss: 1.391973  [ 2816/ 3200]\n",
      "loss: 1.387199  [ 2832/ 3200]\n",
      "loss: 1.386168  [ 2848/ 3200]\n",
      "loss: 1.387243  [ 2864/ 3200]\n",
      "loss: 1.385888  [ 2880/ 3200]\n",
      "loss: 1.387264  [ 2896/ 3200]\n",
      "loss: 1.389652  [ 2912/ 3200]\n",
      "loss: 1.393282  [ 2928/ 3200]\n",
      "loss: 1.385136  [ 2944/ 3200]\n",
      "loss: 1.386059  [ 2960/ 3200]\n",
      "loss: 1.384256  [ 2976/ 3200]\n",
      "loss: 1.389326  [ 2992/ 3200]\n",
      "loss: 1.383396  [ 3008/ 3200]\n",
      "loss: 1.385414  [ 3024/ 3200]\n",
      "loss: 1.382750  [ 3040/ 3200]\n",
      "loss: 1.386814  [ 3056/ 3200]\n",
      "loss: 1.379376  [ 3072/ 3200]\n",
      "loss: 1.389370  [ 3088/ 3200]\n",
      "loss: 1.383891  [ 3104/ 3200]\n",
      "loss: 1.387244  [ 3120/ 3200]\n",
      "loss: 1.382534  [ 3136/ 3200]\n",
      "loss: 1.382918  [ 3152/ 3200]\n",
      "loss: 1.382963  [ 3168/ 3200]\n",
      "loss: 1.383887  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.6199e-04.\n",
      "\n",
      "Epoch: 50\n",
      "-----------------------------\n",
      "loss: 1.391049  [    0/ 3200]\n",
      "loss: 1.387691  [   16/ 3200]\n",
      "loss: 1.387737  [   32/ 3200]\n",
      "loss: 1.388231  [   48/ 3200]\n",
      "loss: 1.387030  [   64/ 3200]\n",
      "loss: 1.387306  [   80/ 3200]\n",
      "loss: 1.384750  [   96/ 3200]\n",
      "loss: 1.386010  [  112/ 3200]\n",
      "loss: 1.386768  [  128/ 3200]\n",
      "loss: 1.381551  [  144/ 3200]\n",
      "loss: 1.392618  [  160/ 3200]\n",
      "loss: 1.388508  [  176/ 3200]\n",
      "loss: 1.383396  [  192/ 3200]\n",
      "loss: 1.391155  [  208/ 3200]\n",
      "loss: 1.387736  [  224/ 3200]\n",
      "loss: 1.384750  [  240/ 3200]\n",
      "loss: 1.385134  [  256/ 3200]\n",
      "loss: 1.386933  [  272/ 3200]\n",
      "loss: 1.389369  [  288/ 3200]\n",
      "loss: 1.385893  [  304/ 3200]\n",
      "loss: 1.383616  [  320/ 3200]\n",
      "loss: 1.385181  [  336/ 3200]\n",
      "loss: 1.389149  [  352/ 3200]\n",
      "loss: 1.382579  [  368/ 3200]\n",
      "loss: 1.385029  [  384/ 3200]\n",
      "loss: 1.383069  [  400/ 3200]\n",
      "loss: 1.385951  [  416/ 3200]\n",
      "loss: 1.381984  [  432/ 3200]\n",
      "loss: 1.383617  [  448/ 3200]\n",
      "loss: 1.389430  [  464/ 3200]\n",
      "loss: 1.386873  [  480/ 3200]\n",
      "loss: 1.384212  [  496/ 3200]\n",
      "loss: 1.382905  [  512/ 3200]\n",
      "loss: 1.390664  [  528/ 3200]\n",
      "loss: 1.386815  [  544/ 3200]\n",
      "loss: 1.378460  [  560/ 3200]\n",
      "loss: 1.387735  [  576/ 3200]\n",
      "loss: 1.384538  [  592/ 3200]\n",
      "loss: 1.385415  [  608/ 3200]\n",
      "loss: 1.388061  [  624/ 3200]\n",
      "loss: 1.386057  [  640/ 3200]\n",
      "loss: 1.385075  [  656/ 3200]\n",
      "loss: 1.387199  [  672/ 3200]\n",
      "loss: 1.384749  [  688/ 3200]\n",
      "loss: 1.381226  [  704/ 3200]\n",
      "loss: 1.386980  [  720/ 3200]\n",
      "loss: 1.382043  [  736/ 3200]\n",
      "loss: 1.388060  [  752/ 3200]\n",
      "loss: 1.388718  [  768/ 3200]\n",
      "loss: 1.386873  [  784/ 3200]\n",
      "loss: 1.386382  [  800/ 3200]\n",
      "loss: 1.389264  [  816/ 3200]\n",
      "loss: 1.392786  [  832/ 3200]\n",
      "loss: 1.381982  [  848/ 3200]\n",
      "loss: 1.390229  [  864/ 3200]\n",
      "loss: 1.386489  [  880/ 3200]\n",
      "loss: 1.385503  [  896/ 3200]\n",
      "loss: 1.389650  [  912/ 3200]\n",
      "loss: 1.388552  [  928/ 3200]\n",
      "loss: 1.382150  [  944/ 3200]\n",
      "loss: 1.388983  [  960/ 3200]\n",
      "loss: 1.383828  [  976/ 3200]\n",
      "loss: 1.391862  [  992/ 3200]\n",
      "loss: 1.386058  [ 1008/ 3200]\n",
      "loss: 1.388015  [ 1024/ 3200]\n",
      "loss: 1.387156  [ 1040/ 3200]\n",
      "loss: 1.385627  [ 1056/ 3200]\n",
      "loss: 1.387243  [ 1072/ 3200]\n",
      "loss: 1.384108  [ 1088/ 3200]\n",
      "loss: 1.387199  [ 1104/ 3200]\n",
      "loss: 1.389304  [ 1120/ 3200]\n",
      "loss: 1.387734  [ 1136/ 3200]\n",
      "loss: 1.385181  [ 1152/ 3200]\n",
      "loss: 1.389902  [ 1168/ 3200]\n",
      "loss: 1.384706  [ 1184/ 3200]\n",
      "loss: 1.389367  [ 1200/ 3200]\n",
      "loss: 1.388059  [ 1216/ 3200]\n",
      "loss: 1.385032  [ 1232/ 3200]\n",
      "loss: 1.389367  [ 1248/ 3200]\n",
      "loss: 1.383336  [ 1264/ 3200]\n",
      "loss: 1.387305  [ 1280/ 3200]\n",
      "loss: 1.384321  [ 1296/ 3200]\n",
      "loss: 1.385031  [ 1312/ 3200]\n",
      "loss: 1.382969  [ 1328/ 3200]\n",
      "loss: 1.387242  [ 1344/ 3200]\n",
      "loss: 1.388655  [ 1360/ 3200]\n",
      "loss: 1.392350  [ 1376/ 3200]\n",
      "loss: 1.379170  [ 1392/ 3200]\n",
      "loss: 1.385461  [ 1408/ 3200]\n",
      "loss: 1.386487  [ 1424/ 3200]\n",
      "loss: 1.380739  [ 1440/ 3200]\n",
      "loss: 1.382153  [ 1456/ 3200]\n",
      "loss: 1.385566  [ 1472/ 3200]\n",
      "loss: 1.384665  [ 1488/ 3200]\n",
      "loss: 1.390656  [ 1504/ 3200]\n",
      "loss: 1.387753  [ 1520/ 3200]\n",
      "loss: 1.380739  [ 1536/ 3200]\n",
      "loss: 1.385626  [ 1552/ 3200]\n",
      "loss: 1.389471  [ 1568/ 3200]\n",
      "loss: 1.385626  [ 1584/ 3200]\n",
      "loss: 1.389366  [ 1600/ 3200]\n",
      "loss: 1.379225  [ 1616/ 3200]\n",
      "loss: 1.386487  [ 1632/ 3200]\n",
      "loss: 1.389796  [ 1648/ 3200]\n",
      "loss: 1.390613  [ 1664/ 3200]\n",
      "loss: 1.378960  [ 1680/ 3200]\n",
      "loss: 1.381064  [ 1696/ 3200]\n",
      "loss: 1.390139  [ 1712/ 3200]\n",
      "loss: 1.389148  [ 1728/ 3200]\n",
      "loss: 1.387242  [ 1744/ 3200]\n",
      "loss: 1.386172  [ 1760/ 3200]\n",
      "loss: 1.380803  [ 1776/ 3200]\n",
      "loss: 1.391533  [ 1792/ 3200]\n",
      "loss: 1.388120  [ 1808/ 3200]\n",
      "loss: 1.389041  [ 1824/ 3200]\n",
      "loss: 1.389425  [ 1840/ 3200]\n",
      "loss: 1.386489  [ 1856/ 3200]\n",
      "loss: 1.388119  [ 1872/ 3200]\n",
      "loss: 1.387688  [ 1888/ 3200]\n",
      "loss: 1.389840  [ 1904/ 3200]\n",
      "loss: 1.383785  [ 1920/ 3200]\n",
      "loss: 1.385522  [ 1936/ 3200]\n",
      "loss: 1.389900  [ 1952/ 3200]\n",
      "loss: 1.383295  [ 1968/ 3200]\n",
      "loss: 1.388873  [ 1984/ 3200]\n",
      "loss: 1.389257  [ 2000/ 3200]\n",
      "loss: 1.392389  [ 2016/ 3200]\n",
      "loss: 1.385462  [ 2032/ 3200]\n",
      "loss: 1.390668  [ 2048/ 3200]\n",
      "loss: 1.382374  [ 2064/ 3200]\n",
      "loss: 1.387688  [ 2080/ 3200]\n",
      "loss: 1.390607  [ 2096/ 3200]\n",
      "loss: 1.385460  [ 2112/ 3200]\n",
      "loss: 1.384647  [ 2128/ 3200]\n",
      "loss: 1.386443  [ 2144/ 3200]\n",
      "loss: 1.383402  [ 2160/ 3200]\n",
      "loss: 1.389099  [ 2176/ 3200]\n",
      "loss: 1.386382  [ 2192/ 3200]\n",
      "loss: 1.388547  [ 2208/ 3200]\n",
      "loss: 1.392017  [ 2224/ 3200]\n",
      "loss: 1.386548  [ 2240/ 3200]\n",
      "loss: 1.379338  [ 2256/ 3200]\n",
      "loss: 1.383616  [ 2272/ 3200]\n",
      "loss: 1.381560  [ 2288/ 3200]\n",
      "loss: 1.382912  [ 2304/ 3200]\n",
      "loss: 1.381235  [ 2320/ 3200]\n",
      "loss: 1.381560  [ 2336/ 3200]\n",
      "loss: 1.389794  [ 2352/ 3200]\n",
      "loss: 1.389145  [ 2368/ 3200]\n",
      "loss: 1.382911  [ 2384/ 3200]\n",
      "loss: 1.383831  [ 2400/ 3200]\n",
      "loss: 1.384215  [ 2416/ 3200]\n",
      "loss: 1.386707  [ 2432/ 3200]\n",
      "loss: 1.389317  [ 2448/ 3200]\n",
      "loss: 1.378256  [ 2464/ 3200]\n",
      "loss: 1.381990  [ 2480/ 3200]\n",
      "loss: 1.384323  [ 2496/ 3200]\n",
      "loss: 1.388872  [ 2512/ 3200]\n",
      "loss: 1.385137  [ 2528/ 3200]\n",
      "loss: 1.388608  [ 2544/ 3200]\n",
      "loss: 1.394555  [ 2560/ 3200]\n",
      "loss: 1.392065  [ 2576/ 3200]\n",
      "loss: 1.391957  [ 2592/ 3200]\n",
      "loss: 1.388161  [ 2608/ 3200]\n",
      "loss: 1.388977  [ 2624/ 3200]\n",
      "loss: 1.384324  [ 2640/ 3200]\n",
      "loss: 1.380810  [ 2656/ 3200]\n",
      "loss: 1.390816  [ 2672/ 3200]\n",
      "loss: 1.386646  [ 2688/ 3200]\n",
      "loss: 1.383299  [ 2704/ 3200]\n",
      "loss: 1.387303  [ 2720/ 3200]\n",
      "loss: 1.389258  [ 2736/ 3200]\n",
      "loss: 1.379398  [ 2752/ 3200]\n",
      "loss: 1.385847  [ 2768/ 3200]\n",
      "loss: 1.383080  [ 2784/ 3200]\n",
      "loss: 1.385139  [ 2800/ 3200]\n",
      "loss: 1.385032  [ 2816/ 3200]\n",
      "loss: 1.387582  [ 2832/ 3200]\n",
      "loss: 1.386381  [ 2848/ 3200]\n",
      "loss: 1.388546  [ 2864/ 3200]\n",
      "loss: 1.386059  [ 2880/ 3200]\n",
      "loss: 1.384217  [ 2896/ 3200]\n",
      "loss: 1.385355  [ 2912/ 3200]\n",
      "loss: 1.388178  [ 2928/ 3200]\n",
      "loss: 1.390712  [ 2944/ 3200]\n",
      "loss: 1.391525  [ 2960/ 3200]\n",
      "loss: 1.385032  [ 2976/ 3200]\n",
      "loss: 1.385137  [ 2992/ 3200]\n",
      "loss: 1.392232  [ 3008/ 3200]\n",
      "loss: 1.390003  [ 3024/ 3200]\n",
      "loss: 1.385568  [ 3040/ 3200]\n",
      "loss: 1.383834  [ 3056/ 3200]\n",
      "loss: 1.383404  [ 3072/ 3200]\n",
      "loss: 1.384986  [ 3088/ 3200]\n",
      "loss: 1.382317  [ 3104/ 3200]\n",
      "loss: 1.387579  [ 3120/ 3200]\n",
      "loss: 1.383619  [ 3136/ 3200]\n",
      "loss: 1.387626  [ 3152/ 3200]\n",
      "loss: 1.385522  [ 3168/ 3200]\n",
      "loss: 1.386551  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.5389e-04.\n",
      "\n",
      "Epoch: 51\n",
      "-----------------------------\n",
      "loss: 1.383298  [    0/ 3200]\n",
      "loss: 1.390279  [   16/ 3200]\n",
      "loss: 1.386381  [   32/ 3200]\n",
      "loss: 1.385079  [   48/ 3200]\n",
      "loss: 1.388224  [   64/ 3200]\n",
      "loss: 1.388116  [   80/ 3200]\n",
      "loss: 1.391033  [   96/ 3200]\n",
      "loss: 1.386442  [  112/ 3200]\n",
      "loss: 1.390601  [  128/ 3200]\n",
      "loss: 1.389143  [  144/ 3200]\n",
      "loss: 1.386488  [  160/ 3200]\n",
      "loss: 1.389300  [  176/ 3200]\n",
      "loss: 1.389081  [  192/ 3200]\n",
      "loss: 1.384755  [  208/ 3200]\n",
      "loss: 1.384649  [  224/ 3200]\n",
      "loss: 1.393515  [  240/ 3200]\n",
      "loss: 1.387793  [  256/ 3200]\n",
      "loss: 1.389035  [  272/ 3200]\n",
      "loss: 1.387301  [  288/ 3200]\n",
      "loss: 1.387134  [  304/ 3200]\n",
      "loss: 1.389418  [  320/ 3200]\n",
      "loss: 1.388545  [  336/ 3200]\n",
      "loss: 1.379898  [  352/ 3200]\n",
      "loss: 1.378976  [  368/ 3200]\n",
      "loss: 1.385892  [  384/ 3200]\n",
      "loss: 1.391197  [  400/ 3200]\n",
      "loss: 1.380326  [  416/ 3200]\n",
      "loss: 1.387532  [  432/ 3200]\n",
      "loss: 1.385510  [  448/ 3200]\n",
      "loss: 1.388437  [  464/ 3200]\n",
      "loss: 1.382978  [  480/ 3200]\n",
      "loss: 1.382918  [  496/ 3200]\n",
      "loss: 1.385894  [  512/ 3200]\n",
      "loss: 1.385952  [  528/ 3200]\n",
      "loss: 1.384866  [  544/ 3200]\n",
      "loss: 1.387194  [  560/ 3200]\n",
      "loss: 1.382655  [  576/ 3200]\n",
      "loss: 1.385893  [  592/ 3200]\n",
      "loss: 1.384603  [  608/ 3200]\n",
      "loss: 1.386059  [  624/ 3200]\n",
      "loss: 1.385629  [  640/ 3200]\n",
      "loss: 1.388114  [  656/ 3200]\n",
      "loss: 1.380266  [  672/ 3200]\n",
      "loss: 1.378698  [  688/ 3200]\n",
      "loss: 1.393683  [  704/ 3200]\n",
      "loss: 1.384863  [  720/ 3200]\n",
      "loss: 1.382917  [  736/ 3200]\n",
      "loss: 1.380047  [  752/ 3200]\n",
      "loss: 1.384429  [  768/ 3200]\n",
      "loss: 1.386057  [  784/ 3200]\n",
      "loss: 1.390707  [  800/ 3200]\n",
      "loss: 1.391212  [  816/ 3200]\n",
      "loss: 1.382976  [  832/ 3200]\n",
      "loss: 1.395309  [  848/ 3200]\n",
      "loss: 1.385627  [  864/ 3200]\n",
      "loss: 1.387242  [  880/ 3200]\n",
      "loss: 1.385953  [  896/ 3200]\n",
      "loss: 1.388054  [  912/ 3200]\n",
      "loss: 1.386603  [  928/ 3200]\n",
      "loss: 1.383837  [  944/ 3200]\n",
      "loss: 1.383082  [  960/ 3200]\n",
      "loss: 1.386753  [  976/ 3200]\n",
      "loss: 1.389401  [  992/ 3200]\n",
      "loss: 1.379513  [ 1008/ 3200]\n",
      "loss: 1.386810  [ 1024/ 3200]\n",
      "loss: 1.386708  [ 1040/ 3200]\n",
      "loss: 1.387685  [ 1056/ 3200]\n",
      "loss: 1.386382  [ 1072/ 3200]\n",
      "loss: 1.385524  [ 1088/ 3200]\n",
      "loss: 1.388393  [ 1104/ 3200]\n",
      "loss: 1.388603  [ 1120/ 3200]\n",
      "loss: 1.388707  [ 1136/ 3200]\n",
      "loss: 1.384327  [ 1152/ 3200]\n",
      "loss: 1.387579  [ 1168/ 3200]\n",
      "loss: 1.382548  [ 1184/ 3200]\n",
      "loss: 1.384651  [ 1200/ 3200]\n",
      "loss: 1.384162  [ 1216/ 3200]\n",
      "loss: 1.383513  [ 1232/ 3200]\n",
      "loss: 1.385906  [ 1248/ 3200]\n",
      "loss: 1.384115  [ 1264/ 3200]\n",
      "loss: 1.388543  [ 1280/ 3200]\n",
      "loss: 1.390215  [ 1296/ 3200]\n",
      "loss: 1.385894  [ 1312/ 3200]\n",
      "loss: 1.385952  [ 1328/ 3200]\n",
      "loss: 1.385905  [ 1344/ 3200]\n",
      "loss: 1.387567  [ 1360/ 3200]\n",
      "loss: 1.385081  [ 1376/ 3200]\n",
      "loss: 1.386334  [ 1392/ 3200]\n",
      "loss: 1.382919  [ 1408/ 3200]\n",
      "loss: 1.394109  [ 1424/ 3200]\n",
      "loss: 1.389843  [ 1440/ 3200]\n",
      "loss: 1.392328  [ 1456/ 3200]\n",
      "loss: 1.389679  [ 1472/ 3200]\n",
      "loss: 1.390214  [ 1488/ 3200]\n",
      "loss: 1.386763  [ 1504/ 3200]\n",
      "loss: 1.388435  [ 1520/ 3200]\n",
      "loss: 1.391132  [ 1536/ 3200]\n",
      "loss: 1.383086  [ 1552/ 3200]\n",
      "loss: 1.382922  [ 1568/ 3200]\n",
      "loss: 1.387848  [ 1584/ 3200]\n",
      "loss: 1.386116  [ 1600/ 3200]\n",
      "loss: 1.382216  [ 1616/ 3200]\n",
      "loss: 1.376441  [ 1632/ 3200]\n",
      "loss: 1.386706  [ 1648/ 3200]\n",
      "loss: 1.387242  [ 1664/ 3200]\n",
      "loss: 1.381678  [ 1680/ 3200]\n",
      "loss: 1.389248  [ 1696/ 3200]\n",
      "loss: 1.386275  [ 1712/ 3200]\n",
      "loss: 1.389460  [ 1728/ 3200]\n",
      "loss: 1.390643  [ 1744/ 3200]\n",
      "loss: 1.388879  [ 1760/ 3200]\n",
      "loss: 1.383792  [ 1776/ 3200]\n",
      "loss: 1.385463  [ 1792/ 3200]\n",
      "loss: 1.383576  [ 1808/ 3200]\n",
      "loss: 1.383898  [ 1824/ 3200]\n",
      "loss: 1.383734  [ 1840/ 3200]\n",
      "loss: 1.390654  [ 1856/ 3200]\n",
      "loss: 1.388006  [ 1872/ 3200]\n",
      "loss: 1.388005  [ 1888/ 3200]\n",
      "loss: 1.390701  [ 1904/ 3200]\n",
      "loss: 1.388053  [ 1920/ 3200]\n",
      "loss: 1.386332  [ 1936/ 3200]\n",
      "loss: 1.388922  [ 1952/ 3200]\n",
      "loss: 1.388541  [ 1968/ 3200]\n",
      "loss: 1.390973  [ 1984/ 3200]\n",
      "loss: 1.386223  [ 2000/ 3200]\n",
      "loss: 1.385785  [ 2016/ 3200]\n",
      "loss: 1.383030  [ 2032/ 3200]\n",
      "loss: 1.381845  [ 2048/ 3200]\n",
      "loss: 1.382169  [ 2064/ 3200]\n",
      "loss: 1.384652  [ 2080/ 3200]\n",
      "loss: 1.383305  [ 2096/ 3200]\n",
      "loss: 1.380611  [ 2112/ 3200]\n",
      "loss: 1.382874  [ 2128/ 3200]\n",
      "loss: 1.386811  [ 2144/ 3200]\n",
      "loss: 1.384221  [ 2160/ 3200]\n",
      "loss: 1.386655  [ 2176/ 3200]\n",
      "loss: 1.386438  [ 2192/ 3200]\n",
      "loss: 1.386326  [ 2208/ 3200]\n",
      "loss: 1.387461  [ 2224/ 3200]\n",
      "loss: 1.384601  [ 2240/ 3200]\n",
      "loss: 1.386813  [ 2256/ 3200]\n",
      "loss: 1.391405  [ 2272/ 3200]\n",
      "loss: 1.388971  [ 2288/ 3200]\n",
      "loss: 1.380391  [ 2304/ 3200]\n",
      "loss: 1.383411  [ 2320/ 3200]\n",
      "loss: 1.389351  [ 2336/ 3200]\n",
      "loss: 1.388973  [ 2352/ 3200]\n",
      "loss: 1.383412  [ 2368/ 3200]\n",
      "loss: 1.388326  [ 2384/ 3200]\n",
      "loss: 1.389352  [ 2400/ 3200]\n",
      "loss: 1.386596  [ 2416/ 3200]\n",
      "loss: 1.384653  [ 2432/ 3200]\n",
      "loss: 1.391511  [ 2448/ 3200]\n",
      "loss: 1.392270  [ 2464/ 3200]\n",
      "loss: 1.386756  [ 2480/ 3200]\n",
      "loss: 1.382601  [ 2496/ 3200]\n",
      "loss: 1.389352  [ 2512/ 3200]\n",
      "loss: 1.391451  [ 2528/ 3200]\n",
      "loss: 1.382495  [ 2544/ 3200]\n",
      "loss: 1.385894  [ 2560/ 3200]\n",
      "loss: 1.382008  [ 2576/ 3200]\n",
      "loss: 1.387135  [ 2592/ 3200]\n",
      "loss: 1.387408  [ 2608/ 3200]\n",
      "loss: 1.389790  [ 2624/ 3200]\n",
      "loss: 1.388052  [ 2640/ 3200]\n",
      "loss: 1.392368  [ 2656/ 3200]\n",
      "loss: 1.385406  [ 2672/ 3200]\n",
      "loss: 1.385034  [ 2688/ 3200]\n",
      "loss: 1.386275  [ 2704/ 3200]\n",
      "loss: 1.389397  [ 2720/ 3200]\n",
      "loss: 1.381637  [ 2736/ 3200]\n",
      "loss: 1.389136  [ 2752/ 3200]\n",
      "loss: 1.382984  [ 2768/ 3200]\n",
      "loss: 1.383414  [ 2784/ 3200]\n",
      "loss: 1.385951  [ 2800/ 3200]\n",
      "loss: 1.380230  [ 2816/ 3200]\n",
      "loss: 1.390647  [ 2832/ 3200]\n",
      "loss: 1.392046  [ 2848/ 3200]\n",
      "loss: 1.382984  [ 2864/ 3200]\n",
      "loss: 1.386976  [ 2880/ 3200]\n",
      "loss: 1.376882  [ 2896/ 3200]\n",
      "loss: 1.385512  [ 2912/ 3200]\n",
      "loss: 1.379528  [ 2928/ 3200]\n",
      "loss: 1.386702  [ 2944/ 3200]\n",
      "loss: 1.380766  [ 2960/ 3200]\n",
      "loss: 1.383198  [ 2976/ 3200]\n",
      "loss: 1.388707  [ 2992/ 3200]\n",
      "loss: 1.391567  [ 3008/ 3200]\n",
      "loss: 1.386060  [ 3024/ 3200]\n",
      "loss: 1.388862  [ 3040/ 3200]\n",
      "loss: 1.381793  [ 3056/ 3200]\n",
      "loss: 1.386273  [ 3072/ 3200]\n",
      "loss: 1.391937  [ 3088/ 3200]\n",
      "loss: 1.386274  [ 3104/ 3200]\n",
      "loss: 1.381364  [ 3120/ 3200]\n",
      "loss: 1.389837  [ 3136/ 3200]\n",
      "loss: 1.390696  [ 3152/ 3200]\n",
      "loss: 1.385894  [ 3168/ 3200]\n",
      "loss: 1.389397  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.4620e-04.\n",
      "\n",
      "Epoch: 52\n",
      "-----------------------------\n",
      "loss: 1.387298  [    0/ 3200]\n",
      "loss: 1.385201  [   16/ 3200]\n",
      "loss: 1.386763  [   32/ 3200]\n",
      "loss: 1.390587  [   48/ 3200]\n",
      "loss: 1.382497  [   64/ 3200]\n",
      "loss: 1.388051  [   80/ 3200]\n",
      "loss: 1.380281  [   96/ 3200]\n",
      "loss: 1.386810  [  112/ 3200]\n",
      "loss: 1.384225  [  128/ 3200]\n",
      "loss: 1.384545  [  144/ 3200]\n",
      "loss: 1.379421  [  160/ 3200]\n",
      "loss: 1.386060  [  176/ 3200]\n",
      "loss: 1.388584  [  192/ 3200]\n",
      "loss: 1.379529  [  208/ 3200]\n",
      "loss: 1.388002  [  224/ 3200]\n",
      "loss: 1.387300  [  240/ 3200]\n",
      "loss: 1.387300  [  256/ 3200]\n",
      "loss: 1.389349  [  272/ 3200]\n",
      "loss: 1.388981  [  288/ 3200]\n",
      "loss: 1.384226  [  304/ 3200]\n",
      "loss: 1.388049  [  320/ 3200]\n",
      "loss: 1.387620  [  336/ 3200]\n",
      "loss: 1.387239  [  352/ 3200]\n",
      "loss: 1.383523  [  368/ 3200]\n",
      "loss: 1.387560  [  384/ 3200]\n",
      "loss: 1.389349  [  400/ 3200]\n",
      "loss: 1.385143  [  416/ 3200]\n",
      "loss: 1.388476  [  432/ 3200]\n",
      "loss: 1.386871  [  448/ 3200]\n",
      "loss: 1.385571  [  464/ 3200]\n",
      "loss: 1.391443  [  480/ 3200]\n",
      "loss: 1.376076  [  496/ 3200]\n",
      "loss: 1.379912  [  512/ 3200]\n",
      "loss: 1.384332  [  528/ 3200]\n",
      "loss: 1.388539  [  544/ 3200]\n",
      "loss: 1.385143  [  560/ 3200]\n",
      "loss: 1.387512  [  576/ 3200]\n",
      "loss: 1.389394  [  592/ 3200]\n",
      "loss: 1.394195  [  608/ 3200]\n",
      "loss: 1.386382  [  624/ 3200]\n",
      "loss: 1.386211  [  640/ 3200]\n",
      "loss: 1.389303  [  656/ 3200]\n",
      "loss: 1.389392  [  672/ 3200]\n",
      "loss: 1.384716  [  688/ 3200]\n",
      "loss: 1.389776  [  704/ 3200]\n",
      "loss: 1.386210  [  720/ 3200]\n",
      "loss: 1.384928  [  736/ 3200]\n",
      "loss: 1.385571  [  752/ 3200]\n",
      "loss: 1.386063  [  768/ 3200]\n",
      "loss: 1.387191  [  784/ 3200]\n",
      "loss: 1.385635  [  800/ 3200]\n",
      "loss: 1.391137  [  816/ 3200]\n",
      "loss: 1.387145  [  832/ 3200]\n",
      "loss: 1.390691  [  848/ 3200]\n",
      "loss: 1.378725  [  864/ 3200]\n",
      "loss: 1.387789  [  880/ 3200]\n",
      "loss: 1.385143  [  896/ 3200]\n",
      "loss: 1.385464  [  912/ 3200]\n",
      "loss: 1.386811  [  928/ 3200]\n",
      "loss: 1.379962  [  944/ 3200]\n",
      "loss: 1.386443  [  960/ 3200]\n",
      "loss: 1.389392  [  976/ 3200]\n",
      "loss: 1.386764  [  992/ 3200]\n",
      "loss: 1.393609  [ 1008/ 3200]\n",
      "loss: 1.391013  [ 1024/ 3200]\n",
      "loss: 1.390371  [ 1040/ 3200]\n",
      "loss: 1.386380  [ 1056/ 3200]\n",
      "loss: 1.387725  [ 1072/ 3200]\n",
      "loss: 1.386489  [ 1088/ 3200]\n",
      "loss: 1.384334  [ 1104/ 3200]\n",
      "loss: 1.385847  [ 1120/ 3200]\n",
      "loss: 1.384227  [ 1136/ 3200]\n",
      "loss: 1.385572  [ 1152/ 3200]\n",
      "loss: 1.385358  [ 1168/ 3200]\n",
      "loss: 1.386870  [ 1184/ 3200]\n",
      "loss: 1.378621  [ 1200/ 3200]\n",
      "loss: 1.386809  [ 1216/ 3200]\n",
      "loss: 1.391498  [ 1232/ 3200]\n",
      "loss: 1.385084  [ 1248/ 3200]\n",
      "loss: 1.386808  [ 1264/ 3200]\n",
      "loss: 1.385144  [ 1280/ 3200]\n",
      "loss: 1.388107  [ 1296/ 3200]\n",
      "loss: 1.385999  [ 1312/ 3200]\n",
      "loss: 1.384289  [ 1328/ 3200]\n",
      "loss: 1.388535  [ 1344/ 3200]\n",
      "loss: 1.386167  [ 1360/ 3200]\n",
      "loss: 1.381586  [ 1376/ 3200]\n",
      "loss: 1.382990  [ 1392/ 3200]\n",
      "loss: 1.388474  [ 1408/ 3200]\n",
      "loss: 1.391070  [ 1424/ 3200]\n",
      "loss: 1.380944  [ 1440/ 3200]\n",
      "loss: 1.381691  [ 1456/ 3200]\n",
      "loss: 1.386702  [ 1472/ 3200]\n",
      "loss: 1.391116  [ 1488/ 3200]\n",
      "loss: 1.384013  [ 1504/ 3200]\n",
      "loss: 1.388213  [ 1520/ 3200]\n",
      "loss: 1.385038  [ 1536/ 3200]\n",
      "loss: 1.387130  [ 1552/ 3200]\n",
      "loss: 1.386658  [ 1568/ 3200]\n",
      "loss: 1.384273  [ 1584/ 3200]\n",
      "loss: 1.385465  [ 1600/ 3200]\n",
      "loss: 1.394463  [ 1616/ 3200]\n",
      "loss: 1.384272  [ 1632/ 3200]\n",
      "loss: 1.388898  [ 1648/ 3200]\n",
      "loss: 1.388107  [ 1664/ 3200]\n",
      "loss: 1.391070  [ 1680/ 3200]\n",
      "loss: 1.386869  [ 1696/ 3200]\n",
      "loss: 1.385402  [ 1712/ 3200]\n",
      "loss: 1.387724  [ 1728/ 3200]\n",
      "loss: 1.386764  [ 1744/ 3200]\n",
      "loss: 1.387936  [ 1760/ 3200]\n",
      "loss: 1.386700  [ 1776/ 3200]\n",
      "loss: 1.397418  [ 1792/ 3200]\n",
      "loss: 1.390578  [ 1808/ 3200]\n",
      "loss: 1.387723  [ 1824/ 3200]\n",
      "loss: 1.385039  [ 1840/ 3200]\n",
      "loss: 1.382676  [ 1856/ 3200]\n",
      "loss: 1.385253  [ 1872/ 3200]\n",
      "loss: 1.389705  [ 1888/ 3200]\n",
      "loss: 1.382931  [ 1904/ 3200]\n",
      "loss: 1.389874  [ 1920/ 3200]\n",
      "loss: 1.381588  [ 1936/ 3200]\n",
      "loss: 1.387361  [ 1952/ 3200]\n",
      "loss: 1.389235  [ 1968/ 3200]\n",
      "loss: 1.387019  [ 1984/ 3200]\n",
      "loss: 1.386338  [ 2000/ 3200]\n",
      "loss: 1.390748  [ 2016/ 3200]\n",
      "loss: 1.388105  [ 2032/ 3200]\n",
      "loss: 1.389021  [ 2048/ 3200]\n",
      "loss: 1.386488  [ 2064/ 3200]\n",
      "loss: 1.386808  [ 2080/ 3200]\n",
      "loss: 1.381696  [ 2096/ 3200]\n",
      "loss: 1.388106  [ 2112/ 3200]\n",
      "loss: 1.387297  [ 2128/ 3200]\n",
      "loss: 1.386595  [ 2144/ 3200]\n",
      "loss: 1.383742  [ 2160/ 3200]\n",
      "loss: 1.384230  [ 2176/ 3200]\n",
      "loss: 1.383421  [ 2192/ 3200]\n",
      "loss: 1.383101  [ 2208/ 3200]\n",
      "loss: 1.389235  [ 2224/ 3200]\n",
      "loss: 1.388169  [ 2240/ 3200]\n",
      "loss: 1.385208  [ 2256/ 3200]\n",
      "loss: 1.389829  [ 2272/ 3200]\n",
      "loss: 1.383741  [ 2288/ 3200]\n",
      "loss: 1.386638  [ 2304/ 3200]\n",
      "loss: 1.383849  [ 2320/ 3200]\n",
      "loss: 1.386870  [ 2336/ 3200]\n",
      "loss: 1.386337  [ 2352/ 3200]\n",
      "loss: 1.382017  [ 2368/ 3200]\n",
      "loss: 1.386425  [ 2384/ 3200]\n",
      "loss: 1.387830  [ 2400/ 3200]\n",
      "loss: 1.388915  [ 2416/ 3200]\n",
      "loss: 1.381529  [ 2432/ 3200]\n",
      "loss: 1.383806  [ 2448/ 3200]\n",
      "loss: 1.381164  [ 2464/ 3200]\n",
      "loss: 1.384166  [ 2480/ 3200]\n",
      "loss: 1.384230  [ 2496/ 3200]\n",
      "loss: 1.391066  [ 2512/ 3200]\n",
      "loss: 1.387998  [ 2528/ 3200]\n",
      "loss: 1.385847  [ 2544/ 3200]\n",
      "loss: 1.388641  [ 2560/ 3200]\n",
      "loss: 1.384548  [ 2576/ 3200]\n",
      "loss: 1.385954  [ 2592/ 3200]\n",
      "loss: 1.385465  [ 2608/ 3200]\n",
      "loss: 1.381971  [ 2624/ 3200]\n",
      "loss: 1.385038  [ 2640/ 3200]\n",
      "loss: 1.389766  [ 2656/ 3200]\n",
      "loss: 1.385147  [ 2672/ 3200]\n",
      "loss: 1.391553  [ 2688/ 3200]\n",
      "loss: 1.385145  [ 2704/ 3200]\n",
      "loss: 1.387189  [ 2720/ 3200]\n",
      "loss: 1.386380  [ 2736/ 3200]\n",
      "loss: 1.384339  [ 2752/ 3200]\n",
      "loss: 1.388214  [ 2768/ 3200]\n",
      "loss: 1.386381  [ 2784/ 3200]\n",
      "loss: 1.384170  [ 2800/ 3200]\n",
      "loss: 1.384828  [ 2816/ 3200]\n",
      "loss: 1.383376  [ 2832/ 3200]\n",
      "loss: 1.384337  [ 2848/ 3200]\n",
      "loss: 1.387404  [ 2864/ 3200]\n",
      "loss: 1.383377  [ 2880/ 3200]\n",
      "loss: 1.387464  [ 2896/ 3200]\n",
      "loss: 1.386428  [ 2912/ 3200]\n",
      "loss: 1.382995  [ 2928/ 3200]\n",
      "loss: 1.386547  [ 2944/ 3200]\n",
      "loss: 1.387722  [ 2960/ 3200]\n",
      "loss: 1.383909  [ 2976/ 3200]\n",
      "loss: 1.390682  [ 2992/ 3200]\n",
      "loss: 1.385679  [ 3008/ 3200]\n",
      "loss: 1.391491  [ 3024/ 3200]\n",
      "loss: 1.387631  [ 3040/ 3200]\n",
      "loss: 1.384126  [ 3056/ 3200]\n",
      "loss: 1.380679  [ 3072/ 3200]\n",
      "loss: 1.383421  [ 3088/ 3200]\n",
      "loss: 1.386701  [ 3104/ 3200]\n",
      "loss: 1.385847  [ 3120/ 3200]\n",
      "loss: 1.389339  [ 3136/ 3200]\n",
      "loss: 1.383801  [ 3152/ 3200]\n",
      "loss: 1.391869  [ 3168/ 3200]\n",
      "loss: 1.383529  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3889e-04.\n",
      "\n",
      "Epoch: 53\n",
      "-----------------------------\n",
      "loss: 1.386061  [    0/ 3200]\n",
      "loss: 1.388212  [   16/ 3200]\n",
      "loss: 1.384823  [   32/ 3200]\n",
      "loss: 1.387130  [   48/ 3200]\n",
      "loss: 1.381700  [   64/ 3200]\n",
      "loss: 1.386762  [   80/ 3200]\n",
      "loss: 1.382565  [   96/ 3200]\n",
      "loss: 1.387190  [  112/ 3200]\n",
      "loss: 1.385680  [  128/ 3200]\n",
      "loss: 1.387294  [  144/ 3200]\n",
      "loss: 1.386914  [  160/ 3200]\n",
      "loss: 1.388160  [  176/ 3200]\n",
      "loss: 1.388588  [  192/ 3200]\n",
      "loss: 1.389017  [  208/ 3200]\n",
      "loss: 1.390622  [  224/ 3200]\n",
      "loss: 1.386544  [  240/ 3200]\n",
      "loss: 1.385030  [  256/ 3200]\n",
      "loss: 1.387998  [  272/ 3200]\n",
      "loss: 1.390576  [  288/ 3200]\n",
      "loss: 1.381539  [  304/ 3200]\n",
      "loss: 1.391488  [  320/ 3200]\n",
      "loss: 1.388635  [  336/ 3200]\n",
      "loss: 1.387939  [  352/ 3200]\n",
      "loss: 1.383745  [  368/ 3200]\n",
      "loss: 1.390679  [  384/ 3200]\n",
      "loss: 1.390192  [  400/ 3200]\n",
      "loss: 1.387781  [  416/ 3200]\n",
      "loss: 1.380789  [  432/ 3200]\n",
      "loss: 1.384719  [  448/ 3200]\n",
      "loss: 1.387721  [  464/ 3200]\n",
      "loss: 1.386486  [  480/ 3200]\n",
      "loss: 1.389706  [  496/ 3200]\n",
      "loss: 1.386276  [  512/ 3200]\n",
      "loss: 1.386485  [  528/ 3200]\n",
      "loss: 1.389764  [  544/ 3200]\n",
      "loss: 1.387339  [  560/ 3200]\n",
      "loss: 1.386381  [  576/ 3200]\n",
      "loss: 1.385423  [  592/ 3200]\n",
      "loss: 1.380363  [  608/ 3200]\n",
      "loss: 1.393146  [  624/ 3200]\n",
      "loss: 1.390295  [  640/ 3200]\n",
      "loss: 1.385086  [  656/ 3200]\n",
      "loss: 1.382084  [  672/ 3200]\n",
      "loss: 1.384235  [  688/ 3200]\n",
      "loss: 1.384825  [  704/ 3200]\n",
      "loss: 1.382790  [  720/ 3200]\n",
      "loss: 1.382998  [  736/ 3200]\n",
      "loss: 1.387570  [  752/ 3200]\n",
      "loss: 1.391163  [  768/ 3200]\n",
      "loss: 1.385574  [  784/ 3200]\n",
      "loss: 1.389014  [  800/ 3200]\n",
      "loss: 1.386382  [  816/ 3200]\n",
      "loss: 1.388528  [  832/ 3200]\n",
      "loss: 1.382190  [  848/ 3200]\n",
      "loss: 1.383320  [  864/ 3200]\n",
      "loss: 1.383635  [  880/ 3200]\n",
      "loss: 1.390571  [  896/ 3200]\n",
      "loss: 1.387189  [  912/ 3200]\n",
      "loss: 1.385954  [  928/ 3200]\n",
      "loss: 1.386807  [  944/ 3200]\n",
      "loss: 1.388146  [  960/ 3200]\n",
      "loss: 1.383703  [  976/ 3200]\n",
      "loss: 1.384555  [  992/ 3200]\n",
      "loss: 1.386276  [ 1008/ 3200]\n",
      "loss: 1.385573  [ 1024/ 3200]\n",
      "loss: 1.388909  [ 1040/ 3200]\n",
      "loss: 1.383367  [ 1056/ 3200]\n",
      "loss: 1.385311  [ 1072/ 3200]\n",
      "loss: 1.387722  [ 1088/ 3200]\n",
      "loss: 1.389823  [ 1104/ 3200]\n",
      "loss: 1.384015  [ 1120/ 3200]\n",
      "loss: 1.388482  [ 1136/ 3200]\n",
      "loss: 1.385895  [ 1152/ 3200]\n",
      "loss: 1.386439  [ 1168/ 3200]\n",
      "loss: 1.381220  [ 1184/ 3200]\n",
      "loss: 1.385953  [ 1200/ 3200]\n",
      "loss: 1.388896  [ 1216/ 3200]\n",
      "loss: 1.385896  [ 1232/ 3200]\n",
      "loss: 1.388526  [ 1248/ 3200]\n",
      "loss: 1.386971  [ 1264/ 3200]\n",
      "loss: 1.386322  [ 1280/ 3200]\n",
      "loss: 1.381173  [ 1296/ 3200]\n",
      "loss: 1.385573  [ 1312/ 3200]\n",
      "loss: 1.385408  [ 1328/ 3200]\n",
      "loss: 1.385633  [ 1344/ 3200]\n",
      "loss: 1.390616  [ 1360/ 3200]\n",
      "loss: 1.385851  [ 1376/ 3200]\n",
      "loss: 1.385512  [ 1392/ 3200]\n",
      "loss: 1.387719  [ 1408/ 3200]\n",
      "loss: 1.385954  [ 1424/ 3200]\n",
      "loss: 1.383382  [ 1440/ 3200]\n",
      "loss: 1.385085  [ 1456/ 3200]\n",
      "loss: 1.383321  [ 1472/ 3200]\n",
      "loss: 1.387614  [ 1488/ 3200]\n",
      "loss: 1.379662  [ 1504/ 3200]\n",
      "loss: 1.390995  [ 1520/ 3200]\n",
      "loss: 1.388632  [ 1536/ 3200]\n",
      "loss: 1.381175  [ 1552/ 3200]\n",
      "loss: 1.388143  [ 1568/ 3200]\n",
      "loss: 1.383746  [ 1584/ 3200]\n",
      "loss: 1.384659  [ 1600/ 3200]\n",
      "loss: 1.390310  [ 1616/ 3200]\n",
      "loss: 1.383851  [ 1632/ 3200]\n",
      "loss: 1.389440  [ 1648/ 3200]\n",
      "loss: 1.376500  [ 1664/ 3200]\n",
      "loss: 1.388308  [ 1680/ 3200]\n",
      "loss: 1.384235  [ 1696/ 3200]\n",
      "loss: 1.381115  [ 1712/ 3200]\n",
      "loss: 1.389337  [ 1728/ 3200]\n",
      "loss: 1.385955  [ 1744/ 3200]\n",
      "loss: 1.385530  [ 1760/ 3200]\n",
      "loss: 1.383809  [ 1776/ 3200]\n",
      "loss: 1.382191  [ 1792/ 3200]\n",
      "loss: 1.386929  [ 1808/ 3200]\n",
      "loss: 1.387719  [ 1824/ 3200]\n",
      "loss: 1.382087  [ 1840/ 3200]\n",
      "loss: 1.387938  [ 1856/ 3200]\n",
      "loss: 1.390673  [ 1872/ 3200]\n",
      "loss: 1.390248  [ 1888/ 3200]\n",
      "loss: 1.388424  [ 1904/ 3200]\n",
      "loss: 1.383322  [ 1920/ 3200]\n",
      "loss: 1.390734  [ 1936/ 3200]\n",
      "loss: 1.384939  [ 1952/ 3200]\n",
      "loss: 1.385041  [ 1968/ 3200]\n",
      "loss: 1.385468  [ 1984/ 3200]\n",
      "loss: 1.384190  [ 2000/ 3200]\n",
      "loss: 1.379238  [ 2016/ 3200]\n",
      "loss: 1.384661  [ 2032/ 3200]\n",
      "loss: 1.383807  [ 2048/ 3200]\n",
      "loss: 1.388909  [ 2064/ 3200]\n",
      "loss: 1.387675  [ 2080/ 3200]\n",
      "loss: 1.388908  [ 2096/ 3200]\n",
      "loss: 1.387720  [ 2112/ 3200]\n",
      "loss: 1.384557  [ 2128/ 3200]\n",
      "loss: 1.385252  [ 2144/ 3200]\n",
      "loss: 1.384122  [ 2160/ 3200]\n",
      "loss: 1.385100  [ 2176/ 3200]\n",
      "loss: 1.389820  [ 2192/ 3200]\n",
      "loss: 1.388955  [ 2208/ 3200]\n",
      "loss: 1.389183  [ 2224/ 3200]\n",
      "loss: 1.389335  [ 2240/ 3200]\n",
      "loss: 1.392288  [ 2256/ 3200]\n",
      "loss: 1.389762  [ 2272/ 3200]\n",
      "loss: 1.389391  [ 2288/ 3200]\n",
      "loss: 1.389761  [ 2304/ 3200]\n",
      "loss: 1.389333  [ 2320/ 3200]\n",
      "loss: 1.386761  [ 2336/ 3200]\n",
      "loss: 1.380750  [ 2352/ 3200]\n",
      "loss: 1.387236  [ 2368/ 3200]\n",
      "loss: 1.383428  [ 2384/ 3200]\n",
      "loss: 1.387187  [ 2400/ 3200]\n",
      "loss: 1.383370  [ 2416/ 3200]\n",
      "loss: 1.383002  [ 2432/ 3200]\n",
      "loss: 1.382679  [ 2448/ 3200]\n",
      "loss: 1.387187  [ 2464/ 3200]\n",
      "loss: 1.384662  [ 2480/ 3200]\n",
      "loss: 1.391423  [ 2496/ 3200]\n",
      "loss: 1.387502  [ 2512/ 3200]\n",
      "loss: 1.385953  [ 2528/ 3200]\n",
      "loss: 1.387510  [ 2544/ 3200]\n",
      "loss: 1.387613  [ 2560/ 3200]\n",
      "loss: 1.390728  [ 2576/ 3200]\n",
      "loss: 1.378653  [ 2592/ 3200]\n",
      "loss: 1.390292  [ 2608/ 3200]\n",
      "loss: 1.390671  [ 2624/ 3200]\n",
      "loss: 1.386911  [ 2640/ 3200]\n",
      "loss: 1.387292  [ 2656/ 3200]\n",
      "loss: 1.383855  [ 2672/ 3200]\n",
      "loss: 1.382517  [ 2688/ 3200]\n",
      "loss: 1.385849  [ 2704/ 3200]\n",
      "loss: 1.388848  [ 2720/ 3200]\n",
      "loss: 1.379841  [ 2736/ 3200]\n",
      "loss: 1.379782  [ 2752/ 3200]\n",
      "loss: 1.384293  [ 2768/ 3200]\n",
      "loss: 1.388952  [ 2784/ 3200]\n",
      "loss: 1.387187  [ 2800/ 3200]\n",
      "loss: 1.389438  [ 2816/ 3200]\n",
      "loss: 1.389332  [ 2832/ 3200]\n",
      "loss: 1.388964  [ 2848/ 3200]\n",
      "loss: 1.385419  [ 2864/ 3200]\n",
      "loss: 1.380051  [ 2880/ 3200]\n",
      "loss: 1.380799  [ 2896/ 3200]\n",
      "loss: 1.384186  [ 2912/ 3200]\n",
      "loss: 1.391421  [ 2928/ 3200]\n",
      "loss: 1.387992  [ 2944/ 3200]\n",
      "loss: 1.392276  [ 2960/ 3200]\n",
      "loss: 1.384662  [ 2976/ 3200]\n",
      "loss: 1.384769  [ 2992/ 3200]\n",
      "loss: 1.386702  [ 3008/ 3200]\n",
      "loss: 1.386701  [ 3024/ 3200]\n",
      "loss: 1.383702  [ 3040/ 3200]\n",
      "loss: 1.388584  [ 3056/ 3200]\n",
      "loss: 1.388205  [ 3072/ 3200]\n",
      "loss: 1.384235  [ 3088/ 3200]\n",
      "loss: 1.391049  [ 3104/ 3200]\n",
      "loss: 1.395708  [ 3120/ 3200]\n",
      "loss: 1.384450  [ 3136/ 3200]\n",
      "loss: 1.388465  [ 3152/ 3200]\n",
      "loss: 1.383809  [ 3168/ 3200]\n",
      "loss: 1.387719  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3194e-04.\n",
      "\n",
      "Epoch: 54\n",
      "-----------------------------\n",
      "loss: 1.389437  [    0/ 3200]\n",
      "loss: 1.392337  [   16/ 3200]\n",
      "loss: 1.386002  [   32/ 3200]\n",
      "loss: 1.382150  [   48/ 3200]\n",
      "loss: 1.383752  [   64/ 3200]\n",
      "loss: 1.386381  [   80/ 3200]\n",
      "loss: 1.382520  [   96/ 3200]\n",
      "loss: 1.385953  [  112/ 3200]\n",
      "loss: 1.384236  [  128/ 3200]\n",
      "loss: 1.388525  [  144/ 3200]\n",
      "loss: 1.383324  [  160/ 3200]\n",
      "loss: 1.387614  [  176/ 3200]\n",
      "loss: 1.385362  [  192/ 3200]\n",
      "loss: 1.387292  [  208/ 3200]\n",
      "loss: 1.380965  [  224/ 3200]\n",
      "loss: 1.393563  [  240/ 3200]\n",
      "loss: 1.385791  [  256/ 3200]\n",
      "loss: 1.387554  [  272/ 3200]\n",
      "loss: 1.388690  [  288/ 3200]\n",
      "loss: 1.388572  [  304/ 3200]\n",
      "loss: 1.382034  [  320/ 3200]\n",
      "loss: 1.385042  [  336/ 3200]\n",
      "loss: 1.384284  [  352/ 3200]\n",
      "loss: 1.387672  [  368/ 3200]\n",
      "loss: 1.383325  [  384/ 3200]\n",
      "loss: 1.388523  [  400/ 3200]\n",
      "loss: 1.390725  [  416/ 3200]\n",
      "loss: 1.380542  [  432/ 3200]\n",
      "loss: 1.383915  [  448/ 3200]\n",
      "loss: 1.384237  [  464/ 3200]\n",
      "loss: 1.389377  [  480/ 3200]\n",
      "loss: 1.394803  [  496/ 3200]\n",
      "loss: 1.380862  [  512/ 3200]\n",
      "loss: 1.388845  [  528/ 3200]\n",
      "loss: 1.384937  [  544/ 3200]\n",
      "loss: 1.385090  [  560/ 3200]\n",
      "loss: 1.386806  [  576/ 3200]\n",
      "loss: 1.382520  [  592/ 3200]\n",
      "loss: 1.381714  [  608/ 3200]\n",
      "loss: 1.390561  [  624/ 3200]\n",
      "loss: 1.386926  [  640/ 3200]\n",
      "loss: 1.383325  [  656/ 3200]\n",
      "loss: 1.389164  [  672/ 3200]\n",
      "loss: 1.387186  [  688/ 3200]\n",
      "loss: 1.386913  [  704/ 3200]\n",
      "loss: 1.387244  [  720/ 3200]\n",
      "loss: 1.382841  [  736/ 3200]\n",
      "loss: 1.381183  [  752/ 3200]\n",
      "loss: 1.389708  [  768/ 3200]\n",
      "loss: 1.386488  [  784/ 3200]\n",
      "loss: 1.386806  [  800/ 3200]\n",
      "loss: 1.384508  [  816/ 3200]\n",
      "loss: 1.392704  [  832/ 3200]\n",
      "loss: 1.380863  [  848/ 3200]\n",
      "loss: 1.383113  [  864/ 3200]\n",
      "loss: 1.389328  [  880/ 3200]\n",
      "loss: 1.385149  [  896/ 3200]\n",
      "loss: 1.386061  [  912/ 3200]\n",
      "loss: 1.382522  [  928/ 3200]\n",
      "loss: 1.394953  [  944/ 3200]\n",
      "loss: 1.388950  [  960/ 3200]\n",
      "loss: 1.383917  [  976/ 3200]\n",
      "loss: 1.386759  [  992/ 3200]\n",
      "loss: 1.381774  [ 1008/ 3200]\n",
      "loss: 1.388901  [ 1024/ 3200]\n",
      "loss: 1.381183  [ 1040/ 3200]\n",
      "loss: 1.381397  [ 1056/ 3200]\n",
      "loss: 1.383860  [ 1072/ 3200]\n",
      "loss: 1.385633  [ 1088/ 3200]\n",
      "loss: 1.384022  [ 1104/ 3200]\n",
      "loss: 1.389491  [ 1120/ 3200]\n",
      "loss: 1.386325  [ 1136/ 3200]\n",
      "loss: 1.390077  [ 1152/ 3200]\n",
      "loss: 1.385681  [ 1168/ 3200]\n",
      "loss: 1.389707  [ 1184/ 3200]\n",
      "loss: 1.384828  [ 1200/ 3200]\n",
      "loss: 1.388579  [ 1216/ 3200]\n",
      "loss: 1.391254  [ 1232/ 3200]\n",
      "loss: 1.391091  [ 1248/ 3200]\n",
      "loss: 1.388523  [ 1264/ 3200]\n",
      "loss: 1.385526  [ 1280/ 3200]\n",
      "loss: 1.382951  [ 1296/ 3200]\n",
      "loss: 1.386751  [ 1312/ 3200]\n",
      "loss: 1.389270  [ 1328/ 3200]\n",
      "loss: 1.385527  [ 1344/ 3200]\n",
      "loss: 1.386702  [ 1360/ 3200]\n",
      "loss: 1.385848  [ 1376/ 3200]\n",
      "loss: 1.389705  [ 1392/ 3200]\n",
      "loss: 1.388950  [ 1408/ 3200]\n",
      "loss: 1.386381  [ 1424/ 3200]\n",
      "loss: 1.379093  [ 1440/ 3200]\n",
      "loss: 1.385364  [ 1456/ 3200]\n",
      "loss: 1.383113  [ 1472/ 3200]\n",
      "loss: 1.386380  [ 1488/ 3200]\n",
      "loss: 1.383813  [ 1504/ 3200]\n",
      "loss: 1.387773  [ 1520/ 3200]\n",
      "loss: 1.382202  [ 1536/ 3200]\n",
      "loss: 1.389006  [ 1552/ 3200]\n",
      "loss: 1.388199  [ 1568/ 3200]\n",
      "loss: 1.380809  [ 1584/ 3200]\n",
      "loss: 1.382096  [ 1600/ 3200]\n",
      "loss: 1.387935  [ 1616/ 3200]\n",
      "loss: 1.383540  [ 1632/ 3200]\n",
      "loss: 1.394038  [ 1648/ 3200]\n",
      "loss: 1.390936  [ 1664/ 3200]\n",
      "loss: 1.384561  [ 1680/ 3200]\n",
      "loss: 1.387233  [ 1696/ 3200]\n",
      "loss: 1.388522  [ 1712/ 3200]\n",
      "loss: 1.389221  [ 1728/ 3200]\n",
      "loss: 1.388949  [ 1744/ 3200]\n",
      "loss: 1.386118  [ 1760/ 3200]\n",
      "loss: 1.386165  [ 1776/ 3200]\n",
      "loss: 1.387128  [ 1792/ 3200]\n",
      "loss: 1.395266  [ 1808/ 3200]\n",
      "loss: 1.386807  [ 1824/ 3200]\n",
      "loss: 1.388046  [ 1840/ 3200]\n",
      "loss: 1.393228  [ 1856/ 3200]\n",
      "loss: 1.388095  [ 1872/ 3200]\n",
      "loss: 1.380435  [ 1888/ 3200]\n",
      "loss: 1.388094  [ 1904/ 3200]\n",
      "loss: 1.382099  [ 1920/ 3200]\n",
      "loss: 1.389004  [ 1936/ 3200]\n",
      "loss: 1.378188  [ 1952/ 3200]\n",
      "loss: 1.383814  [ 1968/ 3200]\n",
      "loss: 1.392262  [ 1984/ 3200]\n",
      "loss: 1.388201  [ 2000/ 3200]\n",
      "loss: 1.387291  [ 2016/ 3200]\n",
      "loss: 1.388888  [ 2032/ 3200]\n",
      "loss: 1.382584  [ 2048/ 3200]\n",
      "loss: 1.386749  [ 2064/ 3200]\n",
      "loss: 1.390176  [ 2080/ 3200]\n",
      "loss: 1.391948  [ 2096/ 3200]\n",
      "loss: 1.389703  [ 2112/ 3200]\n",
      "loss: 1.391463  [ 2128/ 3200]\n",
      "loss: 1.387230  [ 2144/ 3200]\n",
      "loss: 1.382482  [ 2160/ 3200]\n",
      "loss: 1.383331  [ 2176/ 3200]\n",
      "loss: 1.391038  [ 2192/ 3200]\n",
      "loss: 1.382573  [ 2208/ 3200]\n",
      "loss: 1.384620  [ 2224/ 3200]\n",
      "loss: 1.383331  [ 2240/ 3200]\n",
      "loss: 1.384240  [ 2256/ 3200]\n",
      "loss: 1.385149  [ 2272/ 3200]\n",
      "loss: 1.380871  [ 2288/ 3200]\n",
      "loss: 1.387505  [ 2304/ 3200]\n",
      "loss: 1.388152  [ 2320/ 3200]\n",
      "loss: 1.386059  [ 2336/ 3200]\n",
      "loss: 1.386380  [ 2352/ 3200]\n",
      "loss: 1.385682  [ 2368/ 3200]\n",
      "loss: 1.383862  [ 2384/ 3200]\n",
      "loss: 1.383757  [ 2400/ 3200]\n",
      "loss: 1.385424  [ 2416/ 3200]\n",
      "loss: 1.385528  [ 2432/ 3200]\n",
      "loss: 1.384134  [ 2448/ 3200]\n",
      "loss: 1.389381  [ 2464/ 3200]\n",
      "loss: 1.382207  [ 2480/ 3200]\n",
      "loss: 1.385633  [ 2496/ 3200]\n",
      "loss: 1.387667  [ 2512/ 3200]\n",
      "loss: 1.389807  [ 2528/ 3200]\n",
      "loss: 1.386381  [ 2544/ 3200]\n",
      "loss: 1.384241  [ 2560/ 3200]\n",
      "loss: 1.383330  [ 2576/ 3200]\n",
      "loss: 1.390126  [ 2592/ 3200]\n",
      "loss: 1.385044  [ 2608/ 3200]\n",
      "loss: 1.388570  [ 2624/ 3200]\n",
      "loss: 1.387717  [ 2640/ 3200]\n",
      "loss: 1.382577  [ 2656/ 3200]\n",
      "loss: 1.388471  [ 2672/ 3200]\n",
      "loss: 1.391889  [ 2688/ 3200]\n",
      "loss: 1.391945  [ 2704/ 3200]\n",
      "loss: 1.387339  [ 2720/ 3200]\n",
      "loss: 1.388518  [ 2736/ 3200]\n",
      "loss: 1.384135  [ 2752/ 3200]\n",
      "loss: 1.385521  [ 2768/ 3200]\n",
      "loss: 1.386756  [ 2784/ 3200]\n",
      "loss: 1.386274  [ 2800/ 3200]\n",
      "loss: 1.381725  [ 2816/ 3200]\n",
      "loss: 1.386651  [ 2832/ 3200]\n",
      "loss: 1.385469  [ 2848/ 3200]\n",
      "loss: 1.386543  [ 2864/ 3200]\n",
      "loss: 1.387716  [ 2880/ 3200]\n",
      "loss: 1.383011  [ 2896/ 3200]\n",
      "loss: 1.385521  [ 2912/ 3200]\n",
      "loss: 1.389216  [ 2928/ 3200]\n",
      "loss: 1.389158  [ 2944/ 3200]\n",
      "loss: 1.385151  [ 2960/ 3200]\n",
      "loss: 1.381300  [ 2976/ 3200]\n",
      "loss: 1.381725  [ 2992/ 3200]\n",
      "loss: 1.386701  [ 3008/ 3200]\n",
      "loss: 1.385577  [ 3024/ 3200]\n",
      "loss: 1.386701  [ 3040/ 3200]\n",
      "loss: 1.385896  [ 3056/ 3200]\n",
      "loss: 1.386380  [ 3072/ 3200]\n",
      "loss: 1.383437  [ 3088/ 3200]\n",
      "loss: 1.386748  [ 3104/ 3200]\n",
      "loss: 1.392748  [ 3120/ 3200]\n",
      "loss: 1.392747  [ 3136/ 3200]\n",
      "loss: 1.382694  [ 3152/ 3200]\n",
      "loss: 1.383276  [ 3168/ 3200]\n",
      "loss: 1.388199  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2534e-04.\n",
      "\n",
      "Epoch: 55\n",
      "-----------------------------\n",
      "loss: 1.388198  [    0/ 3200]\n",
      "loss: 1.388518  [   16/ 3200]\n",
      "loss: 1.388576  [   32/ 3200]\n",
      "loss: 1.387667  [   48/ 3200]\n",
      "loss: 1.385472  [   64/ 3200]\n",
      "loss: 1.394449  [   80/ 3200]\n",
      "loss: 1.386970  [   96/ 3200]\n",
      "loss: 1.386862  [  112/ 3200]\n",
      "loss: 1.387441  [  128/ 3200]\n",
      "loss: 1.383335  [  144/ 3200]\n",
      "loss: 1.390173  [  160/ 3200]\n",
      "loss: 1.387609  [  176/ 3200]\n",
      "loss: 1.386380  [  192/ 3200]\n",
      "loss: 1.387561  [  208/ 3200]\n",
      "loss: 1.390123  [  224/ 3200]\n",
      "loss: 1.385151  [  240/ 3200]\n",
      "loss: 1.389319  [  256/ 3200]\n",
      "loss: 1.390983  [  272/ 3200]\n",
      "loss: 1.390124  [  288/ 3200]\n",
      "loss: 1.385151  [  304/ 3200]\n",
      "loss: 1.388566  [  320/ 3200]\n",
      "loss: 1.390596  [  336/ 3200]\n",
      "loss: 1.385635  [  352/ 3200]\n",
      "loss: 1.383121  [  368/ 3200]\n",
      "loss: 1.389376  [  384/ 3200]\n",
      "loss: 1.390973  [  400/ 3200]\n",
      "loss: 1.390605  [  416/ 3200]\n",
      "loss: 1.386485  [  432/ 3200]\n",
      "loss: 1.389743  [  448/ 3200]\n",
      "loss: 1.385897  [  464/ 3200]\n",
      "loss: 1.388515  [  480/ 3200]\n",
      "loss: 1.388573  [  496/ 3200]\n",
      "loss: 1.380077  [  512/ 3200]\n",
      "loss: 1.387135  [  528/ 3200]\n",
      "loss: 1.387502  [  544/ 3200]\n",
      "loss: 1.381731  [  560/ 3200]\n",
      "loss: 1.385848  [  576/ 3200]\n",
      "loss: 1.389637  [  592/ 3200]\n",
      "loss: 1.383712  [  608/ 3200]\n",
      "loss: 1.384081  [  624/ 3200]\n",
      "loss: 1.381732  [  640/ 3200]\n",
      "loss: 1.381788  [  656/ 3200]\n",
      "loss: 1.388466  [  672/ 3200]\n",
      "loss: 1.385526  [  688/ 3200]\n",
      "loss: 1.387557  [  704/ 3200]\n",
      "loss: 1.388515  [  720/ 3200]\n",
      "loss: 1.383442  [  736/ 3200]\n",
      "loss: 1.385259  [  752/ 3200]\n",
      "loss: 1.386380  [  768/ 3200]\n",
      "loss: 1.382908  [  784/ 3200]\n",
      "loss: 1.388408  [  800/ 3200]\n",
      "loss: 1.394761  [  816/ 3200]\n",
      "loss: 1.393106  [  832/ 3200]\n",
      "loss: 1.392304  [  848/ 3200]\n",
      "loss: 1.379065  [  864/ 3200]\n",
      "loss: 1.386647  [  880/ 3200]\n",
      "loss: 1.384670  [  896/ 3200]\n",
      "loss: 1.384352  [  912/ 3200]\n",
      "loss: 1.387556  [  928/ 3200]\n",
      "loss: 1.383127  [  944/ 3200]\n",
      "loss: 1.387771  [  960/ 3200]\n",
      "loss: 1.384295  [  976/ 3200]\n",
      "loss: 1.387770  [  992/ 3200]\n",
      "loss: 1.380988  [ 1008/ 3200]\n",
      "loss: 1.385685  [ 1024/ 3200]\n",
      "loss: 1.382590  [ 1040/ 3200]\n",
      "loss: 1.387236  [ 1056/ 3200]\n",
      "loss: 1.388141  [ 1072/ 3200]\n",
      "loss: 1.387287  [ 1088/ 3200]\n",
      "loss: 1.388993  [ 1104/ 3200]\n",
      "loss: 1.377944  [ 1120/ 3200]\n",
      "loss: 1.389744  [ 1136/ 3200]\n",
      "loss: 1.383442  [ 1152/ 3200]\n",
      "loss: 1.386806  [ 1168/ 3200]\n",
      "loss: 1.388030  [ 1184/ 3200]\n",
      "loss: 1.384726  [ 1200/ 3200]\n",
      "loss: 1.385473  [ 1216/ 3200]\n",
      "loss: 1.384620  [ 1232/ 3200]\n",
      "loss: 1.388892  [ 1248/ 3200]\n",
      "loss: 1.382109  [ 1264/ 3200]\n",
      "loss: 1.390651  [ 1280/ 3200]\n",
      "loss: 1.389422  [ 1296/ 3200]\n",
      "loss: 1.387341  [ 1312/ 3200]\n",
      "loss: 1.391507  [ 1328/ 3200]\n",
      "loss: 1.385847  [ 1344/ 3200]\n",
      "loss: 1.384140  [ 1360/ 3200]\n",
      "loss: 1.388034  [ 1376/ 3200]\n",
      "loss: 1.387607  [ 1392/ 3200]\n",
      "loss: 1.382431  [ 1408/ 3200]\n",
      "loss: 1.381895  [ 1424/ 3200]\n",
      "loss: 1.384935  [ 1440/ 3200]\n",
      "loss: 1.384995  [ 1456/ 3200]\n",
      "loss: 1.388942  [ 1472/ 3200]\n",
      "loss: 1.380401  [ 1488/ 3200]\n",
      "loss: 1.381734  [ 1504/ 3200]\n",
      "loss: 1.383285  [ 1520/ 3200]\n",
      "loss: 1.386380  [ 1536/ 3200]\n",
      "loss: 1.388462  [ 1552/ 3200]\n",
      "loss: 1.390277  [ 1568/ 3200]\n",
      "loss: 1.384244  [ 1584/ 3200]\n",
      "loss: 1.389211  [ 1600/ 3200]\n",
      "loss: 1.383016  [ 1616/ 3200]\n",
      "loss: 1.389848  [ 1632/ 3200]\n",
      "loss: 1.384244  [ 1648/ 3200]\n",
      "loss: 1.388035  [ 1664/ 3200]\n",
      "loss: 1.388087  [ 1680/ 3200]\n",
      "loss: 1.382857  [ 1696/ 3200]\n",
      "loss: 1.387128  [ 1712/ 3200]\n",
      "loss: 1.382161  [ 1728/ 3200]\n",
      "loss: 1.386272  [ 1744/ 3200]\n",
      "loss: 1.383763  [ 1760/ 3200]\n",
      "loss: 1.386487  [ 1776/ 3200]\n",
      "loss: 1.389742  [ 1792/ 3200]\n",
      "loss: 1.381253  [ 1808/ 3200]\n",
      "loss: 1.388996  [ 1824/ 3200]\n",
      "loss: 1.383976  [ 1840/ 3200]\n",
      "loss: 1.383336  [ 1856/ 3200]\n",
      "loss: 1.385527  [ 1872/ 3200]\n",
      "loss: 1.382962  [ 1888/ 3200]\n",
      "loss: 1.388035  [ 1904/ 3200]\n",
      "loss: 1.386805  [ 1920/ 3200]\n",
      "loss: 1.391981  [ 1936/ 3200]\n",
      "loss: 1.384456  [ 1952/ 3200]\n",
      "loss: 1.390494  [ 1968/ 3200]\n",
      "loss: 1.378692  [ 1984/ 3200]\n",
      "loss: 1.380721  [ 2000/ 3200]\n",
      "loss: 1.390650  [ 2016/ 3200]\n",
      "loss: 1.383123  [ 2032/ 3200]\n",
      "loss: 1.385684  [ 2048/ 3200]\n",
      "loss: 1.396522  [ 2064/ 3200]\n",
      "loss: 1.390167  [ 2080/ 3200]\n",
      "loss: 1.390649  [ 2096/ 3200]\n",
      "loss: 1.388513  [ 2112/ 3200]\n",
      "loss: 1.381204  [ 2128/ 3200]\n",
      "loss: 1.387662  [ 2144/ 3200]\n",
      "loss: 1.392782  [ 2160/ 3200]\n",
      "loss: 1.383814  [ 2176/ 3200]\n",
      "loss: 1.388513  [ 2192/ 3200]\n",
      "loss: 1.386274  [ 2208/ 3200]\n",
      "loss: 1.383764  [ 2224/ 3200]\n",
      "loss: 1.388939  [ 2240/ 3200]\n",
      "loss: 1.385472  [ 2256/ 3200]\n",
      "loss: 1.385153  [ 2272/ 3200]\n",
      "loss: 1.385635  [ 2288/ 3200]\n",
      "loss: 1.390599  [ 2304/ 3200]\n",
      "loss: 1.385048  [ 2320/ 3200]\n",
      "loss: 1.383445  [ 2336/ 3200]\n",
      "loss: 1.388889  [ 2352/ 3200]\n",
      "loss: 1.379497  [ 2368/ 3200]\n",
      "loss: 1.381149  [ 2384/ 3200]\n",
      "loss: 1.385579  [ 2400/ 3200]\n",
      "loss: 1.383389  [ 2416/ 3200]\n",
      "loss: 1.392250  [ 2432/ 3200]\n",
      "loss: 1.383821  [ 2448/ 3200]\n",
      "loss: 1.384032  [ 2464/ 3200]\n",
      "loss: 1.380828  [ 2480/ 3200]\n",
      "loss: 1.384351  [ 2496/ 3200]\n",
      "loss: 1.386699  [ 2512/ 3200]\n",
      "loss: 1.386485  [ 2528/ 3200]\n",
      "loss: 1.383549  [ 2544/ 3200]\n",
      "loss: 1.379706  [ 2560/ 3200]\n",
      "loss: 1.386059  [ 2576/ 3200]\n",
      "loss: 1.388723  [ 2592/ 3200]\n",
      "loss: 1.386700  [ 2608/ 3200]\n",
      "loss: 1.378373  [ 2624/ 3200]\n",
      "loss: 1.385424  [ 2640/ 3200]\n",
      "loss: 1.390222  [ 2656/ 3200]\n",
      "loss: 1.391876  [ 2672/ 3200]\n",
      "loss: 1.385152  [ 2688/ 3200]\n",
      "loss: 1.390222  [ 2704/ 3200]\n",
      "loss: 1.389364  [ 2720/ 3200]\n",
      "loss: 1.388513  [ 2736/ 3200]\n",
      "loss: 1.383764  [ 2752/ 3200]\n",
      "loss: 1.389692  [ 2768/ 3200]\n",
      "loss: 1.390701  [ 2784/ 3200]\n",
      "loss: 1.387550  [ 2800/ 3200]\n",
      "loss: 1.382161  [ 2816/ 3200]\n",
      "loss: 1.393476  [ 2832/ 3200]\n",
      "loss: 1.387182  [ 2848/ 3200]\n",
      "loss: 1.386756  [ 2864/ 3200]\n",
      "loss: 1.385899  [ 2880/ 3200]\n",
      "loss: 1.391928  [ 2896/ 3200]\n",
      "loss: 1.385048  [ 2912/ 3200]\n",
      "loss: 1.384085  [ 2928/ 3200]\n",
      "loss: 1.385578  [ 2944/ 3200]\n",
      "loss: 1.379925  [ 2960/ 3200]\n",
      "loss: 1.391872  [ 2976/ 3200]\n",
      "loss: 1.385153  [ 2992/ 3200]\n",
      "loss: 1.388088  [ 3008/ 3200]\n",
      "loss: 1.378168  [ 3024/ 3200]\n",
      "loss: 1.391341  [ 3040/ 3200]\n",
      "loss: 1.388831  [ 3056/ 3200]\n",
      "loss: 1.385578  [ 3072/ 3200]\n",
      "loss: 1.387286  [ 3088/ 3200]\n",
      "loss: 1.381207  [ 3104/ 3200]\n",
      "loss: 1.383021  [ 3120/ 3200]\n",
      "loss: 1.387769  [ 3136/ 3200]\n",
      "loss: 1.385154  [ 3152/ 3200]\n",
      "loss: 1.382163  [ 3168/ 3200]\n",
      "loss: 1.385579  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1908e-04.\n",
      "\n",
      "Epoch: 56\n",
      "-----------------------------\n",
      "loss: 1.387768  [    0/ 3200]\n",
      "loss: 1.388136  [   16/ 3200]\n",
      "loss: 1.385636  [   32/ 3200]\n",
      "loss: 1.386757  [   48/ 3200]\n",
      "loss: 1.386755  [   64/ 3200]\n",
      "loss: 1.386805  [   80/ 3200]\n",
      "loss: 1.387448  [   96/ 3200]\n",
      "loss: 1.385529  [  112/ 3200]\n",
      "loss: 1.385793  [  128/ 3200]\n",
      "loss: 1.389793  [  144/ 3200]\n",
      "loss: 1.388031  [  160/ 3200]\n",
      "loss: 1.388511  [  176/ 3200]\n",
      "loss: 1.385153  [  192/ 3200]\n",
      "loss: 1.383767  [  208/ 3200]\n",
      "loss: 1.380783  [  224/ 3200]\n",
      "loss: 1.383928  [  240/ 3200]\n",
      "loss: 1.385793  [  256/ 3200]\n",
      "loss: 1.383502  [  272/ 3200]\n",
      "loss: 1.382219  [  288/ 3200]\n",
      "loss: 1.389524  [  304/ 3200]\n",
      "loss: 1.387711  [  320/ 3200]\n",
      "loss: 1.391502  [  336/ 3200]\n",
      "loss: 1.383286  [  352/ 3200]\n",
      "loss: 1.387661  [  368/ 3200]\n",
      "loss: 1.382965  [  384/ 3200]\n",
      "loss: 1.380300  [  400/ 3200]\n",
      "loss: 1.384672  [  416/ 3200]\n",
      "loss: 1.389256  [  432/ 3200]\n",
      "loss: 1.382645  [  448/ 3200]\n",
      "loss: 1.387710  [  464/ 3200]\n",
      "loss: 1.386860  [  480/ 3200]\n",
      "loss: 1.386218  [  496/ 3200]\n",
      "loss: 1.389312  [  512/ 3200]\n",
      "loss: 1.383021  [  528/ 3200]\n",
      "loss: 1.383765  [  544/ 3200]\n",
      "loss: 1.388456  [  560/ 3200]\n",
      "loss: 1.386861  [  576/ 3200]\n",
      "loss: 1.388405  [  592/ 3200]\n",
      "loss: 1.383609  [  608/ 3200]\n",
      "loss: 1.385098  [  624/ 3200]\n",
      "loss: 1.383717  [  640/ 3200]\n",
      "loss: 1.390652  [  656/ 3200]\n",
      "loss: 1.386753  [  672/ 3200]\n",
      "loss: 1.391126  [  688/ 3200]\n",
      "loss: 1.388938  [  704/ 3200]\n",
      "loss: 1.389313  [  720/ 3200]\n",
      "loss: 1.386755  [  736/ 3200]\n",
      "loss: 1.386323  [  752/ 3200]\n",
      "loss: 1.387286  [  768/ 3200]\n",
      "loss: 1.384942  [  784/ 3200]\n",
      "loss: 1.387231  [  800/ 3200]\n",
      "loss: 1.386804  [  816/ 3200]\n",
      "loss: 1.387285  [  832/ 3200]\n",
      "loss: 1.386699  [  848/ 3200]\n",
      "loss: 1.387717  [  864/ 3200]\n",
      "loss: 1.385629  [  880/ 3200]\n",
      "loss: 1.384729  [  896/ 3200]\n",
      "loss: 1.383872  [  912/ 3200]\n",
      "loss: 1.390642  [  928/ 3200]\n",
      "loss: 1.381210  [  944/ 3200]\n",
      "loss: 1.387711  [  960/ 3200]\n",
      "loss: 1.384567  [  976/ 3200]\n",
      "loss: 1.384566  [  992/ 3200]\n",
      "loss: 1.383341  [ 1008/ 3200]\n",
      "loss: 1.385898  [ 1024/ 3200]\n",
      "loss: 1.386910  [ 1040/ 3200]\n",
      "loss: 1.386379  [ 1056/ 3200]\n",
      "loss: 1.382541  [ 1072/ 3200]\n",
      "loss: 1.386004  [ 1088/ 3200]\n",
      "loss: 1.389842  [ 1104/ 3200]\n",
      "loss: 1.387122  [ 1120/ 3200]\n",
      "loss: 1.389793  [ 1136/ 3200]\n",
      "loss: 1.389630  [ 1152/ 3200]\n",
      "loss: 1.389310  [ 1168/ 3200]\n",
      "loss: 1.388511  [ 1184/ 3200]\n",
      "loss: 1.387873  [ 1200/ 3200]\n",
      "loss: 1.382437  [ 1216/ 3200]\n",
      "loss: 1.384249  [ 1232/ 3200]\n",
      "loss: 1.379823  [ 1248/ 3200]\n",
      "loss: 1.388029  [ 1264/ 3200]\n",
      "loss: 1.382600  [ 1280/ 3200]\n",
      "loss: 1.381692  [ 1296/ 3200]\n",
      "loss: 1.386004  [ 1312/ 3200]\n",
      "loss: 1.382599  [ 1328/ 3200]\n",
      "loss: 1.391549  [ 1344/ 3200]\n",
      "loss: 1.390218  [ 1360/ 3200]\n",
      "loss: 1.389793  [ 1376/ 3200]\n",
      "loss: 1.381317  [ 1392/ 3200]\n",
      "loss: 1.380787  [ 1408/ 3200]\n",
      "loss: 1.385155  [ 1424/ 3200]\n",
      "loss: 1.385098  [ 1440/ 3200]\n",
      "loss: 1.384143  [ 1456/ 3200]\n",
      "loss: 1.391392  [ 1472/ 3200]\n",
      "loss: 1.383716  [ 1488/ 3200]\n",
      "loss: 1.389362  [ 1504/ 3200]\n",
      "loss: 1.387554  [ 1520/ 3200]\n",
      "loss: 1.386324  [ 1536/ 3200]\n",
      "loss: 1.391815  [ 1552/ 3200]\n",
      "loss: 1.380036  [ 1568/ 3200]\n",
      "loss: 1.386112  [ 1584/ 3200]\n",
      "loss: 1.389735  [ 1600/ 3200]\n",
      "loss: 1.389310  [ 1616/ 3200]\n",
      "loss: 1.391922  [ 1632/ 3200]\n",
      "loss: 1.388942  [ 1648/ 3200]\n",
      "loss: 1.388935  [ 1664/ 3200]\n",
      "loss: 1.383450  [ 1680/ 3200]\n",
      "loss: 1.387661  [ 1696/ 3200]\n",
      "loss: 1.381263  [ 1712/ 3200]\n",
      "loss: 1.383506  [ 1728/ 3200]\n",
      "loss: 1.388879  [ 1744/ 3200]\n",
      "loss: 1.383767  [ 1760/ 3200]\n",
      "loss: 1.384249  [ 1776/ 3200]\n",
      "loss: 1.390059  [ 1792/ 3200]\n",
      "loss: 1.386063  [ 1808/ 3200]\n",
      "loss: 1.385897  [ 1824/ 3200]\n",
      "loss: 1.387336  [ 1840/ 3200]\n",
      "loss: 1.388934  [ 1856/ 3200]\n",
      "loss: 1.388460  [ 1872/ 3200]\n",
      "loss: 1.388085  [ 1888/ 3200]\n",
      "loss: 1.387343  [ 1904/ 3200]\n",
      "loss: 1.383132  [ 1920/ 3200]\n",
      "loss: 1.391064  [ 1936/ 3200]\n",
      "loss: 1.386272  [ 1952/ 3200]\n",
      "loss: 1.380096  [ 1968/ 3200]\n",
      "loss: 1.385156  [ 1984/ 3200]\n",
      "loss: 1.387179  [ 2000/ 3200]\n",
      "loss: 1.378497  [ 2016/ 3200]\n",
      "loss: 1.387709  [ 2032/ 3200]\n",
      "loss: 1.391065  [ 2048/ 3200]\n",
      "loss: 1.383450  [ 2064/ 3200]\n",
      "loss: 1.385211  [ 2080/ 3200]\n",
      "loss: 1.387709  [ 2096/ 3200]\n",
      "loss: 1.389253  [ 2112/ 3200]\n",
      "loss: 1.382121  [ 2128/ 3200]\n",
      "loss: 1.390584  [ 2144/ 3200]\n",
      "loss: 1.382969  [ 2160/ 3200]\n",
      "loss: 1.383768  [ 2176/ 3200]\n",
      "loss: 1.383825  [ 2192/ 3200]\n",
      "loss: 1.382812  [ 2208/ 3200]\n",
      "loss: 1.385898  [ 2224/ 3200]\n",
      "loss: 1.382862  [ 2240/ 3200]\n",
      "loss: 1.386330  [ 2256/ 3200]\n",
      "loss: 1.384461  [ 2272/ 3200]\n",
      "loss: 1.382600  [ 2288/ 3200]\n",
      "loss: 1.385530  [ 2304/ 3200]\n",
      "loss: 1.382544  [ 2320/ 3200]\n",
      "loss: 1.385049  [ 2336/ 3200]\n",
      "loss: 1.388565  [ 2352/ 3200]\n",
      "loss: 1.388559  [ 2368/ 3200]\n",
      "loss: 1.392295  [ 2384/ 3200]\n",
      "loss: 1.394899  [ 2400/ 3200]\n",
      "loss: 1.385579  [ 2416/ 3200]\n",
      "loss: 1.381850  [ 2432/ 3200]\n",
      "loss: 1.388558  [ 2448/ 3200]\n",
      "loss: 1.381216  [ 2464/ 3200]\n",
      "loss: 1.387228  [ 2480/ 3200]\n",
      "loss: 1.390214  [ 2496/ 3200]\n",
      "loss: 1.387709  [ 2512/ 3200]\n",
      "loss: 1.382545  [ 2528/ 3200]\n",
      "loss: 1.386002  [ 2544/ 3200]\n",
      "loss: 1.386227  [ 2560/ 3200]\n",
      "loss: 1.389734  [ 2576/ 3200]\n",
      "loss: 1.387815  [ 2592/ 3200]\n",
      "loss: 1.387237  [ 2608/ 3200]\n",
      "loss: 1.386802  [ 2624/ 3200]\n",
      "loss: 1.387555  [ 2640/ 3200]\n",
      "loss: 1.385849  [ 2656/ 3200]\n",
      "loss: 1.388940  [ 2672/ 3200]\n",
      "loss: 1.385210  [ 2688/ 3200]\n",
      "loss: 1.385474  [ 2704/ 3200]\n",
      "loss: 1.386854  [ 2720/ 3200]\n",
      "loss: 1.381641  [ 2736/ 3200]\n",
      "loss: 1.386003  [ 2752/ 3200]\n",
      "loss: 1.391388  [ 2768/ 3200]\n",
      "loss: 1.388028  [ 2784/ 3200]\n",
      "loss: 1.387602  [ 2800/ 3200]\n",
      "loss: 1.382915  [ 2816/ 3200]\n",
      "loss: 1.385155  [ 2832/ 3200]\n",
      "loss: 1.386746  [ 2848/ 3200]\n",
      "loss: 1.381960  [ 2864/ 3200]\n",
      "loss: 1.382545  [ 2880/ 3200]\n",
      "loss: 1.389038  [ 2896/ 3200]\n",
      "loss: 1.385579  [ 2912/ 3200]\n",
      "loss: 1.389470  [ 2928/ 3200]\n",
      "loss: 1.389260  [ 2944/ 3200]\n",
      "loss: 1.382864  [ 2960/ 3200]\n",
      "loss: 1.393140  [ 2976/ 3200]\n",
      "loss: 1.391011  [ 2992/ 3200]\n",
      "loss: 1.385473  [ 3008/ 3200]\n",
      "loss: 1.384731  [ 3024/ 3200]\n",
      "loss: 1.388557  [ 3040/ 3200]\n",
      "loss: 1.391810  [ 3056/ 3200]\n",
      "loss: 1.393139  [ 3072/ 3200]\n",
      "loss: 1.380477  [ 3088/ 3200]\n",
      "loss: 1.386003  [ 3104/ 3200]\n",
      "loss: 1.385792  [ 3120/ 3200]\n",
      "loss: 1.387227  [ 3136/ 3200]\n",
      "loss: 1.390422  [ 3152/ 3200]\n",
      "loss: 1.388507  [ 3168/ 3200]\n",
      "loss: 1.385156  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1312e-04.\n",
      "\n",
      "Epoch: 57\n",
      "-----------------------------\n",
      "loss: 1.387121  [    0/ 3200]\n",
      "loss: 1.385531  [   16/ 3200]\n",
      "loss: 1.386696  [   32/ 3200]\n",
      "loss: 1.381750  [   48/ 3200]\n",
      "loss: 1.387601  [   64/ 3200]\n",
      "loss: 1.379726  [   80/ 3200]\n",
      "loss: 1.385099  [   96/ 3200]\n",
      "loss: 1.386965  [  112/ 3200]\n",
      "loss: 1.388719  [  128/ 3200]\n",
      "loss: 1.391798  [  144/ 3200]\n",
      "loss: 1.387555  [  160/ 3200]\n",
      "loss: 1.391480  [  176/ 3200]\n",
      "loss: 1.387707  [  192/ 3200]\n",
      "loss: 1.387447  [  208/ 3200]\n",
      "loss: 1.386379  [  224/ 3200]\n",
      "loss: 1.388072  [  240/ 3200]\n",
      "loss: 1.385474  [  256/ 3200]\n",
      "loss: 1.388024  [  272/ 3200]\n",
      "loss: 1.387919  [  288/ 3200]\n",
      "loss: 1.389727  [  304/ 3200]\n",
      "loss: 1.386274  [  320/ 3200]\n",
      "loss: 1.384206  [  336/ 3200]\n",
      "loss: 1.380152  [  352/ 3200]\n",
      "loss: 1.388130  [  368/ 3200]\n",
      "loss: 1.382066  [  384/ 3200]\n",
      "loss: 1.386801  [  400/ 3200]\n",
      "loss: 1.385851  [  416/ 3200]\n",
      "loss: 1.388445  [  432/ 3200]\n",
      "loss: 1.383981  [  448/ 3200]\n",
      "loss: 1.386274  [  464/ 3200]\n",
      "loss: 1.388460  [  480/ 3200]\n",
      "loss: 1.386318  [  496/ 3200]\n",
      "loss: 1.385262  [  512/ 3200]\n",
      "loss: 1.389199  [  528/ 3200]\n",
      "loss: 1.387133  [  544/ 3200]\n",
      "loss: 1.387706  [  560/ 3200]\n",
      "loss: 1.386695  [  576/ 3200]\n",
      "loss: 1.384523  [  592/ 3200]\n",
      "loss: 1.386212  [  608/ 3200]\n",
      "loss: 1.387662  [  624/ 3200]\n",
      "loss: 1.389725  [  640/ 3200]\n",
      "loss: 1.386908  [  656/ 3200]\n",
      "loss: 1.385219  [  672/ 3200]\n",
      "loss: 1.387554  [  688/ 3200]\n",
      "loss: 1.385955  [  704/ 3200]\n",
      "loss: 1.384359  [  720/ 3200]\n",
      "loss: 1.386110  [  736/ 3200]\n",
      "loss: 1.390525  [  752/ 3200]\n",
      "loss: 1.389726  [  768/ 3200]\n",
      "loss: 1.384676  [  784/ 3200]\n",
      "loss: 1.388023  [  800/ 3200]\n",
      "loss: 1.386273  [  816/ 3200]\n",
      "loss: 1.388926  [  832/ 3200]\n",
      "loss: 1.390147  [  848/ 3200]\n",
      "loss: 1.379626  [  864/ 3200]\n",
      "loss: 1.386272  [  880/ 3200]\n",
      "loss: 1.388023  [  896/ 3200]\n",
      "loss: 1.388021  [  912/ 3200]\n",
      "loss: 1.390208  [  928/ 3200]\n",
      "loss: 1.386379  [  944/ 3200]\n",
      "loss: 1.383032  [  960/ 3200]\n",
      "loss: 1.395195  [  976/ 3200]\n",
      "loss: 1.385053  [  992/ 3200]\n",
      "loss: 1.385097  [ 1008/ 3200]\n",
      "loss: 1.387284  [ 1024/ 3200]\n",
      "loss: 1.385850  [ 1040/ 3200]\n",
      "loss: 1.389470  [ 1056/ 3200]\n",
      "loss: 1.386754  [ 1072/ 3200]\n",
      "loss: 1.388880  [ 1088/ 3200]\n",
      "loss: 1.386486  [ 1104/ 3200]\n",
      "loss: 1.389256  [ 1120/ 3200]\n",
      "loss: 1.390582  [ 1136/ 3200]\n",
      "loss: 1.384677  [ 1152/ 3200]\n",
      "loss: 1.382491  [ 1168/ 3200]\n",
      "loss: 1.388772  [ 1184/ 3200]\n",
      "loss: 1.388396  [ 1200/ 3200]\n",
      "loss: 1.386003  [ 1216/ 3200]\n",
      "loss: 1.381754  [ 1232/ 3200]\n",
      "loss: 1.377021  [ 1248/ 3200]\n",
      "loss: 1.390629  [ 1264/ 3200]\n",
      "loss: 1.385850  [ 1280/ 3200]\n",
      "loss: 1.379629  [ 1296/ 3200]\n",
      "loss: 1.388503  [ 1312/ 3200]\n",
      "loss: 1.385098  [ 1328/ 3200]\n",
      "loss: 1.386695  [ 1344/ 3200]\n",
      "loss: 1.385957  [ 1360/ 3200]\n",
      "loss: 1.388673  [ 1376/ 3200]\n",
      "loss: 1.392860  [ 1392/ 3200]\n",
      "loss: 1.384362  [ 1408/ 3200]\n",
      "loss: 1.386272  [ 1424/ 3200]\n",
      "loss: 1.384146  [ 1440/ 3200]\n",
      "loss: 1.385052  [ 1456/ 3200]\n",
      "loss: 1.386064  [ 1472/ 3200]\n",
      "loss: 1.388879  [ 1488/ 3200]\n",
      "loss: 1.381717  [ 1504/ 3200]\n",
      "loss: 1.383456  [ 1520/ 3200]\n",
      "loss: 1.384254  [ 1536/ 3200]\n",
      "loss: 1.389034  [ 1552/ 3200]\n",
      "loss: 1.386064  [ 1568/ 3200]\n",
      "loss: 1.388773  [ 1584/ 3200]\n",
      "loss: 1.381814  [ 1600/ 3200]\n",
      "loss: 1.387284  [ 1616/ 3200]\n",
      "loss: 1.384360  [ 1632/ 3200]\n",
      "loss: 1.390944  [ 1648/ 3200]\n",
      "loss: 1.388986  [ 1664/ 3200]\n",
      "loss: 1.386318  [ 1680/ 3200]\n",
      "loss: 1.389830  [ 1696/ 3200]\n",
      "loss: 1.385836  [ 1712/ 3200]\n",
      "loss: 1.385897  [ 1728/ 3200]\n",
      "loss: 1.384844  [ 1744/ 3200]\n",
      "loss: 1.389783  [ 1760/ 3200]\n",
      "loss: 1.386379  [ 1776/ 3200]\n",
      "loss: 1.386002  [ 1792/ 3200]\n",
      "loss: 1.388925  [ 1808/ 3200]\n",
      "loss: 1.384571  [ 1824/ 3200]\n",
      "loss: 1.390566  [ 1840/ 3200]\n",
      "loss: 1.381227  [ 1856/ 3200]\n",
      "loss: 1.385518  [ 1872/ 3200]\n",
      "loss: 1.386020  [ 1888/ 3200]\n",
      "loss: 1.389721  [ 1904/ 3200]\n",
      "loss: 1.383457  [ 1920/ 3200]\n",
      "loss: 1.387703  [ 1936/ 3200]\n",
      "loss: 1.383878  [ 1952/ 3200]\n",
      "loss: 1.383940  [ 1968/ 3200]\n",
      "loss: 1.386379  [ 1984/ 3200]\n",
      "loss: 1.391696  [ 2000/ 3200]\n",
      "loss: 1.383142  [ 2016/ 3200]\n",
      "loss: 1.386063  [ 2032/ 3200]\n",
      "loss: 1.383940  [ 2048/ 3200]\n",
      "loss: 1.384570  [ 2064/ 3200]\n",
      "loss: 1.391002  [ 2080/ 3200]\n",
      "loss: 1.386800  [ 2096/ 3200]\n",
      "loss: 1.387975  [ 2112/ 3200]\n",
      "loss: 1.382238  [ 2128/ 3200]\n",
      "loss: 1.380912  [ 2144/ 3200]\n",
      "loss: 1.383667  [ 2160/ 3200]\n",
      "loss: 1.383833  [ 2176/ 3200]\n",
      "loss: 1.387221  [ 2192/ 3200]\n",
      "loss: 1.386319  [ 2208/ 3200]\n",
      "loss: 1.385012  [ 2224/ 3200]\n",
      "loss: 1.385957  [ 2240/ 3200]\n",
      "loss: 1.385053  [ 2256/ 3200]\n",
      "loss: 1.385369  [ 2272/ 3200]\n",
      "loss: 1.387115  [ 2288/ 3200]\n",
      "loss: 1.383834  [ 2304/ 3200]\n",
      "loss: 1.387975  [ 2320/ 3200]\n",
      "loss: 1.388501  [ 2336/ 3200]\n",
      "loss: 1.382660  [ 2352/ 3200]\n",
      "loss: 1.386755  [ 2368/ 3200]\n",
      "loss: 1.393924  [ 2384/ 3200]\n",
      "loss: 1.391316  [ 2400/ 3200]\n",
      "loss: 1.386018  [ 2416/ 3200]\n",
      "loss: 1.386438  [ 2432/ 3200]\n",
      "loss: 1.381758  [ 2448/ 3200]\n",
      "loss: 1.385052  [ 2464/ 3200]\n",
      "loss: 1.386110  [ 2480/ 3200]\n",
      "loss: 1.385474  [ 2496/ 3200]\n",
      "loss: 1.383458  [ 2512/ 3200]\n",
      "loss: 1.390264  [ 2528/ 3200]\n",
      "loss: 1.381757  [ 2544/ 3200]\n",
      "loss: 1.389300  [ 2560/ 3200]\n",
      "loss: 1.385100  [ 2576/ 3200]\n",
      "loss: 1.386485  [ 2592/ 3200]\n",
      "loss: 1.379844  [ 2608/ 3200]\n",
      "loss: 1.382238  [ 2624/ 3200]\n",
      "loss: 1.381018  [ 2640/ 3200]\n",
      "loss: 1.385054  [ 2656/ 3200]\n",
      "loss: 1.393230  [ 2672/ 3200]\n",
      "loss: 1.385158  [ 2688/ 3200]\n",
      "loss: 1.385791  [ 2704/ 3200]\n",
      "loss: 1.382132  [ 2720/ 3200]\n",
      "loss: 1.387765  [ 2736/ 3200]\n",
      "loss: 1.392328  [ 2752/ 3200]\n",
      "loss: 1.387598  [ 2768/ 3200]\n",
      "loss: 1.386063  [ 2784/ 3200]\n",
      "loss: 1.385006  [ 2800/ 3200]\n",
      "loss: 1.384256  [ 2816/ 3200]\n",
      "loss: 1.389406  [ 2832/ 3200]\n",
      "loss: 1.386800  [ 2848/ 3200]\n",
      "loss: 1.386752  [ 2864/ 3200]\n",
      "loss: 1.386802  [ 2880/ 3200]\n",
      "loss: 1.383517  [ 2896/ 3200]\n",
      "loss: 1.388021  [ 2912/ 3200]\n",
      "loss: 1.387283  [ 2928/ 3200]\n",
      "loss: 1.386485  [ 2944/ 3200]\n",
      "loss: 1.394448  [ 2960/ 3200]\n",
      "loss: 1.380011  [ 2976/ 3200]\n",
      "loss: 1.385850  [ 2992/ 3200]\n",
      "loss: 1.384572  [ 3008/ 3200]\n",
      "loss: 1.394341  [ 3024/ 3200]\n",
      "loss: 1.384737  [ 3040/ 3200]\n",
      "loss: 1.387656  [ 3056/ 3200]\n",
      "loss: 1.382030  [ 3072/ 3200]\n",
      "loss: 1.388185  [ 3088/ 3200]\n",
      "loss: 1.387819  [ 3104/ 3200]\n",
      "loss: 1.387866  [ 3120/ 3200]\n",
      "loss: 1.383827  [ 3136/ 3200]\n",
      "loss: 1.386802  [ 3152/ 3200]\n",
      "loss: 1.381711  [ 3168/ 3200]\n",
      "loss: 1.384362  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0747e-04.\n",
      "\n",
      "Epoch: 58\n",
      "-----------------------------\n",
      "loss: 1.388924  [    0/ 3200]\n",
      "loss: 1.390633  [   16/ 3200]\n",
      "loss: 1.391469  [   32/ 3200]\n",
      "loss: 1.387656  [   48/ 3200]\n",
      "loss: 1.379694  [   64/ 3200]\n",
      "loss: 1.379693  [   80/ 3200]\n",
      "loss: 1.385956  [   96/ 3200]\n",
      "loss: 1.382076  [  112/ 3200]\n",
      "loss: 1.378360  [  128/ 3200]\n",
      "loss: 1.383458  [  144/ 3200]\n",
      "loss: 1.386706  [  160/ 3200]\n",
      "loss: 1.385101  [  176/ 3200]\n",
      "loss: 1.386753  [  192/ 3200]\n",
      "loss: 1.390147  [  208/ 3200]\n",
      "loss: 1.395620  [  224/ 3200]\n",
      "loss: 1.391372  [  240/ 3200]\n",
      "loss: 1.382451  [  256/ 3200]\n",
      "loss: 1.387281  [  272/ 3200]\n",
      "loss: 1.391844  [  288/ 3200]\n",
      "loss: 1.391794  [  304/ 3200]\n",
      "loss: 1.395185  [  320/ 3200]\n",
      "loss: 1.383353  [  336/ 3200]\n",
      "loss: 1.384679  [  352/ 3200]\n",
      "loss: 1.385638  [  368/ 3200]\n",
      "loss: 1.389296  [  384/ 3200]\n",
      "loss: 1.382191  [  400/ 3200]\n",
      "loss: 1.386011  [  416/ 3200]\n",
      "loss: 1.385532  [  432/ 3200]\n",
      "loss: 1.389720  [  448/ 3200]\n",
      "loss: 1.389297  [  464/ 3200]\n",
      "loss: 1.389085  [  480/ 3200]\n",
      "loss: 1.384094  [  496/ 3200]\n",
      "loss: 1.380542  [  512/ 3200]\n",
      "loss: 1.390940  [  528/ 3200]\n",
      "loss: 1.386539  [  544/ 3200]\n",
      "loss: 1.386697  [  560/ 3200]\n",
      "loss: 1.391469  [  576/ 3200]\n",
      "loss: 1.392688  [  592/ 3200]\n",
      "loss: 1.383251  [  608/ 3200]\n",
      "loss: 1.385533  [  624/ 3200]\n",
      "loss: 1.390939  [  640/ 3200]\n",
      "loss: 1.386434  [  656/ 3200]\n",
      "loss: 1.383355  [  672/ 3200]\n",
      "loss: 1.382930  [  688/ 3200]\n",
      "loss: 1.387971  [  704/ 3200]\n",
      "loss: 1.386219  [  720/ 3200]\n",
      "loss: 1.385900  [  736/ 3200]\n",
      "loss: 1.388923  [  752/ 3200]\n",
      "loss: 1.381713  [  768/ 3200]\n",
      "loss: 1.387703  [  784/ 3200]\n",
      "loss: 1.383461  [  800/ 3200]\n",
      "loss: 1.386005  [  816/ 3200]\n",
      "loss: 1.383460  [  832/ 3200]\n",
      "loss: 1.388605  [  848/ 3200]\n",
      "loss: 1.390939  [  864/ 3200]\n",
      "loss: 1.388873  [  880/ 3200]\n",
      "loss: 1.389344  [  896/ 3200]\n",
      "loss: 1.381235  [  912/ 3200]\n",
      "loss: 1.383834  [  928/ 3200]\n",
      "loss: 1.393489  [  944/ 3200]\n",
      "loss: 1.382926  [  960/ 3200]\n",
      "loss: 1.391839  [  976/ 3200]\n",
      "loss: 1.385957  [  992/ 3200]\n",
      "loss: 1.383780  [ 1008/ 3200]\n",
      "loss: 1.383355  [ 1024/ 3200]\n",
      "loss: 1.389400  [ 1040/ 3200]\n",
      "loss: 1.388133  [ 1056/ 3200]\n",
      "loss: 1.386485  [ 1072/ 3200]\n",
      "loss: 1.383038  [ 1088/ 3200]\n",
      "loss: 1.387281  [ 1104/ 3200]\n",
      "loss: 1.378424  [ 1120/ 3200]\n",
      "loss: 1.387599  [ 1136/ 3200]\n",
      "loss: 1.388922  [ 1152/ 3200]\n",
      "loss: 1.380917  [ 1168/ 3200]\n",
      "loss: 1.390627  [ 1184/ 3200]\n",
      "loss: 1.393855  [ 1200/ 3200]\n",
      "loss: 1.377842  [ 1216/ 3200]\n",
      "loss: 1.383517  [ 1232/ 3200]\n",
      "loss: 1.385637  [ 1248/ 3200]\n",
      "loss: 1.388181  [ 1264/ 3200]\n",
      "loss: 1.387384  [ 1280/ 3200]\n",
      "loss: 1.383939  [ 1296/ 3200]\n",
      "loss: 1.388021  [ 1312/ 3200]\n",
      "loss: 1.385105  [ 1328/ 3200]\n",
      "loss: 1.385160  [ 1344/ 3200]\n",
      "loss: 1.388823  [ 1360/ 3200]\n",
      "loss: 1.380545  [ 1376/ 3200]\n",
      "loss: 1.380544  [ 1392/ 3200]\n",
      "loss: 1.389669  [ 1408/ 3200]\n",
      "loss: 1.386748  [ 1424/ 3200]\n",
      "loss: 1.386907  [ 1440/ 3200]\n",
      "loss: 1.382401  [ 1456/ 3200]\n",
      "loss: 1.385210  [ 1472/ 3200]\n",
      "loss: 1.393427  [ 1488/ 3200]\n",
      "loss: 1.384202  [ 1504/ 3200]\n",
      "loss: 1.388020  [ 1520/ 3200]\n",
      "loss: 1.389025  [ 1536/ 3200]\n",
      "loss: 1.380440  [ 1552/ 3200]\n",
      "loss: 1.387757  [ 1568/ 3200]\n",
      "loss: 1.390676  [ 1584/ 3200]\n",
      "loss: 1.388555  [ 1600/ 3200]\n",
      "loss: 1.385263  [ 1616/ 3200]\n",
      "loss: 1.386800  [ 1632/ 3200]\n",
      "loss: 1.393113  [ 1648/ 3200]\n",
      "loss: 1.382458  [ 1664/ 3200]\n",
      "loss: 1.377844  [ 1680/ 3200]\n",
      "loss: 1.385104  [ 1696/ 3200]\n",
      "loss: 1.384258  [ 1712/ 3200]\n",
      "loss: 1.383414  [ 1728/ 3200]\n",
      "loss: 1.385477  [ 1744/ 3200]\n",
      "loss: 1.381659  [ 1760/ 3200]\n",
      "loss: 1.390570  [ 1776/ 3200]\n",
      "loss: 1.393957  [ 1792/ 3200]\n",
      "loss: 1.390937  [ 1808/ 3200]\n",
      "loss: 1.390514  [ 1824/ 3200]\n",
      "loss: 1.382831  [ 1840/ 3200]\n",
      "loss: 1.383885  [ 1856/ 3200]\n",
      "loss: 1.385264  [ 1872/ 3200]\n",
      "loss: 1.384258  [ 1888/ 3200]\n",
      "loss: 1.388498  [ 1904/ 3200]\n",
      "loss: 1.387653  [ 1920/ 3200]\n",
      "loss: 1.386004  [ 1936/ 3200]\n",
      "loss: 1.383040  [ 1952/ 3200]\n",
      "loss: 1.387758  [ 1968/ 3200]\n",
      "loss: 1.385852  [ 1984/ 3200]\n",
      "loss: 1.390195  [ 2000/ 3200]\n",
      "loss: 1.384842  [ 2016/ 3200]\n",
      "loss: 1.388921  [ 2032/ 3200]\n",
      "loss: 1.382666  [ 2048/ 3200]\n",
      "loss: 1.384259  [ 2064/ 3200]\n",
      "loss: 1.381764  [ 2080/ 3200]\n",
      "loss: 1.391096  [ 2096/ 3200]\n",
      "loss: 1.386904  [ 2112/ 3200]\n",
      "loss: 1.384738  [ 2128/ 3200]\n",
      "loss: 1.386800  [ 2144/ 3200]\n",
      "loss: 1.390618  [ 2160/ 3200]\n",
      "loss: 1.386220  [ 2176/ 3200]\n",
      "loss: 1.381025  [ 2192/ 3200]\n",
      "loss: 1.392735  [ 2208/ 3200]\n",
      "loss: 1.382769  [ 2224/ 3200]\n",
      "loss: 1.384315  [ 2240/ 3200]\n",
      "loss: 1.391366  [ 2256/ 3200]\n",
      "loss: 1.388394  [ 2272/ 3200]\n",
      "loss: 1.387596  [ 2288/ 3200]\n",
      "loss: 1.385957  [ 2304/ 3200]\n",
      "loss: 1.388449  [ 2320/ 3200]\n",
      "loss: 1.385581  [ 2336/ 3200]\n",
      "loss: 1.383463  [ 2352/ 3200]\n",
      "loss: 1.385214  [ 2368/ 3200]\n",
      "loss: 1.382141  [ 2384/ 3200]\n",
      "loss: 1.388497  [ 2400/ 3200]\n",
      "loss: 1.385430  [ 2416/ 3200]\n",
      "loss: 1.388602  [ 2432/ 3200]\n",
      "loss: 1.380548  [ 2448/ 3200]\n",
      "loss: 1.387700  [ 2464/ 3200]\n",
      "loss: 1.386855  [ 2480/ 3200]\n",
      "loss: 1.385001  [ 2496/ 3200]\n",
      "loss: 1.385955  [ 2512/ 3200]\n",
      "loss: 1.387700  [ 2528/ 3200]\n",
      "loss: 1.385901  [ 2544/ 3200]\n",
      "loss: 1.384736  [ 2560/ 3200]\n",
      "loss: 1.384737  [ 2576/ 3200]\n",
      "loss: 1.387651  [ 2592/ 3200]\n",
      "loss: 1.380601  [ 2608/ 3200]\n",
      "loss: 1.385158  [ 2624/ 3200]\n",
      "loss: 1.382938  [ 2640/ 3200]\n",
      "loss: 1.385853  [ 2656/ 3200]\n",
      "loss: 1.384839  [ 2672/ 3200]\n",
      "loss: 1.385955  [ 2688/ 3200]\n",
      "loss: 1.380765  [ 2704/ 3200]\n",
      "loss: 1.384103  [ 2720/ 3200]\n",
      "loss: 1.388021  [ 2736/ 3200]\n",
      "loss: 1.384259  [ 2752/ 3200]\n",
      "loss: 1.392261  [ 2768/ 3200]\n",
      "loss: 1.384632  [ 2784/ 3200]\n",
      "loss: 1.391364  [ 2800/ 3200]\n",
      "loss: 1.384156  [ 2816/ 3200]\n",
      "loss: 1.386748  [ 2832/ 3200]\n",
      "loss: 1.393532  [ 2848/ 3200]\n",
      "loss: 1.382987  [ 2864/ 3200]\n",
      "loss: 1.382667  [ 2880/ 3200]\n",
      "loss: 1.386800  [ 2896/ 3200]\n",
      "loss: 1.391835  [ 2912/ 3200]\n",
      "loss: 1.388178  [ 2928/ 3200]\n",
      "loss: 1.388602  [ 2944/ 3200]\n",
      "loss: 1.386905  [ 2960/ 3200]\n",
      "loss: 1.391833  [ 2976/ 3200]\n",
      "loss: 1.387805  [ 2992/ 3200]\n",
      "loss: 1.386059  [ 3008/ 3200]\n",
      "loss: 1.386642  [ 3024/ 3200]\n",
      "loss: 1.390559  [ 3040/ 3200]\n",
      "loss: 1.382039  [ 3056/ 3200]\n",
      "loss: 1.381824  [ 3072/ 3200]\n",
      "loss: 1.387653  [ 3088/ 3200]\n",
      "loss: 1.387439  [ 3104/ 3200]\n",
      "loss: 1.383305  [ 3120/ 3200]\n",
      "loss: 1.383727  [ 3136/ 3200]\n",
      "loss: 1.385430  [ 3152/ 3200]\n",
      "loss: 1.385852  [ 3168/ 3200]\n",
      "loss: 1.388336  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0209e-04.\n",
      "\n",
      "Epoch: 59\n",
      "-----------------------------\n",
      "loss: 1.385525  [    0/ 3200]\n",
      "loss: 1.388178  [   16/ 3200]\n",
      "loss: 1.386426  [   32/ 3200]\n",
      "loss: 1.384739  [   48/ 3200]\n",
      "loss: 1.383465  [   64/ 3200]\n",
      "loss: 1.386754  [   80/ 3200]\n",
      "loss: 1.380659  [   96/ 3200]\n",
      "loss: 1.389397  [  112/ 3200]\n",
      "loss: 1.380502  [  128/ 3200]\n",
      "loss: 1.383887  [  144/ 3200]\n",
      "loss: 1.388074  [  160/ 3200]\n",
      "loss: 1.391833  [  176/ 3200]\n",
      "loss: 1.393163  [  192/ 3200]\n",
      "loss: 1.388179  [  208/ 3200]\n",
      "loss: 1.390718  [  224/ 3200]\n",
      "loss: 1.387595  [  240/ 3200]\n",
      "loss: 1.386799  [  256/ 3200]\n",
      "loss: 1.378751  [  272/ 3200]\n",
      "loss: 1.388918  [  288/ 3200]\n",
      "loss: 1.385957  [  304/ 3200]\n",
      "loss: 1.385900  [  320/ 3200]\n",
      "loss: 1.384953  [  336/ 3200]\n",
      "loss: 1.381347  [  352/ 3200]\n",
      "loss: 1.382191  [  368/ 3200]\n",
      "loss: 1.382939  [  384/ 3200]\n",
      "loss: 1.385535  [  400/ 3200]\n",
      "loss: 1.387596  [  416/ 3200]\n",
      "loss: 1.388765  [  432/ 3200]\n",
      "loss: 1.383465  [  448/ 3200]\n",
      "loss: 1.391514  [  464/ 3200]\n",
      "loss: 1.387700  [  480/ 3200]\n",
      "loss: 1.386800  [  496/ 3200]\n",
      "loss: 1.389713  [  512/ 3200]\n",
      "loss: 1.388869  [  528/ 3200]\n",
      "loss: 1.386060  [  544/ 3200]\n",
      "loss: 1.392731  [  560/ 3200]\n",
      "loss: 1.385113  [  576/ 3200]\n",
      "loss: 1.387382  [  592/ 3200]\n",
      "loss: 1.383148  [  608/ 3200]\n",
      "loss: 1.384261  [  624/ 3200]\n",
      "loss: 1.387277  [  640/ 3200]\n",
      "loss: 1.384102  [  656/ 3200]\n",
      "loss: 1.383418  [  672/ 3200]\n",
      "loss: 1.386697  [  688/ 3200]\n",
      "loss: 1.384261  [  704/ 3200]\n",
      "loss: 1.379335  [  720/ 3200]\n",
      "loss: 1.392627  [  736/ 3200]\n",
      "loss: 1.382088  [  752/ 3200]\n",
      "loss: 1.387174  [  768/ 3200]\n",
      "loss: 1.386012  [  784/ 3200]\n",
      "loss: 1.383782  [  800/ 3200]\n",
      "loss: 1.383148  [  816/ 3200]\n",
      "loss: 1.383465  [  832/ 3200]\n",
      "loss: 1.385056  [  848/ 3200]\n",
      "loss: 1.389452  [  864/ 3200]\n",
      "loss: 1.381348  [  880/ 3200]\n",
      "loss: 1.390932  [  896/ 3200]\n",
      "loss: 1.388655  [  912/ 3200]\n",
      "loss: 1.381348  [  928/ 3200]\n",
      "loss: 1.389768  [  944/ 3200]\n",
      "loss: 1.388919  [  960/ 3200]\n",
      "loss: 1.384206  [  976/ 3200]\n",
      "loss: 1.388502  [  992/ 3200]\n",
      "loss: 1.389769  [ 1008/ 3200]\n",
      "loss: 1.390558  [ 1024/ 3200]\n",
      "loss: 1.385056  [ 1040/ 3200]\n",
      "loss: 1.390190  [ 1056/ 3200]\n",
      "loss: 1.390087  [ 1072/ 3200]\n",
      "loss: 1.385956  [ 1088/ 3200]\n",
      "loss: 1.387755  [ 1104/ 3200]\n",
      "loss: 1.379759  [ 1120/ 3200]\n",
      "loss: 1.388073  [ 1136/ 3200]\n",
      "loss: 1.386752  [ 1152/ 3200]\n",
      "loss: 1.387435  [ 1168/ 3200]\n",
      "loss: 1.386059  [ 1184/ 3200]\n",
      "loss: 1.390454  [ 1200/ 3200]\n",
      "loss: 1.387596  [ 1216/ 3200]\n",
      "loss: 1.393046  [ 1232/ 3200]\n",
      "loss: 1.393625  [ 1248/ 3200]\n",
      "loss: 1.381247  [ 1264/ 3200]\n",
      "loss: 1.388917  [ 1280/ 3200]\n",
      "loss: 1.381247  [ 1296/ 3200]\n",
      "loss: 1.379657  [ 1312/ 3200]\n",
      "loss: 1.383943  [ 1328/ 3200]\n",
      "loss: 1.386958  [ 1344/ 3200]\n",
      "loss: 1.388073  [ 1360/ 3200]\n",
      "loss: 1.386643  [ 1376/ 3200]\n",
      "loss: 1.391461  [ 1392/ 3200]\n",
      "loss: 1.382726  [ 1408/ 3200]\n",
      "loss: 1.388122  [ 1424/ 3200]\n",
      "loss: 1.387699  [ 1440/ 3200]\n",
      "loss: 1.384262  [ 1456/ 3200]\n",
      "loss: 1.388917  [ 1472/ 3200]\n",
      "loss: 1.387859  [ 1488/ 3200]\n",
      "loss: 1.385480  [ 1504/ 3200]\n",
      "loss: 1.389608  [ 1520/ 3200]\n",
      "loss: 1.384262  [ 1536/ 3200]\n",
      "loss: 1.380031  [ 1552/ 3200]\n",
      "loss: 1.389920  [ 1568/ 3200]\n",
      "loss: 1.385854  [ 1584/ 3200]\n",
      "loss: 1.392248  [ 1600/ 3200]\n",
      "loss: 1.386004  [ 1616/ 3200]\n",
      "loss: 1.388915  [ 1632/ 3200]\n",
      "loss: 1.383308  [ 1648/ 3200]\n",
      "loss: 1.388446  [ 1664/ 3200]\n",
      "loss: 1.384319  [ 1680/ 3200]\n",
      "loss: 1.379976  [ 1696/ 3200]\n",
      "loss: 1.386004  [ 1712/ 3200]\n",
      "loss: 1.384111  [ 1728/ 3200]\n",
      "loss: 1.383412  [ 1744/ 3200]\n",
      "loss: 1.387221  [ 1760/ 3200]\n",
      "loss: 1.384684  [ 1776/ 3200]\n",
      "loss: 1.386855  [ 1792/ 3200]\n",
      "loss: 1.382672  [ 1808/ 3200]\n",
      "loss: 1.387756  [ 1824/ 3200]\n",
      "loss: 1.382624  [ 1840/ 3200]\n",
      "loss: 1.380714  [ 1856/ 3200]\n",
      "loss: 1.383840  [ 1872/ 3200]\n",
      "loss: 1.384684  [ 1888/ 3200]\n",
      "loss: 1.389759  [ 1904/ 3200]\n",
      "loss: 1.386751  [ 1920/ 3200]\n",
      "loss: 1.382624  [ 1936/ 3200]\n",
      "loss: 1.380293  [ 1952/ 3200]\n",
      "loss: 1.387651  [ 1968/ 3200]\n",
      "loss: 1.387173  [ 1984/ 3200]\n",
      "loss: 1.383149  [ 2000/ 3200]\n",
      "loss: 1.385058  [ 2016/ 3200]\n",
      "loss: 1.392574  [ 2032/ 3200]\n",
      "loss: 1.390660  [ 2048/ 3200]\n",
      "loss: 1.384263  [ 2064/ 3200]\n",
      "loss: 1.391882  [ 2080/ 3200]\n",
      "loss: 1.383840  [ 2096/ 3200]\n",
      "loss: 1.389336  [ 2112/ 3200]\n",
      "loss: 1.384263  [ 2128/ 3200]\n",
      "loss: 1.378600  [ 2144/ 3200]\n",
      "loss: 1.386855  [ 2160/ 3200]\n",
      "loss: 1.378281  [ 2176/ 3200]\n",
      "loss: 1.390985  [ 2192/ 3200]\n",
      "loss: 1.389241  [ 2208/ 3200]\n",
      "loss: 1.387174  [ 2224/ 3200]\n",
      "loss: 1.386538  [ 2240/ 3200]\n",
      "loss: 1.384261  [ 2256/ 3200]\n",
      "loss: 1.389338  [ 2272/ 3200]\n",
      "loss: 1.382251  [ 2288/ 3200]\n",
      "loss: 1.391135  [ 2304/ 3200]\n",
      "loss: 1.380875  [ 2320/ 3200]\n",
      "loss: 1.385583  [ 2336/ 3200]\n",
      "loss: 1.386004  [ 2352/ 3200]\n",
      "loss: 1.386434  [ 2368/ 3200]\n",
      "loss: 1.390984  [ 2384/ 3200]\n",
      "loss: 1.386274  [ 2400/ 3200]\n",
      "loss: 1.384684  [ 2416/ 3200]\n",
      "loss: 1.383889  [ 2432/ 3200]\n",
      "loss: 1.387699  [ 2448/ 3200]\n",
      "loss: 1.387547  [ 2464/ 3200]\n",
      "loss: 1.383675  [ 2480/ 3200]\n",
      "loss: 1.388867  [ 2496/ 3200]\n",
      "loss: 1.381304  [ 2512/ 3200]\n",
      "loss: 1.391826  [ 2528/ 3200]\n",
      "loss: 1.384214  [ 2544/ 3200]\n",
      "loss: 1.385847  [ 2560/ 3200]\n",
      "loss: 1.384581  [ 2576/ 3200]\n",
      "loss: 1.385528  [ 2592/ 3200]\n",
      "loss: 1.387331  [ 2608/ 3200]\n",
      "loss: 1.384158  [ 2624/ 3200]\n",
      "loss: 1.386004  [ 2640/ 3200]\n",
      "loss: 1.386378  [ 2656/ 3200]\n",
      "loss: 1.387442  [ 2672/ 3200]\n",
      "loss: 1.391721  [ 2688/ 3200]\n",
      "loss: 1.387595  [ 2704/ 3200]\n",
      "loss: 1.387594  [ 2720/ 3200]\n",
      "loss: 1.382148  [ 2736/ 3200]\n",
      "loss: 1.385528  [ 2752/ 3200]\n",
      "loss: 1.376646  [ 2768/ 3200]\n",
      "loss: 1.387593  [ 2784/ 3200]\n",
      "loss: 1.380874  [ 2800/ 3200]\n",
      "loss: 1.389233  [ 2816/ 3200]\n",
      "loss: 1.384366  [ 2832/ 3200]\n",
      "loss: 1.386856  [ 2848/ 3200]\n",
      "loss: 1.386378  [ 2864/ 3200]\n",
      "loss: 1.395156  [ 2880/ 3200]\n",
      "loss: 1.393517  [ 2896/ 3200]\n",
      "loss: 1.387593  [ 2912/ 3200]\n",
      "loss: 1.389449  [ 2928/ 3200]\n",
      "loss: 1.387221  [ 2944/ 3200]\n",
      "loss: 1.382839  [ 2960/ 3200]\n",
      "loss: 1.389660  [ 2976/ 3200]\n",
      "loss: 1.387756  [ 2992/ 3200]\n",
      "loss: 1.393095  [ 3008/ 3200]\n",
      "loss: 1.391402  [ 3024/ 3200]\n",
      "loss: 1.386378  [ 3040/ 3200]\n",
      "loss: 1.391344  [ 3056/ 3200]\n",
      "loss: 1.387698  [ 3072/ 3200]\n",
      "loss: 1.384312  [ 3088/ 3200]\n",
      "loss: 1.384264  [ 3104/ 3200]\n",
      "loss: 1.387545  [ 3120/ 3200]\n",
      "loss: 1.383153  [ 3136/ 3200]\n",
      "loss: 1.386904  [ 3152/ 3200]\n",
      "loss: 1.385745  [ 3168/ 3200]\n",
      "loss: 1.393093  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.6989e-05.\n",
      "\n",
      "Epoch: 60\n",
      "-----------------------------\n",
      "loss: 1.387593  [    0/ 3200]\n",
      "loss: 1.383788  [   16/ 3200]\n",
      "loss: 1.383786  [   32/ 3200]\n",
      "loss: 1.385584  [   48/ 3200]\n",
      "loss: 1.379242  [   64/ 3200]\n",
      "loss: 1.388913  [   80/ 3200]\n",
      "loss: 1.385584  [   96/ 3200]\n",
      "loss: 1.391773  [  112/ 3200]\n",
      "loss: 1.390931  [  128/ 3200]\n",
      "loss: 1.387116  [  144/ 3200]\n",
      "loss: 1.383366  [  160/ 3200]\n",
      "loss: 1.387650  [  176/ 3200]\n",
      "loss: 1.389860  [  192/ 3200]\n",
      "loss: 1.383730  [  208/ 3200]\n",
      "loss: 1.386483  [  224/ 3200]\n",
      "loss: 1.383365  [  240/ 3200]\n",
      "loss: 1.378764  [  256/ 3200]\n",
      "loss: 1.387593  [  272/ 3200]\n",
      "loss: 1.387649  [  288/ 3200]\n",
      "loss: 1.381939  [  304/ 3200]\n",
      "loss: 1.385536  [  320/ 3200]\n",
      "loss: 1.385480  [  336/ 3200]\n",
      "loss: 1.390557  [  352/ 3200]\n",
      "loss: 1.386856  [  368/ 3200]\n",
      "loss: 1.388175  [  384/ 3200]\n",
      "loss: 1.388118  [  400/ 3200]\n",
      "loss: 1.388969  [  416/ 3200]\n",
      "loss: 1.384789  [  432/ 3200]\n",
      "loss: 1.383097  [  448/ 3200]\n",
      "loss: 1.393569  [  464/ 3200]\n",
      "loss: 1.383844  [  480/ 3200]\n",
      "loss: 1.384686  [  496/ 3200]\n",
      "loss: 1.385431  [  512/ 3200]\n",
      "loss: 1.385480  [  528/ 3200]\n",
      "loss: 1.386378  [  544/ 3200]\n",
      "loss: 1.384369  [  560/ 3200]\n",
      "loss: 1.387276  [  576/ 3200]\n",
      "loss: 1.389285  [  592/ 3200]\n",
      "loss: 1.383050  [  608/ 3200]\n",
      "loss: 1.391715  [  624/ 3200]\n",
      "loss: 1.377553  [  640/ 3200]\n",
      "loss: 1.386004  [  656/ 3200]\n",
      "loss: 1.389285  [  672/ 3200]\n",
      "loss: 1.386004  [  688/ 3200]\n",
      "loss: 1.392295  [  704/ 3200]\n",
      "loss: 1.389811  [  720/ 3200]\n",
      "loss: 1.385059  [  736/ 3200]\n",
      "loss: 1.391819  [  752/ 3200]\n",
      "loss: 1.387650  [  768/ 3200]\n",
      "loss: 1.384210  [  784/ 3200]\n",
      "loss: 1.386434  [  800/ 3200]\n",
      "loss: 1.388022  [  816/ 3200]\n",
      "loss: 1.385107  [  832/ 3200]\n",
      "loss: 1.386743  [  848/ 3200]\n",
      "loss: 1.388069  [  864/ 3200]\n",
      "loss: 1.386799  [  880/ 3200]\n",
      "loss: 1.379983  [  896/ 3200]\n",
      "loss: 1.384208  [  912/ 3200]\n",
      "loss: 1.382995  [  928/ 3200]\n",
      "loss: 1.384639  [  944/ 3200]\n",
      "loss: 1.385106  [  960/ 3200]\n",
      "loss: 1.386061  [  976/ 3200]\n",
      "loss: 1.387124  [  992/ 3200]\n",
      "loss: 1.389705  [ 1008/ 3200]\n",
      "loss: 1.391396  [ 1024/ 3200]\n",
      "loss: 1.385375  [ 1040/ 3200]\n",
      "loss: 1.385585  [ 1056/ 3200]\n",
      "loss: 1.379936  [ 1072/ 3200]\n",
      "loss: 1.383366  [ 1088/ 3200]\n",
      "loss: 1.383367  [ 1104/ 3200]\n",
      "loss: 1.378347  [ 1120/ 3200]\n",
      "loss: 1.385270  [ 1136/ 3200]\n",
      "loss: 1.383681  [ 1152/ 3200]\n",
      "loss: 1.386322  [ 1168/ 3200]\n",
      "loss: 1.383949  [ 1184/ 3200]\n",
      "loss: 1.382994  [ 1200/ 3200]\n",
      "loss: 1.383424  [ 1216/ 3200]\n",
      "loss: 1.384264  [ 1232/ 3200]\n",
      "loss: 1.385317  [ 1248/ 3200]\n",
      "loss: 1.390604  [ 1264/ 3200]\n",
      "loss: 1.386014  [ 1280/ 3200]\n",
      "loss: 1.384314  [ 1296/ 3200]\n",
      "loss: 1.385900  [ 1312/ 3200]\n",
      "loss: 1.389295  [ 1328/ 3200]\n",
      "loss: 1.392774  [ 1344/ 3200]\n",
      "loss: 1.382257  [ 1360/ 3200]\n",
      "loss: 1.383948  [ 1376/ 3200]\n",
      "loss: 1.389016  [ 1392/ 3200]\n",
      "loss: 1.391922  [ 1408/ 3200]\n",
      "loss: 1.383367  [ 1424/ 3200]\n",
      "loss: 1.385107  [ 1440/ 3200]\n",
      "loss: 1.385900  [ 1456/ 3200]\n",
      "loss: 1.389284  [ 1472/ 3200]\n",
      "loss: 1.381302  [ 1488/ 3200]\n",
      "loss: 1.384581  [ 1504/ 3200]\n",
      "loss: 1.378348  [ 1520/ 3200]\n",
      "loss: 1.387124  [ 1536/ 3200]\n",
      "loss: 1.387334  [ 1552/ 3200]\n",
      "loss: 1.390661  [ 1568/ 3200]\n",
      "loss: 1.383683  [ 1584/ 3200]\n",
      "loss: 1.389285  [ 1600/ 3200]\n",
      "loss: 1.391396  [ 1616/ 3200]\n",
      "loss: 1.383099  [ 1632/ 3200]\n",
      "loss: 1.382889  [ 1648/ 3200]\n",
      "loss: 1.385163  [ 1664/ 3200]\n",
      "loss: 1.385375  [ 1680/ 3200]\n",
      "loss: 1.389600  [ 1696/ 3200]\n",
      "loss: 1.388337  [ 1712/ 3200]\n",
      "loss: 1.386063  [ 1728/ 3200]\n",
      "loss: 1.390020  [ 1744/ 3200]\n",
      "loss: 1.390976  [ 1760/ 3200]\n",
      "loss: 1.390076  [ 1776/ 3200]\n",
      "loss: 1.391346  [ 1792/ 3200]\n",
      "loss: 1.381782  [ 1808/ 3200]\n",
      "loss: 1.383528  [ 1824/ 3200]\n",
      "loss: 1.384103  [ 1840/ 3200]\n",
      "loss: 1.388013  [ 1856/ 3200]\n",
      "loss: 1.389074  [ 1872/ 3200]\n",
      "loss: 1.384793  [ 1888/ 3200]\n",
      "loss: 1.389389  [ 1904/ 3200]\n",
      "loss: 1.385164  [ 1920/ 3200]\n",
      "loss: 1.384581  [ 1936/ 3200]\n",
      "loss: 1.384637  [ 1952/ 3200]\n",
      "loss: 1.388911  [ 1968/ 3200]\n",
      "loss: 1.386379  [ 1984/ 3200]\n",
      "loss: 1.387591  [ 2000/ 3200]\n",
      "loss: 1.388013  [ 2016/ 3200]\n",
      "loss: 1.389339  [ 2032/ 3200]\n",
      "loss: 1.383530  [ 2048/ 3200]\n",
      "loss: 1.383845  [ 2064/ 3200]\n",
      "loss: 1.386013  [ 2080/ 3200]\n",
      "loss: 1.389283  [ 2096/ 3200]\n",
      "loss: 1.383474  [ 2112/ 3200]\n",
      "loss: 1.390546  [ 2128/ 3200]\n",
      "loss: 1.388013  [ 2144/ 3200]\n",
      "loss: 1.387963  [ 2160/ 3200]\n",
      "loss: 1.386378  [ 2176/ 3200]\n",
      "loss: 1.384581  [ 2192/ 3200]\n",
      "loss: 1.388012  [ 2208/ 3200]\n",
      "loss: 1.388703  [ 2224/ 3200]\n",
      "loss: 1.388966  [ 2240/ 3200]\n",
      "loss: 1.389652  [ 2256/ 3200]\n",
      "loss: 1.381154  [ 2272/ 3200]\n",
      "loss: 1.389493  [ 2288/ 3200]\n",
      "loss: 1.384687  [ 2304/ 3200]\n",
      "loss: 1.390916  [ 2320/ 3200]\n",
      "loss: 1.390285  [ 2336/ 3200]\n",
      "loss: 1.383790  [ 2352/ 3200]\n",
      "loss: 1.389225  [ 2368/ 3200]\n",
      "loss: 1.386378  [ 2384/ 3200]\n",
      "loss: 1.389808  [ 2400/ 3200]\n",
      "loss: 1.386378  [ 2416/ 3200]\n",
      "loss: 1.388966  [ 2432/ 3200]\n",
      "loss: 1.377190  [ 2448/ 3200]\n",
      "loss: 1.380569  [ 2464/ 3200]\n",
      "loss: 1.387591  [ 2480/ 3200]\n",
      "loss: 1.384744  [ 2496/ 3200]\n",
      "loss: 1.383368  [ 2512/ 3200]\n",
      "loss: 1.383312  [ 2528/ 3200]\n",
      "loss: 1.385050  [ 2544/ 3200]\n",
      "loss: 1.382470  [ 2560/ 3200]\n",
      "loss: 1.384956  [ 2576/ 3200]\n",
      "loss: 1.382890  [ 2592/ 3200]\n",
      "loss: 1.387123  [ 2608/ 3200]\n",
      "loss: 1.387648  [ 2624/ 3200]\n",
      "loss: 1.391442  [ 2640/ 3200]\n",
      "loss: 1.393026  [ 2656/ 3200]\n",
      "loss: 1.386904  [ 2672/ 3200]\n",
      "loss: 1.387019  [ 2688/ 3200]\n",
      "loss: 1.390227  [ 2704/ 3200]\n",
      "loss: 1.388918  [ 2720/ 3200]\n",
      "loss: 1.389758  [ 2736/ 3200]\n",
      "loss: 1.384315  [ 2752/ 3200]\n",
      "loss: 1.388011  [ 2768/ 3200]\n",
      "loss: 1.385747  [ 2784/ 3200]\n",
      "loss: 1.385268  [ 2800/ 3200]\n",
      "loss: 1.391496  [ 2816/ 3200]\n",
      "loss: 1.387171  [ 2832/ 3200]\n",
      "loss: 1.387695  [ 2848/ 3200]\n",
      "loss: 1.391447  [ 2864/ 3200]\n",
      "loss: 1.383055  [ 2880/ 3200]\n",
      "loss: 1.389702  [ 2896/ 3200]\n",
      "loss: 1.379359  [ 2912/ 3200]\n",
      "loss: 1.388965  [ 2928/ 3200]\n",
      "loss: 1.382847  [ 2944/ 3200]\n",
      "loss: 1.388591  [ 2960/ 3200]\n",
      "loss: 1.382998  [ 2976/ 3200]\n",
      "loss: 1.378778  [ 2992/ 3200]\n",
      "loss: 1.387807  [ 3008/ 3200]\n",
      "loss: 1.388544  [ 3024/ 3200]\n",
      "loss: 1.385956  [ 3040/ 3200]\n",
      "loss: 1.389232  [ 3056/ 3200]\n",
      "loss: 1.392812  [ 3072/ 3200]\n",
      "loss: 1.390073  [ 3088/ 3200]\n",
      "loss: 1.383950  [ 3104/ 3200]\n",
      "loss: 1.382841  [ 3120/ 3200]\n",
      "loss: 1.391494  [ 3136/ 3200]\n",
      "loss: 1.381206  [ 3152/ 3200]\n",
      "loss: 1.393445  [ 3168/ 3200]\n",
      "loss: 1.382213  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086649\n",
      "f1 macro averaged score: 0.100000\n",
      "Accuracy               : 25.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0],\n",
      "        [  0, 200,   0,   0]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.2140e-05.\n",
      "\n",
      "Best epoch: 1 with f1 macro averaged score: 0.10000000149011612\n",
      "Test Error:\n",
      "Avg loss               : 0.086704\n",
      "f1 macro averaged score: 0.095294\n",
      "Accuracy               : 23.5%\n",
      "Confusion matrix       :\n",
      "tensor([[  0, 297,   0,   0],\n",
      "        [  0, 324,   0,   0],\n",
      "        [  0, 356,   0,   0],\n",
      "        [  0, 399,   0,   0]], device='cuda:0')\n",
      "CPU times: user 32min 32s, sys: 48.2 s, total: 33min 20s\n",
      "Wall time: 34min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "weight_decay_list = [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "p_list = [0.1, 0.2, 0.3]\n",
    "\n",
    "f1_accuracy_30 = validate_weight_decay_dropout(weight_decay_list, p_list, 30)\n",
    "f1_accuracy_60 = validate_weight_decay_dropout(weight_decay_list, p_list, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9w8kFUK4PGus",
    "outputId": "7daca525-66aa-47fc-839c-3969240ab7d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 epochs\n",
      "W Decay\tDropout\tf1_macro_avg\tAccuracy\n",
      "0.0001\t0.1\t0.792476\t78.997093\n",
      "1e-05\t0.3\t0.791860\t78.706395\n",
      "1e-05\t0.1\t0.789297\t78.488372\n",
      "0.001\t0.1\t0.788866\t78.561047\n",
      "0\t0.3\t0.785253\t78.343023\n",
      "0.01\t0.1\t0.785046\t78.343023\n",
      "0.001\t0.3\t0.784738\t78.415698\n",
      "0.0001\t0.3\t0.784109\t78.488372\n",
      "0.001\t0.2\t0.783579\t77.979651\n",
      "0\t0.1\t0.782631\t77.906977\n",
      "1e-05\t0.2\t0.781177\t77.688953\n",
      "0.01\t0.3\t0.780148\t78.125000\n",
      "0.0001\t0.2\t0.776219\t77.325581\n",
      "0.01\t0.2\t0.775034\t77.543605\n",
      "0\t0.2\t0.766852\t76.526163\n",
      "0.1\t0.3\t0.213429\t27.834302\n",
      "1\t0.3\t0.095294\t23.546512\n",
      "0.1\t0.2\t0.095294\t23.546512\n",
      "1\t0.2\t0.095294\t23.546512\n",
      "0.1\t0.1\t0.095294\t23.546512\n",
      "1\t0.1\t0.095294\t23.546512\n",
      "\n",
      "60 epochs\n",
      "W Decay\tDropout\tf1_macro_avg\tAccuracy\n",
      "0.01\t0.2\t0.791173\t78.924419\n",
      "0.0001\t0.1\t0.789194\t78.561047\n",
      "0\t0.3\t0.789020\t78.488372\n",
      "0.01\t0.1\t0.788836\t78.633721\n",
      "0\t0.1\t0.786272\t78.125000\n",
      "0.01\t0.3\t0.785334\t78.343023\n",
      "1e-05\t0.1\t0.784846\t78.052326\n",
      "1e-05\t0.3\t0.783751\t77.834302\n",
      "0.001\t0.1\t0.783534\t77.906977\n",
      "0.001\t0.3\t0.781893\t77.906977\n",
      "1e-05\t0.2\t0.779219\t77.543605\n",
      "0.001\t0.2\t0.779041\t77.470930\n",
      "0.0001\t0.2\t0.774643\t77.252907\n",
      "0.0001\t0.3\t0.772099\t76.889535\n",
      "0\t0.2\t0.763458\t76.090116\n",
      "0.1\t0.3\t0.213679\t27.979651\n",
      "1\t0.3\t0.095294\t23.546512\n",
      "0.1\t0.2\t0.095294\t23.546512\n",
      "1\t0.2\t0.095294\t23.546512\n",
      "0.1\t0.1\t0.095294\t23.546512\n",
      "1\t0.1\t0.095294\t23.546512\n"
     ]
    }
   ],
   "source": [
    "f1_accuracy_30 = sorted(f1_accuracy_30, key=lambda x: x[2], reverse=True)\n",
    "f1_accuracy_60 = sorted(f1_accuracy_60, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"30 epochs\")\n",
    "print(\"W Decay\\tDropout\\tf1_macro_avg\\tAccuracy\")\n",
    "for (weight_decay, dropout, f1_macro_avg, accuracy) in f1_accuracy_30:\n",
    "  print(f\"{weight_decay}\\t{dropout}\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")\n",
    "print()\n",
    "\n",
    "print(\"60 epochs\")\n",
    "print(\"W Decay\\tDropout\\tf1_macro_avg\\tAccuracy\")\n",
    "for (weight_decay, dropout, f1_macro_avg, accuracy) in f1_accuracy_60:\n",
    "  print(f\"{weight_decay}\\t{dropout}\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLQ9KjUwPsLc"
   },
   "source": [
    "The best result is achieved when training for $30$ epochs with weight decay $0.0001$ and dropout $0.1$\n",
    "\n",
    "This gives almost $79\\%$ accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4siyUgx9v6jc"
   },
   "source": [
    "### Step 6 - Training Efficiency\n",
    "\n",
    "+ Batch size\n",
    "\n",
    "We validate our Convolutional Neural Network for $30$ epochs and test it.\n",
    "\n",
    "Our model uses:\n",
    "+ the Adagrad optimizer\n",
    "+ the ELU activation function\n",
    "+ the MultiplicativeLR scheduler\n",
    "+ batch normalization\n",
    "+ weight decay $0.0001$\n",
    "+ dropout $0.1$\n",
    "\n",
    "as stated in the previous steps.\n",
    "\n",
    "The batch sizes used are the following: $2$, $4$, $8$, $16$, $32$, $64$ and $128$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TGu3HK5v9_K",
    "outputId": "0958dfe8-d49b-4457-8478-8ff3a4dce18f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 0.394607  [ 3136/ 3200]\n",
      "loss: 0.757750  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.033116\n",
      "f1 macro averaged score: 0.801575\n",
      "Accuracy               : 80.5%\n",
      "Confusion matrix       :\n",
      "tensor([[187,   7,   0,   6],\n",
      "        [ 19, 123,  24,  34],\n",
      "        [  0,  19, 167,  14],\n",
      "        [  5,  17,  11, 167]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0267e-03.\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 0.395857  [    0/ 3200]\n",
      "loss: 0.463632  [   32/ 3200]\n",
      "loss: 0.408214  [   64/ 3200]\n",
      "loss: 0.340108  [   96/ 3200]\n",
      "loss: 0.258789  [  128/ 3200]\n",
      "loss: 0.286411  [  160/ 3200]\n",
      "loss: 0.317121  [  192/ 3200]\n",
      "loss: 0.426852  [  224/ 3200]\n",
      "loss: 0.430823  [  256/ 3200]\n",
      "loss: 0.476869  [  288/ 3200]\n",
      "loss: 0.442472  [  320/ 3200]\n",
      "loss: 0.247674  [  352/ 3200]\n",
      "loss: 0.410746  [  384/ 3200]\n",
      "loss: 0.479291  [  416/ 3200]\n",
      "loss: 0.330812  [  448/ 3200]\n",
      "loss: 0.368404  [  480/ 3200]\n",
      "loss: 0.349925  [  512/ 3200]\n",
      "loss: 0.362851  [  544/ 3200]\n",
      "loss: 0.388968  [  576/ 3200]\n",
      "loss: 0.481074  [  608/ 3200]\n",
      "loss: 0.267201  [  640/ 3200]\n",
      "loss: 0.408915  [  672/ 3200]\n",
      "loss: 0.495206  [  704/ 3200]\n",
      "loss: 0.480605  [  736/ 3200]\n",
      "loss: 0.340040  [  768/ 3200]\n",
      "loss: 0.312444  [  800/ 3200]\n",
      "loss: 0.518117  [  832/ 3200]\n",
      "loss: 0.419750  [  864/ 3200]\n",
      "loss: 0.351902  [  896/ 3200]\n",
      "loss: 0.401562  [  928/ 3200]\n",
      "loss: 0.240659  [  960/ 3200]\n",
      "loss: 0.390919  [  992/ 3200]\n",
      "loss: 0.556878  [ 1024/ 3200]\n",
      "loss: 0.319984  [ 1056/ 3200]\n",
      "loss: 0.379847  [ 1088/ 3200]\n",
      "loss: 0.327578  [ 1120/ 3200]\n",
      "loss: 0.576627  [ 1152/ 3200]\n",
      "loss: 0.549048  [ 1184/ 3200]\n",
      "loss: 0.344155  [ 1216/ 3200]\n",
      "loss: 0.651090  [ 1248/ 3200]\n",
      "loss: 0.367953  [ 1280/ 3200]\n",
      "loss: 0.373888  [ 1312/ 3200]\n",
      "loss: 0.339273  [ 1344/ 3200]\n",
      "loss: 0.172775  [ 1376/ 3200]\n",
      "loss: 0.300069  [ 1408/ 3200]\n",
      "loss: 0.274872  [ 1440/ 3200]\n",
      "loss: 0.542177  [ 1472/ 3200]\n",
      "loss: 0.381726  [ 1504/ 3200]\n",
      "loss: 0.500168  [ 1536/ 3200]\n",
      "loss: 0.524777  [ 1568/ 3200]\n",
      "loss: 0.404685  [ 1600/ 3200]\n",
      "loss: 0.408352  [ 1632/ 3200]\n",
      "loss: 0.489157  [ 1664/ 3200]\n",
      "loss: 0.471236  [ 1696/ 3200]\n",
      "loss: 0.409777  [ 1728/ 3200]\n",
      "loss: 0.447004  [ 1760/ 3200]\n",
      "loss: 0.313562  [ 1792/ 3200]\n",
      "loss: 0.202805  [ 1824/ 3200]\n",
      "loss: 0.459799  [ 1856/ 3200]\n",
      "loss: 0.390216  [ 1888/ 3200]\n",
      "loss: 0.279950  [ 1920/ 3200]\n",
      "loss: 0.429719  [ 1952/ 3200]\n",
      "loss: 0.227084  [ 1984/ 3200]\n",
      "loss: 0.545256  [ 2016/ 3200]\n",
      "loss: 0.440773  [ 2048/ 3200]\n",
      "loss: 0.533014  [ 2080/ 3200]\n",
      "loss: 0.508390  [ 2112/ 3200]\n",
      "loss: 0.470085  [ 2144/ 3200]\n",
      "loss: 0.427608  [ 2176/ 3200]\n",
      "loss: 0.341931  [ 2208/ 3200]\n",
      "loss: 0.365247  [ 2240/ 3200]\n",
      "loss: 0.612324  [ 2272/ 3200]\n",
      "loss: 0.383448  [ 2304/ 3200]\n",
      "loss: 0.364320  [ 2336/ 3200]\n",
      "loss: 0.490524  [ 2368/ 3200]\n",
      "loss: 0.722310  [ 2400/ 3200]\n",
      "loss: 0.454815  [ 2432/ 3200]\n",
      "loss: 0.539708  [ 2464/ 3200]\n",
      "loss: 0.410389  [ 2496/ 3200]\n",
      "loss: 0.315248  [ 2528/ 3200]\n",
      "loss: 0.333150  [ 2560/ 3200]\n",
      "loss: 0.320066  [ 2592/ 3200]\n",
      "loss: 0.267537  [ 2624/ 3200]\n",
      "loss: 0.395024  [ 2656/ 3200]\n",
      "loss: 0.540973  [ 2688/ 3200]\n",
      "loss: 0.466323  [ 2720/ 3200]\n",
      "loss: 0.613146  [ 2752/ 3200]\n",
      "loss: 0.475444  [ 2784/ 3200]\n",
      "loss: 0.395290  [ 2816/ 3200]\n",
      "loss: 0.471431  [ 2848/ 3200]\n",
      "loss: 0.276374  [ 2880/ 3200]\n",
      "loss: 0.329490  [ 2912/ 3200]\n",
      "loss: 0.221125  [ 2944/ 3200]\n",
      "loss: 0.165698  [ 2976/ 3200]\n",
      "loss: 0.384943  [ 3008/ 3200]\n",
      "loss: 0.923150  [ 3040/ 3200]\n",
      "loss: 0.273861  [ 3072/ 3200]\n",
      "loss: 0.668443  [ 3104/ 3200]\n",
      "loss: 0.318386  [ 3136/ 3200]\n",
      "loss: 0.289467  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035201\n",
      "f1 macro averaged score: 0.774517\n",
      "Accuracy               : 78.5%\n",
      "Confusion matrix       :\n",
      "tensor([[187,   3,   0,  10],\n",
      "        [ 20,  96,  36,  48],\n",
      "        [  0,  11, 173,  16],\n",
      "        [  8,   6,  14, 172]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.7535e-04.\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.499323  [    0/ 3200]\n",
      "loss: 0.242216  [   32/ 3200]\n",
      "loss: 0.323512  [   64/ 3200]\n",
      "loss: 0.302312  [   96/ 3200]\n",
      "loss: 0.289027  [  128/ 3200]\n",
      "loss: 0.474031  [  160/ 3200]\n",
      "loss: 0.511695  [  192/ 3200]\n",
      "loss: 0.251806  [  224/ 3200]\n",
      "loss: 0.263780  [  256/ 3200]\n",
      "loss: 0.270237  [  288/ 3200]\n",
      "loss: 0.443340  [  320/ 3200]\n",
      "loss: 0.325479  [  352/ 3200]\n",
      "loss: 0.381023  [  384/ 3200]\n",
      "loss: 0.247619  [  416/ 3200]\n",
      "loss: 0.545776  [  448/ 3200]\n",
      "loss: 0.601256  [  480/ 3200]\n",
      "loss: 0.453164  [  512/ 3200]\n",
      "loss: 0.483877  [  544/ 3200]\n",
      "loss: 0.509568  [  576/ 3200]\n",
      "loss: 0.470577  [  608/ 3200]\n",
      "loss: 0.214394  [  640/ 3200]\n",
      "loss: 0.458057  [  672/ 3200]\n",
      "loss: 0.514525  [  704/ 3200]\n",
      "loss: 0.238804  [  736/ 3200]\n",
      "loss: 0.313840  [  768/ 3200]\n",
      "loss: 0.550040  [  800/ 3200]\n",
      "loss: 0.271667  [  832/ 3200]\n",
      "loss: 0.223739  [  864/ 3200]\n",
      "loss: 0.330405  [  896/ 3200]\n",
      "loss: 0.265252  [  928/ 3200]\n",
      "loss: 0.363293  [  960/ 3200]\n",
      "loss: 0.338997  [  992/ 3200]\n",
      "loss: 0.269089  [ 1024/ 3200]\n",
      "loss: 0.444379  [ 1056/ 3200]\n",
      "loss: 0.380434  [ 1088/ 3200]\n",
      "loss: 0.431066  [ 1120/ 3200]\n",
      "loss: 0.284235  [ 1152/ 3200]\n",
      "loss: 0.254920  [ 1184/ 3200]\n",
      "loss: 0.375391  [ 1216/ 3200]\n",
      "loss: 0.458827  [ 1248/ 3200]\n",
      "loss: 0.239472  [ 1280/ 3200]\n",
      "loss: 0.394116  [ 1312/ 3200]\n",
      "loss: 0.462344  [ 1344/ 3200]\n",
      "loss: 0.203008  [ 1376/ 3200]\n",
      "loss: 0.295587  [ 1408/ 3200]\n",
      "loss: 0.364804  [ 1440/ 3200]\n",
      "loss: 0.544755  [ 1472/ 3200]\n",
      "loss: 0.624725  [ 1504/ 3200]\n",
      "loss: 0.217169  [ 1536/ 3200]\n",
      "loss: 0.490957  [ 1568/ 3200]\n",
      "loss: 0.321047  [ 1600/ 3200]\n",
      "loss: 0.330110  [ 1632/ 3200]\n",
      "loss: 0.625748  [ 1664/ 3200]\n",
      "loss: 0.306205  [ 1696/ 3200]\n",
      "loss: 0.419718  [ 1728/ 3200]\n",
      "loss: 0.412575  [ 1760/ 3200]\n",
      "loss: 0.622833  [ 1792/ 3200]\n",
      "loss: 0.463938  [ 1824/ 3200]\n",
      "loss: 0.648472  [ 1856/ 3200]\n",
      "loss: 0.675368  [ 1888/ 3200]\n",
      "loss: 0.447794  [ 1920/ 3200]\n",
      "loss: 0.465995  [ 1952/ 3200]\n",
      "loss: 0.380185  [ 1984/ 3200]\n",
      "loss: 0.302921  [ 2016/ 3200]\n",
      "loss: 0.390871  [ 2048/ 3200]\n",
      "loss: 0.482995  [ 2080/ 3200]\n",
      "loss: 0.442105  [ 2112/ 3200]\n",
      "loss: 0.426625  [ 2144/ 3200]\n",
      "loss: 0.327899  [ 2176/ 3200]\n",
      "loss: 0.346238  [ 2208/ 3200]\n",
      "loss: 0.578660  [ 2240/ 3200]\n",
      "loss: 0.352384  [ 2272/ 3200]\n",
      "loss: 0.344143  [ 2304/ 3200]\n",
      "loss: 0.630607  [ 2336/ 3200]\n",
      "loss: 0.192185  [ 2368/ 3200]\n",
      "loss: 0.430509  [ 2400/ 3200]\n",
      "loss: 0.436915  [ 2432/ 3200]\n",
      "loss: 0.562756  [ 2464/ 3200]\n",
      "loss: 0.116540  [ 2496/ 3200]\n",
      "loss: 0.452104  [ 2528/ 3200]\n",
      "loss: 0.256059  [ 2560/ 3200]\n",
      "loss: 0.358203  [ 2592/ 3200]\n",
      "loss: 0.363583  [ 2624/ 3200]\n",
      "loss: 0.518096  [ 2656/ 3200]\n",
      "loss: 0.267727  [ 2688/ 3200]\n",
      "loss: 0.330986  [ 2720/ 3200]\n",
      "loss: 0.294534  [ 2752/ 3200]\n",
      "loss: 0.525815  [ 2784/ 3200]\n",
      "loss: 0.357176  [ 2816/ 3200]\n",
      "loss: 0.694590  [ 2848/ 3200]\n",
      "loss: 0.355389  [ 2880/ 3200]\n",
      "loss: 0.305778  [ 2912/ 3200]\n",
      "loss: 0.235578  [ 2944/ 3200]\n",
      "loss: 0.586233  [ 2976/ 3200]\n",
      "loss: 0.616156  [ 3008/ 3200]\n",
      "loss: 0.390363  [ 3040/ 3200]\n",
      "loss: 0.682586  [ 3072/ 3200]\n",
      "loss: 0.522806  [ 3104/ 3200]\n",
      "loss: 0.332385  [ 3136/ 3200]\n",
      "loss: 0.426956  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036549\n",
      "f1 macro averaged score: 0.777134\n",
      "Accuracy               : 77.8%\n",
      "Confusion matrix       :\n",
      "tensor([[173,  14,   0,  13],\n",
      "        [ 13, 120,  15,  52],\n",
      "        [  0,  25, 154,  21],\n",
      "        [  4,  13,   8, 175]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.2658e-04.\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 0.365508  [    0/ 3200]\n",
      "loss: 0.291769  [   32/ 3200]\n",
      "loss: 0.542590  [   64/ 3200]\n",
      "loss: 0.542663  [   96/ 3200]\n",
      "loss: 0.310463  [  128/ 3200]\n",
      "loss: 0.329790  [  160/ 3200]\n",
      "loss: 0.323295  [  192/ 3200]\n",
      "loss: 0.409992  [  224/ 3200]\n",
      "loss: 0.317868  [  256/ 3200]\n",
      "loss: 0.353576  [  288/ 3200]\n",
      "loss: 0.316518  [  320/ 3200]\n",
      "loss: 0.368547  [  352/ 3200]\n",
      "loss: 0.334018  [  384/ 3200]\n",
      "loss: 0.485683  [  416/ 3200]\n",
      "loss: 0.667262  [  448/ 3200]\n",
      "loss: 0.461463  [  480/ 3200]\n",
      "loss: 0.368940  [  512/ 3200]\n",
      "loss: 0.351972  [  544/ 3200]\n",
      "loss: 0.488588  [  576/ 3200]\n",
      "loss: 0.460507  [  608/ 3200]\n",
      "loss: 0.283823  [  640/ 3200]\n",
      "loss: 0.314532  [  672/ 3200]\n",
      "loss: 0.285275  [  704/ 3200]\n",
      "loss: 0.344991  [  736/ 3200]\n",
      "loss: 0.222492  [  768/ 3200]\n",
      "loss: 0.315657  [  800/ 3200]\n",
      "loss: 0.446009  [  832/ 3200]\n",
      "loss: 0.225728  [  864/ 3200]\n",
      "loss: 0.486978  [  896/ 3200]\n",
      "loss: 0.526253  [  928/ 3200]\n",
      "loss: 0.289140  [  960/ 3200]\n",
      "loss: 0.213123  [  992/ 3200]\n",
      "loss: 0.391053  [ 1024/ 3200]\n",
      "loss: 0.422897  [ 1056/ 3200]\n",
      "loss: 0.296950  [ 1088/ 3200]\n",
      "loss: 0.486245  [ 1120/ 3200]\n",
      "loss: 0.673400  [ 1152/ 3200]\n",
      "loss: 0.465858  [ 1184/ 3200]\n",
      "loss: 0.392305  [ 1216/ 3200]\n",
      "loss: 0.307536  [ 1248/ 3200]\n",
      "loss: 0.155283  [ 1280/ 3200]\n",
      "loss: 0.400123  [ 1312/ 3200]\n",
      "loss: 0.298194  [ 1344/ 3200]\n",
      "loss: 0.439737  [ 1376/ 3200]\n",
      "loss: 0.317625  [ 1408/ 3200]\n",
      "loss: 0.688319  [ 1440/ 3200]\n",
      "loss: 0.579698  [ 1472/ 3200]\n",
      "loss: 0.470323  [ 1504/ 3200]\n",
      "loss: 0.344718  [ 1536/ 3200]\n",
      "loss: 0.297168  [ 1568/ 3200]\n",
      "loss: 0.334434  [ 1600/ 3200]\n",
      "loss: 0.359610  [ 1632/ 3200]\n",
      "loss: 0.327423  [ 1664/ 3200]\n",
      "loss: 0.554386  [ 1696/ 3200]\n",
      "loss: 0.532786  [ 1728/ 3200]\n",
      "loss: 0.300422  [ 1760/ 3200]\n",
      "loss: 0.325494  [ 1792/ 3200]\n",
      "loss: 0.446199  [ 1824/ 3200]\n",
      "loss: 0.350868  [ 1856/ 3200]\n",
      "loss: 0.264047  [ 1888/ 3200]\n",
      "loss: 0.406965  [ 1920/ 3200]\n",
      "loss: 0.342661  [ 1952/ 3200]\n",
      "loss: 0.350181  [ 1984/ 3200]\n",
      "loss: 0.329526  [ 2016/ 3200]\n",
      "loss: 0.533312  [ 2048/ 3200]\n",
      "loss: 0.351281  [ 2080/ 3200]\n",
      "loss: 0.273972  [ 2112/ 3200]\n",
      "loss: 0.317640  [ 2144/ 3200]\n",
      "loss: 0.253747  [ 2176/ 3200]\n",
      "loss: 0.347564  [ 2208/ 3200]\n",
      "loss: 0.248281  [ 2240/ 3200]\n",
      "loss: 0.398386  [ 2272/ 3200]\n",
      "loss: 0.403919  [ 2304/ 3200]\n",
      "loss: 0.328157  [ 2336/ 3200]\n",
      "loss: 0.466349  [ 2368/ 3200]\n",
      "loss: 0.394570  [ 2400/ 3200]\n",
      "loss: 0.474373  [ 2432/ 3200]\n",
      "loss: 0.200998  [ 2464/ 3200]\n",
      "loss: 0.405018  [ 2496/ 3200]\n",
      "loss: 0.339561  [ 2528/ 3200]\n",
      "loss: 0.454001  [ 2560/ 3200]\n",
      "loss: 0.211431  [ 2592/ 3200]\n",
      "loss: 0.145845  [ 2624/ 3200]\n",
      "loss: 0.600395  [ 2656/ 3200]\n",
      "loss: 0.785298  [ 2688/ 3200]\n",
      "loss: 0.282667  [ 2720/ 3200]\n",
      "loss: 0.318449  [ 2752/ 3200]\n",
      "loss: 0.387154  [ 2784/ 3200]\n",
      "loss: 0.401655  [ 2816/ 3200]\n",
      "loss: 0.422684  [ 2848/ 3200]\n",
      "loss: 0.281874  [ 2880/ 3200]\n",
      "loss: 0.426505  [ 2912/ 3200]\n",
      "loss: 0.278149  [ 2944/ 3200]\n",
      "loss: 0.163235  [ 2976/ 3200]\n",
      "loss: 0.207776  [ 3008/ 3200]\n",
      "loss: 0.274263  [ 3040/ 3200]\n",
      "loss: 0.372573  [ 3072/ 3200]\n",
      "loss: 0.604172  [ 3104/ 3200]\n",
      "loss: 0.516174  [ 3136/ 3200]\n",
      "loss: 0.322280  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034478\n",
      "f1 macro averaged score: 0.787201\n",
      "Accuracy               : 78.6%\n",
      "Confusion matrix       :\n",
      "tensor([[173,  16,   0,  11],\n",
      "        [ 13, 134,  18,  35],\n",
      "        [  0,  30, 155,  15],\n",
      "        [  3,  19,  11, 167]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 8.8025e-04.\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 0.308667  [    0/ 3200]\n",
      "loss: 0.198231  [   32/ 3200]\n",
      "loss: 0.706247  [   64/ 3200]\n",
      "loss: 0.656823  [   96/ 3200]\n",
      "loss: 0.216407  [  128/ 3200]\n",
      "loss: 0.351830  [  160/ 3200]\n",
      "loss: 0.506220  [  192/ 3200]\n",
      "loss: 0.297309  [  224/ 3200]\n",
      "loss: 0.212911  [  256/ 3200]\n",
      "loss: 0.434591  [  288/ 3200]\n",
      "loss: 0.457532  [  320/ 3200]\n",
      "loss: 0.463782  [  352/ 3200]\n",
      "loss: 0.422142  [  384/ 3200]\n",
      "loss: 0.253820  [  416/ 3200]\n",
      "loss: 0.361770  [  448/ 3200]\n",
      "loss: 0.205920  [  480/ 3200]\n",
      "loss: 0.337088  [  512/ 3200]\n",
      "loss: 0.427820  [  544/ 3200]\n",
      "loss: 0.376192  [  576/ 3200]\n",
      "loss: 0.351866  [  608/ 3200]\n",
      "loss: 0.338386  [  640/ 3200]\n",
      "loss: 0.469673  [  672/ 3200]\n",
      "loss: 0.455750  [  704/ 3200]\n",
      "loss: 0.344126  [  736/ 3200]\n",
      "loss: 0.212515  [  768/ 3200]\n",
      "loss: 0.348434  [  800/ 3200]\n",
      "loss: 0.491806  [  832/ 3200]\n",
      "loss: 0.225177  [  864/ 3200]\n",
      "loss: 0.315920  [  896/ 3200]\n",
      "loss: 0.295492  [  928/ 3200]\n",
      "loss: 0.248315  [  960/ 3200]\n",
      "loss: 0.235245  [  992/ 3200]\n",
      "loss: 0.177434  [ 1024/ 3200]\n",
      "loss: 0.339225  [ 1056/ 3200]\n",
      "loss: 0.501999  [ 1088/ 3200]\n",
      "loss: 0.589337  [ 1120/ 3200]\n",
      "loss: 0.369207  [ 1152/ 3200]\n",
      "loss: 0.395167  [ 1184/ 3200]\n",
      "loss: 0.475944  [ 1216/ 3200]\n",
      "loss: 0.452619  [ 1248/ 3200]\n",
      "loss: 0.395807  [ 1280/ 3200]\n",
      "loss: 0.433969  [ 1312/ 3200]\n",
      "loss: 0.381050  [ 1344/ 3200]\n",
      "loss: 0.482461  [ 1376/ 3200]\n",
      "loss: 0.318245  [ 1408/ 3200]\n",
      "loss: 0.264664  [ 1440/ 3200]\n",
      "loss: 0.251077  [ 1472/ 3200]\n",
      "loss: 0.279457  [ 1504/ 3200]\n",
      "loss: 0.444127  [ 1536/ 3200]\n",
      "loss: 0.401599  [ 1568/ 3200]\n",
      "loss: 0.637763  [ 1600/ 3200]\n",
      "loss: 0.274775  [ 1632/ 3200]\n",
      "loss: 0.270999  [ 1664/ 3200]\n",
      "loss: 0.298787  [ 1696/ 3200]\n",
      "loss: 0.495525  [ 1728/ 3200]\n",
      "loss: 0.254001  [ 1760/ 3200]\n",
      "loss: 0.703429  [ 1792/ 3200]\n",
      "loss: 0.328024  [ 1824/ 3200]\n",
      "loss: 0.472120  [ 1856/ 3200]\n",
      "loss: 0.283417  [ 1888/ 3200]\n",
      "loss: 0.406941  [ 1920/ 3200]\n",
      "loss: 0.346392  [ 1952/ 3200]\n",
      "loss: 0.289372  [ 1984/ 3200]\n",
      "loss: 0.233066  [ 2016/ 3200]\n",
      "loss: 0.466652  [ 2048/ 3200]\n",
      "loss: 0.159810  [ 2080/ 3200]\n",
      "loss: 0.522610  [ 2112/ 3200]\n",
      "loss: 0.331051  [ 2144/ 3200]\n",
      "loss: 0.289229  [ 2176/ 3200]\n",
      "loss: 0.563747  [ 2208/ 3200]\n",
      "loss: 0.334882  [ 2240/ 3200]\n",
      "loss: 0.335383  [ 2272/ 3200]\n",
      "loss: 0.166138  [ 2304/ 3200]\n",
      "loss: 0.151076  [ 2336/ 3200]\n",
      "loss: 0.419620  [ 2368/ 3200]\n",
      "loss: 0.520967  [ 2400/ 3200]\n",
      "loss: 0.436086  [ 2432/ 3200]\n",
      "loss: 0.196322  [ 2464/ 3200]\n",
      "loss: 0.221486  [ 2496/ 3200]\n",
      "loss: 0.226952  [ 2528/ 3200]\n",
      "loss: 0.495111  [ 2560/ 3200]\n",
      "loss: 0.255366  [ 2592/ 3200]\n",
      "loss: 0.404448  [ 2624/ 3200]\n",
      "loss: 0.385951  [ 2656/ 3200]\n",
      "loss: 0.250451  [ 2688/ 3200]\n",
      "loss: 0.305204  [ 2720/ 3200]\n",
      "loss: 0.333579  [ 2752/ 3200]\n",
      "loss: 0.309414  [ 2784/ 3200]\n",
      "loss: 0.244719  [ 2816/ 3200]\n",
      "loss: 0.414968  [ 2848/ 3200]\n",
      "loss: 0.331485  [ 2880/ 3200]\n",
      "loss: 0.421472  [ 2912/ 3200]\n",
      "loss: 0.327987  [ 2944/ 3200]\n",
      "loss: 0.152201  [ 2976/ 3200]\n",
      "loss: 0.606428  [ 3008/ 3200]\n",
      "loss: 0.365184  [ 3040/ 3200]\n",
      "loss: 0.574487  [ 3072/ 3200]\n",
      "loss: 0.447176  [ 3104/ 3200]\n",
      "loss: 0.460726  [ 3136/ 3200]\n",
      "loss: 0.321309  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.033466\n",
      "f1 macro averaged score: 0.794103\n",
      "Accuracy               : 79.5%\n",
      "Confusion matrix       :\n",
      "tensor([[178,  14,   0,   8],\n",
      "        [ 13, 128,  23,  36],\n",
      "        [  0,  21, 164,  15],\n",
      "        [  4,  18,  12, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 8.3624e-04.\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 0.301165  [    0/ 3200]\n",
      "loss: 0.504424  [   32/ 3200]\n",
      "loss: 0.243334  [   64/ 3200]\n",
      "loss: 0.387872  [   96/ 3200]\n",
      "loss: 0.340451  [  128/ 3200]\n",
      "loss: 0.274431  [  160/ 3200]\n",
      "loss: 0.278916  [  192/ 3200]\n",
      "loss: 0.425906  [  224/ 3200]\n",
      "loss: 0.214105  [  256/ 3200]\n",
      "loss: 0.384074  [  288/ 3200]\n",
      "loss: 0.340631  [  320/ 3200]\n",
      "loss: 0.553619  [  352/ 3200]\n",
      "loss: 0.267081  [  384/ 3200]\n",
      "loss: 0.391128  [  416/ 3200]\n",
      "loss: 0.374705  [  448/ 3200]\n",
      "loss: 0.177335  [  480/ 3200]\n",
      "loss: 0.488891  [  512/ 3200]\n",
      "loss: 0.397627  [  544/ 3200]\n",
      "loss: 0.293732  [  576/ 3200]\n",
      "loss: 0.579746  [  608/ 3200]\n",
      "loss: 0.430436  [  640/ 3200]\n",
      "loss: 0.431886  [  672/ 3200]\n",
      "loss: 0.344479  [  704/ 3200]\n",
      "loss: 0.481728  [  736/ 3200]\n",
      "loss: 0.250369  [  768/ 3200]\n",
      "loss: 0.534217  [  800/ 3200]\n",
      "loss: 0.321661  [  832/ 3200]\n",
      "loss: 0.181395  [  864/ 3200]\n",
      "loss: 0.217358  [  896/ 3200]\n",
      "loss: 0.335457  [  928/ 3200]\n",
      "loss: 0.334707  [  960/ 3200]\n",
      "loss: 0.386252  [  992/ 3200]\n",
      "loss: 0.439204  [ 1024/ 3200]\n",
      "loss: 0.497680  [ 1056/ 3200]\n",
      "loss: 0.393544  [ 1088/ 3200]\n",
      "loss: 0.237111  [ 1120/ 3200]\n",
      "loss: 0.434304  [ 1152/ 3200]\n",
      "loss: 0.228007  [ 1184/ 3200]\n",
      "loss: 0.363621  [ 1216/ 3200]\n",
      "loss: 0.538425  [ 1248/ 3200]\n",
      "loss: 0.148718  [ 1280/ 3200]\n",
      "loss: 0.219763  [ 1312/ 3200]\n",
      "loss: 0.205716  [ 1344/ 3200]\n",
      "loss: 0.432433  [ 1376/ 3200]\n",
      "loss: 0.226300  [ 1408/ 3200]\n",
      "loss: 0.327305  [ 1440/ 3200]\n",
      "loss: 0.376181  [ 1472/ 3200]\n",
      "loss: 0.308193  [ 1504/ 3200]\n",
      "loss: 0.352438  [ 1536/ 3200]\n",
      "loss: 0.515735  [ 1568/ 3200]\n",
      "loss: 0.271352  [ 1600/ 3200]\n",
      "loss: 0.434125  [ 1632/ 3200]\n",
      "loss: 0.275448  [ 1664/ 3200]\n",
      "loss: 0.277713  [ 1696/ 3200]\n",
      "loss: 0.402742  [ 1728/ 3200]\n",
      "loss: 0.431165  [ 1760/ 3200]\n",
      "loss: 0.320693  [ 1792/ 3200]\n",
      "loss: 0.336882  [ 1824/ 3200]\n",
      "loss: 0.279442  [ 1856/ 3200]\n",
      "loss: 0.139648  [ 1888/ 3200]\n",
      "loss: 0.334326  [ 1920/ 3200]\n",
      "loss: 0.465408  [ 1952/ 3200]\n",
      "loss: 0.265710  [ 1984/ 3200]\n",
      "loss: 0.459082  [ 2016/ 3200]\n",
      "loss: 0.203121  [ 2048/ 3200]\n",
      "loss: 0.201715  [ 2080/ 3200]\n",
      "loss: 0.501739  [ 2112/ 3200]\n",
      "loss: 0.375765  [ 2144/ 3200]\n",
      "loss: 0.313909  [ 2176/ 3200]\n",
      "loss: 0.299009  [ 2208/ 3200]\n",
      "loss: 0.259822  [ 2240/ 3200]\n",
      "loss: 0.253750  [ 2272/ 3200]\n",
      "loss: 0.224799  [ 2304/ 3200]\n",
      "loss: 0.613649  [ 2336/ 3200]\n",
      "loss: 0.209098  [ 2368/ 3200]\n",
      "loss: 0.297607  [ 2400/ 3200]\n",
      "loss: 0.339033  [ 2432/ 3200]\n",
      "loss: 0.289247  [ 2464/ 3200]\n",
      "loss: 0.354011  [ 2496/ 3200]\n",
      "loss: 0.455296  [ 2528/ 3200]\n",
      "loss: 0.440405  [ 2560/ 3200]\n",
      "loss: 0.621276  [ 2592/ 3200]\n",
      "loss: 0.297835  [ 2624/ 3200]\n",
      "loss: 0.221206  [ 2656/ 3200]\n",
      "loss: 0.402011  [ 2688/ 3200]\n",
      "loss: 0.469395  [ 2720/ 3200]\n",
      "loss: 0.542842  [ 2752/ 3200]\n",
      "loss: 0.394334  [ 2784/ 3200]\n",
      "loss: 0.451586  [ 2816/ 3200]\n",
      "loss: 0.313912  [ 2848/ 3200]\n",
      "loss: 0.411568  [ 2880/ 3200]\n",
      "loss: 0.221198  [ 2912/ 3200]\n",
      "loss: 0.344250  [ 2944/ 3200]\n",
      "loss: 0.462024  [ 2976/ 3200]\n",
      "loss: 0.514658  [ 3008/ 3200]\n",
      "loss: 0.323450  [ 3040/ 3200]\n",
      "loss: 0.447179  [ 3072/ 3200]\n",
      "loss: 0.360066  [ 3104/ 3200]\n",
      "loss: 0.589404  [ 3136/ 3200]\n",
      "loss: 0.355911  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034085\n",
      "f1 macro averaged score: 0.791665\n",
      "Accuracy               : 79.4%\n",
      "Confusion matrix       :\n",
      "tensor([[181,  10,   0,   9],\n",
      "        [ 17, 125,  21,  37],\n",
      "        [  0,  19, 163,  18],\n",
      "        [  7,  15,  12, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.9443e-04.\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 0.300527  [    0/ 3200]\n",
      "loss: 0.277439  [   32/ 3200]\n",
      "loss: 0.337035  [   64/ 3200]\n",
      "loss: 0.363807  [   96/ 3200]\n",
      "loss: 0.215471  [  128/ 3200]\n",
      "loss: 0.266637  [  160/ 3200]\n",
      "loss: 0.309335  [  192/ 3200]\n",
      "loss: 0.428517  [  224/ 3200]\n",
      "loss: 0.432414  [  256/ 3200]\n",
      "loss: 0.185938  [  288/ 3200]\n",
      "loss: 0.287712  [  320/ 3200]\n",
      "loss: 0.614628  [  352/ 3200]\n",
      "loss: 0.702546  [  384/ 3200]\n",
      "loss: 0.216365  [  416/ 3200]\n",
      "loss: 0.272608  [  448/ 3200]\n",
      "loss: 0.420200  [  480/ 3200]\n",
      "loss: 0.190011  [  512/ 3200]\n",
      "loss: 0.289948  [  544/ 3200]\n",
      "loss: 0.267576  [  576/ 3200]\n",
      "loss: 0.280293  [  608/ 3200]\n",
      "loss: 0.340269  [  640/ 3200]\n",
      "loss: 0.354376  [  672/ 3200]\n",
      "loss: 0.212693  [  704/ 3200]\n",
      "loss: 0.416661  [  736/ 3200]\n",
      "loss: 0.458302  [  768/ 3200]\n",
      "loss: 0.473421  [  800/ 3200]\n",
      "loss: 0.354753  [  832/ 3200]\n",
      "loss: 0.579768  [  864/ 3200]\n",
      "loss: 0.360061  [  896/ 3200]\n",
      "loss: 0.361603  [  928/ 3200]\n",
      "loss: 0.333069  [  960/ 3200]\n",
      "loss: 0.258272  [  992/ 3200]\n",
      "loss: 0.189466  [ 1024/ 3200]\n",
      "loss: 0.210621  [ 1056/ 3200]\n",
      "loss: 0.261868  [ 1088/ 3200]\n",
      "loss: 0.492636  [ 1120/ 3200]\n",
      "loss: 0.390961  [ 1152/ 3200]\n",
      "loss: 0.699406  [ 1184/ 3200]\n",
      "loss: 0.483612  [ 1216/ 3200]\n",
      "loss: 0.483967  [ 1248/ 3200]\n",
      "loss: 0.330816  [ 1280/ 3200]\n",
      "loss: 0.426598  [ 1312/ 3200]\n",
      "loss: 0.719494  [ 1344/ 3200]\n",
      "loss: 0.374239  [ 1376/ 3200]\n",
      "loss: 0.486636  [ 1408/ 3200]\n",
      "loss: 0.398440  [ 1440/ 3200]\n",
      "loss: 0.375819  [ 1472/ 3200]\n",
      "loss: 0.215577  [ 1504/ 3200]\n",
      "loss: 0.291045  [ 1536/ 3200]\n",
      "loss: 0.334928  [ 1568/ 3200]\n",
      "loss: 0.247100  [ 1600/ 3200]\n",
      "loss: 0.241115  [ 1632/ 3200]\n",
      "loss: 0.495486  [ 1664/ 3200]\n",
      "loss: 0.233987  [ 1696/ 3200]\n",
      "loss: 0.323363  [ 1728/ 3200]\n",
      "loss: 0.204508  [ 1760/ 3200]\n",
      "loss: 0.206883  [ 1792/ 3200]\n",
      "loss: 0.518293  [ 1824/ 3200]\n",
      "loss: 0.479073  [ 1856/ 3200]\n",
      "loss: 0.233435  [ 1888/ 3200]\n",
      "loss: 0.350170  [ 1920/ 3200]\n",
      "loss: 0.221632  [ 1952/ 3200]\n",
      "loss: 0.226478  [ 1984/ 3200]\n",
      "loss: 0.511426  [ 2016/ 3200]\n",
      "loss: 0.386698  [ 2048/ 3200]\n",
      "loss: 0.310544  [ 2080/ 3200]\n",
      "loss: 0.267399  [ 2112/ 3200]\n",
      "loss: 0.502601  [ 2144/ 3200]\n",
      "loss: 0.260173  [ 2176/ 3200]\n",
      "loss: 0.313530  [ 2208/ 3200]\n",
      "loss: 0.213678  [ 2240/ 3200]\n",
      "loss: 0.460315  [ 2272/ 3200]\n",
      "loss: 0.311644  [ 2304/ 3200]\n",
      "loss: 0.464737  [ 2336/ 3200]\n",
      "loss: 0.442915  [ 2368/ 3200]\n",
      "loss: 0.504719  [ 2400/ 3200]\n",
      "loss: 0.267821  [ 2432/ 3200]\n",
      "loss: 0.494298  [ 2464/ 3200]\n",
      "loss: 0.255524  [ 2496/ 3200]\n",
      "loss: 0.266064  [ 2528/ 3200]\n",
      "loss: 0.320080  [ 2560/ 3200]\n",
      "loss: 0.302888  [ 2592/ 3200]\n",
      "loss: 0.402673  [ 2624/ 3200]\n",
      "loss: 0.167485  [ 2656/ 3200]\n",
      "loss: 0.114189  [ 2688/ 3200]\n",
      "loss: 0.434945  [ 2720/ 3200]\n",
      "loss: 0.358958  [ 2752/ 3200]\n",
      "loss: 0.519933  [ 2784/ 3200]\n",
      "loss: 0.463115  [ 2816/ 3200]\n",
      "loss: 0.223172  [ 2848/ 3200]\n",
      "loss: 0.147850  [ 2880/ 3200]\n",
      "loss: 0.601323  [ 2912/ 3200]\n",
      "loss: 0.249987  [ 2944/ 3200]\n",
      "loss: 0.279759  [ 2976/ 3200]\n",
      "loss: 0.266971  [ 3008/ 3200]\n",
      "loss: 0.228036  [ 3040/ 3200]\n",
      "loss: 0.404286  [ 3072/ 3200]\n",
      "loss: 0.223729  [ 3104/ 3200]\n",
      "loss: 0.293846  [ 3136/ 3200]\n",
      "loss: 0.511945  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.033690\n",
      "f1 macro averaged score: 0.792701\n",
      "Accuracy               : 79.2%\n",
      "Confusion matrix       :\n",
      "tensor([[176,  16,   0,   8],\n",
      "        [ 13, 135,  25,  27],\n",
      "        [  0,  22, 166,  12],\n",
      "        [  5,  25,  13, 157]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.5471e-04.\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 0.438297  [    0/ 3200]\n",
      "loss: 0.160135  [   32/ 3200]\n",
      "loss: 0.359757  [   64/ 3200]\n",
      "loss: 0.262612  [   96/ 3200]\n",
      "loss: 0.489249  [  128/ 3200]\n",
      "loss: 0.436591  [  160/ 3200]\n",
      "loss: 0.408679  [  192/ 3200]\n",
      "loss: 0.532551  [  224/ 3200]\n",
      "loss: 0.455056  [  256/ 3200]\n",
      "loss: 0.515501  [  288/ 3200]\n",
      "loss: 0.428101  [  320/ 3200]\n",
      "loss: 0.368940  [  352/ 3200]\n",
      "loss: 0.311657  [  384/ 3200]\n",
      "loss: 0.422233  [  416/ 3200]\n",
      "loss: 0.422099  [  448/ 3200]\n",
      "loss: 0.636497  [  480/ 3200]\n",
      "loss: 0.319998  [  512/ 3200]\n",
      "loss: 0.216067  [  544/ 3200]\n",
      "loss: 0.383587  [  576/ 3200]\n",
      "loss: 0.237506  [  608/ 3200]\n",
      "loss: 0.461021  [  640/ 3200]\n",
      "loss: 0.337412  [  672/ 3200]\n",
      "loss: 0.273250  [  704/ 3200]\n",
      "loss: 0.362517  [  736/ 3200]\n",
      "loss: 0.230198  [  768/ 3200]\n",
      "loss: 0.293494  [  800/ 3200]\n",
      "loss: 0.216531  [  832/ 3200]\n",
      "loss: 0.390268  [  864/ 3200]\n",
      "loss: 0.259002  [  896/ 3200]\n",
      "loss: 0.251166  [  928/ 3200]\n",
      "loss: 0.222598  [  960/ 3200]\n",
      "loss: 0.463459  [  992/ 3200]\n",
      "loss: 0.346515  [ 1024/ 3200]\n",
      "loss: 0.299975  [ 1056/ 3200]\n",
      "loss: 0.486574  [ 1088/ 3200]\n",
      "loss: 0.231382  [ 1120/ 3200]\n",
      "loss: 0.272855  [ 1152/ 3200]\n",
      "loss: 0.385146  [ 1184/ 3200]\n",
      "loss: 0.354178  [ 1216/ 3200]\n",
      "loss: 0.374824  [ 1248/ 3200]\n",
      "loss: 0.251534  [ 1280/ 3200]\n",
      "loss: 0.387286  [ 1312/ 3200]\n",
      "loss: 0.312557  [ 1344/ 3200]\n",
      "loss: 0.247897  [ 1376/ 3200]\n",
      "loss: 0.394089  [ 1408/ 3200]\n",
      "loss: 0.567123  [ 1440/ 3200]\n",
      "loss: 0.475633  [ 1472/ 3200]\n",
      "loss: 0.456842  [ 1504/ 3200]\n",
      "loss: 0.221594  [ 1536/ 3200]\n",
      "loss: 0.378281  [ 1568/ 3200]\n",
      "loss: 0.400035  [ 1600/ 3200]\n",
      "loss: 0.226703  [ 1632/ 3200]\n",
      "loss: 0.266569  [ 1664/ 3200]\n",
      "loss: 0.259214  [ 1696/ 3200]\n",
      "loss: 0.234742  [ 1728/ 3200]\n",
      "loss: 0.205655  [ 1760/ 3200]\n",
      "loss: 0.286429  [ 1792/ 3200]\n",
      "loss: 0.338813  [ 1824/ 3200]\n",
      "loss: 0.385615  [ 1856/ 3200]\n",
      "loss: 0.183904  [ 1888/ 3200]\n",
      "loss: 0.262787  [ 1920/ 3200]\n",
      "loss: 0.405541  [ 1952/ 3200]\n",
      "loss: 0.286241  [ 1984/ 3200]\n",
      "loss: 0.211851  [ 2016/ 3200]\n",
      "loss: 0.265741  [ 2048/ 3200]\n",
      "loss: 0.312139  [ 2080/ 3200]\n",
      "loss: 0.308674  [ 2112/ 3200]\n",
      "loss: 0.265524  [ 2144/ 3200]\n",
      "loss: 0.247709  [ 2176/ 3200]\n",
      "loss: 0.238150  [ 2208/ 3200]\n",
      "loss: 0.287794  [ 2240/ 3200]\n",
      "loss: 0.481509  [ 2272/ 3200]\n",
      "loss: 0.194713  [ 2304/ 3200]\n",
      "loss: 0.359067  [ 2336/ 3200]\n",
      "loss: 0.476095  [ 2368/ 3200]\n",
      "loss: 0.461995  [ 2400/ 3200]\n",
      "loss: 0.447039  [ 2432/ 3200]\n",
      "loss: 0.288942  [ 2464/ 3200]\n",
      "loss: 0.359598  [ 2496/ 3200]\n",
      "loss: 0.204853  [ 2528/ 3200]\n",
      "loss: 0.308345  [ 2560/ 3200]\n",
      "loss: 0.235187  [ 2592/ 3200]\n",
      "loss: 0.359589  [ 2624/ 3200]\n",
      "loss: 0.558252  [ 2656/ 3200]\n",
      "loss: 0.167247  [ 2688/ 3200]\n",
      "loss: 0.243335  [ 2720/ 3200]\n",
      "loss: 0.357240  [ 2752/ 3200]\n",
      "loss: 0.238347  [ 2784/ 3200]\n",
      "loss: 0.382592  [ 2816/ 3200]\n",
      "loss: 0.295610  [ 2848/ 3200]\n",
      "loss: 0.467167  [ 2880/ 3200]\n",
      "loss: 0.355483  [ 2912/ 3200]\n",
      "loss: 0.194723  [ 2944/ 3200]\n",
      "loss: 0.349255  [ 2976/ 3200]\n",
      "loss: 0.242368  [ 3008/ 3200]\n",
      "loss: 0.428618  [ 3040/ 3200]\n",
      "loss: 0.282544  [ 3072/ 3200]\n",
      "loss: 0.363337  [ 3104/ 3200]\n",
      "loss: 0.409857  [ 3136/ 3200]\n",
      "loss: 0.208020  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035313\n",
      "f1 macro averaged score: 0.782042\n",
      "Accuracy               : 78.5%\n",
      "Confusion matrix       :\n",
      "tensor([[177,  12,   0,  11],\n",
      "        [ 12, 114,  24,  50],\n",
      "        [  0,  17, 164,  19],\n",
      "        [  5,  10,  12, 173]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.1697e-04.\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.352974  [    0/ 3200]\n",
      "loss: 0.383294  [   32/ 3200]\n",
      "loss: 0.245248  [   64/ 3200]\n",
      "loss: 0.252026  [   96/ 3200]\n",
      "loss: 0.405817  [  128/ 3200]\n",
      "loss: 0.465005  [  160/ 3200]\n",
      "loss: 0.312482  [  192/ 3200]\n",
      "loss: 0.382512  [  224/ 3200]\n",
      "loss: 0.304349  [  256/ 3200]\n",
      "loss: 0.491500  [  288/ 3200]\n",
      "loss: 0.578668  [  320/ 3200]\n",
      "loss: 0.293678  [  352/ 3200]\n",
      "loss: 0.126651  [  384/ 3200]\n",
      "loss: 0.290931  [  416/ 3200]\n",
      "loss: 0.214915  [  448/ 3200]\n",
      "loss: 0.372881  [  480/ 3200]\n",
      "loss: 0.207394  [  512/ 3200]\n",
      "loss: 0.532732  [  544/ 3200]\n",
      "loss: 0.302857  [  576/ 3200]\n",
      "loss: 0.254467  [  608/ 3200]\n",
      "loss: 0.287426  [  640/ 3200]\n",
      "loss: 0.408427  [  672/ 3200]\n",
      "loss: 0.300966  [  704/ 3200]\n",
      "loss: 0.451345  [  736/ 3200]\n",
      "loss: 0.231635  [  768/ 3200]\n",
      "loss: 0.473795  [  800/ 3200]\n",
      "loss: 0.309272  [  832/ 3200]\n",
      "loss: 0.476731  [  864/ 3200]\n",
      "loss: 0.244331  [  896/ 3200]\n",
      "loss: 0.362541  [  928/ 3200]\n",
      "loss: 0.375205  [  960/ 3200]\n",
      "loss: 0.426124  [  992/ 3200]\n",
      "loss: 0.434021  [ 1024/ 3200]\n",
      "loss: 0.296603  [ 1056/ 3200]\n",
      "loss: 0.271830  [ 1088/ 3200]\n",
      "loss: 0.345705  [ 1120/ 3200]\n",
      "loss: 0.275435  [ 1152/ 3200]\n",
      "loss: 0.324584  [ 1184/ 3200]\n",
      "loss: 0.121618  [ 1216/ 3200]\n",
      "loss: 0.303741  [ 1248/ 3200]\n",
      "loss: 0.180241  [ 1280/ 3200]\n",
      "loss: 0.424016  [ 1312/ 3200]\n",
      "loss: 0.266568  [ 1344/ 3200]\n",
      "loss: 0.325756  [ 1376/ 3200]\n",
      "loss: 0.430797  [ 1408/ 3200]\n",
      "loss: 0.270089  [ 1440/ 3200]\n",
      "loss: 0.492935  [ 1472/ 3200]\n",
      "loss: 0.251745  [ 1504/ 3200]\n",
      "loss: 0.314745  [ 1536/ 3200]\n",
      "loss: 0.227198  [ 1568/ 3200]\n",
      "loss: 0.313127  [ 1600/ 3200]\n",
      "loss: 0.285655  [ 1632/ 3200]\n",
      "loss: 0.287892  [ 1664/ 3200]\n",
      "loss: 0.548910  [ 1696/ 3200]\n",
      "loss: 0.268260  [ 1728/ 3200]\n",
      "loss: 0.288164  [ 1760/ 3200]\n",
      "loss: 0.314920  [ 1792/ 3200]\n",
      "loss: 0.312872  [ 1824/ 3200]\n",
      "loss: 0.313253  [ 1856/ 3200]\n",
      "loss: 0.249121  [ 1888/ 3200]\n",
      "loss: 0.475762  [ 1920/ 3200]\n",
      "loss: 0.261042  [ 1952/ 3200]\n",
      "loss: 0.290365  [ 1984/ 3200]\n",
      "loss: 0.413582  [ 2016/ 3200]\n",
      "loss: 0.301803  [ 2048/ 3200]\n",
      "loss: 0.216217  [ 2080/ 3200]\n",
      "loss: 0.414811  [ 2112/ 3200]\n",
      "loss: 0.492500  [ 2144/ 3200]\n",
      "loss: 0.186636  [ 2176/ 3200]\n",
      "loss: 0.282191  [ 2208/ 3200]\n",
      "loss: 0.280018  [ 2240/ 3200]\n",
      "loss: 0.248919  [ 2272/ 3200]\n",
      "loss: 0.341244  [ 2304/ 3200]\n",
      "loss: 0.278089  [ 2336/ 3200]\n",
      "loss: 0.554073  [ 2368/ 3200]\n",
      "loss: 0.264127  [ 2400/ 3200]\n",
      "loss: 0.291620  [ 2432/ 3200]\n",
      "loss: 0.319427  [ 2464/ 3200]\n",
      "loss: 0.481996  [ 2496/ 3200]\n",
      "loss: 0.281799  [ 2528/ 3200]\n",
      "loss: 0.443177  [ 2560/ 3200]\n",
      "loss: 0.300887  [ 2592/ 3200]\n",
      "loss: 0.253306  [ 2624/ 3200]\n",
      "loss: 0.253710  [ 2656/ 3200]\n",
      "loss: 0.676643  [ 2688/ 3200]\n",
      "loss: 0.111492  [ 2720/ 3200]\n",
      "loss: 0.252131  [ 2752/ 3200]\n",
      "loss: 0.392433  [ 2784/ 3200]\n",
      "loss: 0.319252  [ 2816/ 3200]\n",
      "loss: 0.403387  [ 2848/ 3200]\n",
      "loss: 0.426693  [ 2880/ 3200]\n",
      "loss: 0.350925  [ 2912/ 3200]\n",
      "loss: 0.216616  [ 2944/ 3200]\n",
      "loss: 0.383435  [ 2976/ 3200]\n",
      "loss: 0.415359  [ 3008/ 3200]\n",
      "loss: 0.380536  [ 3040/ 3200]\n",
      "loss: 0.435787  [ 3072/ 3200]\n",
      "loss: 0.262378  [ 3104/ 3200]\n",
      "loss: 0.333768  [ 3136/ 3200]\n",
      "loss: 0.352128  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034780\n",
      "f1 macro averaged score: 0.796995\n",
      "Accuracy               : 80.0%\n",
      "Confusion matrix       :\n",
      "tensor([[186,   6,   0,   8],\n",
      "        [ 16, 120,  17,  47],\n",
      "        [  0,  20, 161,  19],\n",
      "        [  6,  11,  10, 173]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.8112e-04.\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 0.371865  [    0/ 3200]\n",
      "loss: 0.255425  [   32/ 3200]\n",
      "loss: 0.191574  [   64/ 3200]\n",
      "loss: 0.420546  [   96/ 3200]\n",
      "loss: 0.368673  [  128/ 3200]\n",
      "loss: 0.250951  [  160/ 3200]\n",
      "loss: 0.363444  [  192/ 3200]\n",
      "loss: 0.269035  [  224/ 3200]\n",
      "loss: 0.300419  [  256/ 3200]\n",
      "loss: 0.522532  [  288/ 3200]\n",
      "loss: 0.106623  [  320/ 3200]\n",
      "loss: 0.599177  [  352/ 3200]\n",
      "loss: 0.352433  [  384/ 3200]\n",
      "loss: 0.187879  [  416/ 3200]\n",
      "loss: 0.249313  [  448/ 3200]\n",
      "loss: 0.251744  [  480/ 3200]\n",
      "loss: 0.180503  [  512/ 3200]\n",
      "loss: 0.253097  [  544/ 3200]\n",
      "loss: 0.165361  [  576/ 3200]\n",
      "loss: 0.219672  [  608/ 3200]\n",
      "loss: 0.288949  [  640/ 3200]\n",
      "loss: 0.263252  [  672/ 3200]\n",
      "loss: 0.376296  [  704/ 3200]\n",
      "loss: 0.304114  [  736/ 3200]\n",
      "loss: 0.304710  [  768/ 3200]\n",
      "loss: 0.243600  [  800/ 3200]\n",
      "loss: 0.213761  [  832/ 3200]\n",
      "loss: 0.279375  [  864/ 3200]\n",
      "loss: 0.397332  [  896/ 3200]\n",
      "loss: 0.365631  [  928/ 3200]\n",
      "loss: 0.533559  [  960/ 3200]\n",
      "loss: 0.271267  [  992/ 3200]\n",
      "loss: 0.403422  [ 1024/ 3200]\n",
      "loss: 0.338297  [ 1056/ 3200]\n",
      "loss: 0.220109  [ 1088/ 3200]\n",
      "loss: 0.199103  [ 1120/ 3200]\n",
      "loss: 0.431890  [ 1152/ 3200]\n",
      "loss: 0.276800  [ 1184/ 3200]\n",
      "loss: 0.200567  [ 1216/ 3200]\n",
      "loss: 0.283354  [ 1248/ 3200]\n",
      "loss: 0.278633  [ 1280/ 3200]\n",
      "loss: 0.175877  [ 1312/ 3200]\n",
      "loss: 0.209349  [ 1344/ 3200]\n",
      "loss: 0.247760  [ 1376/ 3200]\n",
      "loss: 0.267150  [ 1408/ 3200]\n",
      "loss: 0.462211  [ 1440/ 3200]\n",
      "loss: 0.201687  [ 1472/ 3200]\n",
      "loss: 0.266318  [ 1504/ 3200]\n",
      "loss: 0.241253  [ 1536/ 3200]\n",
      "loss: 0.296495  [ 1568/ 3200]\n",
      "loss: 0.731813  [ 1600/ 3200]\n",
      "loss: 0.441682  [ 1632/ 3200]\n",
      "loss: 0.151935  [ 1664/ 3200]\n",
      "loss: 0.317256  [ 1696/ 3200]\n",
      "loss: 0.231460  [ 1728/ 3200]\n",
      "loss: 0.222517  [ 1760/ 3200]\n",
      "loss: 0.323325  [ 1792/ 3200]\n",
      "loss: 0.463596  [ 1824/ 3200]\n",
      "loss: 0.298003  [ 1856/ 3200]\n",
      "loss: 0.221886  [ 1888/ 3200]\n",
      "loss: 0.215122  [ 1920/ 3200]\n",
      "loss: 0.471121  [ 1952/ 3200]\n",
      "loss: 0.359880  [ 1984/ 3200]\n",
      "loss: 0.309714  [ 2016/ 3200]\n",
      "loss: 0.161884  [ 2048/ 3200]\n",
      "loss: 0.365751  [ 2080/ 3200]\n",
      "loss: 0.345250  [ 2112/ 3200]\n",
      "loss: 0.484134  [ 2144/ 3200]\n",
      "loss: 0.341487  [ 2176/ 3200]\n",
      "loss: 0.289713  [ 2208/ 3200]\n",
      "loss: 0.246303  [ 2240/ 3200]\n",
      "loss: 0.293164  [ 2272/ 3200]\n",
      "loss: 0.263381  [ 2304/ 3200]\n",
      "loss: 0.391539  [ 2336/ 3200]\n",
      "loss: 0.654350  [ 2368/ 3200]\n",
      "loss: 0.328920  [ 2400/ 3200]\n",
      "loss: 0.320238  [ 2432/ 3200]\n",
      "loss: 0.370812  [ 2464/ 3200]\n",
      "loss: 0.246796  [ 2496/ 3200]\n",
      "loss: 0.260013  [ 2528/ 3200]\n",
      "loss: 0.301791  [ 2560/ 3200]\n",
      "loss: 0.663487  [ 2592/ 3200]\n",
      "loss: 0.541714  [ 2624/ 3200]\n",
      "loss: 0.339942  [ 2656/ 3200]\n",
      "loss: 0.238165  [ 2688/ 3200]\n",
      "loss: 0.193077  [ 2720/ 3200]\n",
      "loss: 0.261212  [ 2752/ 3200]\n",
      "loss: 0.407848  [ 2784/ 3200]\n",
      "loss: 0.486801  [ 2816/ 3200]\n",
      "loss: 0.233179  [ 2848/ 3200]\n",
      "loss: 0.355809  [ 2880/ 3200]\n",
      "loss: 0.357551  [ 2912/ 3200]\n",
      "loss: 0.478166  [ 2944/ 3200]\n",
      "loss: 0.139338  [ 2976/ 3200]\n",
      "loss: 0.435418  [ 3008/ 3200]\n",
      "loss: 0.558897  [ 3040/ 3200]\n",
      "loss: 0.591169  [ 3072/ 3200]\n",
      "loss: 0.527420  [ 3104/ 3200]\n",
      "loss: 0.108516  [ 3136/ 3200]\n",
      "loss: 0.372426  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034003\n",
      "f1 macro averaged score: 0.789651\n",
      "Accuracy               : 79.0%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  16,   0,  10],\n",
      "        [ 12, 129,  24,  35],\n",
      "        [  0,  20, 164,  16],\n",
      "        [  5,  19,  11, 165]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.4707e-04.\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.433798  [    0/ 3200]\n",
      "loss: 0.277396  [   32/ 3200]\n",
      "loss: 0.161492  [   64/ 3200]\n",
      "loss: 0.302139  [   96/ 3200]\n",
      "loss: 0.345668  [  128/ 3200]\n",
      "loss: 0.141909  [  160/ 3200]\n",
      "loss: 0.382989  [  192/ 3200]\n",
      "loss: 0.209326  [  224/ 3200]\n",
      "loss: 0.196725  [  256/ 3200]\n",
      "loss: 0.238445  [  288/ 3200]\n",
      "loss: 0.350235  [  320/ 3200]\n",
      "loss: 0.442565  [  352/ 3200]\n",
      "loss: 0.352983  [  384/ 3200]\n",
      "loss: 0.321050  [  416/ 3200]\n",
      "loss: 0.377818  [  448/ 3200]\n",
      "loss: 0.203459  [  480/ 3200]\n",
      "loss: 0.295790  [  512/ 3200]\n",
      "loss: 0.282393  [  544/ 3200]\n",
      "loss: 0.348988  [  576/ 3200]\n",
      "loss: 0.176490  [  608/ 3200]\n",
      "loss: 0.496353  [  640/ 3200]\n",
      "loss: 0.186930  [  672/ 3200]\n",
      "loss: 0.227991  [  704/ 3200]\n",
      "loss: 0.604432  [  736/ 3200]\n",
      "loss: 0.286565  [  768/ 3200]\n",
      "loss: 0.308886  [  800/ 3200]\n",
      "loss: 0.266654  [  832/ 3200]\n",
      "loss: 0.241403  [  864/ 3200]\n",
      "loss: 0.238653  [  896/ 3200]\n",
      "loss: 0.183094  [  928/ 3200]\n",
      "loss: 0.194494  [  960/ 3200]\n",
      "loss: 0.370186  [  992/ 3200]\n",
      "loss: 0.165608  [ 1024/ 3200]\n",
      "loss: 0.291271  [ 1056/ 3200]\n",
      "loss: 0.362954  [ 1088/ 3200]\n",
      "loss: 0.448610  [ 1120/ 3200]\n",
      "loss: 0.230642  [ 1152/ 3200]\n",
      "loss: 0.161219  [ 1184/ 3200]\n",
      "loss: 0.196849  [ 1216/ 3200]\n",
      "loss: 0.285718  [ 1248/ 3200]\n",
      "loss: 0.199302  [ 1280/ 3200]\n",
      "loss: 0.233790  [ 1312/ 3200]\n",
      "loss: 0.208719  [ 1344/ 3200]\n",
      "loss: 0.294994  [ 1376/ 3200]\n",
      "loss: 0.550475  [ 1408/ 3200]\n",
      "loss: 0.441041  [ 1440/ 3200]\n",
      "loss: 0.207320  [ 1472/ 3200]\n",
      "loss: 0.363267  [ 1504/ 3200]\n",
      "loss: 0.261553  [ 1536/ 3200]\n",
      "loss: 0.232986  [ 1568/ 3200]\n",
      "loss: 0.300697  [ 1600/ 3200]\n",
      "loss: 0.278751  [ 1632/ 3200]\n",
      "loss: 0.446146  [ 1664/ 3200]\n",
      "loss: 0.406667  [ 1696/ 3200]\n",
      "loss: 0.266342  [ 1728/ 3200]\n",
      "loss: 0.421391  [ 1760/ 3200]\n",
      "loss: 0.370504  [ 1792/ 3200]\n",
      "loss: 0.315446  [ 1824/ 3200]\n",
      "loss: 0.482784  [ 1856/ 3200]\n",
      "loss: 0.161331  [ 1888/ 3200]\n",
      "loss: 0.200846  [ 1920/ 3200]\n",
      "loss: 0.187248  [ 1952/ 3200]\n",
      "loss: 0.617019  [ 1984/ 3200]\n",
      "loss: 0.208760  [ 2016/ 3200]\n",
      "loss: 0.256883  [ 2048/ 3200]\n",
      "loss: 0.460387  [ 2080/ 3200]\n",
      "loss: 0.495191  [ 2112/ 3200]\n",
      "loss: 0.304573  [ 2144/ 3200]\n",
      "loss: 0.222116  [ 2176/ 3200]\n",
      "loss: 0.471083  [ 2208/ 3200]\n",
      "loss: 0.403682  [ 2240/ 3200]\n",
      "loss: 0.310432  [ 2272/ 3200]\n",
      "loss: 0.229059  [ 2304/ 3200]\n",
      "loss: 0.317307  [ 2336/ 3200]\n",
      "loss: 0.477256  [ 2368/ 3200]\n",
      "loss: 0.374259  [ 2400/ 3200]\n",
      "loss: 0.280495  [ 2432/ 3200]\n",
      "loss: 0.292727  [ 2464/ 3200]\n",
      "loss: 0.317904  [ 2496/ 3200]\n",
      "loss: 0.384209  [ 2528/ 3200]\n",
      "loss: 0.273523  [ 2560/ 3200]\n",
      "loss: 0.234660  [ 2592/ 3200]\n",
      "loss: 0.133832  [ 2624/ 3200]\n",
      "loss: 0.394262  [ 2656/ 3200]\n",
      "loss: 0.235740  [ 2688/ 3200]\n",
      "loss: 0.168263  [ 2720/ 3200]\n",
      "loss: 0.256160  [ 2752/ 3200]\n",
      "loss: 0.375287  [ 2784/ 3200]\n",
      "loss: 0.348776  [ 2816/ 3200]\n",
      "loss: 0.243392  [ 2848/ 3200]\n",
      "loss: 0.220964  [ 2880/ 3200]\n",
      "loss: 0.292375  [ 2912/ 3200]\n",
      "loss: 0.301794  [ 2944/ 3200]\n",
      "loss: 0.590889  [ 2976/ 3200]\n",
      "loss: 0.542425  [ 3008/ 3200]\n",
      "loss: 0.273220  [ 3040/ 3200]\n",
      "loss: 0.196016  [ 3072/ 3200]\n",
      "loss: 0.236154  [ 3104/ 3200]\n",
      "loss: 0.233602  [ 3136/ 3200]\n",
      "loss: 0.355699  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036257\n",
      "f1 macro averaged score: 0.784458\n",
      "Accuracy               : 78.8%\n",
      "Confusion matrix       :\n",
      "tensor([[182,   5,   0,  13],\n",
      "        [ 16, 116,  19,  49],\n",
      "        [  0,  19, 157,  24],\n",
      "        [  8,   7,  10, 175]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.1471e-04.\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.191045  [    0/ 3200]\n",
      "loss: 0.442842  [   32/ 3200]\n",
      "loss: 0.219884  [   64/ 3200]\n",
      "loss: 0.255528  [   96/ 3200]\n",
      "loss: 0.497702  [  128/ 3200]\n",
      "loss: 0.138097  [  160/ 3200]\n",
      "loss: 0.353304  [  192/ 3200]\n",
      "loss: 0.244738  [  224/ 3200]\n",
      "loss: 0.308173  [  256/ 3200]\n",
      "loss: 0.188118  [  288/ 3200]\n",
      "loss: 0.343827  [  320/ 3200]\n",
      "loss: 0.523048  [  352/ 3200]\n",
      "loss: 0.224420  [  384/ 3200]\n",
      "loss: 0.331938  [  416/ 3200]\n",
      "loss: 0.241042  [  448/ 3200]\n",
      "loss: 0.303865  [  480/ 3200]\n",
      "loss: 0.326481  [  512/ 3200]\n",
      "loss: 0.181825  [  544/ 3200]\n",
      "loss: 0.361760  [  576/ 3200]\n",
      "loss: 0.503590  [  608/ 3200]\n",
      "loss: 0.133021  [  640/ 3200]\n",
      "loss: 0.263982  [  672/ 3200]\n",
      "loss: 0.220098  [  704/ 3200]\n",
      "loss: 0.337313  [  736/ 3200]\n",
      "loss: 0.347048  [  768/ 3200]\n",
      "loss: 0.261233  [  800/ 3200]\n",
      "loss: 0.399804  [  832/ 3200]\n",
      "loss: 0.358326  [  864/ 3200]\n",
      "loss: 0.269387  [  896/ 3200]\n",
      "loss: 0.246173  [  928/ 3200]\n",
      "loss: 0.332351  [  960/ 3200]\n",
      "loss: 0.337656  [  992/ 3200]\n",
      "loss: 0.374856  [ 1024/ 3200]\n",
      "loss: 0.191436  [ 1056/ 3200]\n",
      "loss: 0.357308  [ 1088/ 3200]\n",
      "loss: 0.337953  [ 1120/ 3200]\n",
      "loss: 0.238080  [ 1152/ 3200]\n",
      "loss: 0.330489  [ 1184/ 3200]\n",
      "loss: 0.602268  [ 1216/ 3200]\n",
      "loss: 0.208632  [ 1248/ 3200]\n",
      "loss: 0.701620  [ 1280/ 3200]\n",
      "loss: 0.193327  [ 1312/ 3200]\n",
      "loss: 0.233308  [ 1344/ 3200]\n",
      "loss: 0.282049  [ 1376/ 3200]\n",
      "loss: 0.305537  [ 1408/ 3200]\n",
      "loss: 0.305004  [ 1440/ 3200]\n",
      "loss: 0.324699  [ 1472/ 3200]\n",
      "loss: 0.288287  [ 1504/ 3200]\n",
      "loss: 0.225336  [ 1536/ 3200]\n",
      "loss: 0.257976  [ 1568/ 3200]\n",
      "loss: 0.496894  [ 1600/ 3200]\n",
      "loss: 0.202491  [ 1632/ 3200]\n",
      "loss: 0.140380  [ 1664/ 3200]\n",
      "loss: 0.236711  [ 1696/ 3200]\n",
      "loss: 0.169491  [ 1728/ 3200]\n",
      "loss: 0.273218  [ 1760/ 3200]\n",
      "loss: 0.371232  [ 1792/ 3200]\n",
      "loss: 0.163632  [ 1824/ 3200]\n",
      "loss: 0.345514  [ 1856/ 3200]\n",
      "loss: 0.162893  [ 1888/ 3200]\n",
      "loss: 0.201649  [ 1920/ 3200]\n",
      "loss: 0.574246  [ 1952/ 3200]\n",
      "loss: 0.307652  [ 1984/ 3200]\n",
      "loss: 0.318198  [ 2016/ 3200]\n",
      "loss: 0.328057  [ 2048/ 3200]\n",
      "loss: 0.145397  [ 2080/ 3200]\n",
      "loss: 0.397468  [ 2112/ 3200]\n",
      "loss: 0.358471  [ 2144/ 3200]\n",
      "loss: 0.263535  [ 2176/ 3200]\n",
      "loss: 0.222710  [ 2208/ 3200]\n",
      "loss: 0.195400  [ 2240/ 3200]\n",
      "loss: 0.251321  [ 2272/ 3200]\n",
      "loss: 0.220385  [ 2304/ 3200]\n",
      "loss: 0.301826  [ 2336/ 3200]\n",
      "loss: 0.424397  [ 2368/ 3200]\n",
      "loss: 0.337987  [ 2400/ 3200]\n",
      "loss: 0.230343  [ 2432/ 3200]\n",
      "loss: 0.251024  [ 2464/ 3200]\n",
      "loss: 0.240923  [ 2496/ 3200]\n",
      "loss: 0.600608  [ 2528/ 3200]\n",
      "loss: 0.343648  [ 2560/ 3200]\n",
      "loss: 0.213567  [ 2592/ 3200]\n",
      "loss: 0.293442  [ 2624/ 3200]\n",
      "loss: 0.266438  [ 2656/ 3200]\n",
      "loss: 0.200657  [ 2688/ 3200]\n",
      "loss: 0.331822  [ 2720/ 3200]\n",
      "loss: 0.256752  [ 2752/ 3200]\n",
      "loss: 0.204436  [ 2784/ 3200]\n",
      "loss: 0.324019  [ 2816/ 3200]\n",
      "loss: 0.221589  [ 2848/ 3200]\n",
      "loss: 0.296615  [ 2880/ 3200]\n",
      "loss: 0.418975  [ 2912/ 3200]\n",
      "loss: 0.461761  [ 2944/ 3200]\n",
      "loss: 0.341649  [ 2976/ 3200]\n",
      "loss: 0.334992  [ 3008/ 3200]\n",
      "loss: 0.223965  [ 3040/ 3200]\n",
      "loss: 0.331605  [ 3072/ 3200]\n",
      "loss: 0.270889  [ 3104/ 3200]\n",
      "loss: 0.414297  [ 3136/ 3200]\n",
      "loss: 0.341040  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034045\n",
      "f1 macro averaged score: 0.800631\n",
      "Accuracy               : 80.2%\n",
      "Confusion matrix       :\n",
      "tensor([[189,   8,   0,   3],\n",
      "        [ 16, 134,  24,  26],\n",
      "        [  0,  20, 165,  15],\n",
      "        [  8,  25,  13, 154]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.8398e-04.\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.277444  [    0/ 3200]\n",
      "loss: 0.404963  [   32/ 3200]\n",
      "loss: 0.253241  [   64/ 3200]\n",
      "loss: 0.173801  [   96/ 3200]\n",
      "loss: 0.356186  [  128/ 3200]\n",
      "loss: 0.192891  [  160/ 3200]\n",
      "loss: 0.394472  [  192/ 3200]\n",
      "loss: 0.188081  [  224/ 3200]\n",
      "loss: 0.415538  [  256/ 3200]\n",
      "loss: 0.099653  [  288/ 3200]\n",
      "loss: 0.622256  [  320/ 3200]\n",
      "loss: 0.465283  [  352/ 3200]\n",
      "loss: 0.559731  [  384/ 3200]\n",
      "loss: 0.169331  [  416/ 3200]\n",
      "loss: 0.170465  [  448/ 3200]\n",
      "loss: 0.181384  [  480/ 3200]\n",
      "loss: 0.345889  [  512/ 3200]\n",
      "loss: 0.294888  [  544/ 3200]\n",
      "loss: 0.348536  [  576/ 3200]\n",
      "loss: 0.376562  [  608/ 3200]\n",
      "loss: 0.261855  [  640/ 3200]\n",
      "loss: 0.299708  [  672/ 3200]\n",
      "loss: 0.311011  [  704/ 3200]\n",
      "loss: 0.109732  [  736/ 3200]\n",
      "loss: 0.519119  [  768/ 3200]\n",
      "loss: 0.365174  [  800/ 3200]\n",
      "loss: 0.226041  [  832/ 3200]\n",
      "loss: 0.276002  [  864/ 3200]\n",
      "loss: 0.181093  [  896/ 3200]\n",
      "loss: 0.424275  [  928/ 3200]\n",
      "loss: 0.304407  [  960/ 3200]\n",
      "loss: 0.278965  [  992/ 3200]\n",
      "loss: 0.300143  [ 1024/ 3200]\n",
      "loss: 0.260846  [ 1056/ 3200]\n",
      "loss: 0.371947  [ 1088/ 3200]\n",
      "loss: 0.225525  [ 1120/ 3200]\n",
      "loss: 0.158223  [ 1152/ 3200]\n",
      "loss: 0.392016  [ 1184/ 3200]\n",
      "loss: 0.318427  [ 1216/ 3200]\n",
      "loss: 0.355086  [ 1248/ 3200]\n",
      "loss: 0.325160  [ 1280/ 3200]\n",
      "loss: 0.297174  [ 1312/ 3200]\n",
      "loss: 0.154153  [ 1344/ 3200]\n",
      "loss: 0.237287  [ 1376/ 3200]\n",
      "loss: 0.259311  [ 1408/ 3200]\n",
      "loss: 0.461857  [ 1440/ 3200]\n",
      "loss: 0.245019  [ 1472/ 3200]\n",
      "loss: 0.228732  [ 1504/ 3200]\n",
      "loss: 0.211071  [ 1536/ 3200]\n",
      "loss: 0.245952  [ 1568/ 3200]\n",
      "loss: 0.222238  [ 1600/ 3200]\n",
      "loss: 0.140602  [ 1632/ 3200]\n",
      "loss: 0.342586  [ 1664/ 3200]\n",
      "loss: 0.135211  [ 1696/ 3200]\n",
      "loss: 0.377219  [ 1728/ 3200]\n",
      "loss: 0.134842  [ 1760/ 3200]\n",
      "loss: 0.196415  [ 1792/ 3200]\n",
      "loss: 0.322130  [ 1824/ 3200]\n",
      "loss: 0.286079  [ 1856/ 3200]\n",
      "loss: 0.333217  [ 1888/ 3200]\n",
      "loss: 0.290553  [ 1920/ 3200]\n",
      "loss: 0.368562  [ 1952/ 3200]\n",
      "loss: 0.324041  [ 1984/ 3200]\n",
      "loss: 0.523229  [ 2016/ 3200]\n",
      "loss: 0.411234  [ 2048/ 3200]\n",
      "loss: 0.334750  [ 2080/ 3200]\n",
      "loss: 0.385166  [ 2112/ 3200]\n",
      "loss: 0.220561  [ 2144/ 3200]\n",
      "loss: 0.238125  [ 2176/ 3200]\n",
      "loss: 0.232597  [ 2208/ 3200]\n",
      "loss: 0.261301  [ 2240/ 3200]\n",
      "loss: 0.256311  [ 2272/ 3200]\n",
      "loss: 0.242299  [ 2304/ 3200]\n",
      "loss: 0.302842  [ 2336/ 3200]\n",
      "loss: 0.407446  [ 2368/ 3200]\n",
      "loss: 0.311728  [ 2400/ 3200]\n",
      "loss: 0.391005  [ 2432/ 3200]\n",
      "loss: 0.209003  [ 2464/ 3200]\n",
      "loss: 0.565756  [ 2496/ 3200]\n",
      "loss: 0.561279  [ 2528/ 3200]\n",
      "loss: 0.141203  [ 2560/ 3200]\n",
      "loss: 0.312625  [ 2592/ 3200]\n",
      "loss: 0.296933  [ 2624/ 3200]\n",
      "loss: 0.407987  [ 2656/ 3200]\n",
      "loss: 0.395525  [ 2688/ 3200]\n",
      "loss: 0.330949  [ 2720/ 3200]\n",
      "loss: 0.116932  [ 2752/ 3200]\n",
      "loss: 0.274509  [ 2784/ 3200]\n",
      "loss: 0.148755  [ 2816/ 3200]\n",
      "loss: 0.204944  [ 2848/ 3200]\n",
      "loss: 0.231080  [ 2880/ 3200]\n",
      "loss: 0.405770  [ 2912/ 3200]\n",
      "loss: 0.564256  [ 2944/ 3200]\n",
      "loss: 0.211873  [ 2976/ 3200]\n",
      "loss: 0.425632  [ 3008/ 3200]\n",
      "loss: 0.277303  [ 3040/ 3200]\n",
      "loss: 0.104496  [ 3072/ 3200]\n",
      "loss: 0.212561  [ 3104/ 3200]\n",
      "loss: 0.247227  [ 3136/ 3200]\n",
      "loss: 0.303649  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035167\n",
      "f1 macro averaged score: 0.788812\n",
      "Accuracy               : 78.8%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  18,   0,  10],\n",
      "        [ 13, 141,  21,  25],\n",
      "        [  0,  26, 163,  11],\n",
      "        [  7,  27,  12, 154]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.5478e-04.\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.219924  [    0/ 3200]\n",
      "loss: 0.199230  [   32/ 3200]\n",
      "loss: 0.367851  [   64/ 3200]\n",
      "loss: 0.356790  [   96/ 3200]\n",
      "loss: 0.273016  [  128/ 3200]\n",
      "loss: 0.288145  [  160/ 3200]\n",
      "loss: 0.171280  [  192/ 3200]\n",
      "loss: 0.280351  [  224/ 3200]\n",
      "loss: 0.204351  [  256/ 3200]\n",
      "loss: 0.286976  [  288/ 3200]\n",
      "loss: 0.193029  [  320/ 3200]\n",
      "loss: 0.505114  [  352/ 3200]\n",
      "loss: 0.292871  [  384/ 3200]\n",
      "loss: 0.270809  [  416/ 3200]\n",
      "loss: 0.370557  [  448/ 3200]\n",
      "loss: 0.435408  [  480/ 3200]\n",
      "loss: 0.177106  [  512/ 3200]\n",
      "loss: 0.272670  [  544/ 3200]\n",
      "loss: 0.278224  [  576/ 3200]\n",
      "loss: 0.289893  [  608/ 3200]\n",
      "loss: 0.287878  [  640/ 3200]\n",
      "loss: 0.363826  [  672/ 3200]\n",
      "loss: 0.263892  [  704/ 3200]\n",
      "loss: 0.198735  [  736/ 3200]\n",
      "loss: 0.326395  [  768/ 3200]\n",
      "loss: 0.219309  [  800/ 3200]\n",
      "loss: 0.294942  [  832/ 3200]\n",
      "loss: 0.363660  [  864/ 3200]\n",
      "loss: 0.443020  [  896/ 3200]\n",
      "loss: 0.456454  [  928/ 3200]\n",
      "loss: 0.235430  [  960/ 3200]\n",
      "loss: 0.284673  [  992/ 3200]\n",
      "loss: 0.277638  [ 1024/ 3200]\n",
      "loss: 0.269642  [ 1056/ 3200]\n",
      "loss: 0.217199  [ 1088/ 3200]\n",
      "loss: 0.377896  [ 1120/ 3200]\n",
      "loss: 0.380095  [ 1152/ 3200]\n",
      "loss: 0.322667  [ 1184/ 3200]\n",
      "loss: 0.155306  [ 1216/ 3200]\n",
      "loss: 0.240090  [ 1248/ 3200]\n",
      "loss: 0.252913  [ 1280/ 3200]\n",
      "loss: 0.177109  [ 1312/ 3200]\n",
      "loss: 0.345792  [ 1344/ 3200]\n",
      "loss: 0.309087  [ 1376/ 3200]\n",
      "loss: 0.335943  [ 1408/ 3200]\n",
      "loss: 0.309874  [ 1440/ 3200]\n",
      "loss: 0.218788  [ 1472/ 3200]\n",
      "loss: 0.250310  [ 1504/ 3200]\n",
      "loss: 0.339855  [ 1536/ 3200]\n",
      "loss: 0.438578  [ 1568/ 3200]\n",
      "loss: 0.233682  [ 1600/ 3200]\n",
      "loss: 0.273795  [ 1632/ 3200]\n",
      "loss: 0.532335  [ 1664/ 3200]\n",
      "loss: 0.301873  [ 1696/ 3200]\n",
      "loss: 0.284448  [ 1728/ 3200]\n",
      "loss: 0.224696  [ 1760/ 3200]\n",
      "loss: 0.340767  [ 1792/ 3200]\n",
      "loss: 0.188537  [ 1824/ 3200]\n",
      "loss: 0.358442  [ 1856/ 3200]\n",
      "loss: 0.254539  [ 1888/ 3200]\n",
      "loss: 0.257096  [ 1920/ 3200]\n",
      "loss: 0.346932  [ 1952/ 3200]\n",
      "loss: 0.303963  [ 1984/ 3200]\n",
      "loss: 0.295521  [ 2016/ 3200]\n",
      "loss: 0.239269  [ 2048/ 3200]\n",
      "loss: 0.169563  [ 2080/ 3200]\n",
      "loss: 0.241548  [ 2112/ 3200]\n",
      "loss: 0.321948  [ 2144/ 3200]\n",
      "loss: 0.294390  [ 2176/ 3200]\n",
      "loss: 0.323115  [ 2208/ 3200]\n",
      "loss: 0.315779  [ 2240/ 3200]\n",
      "loss: 0.119693  [ 2272/ 3200]\n",
      "loss: 0.154548  [ 2304/ 3200]\n",
      "loss: 0.319181  [ 2336/ 3200]\n",
      "loss: 0.209427  [ 2368/ 3200]\n",
      "loss: 0.390064  [ 2400/ 3200]\n",
      "loss: 0.223042  [ 2432/ 3200]\n",
      "loss: 0.194396  [ 2464/ 3200]\n",
      "loss: 0.198931  [ 2496/ 3200]\n",
      "loss: 0.423339  [ 2528/ 3200]\n",
      "loss: 0.353499  [ 2560/ 3200]\n",
      "loss: 0.320889  [ 2592/ 3200]\n",
      "loss: 0.213679  [ 2624/ 3200]\n",
      "loss: 0.403388  [ 2656/ 3200]\n",
      "loss: 0.283373  [ 2688/ 3200]\n",
      "loss: 0.211726  [ 2720/ 3200]\n",
      "loss: 0.277442  [ 2752/ 3200]\n",
      "loss: 0.287953  [ 2784/ 3200]\n",
      "loss: 0.313314  [ 2816/ 3200]\n",
      "loss: 0.169576  [ 2848/ 3200]\n",
      "loss: 0.339636  [ 2880/ 3200]\n",
      "loss: 0.280765  [ 2912/ 3200]\n",
      "loss: 0.268528  [ 2944/ 3200]\n",
      "loss: 0.328480  [ 2976/ 3200]\n",
      "loss: 0.139039  [ 3008/ 3200]\n",
      "loss: 0.298329  [ 3040/ 3200]\n",
      "loss: 0.199911  [ 3072/ 3200]\n",
      "loss: 0.182679  [ 3104/ 3200]\n",
      "loss: 0.345592  [ 3136/ 3200]\n",
      "loss: 0.575944  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034063\n",
      "f1 macro averaged score: 0.798686\n",
      "Accuracy               : 80.0%\n",
      "Confusion matrix       :\n",
      "tensor([[185,  10,   0,   5],\n",
      "        [ 13, 133,  24,  30],\n",
      "        [  0,  22, 163,  15],\n",
      "        [  8,  20,  13, 159]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.2704e-04.\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 0.469302  [    0/ 3200]\n",
      "loss: 0.177253  [   32/ 3200]\n",
      "loss: 0.203687  [   64/ 3200]\n",
      "loss: 0.327282  [   96/ 3200]\n",
      "loss: 0.266770  [  128/ 3200]\n",
      "loss: 0.158171  [  160/ 3200]\n",
      "loss: 0.253462  [  192/ 3200]\n",
      "loss: 0.331992  [  224/ 3200]\n",
      "loss: 0.286223  [  256/ 3200]\n",
      "loss: 0.236710  [  288/ 3200]\n",
      "loss: 0.187963  [  320/ 3200]\n",
      "loss: 0.197649  [  352/ 3200]\n",
      "loss: 0.304501  [  384/ 3200]\n",
      "loss: 0.300763  [  416/ 3200]\n",
      "loss: 0.529632  [  448/ 3200]\n",
      "loss: 0.219047  [  480/ 3200]\n",
      "loss: 0.117475  [  512/ 3200]\n",
      "loss: 0.202878  [  544/ 3200]\n",
      "loss: 0.251811  [  576/ 3200]\n",
      "loss: 0.336026  [  608/ 3200]\n",
      "loss: 0.328910  [  640/ 3200]\n",
      "loss: 0.251972  [  672/ 3200]\n",
      "loss: 0.426339  [  704/ 3200]\n",
      "loss: 0.110385  [  736/ 3200]\n",
      "loss: 0.226275  [  768/ 3200]\n",
      "loss: 0.309477  [  800/ 3200]\n",
      "loss: 0.171858  [  832/ 3200]\n",
      "loss: 0.322023  [  864/ 3200]\n",
      "loss: 0.193718  [  896/ 3200]\n",
      "loss: 0.400198  [  928/ 3200]\n",
      "loss: 0.130456  [  960/ 3200]\n",
      "loss: 0.234076  [  992/ 3200]\n",
      "loss: 0.375641  [ 1024/ 3200]\n",
      "loss: 0.517014  [ 1056/ 3200]\n",
      "loss: 0.257076  [ 1088/ 3200]\n",
      "loss: 0.215756  [ 1120/ 3200]\n",
      "loss: 0.069722  [ 1152/ 3200]\n",
      "loss: 0.274702  [ 1184/ 3200]\n",
      "loss: 0.085546  [ 1216/ 3200]\n",
      "loss: 0.333068  [ 1248/ 3200]\n",
      "loss: 0.084080  [ 1280/ 3200]\n",
      "loss: 0.320308  [ 1312/ 3200]\n",
      "loss: 0.386558  [ 1344/ 3200]\n",
      "loss: 0.366256  [ 1376/ 3200]\n",
      "loss: 0.201291  [ 1408/ 3200]\n",
      "loss: 0.402624  [ 1440/ 3200]\n",
      "loss: 0.322251  [ 1472/ 3200]\n",
      "loss: 0.358113  [ 1504/ 3200]\n",
      "loss: 0.463377  [ 1536/ 3200]\n",
      "loss: 0.180477  [ 1568/ 3200]\n",
      "loss: 0.258773  [ 1600/ 3200]\n",
      "loss: 0.176312  [ 1632/ 3200]\n",
      "loss: 0.171864  [ 1664/ 3200]\n",
      "loss: 0.536606  [ 1696/ 3200]\n",
      "loss: 0.216542  [ 1728/ 3200]\n",
      "loss: 0.281438  [ 1760/ 3200]\n",
      "loss: 0.407042  [ 1792/ 3200]\n",
      "loss: 0.393441  [ 1824/ 3200]\n",
      "loss: 0.450786  [ 1856/ 3200]\n",
      "loss: 0.153719  [ 1888/ 3200]\n",
      "loss: 0.294744  [ 1920/ 3200]\n",
      "loss: 0.196633  [ 1952/ 3200]\n",
      "loss: 0.325654  [ 1984/ 3200]\n",
      "loss: 0.139637  [ 2016/ 3200]\n",
      "loss: 0.398974  [ 2048/ 3200]\n",
      "loss: 0.333782  [ 2080/ 3200]\n",
      "loss: 0.257532  [ 2112/ 3200]\n",
      "loss: 0.259652  [ 2144/ 3200]\n",
      "loss: 0.324869  [ 2176/ 3200]\n",
      "loss: 0.387139  [ 2208/ 3200]\n",
      "loss: 0.281257  [ 2240/ 3200]\n",
      "loss: 0.254776  [ 2272/ 3200]\n",
      "loss: 0.578438  [ 2304/ 3200]\n",
      "loss: 0.264538  [ 2336/ 3200]\n",
      "loss: 0.115549  [ 2368/ 3200]\n",
      "loss: 0.236103  [ 2400/ 3200]\n",
      "loss: 0.582492  [ 2432/ 3200]\n",
      "loss: 0.268489  [ 2464/ 3200]\n",
      "loss: 0.432882  [ 2496/ 3200]\n",
      "loss: 0.263095  [ 2528/ 3200]\n",
      "loss: 0.134984  [ 2560/ 3200]\n",
      "loss: 0.140662  [ 2592/ 3200]\n",
      "loss: 0.284178  [ 2624/ 3200]\n",
      "loss: 0.426320  [ 2656/ 3200]\n",
      "loss: 0.446223  [ 2688/ 3200]\n",
      "loss: 0.208959  [ 2720/ 3200]\n",
      "loss: 0.208210  [ 2752/ 3200]\n",
      "loss: 0.352439  [ 2784/ 3200]\n",
      "loss: 0.234176  [ 2816/ 3200]\n",
      "loss: 0.367668  [ 2848/ 3200]\n",
      "loss: 0.160747  [ 2880/ 3200]\n",
      "loss: 0.203906  [ 2912/ 3200]\n",
      "loss: 0.427223  [ 2944/ 3200]\n",
      "loss: 0.153131  [ 2976/ 3200]\n",
      "loss: 0.353026  [ 3008/ 3200]\n",
      "loss: 0.309640  [ 3040/ 3200]\n",
      "loss: 0.240984  [ 3072/ 3200]\n",
      "loss: 0.292989  [ 3104/ 3200]\n",
      "loss: 0.434728  [ 3136/ 3200]\n",
      "loss: 0.318181  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034305\n",
      "f1 macro averaged score: 0.791012\n",
      "Accuracy               : 79.2%\n",
      "Confusion matrix       :\n",
      "tensor([[181,   9,   0,  10],\n",
      "        [ 14, 127,  25,  34],\n",
      "        [  0,  23, 163,  14],\n",
      "        [  6,  19,  12, 163]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.0069e-04.\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.163511  [    0/ 3200]\n",
      "loss: 0.203573  [   32/ 3200]\n",
      "loss: 0.299024  [   64/ 3200]\n",
      "loss: 0.241281  [   96/ 3200]\n",
      "loss: 0.290243  [  128/ 3200]\n",
      "loss: 0.294466  [  160/ 3200]\n",
      "loss: 0.140222  [  192/ 3200]\n",
      "loss: 0.319807  [  224/ 3200]\n",
      "loss: 0.263504  [  256/ 3200]\n",
      "loss: 0.424508  [  288/ 3200]\n",
      "loss: 0.285557  [  320/ 3200]\n",
      "loss: 0.237215  [  352/ 3200]\n",
      "loss: 0.650205  [  384/ 3200]\n",
      "loss: 0.523619  [  416/ 3200]\n",
      "loss: 0.512753  [  448/ 3200]\n",
      "loss: 0.209480  [  480/ 3200]\n",
      "loss: 0.182601  [  512/ 3200]\n",
      "loss: 0.184188  [  544/ 3200]\n",
      "loss: 0.268618  [  576/ 3200]\n",
      "loss: 0.084690  [  608/ 3200]\n",
      "loss: 0.359865  [  640/ 3200]\n",
      "loss: 0.282719  [  672/ 3200]\n",
      "loss: 0.169605  [  704/ 3200]\n",
      "loss: 0.181261  [  736/ 3200]\n",
      "loss: 0.205759  [  768/ 3200]\n",
      "loss: 0.280610  [  800/ 3200]\n",
      "loss: 0.206174  [  832/ 3200]\n",
      "loss: 0.584711  [  864/ 3200]\n",
      "loss: 0.220280  [  896/ 3200]\n",
      "loss: 0.408816  [  928/ 3200]\n",
      "loss: 0.199736  [  960/ 3200]\n",
      "loss: 0.269126  [  992/ 3200]\n",
      "loss: 0.349889  [ 1024/ 3200]\n",
      "loss: 0.218797  [ 1056/ 3200]\n",
      "loss: 0.216728  [ 1088/ 3200]\n",
      "loss: 0.166391  [ 1120/ 3200]\n",
      "loss: 0.332452  [ 1152/ 3200]\n",
      "loss: 0.376651  [ 1184/ 3200]\n",
      "loss: 0.213711  [ 1216/ 3200]\n",
      "loss: 0.306314  [ 1248/ 3200]\n",
      "loss: 0.260027  [ 1280/ 3200]\n",
      "loss: 0.436477  [ 1312/ 3200]\n",
      "loss: 0.224112  [ 1344/ 3200]\n",
      "loss: 0.201822  [ 1376/ 3200]\n",
      "loss: 0.240714  [ 1408/ 3200]\n",
      "loss: 0.262390  [ 1440/ 3200]\n",
      "loss: 0.189236  [ 1472/ 3200]\n",
      "loss: 0.222148  [ 1504/ 3200]\n",
      "loss: 0.376260  [ 1536/ 3200]\n",
      "loss: 0.226044  [ 1568/ 3200]\n",
      "loss: 0.307697  [ 1600/ 3200]\n",
      "loss: 0.173899  [ 1632/ 3200]\n",
      "loss: 0.129616  [ 1664/ 3200]\n",
      "loss: 0.432801  [ 1696/ 3200]\n",
      "loss: 0.202558  [ 1728/ 3200]\n",
      "loss: 0.298884  [ 1760/ 3200]\n",
      "loss: 0.221389  [ 1792/ 3200]\n",
      "loss: 0.323038  [ 1824/ 3200]\n",
      "loss: 0.247006  [ 1856/ 3200]\n",
      "loss: 0.376300  [ 1888/ 3200]\n",
      "loss: 0.178137  [ 1920/ 3200]\n",
      "loss: 0.280113  [ 1952/ 3200]\n",
      "loss: 0.488351  [ 1984/ 3200]\n",
      "loss: 0.325377  [ 2016/ 3200]\n",
      "loss: 0.142685  [ 2048/ 3200]\n",
      "loss: 0.229733  [ 2080/ 3200]\n",
      "loss: 0.165044  [ 2112/ 3200]\n",
      "loss: 0.220088  [ 2144/ 3200]\n",
      "loss: 0.322590  [ 2176/ 3200]\n",
      "loss: 0.179042  [ 2208/ 3200]\n",
      "loss: 0.466510  [ 2240/ 3200]\n",
      "loss: 0.161105  [ 2272/ 3200]\n",
      "loss: 0.281453  [ 2304/ 3200]\n",
      "loss: 0.344593  [ 2336/ 3200]\n",
      "loss: 0.566928  [ 2368/ 3200]\n",
      "loss: 0.173377  [ 2400/ 3200]\n",
      "loss: 0.251693  [ 2432/ 3200]\n",
      "loss: 0.498681  [ 2464/ 3200]\n",
      "loss: 0.403969  [ 2496/ 3200]\n",
      "loss: 0.216453  [ 2528/ 3200]\n",
      "loss: 0.178115  [ 2560/ 3200]\n",
      "loss: 0.419231  [ 2592/ 3200]\n",
      "loss: 0.601014  [ 2624/ 3200]\n",
      "loss: 0.326011  [ 2656/ 3200]\n",
      "loss: 0.302002  [ 2688/ 3200]\n",
      "loss: 0.113614  [ 2720/ 3200]\n",
      "loss: 0.215967  [ 2752/ 3200]\n",
      "loss: 0.272015  [ 2784/ 3200]\n",
      "loss: 0.298322  [ 2816/ 3200]\n",
      "loss: 0.140327  [ 2848/ 3200]\n",
      "loss: 0.228658  [ 2880/ 3200]\n",
      "loss: 0.191314  [ 2912/ 3200]\n",
      "loss: 0.285151  [ 2944/ 3200]\n",
      "loss: 0.271730  [ 2976/ 3200]\n",
      "loss: 0.236620  [ 3008/ 3200]\n",
      "loss: 0.299937  [ 3040/ 3200]\n",
      "loss: 0.184521  [ 3072/ 3200]\n",
      "loss: 0.249134  [ 3104/ 3200]\n",
      "loss: 0.323702  [ 3136/ 3200]\n",
      "loss: 0.168228  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035180\n",
      "f1 macro averaged score: 0.782632\n",
      "Accuracy               : 78.6%\n",
      "Confusion matrix       :\n",
      "tensor([[182,   6,   0,  12],\n",
      "        [ 14, 115,  26,  45],\n",
      "        [  0,  22, 161,  17],\n",
      "        [  7,  11,  11, 171]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.7565e-04.\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 0.268264  [    0/ 3200]\n",
      "loss: 0.198630  [   32/ 3200]\n",
      "loss: 0.407888  [   64/ 3200]\n",
      "loss: 0.371311  [   96/ 3200]\n",
      "loss: 0.325781  [  128/ 3200]\n",
      "loss: 0.306750  [  160/ 3200]\n",
      "loss: 0.289670  [  192/ 3200]\n",
      "loss: 0.292801  [  224/ 3200]\n",
      "loss: 0.202199  [  256/ 3200]\n",
      "loss: 0.464199  [  288/ 3200]\n",
      "loss: 0.210693  [  320/ 3200]\n",
      "loss: 0.167992  [  352/ 3200]\n",
      "loss: 0.198639  [  384/ 3200]\n",
      "loss: 0.555643  [  416/ 3200]\n",
      "loss: 0.139902  [  448/ 3200]\n",
      "loss: 0.345505  [  480/ 3200]\n",
      "loss: 0.208305  [  512/ 3200]\n",
      "loss: 0.239590  [  544/ 3200]\n",
      "loss: 0.419861  [  576/ 3200]\n",
      "loss: 0.101526  [  608/ 3200]\n",
      "loss: 0.248058  [  640/ 3200]\n",
      "loss: 0.452189  [  672/ 3200]\n",
      "loss: 0.255159  [  704/ 3200]\n",
      "loss: 0.174142  [  736/ 3200]\n",
      "loss: 0.224047  [  768/ 3200]\n",
      "loss: 0.159827  [  800/ 3200]\n",
      "loss: 0.322479  [  832/ 3200]\n",
      "loss: 0.110820  [  864/ 3200]\n",
      "loss: 0.269603  [  896/ 3200]\n",
      "loss: 0.123939  [  928/ 3200]\n",
      "loss: 0.329210  [  960/ 3200]\n",
      "loss: 0.241881  [  992/ 3200]\n",
      "loss: 0.275548  [ 1024/ 3200]\n",
      "loss: 0.547529  [ 1056/ 3200]\n",
      "loss: 0.114464  [ 1088/ 3200]\n",
      "loss: 0.249126  [ 1120/ 3200]\n",
      "loss: 0.222579  [ 1152/ 3200]\n",
      "loss: 0.338421  [ 1184/ 3200]\n",
      "loss: 0.224776  [ 1216/ 3200]\n",
      "loss: 0.111792  [ 1248/ 3200]\n",
      "loss: 0.347834  [ 1280/ 3200]\n",
      "loss: 0.256584  [ 1312/ 3200]\n",
      "loss: 0.105501  [ 1344/ 3200]\n",
      "loss: 0.187940  [ 1376/ 3200]\n",
      "loss: 0.279210  [ 1408/ 3200]\n",
      "loss: 0.227529  [ 1440/ 3200]\n",
      "loss: 0.369841  [ 1472/ 3200]\n",
      "loss: 0.269988  [ 1504/ 3200]\n",
      "loss: 0.385590  [ 1536/ 3200]\n",
      "loss: 0.264744  [ 1568/ 3200]\n",
      "loss: 0.164055  [ 1600/ 3200]\n",
      "loss: 0.208754  [ 1632/ 3200]\n",
      "loss: 0.254487  [ 1664/ 3200]\n",
      "loss: 0.314324  [ 1696/ 3200]\n",
      "loss: 0.325948  [ 1728/ 3200]\n",
      "loss: 0.159479  [ 1760/ 3200]\n",
      "loss: 0.306415  [ 1792/ 3200]\n",
      "loss: 0.324817  [ 1824/ 3200]\n",
      "loss: 0.081683  [ 1856/ 3200]\n",
      "loss: 0.627973  [ 1888/ 3200]\n",
      "loss: 0.153647  [ 1920/ 3200]\n",
      "loss: 0.140471  [ 1952/ 3200]\n",
      "loss: 0.355201  [ 1984/ 3200]\n",
      "loss: 0.211582  [ 2016/ 3200]\n",
      "loss: 0.407461  [ 2048/ 3200]\n",
      "loss: 0.346697  [ 2080/ 3200]\n",
      "loss: 0.255623  [ 2112/ 3200]\n",
      "loss: 0.345556  [ 2144/ 3200]\n",
      "loss: 0.218423  [ 2176/ 3200]\n",
      "loss: 0.181552  [ 2208/ 3200]\n",
      "loss: 0.219090  [ 2240/ 3200]\n",
      "loss: 0.297300  [ 2272/ 3200]\n",
      "loss: 0.422768  [ 2304/ 3200]\n",
      "loss: 0.195428  [ 2336/ 3200]\n",
      "loss: 0.187298  [ 2368/ 3200]\n",
      "loss: 0.319938  [ 2400/ 3200]\n",
      "loss: 0.491996  [ 2432/ 3200]\n",
      "loss: 0.203082  [ 2464/ 3200]\n",
      "loss: 0.204479  [ 2496/ 3200]\n",
      "loss: 0.193139  [ 2528/ 3200]\n",
      "loss: 0.247285  [ 2560/ 3200]\n",
      "loss: 0.251257  [ 2592/ 3200]\n",
      "loss: 0.383480  [ 2624/ 3200]\n",
      "loss: 0.176497  [ 2656/ 3200]\n",
      "loss: 0.308809  [ 2688/ 3200]\n",
      "loss: 0.289119  [ 2720/ 3200]\n",
      "loss: 0.142926  [ 2752/ 3200]\n",
      "loss: 0.253007  [ 2784/ 3200]\n",
      "loss: 0.171632  [ 2816/ 3200]\n",
      "loss: 0.257571  [ 2848/ 3200]\n",
      "loss: 0.327658  [ 2880/ 3200]\n",
      "loss: 0.222161  [ 2912/ 3200]\n",
      "loss: 0.387733  [ 2944/ 3200]\n",
      "loss: 0.244446  [ 2976/ 3200]\n",
      "loss: 0.251904  [ 3008/ 3200]\n",
      "loss: 0.213114  [ 3040/ 3200]\n",
      "loss: 0.213328  [ 3072/ 3200]\n",
      "loss: 0.256903  [ 3104/ 3200]\n",
      "loss: 0.404969  [ 3136/ 3200]\n",
      "loss: 0.307224  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034638\n",
      "f1 macro averaged score: 0.801629\n",
      "Accuracy               : 80.4%\n",
      "Confusion matrix       :\n",
      "tensor([[186,   9,   0,   5],\n",
      "        [ 12, 130,  28,  30],\n",
      "        [  0,  19, 167,  14],\n",
      "        [  9,  18,  13, 160]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.5187e-04.\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.201892  [    0/ 3200]\n",
      "loss: 0.171361  [   32/ 3200]\n",
      "loss: 0.247945  [   64/ 3200]\n",
      "loss: 0.272866  [   96/ 3200]\n",
      "loss: 0.353709  [  128/ 3200]\n",
      "loss: 0.368209  [  160/ 3200]\n",
      "loss: 0.326436  [  192/ 3200]\n",
      "loss: 0.284360  [  224/ 3200]\n",
      "loss: 0.106651  [  256/ 3200]\n",
      "loss: 0.238849  [  288/ 3200]\n",
      "loss: 0.417513  [  320/ 3200]\n",
      "loss: 0.225795  [  352/ 3200]\n",
      "loss: 0.204950  [  384/ 3200]\n",
      "loss: 0.259905  [  416/ 3200]\n",
      "loss: 0.335137  [  448/ 3200]\n",
      "loss: 0.146956  [  480/ 3200]\n",
      "loss: 0.145925  [  512/ 3200]\n",
      "loss: 0.285649  [  544/ 3200]\n",
      "loss: 0.099510  [  576/ 3200]\n",
      "loss: 0.230909  [  608/ 3200]\n",
      "loss: 0.126758  [  640/ 3200]\n",
      "loss: 0.234136  [  672/ 3200]\n",
      "loss: 0.371518  [  704/ 3200]\n",
      "loss: 0.320661  [  736/ 3200]\n",
      "loss: 0.329144  [  768/ 3200]\n",
      "loss: 0.226774  [  800/ 3200]\n",
      "loss: 0.146451  [  832/ 3200]\n",
      "loss: 0.305889  [  864/ 3200]\n",
      "loss: 0.259695  [  896/ 3200]\n",
      "loss: 0.421031  [  928/ 3200]\n",
      "loss: 0.316564  [  960/ 3200]\n",
      "loss: 0.248734  [  992/ 3200]\n",
      "loss: 0.212192  [ 1024/ 3200]\n",
      "loss: 0.300695  [ 1056/ 3200]\n",
      "loss: 0.248818  [ 1088/ 3200]\n",
      "loss: 0.160187  [ 1120/ 3200]\n",
      "loss: 0.251406  [ 1152/ 3200]\n",
      "loss: 0.136299  [ 1184/ 3200]\n",
      "loss: 0.227782  [ 1216/ 3200]\n",
      "loss: 0.210608  [ 1248/ 3200]\n",
      "loss: 0.135260  [ 1280/ 3200]\n",
      "loss: 0.139970  [ 1312/ 3200]\n",
      "loss: 0.215311  [ 1344/ 3200]\n",
      "loss: 0.265683  [ 1376/ 3200]\n",
      "loss: 0.347432  [ 1408/ 3200]\n",
      "loss: 0.206199  [ 1440/ 3200]\n",
      "loss: 0.166177  [ 1472/ 3200]\n",
      "loss: 0.351652  [ 1504/ 3200]\n",
      "loss: 0.174078  [ 1536/ 3200]\n",
      "loss: 0.278454  [ 1568/ 3200]\n",
      "loss: 0.300517  [ 1600/ 3200]\n",
      "loss: 0.443216  [ 1632/ 3200]\n",
      "loss: 0.327774  [ 1664/ 3200]\n",
      "loss: 0.414524  [ 1696/ 3200]\n",
      "loss: 0.339452  [ 1728/ 3200]\n",
      "loss: 0.475683  [ 1760/ 3200]\n",
      "loss: 0.240294  [ 1792/ 3200]\n",
      "loss: 0.187130  [ 1824/ 3200]\n",
      "loss: 0.464399  [ 1856/ 3200]\n",
      "loss: 0.127214  [ 1888/ 3200]\n",
      "loss: 0.267882  [ 1920/ 3200]\n",
      "loss: 0.257911  [ 1952/ 3200]\n",
      "loss: 0.393408  [ 1984/ 3200]\n",
      "loss: 0.237186  [ 2016/ 3200]\n",
      "loss: 0.302930  [ 2048/ 3200]\n",
      "loss: 0.304957  [ 2080/ 3200]\n",
      "loss: 0.283276  [ 2112/ 3200]\n",
      "loss: 0.272374  [ 2144/ 3200]\n",
      "loss: 0.350604  [ 2176/ 3200]\n",
      "loss: 0.298696  [ 2208/ 3200]\n",
      "loss: 0.283061  [ 2240/ 3200]\n",
      "loss: 0.369267  [ 2272/ 3200]\n",
      "loss: 0.399962  [ 2304/ 3200]\n",
      "loss: 0.233850  [ 2336/ 3200]\n",
      "loss: 0.189318  [ 2368/ 3200]\n",
      "loss: 0.343401  [ 2400/ 3200]\n",
      "loss: 0.400364  [ 2432/ 3200]\n",
      "loss: 0.123592  [ 2464/ 3200]\n",
      "loss: 0.161641  [ 2496/ 3200]\n",
      "loss: 0.255965  [ 2528/ 3200]\n",
      "loss: 0.114808  [ 2560/ 3200]\n",
      "loss: 0.172477  [ 2592/ 3200]\n",
      "loss: 0.255614  [ 2624/ 3200]\n",
      "loss: 0.189313  [ 2656/ 3200]\n",
      "loss: 0.621320  [ 2688/ 3200]\n",
      "loss: 0.196015  [ 2720/ 3200]\n",
      "loss: 0.242001  [ 2752/ 3200]\n",
      "loss: 0.268374  [ 2784/ 3200]\n",
      "loss: 0.236878  [ 2816/ 3200]\n",
      "loss: 0.132688  [ 2848/ 3200]\n",
      "loss: 0.459475  [ 2880/ 3200]\n",
      "loss: 0.123274  [ 2912/ 3200]\n",
      "loss: 0.234962  [ 2944/ 3200]\n",
      "loss: 0.377867  [ 2976/ 3200]\n",
      "loss: 0.159329  [ 3008/ 3200]\n",
      "loss: 0.349814  [ 3040/ 3200]\n",
      "loss: 0.149023  [ 3072/ 3200]\n",
      "loss: 0.225125  [ 3104/ 3200]\n",
      "loss: 0.216828  [ 3136/ 3200]\n",
      "loss: 0.182399  [ 3168/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036056\n",
      "f1 macro averaged score: 0.783089\n",
      "Accuracy               : 79.0%\n",
      "Confusion matrix       :\n",
      "tensor([[184,   6,   0,  10],\n",
      "        [ 17, 106,  34,  43],\n",
      "        [  0,  14, 171,  15],\n",
      "        [  6,  10,  13, 171]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.2928e-04.\n",
      "\n",
      "Best epoch: 29 with f1 macro averaged score: 0.8016294240951538\n",
      "Test Error:\n",
      "Avg loss               : 0.036615\n",
      "f1 macro averaged score: 0.790501\n",
      "Accuracy               : 79.0%\n",
      "Confusion matrix       :\n",
      "tensor([[284,   7,   2,   4],\n",
      "        [ 14, 197,  38,  75],\n",
      "        [  3,  27, 309,  17],\n",
      "        [ 12,  52,  38, 297]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "  Batch size: 64\n",
      "Epoch: 1\n",
      "-----------------------------\n",
      "loss: 1.399340  [    0/ 3200]\n",
      "loss: 2.834064  [   64/ 3200]\n",
      "loss: 1.450959  [  128/ 3200]\n",
      "loss: 1.516003  [  192/ 3200]\n",
      "loss: 2.388268  [  256/ 3200]\n",
      "loss: 1.504750  [  320/ 3200]\n",
      "loss: 2.294241  [  384/ 3200]\n",
      "loss: 1.511468  [  448/ 3200]\n",
      "loss: 1.411367  [  512/ 3200]\n",
      "loss: 1.397024  [  576/ 3200]\n",
      "loss: 1.404754  [  640/ 3200]\n",
      "loss: 1.405641  [  704/ 3200]\n",
      "loss: 1.457701  [  768/ 3200]\n",
      "loss: 1.404524  [  832/ 3200]\n",
      "loss: 1.381395  [  896/ 3200]\n",
      "loss: 1.367878  [  960/ 3200]\n",
      "loss: 1.356033  [ 1024/ 3200]\n",
      "loss: 1.785039  [ 1088/ 3200]\n",
      "loss: 1.382843  [ 1152/ 3200]\n",
      "loss: 1.371958  [ 1216/ 3200]\n",
      "loss: 1.417828  [ 1280/ 3200]\n",
      "loss: 1.399568  [ 1344/ 3200]\n",
      "loss: 1.377570  [ 1408/ 3200]\n",
      "loss: 1.349359  [ 1472/ 3200]\n",
      "loss: 1.378562  [ 1536/ 3200]\n",
      "loss: 1.352710  [ 1600/ 3200]\n",
      "loss: 1.344736  [ 1664/ 3200]\n",
      "loss: 1.321023  [ 1728/ 3200]\n",
      "loss: 1.380100  [ 1792/ 3200]\n",
      "loss: 1.432899  [ 1856/ 3200]\n",
      "loss: 1.356018  [ 1920/ 3200]\n",
      "loss: 1.339981  [ 1984/ 3200]\n",
      "loss: 1.369534  [ 2048/ 3200]\n",
      "loss: 1.309847  [ 2112/ 3200]\n",
      "loss: 1.281178  [ 2176/ 3200]\n",
      "loss: 1.232543  [ 2240/ 3200]\n",
      "loss: 1.377936  [ 2304/ 3200]\n",
      "loss: 1.378770  [ 2368/ 3200]\n",
      "loss: 1.289507  [ 2432/ 3200]\n",
      "loss: 1.295430  [ 2496/ 3200]\n",
      "loss: 1.249571  [ 2560/ 3200]\n",
      "loss: 1.227539  [ 2624/ 3200]\n",
      "loss: 1.339710  [ 2688/ 3200]\n",
      "loss: 1.302870  [ 2752/ 3200]\n",
      "loss: 1.264380  [ 2816/ 3200]\n",
      "loss: 1.219182  [ 2880/ 3200]\n",
      "loss: 1.395328  [ 2944/ 3200]\n",
      "loss: 1.245086  [ 3008/ 3200]\n",
      "loss: 1.294493  [ 3072/ 3200]\n",
      "loss: 1.197320  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.071622\n",
      "f1 macro averaged score: 0.589482\n",
      "Accuracy               : 62.4%\n",
      "Confusion matrix       :\n",
      "tensor([[146,  17,  10,  27],\n",
      "        [ 52,  36,  38,  74],\n",
      "        [ 22,  11, 151,  16],\n",
      "        [  9,   7,  18, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.9000e-03.\n",
      "\n",
      "Epoch: 2\n",
      "-----------------------------\n",
      "loss: 1.122009  [    0/ 3200]\n",
      "loss: 1.104627  [   64/ 3200]\n",
      "loss: 1.313993  [  128/ 3200]\n",
      "loss: 1.183491  [  192/ 3200]\n",
      "loss: 1.032536  [  256/ 3200]\n",
      "loss: 1.165009  [  320/ 3200]\n",
      "loss: 1.124979  [  384/ 3200]\n",
      "loss: 1.081495  [  448/ 3200]\n",
      "loss: 1.246796  [  512/ 3200]\n",
      "loss: 1.209623  [  576/ 3200]\n",
      "loss: 0.975192  [  640/ 3200]\n",
      "loss: 0.950143  [  704/ 3200]\n",
      "loss: 1.119397  [  768/ 3200]\n",
      "loss: 1.307030  [  832/ 3200]\n",
      "loss: 1.008286  [  896/ 3200]\n",
      "loss: 1.204042  [  960/ 3200]\n",
      "loss: 1.116045  [ 1024/ 3200]\n",
      "loss: 1.041199  [ 1088/ 3200]\n",
      "loss: 0.979122  [ 1152/ 3200]\n",
      "loss: 0.920542  [ 1216/ 3200]\n",
      "loss: 0.919001  [ 1280/ 3200]\n",
      "loss: 1.010360  [ 1344/ 3200]\n",
      "loss: 0.962232  [ 1408/ 3200]\n",
      "loss: 0.819884  [ 1472/ 3200]\n",
      "loss: 0.882112  [ 1536/ 3200]\n",
      "loss: 0.964607  [ 1600/ 3200]\n",
      "loss: 0.913805  [ 1664/ 3200]\n",
      "loss: 0.873330  [ 1728/ 3200]\n",
      "loss: 1.009769  [ 1792/ 3200]\n",
      "loss: 1.099521  [ 1856/ 3200]\n",
      "loss: 1.082047  [ 1920/ 3200]\n",
      "loss: 1.190519  [ 1984/ 3200]\n",
      "loss: 0.972709  [ 2048/ 3200]\n",
      "loss: 0.983708  [ 2112/ 3200]\n",
      "loss: 0.961180  [ 2176/ 3200]\n",
      "loss: 0.812734  [ 2240/ 3200]\n",
      "loss: 0.956647  [ 2304/ 3200]\n",
      "loss: 1.047864  [ 2368/ 3200]\n",
      "loss: 1.113583  [ 2432/ 3200]\n",
      "loss: 1.123123  [ 2496/ 3200]\n",
      "loss: 1.127263  [ 2560/ 3200]\n",
      "loss: 1.124679  [ 2624/ 3200]\n",
      "loss: 0.914891  [ 2688/ 3200]\n",
      "loss: 0.861387  [ 2752/ 3200]\n",
      "loss: 0.837951  [ 2816/ 3200]\n",
      "loss: 0.970260  [ 2880/ 3200]\n",
      "loss: 0.965819  [ 2944/ 3200]\n",
      "loss: 1.001831  [ 3008/ 3200]\n",
      "loss: 0.994381  [ 3072/ 3200]\n",
      "loss: 0.901692  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.059112\n",
      "f1 macro averaged score: 0.548703\n",
      "Accuracy               : 59.1%\n",
      "Confusion matrix       :\n",
      "tensor([[108,  46,  20,  26],\n",
      "        [ 26,  22,  55,  97],\n",
      "        [  0,   5, 165,  30],\n",
      "        [  2,   3,  17, 178]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.8050e-03.\n",
      "\n",
      "Epoch: 3\n",
      "-----------------------------\n",
      "loss: 0.895398  [    0/ 3200]\n",
      "loss: 0.850444  [   64/ 3200]\n",
      "loss: 0.812318  [  128/ 3200]\n",
      "loss: 0.909081  [  192/ 3200]\n",
      "loss: 0.746759  [  256/ 3200]\n",
      "loss: 0.782566  [  320/ 3200]\n",
      "loss: 0.808698  [  384/ 3200]\n",
      "loss: 0.831625  [  448/ 3200]\n",
      "loss: 0.889457  [  512/ 3200]\n",
      "loss: 0.989272  [  576/ 3200]\n",
      "loss: 0.763532  [  640/ 3200]\n",
      "loss: 0.765558  [  704/ 3200]\n",
      "loss: 0.850199  [  768/ 3200]\n",
      "loss: 0.736563  [  832/ 3200]\n",
      "loss: 1.059289  [  896/ 3200]\n",
      "loss: 1.194809  [  960/ 3200]\n",
      "loss: 1.089434  [ 1024/ 3200]\n",
      "loss: 0.778441  [ 1088/ 3200]\n",
      "loss: 0.841999  [ 1152/ 3200]\n",
      "loss: 0.747906  [ 1216/ 3200]\n",
      "loss: 0.832916  [ 1280/ 3200]\n",
      "loss: 0.667257  [ 1344/ 3200]\n",
      "loss: 0.744379  [ 1408/ 3200]\n",
      "loss: 0.772750  [ 1472/ 3200]\n",
      "loss: 0.950993  [ 1536/ 3200]\n",
      "loss: 0.883765  [ 1600/ 3200]\n",
      "loss: 1.069082  [ 1664/ 3200]\n",
      "loss: 0.745929  [ 1728/ 3200]\n",
      "loss: 0.758952  [ 1792/ 3200]\n",
      "loss: 1.232484  [ 1856/ 3200]\n",
      "loss: 0.895074  [ 1920/ 3200]\n",
      "loss: 0.799070  [ 1984/ 3200]\n",
      "loss: 0.820101  [ 2048/ 3200]\n",
      "loss: 0.868729  [ 2112/ 3200]\n",
      "loss: 0.787255  [ 2176/ 3200]\n",
      "loss: 0.922504  [ 2240/ 3200]\n",
      "loss: 0.796210  [ 2304/ 3200]\n",
      "loss: 0.746370  [ 2368/ 3200]\n",
      "loss: 1.005368  [ 2432/ 3200]\n",
      "loss: 0.785030  [ 2496/ 3200]\n",
      "loss: 0.712912  [ 2560/ 3200]\n",
      "loss: 0.712426  [ 2624/ 3200]\n",
      "loss: 1.023421  [ 2688/ 3200]\n",
      "loss: 0.648448  [ 2752/ 3200]\n",
      "loss: 0.763504  [ 2816/ 3200]\n",
      "loss: 0.753291  [ 2880/ 3200]\n",
      "loss: 0.683370  [ 2944/ 3200]\n",
      "loss: 0.810339  [ 3008/ 3200]\n",
      "loss: 0.719028  [ 3072/ 3200]\n",
      "loss: 0.812179  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.049874\n",
      "f1 macro averaged score: 0.665822\n",
      "Accuracy               : 67.6%\n",
      "Confusion matrix       :\n",
      "tensor([[118,  61,   5,  16],\n",
      "        [ 23,  78,  33,  66],\n",
      "        [  0,  12, 171,  17],\n",
      "        [  0,   8,  18, 174]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.7147e-03.\n",
      "\n",
      "Epoch: 4\n",
      "-----------------------------\n",
      "loss: 0.708172  [    0/ 3200]\n",
      "loss: 0.819521  [   64/ 3200]\n",
      "loss: 0.707996  [  128/ 3200]\n",
      "loss: 0.743941  [  192/ 3200]\n",
      "loss: 0.661925  [  256/ 3200]\n",
      "loss: 0.710298  [  320/ 3200]\n",
      "loss: 0.669995  [  384/ 3200]\n",
      "loss: 0.699965  [  448/ 3200]\n",
      "loss: 0.760494  [  512/ 3200]\n",
      "loss: 0.947523  [  576/ 3200]\n",
      "loss: 0.774030  [  640/ 3200]\n",
      "loss: 1.134706  [  704/ 3200]\n",
      "loss: 0.726244  [  768/ 3200]\n",
      "loss: 0.578630  [  832/ 3200]\n",
      "loss: 0.666353  [  896/ 3200]\n",
      "loss: 0.632521  [  960/ 3200]\n",
      "loss: 0.764806  [ 1024/ 3200]\n",
      "loss: 0.728298  [ 1088/ 3200]\n",
      "loss: 0.816039  [ 1152/ 3200]\n",
      "loss: 0.648668  [ 1216/ 3200]\n",
      "loss: 0.588289  [ 1280/ 3200]\n",
      "loss: 0.764038  [ 1344/ 3200]\n",
      "loss: 0.669170  [ 1408/ 3200]\n",
      "loss: 0.744104  [ 1472/ 3200]\n",
      "loss: 0.786796  [ 1536/ 3200]\n",
      "loss: 0.733701  [ 1600/ 3200]\n",
      "loss: 0.856434  [ 1664/ 3200]\n",
      "loss: 0.676150  [ 1728/ 3200]\n",
      "loss: 0.720457  [ 1792/ 3200]\n",
      "loss: 0.616640  [ 1856/ 3200]\n",
      "loss: 0.699567  [ 1920/ 3200]\n",
      "loss: 0.646591  [ 1984/ 3200]\n",
      "loss: 0.649518  [ 2048/ 3200]\n",
      "loss: 0.796789  [ 2112/ 3200]\n",
      "loss: 0.881100  [ 2176/ 3200]\n",
      "loss: 0.799983  [ 2240/ 3200]\n",
      "loss: 0.806789  [ 2304/ 3200]\n",
      "loss: 0.598107  [ 2368/ 3200]\n",
      "loss: 0.487379  [ 2432/ 3200]\n",
      "loss: 0.800873  [ 2496/ 3200]\n",
      "loss: 0.747252  [ 2560/ 3200]\n",
      "loss: 0.788492  [ 2624/ 3200]\n",
      "loss: 0.847413  [ 2688/ 3200]\n",
      "loss: 0.644936  [ 2752/ 3200]\n",
      "loss: 0.910916  [ 2816/ 3200]\n",
      "loss: 0.704684  [ 2880/ 3200]\n",
      "loss: 0.689002  [ 2944/ 3200]\n",
      "loss: 0.784176  [ 3008/ 3200]\n",
      "loss: 0.687101  [ 3072/ 3200]\n",
      "loss: 0.731389  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.046120\n",
      "f1 macro averaged score: 0.689166\n",
      "Accuracy               : 69.5%\n",
      "Confusion matrix       :\n",
      "tensor([[137,  47,  11,   5],\n",
      "        [ 21,  86,  52,  41],\n",
      "        [  0,  13, 177,  10],\n",
      "        [  0,  23,  21, 156]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.6290e-03.\n",
      "\n",
      "Epoch: 5\n",
      "-----------------------------\n",
      "loss: 0.641020  [    0/ 3200]\n",
      "loss: 0.648667  [   64/ 3200]\n",
      "loss: 0.529399  [  128/ 3200]\n",
      "loss: 0.555381  [  192/ 3200]\n",
      "loss: 0.572911  [  256/ 3200]\n",
      "loss: 0.599334  [  320/ 3200]\n",
      "loss: 0.513350  [  384/ 3200]\n",
      "loss: 0.623508  [  448/ 3200]\n",
      "loss: 0.743531  [  512/ 3200]\n",
      "loss: 0.872698  [  576/ 3200]\n",
      "loss: 0.889447  [  640/ 3200]\n",
      "loss: 0.579241  [  704/ 3200]\n",
      "loss: 0.604988  [  768/ 3200]\n",
      "loss: 0.683476  [  832/ 3200]\n",
      "loss: 0.677717  [  896/ 3200]\n",
      "loss: 0.680531  [  960/ 3200]\n",
      "loss: 0.697696  [ 1024/ 3200]\n",
      "loss: 0.652495  [ 1088/ 3200]\n",
      "loss: 0.718740  [ 1152/ 3200]\n",
      "loss: 0.875060  [ 1216/ 3200]\n",
      "loss: 0.666482  [ 1280/ 3200]\n",
      "loss: 0.587278  [ 1344/ 3200]\n",
      "loss: 0.648796  [ 1408/ 3200]\n",
      "loss: 0.550947  [ 1472/ 3200]\n",
      "loss: 0.595821  [ 1536/ 3200]\n",
      "loss: 0.481527  [ 1600/ 3200]\n",
      "loss: 0.618171  [ 1664/ 3200]\n",
      "loss: 0.740871  [ 1728/ 3200]\n",
      "loss: 0.636603  [ 1792/ 3200]\n",
      "loss: 0.771880  [ 1856/ 3200]\n",
      "loss: 0.600061  [ 1920/ 3200]\n",
      "loss: 0.698585  [ 1984/ 3200]\n",
      "loss: 0.715588  [ 2048/ 3200]\n",
      "loss: 0.641995  [ 2112/ 3200]\n",
      "loss: 0.727304  [ 2176/ 3200]\n",
      "loss: 0.603621  [ 2240/ 3200]\n",
      "loss: 0.682260  [ 2304/ 3200]\n",
      "loss: 0.588624  [ 2368/ 3200]\n",
      "loss: 0.642968  [ 2432/ 3200]\n",
      "loss: 0.644276  [ 2496/ 3200]\n",
      "loss: 0.770547  [ 2560/ 3200]\n",
      "loss: 0.693209  [ 2624/ 3200]\n",
      "loss: 0.643132  [ 2688/ 3200]\n",
      "loss: 0.630185  [ 2752/ 3200]\n",
      "loss: 0.734209  [ 2816/ 3200]\n",
      "loss: 0.618809  [ 2880/ 3200]\n",
      "loss: 0.790802  [ 2944/ 3200]\n",
      "loss: 0.754872  [ 3008/ 3200]\n",
      "loss: 0.584371  [ 3072/ 3200]\n",
      "loss: 0.533413  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.040920\n",
      "f1 macro averaged score: 0.747925\n",
      "Accuracy               : 74.6%\n",
      "Confusion matrix       :\n",
      "tensor([[146,  48,   3,   3],\n",
      "        [ 18, 121,  33,  28],\n",
      "        [  0,  19, 174,   7],\n",
      "        [  0,  26,  18, 156]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.5476e-03.\n",
      "\n",
      "Epoch: 6\n",
      "-----------------------------\n",
      "loss: 0.482157  [    0/ 3200]\n",
      "loss: 0.485314  [   64/ 3200]\n",
      "loss: 0.680393  [  128/ 3200]\n",
      "loss: 0.541210  [  192/ 3200]\n",
      "loss: 0.471193  [  256/ 3200]\n",
      "loss: 0.530493  [  320/ 3200]\n",
      "loss: 0.502603  [  384/ 3200]\n",
      "loss: 0.582399  [  448/ 3200]\n",
      "loss: 0.723328  [  512/ 3200]\n",
      "loss: 0.783687  [  576/ 3200]\n",
      "loss: 0.649975  [  640/ 3200]\n",
      "loss: 0.555712  [  704/ 3200]\n",
      "loss: 0.664708  [  768/ 3200]\n",
      "loss: 0.636856  [  832/ 3200]\n",
      "loss: 0.609406  [  896/ 3200]\n",
      "loss: 0.656248  [  960/ 3200]\n",
      "loss: 0.576538  [ 1024/ 3200]\n",
      "loss: 0.477911  [ 1088/ 3200]\n",
      "loss: 0.415725  [ 1152/ 3200]\n",
      "loss: 0.515835  [ 1216/ 3200]\n",
      "loss: 0.568720  [ 1280/ 3200]\n",
      "loss: 0.562289  [ 1344/ 3200]\n",
      "loss: 0.587913  [ 1408/ 3200]\n",
      "loss: 0.701605  [ 1472/ 3200]\n",
      "loss: 0.736422  [ 1536/ 3200]\n",
      "loss: 0.416397  [ 1600/ 3200]\n",
      "loss: 0.607413  [ 1664/ 3200]\n",
      "loss: 0.448526  [ 1728/ 3200]\n",
      "loss: 0.619351  [ 1792/ 3200]\n",
      "loss: 0.401583  [ 1856/ 3200]\n",
      "loss: 0.617625  [ 1920/ 3200]\n",
      "loss: 0.564874  [ 1984/ 3200]\n",
      "loss: 0.532254  [ 2048/ 3200]\n",
      "loss: 0.713425  [ 2112/ 3200]\n",
      "loss: 0.939009  [ 2176/ 3200]\n",
      "loss: 1.292462  [ 2240/ 3200]\n",
      "loss: 1.104081  [ 2304/ 3200]\n",
      "loss: 0.766501  [ 2368/ 3200]\n",
      "loss: 0.557238  [ 2432/ 3200]\n",
      "loss: 0.561547  [ 2496/ 3200]\n",
      "loss: 0.513608  [ 2560/ 3200]\n",
      "loss: 0.539528  [ 2624/ 3200]\n",
      "loss: 0.581223  [ 2688/ 3200]\n",
      "loss: 0.421798  [ 2752/ 3200]\n",
      "loss: 0.539062  [ 2816/ 3200]\n",
      "loss: 0.587582  [ 2880/ 3200]\n",
      "loss: 0.439478  [ 2944/ 3200]\n",
      "loss: 0.654020  [ 3008/ 3200]\n",
      "loss: 0.738730  [ 3072/ 3200]\n",
      "loss: 0.777472  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.040417\n",
      "f1 macro averaged score: 0.737344\n",
      "Accuracy               : 74.2%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  16,   0,  12],\n",
      "        [ 24,  93,  14,  69],\n",
      "        [  0,  20, 156,  24],\n",
      "        [  1,  14,  12, 173]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.4702e-03.\n",
      "\n",
      "Epoch: 7\n",
      "-----------------------------\n",
      "loss: 0.666753  [    0/ 3200]\n",
      "loss: 0.690475  [   64/ 3200]\n",
      "loss: 0.726424  [  128/ 3200]\n",
      "loss: 0.684553  [  192/ 3200]\n",
      "loss: 0.622892  [  256/ 3200]\n",
      "loss: 0.599561  [  320/ 3200]\n",
      "loss: 0.580832  [  384/ 3200]\n",
      "loss: 0.653297  [  448/ 3200]\n",
      "loss: 0.699258  [  512/ 3200]\n",
      "loss: 0.771752  [  576/ 3200]\n",
      "loss: 0.741796  [  640/ 3200]\n",
      "loss: 0.507172  [  704/ 3200]\n",
      "loss: 0.588621  [  768/ 3200]\n",
      "loss: 0.447772  [  832/ 3200]\n",
      "loss: 0.673119  [  896/ 3200]\n",
      "loss: 0.454834  [  960/ 3200]\n",
      "loss: 0.587642  [ 1024/ 3200]\n",
      "loss: 0.555315  [ 1088/ 3200]\n",
      "loss: 0.642660  [ 1152/ 3200]\n",
      "loss: 0.495235  [ 1216/ 3200]\n",
      "loss: 0.403479  [ 1280/ 3200]\n",
      "loss: 0.710398  [ 1344/ 3200]\n",
      "loss: 0.592221  [ 1408/ 3200]\n",
      "loss: 0.626929  [ 1472/ 3200]\n",
      "loss: 0.512105  [ 1536/ 3200]\n",
      "loss: 0.657434  [ 1600/ 3200]\n",
      "loss: 0.596406  [ 1664/ 3200]\n",
      "loss: 0.592883  [ 1728/ 3200]\n",
      "loss: 0.444200  [ 1792/ 3200]\n",
      "loss: 0.610705  [ 1856/ 3200]\n",
      "loss: 0.474740  [ 1920/ 3200]\n",
      "loss: 0.495603  [ 1984/ 3200]\n",
      "loss: 0.403002  [ 2048/ 3200]\n",
      "loss: 0.500300  [ 2112/ 3200]\n",
      "loss: 0.482215  [ 2176/ 3200]\n",
      "loss: 0.910617  [ 2240/ 3200]\n",
      "loss: 0.605808  [ 2304/ 3200]\n",
      "loss: 0.601876  [ 2368/ 3200]\n",
      "loss: 0.398100  [ 2432/ 3200]\n",
      "loss: 0.424749  [ 2496/ 3200]\n",
      "loss: 0.640708  [ 2560/ 3200]\n",
      "loss: 0.542867  [ 2624/ 3200]\n",
      "loss: 0.468699  [ 2688/ 3200]\n",
      "loss: 0.648154  [ 2752/ 3200]\n",
      "loss: 0.445144  [ 2816/ 3200]\n",
      "loss: 0.560301  [ 2880/ 3200]\n",
      "loss: 0.460968  [ 2944/ 3200]\n",
      "loss: 0.798352  [ 3008/ 3200]\n",
      "loss: 0.569929  [ 3072/ 3200]\n",
      "loss: 0.605256  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.037178\n",
      "f1 macro averaged score: 0.771865\n",
      "Accuracy               : 76.8%\n",
      "Confusion matrix       :\n",
      "tensor([[156,  41,   0,   3],\n",
      "        [ 18, 140,  27,  15],\n",
      "        [  0,  21, 177,   2],\n",
      "        [  0,  43,  16, 141]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3967e-03.\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 0.638641  [    0/ 3200]\n",
      "loss: 0.533696  [   64/ 3200]\n",
      "loss: 0.581803  [  128/ 3200]\n",
      "loss: 0.607291  [  192/ 3200]\n",
      "loss: 0.534374  [  256/ 3200]\n",
      "loss: 0.612619  [  320/ 3200]\n",
      "loss: 0.594822  [  384/ 3200]\n",
      "loss: 0.511132  [  448/ 3200]\n",
      "loss: 0.727455  [  512/ 3200]\n",
      "loss: 0.466595  [  576/ 3200]\n",
      "loss: 0.556964  [  640/ 3200]\n",
      "loss: 0.397821  [  704/ 3200]\n",
      "loss: 0.551860  [  768/ 3200]\n",
      "loss: 0.563579  [  832/ 3200]\n",
      "loss: 0.454004  [  896/ 3200]\n",
      "loss: 0.457965  [  960/ 3200]\n",
      "loss: 0.521335  [ 1024/ 3200]\n",
      "loss: 0.400817  [ 1088/ 3200]\n",
      "loss: 0.681065  [ 1152/ 3200]\n",
      "loss: 0.447541  [ 1216/ 3200]\n",
      "loss: 0.557615  [ 1280/ 3200]\n",
      "loss: 0.507482  [ 1344/ 3200]\n",
      "loss: 0.791905  [ 1408/ 3200]\n",
      "loss: 0.530299  [ 1472/ 3200]\n",
      "loss: 0.740653  [ 1536/ 3200]\n",
      "loss: 0.531416  [ 1600/ 3200]\n",
      "loss: 0.694416  [ 1664/ 3200]\n",
      "loss: 0.430730  [ 1728/ 3200]\n",
      "loss: 0.535566  [ 1792/ 3200]\n",
      "loss: 0.493442  [ 1856/ 3200]\n",
      "loss: 0.347789  [ 1920/ 3200]\n",
      "loss: 0.477366  [ 1984/ 3200]\n",
      "loss: 0.666249  [ 2048/ 3200]\n",
      "loss: 0.618891  [ 2112/ 3200]\n",
      "loss: 0.443510  [ 2176/ 3200]\n",
      "loss: 0.631969  [ 2240/ 3200]\n",
      "loss: 0.626551  [ 2304/ 3200]\n",
      "loss: 0.556127  [ 2368/ 3200]\n",
      "loss: 0.620678  [ 2432/ 3200]\n",
      "loss: 0.439273  [ 2496/ 3200]\n",
      "loss: 0.612225  [ 2560/ 3200]\n",
      "loss: 0.468388  [ 2624/ 3200]\n",
      "loss: 0.738385  [ 2688/ 3200]\n",
      "loss: 0.536444  [ 2752/ 3200]\n",
      "loss: 0.569565  [ 2816/ 3200]\n",
      "loss: 0.604338  [ 2880/ 3200]\n",
      "loss: 0.480127  [ 2944/ 3200]\n",
      "loss: 0.446174  [ 3008/ 3200]\n",
      "loss: 0.429613  [ 3072/ 3200]\n",
      "loss: 0.433956  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036200\n",
      "f1 macro averaged score: 0.767902\n",
      "Accuracy               : 76.8%\n",
      "Confusion matrix       :\n",
      "tensor([[160,  30,   0,  10],\n",
      "        [ 17, 124,  26,  33],\n",
      "        [  0,  21, 168,  11],\n",
      "        [  1,  22,  15, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3268e-03.\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 0.497353  [    0/ 3200]\n",
      "loss: 0.504846  [   64/ 3200]\n",
      "loss: 0.581351  [  128/ 3200]\n",
      "loss: 0.534642  [  192/ 3200]\n",
      "loss: 0.440358  [  256/ 3200]\n",
      "loss: 0.604541  [  320/ 3200]\n",
      "loss: 0.400445  [  384/ 3200]\n",
      "loss: 0.426394  [  448/ 3200]\n",
      "loss: 0.528256  [  512/ 3200]\n",
      "loss: 0.487903  [  576/ 3200]\n",
      "loss: 0.471138  [  640/ 3200]\n",
      "loss: 0.624185  [  704/ 3200]\n",
      "loss: 0.514184  [  768/ 3200]\n",
      "loss: 0.580439  [  832/ 3200]\n",
      "loss: 0.405108  [  896/ 3200]\n",
      "loss: 0.641883  [  960/ 3200]\n",
      "loss: 0.603906  [ 1024/ 3200]\n",
      "loss: 0.530058  [ 1088/ 3200]\n",
      "loss: 0.499641  [ 1152/ 3200]\n",
      "loss: 0.568753  [ 1216/ 3200]\n",
      "loss: 0.540418  [ 1280/ 3200]\n",
      "loss: 0.469413  [ 1344/ 3200]\n",
      "loss: 0.677303  [ 1408/ 3200]\n",
      "loss: 0.868073  [ 1472/ 3200]\n",
      "loss: 0.814903  [ 1536/ 3200]\n",
      "loss: 0.515978  [ 1600/ 3200]\n",
      "loss: 0.488255  [ 1664/ 3200]\n",
      "loss: 0.544422  [ 1728/ 3200]\n",
      "loss: 0.470913  [ 1792/ 3200]\n",
      "loss: 0.464872  [ 1856/ 3200]\n",
      "loss: 0.466489  [ 1920/ 3200]\n",
      "loss: 0.430066  [ 1984/ 3200]\n",
      "loss: 0.400244  [ 2048/ 3200]\n",
      "loss: 0.548596  [ 2112/ 3200]\n",
      "loss: 0.399072  [ 2176/ 3200]\n",
      "loss: 0.502080  [ 2240/ 3200]\n",
      "loss: 0.604217  [ 2304/ 3200]\n",
      "loss: 0.470653  [ 2368/ 3200]\n",
      "loss: 0.355268  [ 2432/ 3200]\n",
      "loss: 0.479910  [ 2496/ 3200]\n",
      "loss: 0.513344  [ 2560/ 3200]\n",
      "loss: 0.412544  [ 2624/ 3200]\n",
      "loss: 0.576248  [ 2688/ 3200]\n",
      "loss: 0.635958  [ 2752/ 3200]\n",
      "loss: 0.582166  [ 2816/ 3200]\n",
      "loss: 0.501871  [ 2880/ 3200]\n",
      "loss: 0.591051  [ 2944/ 3200]\n",
      "loss: 0.475427  [ 3008/ 3200]\n",
      "loss: 0.384301  [ 3072/ 3200]\n",
      "loss: 0.553380  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034628\n",
      "f1 macro averaged score: 0.777297\n",
      "Accuracy               : 77.9%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  18,   0,   8],\n",
      "        [ 17, 122,  31,  30],\n",
      "        [  0,  19, 174,   7],\n",
      "        [  3,  28,  16, 153]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2605e-03.\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 0.524530  [    0/ 3200]\n",
      "loss: 0.569987  [   64/ 3200]\n",
      "loss: 0.441045  [  128/ 3200]\n",
      "loss: 0.542060  [  192/ 3200]\n",
      "loss: 0.730000  [  256/ 3200]\n",
      "loss: 0.581254  [  320/ 3200]\n",
      "loss: 0.403068  [  384/ 3200]\n",
      "loss: 0.425395  [  448/ 3200]\n",
      "loss: 0.442777  [  512/ 3200]\n",
      "loss: 0.574367  [  576/ 3200]\n",
      "loss: 0.446945  [  640/ 3200]\n",
      "loss: 0.678618  [  704/ 3200]\n",
      "loss: 0.773682  [  768/ 3200]\n",
      "loss: 0.541863  [  832/ 3200]\n",
      "loss: 0.432056  [  896/ 3200]\n",
      "loss: 0.529799  [  960/ 3200]\n",
      "loss: 0.490979  [ 1024/ 3200]\n",
      "loss: 0.579270  [ 1088/ 3200]\n",
      "loss: 0.417843  [ 1152/ 3200]\n",
      "loss: 0.413319  [ 1216/ 3200]\n",
      "loss: 0.336302  [ 1280/ 3200]\n",
      "loss: 0.276097  [ 1344/ 3200]\n",
      "loss: 0.309357  [ 1408/ 3200]\n",
      "loss: 0.405326  [ 1472/ 3200]\n",
      "loss: 0.503316  [ 1536/ 3200]\n",
      "loss: 0.436108  [ 1600/ 3200]\n",
      "loss: 0.520334  [ 1664/ 3200]\n",
      "loss: 0.688991  [ 1728/ 3200]\n",
      "loss: 0.376782  [ 1792/ 3200]\n",
      "loss: 0.468388  [ 1856/ 3200]\n",
      "loss: 0.574717  [ 1920/ 3200]\n",
      "loss: 0.511398  [ 1984/ 3200]\n",
      "loss: 0.454417  [ 2048/ 3200]\n",
      "loss: 0.471084  [ 2112/ 3200]\n",
      "loss: 0.445197  [ 2176/ 3200]\n",
      "loss: 0.439773  [ 2240/ 3200]\n",
      "loss: 0.608608  [ 2304/ 3200]\n",
      "loss: 0.284100  [ 2368/ 3200]\n",
      "loss: 0.569129  [ 2432/ 3200]\n",
      "loss: 0.513375  [ 2496/ 3200]\n",
      "loss: 0.471346  [ 2560/ 3200]\n",
      "loss: 0.497868  [ 2624/ 3200]\n",
      "loss: 0.400653  [ 2688/ 3200]\n",
      "loss: 0.534619  [ 2752/ 3200]\n",
      "loss: 0.382671  [ 2816/ 3200]\n",
      "loss: 0.600477  [ 2880/ 3200]\n",
      "loss: 0.530644  [ 2944/ 3200]\n",
      "loss: 0.452562  [ 3008/ 3200]\n",
      "loss: 0.683380  [ 3072/ 3200]\n",
      "loss: 0.462741  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035148\n",
      "f1 macro averaged score: 0.778970\n",
      "Accuracy               : 77.8%\n",
      "Confusion matrix       :\n",
      "tensor([[163,  26,   0,  11],\n",
      "        [ 14, 134,  24,  28],\n",
      "        [  0,  24, 164,  12],\n",
      "        [  3,  23,  13, 161]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1975e-03.\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 0.534791  [    0/ 3200]\n",
      "loss: 0.487200  [   64/ 3200]\n",
      "loss: 0.367453  [  128/ 3200]\n",
      "loss: 0.445287  [  192/ 3200]\n",
      "loss: 0.453456  [  256/ 3200]\n",
      "loss: 0.688386  [  320/ 3200]\n",
      "loss: 0.386682  [  384/ 3200]\n",
      "loss: 0.510606  [  448/ 3200]\n",
      "loss: 0.567333  [  512/ 3200]\n",
      "loss: 0.434769  [  576/ 3200]\n",
      "loss: 0.605303  [  640/ 3200]\n",
      "loss: 0.434351  [  704/ 3200]\n",
      "loss: 0.364886  [  768/ 3200]\n",
      "loss: 0.465275  [  832/ 3200]\n",
      "loss: 0.438378  [  896/ 3200]\n",
      "loss: 0.387062  [  960/ 3200]\n",
      "loss: 0.326800  [ 1024/ 3200]\n",
      "loss: 0.377981  [ 1088/ 3200]\n",
      "loss: 0.409254  [ 1152/ 3200]\n",
      "loss: 0.305306  [ 1216/ 3200]\n",
      "loss: 0.322948  [ 1280/ 3200]\n",
      "loss: 0.486137  [ 1344/ 3200]\n",
      "loss: 0.546255  [ 1408/ 3200]\n",
      "loss: 0.275709  [ 1472/ 3200]\n",
      "loss: 0.432367  [ 1536/ 3200]\n",
      "loss: 0.541180  [ 1600/ 3200]\n",
      "loss: 0.524570  [ 1664/ 3200]\n",
      "loss: 0.573032  [ 1728/ 3200]\n",
      "loss: 0.471640  [ 1792/ 3200]\n",
      "loss: 0.604712  [ 1856/ 3200]\n",
      "loss: 0.420618  [ 1920/ 3200]\n",
      "loss: 0.475104  [ 1984/ 3200]\n",
      "loss: 0.532165  [ 2048/ 3200]\n",
      "loss: 0.491404  [ 2112/ 3200]\n",
      "loss: 0.587087  [ 2176/ 3200]\n",
      "loss: 0.513655  [ 2240/ 3200]\n",
      "loss: 0.556721  [ 2304/ 3200]\n",
      "loss: 0.569346  [ 2368/ 3200]\n",
      "loss: 0.537299  [ 2432/ 3200]\n",
      "loss: 0.457859  [ 2496/ 3200]\n",
      "loss: 0.496830  [ 2560/ 3200]\n",
      "loss: 0.628982  [ 2624/ 3200]\n",
      "loss: 0.521822  [ 2688/ 3200]\n",
      "loss: 0.625974  [ 2752/ 3200]\n",
      "loss: 0.461959  [ 2816/ 3200]\n",
      "loss: 0.392455  [ 2880/ 3200]\n",
      "loss: 0.342552  [ 2944/ 3200]\n",
      "loss: 0.317902  [ 3008/ 3200]\n",
      "loss: 0.601069  [ 3072/ 3200]\n",
      "loss: 0.652577  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035670\n",
      "f1 macro averaged score: 0.777241\n",
      "Accuracy               : 78.1%\n",
      "Confusion matrix       :\n",
      "tensor([[188,   4,   0,   8],\n",
      "        [ 29, 118,  15,  38],\n",
      "        [  0,  28, 152,  20],\n",
      "        [  7,  15,  11, 167]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1376e-03.\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 0.532170  [    0/ 3200]\n",
      "loss: 0.464827  [   64/ 3200]\n",
      "loss: 0.402393  [  128/ 3200]\n",
      "loss: 0.529060  [  192/ 3200]\n",
      "loss: 0.625077  [  256/ 3200]\n",
      "loss: 0.431508  [  320/ 3200]\n",
      "loss: 0.314093  [  384/ 3200]\n",
      "loss: 0.492410  [  448/ 3200]\n",
      "loss: 0.464655  [  512/ 3200]\n",
      "loss: 0.494615  [  576/ 3200]\n",
      "loss: 0.444063  [  640/ 3200]\n",
      "loss: 0.399060  [  704/ 3200]\n",
      "loss: 0.470448  [  768/ 3200]\n",
      "loss: 0.552992  [  832/ 3200]\n",
      "loss: 0.313403  [  896/ 3200]\n",
      "loss: 0.576192  [  960/ 3200]\n",
      "loss: 0.453440  [ 1024/ 3200]\n",
      "loss: 0.573044  [ 1088/ 3200]\n",
      "loss: 0.497947  [ 1152/ 3200]\n",
      "loss: 0.528539  [ 1216/ 3200]\n",
      "loss: 0.353469  [ 1280/ 3200]\n",
      "loss: 0.430086  [ 1344/ 3200]\n",
      "loss: 0.436300  [ 1408/ 3200]\n",
      "loss: 0.544862  [ 1472/ 3200]\n",
      "loss: 0.494615  [ 1536/ 3200]\n",
      "loss: 0.484575  [ 1600/ 3200]\n",
      "loss: 0.522166  [ 1664/ 3200]\n",
      "loss: 0.315298  [ 1728/ 3200]\n",
      "loss: 0.352224  [ 1792/ 3200]\n",
      "loss: 0.384077  [ 1856/ 3200]\n",
      "loss: 0.371999  [ 1920/ 3200]\n",
      "loss: 0.541286  [ 1984/ 3200]\n",
      "loss: 0.579087  [ 2048/ 3200]\n",
      "loss: 0.735948  [ 2112/ 3200]\n",
      "loss: 0.331617  [ 2176/ 3200]\n",
      "loss: 0.446171  [ 2240/ 3200]\n",
      "loss: 0.519421  [ 2304/ 3200]\n",
      "loss: 0.538721  [ 2368/ 3200]\n",
      "loss: 0.550488  [ 2432/ 3200]\n",
      "loss: 0.408527  [ 2496/ 3200]\n",
      "loss: 0.353609  [ 2560/ 3200]\n",
      "loss: 0.309836  [ 2624/ 3200]\n",
      "loss: 0.394734  [ 2688/ 3200]\n",
      "loss: 0.425260  [ 2752/ 3200]\n",
      "loss: 0.556021  [ 2816/ 3200]\n",
      "loss: 0.447563  [ 2880/ 3200]\n",
      "loss: 0.361763  [ 2944/ 3200]\n",
      "loss: 0.575000  [ 3008/ 3200]\n",
      "loss: 0.516519  [ 3072/ 3200]\n",
      "loss: 0.356779  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034435\n",
      "f1 macro averaged score: 0.755638\n",
      "Accuracy               : 76.8%\n",
      "Confusion matrix       :\n",
      "tensor([[189,   1,   0,  10],\n",
      "        [ 27,  90,  27,  56],\n",
      "        [  0,  16, 168,  16],\n",
      "        [ 10,  10,  13, 167]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0807e-03.\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 0.337566  [    0/ 3200]\n",
      "loss: 0.457090  [   64/ 3200]\n",
      "loss: 0.405212  [  128/ 3200]\n",
      "loss: 0.363103  [  192/ 3200]\n",
      "loss: 0.299875  [  256/ 3200]\n",
      "loss: 0.496053  [  320/ 3200]\n",
      "loss: 0.476601  [  384/ 3200]\n",
      "loss: 0.370325  [  448/ 3200]\n",
      "loss: 0.539923  [  512/ 3200]\n",
      "loss: 0.405641  [  576/ 3200]\n",
      "loss: 0.551815  [  640/ 3200]\n",
      "loss: 0.526042  [  704/ 3200]\n",
      "loss: 0.418085  [  768/ 3200]\n",
      "loss: 0.408878  [  832/ 3200]\n",
      "loss: 0.383905  [  896/ 3200]\n",
      "loss: 0.451864  [  960/ 3200]\n",
      "loss: 0.490153  [ 1024/ 3200]\n",
      "loss: 0.496497  [ 1088/ 3200]\n",
      "loss: 0.547738  [ 1152/ 3200]\n",
      "loss: 0.392152  [ 1216/ 3200]\n",
      "loss: 0.539959  [ 1280/ 3200]\n",
      "loss: 0.299826  [ 1344/ 3200]\n",
      "loss: 0.455157  [ 1408/ 3200]\n",
      "loss: 0.390904  [ 1472/ 3200]\n",
      "loss: 0.457481  [ 1536/ 3200]\n",
      "loss: 0.336422  [ 1600/ 3200]\n",
      "loss: 0.280864  [ 1664/ 3200]\n",
      "loss: 0.497882  [ 1728/ 3200]\n",
      "loss: 0.565892  [ 1792/ 3200]\n",
      "loss: 0.309977  [ 1856/ 3200]\n",
      "loss: 0.270544  [ 1920/ 3200]\n",
      "loss: 0.349992  [ 1984/ 3200]\n",
      "loss: 0.612599  [ 2048/ 3200]\n",
      "loss: 0.449117  [ 2112/ 3200]\n",
      "loss: 0.425554  [ 2176/ 3200]\n",
      "loss: 0.458935  [ 2240/ 3200]\n",
      "loss: 0.449558  [ 2304/ 3200]\n",
      "loss: 0.487536  [ 2368/ 3200]\n",
      "loss: 0.572318  [ 2432/ 3200]\n",
      "loss: 0.270336  [ 2496/ 3200]\n",
      "loss: 0.435244  [ 2560/ 3200]\n",
      "loss: 0.554925  [ 2624/ 3200]\n",
      "loss: 0.551171  [ 2688/ 3200]\n",
      "loss: 0.432128  [ 2752/ 3200]\n",
      "loss: 0.366521  [ 2816/ 3200]\n",
      "loss: 0.360201  [ 2880/ 3200]\n",
      "loss: 0.483005  [ 2944/ 3200]\n",
      "loss: 0.403936  [ 3008/ 3200]\n",
      "loss: 0.417890  [ 3072/ 3200]\n",
      "loss: 0.622159  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.033416\n",
      "f1 macro averaged score: 0.790090\n",
      "Accuracy               : 79.6%\n",
      "Confusion matrix       :\n",
      "tensor([[189,   2,   0,   9],\n",
      "        [ 28, 113,  22,  37],\n",
      "        [  0,  19, 169,  12],\n",
      "        [  8,  13,  13, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0267e-03.\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 0.470625  [    0/ 3200]\n",
      "loss: 0.391485  [   64/ 3200]\n",
      "loss: 0.299461  [  128/ 3200]\n",
      "loss: 0.382418  [  192/ 3200]\n",
      "loss: 0.437113  [  256/ 3200]\n",
      "loss: 0.386339  [  320/ 3200]\n",
      "loss: 0.446028  [  384/ 3200]\n",
      "loss: 0.334916  [  448/ 3200]\n",
      "loss: 0.385891  [  512/ 3200]\n",
      "loss: 0.416667  [  576/ 3200]\n",
      "loss: 0.411063  [  640/ 3200]\n",
      "loss: 0.513178  [  704/ 3200]\n",
      "loss: 0.337590  [  768/ 3200]\n",
      "loss: 0.442625  [  832/ 3200]\n",
      "loss: 0.431201  [  896/ 3200]\n",
      "loss: 0.264721  [  960/ 3200]\n",
      "loss: 0.438303  [ 1024/ 3200]\n",
      "loss: 0.446373  [ 1088/ 3200]\n",
      "loss: 0.575806  [ 1152/ 3200]\n",
      "loss: 0.451540  [ 1216/ 3200]\n",
      "loss: 0.421723  [ 1280/ 3200]\n",
      "loss: 0.266084  [ 1344/ 3200]\n",
      "loss: 0.287327  [ 1408/ 3200]\n",
      "loss: 0.459286  [ 1472/ 3200]\n",
      "loss: 0.486878  [ 1536/ 3200]\n",
      "loss: 0.391496  [ 1600/ 3200]\n",
      "loss: 0.546235  [ 1664/ 3200]\n",
      "loss: 0.416225  [ 1728/ 3200]\n",
      "loss: 0.315874  [ 1792/ 3200]\n",
      "loss: 0.486166  [ 1856/ 3200]\n",
      "loss: 0.370947  [ 1920/ 3200]\n",
      "loss: 0.318165  [ 1984/ 3200]\n",
      "loss: 0.505624  [ 2048/ 3200]\n",
      "loss: 0.386825  [ 2112/ 3200]\n",
      "loss: 0.402366  [ 2176/ 3200]\n",
      "loss: 0.433706  [ 2240/ 3200]\n",
      "loss: 0.310651  [ 2304/ 3200]\n",
      "loss: 0.479557  [ 2368/ 3200]\n",
      "loss: 0.567436  [ 2432/ 3200]\n",
      "loss: 0.332039  [ 2496/ 3200]\n",
      "loss: 0.378078  [ 2560/ 3200]\n",
      "loss: 0.322208  [ 2624/ 3200]\n",
      "loss: 0.522435  [ 2688/ 3200]\n",
      "loss: 0.589116  [ 2752/ 3200]\n",
      "loss: 0.471825  [ 2816/ 3200]\n",
      "loss: 0.357406  [ 2880/ 3200]\n",
      "loss: 0.236126  [ 2944/ 3200]\n",
      "loss: 0.564406  [ 3008/ 3200]\n",
      "loss: 0.530803  [ 3072/ 3200]\n",
      "loss: 0.336039  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035362\n",
      "f1 macro averaged score: 0.768304\n",
      "Accuracy               : 77.8%\n",
      "Confusion matrix       :\n",
      "tensor([[184,   2,   0,  14],\n",
      "        [ 18,  95,  27,  60],\n",
      "        [  0,  15, 169,  16],\n",
      "        [  8,   4,  14, 174]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.7535e-04.\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.410039  [    0/ 3200]\n",
      "loss: 0.369232  [   64/ 3200]\n",
      "loss: 0.363563  [  128/ 3200]\n",
      "loss: 0.462300  [  192/ 3200]\n",
      "loss: 0.294747  [  256/ 3200]\n",
      "loss: 0.390401  [  320/ 3200]\n",
      "loss: 0.297324  [  384/ 3200]\n",
      "loss: 0.558345  [  448/ 3200]\n",
      "loss: 0.484456  [  512/ 3200]\n",
      "loss: 0.525891  [  576/ 3200]\n",
      "loss: 0.383284  [  640/ 3200]\n",
      "loss: 0.450766  [  704/ 3200]\n",
      "loss: 0.379711  [  768/ 3200]\n",
      "loss: 0.268747  [  832/ 3200]\n",
      "loss: 0.290186  [  896/ 3200]\n",
      "loss: 0.362018  [  960/ 3200]\n",
      "loss: 0.393747  [ 1024/ 3200]\n",
      "loss: 0.358698  [ 1088/ 3200]\n",
      "loss: 0.251280  [ 1152/ 3200]\n",
      "loss: 0.399356  [ 1216/ 3200]\n",
      "loss: 0.272978  [ 1280/ 3200]\n",
      "loss: 0.376329  [ 1344/ 3200]\n",
      "loss: 0.347363  [ 1408/ 3200]\n",
      "loss: 0.514633  [ 1472/ 3200]\n",
      "loss: 0.374779  [ 1536/ 3200]\n",
      "loss: 0.349907  [ 1600/ 3200]\n",
      "loss: 0.503117  [ 1664/ 3200]\n",
      "loss: 0.474935  [ 1728/ 3200]\n",
      "loss: 0.560324  [ 1792/ 3200]\n",
      "loss: 0.638407  [ 1856/ 3200]\n",
      "loss: 0.387214  [ 1920/ 3200]\n",
      "loss: 0.323884  [ 1984/ 3200]\n",
      "loss: 0.444234  [ 2048/ 3200]\n",
      "loss: 0.422969  [ 2112/ 3200]\n",
      "loss: 0.334418  [ 2176/ 3200]\n",
      "loss: 0.477190  [ 2240/ 3200]\n",
      "loss: 0.482450  [ 2304/ 3200]\n",
      "loss: 0.290985  [ 2368/ 3200]\n",
      "loss: 0.503784  [ 2432/ 3200]\n",
      "loss: 0.299984  [ 2496/ 3200]\n",
      "loss: 0.316925  [ 2560/ 3200]\n",
      "loss: 0.418216  [ 2624/ 3200]\n",
      "loss: 0.314145  [ 2688/ 3200]\n",
      "loss: 0.450739  [ 2752/ 3200]\n",
      "loss: 0.483906  [ 2816/ 3200]\n",
      "loss: 0.331295  [ 2880/ 3200]\n",
      "loss: 0.446080  [ 2944/ 3200]\n",
      "loss: 0.463745  [ 3008/ 3200]\n",
      "loss: 0.575728  [ 3072/ 3200]\n",
      "loss: 0.374012  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035914\n",
      "f1 macro averaged score: 0.771029\n",
      "Accuracy               : 76.9%\n",
      "Confusion matrix       :\n",
      "tensor([[167,  12,   0,  21],\n",
      "        [ 13, 136,  13,  38],\n",
      "        [  0,  31, 148,  21],\n",
      "        [  5,  21,  10, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.2658e-04.\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 0.363924  [    0/ 3200]\n",
      "loss: 0.560100  [   64/ 3200]\n",
      "loss: 0.323538  [  128/ 3200]\n",
      "loss: 0.367100  [  192/ 3200]\n",
      "loss: 0.349360  [  256/ 3200]\n",
      "loss: 0.356687  [  320/ 3200]\n",
      "loss: 0.424537  [  384/ 3200]\n",
      "loss: 0.508699  [  448/ 3200]\n",
      "loss: 0.394392  [  512/ 3200]\n",
      "loss: 0.459791  [  576/ 3200]\n",
      "loss: 0.285596  [  640/ 3200]\n",
      "loss: 0.280441  [  704/ 3200]\n",
      "loss: 0.292297  [  768/ 3200]\n",
      "loss: 0.354722  [  832/ 3200]\n",
      "loss: 0.491364  [  896/ 3200]\n",
      "loss: 0.325558  [  960/ 3200]\n",
      "loss: 0.425790  [ 1024/ 3200]\n",
      "loss: 0.346816  [ 1088/ 3200]\n",
      "loss: 0.568789  [ 1152/ 3200]\n",
      "loss: 0.370605  [ 1216/ 3200]\n",
      "loss: 0.318472  [ 1280/ 3200]\n",
      "loss: 0.346362  [ 1344/ 3200]\n",
      "loss: 0.495925  [ 1408/ 3200]\n",
      "loss: 0.462425  [ 1472/ 3200]\n",
      "loss: 0.320269  [ 1536/ 3200]\n",
      "loss: 0.367071  [ 1600/ 3200]\n",
      "loss: 0.383824  [ 1664/ 3200]\n",
      "loss: 0.477680  [ 1728/ 3200]\n",
      "loss: 0.434401  [ 1792/ 3200]\n",
      "loss: 0.352753  [ 1856/ 3200]\n",
      "loss: 0.380576  [ 1920/ 3200]\n",
      "loss: 0.380264  [ 1984/ 3200]\n",
      "loss: 0.411531  [ 2048/ 3200]\n",
      "loss: 0.283765  [ 2112/ 3200]\n",
      "loss: 0.310419  [ 2176/ 3200]\n",
      "loss: 0.344141  [ 2240/ 3200]\n",
      "loss: 0.425558  [ 2304/ 3200]\n",
      "loss: 0.477126  [ 2368/ 3200]\n",
      "loss: 0.376376  [ 2432/ 3200]\n",
      "loss: 0.412466  [ 2496/ 3200]\n",
      "loss: 0.353921  [ 2560/ 3200]\n",
      "loss: 0.379381  [ 2624/ 3200]\n",
      "loss: 0.496211  [ 2688/ 3200]\n",
      "loss: 0.370549  [ 2752/ 3200]\n",
      "loss: 0.415827  [ 2816/ 3200]\n",
      "loss: 0.402926  [ 2880/ 3200]\n",
      "loss: 0.274811  [ 2944/ 3200]\n",
      "loss: 0.259552  [ 3008/ 3200]\n",
      "loss: 0.485942  [ 3072/ 3200]\n",
      "loss: 0.419765  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034128\n",
      "f1 macro averaged score: 0.795273\n",
      "Accuracy               : 79.4%\n",
      "Confusion matrix       :\n",
      "tensor([[170,  19,   0,  11],\n",
      "        [ 12, 142,  17,  29],\n",
      "        [  0,  28, 160,  12],\n",
      "        [  5,  20,  12, 163]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 8.8025e-04.\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 0.278193  [    0/ 3200]\n",
      "loss: 0.597345  [   64/ 3200]\n",
      "loss: 0.322035  [  128/ 3200]\n",
      "loss: 0.402328  [  192/ 3200]\n",
      "loss: 0.395886  [  256/ 3200]\n",
      "loss: 0.437581  [  320/ 3200]\n",
      "loss: 0.340712  [  384/ 3200]\n",
      "loss: 0.287254  [  448/ 3200]\n",
      "loss: 0.359137  [  512/ 3200]\n",
      "loss: 0.415468  [  576/ 3200]\n",
      "loss: 0.453642  [  640/ 3200]\n",
      "loss: 0.420337  [  704/ 3200]\n",
      "loss: 0.267805  [  768/ 3200]\n",
      "loss: 0.345748  [  832/ 3200]\n",
      "loss: 0.293074  [  896/ 3200]\n",
      "loss: 0.263517  [  960/ 3200]\n",
      "loss: 0.245919  [ 1024/ 3200]\n",
      "loss: 0.558478  [ 1088/ 3200]\n",
      "loss: 0.426692  [ 1152/ 3200]\n",
      "loss: 0.460437  [ 1216/ 3200]\n",
      "loss: 0.363881  [ 1280/ 3200]\n",
      "loss: 0.419064  [ 1344/ 3200]\n",
      "loss: 0.266077  [ 1408/ 3200]\n",
      "loss: 0.272080  [ 1472/ 3200]\n",
      "loss: 0.495433  [ 1536/ 3200]\n",
      "loss: 0.432170  [ 1600/ 3200]\n",
      "loss: 0.292231  [ 1664/ 3200]\n",
      "loss: 0.378229  [ 1728/ 3200]\n",
      "loss: 0.573107  [ 1792/ 3200]\n",
      "loss: 0.368522  [ 1856/ 3200]\n",
      "loss: 0.350541  [ 1920/ 3200]\n",
      "loss: 0.376498  [ 1984/ 3200]\n",
      "loss: 0.314625  [ 2048/ 3200]\n",
      "loss: 0.433848  [ 2112/ 3200]\n",
      "loss: 0.456857  [ 2176/ 3200]\n",
      "loss: 0.343560  [ 2240/ 3200]\n",
      "loss: 0.188527  [ 2304/ 3200]\n",
      "loss: 0.413284  [ 2368/ 3200]\n",
      "loss: 0.308837  [ 2432/ 3200]\n",
      "loss: 0.230477  [ 2496/ 3200]\n",
      "loss: 0.347037  [ 2560/ 3200]\n",
      "loss: 0.413004  [ 2624/ 3200]\n",
      "loss: 0.282149  [ 2688/ 3200]\n",
      "loss: 0.365334  [ 2752/ 3200]\n",
      "loss: 0.395211  [ 2816/ 3200]\n",
      "loss: 0.346376  [ 2880/ 3200]\n",
      "loss: 0.323629  [ 2944/ 3200]\n",
      "loss: 0.436999  [ 3008/ 3200]\n",
      "loss: 0.447137  [ 3072/ 3200]\n",
      "loss: 0.404991  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.033644\n",
      "f1 macro averaged score: 0.789896\n",
      "Accuracy               : 79.0%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  11,   0,  14],\n",
      "        [ 15, 133,  23,  29],\n",
      "        [  0,  26, 165,   9],\n",
      "        [  5,  23,  13, 159]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 8.3624e-04.\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 0.407911  [    0/ 3200]\n",
      "loss: 0.258214  [   64/ 3200]\n",
      "loss: 0.281450  [  128/ 3200]\n",
      "loss: 0.373599  [  192/ 3200]\n",
      "loss: 0.286565  [  256/ 3200]\n",
      "loss: 0.403808  [  320/ 3200]\n",
      "loss: 0.359300  [  384/ 3200]\n",
      "loss: 0.302047  [  448/ 3200]\n",
      "loss: 0.492175  [  512/ 3200]\n",
      "loss: 0.452480  [  576/ 3200]\n",
      "loss: 0.391635  [  640/ 3200]\n",
      "loss: 0.475739  [  704/ 3200]\n",
      "loss: 0.373342  [  768/ 3200]\n",
      "loss: 0.242878  [  832/ 3200]\n",
      "loss: 0.258659  [  896/ 3200]\n",
      "loss: 0.413409  [  960/ 3200]\n",
      "loss: 0.468329  [ 1024/ 3200]\n",
      "loss: 0.383124  [ 1088/ 3200]\n",
      "loss: 0.423135  [ 1152/ 3200]\n",
      "loss: 0.458900  [ 1216/ 3200]\n",
      "loss: 0.215525  [ 1280/ 3200]\n",
      "loss: 0.269974  [ 1344/ 3200]\n",
      "loss: 0.274232  [ 1408/ 3200]\n",
      "loss: 0.381421  [ 1472/ 3200]\n",
      "loss: 0.375829  [ 1536/ 3200]\n",
      "loss: 0.368501  [ 1600/ 3200]\n",
      "loss: 0.289788  [ 1664/ 3200]\n",
      "loss: 0.435274  [ 1728/ 3200]\n",
      "loss: 0.334103  [ 1792/ 3200]\n",
      "loss: 0.227219  [ 1856/ 3200]\n",
      "loss: 0.445670  [ 1920/ 3200]\n",
      "loss: 0.433152  [ 1984/ 3200]\n",
      "loss: 0.210318  [ 2048/ 3200]\n",
      "loss: 0.386350  [ 2112/ 3200]\n",
      "loss: 0.281031  [ 2176/ 3200]\n",
      "loss: 0.254062  [ 2240/ 3200]\n",
      "loss: 0.358805  [ 2304/ 3200]\n",
      "loss: 0.266806  [ 2368/ 3200]\n",
      "loss: 0.322859  [ 2432/ 3200]\n",
      "loss: 0.426672  [ 2496/ 3200]\n",
      "loss: 0.523074  [ 2560/ 3200]\n",
      "loss: 0.294257  [ 2624/ 3200]\n",
      "loss: 0.396807  [ 2688/ 3200]\n",
      "loss: 0.434844  [ 2752/ 3200]\n",
      "loss: 0.409875  [ 2816/ 3200]\n",
      "loss: 0.337394  [ 2880/ 3200]\n",
      "loss: 0.464577  [ 2944/ 3200]\n",
      "loss: 0.417033  [ 3008/ 3200]\n",
      "loss: 0.410561  [ 3072/ 3200]\n",
      "loss: 0.548238  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035239\n",
      "f1 macro averaged score: 0.770273\n",
      "Accuracy               : 77.6%\n",
      "Confusion matrix       :\n",
      "tensor([[183,   2,   0,  15],\n",
      "        [ 21, 102,  20,  57],\n",
      "        [  0,  14, 163,  23],\n",
      "        [  7,   9,  11, 173]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.9443e-04.\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 0.308072  [    0/ 3200]\n",
      "loss: 0.360133  [   64/ 3200]\n",
      "loss: 0.262035  [  128/ 3200]\n",
      "loss: 0.338704  [  192/ 3200]\n",
      "loss: 0.318390  [  256/ 3200]\n",
      "loss: 0.374433  [  320/ 3200]\n",
      "loss: 0.390311  [  384/ 3200]\n",
      "loss: 0.385597  [  448/ 3200]\n",
      "loss: 0.284415  [  512/ 3200]\n",
      "loss: 0.266376  [  576/ 3200]\n",
      "loss: 0.313843  [  640/ 3200]\n",
      "loss: 0.317450  [  704/ 3200]\n",
      "loss: 0.484534  [  768/ 3200]\n",
      "loss: 0.437083  [  832/ 3200]\n",
      "loss: 0.351631  [  896/ 3200]\n",
      "loss: 0.291777  [  960/ 3200]\n",
      "loss: 0.248793  [ 1024/ 3200]\n",
      "loss: 0.370502  [ 1088/ 3200]\n",
      "loss: 0.524516  [ 1152/ 3200]\n",
      "loss: 0.568194  [ 1216/ 3200]\n",
      "loss: 0.332400  [ 1280/ 3200]\n",
      "loss: 0.553183  [ 1344/ 3200]\n",
      "loss: 0.479059  [ 1408/ 3200]\n",
      "loss: 0.338513  [ 1472/ 3200]\n",
      "loss: 0.357870  [ 1536/ 3200]\n",
      "loss: 0.310810  [ 1600/ 3200]\n",
      "loss: 0.374523  [ 1664/ 3200]\n",
      "loss: 0.278128  [ 1728/ 3200]\n",
      "loss: 0.376504  [ 1792/ 3200]\n",
      "loss: 0.329898  [ 1856/ 3200]\n",
      "loss: 0.311125  [ 1920/ 3200]\n",
      "loss: 0.412777  [ 1984/ 3200]\n",
      "loss: 0.343138  [ 2048/ 3200]\n",
      "loss: 0.324276  [ 2112/ 3200]\n",
      "loss: 0.263417  [ 2176/ 3200]\n",
      "loss: 0.293373  [ 2240/ 3200]\n",
      "loss: 0.387501  [ 2304/ 3200]\n",
      "loss: 0.487463  [ 2368/ 3200]\n",
      "loss: 0.349848  [ 2432/ 3200]\n",
      "loss: 0.251351  [ 2496/ 3200]\n",
      "loss: 0.390160  [ 2560/ 3200]\n",
      "loss: 0.326564  [ 2624/ 3200]\n",
      "loss: 0.303980  [ 2688/ 3200]\n",
      "loss: 0.342174  [ 2752/ 3200]\n",
      "loss: 0.324028  [ 2816/ 3200]\n",
      "loss: 0.394736  [ 2880/ 3200]\n",
      "loss: 0.228798  [ 2944/ 3200]\n",
      "loss: 0.262572  [ 3008/ 3200]\n",
      "loss: 0.378193  [ 3072/ 3200]\n",
      "loss: 0.428490  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034963\n",
      "f1 macro averaged score: 0.787823\n",
      "Accuracy               : 79.0%\n",
      "Confusion matrix       :\n",
      "tensor([[167,  18,   1,  14],\n",
      "        [ 10, 121,  40,  29],\n",
      "        [  0,  16, 181,   3],\n",
      "        [  3,  18,  16, 163]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.5471e-04.\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 0.317796  [    0/ 3200]\n",
      "loss: 0.339069  [   64/ 3200]\n",
      "loss: 0.408367  [  128/ 3200]\n",
      "loss: 0.488182  [  192/ 3200]\n",
      "loss: 0.462901  [  256/ 3200]\n",
      "loss: 0.339464  [  320/ 3200]\n",
      "loss: 0.370970  [  384/ 3200]\n",
      "loss: 0.449963  [  448/ 3200]\n",
      "loss: 0.284903  [  512/ 3200]\n",
      "loss: 0.319473  [  576/ 3200]\n",
      "loss: 0.418239  [  640/ 3200]\n",
      "loss: 0.359200  [  704/ 3200]\n",
      "loss: 0.306664  [  768/ 3200]\n",
      "loss: 0.270957  [  832/ 3200]\n",
      "loss: 0.237167  [  896/ 3200]\n",
      "loss: 0.343914  [  960/ 3200]\n",
      "loss: 0.365948  [ 1024/ 3200]\n",
      "loss: 0.369766  [ 1088/ 3200]\n",
      "loss: 0.306849  [ 1152/ 3200]\n",
      "loss: 0.428741  [ 1216/ 3200]\n",
      "loss: 0.339282  [ 1280/ 3200]\n",
      "loss: 0.298378  [ 1344/ 3200]\n",
      "loss: 0.460268  [ 1408/ 3200]\n",
      "loss: 0.512041  [ 1472/ 3200]\n",
      "loss: 0.308995  [ 1536/ 3200]\n",
      "loss: 0.275063  [ 1600/ 3200]\n",
      "loss: 0.226394  [ 1664/ 3200]\n",
      "loss: 0.224267  [ 1728/ 3200]\n",
      "loss: 0.361066  [ 1792/ 3200]\n",
      "loss: 0.262988  [ 1856/ 3200]\n",
      "loss: 0.410172  [ 1920/ 3200]\n",
      "loss: 0.311441  [ 1984/ 3200]\n",
      "loss: 0.368277  [ 2048/ 3200]\n",
      "loss: 0.297729  [ 2112/ 3200]\n",
      "loss: 0.283821  [ 2176/ 3200]\n",
      "loss: 0.368880  [ 2240/ 3200]\n",
      "loss: 0.244192  [ 2304/ 3200]\n",
      "loss: 0.433029  [ 2368/ 3200]\n",
      "loss: 0.374825  [ 2432/ 3200]\n",
      "loss: 0.334665  [ 2496/ 3200]\n",
      "loss: 0.285166  [ 2560/ 3200]\n",
      "loss: 0.443535  [ 2624/ 3200]\n",
      "loss: 0.225082  [ 2688/ 3200]\n",
      "loss: 0.342673  [ 2752/ 3200]\n",
      "loss: 0.309632  [ 2816/ 3200]\n",
      "loss: 0.300718  [ 2880/ 3200]\n",
      "loss: 0.355245  [ 2944/ 3200]\n",
      "loss: 0.320384  [ 3008/ 3200]\n",
      "loss: 0.335471  [ 3072/ 3200]\n",
      "loss: 0.277879  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034263\n",
      "f1 macro averaged score: 0.785490\n",
      "Accuracy               : 78.9%\n",
      "Confusion matrix       :\n",
      "tensor([[179,   5,   0,  16],\n",
      "        [ 16, 116,  24,  44],\n",
      "        [  0,  19, 165,  16],\n",
      "        [  6,  12,  11, 171]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.1697e-04.\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.340623  [    0/ 3200]\n",
      "loss: 0.259500  [   64/ 3200]\n",
      "loss: 0.363774  [  128/ 3200]\n",
      "loss: 0.339108  [  192/ 3200]\n",
      "loss: 0.437307  [  256/ 3200]\n",
      "loss: 0.420253  [  320/ 3200]\n",
      "loss: 0.233860  [  384/ 3200]\n",
      "loss: 0.347116  [  448/ 3200]\n",
      "loss: 0.360004  [  512/ 3200]\n",
      "loss: 0.242737  [  576/ 3200]\n",
      "loss: 0.323535  [  640/ 3200]\n",
      "loss: 0.347363  [  704/ 3200]\n",
      "loss: 0.386352  [  768/ 3200]\n",
      "loss: 0.376603  [  832/ 3200]\n",
      "loss: 0.302736  [  896/ 3200]\n",
      "loss: 0.431173  [  960/ 3200]\n",
      "loss: 0.334769  [ 1024/ 3200]\n",
      "loss: 0.298942  [ 1088/ 3200]\n",
      "loss: 0.254933  [ 1152/ 3200]\n",
      "loss: 0.253359  [ 1216/ 3200]\n",
      "loss: 0.281183  [ 1280/ 3200]\n",
      "loss: 0.378877  [ 1344/ 3200]\n",
      "loss: 0.350681  [ 1408/ 3200]\n",
      "loss: 0.322912  [ 1472/ 3200]\n",
      "loss: 0.268187  [ 1536/ 3200]\n",
      "loss: 0.315644  [ 1600/ 3200]\n",
      "loss: 0.413893  [ 1664/ 3200]\n",
      "loss: 0.336055  [ 1728/ 3200]\n",
      "loss: 0.303956  [ 1792/ 3200]\n",
      "loss: 0.310620  [ 1856/ 3200]\n",
      "loss: 0.321918  [ 1920/ 3200]\n",
      "loss: 0.351413  [ 1984/ 3200]\n",
      "loss: 0.300847  [ 2048/ 3200]\n",
      "loss: 0.488827  [ 2112/ 3200]\n",
      "loss: 0.255340  [ 2176/ 3200]\n",
      "loss: 0.293451  [ 2240/ 3200]\n",
      "loss: 0.327540  [ 2304/ 3200]\n",
      "loss: 0.415894  [ 2368/ 3200]\n",
      "loss: 0.363376  [ 2432/ 3200]\n",
      "loss: 0.335644  [ 2496/ 3200]\n",
      "loss: 0.424281  [ 2560/ 3200]\n",
      "loss: 0.282623  [ 2624/ 3200]\n",
      "loss: 0.401170  [ 2688/ 3200]\n",
      "loss: 0.309524  [ 2752/ 3200]\n",
      "loss: 0.427349  [ 2816/ 3200]\n",
      "loss: 0.345619  [ 2880/ 3200]\n",
      "loss: 0.306120  [ 2944/ 3200]\n",
      "loss: 0.369611  [ 3008/ 3200]\n",
      "loss: 0.279186  [ 3072/ 3200]\n",
      "loss: 0.275675  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036213\n",
      "f1 macro averaged score: 0.782614\n",
      "Accuracy               : 78.8%\n",
      "Confusion matrix       :\n",
      "tensor([[183,   4,   0,  13],\n",
      "        [ 19, 110,  19,  52],\n",
      "        [  0,  19, 162,  19],\n",
      "        [  9,   5,  11, 175]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.8112e-04.\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 0.344719  [    0/ 3200]\n",
      "loss: 0.303437  [   64/ 3200]\n",
      "loss: 0.348643  [  128/ 3200]\n",
      "loss: 0.305900  [  192/ 3200]\n",
      "loss: 0.407500  [  256/ 3200]\n",
      "loss: 0.376024  [  320/ 3200]\n",
      "loss: 0.283303  [  384/ 3200]\n",
      "loss: 0.280027  [  448/ 3200]\n",
      "loss: 0.208384  [  512/ 3200]\n",
      "loss: 0.197677  [  576/ 3200]\n",
      "loss: 0.375387  [  640/ 3200]\n",
      "loss: 0.318805  [  704/ 3200]\n",
      "loss: 0.289894  [  768/ 3200]\n",
      "loss: 0.223924  [  832/ 3200]\n",
      "loss: 0.387797  [  896/ 3200]\n",
      "loss: 0.391420  [  960/ 3200]\n",
      "loss: 0.318887  [ 1024/ 3200]\n",
      "loss: 0.231108  [ 1088/ 3200]\n",
      "loss: 0.310473  [ 1152/ 3200]\n",
      "loss: 0.259078  [ 1216/ 3200]\n",
      "loss: 0.243405  [ 1280/ 3200]\n",
      "loss: 0.236731  [ 1344/ 3200]\n",
      "loss: 0.312772  [ 1408/ 3200]\n",
      "loss: 0.229436  [ 1472/ 3200]\n",
      "loss: 0.336289  [ 1536/ 3200]\n",
      "loss: 0.520431  [ 1600/ 3200]\n",
      "loss: 0.251088  [ 1664/ 3200]\n",
      "loss: 0.249639  [ 1728/ 3200]\n",
      "loss: 0.394608  [ 1792/ 3200]\n",
      "loss: 0.278963  [ 1856/ 3200]\n",
      "loss: 0.270303  [ 1920/ 3200]\n",
      "loss: 0.328116  [ 1984/ 3200]\n",
      "loss: 0.317561  [ 2048/ 3200]\n",
      "loss: 0.343953  [ 2112/ 3200]\n",
      "loss: 0.380562  [ 2176/ 3200]\n",
      "loss: 0.290166  [ 2240/ 3200]\n",
      "loss: 0.346911  [ 2304/ 3200]\n",
      "loss: 0.429582  [ 2368/ 3200]\n",
      "loss: 0.337305  [ 2432/ 3200]\n",
      "loss: 0.248091  [ 2496/ 3200]\n",
      "loss: 0.434706  [ 2560/ 3200]\n",
      "loss: 0.459151  [ 2624/ 3200]\n",
      "loss: 0.193197  [ 2688/ 3200]\n",
      "loss: 0.360539  [ 2752/ 3200]\n",
      "loss: 0.336706  [ 2816/ 3200]\n",
      "loss: 0.316284  [ 2880/ 3200]\n",
      "loss: 0.308353  [ 2944/ 3200]\n",
      "loss: 0.490391  [ 3008/ 3200]\n",
      "loss: 0.529997  [ 3072/ 3200]\n",
      "loss: 0.207304  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034706\n",
      "f1 macro averaged score: 0.786375\n",
      "Accuracy               : 78.9%\n",
      "Confusion matrix       :\n",
      "tensor([[179,   7,   0,  14],\n",
      "        [ 15, 121,  24,  40],\n",
      "        [  0,  19, 163,  18],\n",
      "        [  7,  12,  13, 168]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.4707e-04.\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.329801  [    0/ 3200]\n",
      "loss: 0.248922  [   64/ 3200]\n",
      "loss: 0.303760  [  128/ 3200]\n",
      "loss: 0.339256  [  192/ 3200]\n",
      "loss: 0.192184  [  256/ 3200]\n",
      "loss: 0.384299  [  320/ 3200]\n",
      "loss: 0.296815  [  384/ 3200]\n",
      "loss: 0.302156  [  448/ 3200]\n",
      "loss: 0.264229  [  512/ 3200]\n",
      "loss: 0.284114  [  576/ 3200]\n",
      "loss: 0.388731  [  640/ 3200]\n",
      "loss: 0.372376  [  704/ 3200]\n",
      "loss: 0.347910  [  768/ 3200]\n",
      "loss: 0.255636  [  832/ 3200]\n",
      "loss: 0.284342  [  896/ 3200]\n",
      "loss: 0.290674  [  960/ 3200]\n",
      "loss: 0.222543  [ 1024/ 3200]\n",
      "loss: 0.346588  [ 1088/ 3200]\n",
      "loss: 0.211656  [ 1152/ 3200]\n",
      "loss: 0.256476  [ 1216/ 3200]\n",
      "loss: 0.209546  [ 1280/ 3200]\n",
      "loss: 0.279555  [ 1344/ 3200]\n",
      "loss: 0.485651  [ 1408/ 3200]\n",
      "loss: 0.284212  [ 1472/ 3200]\n",
      "loss: 0.205316  [ 1536/ 3200]\n",
      "loss: 0.312081  [ 1600/ 3200]\n",
      "loss: 0.424371  [ 1664/ 3200]\n",
      "loss: 0.305061  [ 1728/ 3200]\n",
      "loss: 0.365838  [ 1792/ 3200]\n",
      "loss: 0.348681  [ 1856/ 3200]\n",
      "loss: 0.230154  [ 1920/ 3200]\n",
      "loss: 0.388961  [ 1984/ 3200]\n",
      "loss: 0.306288  [ 2048/ 3200]\n",
      "loss: 0.484331  [ 2112/ 3200]\n",
      "loss: 0.375765  [ 2176/ 3200]\n",
      "loss: 0.342017  [ 2240/ 3200]\n",
      "loss: 0.251107  [ 2304/ 3200]\n",
      "loss: 0.471865  [ 2368/ 3200]\n",
      "loss: 0.274451  [ 2432/ 3200]\n",
      "loss: 0.359266  [ 2496/ 3200]\n",
      "loss: 0.239277  [ 2560/ 3200]\n",
      "loss: 0.287371  [ 2624/ 3200]\n",
      "loss: 0.231043  [ 2688/ 3200]\n",
      "loss: 0.275161  [ 2752/ 3200]\n",
      "loss: 0.304766  [ 2816/ 3200]\n",
      "loss: 0.245667  [ 2880/ 3200]\n",
      "loss: 0.431702  [ 2944/ 3200]\n",
      "loss: 0.375868  [ 3008/ 3200]\n",
      "loss: 0.244974  [ 3072/ 3200]\n",
      "loss: 0.368578  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.037410\n",
      "f1 macro averaged score: 0.762065\n",
      "Accuracy               : 76.8%\n",
      "Confusion matrix       :\n",
      "tensor([[185,   1,   0,  14],\n",
      "        [ 24, 103,  14,  59],\n",
      "        [  0,  22, 153,  25],\n",
      "        [ 10,   8,   9, 173]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.1471e-04.\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.373046  [    0/ 3200]\n",
      "loss: 0.299128  [   64/ 3200]\n",
      "loss: 0.330374  [  128/ 3200]\n",
      "loss: 0.287562  [  192/ 3200]\n",
      "loss: 0.236894  [  256/ 3200]\n",
      "loss: 0.335003  [  320/ 3200]\n",
      "loss: 0.252471  [  384/ 3200]\n",
      "loss: 0.244888  [  448/ 3200]\n",
      "loss: 0.227044  [  512/ 3200]\n",
      "loss: 0.435619  [  576/ 3200]\n",
      "loss: 0.220833  [  640/ 3200]\n",
      "loss: 0.244771  [  704/ 3200]\n",
      "loss: 0.334745  [  768/ 3200]\n",
      "loss: 0.406324  [  832/ 3200]\n",
      "loss: 0.284635  [  896/ 3200]\n",
      "loss: 0.335237  [  960/ 3200]\n",
      "loss: 0.290422  [ 1024/ 3200]\n",
      "loss: 0.357298  [ 1088/ 3200]\n",
      "loss: 0.261867  [ 1152/ 3200]\n",
      "loss: 0.392860  [ 1216/ 3200]\n",
      "loss: 0.363939  [ 1280/ 3200]\n",
      "loss: 0.222662  [ 1344/ 3200]\n",
      "loss: 0.308657  [ 1408/ 3200]\n",
      "loss: 0.313694  [ 1472/ 3200]\n",
      "loss: 0.294341  [ 1536/ 3200]\n",
      "loss: 0.359738  [ 1600/ 3200]\n",
      "loss: 0.217774  [ 1664/ 3200]\n",
      "loss: 0.225551  [ 1728/ 3200]\n",
      "loss: 0.282004  [ 1792/ 3200]\n",
      "loss: 0.303534  [ 1856/ 3200]\n",
      "loss: 0.371067  [ 1920/ 3200]\n",
      "loss: 0.377078  [ 1984/ 3200]\n",
      "loss: 0.220651  [ 2048/ 3200]\n",
      "loss: 0.334143  [ 2112/ 3200]\n",
      "loss: 0.240064  [ 2176/ 3200]\n",
      "loss: 0.266923  [ 2240/ 3200]\n",
      "loss: 0.236781  [ 2304/ 3200]\n",
      "loss: 0.423698  [ 2368/ 3200]\n",
      "loss: 0.298034  [ 2432/ 3200]\n",
      "loss: 0.345241  [ 2496/ 3200]\n",
      "loss: 0.290026  [ 2560/ 3200]\n",
      "loss: 0.300270  [ 2624/ 3200]\n",
      "loss: 0.321357  [ 2688/ 3200]\n",
      "loss: 0.268175  [ 2752/ 3200]\n",
      "loss: 0.379219  [ 2816/ 3200]\n",
      "loss: 0.329478  [ 2880/ 3200]\n",
      "loss: 0.333317  [ 2944/ 3200]\n",
      "loss: 0.291465  [ 3008/ 3200]\n",
      "loss: 0.281505  [ 3072/ 3200]\n",
      "loss: 0.415886  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.033978\n",
      "f1 macro averaged score: 0.801799\n",
      "Accuracy               : 80.2%\n",
      "Confusion matrix       :\n",
      "tensor([[183,   6,   0,  11],\n",
      "        [ 13, 135,  24,  28],\n",
      "        [  1,  27, 162,  10],\n",
      "        [  5,  22,  11, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.8398e-04.\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.283174  [    0/ 3200]\n",
      "loss: 0.216994  [   64/ 3200]\n",
      "loss: 0.278127  [  128/ 3200]\n",
      "loss: 0.338281  [  192/ 3200]\n",
      "loss: 0.341088  [  256/ 3200]\n",
      "loss: 0.503704  [  320/ 3200]\n",
      "loss: 0.446544  [  384/ 3200]\n",
      "loss: 0.266426  [  448/ 3200]\n",
      "loss: 0.268609  [  512/ 3200]\n",
      "loss: 0.330322  [  576/ 3200]\n",
      "loss: 0.245831  [  640/ 3200]\n",
      "loss: 0.203442  [  704/ 3200]\n",
      "loss: 0.473257  [  768/ 3200]\n",
      "loss: 0.247235  [  832/ 3200]\n",
      "loss: 0.360634  [  896/ 3200]\n",
      "loss: 0.341521  [  960/ 3200]\n",
      "loss: 0.368139  [ 1024/ 3200]\n",
      "loss: 0.315897  [ 1088/ 3200]\n",
      "loss: 0.236405  [ 1152/ 3200]\n",
      "loss: 0.389884  [ 1216/ 3200]\n",
      "loss: 0.252793  [ 1280/ 3200]\n",
      "loss: 0.191033  [ 1344/ 3200]\n",
      "loss: 0.373719  [ 1408/ 3200]\n",
      "loss: 0.240542  [ 1472/ 3200]\n",
      "loss: 0.201076  [ 1536/ 3200]\n",
      "loss: 0.189231  [ 1600/ 3200]\n",
      "loss: 0.202044  [ 1664/ 3200]\n",
      "loss: 0.301217  [ 1728/ 3200]\n",
      "loss: 0.292246  [ 1792/ 3200]\n",
      "loss: 0.314627  [ 1856/ 3200]\n",
      "loss: 0.299721  [ 1920/ 3200]\n",
      "loss: 0.449013  [ 1984/ 3200]\n",
      "loss: 0.385014  [ 2048/ 3200]\n",
      "loss: 0.275679  [ 2112/ 3200]\n",
      "loss: 0.253844  [ 2176/ 3200]\n",
      "loss: 0.240122  [ 2240/ 3200]\n",
      "loss: 0.274946  [ 2304/ 3200]\n",
      "loss: 0.362862  [ 2368/ 3200]\n",
      "loss: 0.308269  [ 2432/ 3200]\n",
      "loss: 0.439038  [ 2496/ 3200]\n",
      "loss: 0.259625  [ 2560/ 3200]\n",
      "loss: 0.294186  [ 2624/ 3200]\n",
      "loss: 0.327546  [ 2688/ 3200]\n",
      "loss: 0.218709  [ 2752/ 3200]\n",
      "loss: 0.183478  [ 2816/ 3200]\n",
      "loss: 0.343249  [ 2880/ 3200]\n",
      "loss: 0.375618  [ 2944/ 3200]\n",
      "loss: 0.335886  [ 3008/ 3200]\n",
      "loss: 0.175517  [ 3072/ 3200]\n",
      "loss: 0.263524  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034533\n",
      "f1 macro averaged score: 0.802739\n",
      "Accuracy               : 80.2%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  13,   0,  15],\n",
      "        [ 11, 138,  24,  27],\n",
      "        [  0,  24, 170,   6],\n",
      "        [  5,  20,  13, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.5478e-04.\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.281965  [    0/ 3200]\n",
      "loss: 0.382748  [   64/ 3200]\n",
      "loss: 0.221446  [  128/ 3200]\n",
      "loss: 0.209609  [  192/ 3200]\n",
      "loss: 0.332876  [  256/ 3200]\n",
      "loss: 0.345989  [  320/ 3200]\n",
      "loss: 0.278118  [  384/ 3200]\n",
      "loss: 0.367113  [  448/ 3200]\n",
      "loss: 0.236623  [  512/ 3200]\n",
      "loss: 0.242781  [  576/ 3200]\n",
      "loss: 0.358597  [  640/ 3200]\n",
      "loss: 0.250218  [  704/ 3200]\n",
      "loss: 0.308827  [  768/ 3200]\n",
      "loss: 0.342344  [  832/ 3200]\n",
      "loss: 0.356565  [  896/ 3200]\n",
      "loss: 0.251840  [  960/ 3200]\n",
      "loss: 0.258027  [ 1024/ 3200]\n",
      "loss: 0.291156  [ 1088/ 3200]\n",
      "loss: 0.343756  [ 1152/ 3200]\n",
      "loss: 0.262383  [ 1216/ 3200]\n",
      "loss: 0.179967  [ 1280/ 3200]\n",
      "loss: 0.318899  [ 1344/ 3200]\n",
      "loss: 0.272666  [ 1408/ 3200]\n",
      "loss: 0.245059  [ 1472/ 3200]\n",
      "loss: 0.331196  [ 1536/ 3200]\n",
      "loss: 0.221227  [ 1600/ 3200]\n",
      "loss: 0.434377  [ 1664/ 3200]\n",
      "loss: 0.255639  [ 1728/ 3200]\n",
      "loss: 0.293712  [ 1792/ 3200]\n",
      "loss: 0.317254  [ 1856/ 3200]\n",
      "loss: 0.307977  [ 1920/ 3200]\n",
      "loss: 0.341107  [ 1984/ 3200]\n",
      "loss: 0.268947  [ 2048/ 3200]\n",
      "loss: 0.282140  [ 2112/ 3200]\n",
      "loss: 0.347003  [ 2176/ 3200]\n",
      "loss: 0.264217  [ 2240/ 3200]\n",
      "loss: 0.247800  [ 2304/ 3200]\n",
      "loss: 0.240479  [ 2368/ 3200]\n",
      "loss: 0.205139  [ 2432/ 3200]\n",
      "loss: 0.231820  [ 2496/ 3200]\n",
      "loss: 0.295058  [ 2560/ 3200]\n",
      "loss: 0.287391  [ 2624/ 3200]\n",
      "loss: 0.282479  [ 2688/ 3200]\n",
      "loss: 0.300019  [ 2752/ 3200]\n",
      "loss: 0.293368  [ 2816/ 3200]\n",
      "loss: 0.374665  [ 2880/ 3200]\n",
      "loss: 0.263065  [ 2944/ 3200]\n",
      "loss: 0.289497  [ 3008/ 3200]\n",
      "loss: 0.245705  [ 3072/ 3200]\n",
      "loss: 0.467804  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034187\n",
      "f1 macro averaged score: 0.794430\n",
      "Accuracy               : 79.5%\n",
      "Confusion matrix       :\n",
      "tensor([[179,   9,   0,  12],\n",
      "        [ 12, 131,  21,  36],\n",
      "        [  0,  25, 166,   9],\n",
      "        [  7,  21,  12, 160]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.2704e-04.\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 0.314001  [    0/ 3200]\n",
      "loss: 0.217293  [   64/ 3200]\n",
      "loss: 0.250559  [  128/ 3200]\n",
      "loss: 0.310544  [  192/ 3200]\n",
      "loss: 0.292048  [  256/ 3200]\n",
      "loss: 0.243766  [  320/ 3200]\n",
      "loss: 0.313171  [  384/ 3200]\n",
      "loss: 0.304736  [  448/ 3200]\n",
      "loss: 0.199713  [  512/ 3200]\n",
      "loss: 0.329248  [  576/ 3200]\n",
      "loss: 0.274250  [  640/ 3200]\n",
      "loss: 0.293237  [  704/ 3200]\n",
      "loss: 0.189462  [  768/ 3200]\n",
      "loss: 0.275884  [  832/ 3200]\n",
      "loss: 0.268399  [  896/ 3200]\n",
      "loss: 0.207585  [  960/ 3200]\n",
      "loss: 0.371770  [ 1024/ 3200]\n",
      "loss: 0.254423  [ 1088/ 3200]\n",
      "loss: 0.162744  [ 1152/ 3200]\n",
      "loss: 0.219183  [ 1216/ 3200]\n",
      "loss: 0.171590  [ 1280/ 3200]\n",
      "loss: 0.345681  [ 1344/ 3200]\n",
      "loss: 0.333908  [ 1408/ 3200]\n",
      "loss: 0.251272  [ 1472/ 3200]\n",
      "loss: 0.297417  [ 1536/ 3200]\n",
      "loss: 0.201258  [ 1600/ 3200]\n",
      "loss: 0.403140  [ 1664/ 3200]\n",
      "loss: 0.224136  [ 1728/ 3200]\n",
      "loss: 0.375151  [ 1792/ 3200]\n",
      "loss: 0.325337  [ 1856/ 3200]\n",
      "loss: 0.265826  [ 1920/ 3200]\n",
      "loss: 0.206424  [ 1984/ 3200]\n",
      "loss: 0.383379  [ 2048/ 3200]\n",
      "loss: 0.234872  [ 2112/ 3200]\n",
      "loss: 0.400590  [ 2176/ 3200]\n",
      "loss: 0.237049  [ 2240/ 3200]\n",
      "loss: 0.477826  [ 2304/ 3200]\n",
      "loss: 0.246910  [ 2368/ 3200]\n",
      "loss: 0.398533  [ 2432/ 3200]\n",
      "loss: 0.279388  [ 2496/ 3200]\n",
      "loss: 0.144996  [ 2560/ 3200]\n",
      "loss: 0.277951  [ 2624/ 3200]\n",
      "loss: 0.340493  [ 2688/ 3200]\n",
      "loss: 0.195873  [ 2752/ 3200]\n",
      "loss: 0.266997  [ 2816/ 3200]\n",
      "loss: 0.135916  [ 2880/ 3200]\n",
      "loss: 0.315642  [ 2944/ 3200]\n",
      "loss: 0.404035  [ 3008/ 3200]\n",
      "loss: 0.313696  [ 3072/ 3200]\n",
      "loss: 0.338985  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034808\n",
      "f1 macro averaged score: 0.793177\n",
      "Accuracy               : 79.6%\n",
      "Confusion matrix       :\n",
      "tensor([[189,   2,   0,   9],\n",
      "        [ 19, 126,  19,  36],\n",
      "        [  1,  28, 156,  15],\n",
      "        [ 10,  13,  11, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.0069e-04.\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.211946  [    0/ 3200]\n",
      "loss: 0.282918  [   64/ 3200]\n",
      "loss: 0.407228  [  128/ 3200]\n",
      "loss: 0.236761  [  192/ 3200]\n",
      "loss: 0.430464  [  256/ 3200]\n",
      "loss: 0.226402  [  320/ 3200]\n",
      "loss: 0.535861  [  384/ 3200]\n",
      "loss: 0.292783  [  448/ 3200]\n",
      "loss: 0.170993  [  512/ 3200]\n",
      "loss: 0.196662  [  576/ 3200]\n",
      "loss: 0.272931  [  640/ 3200]\n",
      "loss: 0.189787  [  704/ 3200]\n",
      "loss: 0.281764  [  768/ 3200]\n",
      "loss: 0.330401  [  832/ 3200]\n",
      "loss: 0.297325  [  896/ 3200]\n",
      "loss: 0.231658  [  960/ 3200]\n",
      "loss: 0.267704  [ 1024/ 3200]\n",
      "loss: 0.184222  [ 1088/ 3200]\n",
      "loss: 0.381289  [ 1152/ 3200]\n",
      "loss: 0.248889  [ 1216/ 3200]\n",
      "loss: 0.312663  [ 1280/ 3200]\n",
      "loss: 0.197373  [ 1344/ 3200]\n",
      "loss: 0.258951  [ 1408/ 3200]\n",
      "loss: 0.248514  [ 1472/ 3200]\n",
      "loss: 0.314866  [ 1536/ 3200]\n",
      "loss: 0.233008  [ 1600/ 3200]\n",
      "loss: 0.260624  [ 1664/ 3200]\n",
      "loss: 0.273306  [ 1728/ 3200]\n",
      "loss: 0.254549  [ 1792/ 3200]\n",
      "loss: 0.237331  [ 1856/ 3200]\n",
      "loss: 0.252219  [ 1920/ 3200]\n",
      "loss: 0.386261  [ 1984/ 3200]\n",
      "loss: 0.285844  [ 2048/ 3200]\n",
      "loss: 0.199825  [ 2112/ 3200]\n",
      "loss: 0.244162  [ 2176/ 3200]\n",
      "loss: 0.276920  [ 2240/ 3200]\n",
      "loss: 0.281478  [ 2304/ 3200]\n",
      "loss: 0.343052  [ 2368/ 3200]\n",
      "loss: 0.373281  [ 2432/ 3200]\n",
      "loss: 0.335119  [ 2496/ 3200]\n",
      "loss: 0.341935  [ 2560/ 3200]\n",
      "loss: 0.414871  [ 2624/ 3200]\n",
      "loss: 0.242479  [ 2688/ 3200]\n",
      "loss: 0.210506  [ 2752/ 3200]\n",
      "loss: 0.205102  [ 2816/ 3200]\n",
      "loss: 0.221813  [ 2880/ 3200]\n",
      "loss: 0.292913  [ 2944/ 3200]\n",
      "loss: 0.260325  [ 3008/ 3200]\n",
      "loss: 0.179569  [ 3072/ 3200]\n",
      "loss: 0.262222  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035350\n",
      "f1 macro averaged score: 0.779424\n",
      "Accuracy               : 78.1%\n",
      "Confusion matrix       :\n",
      "tensor([[180,   4,   0,  16],\n",
      "        [ 12, 120,  23,  45],\n",
      "        [  0,  24, 161,  15],\n",
      "        [  8,  15,  13, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.7565e-04.\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 0.282193  [    0/ 3200]\n",
      "loss: 0.349836  [   64/ 3200]\n",
      "loss: 0.277007  [  128/ 3200]\n",
      "loss: 0.299584  [  192/ 3200]\n",
      "loss: 0.359215  [  256/ 3200]\n",
      "loss: 0.198857  [  320/ 3200]\n",
      "loss: 0.354667  [  384/ 3200]\n",
      "loss: 0.238035  [  448/ 3200]\n",
      "loss: 0.256563  [  512/ 3200]\n",
      "loss: 0.216777  [  576/ 3200]\n",
      "loss: 0.349313  [  640/ 3200]\n",
      "loss: 0.296247  [  704/ 3200]\n",
      "loss: 0.187914  [  768/ 3200]\n",
      "loss: 0.238089  [  832/ 3200]\n",
      "loss: 0.202428  [  896/ 3200]\n",
      "loss: 0.284121  [  960/ 3200]\n",
      "loss: 0.386900  [ 1024/ 3200]\n",
      "loss: 0.223698  [ 1088/ 3200]\n",
      "loss: 0.297749  [ 1152/ 3200]\n",
      "loss: 0.189888  [ 1216/ 3200]\n",
      "loss: 0.316839  [ 1280/ 3200]\n",
      "loss: 0.124116  [ 1344/ 3200]\n",
      "loss: 0.275443  [ 1408/ 3200]\n",
      "loss: 0.275995  [ 1472/ 3200]\n",
      "loss: 0.328252  [ 1536/ 3200]\n",
      "loss: 0.207719  [ 1600/ 3200]\n",
      "loss: 0.248498  [ 1664/ 3200]\n",
      "loss: 0.234371  [ 1728/ 3200]\n",
      "loss: 0.268145  [ 1792/ 3200]\n",
      "loss: 0.348835  [ 1856/ 3200]\n",
      "loss: 0.140510  [ 1920/ 3200]\n",
      "loss: 0.317711  [ 1984/ 3200]\n",
      "loss: 0.379382  [ 2048/ 3200]\n",
      "loss: 0.336446  [ 2112/ 3200]\n",
      "loss: 0.202922  [ 2176/ 3200]\n",
      "loss: 0.281166  [ 2240/ 3200]\n",
      "loss: 0.259310  [ 2304/ 3200]\n",
      "loss: 0.208857  [ 2368/ 3200]\n",
      "loss: 0.352019  [ 2432/ 3200]\n",
      "loss: 0.176925  [ 2496/ 3200]\n",
      "loss: 0.255847  [ 2560/ 3200]\n",
      "loss: 0.279390  [ 2624/ 3200]\n",
      "loss: 0.240717  [ 2688/ 3200]\n",
      "loss: 0.175376  [ 2752/ 3200]\n",
      "loss: 0.188633  [ 2816/ 3200]\n",
      "loss: 0.225686  [ 2880/ 3200]\n",
      "loss: 0.368395  [ 2944/ 3200]\n",
      "loss: 0.234716  [ 3008/ 3200]\n",
      "loss: 0.247133  [ 3072/ 3200]\n",
      "loss: 0.370038  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035231\n",
      "f1 macro averaged score: 0.784542\n",
      "Accuracy               : 78.8%\n",
      "Confusion matrix       :\n",
      "tensor([[180,   8,   0,  12],\n",
      "        [ 18, 122,  29,  31],\n",
      "        [  0,  23, 170,   7],\n",
      "        [  8,  18,  16, 158]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.5187e-04.\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.216905  [    0/ 3200]\n",
      "loss: 0.255471  [   64/ 3200]\n",
      "loss: 0.278085  [  128/ 3200]\n",
      "loss: 0.281571  [  192/ 3200]\n",
      "loss: 0.182101  [  256/ 3200]\n",
      "loss: 0.278923  [  320/ 3200]\n",
      "loss: 0.303378  [  384/ 3200]\n",
      "loss: 0.214787  [  448/ 3200]\n",
      "loss: 0.288888  [  512/ 3200]\n",
      "loss: 0.124320  [  576/ 3200]\n",
      "loss: 0.190032  [  640/ 3200]\n",
      "loss: 0.349735  [  704/ 3200]\n",
      "loss: 0.245446  [  768/ 3200]\n",
      "loss: 0.217543  [  832/ 3200]\n",
      "loss: 0.318714  [  896/ 3200]\n",
      "loss: 0.327621  [  960/ 3200]\n",
      "loss: 0.256856  [ 1024/ 3200]\n",
      "loss: 0.224199  [ 1088/ 3200]\n",
      "loss: 0.195148  [ 1152/ 3200]\n",
      "loss: 0.223131  [ 1216/ 3200]\n",
      "loss: 0.189999  [ 1280/ 3200]\n",
      "loss: 0.238944  [ 1344/ 3200]\n",
      "loss: 0.246251  [ 1408/ 3200]\n",
      "loss: 0.268509  [ 1472/ 3200]\n",
      "loss: 0.184043  [ 1536/ 3200]\n",
      "loss: 0.350817  [ 1600/ 3200]\n",
      "loss: 0.383025  [ 1664/ 3200]\n",
      "loss: 0.413777  [ 1728/ 3200]\n",
      "loss: 0.271806  [ 1792/ 3200]\n",
      "loss: 0.317153  [ 1856/ 3200]\n",
      "loss: 0.253852  [ 1920/ 3200]\n",
      "loss: 0.314229  [ 1984/ 3200]\n",
      "loss: 0.293647  [ 2048/ 3200]\n",
      "loss: 0.274745  [ 2112/ 3200]\n",
      "loss: 0.288110  [ 2176/ 3200]\n",
      "loss: 0.320004  [ 2240/ 3200]\n",
      "loss: 0.273831  [ 2304/ 3200]\n",
      "loss: 0.292931  [ 2368/ 3200]\n",
      "loss: 0.275479  [ 2432/ 3200]\n",
      "loss: 0.231103  [ 2496/ 3200]\n",
      "loss: 0.179909  [ 2560/ 3200]\n",
      "loss: 0.216445  [ 2624/ 3200]\n",
      "loss: 0.319228  [ 2688/ 3200]\n",
      "loss: 0.247097  [ 2752/ 3200]\n",
      "loss: 0.198070  [ 2816/ 3200]\n",
      "loss: 0.245384  [ 2880/ 3200]\n",
      "loss: 0.350193  [ 2944/ 3200]\n",
      "loss: 0.258840  [ 3008/ 3200]\n",
      "loss: 0.195851  [ 3072/ 3200]\n",
      "loss: 0.204538  [ 3136/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036160\n",
      "f1 macro averaged score: 0.771027\n",
      "Accuracy               : 77.8%\n",
      "Confusion matrix       :\n",
      "tensor([[180,   4,   0,  16],\n",
      "        [ 18, 103,  28,  51],\n",
      "        [  1,  14, 168,  17],\n",
      "        [  7,   9,  13, 171]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.2928e-04.\n",
      "\n",
      "Best epoch: 25 with f1 macro averaged score: 0.8027392625808716\n",
      "Test Error:\n",
      "Avg loss               : 0.039423\n",
      "f1 macro averaged score: 0.769001\n",
      "Accuracy               : 77.2%\n",
      "Confusion matrix       :\n",
      "tensor([[280,   9,   3,   5],\n",
      "        [ 14, 171,  47,  92],\n",
      "        [  4,  24, 305,  23],\n",
      "        [ 16,  41,  36, 306]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "  Batch size: 128\n",
      "Epoch: 1\n",
      "-----------------------------\n",
      "loss: 1.390470  [    0/ 3200]\n",
      "loss: 4.557989  [  128/ 3200]\n",
      "loss: 1.491062  [  256/ 3200]\n",
      "loss: 2.192979  [  384/ 3200]\n",
      "loss: 1.712780  [  512/ 3200]\n",
      "loss: 1.798857  [  640/ 3200]\n",
      "loss: 1.552560  [  768/ 3200]\n",
      "loss: 1.812007  [  896/ 3200]\n",
      "loss: 2.081413  [ 1024/ 3200]\n",
      "loss: 1.416789  [ 1152/ 3200]\n",
      "loss: 1.386114  [ 1280/ 3200]\n",
      "loss: 1.419125  [ 1408/ 3200]\n",
      "loss: 1.390396  [ 1536/ 3200]\n",
      "loss: 1.431581  [ 1664/ 3200]\n",
      "loss: 1.380448  [ 1792/ 3200]\n",
      "loss: 1.396198  [ 1920/ 3200]\n",
      "loss: 1.376591  [ 2048/ 3200]\n",
      "loss: 1.370904  [ 2176/ 3200]\n",
      "loss: 1.400335  [ 2304/ 3200]\n",
      "loss: 1.405069  [ 2432/ 3200]\n",
      "loss: 1.383804  [ 2560/ 3200]\n",
      "loss: 1.382608  [ 2688/ 3200]\n",
      "loss: 1.384140  [ 2816/ 3200]\n",
      "loss: 1.377894  [ 2944/ 3200]\n",
      "loss: 1.385638  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086468\n",
      "f1 macro averaged score: 0.239658\n",
      "Accuracy               : 27.6%\n",
      "Confusion matrix       :\n",
      "tensor([[ 18, 118,  56,   8],\n",
      "        [ 20, 100,  64,  16],\n",
      "        [ 20,  80,  88,  12],\n",
      "        [ 14, 119,  52,  15]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.9000e-03.\n",
      "\n",
      "Epoch: 2\n",
      "-----------------------------\n",
      "loss: 1.375686  [    0/ 3200]\n",
      "loss: 1.395366  [  128/ 3200]\n",
      "loss: 1.377959  [  256/ 3200]\n",
      "loss: 1.379354  [  384/ 3200]\n",
      "loss: 1.388994  [  512/ 3200]\n",
      "loss: 1.361025  [  640/ 3200]\n",
      "loss: 1.387677  [  768/ 3200]\n",
      "loss: 1.355055  [  896/ 3200]\n",
      "loss: 1.384341  [ 1024/ 3200]\n",
      "loss: 1.377545  [ 1152/ 3200]\n",
      "loss: 1.404800  [ 1280/ 3200]\n",
      "loss: 1.372423  [ 1408/ 3200]\n",
      "loss: 1.361603  [ 1536/ 3200]\n",
      "loss: 1.337330  [ 1664/ 3200]\n",
      "loss: 1.328470  [ 1792/ 3200]\n",
      "loss: 1.322784  [ 1920/ 3200]\n",
      "loss: 1.499732  [ 2048/ 3200]\n",
      "loss: 1.540012  [ 2176/ 3200]\n",
      "loss: 1.414065  [ 2304/ 3200]\n",
      "loss: 1.420684  [ 2432/ 3200]\n",
      "loss: 1.388568  [ 2560/ 3200]\n",
      "loss: 1.380267  [ 2688/ 3200]\n",
      "loss: 1.341970  [ 2816/ 3200]\n",
      "loss: 1.416895  [ 2944/ 3200]\n",
      "loss: 1.383726  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.086074\n",
      "f1 macro averaged score: 0.204553\n",
      "Accuracy               : 25.2%\n",
      "Confusion matrix       :\n",
      "tensor([[130,  57,   5,   8],\n",
      "        [131,  43,   9,  17],\n",
      "        [106,  65,  14,  15],\n",
      "        [128,  54,   3,  15]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.8050e-03.\n",
      "\n",
      "Epoch: 3\n",
      "-----------------------------\n",
      "loss: 1.413092  [    0/ 3200]\n",
      "loss: 1.401943  [  128/ 3200]\n",
      "loss: 1.361732  [  256/ 3200]\n",
      "loss: 1.344454  [  384/ 3200]\n",
      "loss: 1.375465  [  512/ 3200]\n",
      "loss: 1.343541  [  640/ 3200]\n",
      "loss: 1.360563  [  768/ 3200]\n",
      "loss: 1.388524  [  896/ 3200]\n",
      "loss: 1.348722  [ 1024/ 3200]\n",
      "loss: 1.361736  [ 1152/ 3200]\n",
      "loss: 1.324506  [ 1280/ 3200]\n",
      "loss: 1.345864  [ 1408/ 3200]\n",
      "loss: 1.347407  [ 1536/ 3200]\n",
      "loss: 1.351011  [ 1664/ 3200]\n",
      "loss: 1.324518  [ 1792/ 3200]\n",
      "loss: 1.330561  [ 1920/ 3200]\n",
      "loss: 1.346783  [ 2048/ 3200]\n",
      "loss: 1.325404  [ 2176/ 3200]\n",
      "loss: 1.373460  [ 2304/ 3200]\n",
      "loss: 1.340105  [ 2432/ 3200]\n",
      "loss: 1.321469  [ 2560/ 3200]\n",
      "loss: 1.290457  [ 2688/ 3200]\n",
      "loss: 1.303709  [ 2816/ 3200]\n",
      "loss: 1.317971  [ 2944/ 3200]\n",
      "loss: 1.247568  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.078035\n",
      "f1 macro averaged score: 0.506766\n",
      "Accuracy               : 55.1%\n",
      "Confusion matrix       :\n",
      "tensor([[117,  24,  16,  43],\n",
      "        [ 34,  16,  46, 104],\n",
      "        [ 12,   9, 148,  31],\n",
      "        [ 16,   4,  20, 160]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.7147e-03.\n",
      "\n",
      "Epoch: 4\n",
      "-----------------------------\n",
      "loss: 1.258371  [    0/ 3200]\n",
      "loss: 1.271604  [  128/ 3200]\n",
      "loss: 1.238348  [  256/ 3200]\n",
      "loss: 1.433138  [  384/ 3200]\n",
      "loss: 1.359593  [  512/ 3200]\n",
      "loss: 1.276559  [  640/ 3200]\n",
      "loss: 1.243425  [  768/ 3200]\n",
      "loss: 1.297127  [  896/ 3200]\n",
      "loss: 1.276631  [ 1024/ 3200]\n",
      "loss: 1.263146  [ 1152/ 3200]\n",
      "loss: 1.274208  [ 1280/ 3200]\n",
      "loss: 1.269125  [ 1408/ 3200]\n",
      "loss: 1.177062  [ 1536/ 3200]\n",
      "loss: 1.313439  [ 1664/ 3200]\n",
      "loss: 1.360406  [ 1792/ 3200]\n",
      "loss: 1.199136  [ 1920/ 3200]\n",
      "loss: 1.175343  [ 2048/ 3200]\n",
      "loss: 1.222432  [ 2176/ 3200]\n",
      "loss: 1.154727  [ 2304/ 3200]\n",
      "loss: 1.119594  [ 2432/ 3200]\n",
      "loss: 1.198398  [ 2560/ 3200]\n",
      "loss: 1.290882  [ 2688/ 3200]\n",
      "loss: 1.277773  [ 2816/ 3200]\n",
      "loss: 1.316603  [ 2944/ 3200]\n",
      "loss: 1.174447  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.071245\n",
      "f1 macro averaged score: 0.437597\n",
      "Accuracy               : 47.8%\n",
      "Confusion matrix       :\n",
      "tensor([[121,  13,   2,  64],\n",
      "        [ 35,  11,   7, 147],\n",
      "        [  5,  13,  69, 113],\n",
      "        [  5,   8,   6, 181]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.6290e-03.\n",
      "\n",
      "Epoch: 5\n",
      "-----------------------------\n",
      "loss: 1.108640  [    0/ 3200]\n",
      "loss: 1.113533  [  128/ 3200]\n",
      "loss: 1.106897  [  256/ 3200]\n",
      "loss: 1.138285  [  384/ 3200]\n",
      "loss: 1.238193  [  512/ 3200]\n",
      "loss: 1.079898  [  640/ 3200]\n",
      "loss: 1.060593  [  768/ 3200]\n",
      "loss: 1.140482  [  896/ 3200]\n",
      "loss: 1.031223  [ 1024/ 3200]\n",
      "loss: 1.038116  [ 1152/ 3200]\n",
      "loss: 0.967874  [ 1280/ 3200]\n",
      "loss: 0.994251  [ 1408/ 3200]\n",
      "loss: 1.202236  [ 1536/ 3200]\n",
      "loss: 1.319476  [ 1664/ 3200]\n",
      "loss: 1.343798  [ 1792/ 3200]\n",
      "loss: 1.386424  [ 1920/ 3200]\n",
      "loss: 1.271206  [ 2048/ 3200]\n",
      "loss: 1.215623  [ 2176/ 3200]\n",
      "loss: 1.133470  [ 2304/ 3200]\n",
      "loss: 1.086860  [ 2432/ 3200]\n",
      "loss: 1.298634  [ 2560/ 3200]\n",
      "loss: 1.158839  [ 2688/ 3200]\n",
      "loss: 1.131209  [ 2816/ 3200]\n",
      "loss: 1.071668  [ 2944/ 3200]\n",
      "loss: 1.029050  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.063348\n",
      "f1 macro averaged score: 0.595860\n",
      "Accuracy               : 62.9%\n",
      "Confusion matrix       :\n",
      "tensor([[127,  23,  17,  33],\n",
      "        [ 27,  38,  54,  81],\n",
      "        [  3,   9, 170,  18],\n",
      "        [  6,   4,  22, 168]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.5476e-03.\n",
      "\n",
      "Epoch: 6\n",
      "-----------------------------\n",
      "loss: 0.930790  [    0/ 3200]\n",
      "loss: 0.984598  [  128/ 3200]\n",
      "loss: 0.873822  [  256/ 3200]\n",
      "loss: 0.941176  [  384/ 3200]\n",
      "loss: 1.100295  [  512/ 3200]\n",
      "loss: 0.934018  [  640/ 3200]\n",
      "loss: 0.955755  [  768/ 3200]\n",
      "loss: 0.936242  [  896/ 3200]\n",
      "loss: 0.886114  [ 1024/ 3200]\n",
      "loss: 0.798549  [ 1152/ 3200]\n",
      "loss: 0.864370  [ 1280/ 3200]\n",
      "loss: 1.015117  [ 1408/ 3200]\n",
      "loss: 0.988614  [ 1536/ 3200]\n",
      "loss: 1.160742  [ 1664/ 3200]\n",
      "loss: 1.128133  [ 1792/ 3200]\n",
      "loss: 1.103479  [ 1920/ 3200]\n",
      "loss: 0.982124  [ 2048/ 3200]\n",
      "loss: 1.067465  [ 2176/ 3200]\n",
      "loss: 1.075768  [ 2304/ 3200]\n",
      "loss: 0.871776  [ 2432/ 3200]\n",
      "loss: 0.789241  [ 2560/ 3200]\n",
      "loss: 0.780025  [ 2688/ 3200]\n",
      "loss: 0.853276  [ 2816/ 3200]\n",
      "loss: 0.796215  [ 2944/ 3200]\n",
      "loss: 0.927379  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.060007\n",
      "f1 macro averaged score: 0.565016\n",
      "Accuracy               : 57.5%\n",
      "Confusion matrix       :\n",
      "tensor([[140,  40,   0,  20],\n",
      "        [ 26,  70,   1, 103],\n",
      "        [  3,  32,  71,  94],\n",
      "        [  5,  13,   3, 179]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.4702e-03.\n",
      "\n",
      "Epoch: 7\n",
      "-----------------------------\n",
      "loss: 1.037590  [    0/ 3200]\n",
      "loss: 0.920972  [  128/ 3200]\n",
      "loss: 0.962559  [  256/ 3200]\n",
      "loss: 0.981250  [  384/ 3200]\n",
      "loss: 1.033365  [  512/ 3200]\n",
      "loss: 1.022701  [  640/ 3200]\n",
      "loss: 0.811441  [  768/ 3200]\n",
      "loss: 0.763117  [  896/ 3200]\n",
      "loss: 0.784663  [ 1024/ 3200]\n",
      "loss: 0.813151  [ 1152/ 3200]\n",
      "loss: 0.740720  [ 1280/ 3200]\n",
      "loss: 0.736410  [ 1408/ 3200]\n",
      "loss: 0.806347  [ 1536/ 3200]\n",
      "loss: 1.100851  [ 1664/ 3200]\n",
      "loss: 1.016228  [ 1792/ 3200]\n",
      "loss: 0.964317  [ 1920/ 3200]\n",
      "loss: 0.786560  [ 2048/ 3200]\n",
      "loss: 0.913142  [ 2176/ 3200]\n",
      "loss: 0.883572  [ 2304/ 3200]\n",
      "loss: 0.677829  [ 2432/ 3200]\n",
      "loss: 0.848829  [ 2560/ 3200]\n",
      "loss: 0.783136  [ 2688/ 3200]\n",
      "loss: 0.960461  [ 2816/ 3200]\n",
      "loss: 1.121007  [ 2944/ 3200]\n",
      "loss: 0.913362  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.050428\n",
      "f1 macro averaged score: 0.693691\n",
      "Accuracy               : 70.0%\n",
      "Confusion matrix       :\n",
      "tensor([[164,  31,   0,   5],\n",
      "        [ 47,  87,  23,  43],\n",
      "        [  3,  24, 155,  18],\n",
      "        [ 12,  20,  14, 154]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3967e-03.\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 0.887243  [    0/ 3200]\n",
      "loss: 0.770561  [  128/ 3200]\n",
      "loss: 0.893798  [  256/ 3200]\n",
      "loss: 0.810310  [  384/ 3200]\n",
      "loss: 0.803458  [  512/ 3200]\n",
      "loss: 0.790097  [  640/ 3200]\n",
      "loss: 0.936276  [  768/ 3200]\n",
      "loss: 0.799821  [  896/ 3200]\n",
      "loss: 0.754999  [ 1024/ 3200]\n",
      "loss: 0.806429  [ 1152/ 3200]\n",
      "loss: 0.778130  [ 1280/ 3200]\n",
      "loss: 0.822143  [ 1408/ 3200]\n",
      "loss: 0.738270  [ 1536/ 3200]\n",
      "loss: 0.722362  [ 1664/ 3200]\n",
      "loss: 0.700208  [ 1792/ 3200]\n",
      "loss: 0.747032  [ 1920/ 3200]\n",
      "loss: 0.940077  [ 2048/ 3200]\n",
      "loss: 0.924725  [ 2176/ 3200]\n",
      "loss: 0.909971  [ 2304/ 3200]\n",
      "loss: 0.755640  [ 2432/ 3200]\n",
      "loss: 0.752623  [ 2560/ 3200]\n",
      "loss: 0.809352  [ 2688/ 3200]\n",
      "loss: 0.896783  [ 2816/ 3200]\n",
      "loss: 0.706508  [ 2944/ 3200]\n",
      "loss: 0.615982  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.048428\n",
      "f1 macro averaged score: 0.668123\n",
      "Accuracy               : 67.9%\n",
      "Confusion matrix       :\n",
      "tensor([[135,  50,   7,   8],\n",
      "        [ 22,  73,  63,  42],\n",
      "        [  1,  10, 184,   5],\n",
      "        [  5,  23,  21, 151]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3268e-03.\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 0.816371  [    0/ 3200]\n",
      "loss: 0.950522  [  128/ 3200]\n",
      "loss: 0.971073  [  256/ 3200]\n",
      "loss: 0.968031  [  384/ 3200]\n",
      "loss: 0.906999  [  512/ 3200]\n",
      "loss: 0.781840  [  640/ 3200]\n",
      "loss: 0.725530  [  768/ 3200]\n",
      "loss: 0.684946  [  896/ 3200]\n",
      "loss: 0.780945  [ 1024/ 3200]\n",
      "loss: 0.711660  [ 1152/ 3200]\n",
      "loss: 0.839437  [ 1280/ 3200]\n",
      "loss: 0.876334  [ 1408/ 3200]\n",
      "loss: 0.725707  [ 1536/ 3200]\n",
      "loss: 0.787826  [ 1664/ 3200]\n",
      "loss: 0.778672  [ 1792/ 3200]\n",
      "loss: 0.680423  [ 1920/ 3200]\n",
      "loss: 0.723198  [ 2048/ 3200]\n",
      "loss: 0.620120  [ 2176/ 3200]\n",
      "loss: 0.755103  [ 2304/ 3200]\n",
      "loss: 0.729212  [ 2432/ 3200]\n",
      "loss: 0.689262  [ 2560/ 3200]\n",
      "loss: 0.707785  [ 2688/ 3200]\n",
      "loss: 0.730121  [ 2816/ 3200]\n",
      "loss: 0.720286  [ 2944/ 3200]\n",
      "loss: 0.628976  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.043887\n",
      "f1 macro averaged score: 0.694985\n",
      "Accuracy               : 70.6%\n",
      "Confusion matrix       :\n",
      "tensor([[157,  32,   4,   7],\n",
      "        [ 33,  78,  52,  37],\n",
      "        [  1,  14, 176,   9],\n",
      "        [  6,  20,  20, 154]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2605e-03.\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 0.724780  [    0/ 3200]\n",
      "loss: 0.659065  [  128/ 3200]\n",
      "loss: 0.840197  [  256/ 3200]\n",
      "loss: 0.745089  [  384/ 3200]\n",
      "loss: 0.668530  [  512/ 3200]\n",
      "loss: 0.767810  [  640/ 3200]\n",
      "loss: 0.774235  [  768/ 3200]\n",
      "loss: 0.628152  [  896/ 3200]\n",
      "loss: 0.677700  [ 1024/ 3200]\n",
      "loss: 0.639130  [ 1152/ 3200]\n",
      "loss: 0.542229  [ 1280/ 3200]\n",
      "loss: 0.526062  [ 1408/ 3200]\n",
      "loss: 0.613107  [ 1536/ 3200]\n",
      "loss: 0.867804  [ 1664/ 3200]\n",
      "loss: 0.656962  [ 1792/ 3200]\n",
      "loss: 0.821678  [ 1920/ 3200]\n",
      "loss: 1.048390  [ 2048/ 3200]\n",
      "loss: 0.905822  [ 2176/ 3200]\n",
      "loss: 0.911059  [ 2304/ 3200]\n",
      "loss: 0.711180  [ 2432/ 3200]\n",
      "loss: 0.725273  [ 2560/ 3200]\n",
      "loss: 0.692410  [ 2688/ 3200]\n",
      "loss: 0.694445  [ 2816/ 3200]\n",
      "loss: 0.678815  [ 2944/ 3200]\n",
      "loss: 0.763999  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.045672\n",
      "f1 macro averaged score: 0.692869\n",
      "Accuracy               : 69.6%\n",
      "Confusion matrix       :\n",
      "tensor([[146,  42,   2,  10],\n",
      "        [ 27,  90,  18,  65],\n",
      "        [  1,  26, 150,  23],\n",
      "        [  4,  12,  13, 171]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1975e-03.\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 0.681919  [    0/ 3200]\n",
      "loss: 0.666269  [  128/ 3200]\n",
      "loss: 0.743661  [  256/ 3200]\n",
      "loss: 0.617932  [  384/ 3200]\n",
      "loss: 0.662894  [  512/ 3200]\n",
      "loss: 0.747542  [  640/ 3200]\n",
      "loss: 0.733959  [  768/ 3200]\n",
      "loss: 0.666357  [  896/ 3200]\n",
      "loss: 0.670262  [ 1024/ 3200]\n",
      "loss: 0.646021  [ 1152/ 3200]\n",
      "loss: 0.743457  [ 1280/ 3200]\n",
      "loss: 0.605669  [ 1408/ 3200]\n",
      "loss: 0.616590  [ 1536/ 3200]\n",
      "loss: 0.802578  [ 1664/ 3200]\n",
      "loss: 0.723091  [ 1792/ 3200]\n",
      "loss: 0.668991  [ 1920/ 3200]\n",
      "loss: 0.765903  [ 2048/ 3200]\n",
      "loss: 0.735736  [ 2176/ 3200]\n",
      "loss: 0.668168  [ 2304/ 3200]\n",
      "loss: 0.628324  [ 2432/ 3200]\n",
      "loss: 0.798005  [ 2560/ 3200]\n",
      "loss: 0.705185  [ 2688/ 3200]\n",
      "loss: 0.702720  [ 2816/ 3200]\n",
      "loss: 0.535020  [ 2944/ 3200]\n",
      "loss: 0.820613  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.041978\n",
      "f1 macro averaged score: 0.708653\n",
      "Accuracy               : 71.9%\n",
      "Confusion matrix       :\n",
      "tensor([[161,  27,   2,  10],\n",
      "        [ 32,  82,  31,  55],\n",
      "        [  1,  18, 167,  14],\n",
      "        [  5,  15,  15, 165]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1376e-03.\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 0.719461  [    0/ 3200]\n",
      "loss: 0.654596  [  128/ 3200]\n",
      "loss: 0.734831  [  256/ 3200]\n",
      "loss: 0.585771  [  384/ 3200]\n",
      "loss: 0.767111  [  512/ 3200]\n",
      "loss: 0.651234  [  640/ 3200]\n",
      "loss: 0.695909  [  768/ 3200]\n",
      "loss: 0.661343  [  896/ 3200]\n",
      "loss: 0.718588  [ 1024/ 3200]\n",
      "loss: 0.736832  [ 1152/ 3200]\n",
      "loss: 0.726577  [ 1280/ 3200]\n",
      "loss: 1.041754  [ 1408/ 3200]\n",
      "loss: 0.703237  [ 1536/ 3200]\n",
      "loss: 0.623616  [ 1664/ 3200]\n",
      "loss: 0.526805  [ 1792/ 3200]\n",
      "loss: 0.641736  [ 1920/ 3200]\n",
      "loss: 0.629287  [ 2048/ 3200]\n",
      "loss: 0.561203  [ 2176/ 3200]\n",
      "loss: 0.694728  [ 2304/ 3200]\n",
      "loss: 0.642909  [ 2432/ 3200]\n",
      "loss: 0.507866  [ 2560/ 3200]\n",
      "loss: 0.541012  [ 2688/ 3200]\n",
      "loss: 0.768809  [ 2816/ 3200]\n",
      "loss: 0.728955  [ 2944/ 3200]\n",
      "loss: 0.642971  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.040337\n",
      "f1 macro averaged score: 0.697624\n",
      "Accuracy               : 71.9%\n",
      "Confusion matrix       :\n",
      "tensor([[176,  14,   0,  10],\n",
      "        [ 44,  65,  33,  58],\n",
      "        [  1,  14, 168,  17],\n",
      "        [ 10,   8,  16, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0807e-03.\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 0.541627  [    0/ 3200]\n",
      "loss: 0.591410  [  128/ 3200]\n",
      "loss: 0.648526  [  256/ 3200]\n",
      "loss: 0.662712  [  384/ 3200]\n",
      "loss: 0.730858  [  512/ 3200]\n",
      "loss: 0.724828  [  640/ 3200]\n",
      "loss: 0.646603  [  768/ 3200]\n",
      "loss: 0.617517  [  896/ 3200]\n",
      "loss: 0.702146  [ 1024/ 3200]\n",
      "loss: 0.690894  [ 1152/ 3200]\n",
      "loss: 0.674624  [ 1280/ 3200]\n",
      "loss: 0.768210  [ 1408/ 3200]\n",
      "loss: 0.795955  [ 1536/ 3200]\n",
      "loss: 0.627726  [ 1664/ 3200]\n",
      "loss: 0.657652  [ 1792/ 3200]\n",
      "loss: 0.526921  [ 1920/ 3200]\n",
      "loss: 0.785939  [ 2048/ 3200]\n",
      "loss: 0.571250  [ 2176/ 3200]\n",
      "loss: 0.706785  [ 2304/ 3200]\n",
      "loss: 0.603960  [ 2432/ 3200]\n",
      "loss: 0.637644  [ 2560/ 3200]\n",
      "loss: 0.651181  [ 2688/ 3200]\n",
      "loss: 0.502346  [ 2816/ 3200]\n",
      "loss: 0.661052  [ 2944/ 3200]\n",
      "loss: 0.645870  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.040588\n",
      "f1 macro averaged score: 0.742175\n",
      "Accuracy               : 74.0%\n",
      "Confusion matrix       :\n",
      "tensor([[141,  48,   1,  10],\n",
      "        [ 16, 125,  34,  25],\n",
      "        [  0,  21, 173,   6],\n",
      "        [  2,  28,  17, 153]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0267e-03.\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 0.630617  [    0/ 3200]\n",
      "loss: 0.569727  [  128/ 3200]\n",
      "loss: 0.595862  [  256/ 3200]\n",
      "loss: 0.614116  [  384/ 3200]\n",
      "loss: 0.567756  [  512/ 3200]\n",
      "loss: 0.702762  [  640/ 3200]\n",
      "loss: 0.552951  [  768/ 3200]\n",
      "loss: 0.576238  [  896/ 3200]\n",
      "loss: 0.601500  [ 1024/ 3200]\n",
      "loss: 0.705309  [ 1152/ 3200]\n",
      "loss: 0.582755  [ 1280/ 3200]\n",
      "loss: 0.663491  [ 1408/ 3200]\n",
      "loss: 0.828436  [ 1536/ 3200]\n",
      "loss: 0.749865  [ 1664/ 3200]\n",
      "loss: 0.614258  [ 1792/ 3200]\n",
      "loss: 0.522655  [ 1920/ 3200]\n",
      "loss: 0.677306  [ 2048/ 3200]\n",
      "loss: 0.628208  [ 2176/ 3200]\n",
      "loss: 0.657787  [ 2304/ 3200]\n",
      "loss: 0.791041  [ 2432/ 3200]\n",
      "loss: 0.607793  [ 2560/ 3200]\n",
      "loss: 0.687370  [ 2688/ 3200]\n",
      "loss: 0.627005  [ 2816/ 3200]\n",
      "loss: 0.602075  [ 2944/ 3200]\n",
      "loss: 0.583679  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.039277\n",
      "f1 macro averaged score: 0.711056\n",
      "Accuracy               : 72.4%\n",
      "Confusion matrix       :\n",
      "tensor([[168,  20,   1,  11],\n",
      "        [ 32,  79,  37,  52],\n",
      "        [  1,  16, 170,  13],\n",
      "        [  5,  15,  18, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.7535e-04.\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.579029  [    0/ 3200]\n",
      "loss: 0.604801  [  128/ 3200]\n",
      "loss: 0.551714  [  256/ 3200]\n",
      "loss: 0.566574  [  384/ 3200]\n",
      "loss: 0.741771  [  512/ 3200]\n",
      "loss: 0.581034  [  640/ 3200]\n",
      "loss: 0.528813  [  768/ 3200]\n",
      "loss: 0.523628  [  896/ 3200]\n",
      "loss: 0.640942  [ 1024/ 3200]\n",
      "loss: 0.508238  [ 1152/ 3200]\n",
      "loss: 0.524526  [ 1280/ 3200]\n",
      "loss: 0.581343  [ 1408/ 3200]\n",
      "loss: 0.623649  [ 1536/ 3200]\n",
      "loss: 0.620292  [ 1664/ 3200]\n",
      "loss: 0.709079  [ 1792/ 3200]\n",
      "loss: 0.572089  [ 1920/ 3200]\n",
      "loss: 0.611926  [ 2048/ 3200]\n",
      "loss: 0.686665  [ 2176/ 3200]\n",
      "loss: 0.606314  [ 2304/ 3200]\n",
      "loss: 0.588448  [ 2432/ 3200]\n",
      "loss: 0.543632  [ 2560/ 3200]\n",
      "loss: 0.507754  [ 2688/ 3200]\n",
      "loss: 0.577506  [ 2816/ 3200]\n",
      "loss: 0.632089  [ 2944/ 3200]\n",
      "loss: 0.701933  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.043583\n",
      "f1 macro averaged score: 0.711763\n",
      "Accuracy               : 70.9%\n",
      "Confusion matrix       :\n",
      "tensor([[120,  67,   1,  12],\n",
      "        [ 10, 124,  33,  33],\n",
      "        [  0,  23, 164,  13],\n",
      "        [  0,  25,  16, 159]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.2658e-04.\n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------\n",
      "loss: 0.631510  [    0/ 3200]\n",
      "loss: 0.505353  [  128/ 3200]\n",
      "loss: 0.532552  [  256/ 3200]\n",
      "loss: 0.629947  [  384/ 3200]\n",
      "loss: 0.602208  [  512/ 3200]\n",
      "loss: 0.500841  [  640/ 3200]\n",
      "loss: 0.534086  [  768/ 3200]\n",
      "loss: 0.600999  [  896/ 3200]\n",
      "loss: 0.596910  [ 1024/ 3200]\n",
      "loss: 0.719015  [ 1152/ 3200]\n",
      "loss: 0.679063  [ 1280/ 3200]\n",
      "loss: 0.674238  [ 1408/ 3200]\n",
      "loss: 0.564941  [ 1536/ 3200]\n",
      "loss: 0.634652  [ 1664/ 3200]\n",
      "loss: 0.587174  [ 1792/ 3200]\n",
      "loss: 0.571791  [ 1920/ 3200]\n",
      "loss: 0.589209  [ 2048/ 3200]\n",
      "loss: 0.532849  [ 2176/ 3200]\n",
      "loss: 0.633333  [ 2304/ 3200]\n",
      "loss: 0.524774  [ 2432/ 3200]\n",
      "loss: 0.624039  [ 2560/ 3200]\n",
      "loss: 0.754656  [ 2688/ 3200]\n",
      "loss: 0.569431  [ 2816/ 3200]\n",
      "loss: 0.510674  [ 2944/ 3200]\n",
      "loss: 0.687298  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.037943\n",
      "f1 macro averaged score: 0.754913\n",
      "Accuracy               : 75.8%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  17,   1,  10],\n",
      "        [ 26, 115,  26,  33],\n",
      "        [  0,  26, 159,  15],\n",
      "        [  7,  18,  15, 160]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 8.8025e-04.\n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------\n",
      "loss: 0.635868  [    0/ 3200]\n",
      "loss: 0.542944  [  128/ 3200]\n",
      "loss: 0.588248  [  256/ 3200]\n",
      "loss: 0.575580  [  384/ 3200]\n",
      "loss: 0.556687  [  512/ 3200]\n",
      "loss: 0.546897  [  640/ 3200]\n",
      "loss: 0.490572  [  768/ 3200]\n",
      "loss: 0.515718  [  896/ 3200]\n",
      "loss: 0.587541  [ 1024/ 3200]\n",
      "loss: 0.702604  [ 1152/ 3200]\n",
      "loss: 0.608709  [ 1280/ 3200]\n",
      "loss: 0.477177  [ 1408/ 3200]\n",
      "loss: 0.705478  [ 1536/ 3200]\n",
      "loss: 0.562416  [ 1664/ 3200]\n",
      "loss: 0.641509  [ 1792/ 3200]\n",
      "loss: 0.528735  [ 1920/ 3200]\n",
      "loss: 0.571908  [ 2048/ 3200]\n",
      "loss: 0.613878  [ 2176/ 3200]\n",
      "loss: 0.519788  [ 2304/ 3200]\n",
      "loss: 0.436168  [ 2432/ 3200]\n",
      "loss: 0.683033  [ 2560/ 3200]\n",
      "loss: 0.564599  [ 2688/ 3200]\n",
      "loss: 0.584217  [ 2816/ 3200]\n",
      "loss: 0.591234  [ 2944/ 3200]\n",
      "loss: 0.602231  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.039670\n",
      "f1 macro averaged score: 0.714206\n",
      "Accuracy               : 73.5%\n",
      "Confusion matrix       :\n",
      "tensor([[190,   2,   0,   8],\n",
      "        [ 48,  69,  21,  62],\n",
      "        [  1,  17, 161,  21],\n",
      "        [ 15,   4,  13, 168]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 8.3624e-04.\n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------\n",
      "loss: 0.582045  [    0/ 3200]\n",
      "loss: 0.593362  [  128/ 3200]\n",
      "loss: 0.584071  [  256/ 3200]\n",
      "loss: 0.512694  [  384/ 3200]\n",
      "loss: 0.614027  [  512/ 3200]\n",
      "loss: 0.625924  [  640/ 3200]\n",
      "loss: 0.540296  [  768/ 3200]\n",
      "loss: 0.588525  [  896/ 3200]\n",
      "loss: 0.639803  [ 1024/ 3200]\n",
      "loss: 0.652318  [ 1152/ 3200]\n",
      "loss: 0.411080  [ 1280/ 3200]\n",
      "loss: 0.514722  [ 1408/ 3200]\n",
      "loss: 0.609366  [ 1536/ 3200]\n",
      "loss: 0.604156  [ 1664/ 3200]\n",
      "loss: 0.431812  [ 1792/ 3200]\n",
      "loss: 0.609975  [ 1920/ 3200]\n",
      "loss: 0.525789  [ 2048/ 3200]\n",
      "loss: 0.498981  [ 2176/ 3200]\n",
      "loss: 0.621641  [ 2304/ 3200]\n",
      "loss: 0.607106  [ 2432/ 3200]\n",
      "loss: 0.666006  [ 2560/ 3200]\n",
      "loss: 0.577119  [ 2688/ 3200]\n",
      "loss: 0.543551  [ 2816/ 3200]\n",
      "loss: 0.756262  [ 2944/ 3200]\n",
      "loss: 0.754251  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038128\n",
      "f1 macro averaged score: 0.749479\n",
      "Accuracy               : 74.9%\n",
      "Confusion matrix       :\n",
      "tensor([[157,  33,   1,   9],\n",
      "        [ 18, 120,  32,  30],\n",
      "        [  0,  23, 167,  10],\n",
      "        [  3,  26,  16, 155]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.9443e-04.\n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------\n",
      "loss: 0.591937  [    0/ 3200]\n",
      "loss: 0.607297  [  128/ 3200]\n",
      "loss: 0.639548  [  256/ 3200]\n",
      "loss: 0.540469  [  384/ 3200]\n",
      "loss: 0.498811  [  512/ 3200]\n",
      "loss: 0.532422  [  640/ 3200]\n",
      "loss: 0.643368  [  768/ 3200]\n",
      "loss: 0.514058  [  896/ 3200]\n",
      "loss: 0.560094  [ 1024/ 3200]\n",
      "loss: 0.694210  [ 1152/ 3200]\n",
      "loss: 0.613399  [ 1280/ 3200]\n",
      "loss: 0.567204  [ 1408/ 3200]\n",
      "loss: 0.544373  [ 1536/ 3200]\n",
      "loss: 0.542804  [ 1664/ 3200]\n",
      "loss: 0.509178  [ 1792/ 3200]\n",
      "loss: 0.516844  [ 1920/ 3200]\n",
      "loss: 0.603762  [ 2048/ 3200]\n",
      "loss: 0.481204  [ 2176/ 3200]\n",
      "loss: 0.629065  [ 2304/ 3200]\n",
      "loss: 0.555383  [ 2432/ 3200]\n",
      "loss: 0.602441  [ 2560/ 3200]\n",
      "loss: 0.491372  [ 2688/ 3200]\n",
      "loss: 0.466758  [ 2816/ 3200]\n",
      "loss: 0.469096  [ 2944/ 3200]\n",
      "loss: 0.526369  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036477\n",
      "f1 macro averaged score: 0.759124\n",
      "Accuracy               : 76.1%\n",
      "Confusion matrix       :\n",
      "tensor([[168,  22,   1,   9],\n",
      "        [ 23, 116,  35,  26],\n",
      "        [  0,  19, 171,  10],\n",
      "        [  4,  25,  17, 154]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.5471e-04.\n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------\n",
      "loss: 0.474235  [    0/ 3200]\n",
      "loss: 0.652839  [  128/ 3200]\n",
      "loss: 0.656952  [  256/ 3200]\n",
      "loss: 0.573835  [  384/ 3200]\n",
      "loss: 0.547370  [  512/ 3200]\n",
      "loss: 0.595910  [  640/ 3200]\n",
      "loss: 0.507438  [  768/ 3200]\n",
      "loss: 0.508752  [  896/ 3200]\n",
      "loss: 0.522668  [ 1024/ 3200]\n",
      "loss: 0.618092  [ 1152/ 3200]\n",
      "loss: 0.530169  [ 1280/ 3200]\n",
      "loss: 0.725808  [ 1408/ 3200]\n",
      "loss: 0.506820  [ 1536/ 3200]\n",
      "loss: 0.417111  [ 1664/ 3200]\n",
      "loss: 0.492360  [ 1792/ 3200]\n",
      "loss: 0.561220  [ 1920/ 3200]\n",
      "loss: 0.521461  [ 2048/ 3200]\n",
      "loss: 0.540004  [ 2176/ 3200]\n",
      "loss: 0.572630  [ 2304/ 3200]\n",
      "loss: 0.509719  [ 2432/ 3200]\n",
      "loss: 0.550356  [ 2560/ 3200]\n",
      "loss: 0.432179  [ 2688/ 3200]\n",
      "loss: 0.537778  [ 2816/ 3200]\n",
      "loss: 0.506358  [ 2944/ 3200]\n",
      "loss: 0.481732  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035142\n",
      "f1 macro averaged score: 0.763160\n",
      "Accuracy               : 76.9%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  13,   1,  11],\n",
      "        [ 23, 107,  36,  34],\n",
      "        [  0,  15, 170,  15],\n",
      "        [  5,  16,  16, 163]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 7.1697e-04.\n",
      "\n",
      "Epoch: 21\n",
      "-----------------------------\n",
      "loss: 0.492150  [    0/ 3200]\n",
      "loss: 0.538013  [  128/ 3200]\n",
      "loss: 0.590806  [  256/ 3200]\n",
      "loss: 0.487246  [  384/ 3200]\n",
      "loss: 0.522009  [  512/ 3200]\n",
      "loss: 0.581949  [  640/ 3200]\n",
      "loss: 0.669480  [  768/ 3200]\n",
      "loss: 0.740944  [  896/ 3200]\n",
      "loss: 0.541077  [ 1024/ 3200]\n",
      "loss: 0.514603  [ 1152/ 3200]\n",
      "loss: 0.508014  [ 1280/ 3200]\n",
      "loss: 0.595764  [ 1408/ 3200]\n",
      "loss: 0.511131  [ 1536/ 3200]\n",
      "loss: 0.461657  [ 1664/ 3200]\n",
      "loss: 0.554657  [ 1792/ 3200]\n",
      "loss: 0.570932  [ 1920/ 3200]\n",
      "loss: 0.611375  [ 2048/ 3200]\n",
      "loss: 0.449406  [ 2176/ 3200]\n",
      "loss: 0.590511  [ 2304/ 3200]\n",
      "loss: 0.500542  [ 2432/ 3200]\n",
      "loss: 0.533665  [ 2560/ 3200]\n",
      "loss: 0.569661  [ 2688/ 3200]\n",
      "loss: 0.566214  [ 2816/ 3200]\n",
      "loss: 0.565676  [ 2944/ 3200]\n",
      "loss: 0.518784  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.037131\n",
      "f1 macro averaged score: 0.732422\n",
      "Accuracy               : 74.8%\n",
      "Confusion matrix       :\n",
      "tensor([[187,   4,   0,   9],\n",
      "        [ 39,  81,  26,  54],\n",
      "        [  0,  17, 164,  19],\n",
      "        [ 11,   9,  14, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.8112e-04.\n",
      "\n",
      "Epoch: 22\n",
      "-----------------------------\n",
      "loss: 0.512479  [    0/ 3200]\n",
      "loss: 0.553146  [  128/ 3200]\n",
      "loss: 0.719621  [  256/ 3200]\n",
      "loss: 0.517877  [  384/ 3200]\n",
      "loss: 0.336334  [  512/ 3200]\n",
      "loss: 0.476249  [  640/ 3200]\n",
      "loss: 0.389415  [  768/ 3200]\n",
      "loss: 0.581272  [  896/ 3200]\n",
      "loss: 0.553849  [ 1024/ 3200]\n",
      "loss: 0.596778  [ 1152/ 3200]\n",
      "loss: 0.377553  [ 1280/ 3200]\n",
      "loss: 0.511873  [ 1408/ 3200]\n",
      "loss: 0.650884  [ 1536/ 3200]\n",
      "loss: 0.445648  [ 1664/ 3200]\n",
      "loss: 0.517153  [ 1792/ 3200]\n",
      "loss: 0.530390  [ 1920/ 3200]\n",
      "loss: 0.611040  [ 2048/ 3200]\n",
      "loss: 0.531292  [ 2176/ 3200]\n",
      "loss: 0.583279  [ 2304/ 3200]\n",
      "loss: 0.440803  [ 2432/ 3200]\n",
      "loss: 0.623964  [ 2560/ 3200]\n",
      "loss: 0.441331  [ 2688/ 3200]\n",
      "loss: 0.620384  [ 2816/ 3200]\n",
      "loss: 0.517056  [ 2944/ 3200]\n",
      "loss: 0.626698  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036841\n",
      "f1 macro averaged score: 0.764225\n",
      "Accuracy               : 76.8%\n",
      "Confusion matrix       :\n",
      "tensor([[166,  21,   1,  12],\n",
      "        [ 19, 113,  43,  25],\n",
      "        [  0,  16, 176,   8],\n",
      "        [  4,  20,  17, 159]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.4707e-04.\n",
      "\n",
      "Epoch: 23\n",
      "-----------------------------\n",
      "loss: 0.530781  [    0/ 3200]\n",
      "loss: 0.526635  [  128/ 3200]\n",
      "loss: 0.508856  [  256/ 3200]\n",
      "loss: 0.545668  [  384/ 3200]\n",
      "loss: 0.452336  [  512/ 3200]\n",
      "loss: 0.598181  [  640/ 3200]\n",
      "loss: 0.590512  [  768/ 3200]\n",
      "loss: 0.581153  [  896/ 3200]\n",
      "loss: 0.582372  [ 1024/ 3200]\n",
      "loss: 0.448842  [ 1152/ 3200]\n",
      "loss: 0.429323  [ 1280/ 3200]\n",
      "loss: 0.587447  [ 1408/ 3200]\n",
      "loss: 0.478664  [ 1536/ 3200]\n",
      "loss: 0.567080  [ 1664/ 3200]\n",
      "loss: 0.549054  [ 1792/ 3200]\n",
      "loss: 0.573418  [ 1920/ 3200]\n",
      "loss: 0.620633  [ 2048/ 3200]\n",
      "loss: 0.469871  [ 2176/ 3200]\n",
      "loss: 0.586687  [ 2304/ 3200]\n",
      "loss: 0.586892  [ 2432/ 3200]\n",
      "loss: 0.477291  [ 2560/ 3200]\n",
      "loss: 0.440497  [ 2688/ 3200]\n",
      "loss: 0.579105  [ 2816/ 3200]\n",
      "loss: 0.616459  [ 2944/ 3200]\n",
      "loss: 0.459896  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.037333\n",
      "f1 macro averaged score: 0.740626\n",
      "Accuracy               : 75.2%\n",
      "Confusion matrix       :\n",
      "tensor([[185,   5,   0,  10],\n",
      "        [ 37,  88,  24,  51],\n",
      "        [  0,  15, 162,  23],\n",
      "        [  9,  10,  14, 167]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.1471e-04.\n",
      "\n",
      "Epoch: 24\n",
      "-----------------------------\n",
      "loss: 0.486400  [    0/ 3200]\n",
      "loss: 0.441577  [  128/ 3200]\n",
      "loss: 0.530651  [  256/ 3200]\n",
      "loss: 0.465441  [  384/ 3200]\n",
      "loss: 0.634630  [  512/ 3200]\n",
      "loss: 0.493034  [  640/ 3200]\n",
      "loss: 0.592320  [  768/ 3200]\n",
      "loss: 0.495946  [  896/ 3200]\n",
      "loss: 0.610489  [ 1024/ 3200]\n",
      "loss: 0.546767  [ 1152/ 3200]\n",
      "loss: 0.516050  [ 1280/ 3200]\n",
      "loss: 0.568229  [ 1408/ 3200]\n",
      "loss: 0.543031  [ 1536/ 3200]\n",
      "loss: 0.397411  [ 1664/ 3200]\n",
      "loss: 0.529883  [ 1792/ 3200]\n",
      "loss: 0.580239  [ 1920/ 3200]\n",
      "loss: 0.508070  [ 2048/ 3200]\n",
      "loss: 0.538148  [ 2176/ 3200]\n",
      "loss: 0.485315  [ 2304/ 3200]\n",
      "loss: 0.530586  [ 2432/ 3200]\n",
      "loss: 0.521472  [ 2560/ 3200]\n",
      "loss: 0.470108  [ 2688/ 3200]\n",
      "loss: 0.503111  [ 2816/ 3200]\n",
      "loss: 0.574721  [ 2944/ 3200]\n",
      "loss: 0.544208  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035747\n",
      "f1 macro averaged score: 0.771464\n",
      "Accuracy               : 77.6%\n",
      "Confusion matrix       :\n",
      "tensor([[179,  12,   1,   8],\n",
      "        [ 27, 113,  34,  26],\n",
      "        [  0,  17, 172,  11],\n",
      "        [  6,  21,  16, 157]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.8398e-04.\n",
      "\n",
      "Epoch: 25\n",
      "-----------------------------\n",
      "loss: 0.528800  [    0/ 3200]\n",
      "loss: 0.518315  [  128/ 3200]\n",
      "loss: 0.675079  [  256/ 3200]\n",
      "loss: 0.521733  [  384/ 3200]\n",
      "loss: 0.507357  [  512/ 3200]\n",
      "loss: 0.444161  [  640/ 3200]\n",
      "loss: 0.591878  [  768/ 3200]\n",
      "loss: 0.465021  [  896/ 3200]\n",
      "loss: 0.562221  [ 1024/ 3200]\n",
      "loss: 0.526465  [ 1152/ 3200]\n",
      "loss: 0.449154  [ 1280/ 3200]\n",
      "loss: 0.550421  [ 1408/ 3200]\n",
      "loss: 0.388940  [ 1536/ 3200]\n",
      "loss: 0.485899  [ 1664/ 3200]\n",
      "loss: 0.520534  [ 1792/ 3200]\n",
      "loss: 0.619100  [ 1920/ 3200]\n",
      "loss: 0.518945  [ 2048/ 3200]\n",
      "loss: 0.434477  [ 2176/ 3200]\n",
      "loss: 0.531759  [ 2304/ 3200]\n",
      "loss: 0.571065  [ 2432/ 3200]\n",
      "loss: 0.544296  [ 2560/ 3200]\n",
      "loss: 0.523899  [ 2688/ 3200]\n",
      "loss: 0.548847  [ 2816/ 3200]\n",
      "loss: 0.635295  [ 2944/ 3200]\n",
      "loss: 0.449439  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035325\n",
      "f1 macro averaged score: 0.777693\n",
      "Accuracy               : 77.8%\n",
      "Confusion matrix       :\n",
      "tensor([[169,  23,   0,   8],\n",
      "        [ 18, 131,  32,  19],\n",
      "        [  0,  23, 172,   5],\n",
      "        [  7,  27,  16, 150]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.5478e-04.\n",
      "\n",
      "Epoch: 26\n",
      "-----------------------------\n",
      "loss: 0.570796  [    0/ 3200]\n",
      "loss: 0.545841  [  128/ 3200]\n",
      "loss: 0.573266  [  256/ 3200]\n",
      "loss: 0.514920  [  384/ 3200]\n",
      "loss: 0.551754  [  512/ 3200]\n",
      "loss: 0.612928  [  640/ 3200]\n",
      "loss: 0.509598  [  768/ 3200]\n",
      "loss: 0.517931  [  896/ 3200]\n",
      "loss: 0.455059  [ 1024/ 3200]\n",
      "loss: 0.499334  [ 1152/ 3200]\n",
      "loss: 0.440922  [ 1280/ 3200]\n",
      "loss: 0.520331  [ 1408/ 3200]\n",
      "loss: 0.563077  [ 1536/ 3200]\n",
      "loss: 0.577481  [ 1664/ 3200]\n",
      "loss: 0.495840  [ 1792/ 3200]\n",
      "loss: 0.517107  [ 1920/ 3200]\n",
      "loss: 0.523182  [ 2048/ 3200]\n",
      "loss: 0.515503  [ 2176/ 3200]\n",
      "loss: 0.459503  [ 2304/ 3200]\n",
      "loss: 0.416701  [ 2432/ 3200]\n",
      "loss: 0.567022  [ 2560/ 3200]\n",
      "loss: 0.576827  [ 2688/ 3200]\n",
      "loss: 0.488707  [ 2816/ 3200]\n",
      "loss: 0.492636  [ 2944/ 3200]\n",
      "loss: 0.555029  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035192\n",
      "f1 macro averaged score: 0.780202\n",
      "Accuracy               : 78.2%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  14,   1,  11],\n",
      "        [ 22, 124,  30,  24],\n",
      "        [  0,  19, 169,  12],\n",
      "        [  7,  19,  15, 159]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.2704e-04.\n",
      "\n",
      "Epoch: 27\n",
      "-----------------------------\n",
      "loss: 0.494092  [    0/ 3200]\n",
      "loss: 0.472456  [  128/ 3200]\n",
      "loss: 0.516562  [  256/ 3200]\n",
      "loss: 0.643360  [  384/ 3200]\n",
      "loss: 0.450660  [  512/ 3200]\n",
      "loss: 0.516047  [  640/ 3200]\n",
      "loss: 0.496801  [  768/ 3200]\n",
      "loss: 0.454011  [  896/ 3200]\n",
      "loss: 0.518136  [ 1024/ 3200]\n",
      "loss: 0.439888  [ 1152/ 3200]\n",
      "loss: 0.522619  [ 1280/ 3200]\n",
      "loss: 0.542852  [ 1408/ 3200]\n",
      "loss: 0.523491  [ 1536/ 3200]\n",
      "loss: 0.586253  [ 1664/ 3200]\n",
      "loss: 0.520132  [ 1792/ 3200]\n",
      "loss: 0.423809  [ 1920/ 3200]\n",
      "loss: 0.534243  [ 2048/ 3200]\n",
      "loss: 0.561757  [ 2176/ 3200]\n",
      "loss: 0.473868  [ 2304/ 3200]\n",
      "loss: 0.548164  [ 2432/ 3200]\n",
      "loss: 0.586938  [ 2560/ 3200]\n",
      "loss: 0.570915  [ 2688/ 3200]\n",
      "loss: 0.451064  [ 2816/ 3200]\n",
      "loss: 0.549933  [ 2944/ 3200]\n",
      "loss: 0.523701  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034815\n",
      "f1 macro averaged score: 0.767075\n",
      "Accuracy               : 77.2%\n",
      "Confusion matrix       :\n",
      "tensor([[187,   8,   0,   5],\n",
      "        [ 37, 113,  26,  24],\n",
      "        [  0,  21, 168,  11],\n",
      "        [ 10,  25,  15, 150]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 5.0069e-04.\n",
      "\n",
      "Epoch: 28\n",
      "-----------------------------\n",
      "loss: 0.419793  [    0/ 3200]\n",
      "loss: 0.594819  [  128/ 3200]\n",
      "loss: 0.570049  [  256/ 3200]\n",
      "loss: 0.676492  [  384/ 3200]\n",
      "loss: 0.379756  [  512/ 3200]\n",
      "loss: 0.501583  [  640/ 3200]\n",
      "loss: 0.538911  [  768/ 3200]\n",
      "loss: 0.518751  [  896/ 3200]\n",
      "loss: 0.549024  [ 1024/ 3200]\n",
      "loss: 0.457988  [ 1152/ 3200]\n",
      "loss: 0.486198  [ 1280/ 3200]\n",
      "loss: 0.475887  [ 1408/ 3200]\n",
      "loss: 0.464930  [ 1536/ 3200]\n",
      "loss: 0.495399  [ 1664/ 3200]\n",
      "loss: 0.455818  [ 1792/ 3200]\n",
      "loss: 0.551194  [ 1920/ 3200]\n",
      "loss: 0.443440  [ 2048/ 3200]\n",
      "loss: 0.582263  [ 2176/ 3200]\n",
      "loss: 0.592010  [ 2304/ 3200]\n",
      "loss: 0.553053  [ 2432/ 3200]\n",
      "loss: 0.583336  [ 2560/ 3200]\n",
      "loss: 0.485719  [ 2688/ 3200]\n",
      "loss: 0.477841  [ 2816/ 3200]\n",
      "loss: 0.514807  [ 2944/ 3200]\n",
      "loss: 0.457787  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035336\n",
      "f1 macro averaged score: 0.777414\n",
      "Accuracy               : 78.0%\n",
      "Confusion matrix       :\n",
      "tensor([[169,  15,   1,  15],\n",
      "        [ 19, 120,  32,  29],\n",
      "        [  0,  17, 171,  12],\n",
      "        [  4,  16,  16, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.7565e-04.\n",
      "\n",
      "Epoch: 29\n",
      "-----------------------------\n",
      "loss: 0.582388  [    0/ 3200]\n",
      "loss: 0.514482  [  128/ 3200]\n",
      "loss: 0.445968  [  256/ 3200]\n",
      "loss: 0.431928  [  384/ 3200]\n",
      "loss: 0.483195  [  512/ 3200]\n",
      "loss: 0.638423  [  640/ 3200]\n",
      "loss: 0.467593  [  768/ 3200]\n",
      "loss: 0.489363  [  896/ 3200]\n",
      "loss: 0.541057  [ 1024/ 3200]\n",
      "loss: 0.452869  [ 1152/ 3200]\n",
      "loss: 0.463670  [ 1280/ 3200]\n",
      "loss: 0.525089  [ 1408/ 3200]\n",
      "loss: 0.572198  [ 1536/ 3200]\n",
      "loss: 0.564233  [ 1664/ 3200]\n",
      "loss: 0.604701  [ 1792/ 3200]\n",
      "loss: 0.514424  [ 1920/ 3200]\n",
      "loss: 0.505815  [ 2048/ 3200]\n",
      "loss: 0.435102  [ 2176/ 3200]\n",
      "loss: 0.513999  [ 2304/ 3200]\n",
      "loss: 0.445645  [ 2432/ 3200]\n",
      "loss: 0.465574  [ 2560/ 3200]\n",
      "loss: 0.461205  [ 2688/ 3200]\n",
      "loss: 0.428167  [ 2816/ 3200]\n",
      "loss: 0.570053  [ 2944/ 3200]\n",
      "loss: 0.571068  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034060\n",
      "f1 macro averaged score: 0.781813\n",
      "Accuracy               : 78.5%\n",
      "Confusion matrix       :\n",
      "tensor([[181,  12,   0,   7],\n",
      "        [ 24, 122,  30,  24],\n",
      "        [  0,  19, 170,  11],\n",
      "        [  7,  21,  17, 155]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.5187e-04.\n",
      "\n",
      "Epoch: 30\n",
      "-----------------------------\n",
      "loss: 0.484265  [    0/ 3200]\n",
      "loss: 0.516069  [  128/ 3200]\n",
      "loss: 0.389598  [  256/ 3200]\n",
      "loss: 0.491364  [  384/ 3200]\n",
      "loss: 0.457027  [  512/ 3200]\n",
      "loss: 0.511772  [  640/ 3200]\n",
      "loss: 0.512961  [  768/ 3200]\n",
      "loss: 0.590088  [  896/ 3200]\n",
      "loss: 0.527996  [ 1024/ 3200]\n",
      "loss: 0.425852  [ 1152/ 3200]\n",
      "loss: 0.466074  [ 1280/ 3200]\n",
      "loss: 0.436519  [ 1408/ 3200]\n",
      "loss: 0.526594  [ 1536/ 3200]\n",
      "loss: 0.588882  [ 1664/ 3200]\n",
      "loss: 0.460679  [ 1792/ 3200]\n",
      "loss: 0.631657  [ 1920/ 3200]\n",
      "loss: 0.532761  [ 2048/ 3200]\n",
      "loss: 0.598045  [ 2176/ 3200]\n",
      "loss: 0.480183  [ 2304/ 3200]\n",
      "loss: 0.449713  [ 2432/ 3200]\n",
      "loss: 0.431173  [ 2560/ 3200]\n",
      "loss: 0.605909  [ 2688/ 3200]\n",
      "loss: 0.481898  [ 2816/ 3200]\n",
      "loss: 0.531300  [ 2944/ 3200]\n",
      "loss: 0.452243  [ 3072/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034935\n",
      "f1 macro averaged score: 0.770897\n",
      "Accuracy               : 77.8%\n",
      "Confusion matrix       :\n",
      "tensor([[185,   7,   0,   8],\n",
      "        [ 32, 108,  25,  35],\n",
      "        [  0,  18, 167,  15],\n",
      "        [  8,  16,  14, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.2928e-04.\n",
      "\n",
      "Best epoch: 29 with f1 macro averaged score: 0.7818127870559692\n",
      "Test Error:\n",
      "Avg loss               : 0.038926\n",
      "f1 macro averaged score: 0.747524\n",
      "Accuracy               : 74.8%\n",
      "Confusion matrix       :\n",
      "tensor([[285,   7,   1,   4],\n",
      "        [ 17, 174,  55,  78],\n",
      "        [  3,  49, 280,  24],\n",
      "        [ 18,  59,  32, 290]], device='cuda:0')\n",
      "CPU times: user 7min 47s, sys: 10.8 s, total: 7min 58s\n",
      "Wall time: 8min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size_list = [2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "epochs = 30\n",
    "learning_rate_0 = 0.002\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "f1_accuracy = []\n",
    "for batch_size in batch_size_list:\n",
    "  torch_seed(0)\n",
    "  cnn_model = Net(ELU, 0.1).to(device)\n",
    "  optimizer = Adagrad(params=cnn_model.parameters(), lr=learning_rate_0, weight_decay=0.0001)\n",
    "  scheduler = MultiplicativeLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95, verbose=True)\n",
    "\n",
    "  print(\"  Batch size:\", batch_size)\n",
    "  train_dataloader = DataLoader(list(zip(melgram_x_train, melgram_y_train)), batch_size=batch_size, shuffle=True, generator=torch.Generator(device='cpu'))\n",
    "  train_dataloader.set_epoch = set_epoch\n",
    "\n",
    "  t_0 = timer()\n",
    "  best_model, f1_per_epoch = validate_convolutional_neural_network(\n",
    "      epochs, optimizer, train_dataloader, val_dataloader, loss_function, cnn_model, True, scheduler\n",
    "      )\n",
    "  t_1 = timer()\n",
    "  results = test_convolutional_neural_network(test_dataloader, loss_function, best_model)\n",
    "  t_2 = timer()\n",
    "  validation_time = t_1 - t_0\n",
    "  test_time = t_2 - t_1\n",
    "  f1_accuracy.append((batch_size, results[1], results[2], validation_time, test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCygeGRbzUTJ",
    "outputId": "14838d15-2904-4966-c735-fd9f7860a218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 epochs\n",
      "Batch size\tf1_macro_avg\tAccuracy\tVal time (s)\tTest time (s)\n",
      "16\t\t0.793261\t78.997093\t32.784670\t0.131117\n",
      "32\t\t0.790501\t78.997093\t19.710497\t0.170993\n",
      "2\t\t0.779132\t77.398256\t229.187439\t0.141961\n",
      "64\t\t0.769001\t77.180233\t14.809057\t0.172510\n",
      "8\t\t0.764977\t76.308140\t61.329892\t0.166907\n",
      "4\t\t0.761819\t75.799419\t115.645495\t0.129809\n",
      "128\t\t0.747524\t74.781977\t13.736584\t0.180882\n"
     ]
    }
   ],
   "source": [
    "f1_accuracy = sorted(f1_accuracy, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"30 epochs\")\n",
    "print(\"Batch size\\tf1_macro_avg\\tAccuracy\\tVal time (s)\\tTest time (s)\")\n",
    "for (batch_size, f1_macro_avg, accuracy, validation_time, test_time) in f1_accuracy:\n",
    "  print(f\"{batch_size}\\t\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\\t{validation_time:>2f}\\t{test_time:>2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0konQjz50mV"
   },
   "source": [
    "From the table shown above, what we can observe is that a larger batch size reduces training-validation time.\n",
    "\n",
    "There is no relation between batch size and accuracy for our dataset, because getting a larger batch size may lead to both better or worse accuracy.\n",
    "<br></br>\n",
    "\n",
    "In general, a larger batch size reduces the model's ability to generalize, because it tends to find minima with much more sharpness. [[4]](#reference)\n",
    "<br></br>\n",
    "\n",
    "Batch sizes $16$ and $32$ yield the same accuracy, but using batch size $32$ makes computations much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G319hQ4gzwcd"
   },
   "source": [
    "+ Early stopping\n",
    "\n",
    "Redefine the validation process, so that the Convolutional Neural Network stops being trained after not increasing its f1 score for a number of consecutive epochs (patience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "kJo2lUEA2X0-"
   },
   "outputs": [],
   "source": [
    "def validate_convolutional_neural_network(epochs, optimizer, train_dataloader, val_dataloader, loss_function, model, reproducibility, scheduler, patience=7):\n",
    "  size = len(train_dataloader.dataset)\n",
    "\n",
    "  best = (0, 0, 0) # best model, best epoch, best f1 score\n",
    "  f1_list = []\n",
    "  for epoch in range(0, epochs):\n",
    "    # train model with train data\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"-----------------------------\")\n",
    "    if reproducibility:\n",
    "      train_dataloader.set_epoch(train_dataloader, epoch)\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # calculate prediction and loss\n",
    "      prediction = model(torch.unsqueeze(x, 1))\n",
    "      loss = loss_function(prediction, y)\n",
    "\n",
    "      # backpropagation\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss, current = loss.item(), batch * len(x)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # test model on validation set\n",
    "    results = test_convolutional_neural_network(val_dataloader, loss_function, model)\n",
    "    f1_macro_avg = results[1]\n",
    "    f1_list.append(f1_macro_avg)\n",
    "    if f1_macro_avg > best[2]:\n",
    "      best = (model, epoch, f1_macro_avg)\n",
    "    else:\n",
    "      patience -= 1\n",
    "      if patience == 0:\n",
    "        return best[0], f1_list, epoch\n",
    "\n",
    "    # adjust learning rate\n",
    "    scheduler.step()\n",
    "    print()\n",
    "\n",
    "  print(f\"Best epoch: {(best[1] + 1)} with f1 macro averaged score: {best[2]}\")\n",
    "  return best[0], f1_list, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyBXrxCUmJ_O"
   },
   "source": [
    "We validate our Convolutional Neural Network for $30$ epochs and test it.\n",
    "\n",
    "Our model uses:\n",
    "+ the Adagrad optimizer\n",
    "+ the ELU activation function\n",
    "+ the MultiplicativeLR scheduler\n",
    "+ batch normalization\n",
    "+ weight decay $0.0001$\n",
    "+ dropout $0.1$\n",
    "+ batch size $16$\n",
    "\n",
    "as stated in the previous steps.\n",
    "\n",
    "The batch sizes used are the following: $2$, $4$, $8$, $16$, $32$, $64$ and $128$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COLgdUYu36Ct",
    "outputId": "109b122e-88a0-4184-aaf2-f4035de6bc59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
      "loss: 0.313746  [ 2048/ 3200]\n",
      "loss: 0.265838  [ 2064/ 3200]\n",
      "loss: 0.150261  [ 2080/ 3200]\n",
      "loss: 0.461560  [ 2096/ 3200]\n",
      "loss: 0.267711  [ 2112/ 3200]\n",
      "loss: 0.385817  [ 2128/ 3200]\n",
      "loss: 0.586709  [ 2144/ 3200]\n",
      "loss: 0.249289  [ 2160/ 3200]\n",
      "loss: 0.312473  [ 2176/ 3200]\n",
      "loss: 0.235886  [ 2192/ 3200]\n",
      "loss: 0.408655  [ 2208/ 3200]\n",
      "loss: 0.213867  [ 2224/ 3200]\n",
      "loss: 0.485417  [ 2240/ 3200]\n",
      "loss: 0.793733  [ 2256/ 3200]\n",
      "loss: 1.178935  [ 2272/ 3200]\n",
      "loss: 0.546151  [ 2288/ 3200]\n",
      "loss: 0.318037  [ 2304/ 3200]\n",
      "loss: 0.498426  [ 2320/ 3200]\n",
      "loss: 0.411667  [ 2336/ 3200]\n",
      "loss: 0.450565  [ 2352/ 3200]\n",
      "loss: 0.426429  [ 2368/ 3200]\n",
      "loss: 0.288943  [ 2384/ 3200]\n",
      "loss: 0.829687  [ 2400/ 3200]\n",
      "loss: 0.419583  [ 2416/ 3200]\n",
      "loss: 0.264257  [ 2432/ 3200]\n",
      "loss: 0.414830  [ 2448/ 3200]\n",
      "loss: 0.228314  [ 2464/ 3200]\n",
      "loss: 0.417509  [ 2480/ 3200]\n",
      "loss: 0.411148  [ 2496/ 3200]\n",
      "loss: 0.385890  [ 2512/ 3200]\n",
      "loss: 0.342760  [ 2528/ 3200]\n",
      "loss: 0.317699  [ 2544/ 3200]\n",
      "loss: 0.552306  [ 2560/ 3200]\n",
      "loss: 0.829515  [ 2576/ 3200]\n",
      "loss: 0.577318  [ 2592/ 3200]\n",
      "loss: 0.232861  [ 2608/ 3200]\n",
      "loss: 0.441497  [ 2624/ 3200]\n",
      "loss: 0.149651  [ 2640/ 3200]\n",
      "loss: 0.518892  [ 2656/ 3200]\n",
      "loss: 0.305096  [ 2672/ 3200]\n",
      "loss: 0.504217  [ 2688/ 3200]\n",
      "loss: 0.359586  [ 2704/ 3200]\n",
      "loss: 0.203213  [ 2720/ 3200]\n",
      "loss: 0.285921  [ 2736/ 3200]\n",
      "loss: 0.581766  [ 2752/ 3200]\n",
      "loss: 0.636225  [ 2768/ 3200]\n",
      "loss: 0.147685  [ 2784/ 3200]\n",
      "loss: 0.487472  [ 2800/ 3200]\n",
      "loss: 0.290951  [ 2816/ 3200]\n",
      "loss: 0.203815  [ 2832/ 3200]\n",
      "loss: 0.278253  [ 2848/ 3200]\n",
      "loss: 0.943583  [ 2864/ 3200]\n",
      "loss: 0.889826  [ 2880/ 3200]\n",
      "loss: 0.247941  [ 2896/ 3200]\n",
      "loss: 0.429018  [ 2912/ 3200]\n",
      "loss: 0.455302  [ 2928/ 3200]\n",
      "loss: 0.148181  [ 2944/ 3200]\n",
      "loss: 0.327788  [ 2960/ 3200]\n",
      "loss: 0.522843  [ 2976/ 3200]\n",
      "loss: 0.283933  [ 2992/ 3200]\n",
      "loss: 0.688349  [ 3008/ 3200]\n",
      "loss: 0.848309  [ 3024/ 3200]\n",
      "loss: 0.689017  [ 3040/ 3200]\n",
      "loss: 0.494857  [ 3056/ 3200]\n",
      "loss: 0.505716  [ 3072/ 3200]\n",
      "loss: 0.313143  [ 3088/ 3200]\n",
      "loss: 0.091239  [ 3104/ 3200]\n",
      "loss: 0.526228  [ 3120/ 3200]\n",
      "loss: 0.349981  [ 3136/ 3200]\n",
      "loss: 0.411142  [ 3152/ 3200]\n",
      "loss: 0.403561  [ 3168/ 3200]\n",
      "loss: 0.463384  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034550\n",
      "f1 macro averaged score: 0.801111\n",
      "Accuracy               : 80.0%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  22,   0,   3],\n",
      "        [ 12, 142,  28,  18],\n",
      "        [  0,  23, 171,   6],\n",
      "        [  6,  26,  16, 152]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3967e-03.\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 0.722626  [    0/ 3200]\n",
      "loss: 0.345807  [   16/ 3200]\n",
      "loss: 0.405905  [   32/ 3200]\n",
      "loss: 0.382185  [   48/ 3200]\n",
      "loss: 0.292689  [   64/ 3200]\n",
      "loss: 0.615524  [   80/ 3200]\n",
      "loss: 0.332033  [   96/ 3200]\n",
      "loss: 0.233436  [  112/ 3200]\n",
      "loss: 0.340839  [  128/ 3200]\n",
      "loss: 0.537426  [  144/ 3200]\n",
      "loss: 0.231684  [  160/ 3200]\n",
      "loss: 0.364387  [  176/ 3200]\n",
      "loss: 0.668983  [  192/ 3200]\n",
      "loss: 0.401259  [  208/ 3200]\n",
      "loss: 0.333334  [  224/ 3200]\n",
      "loss: 0.265595  [  240/ 3200]\n",
      "loss: 0.744242  [  256/ 3200]\n",
      "loss: 0.315843  [  272/ 3200]\n",
      "loss: 0.351039  [  288/ 3200]\n",
      "loss: 0.253337  [  304/ 3200]\n",
      "loss: 0.756704  [  320/ 3200]\n",
      "loss: 0.358875  [  336/ 3200]\n",
      "loss: 0.667139  [  352/ 3200]\n",
      "loss: 0.318722  [  368/ 3200]\n",
      "loss: 0.362918  [  384/ 3200]\n",
      "loss: 0.653803  [  400/ 3200]\n",
      "loss: 0.347193  [  416/ 3200]\n",
      "loss: 0.806887  [  432/ 3200]\n",
      "loss: 0.364402  [  448/ 3200]\n",
      "loss: 0.480157  [  464/ 3200]\n",
      "loss: 0.190588  [  480/ 3200]\n",
      "loss: 0.257062  [  496/ 3200]\n",
      "loss: 0.695572  [  512/ 3200]\n",
      "loss: 0.329582  [  528/ 3200]\n",
      "loss: 0.814550  [  544/ 3200]\n",
      "loss: 0.578294  [  560/ 3200]\n",
      "loss: 0.474079  [  576/ 3200]\n",
      "loss: 0.333563  [  592/ 3200]\n",
      "loss: 0.309042  [  608/ 3200]\n",
      "loss: 0.243152  [  624/ 3200]\n",
      "loss: 0.297518  [  640/ 3200]\n",
      "loss: 0.713605  [  656/ 3200]\n",
      "loss: 0.477687  [  672/ 3200]\n",
      "loss: 0.429909  [  688/ 3200]\n",
      "loss: 0.309330  [  704/ 3200]\n",
      "loss: 0.382032  [  720/ 3200]\n",
      "loss: 0.224526  [  736/ 3200]\n",
      "loss: 0.387950  [  752/ 3200]\n",
      "loss: 0.461846  [  768/ 3200]\n",
      "loss: 0.538864  [  784/ 3200]\n",
      "loss: 0.211462  [  800/ 3200]\n",
      "loss: 0.481722  [  816/ 3200]\n",
      "loss: 0.231272  [  832/ 3200]\n",
      "loss: 0.253180  [  848/ 3200]\n",
      "loss: 0.678171  [  864/ 3200]\n",
      "loss: 0.417572  [  880/ 3200]\n",
      "loss: 0.294559  [  896/ 3200]\n",
      "loss: 0.166171  [  912/ 3200]\n",
      "loss: 0.594097  [  928/ 3200]\n",
      "loss: 0.326055  [  944/ 3200]\n",
      "loss: 0.371613  [  960/ 3200]\n",
      "loss: 0.230570  [  976/ 3200]\n",
      "loss: 0.268375  [  992/ 3200]\n",
      "loss: 0.472625  [ 1008/ 3200]\n",
      "loss: 0.699860  [ 1024/ 3200]\n",
      "loss: 0.488638  [ 1040/ 3200]\n",
      "loss: 0.293000  [ 1056/ 3200]\n",
      "loss: 0.500300  [ 1072/ 3200]\n",
      "loss: 0.422985  [ 1088/ 3200]\n",
      "loss: 0.610486  [ 1104/ 3200]\n",
      "loss: 0.156949  [ 1120/ 3200]\n",
      "loss: 0.231346  [ 1136/ 3200]\n",
      "loss: 0.358521  [ 1152/ 3200]\n",
      "loss: 0.634192  [ 1168/ 3200]\n",
      "loss: 0.572526  [ 1184/ 3200]\n",
      "loss: 0.524115  [ 1200/ 3200]\n",
      "loss: 0.464204  [ 1216/ 3200]\n",
      "loss: 0.182070  [ 1232/ 3200]\n",
      "loss: 0.285385  [ 1248/ 3200]\n",
      "loss: 0.245276  [ 1264/ 3200]\n",
      "loss: 0.489475  [ 1280/ 3200]\n",
      "loss: 0.342785  [ 1296/ 3200]\n",
      "loss: 0.395709  [ 1312/ 3200]\n",
      "loss: 0.405147  [ 1328/ 3200]\n",
      "loss: 0.312548  [ 1344/ 3200]\n",
      "loss: 0.271835  [ 1360/ 3200]\n",
      "loss: 0.335847  [ 1376/ 3200]\n",
      "loss: 0.210596  [ 1392/ 3200]\n",
      "loss: 0.465798  [ 1408/ 3200]\n",
      "loss: 0.513131  [ 1424/ 3200]\n",
      "loss: 0.694038  [ 1440/ 3200]\n",
      "loss: 0.482798  [ 1456/ 3200]\n",
      "loss: 0.121615  [ 1472/ 3200]\n",
      "loss: 0.304815  [ 1488/ 3200]\n",
      "loss: 0.231532  [ 1504/ 3200]\n",
      "loss: 0.627718  [ 1520/ 3200]\n",
      "loss: 0.577494  [ 1536/ 3200]\n",
      "loss: 0.329413  [ 1552/ 3200]\n",
      "loss: 0.148028  [ 1568/ 3200]\n",
      "loss: 0.409571  [ 1584/ 3200]\n",
      "loss: 0.568524  [ 1600/ 3200]\n",
      "loss: 0.176837  [ 1616/ 3200]\n",
      "loss: 0.196965  [ 1632/ 3200]\n",
      "loss: 0.277437  [ 1648/ 3200]\n",
      "loss: 0.875362  [ 1664/ 3200]\n",
      "loss: 0.450626  [ 1680/ 3200]\n",
      "loss: 0.695380  [ 1696/ 3200]\n",
      "loss: 0.297884  [ 1712/ 3200]\n",
      "loss: 0.248667  [ 1728/ 3200]\n",
      "loss: 0.147117  [ 1744/ 3200]\n",
      "loss: 0.226490  [ 1760/ 3200]\n",
      "loss: 0.605389  [ 1776/ 3200]\n",
      "loss: 0.276510  [ 1792/ 3200]\n",
      "loss: 0.326850  [ 1808/ 3200]\n",
      "loss: 0.546987  [ 1824/ 3200]\n",
      "loss: 0.324208  [ 1840/ 3200]\n",
      "loss: 0.448353  [ 1856/ 3200]\n",
      "loss: 0.344595  [ 1872/ 3200]\n",
      "loss: 0.222335  [ 1888/ 3200]\n",
      "loss: 0.349382  [ 1904/ 3200]\n",
      "loss: 0.230836  [ 1920/ 3200]\n",
      "loss: 0.375062  [ 1936/ 3200]\n",
      "loss: 0.167615  [ 1952/ 3200]\n",
      "loss: 0.190974  [ 1968/ 3200]\n",
      "loss: 0.315448  [ 1984/ 3200]\n",
      "loss: 0.267881  [ 2000/ 3200]\n",
      "loss: 0.423506  [ 2016/ 3200]\n",
      "loss: 0.423203  [ 2032/ 3200]\n",
      "loss: 0.609608  [ 2048/ 3200]\n",
      "loss: 0.298387  [ 2064/ 3200]\n",
      "loss: 0.449180  [ 2080/ 3200]\n",
      "loss: 0.693562  [ 2096/ 3200]\n",
      "loss: 0.436794  [ 2112/ 3200]\n",
      "loss: 0.550257  [ 2128/ 3200]\n",
      "loss: 0.543186  [ 2144/ 3200]\n",
      "loss: 0.696054  [ 2160/ 3200]\n",
      "loss: 0.272787  [ 2176/ 3200]\n",
      "loss: 0.551335  [ 2192/ 3200]\n",
      "loss: 0.223390  [ 2208/ 3200]\n",
      "loss: 0.441858  [ 2224/ 3200]\n",
      "loss: 0.503308  [ 2240/ 3200]\n",
      "loss: 0.260238  [ 2256/ 3200]\n",
      "loss: 0.401740  [ 2272/ 3200]\n",
      "loss: 0.696788  [ 2288/ 3200]\n",
      "loss: 0.435091  [ 2304/ 3200]\n",
      "loss: 0.500743  [ 2320/ 3200]\n",
      "loss: 0.553019  [ 2336/ 3200]\n",
      "loss: 0.415142  [ 2352/ 3200]\n",
      "loss: 0.577425  [ 2368/ 3200]\n",
      "loss: 0.418053  [ 2384/ 3200]\n",
      "loss: 0.400122  [ 2400/ 3200]\n",
      "loss: 0.287996  [ 2416/ 3200]\n",
      "loss: 0.790627  [ 2432/ 3200]\n",
      "loss: 0.425696  [ 2448/ 3200]\n",
      "loss: 0.557977  [ 2464/ 3200]\n",
      "loss: 0.218739  [ 2480/ 3200]\n",
      "loss: 0.185185  [ 2496/ 3200]\n",
      "loss: 0.507042  [ 2512/ 3200]\n",
      "loss: 0.228793  [ 2528/ 3200]\n",
      "loss: 0.346087  [ 2544/ 3200]\n",
      "loss: 0.215706  [ 2560/ 3200]\n",
      "loss: 0.496995  [ 2576/ 3200]\n",
      "loss: 0.329096  [ 2592/ 3200]\n",
      "loss: 0.460788  [ 2608/ 3200]\n",
      "loss: 0.296008  [ 2624/ 3200]\n",
      "loss: 0.331492  [ 2640/ 3200]\n",
      "loss: 0.210430  [ 2656/ 3200]\n",
      "loss: 0.246874  [ 2672/ 3200]\n",
      "loss: 0.417587  [ 2688/ 3200]\n",
      "loss: 0.568795  [ 2704/ 3200]\n",
      "loss: 0.758683  [ 2720/ 3200]\n",
      "loss: 0.209591  [ 2736/ 3200]\n",
      "loss: 0.293696  [ 2752/ 3200]\n",
      "loss: 0.221161  [ 2768/ 3200]\n",
      "loss: 0.383369  [ 2784/ 3200]\n",
      "loss: 0.135280  [ 2800/ 3200]\n",
      "loss: 0.389327  [ 2816/ 3200]\n",
      "loss: 0.304076  [ 2832/ 3200]\n",
      "loss: 0.788973  [ 2848/ 3200]\n",
      "loss: 0.199706  [ 2864/ 3200]\n",
      "loss: 0.564730  [ 2880/ 3200]\n",
      "loss: 0.397487  [ 2896/ 3200]\n",
      "loss: 0.724751  [ 2912/ 3200]\n",
      "loss: 0.509243  [ 2928/ 3200]\n",
      "loss: 0.457347  [ 2944/ 3200]\n",
      "loss: 0.520009  [ 2960/ 3200]\n",
      "loss: 0.237501  [ 2976/ 3200]\n",
      "loss: 0.265768  [ 2992/ 3200]\n",
      "loss: 0.224200  [ 3008/ 3200]\n",
      "loss: 0.084592  [ 3024/ 3200]\n",
      "loss: 0.946365  [ 3040/ 3200]\n",
      "loss: 0.259146  [ 3056/ 3200]\n",
      "loss: 0.197989  [ 3072/ 3200]\n",
      "loss: 0.100926  [ 3088/ 3200]\n",
      "loss: 0.304528  [ 3104/ 3200]\n",
      "loss: 0.626523  [ 3120/ 3200]\n",
      "loss: 0.321436  [ 3136/ 3200]\n",
      "loss: 0.582522  [ 3152/ 3200]\n",
      "loss: 0.363273  [ 3168/ 3200]\n",
      "loss: 0.436696  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036598\n",
      "f1 macro averaged score: 0.767152\n",
      "Accuracy               : 77.2%\n",
      "Confusion matrix       :\n",
      "tensor([[179,   7,   0,  14],\n",
      "        [ 17, 104,  25,  54],\n",
      "        [  0,  13, 165,  22],\n",
      "        [  8,   8,  14, 170]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3268e-03.\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 0.282086  [    0/ 3200]\n",
      "loss: 0.420427  [   16/ 3200]\n",
      "loss: 0.450545  [   32/ 3200]\n",
      "loss: 0.359270  [   48/ 3200]\n",
      "loss: 0.274293  [   64/ 3200]\n",
      "loss: 0.373022  [   80/ 3200]\n",
      "loss: 0.366253  [   96/ 3200]\n",
      "loss: 0.148968  [  112/ 3200]\n",
      "loss: 0.297991  [  128/ 3200]\n",
      "loss: 0.597421  [  144/ 3200]\n",
      "loss: 0.591509  [  160/ 3200]\n",
      "loss: 0.376630  [  176/ 3200]\n",
      "loss: 0.448975  [  192/ 3200]\n",
      "loss: 0.183361  [  208/ 3200]\n",
      "loss: 0.434079  [  224/ 3200]\n",
      "loss: 0.307609  [  240/ 3200]\n",
      "loss: 0.230578  [  256/ 3200]\n",
      "loss: 0.294995  [  272/ 3200]\n",
      "loss: 0.565902  [  288/ 3200]\n",
      "loss: 0.227452  [  304/ 3200]\n",
      "loss: 0.626587  [  320/ 3200]\n",
      "loss: 0.368665  [  336/ 3200]\n",
      "loss: 0.432608  [  352/ 3200]\n",
      "loss: 0.462458  [  368/ 3200]\n",
      "loss: 0.093482  [  384/ 3200]\n",
      "loss: 0.386988  [  400/ 3200]\n",
      "loss: 0.445908  [  416/ 3200]\n",
      "loss: 0.078382  [  432/ 3200]\n",
      "loss: 0.274780  [  448/ 3200]\n",
      "loss: 0.197692  [  464/ 3200]\n",
      "loss: 0.411595  [  480/ 3200]\n",
      "loss: 0.187335  [  496/ 3200]\n",
      "loss: 0.294994  [  512/ 3200]\n",
      "loss: 0.330018  [  528/ 3200]\n",
      "loss: 0.347933  [  544/ 3200]\n",
      "loss: 0.455340  [  560/ 3200]\n",
      "loss: 0.355590  [  576/ 3200]\n",
      "loss: 0.327525  [  592/ 3200]\n",
      "loss: 0.341575  [  608/ 3200]\n",
      "loss: 0.568943  [  624/ 3200]\n",
      "loss: 0.446090  [  640/ 3200]\n",
      "loss: 0.230266  [  656/ 3200]\n",
      "loss: 0.399971  [  672/ 3200]\n",
      "loss: 0.150004  [  688/ 3200]\n",
      "loss: 0.578657  [  704/ 3200]\n",
      "loss: 0.666262  [  720/ 3200]\n",
      "loss: 0.389547  [  736/ 3200]\n",
      "loss: 0.347071  [  752/ 3200]\n",
      "loss: 0.310134  [  768/ 3200]\n",
      "loss: 0.393648  [  784/ 3200]\n",
      "loss: 0.388479  [  800/ 3200]\n",
      "loss: 0.399557  [  816/ 3200]\n",
      "loss: 0.237430  [  832/ 3200]\n",
      "loss: 0.148910  [  848/ 3200]\n",
      "loss: 0.509575  [  864/ 3200]\n",
      "loss: 0.255856  [  880/ 3200]\n",
      "loss: 0.199251  [  896/ 3200]\n",
      "loss: 0.444741  [  912/ 3200]\n",
      "loss: 0.198809  [  928/ 3200]\n",
      "loss: 0.296737  [  944/ 3200]\n",
      "loss: 0.538546  [  960/ 3200]\n",
      "loss: 0.310363  [  976/ 3200]\n",
      "loss: 0.692516  [  992/ 3200]\n",
      "loss: 0.413360  [ 1008/ 3200]\n",
      "loss: 0.506782  [ 1024/ 3200]\n",
      "loss: 0.429556  [ 1040/ 3200]\n",
      "loss: 0.350586  [ 1056/ 3200]\n",
      "loss: 0.585835  [ 1072/ 3200]\n",
      "loss: 0.760585  [ 1088/ 3200]\n",
      "loss: 0.242767  [ 1104/ 3200]\n",
      "loss: 0.404011  [ 1120/ 3200]\n",
      "loss: 0.317672  [ 1136/ 3200]\n",
      "loss: 0.305030  [ 1152/ 3200]\n",
      "loss: 0.345707  [ 1168/ 3200]\n",
      "loss: 0.367834  [ 1184/ 3200]\n",
      "loss: 0.265413  [ 1200/ 3200]\n",
      "loss: 0.643402  [ 1216/ 3200]\n",
      "loss: 0.475199  [ 1232/ 3200]\n",
      "loss: 0.338463  [ 1248/ 3200]\n",
      "loss: 0.472013  [ 1264/ 3200]\n",
      "loss: 0.327664  [ 1280/ 3200]\n",
      "loss: 0.429654  [ 1296/ 3200]\n",
      "loss: 0.357437  [ 1312/ 3200]\n",
      "loss: 0.448934  [ 1328/ 3200]\n",
      "loss: 0.461075  [ 1344/ 3200]\n",
      "loss: 0.326795  [ 1360/ 3200]\n",
      "loss: 0.334395  [ 1376/ 3200]\n",
      "loss: 0.251338  [ 1392/ 3200]\n",
      "loss: 0.616517  [ 1408/ 3200]\n",
      "loss: 0.281684  [ 1424/ 3200]\n",
      "loss: 0.861475  [ 1440/ 3200]\n",
      "loss: 0.443447  [ 1456/ 3200]\n",
      "loss: 0.629516  [ 1472/ 3200]\n",
      "loss: 0.690555  [ 1488/ 3200]\n",
      "loss: 0.163329  [ 1504/ 3200]\n",
      "loss: 0.128532  [ 1520/ 3200]\n",
      "loss: 0.271841  [ 1536/ 3200]\n",
      "loss: 0.448573  [ 1552/ 3200]\n",
      "loss: 0.210040  [ 1568/ 3200]\n",
      "loss: 0.087724  [ 1584/ 3200]\n",
      "loss: 0.361007  [ 1600/ 3200]\n",
      "loss: 0.323061  [ 1616/ 3200]\n",
      "loss: 0.345564  [ 1632/ 3200]\n",
      "loss: 0.346124  [ 1648/ 3200]\n",
      "loss: 0.379152  [ 1664/ 3200]\n",
      "loss: 0.304482  [ 1680/ 3200]\n",
      "loss: 0.207771  [ 1696/ 3200]\n",
      "loss: 0.309980  [ 1712/ 3200]\n",
      "loss: 0.416897  [ 1728/ 3200]\n",
      "loss: 0.544835  [ 1744/ 3200]\n",
      "loss: 0.283280  [ 1760/ 3200]\n",
      "loss: 0.313717  [ 1776/ 3200]\n",
      "loss: 0.720401  [ 1792/ 3200]\n",
      "loss: 0.396924  [ 1808/ 3200]\n",
      "loss: 0.184580  [ 1824/ 3200]\n",
      "loss: 0.380963  [ 1840/ 3200]\n",
      "loss: 0.537112  [ 1856/ 3200]\n",
      "loss: 0.311721  [ 1872/ 3200]\n",
      "loss: 0.167252  [ 1888/ 3200]\n",
      "loss: 0.338658  [ 1904/ 3200]\n",
      "loss: 0.298263  [ 1920/ 3200]\n",
      "loss: 0.294663  [ 1936/ 3200]\n",
      "loss: 0.593865  [ 1952/ 3200]\n",
      "loss: 0.235662  [ 1968/ 3200]\n",
      "loss: 0.271829  [ 1984/ 3200]\n",
      "loss: 0.356518  [ 2000/ 3200]\n",
      "loss: 0.263480  [ 2016/ 3200]\n",
      "loss: 0.310933  [ 2032/ 3200]\n",
      "loss: 0.318392  [ 2048/ 3200]\n",
      "loss: 0.266504  [ 2064/ 3200]\n",
      "loss: 0.281108  [ 2080/ 3200]\n",
      "loss: 0.103047  [ 2096/ 3200]\n",
      "loss: 0.270487  [ 2112/ 3200]\n",
      "loss: 0.305065  [ 2128/ 3200]\n",
      "loss: 0.209137  [ 2144/ 3200]\n",
      "loss: 0.689895  [ 2160/ 3200]\n",
      "loss: 0.263024  [ 2176/ 3200]\n",
      "loss: 0.163188  [ 2192/ 3200]\n",
      "loss: 0.190254  [ 2208/ 3200]\n",
      "loss: 0.640272  [ 2224/ 3200]\n",
      "loss: 0.214706  [ 2240/ 3200]\n",
      "loss: 0.374340  [ 2256/ 3200]\n",
      "loss: 0.306432  [ 2272/ 3200]\n",
      "loss: 0.508167  [ 2288/ 3200]\n",
      "loss: 0.455308  [ 2304/ 3200]\n",
      "loss: 0.541949  [ 2320/ 3200]\n",
      "loss: 0.329828  [ 2336/ 3200]\n",
      "loss: 0.482659  [ 2352/ 3200]\n",
      "loss: 0.705185  [ 2368/ 3200]\n",
      "loss: 0.282469  [ 2384/ 3200]\n",
      "loss: 0.247311  [ 2400/ 3200]\n",
      "loss: 0.250864  [ 2416/ 3200]\n",
      "loss: 0.214831  [ 2432/ 3200]\n",
      "loss: 0.195428  [ 2448/ 3200]\n",
      "loss: 0.181829  [ 2464/ 3200]\n",
      "loss: 0.278436  [ 2480/ 3200]\n",
      "loss: 0.187814  [ 2496/ 3200]\n",
      "loss: 0.343801  [ 2512/ 3200]\n",
      "loss: 0.432514  [ 2528/ 3200]\n",
      "loss: 0.256411  [ 2544/ 3200]\n",
      "loss: 0.264427  [ 2560/ 3200]\n",
      "loss: 0.341889  [ 2576/ 3200]\n",
      "loss: 0.444512  [ 2592/ 3200]\n",
      "loss: 0.455383  [ 2608/ 3200]\n",
      "loss: 0.339764  [ 2624/ 3200]\n",
      "loss: 0.493646  [ 2640/ 3200]\n",
      "loss: 0.246953  [ 2656/ 3200]\n",
      "loss: 0.346781  [ 2672/ 3200]\n",
      "loss: 0.520762  [ 2688/ 3200]\n",
      "loss: 0.169824  [ 2704/ 3200]\n",
      "loss: 0.561909  [ 2720/ 3200]\n",
      "loss: 0.407449  [ 2736/ 3200]\n",
      "loss: 0.335330  [ 2752/ 3200]\n",
      "loss: 0.878565  [ 2768/ 3200]\n",
      "loss: 0.549145  [ 2784/ 3200]\n",
      "loss: 0.145075  [ 2800/ 3200]\n",
      "loss: 0.162517  [ 2816/ 3200]\n",
      "loss: 0.455696  [ 2832/ 3200]\n",
      "loss: 0.618392  [ 2848/ 3200]\n",
      "loss: 0.347112  [ 2864/ 3200]\n",
      "loss: 0.375558  [ 2880/ 3200]\n",
      "loss: 0.482112  [ 2896/ 3200]\n",
      "loss: 0.420248  [ 2912/ 3200]\n",
      "loss: 0.207195  [ 2928/ 3200]\n",
      "loss: 0.465149  [ 2944/ 3200]\n",
      "loss: 0.240517  [ 2960/ 3200]\n",
      "loss: 0.744863  [ 2976/ 3200]\n",
      "loss: 0.541458  [ 2992/ 3200]\n",
      "loss: 0.296060  [ 3008/ 3200]\n",
      "loss: 0.213908  [ 3024/ 3200]\n",
      "loss: 0.245373  [ 3040/ 3200]\n",
      "loss: 0.373721  [ 3056/ 3200]\n",
      "loss: 0.419714  [ 3072/ 3200]\n",
      "loss: 0.219925  [ 3088/ 3200]\n",
      "loss: 0.118860  [ 3104/ 3200]\n",
      "loss: 0.330247  [ 3120/ 3200]\n",
      "loss: 0.590511  [ 3136/ 3200]\n",
      "loss: 0.405296  [ 3152/ 3200]\n",
      "loss: 0.220691  [ 3168/ 3200]\n",
      "loss: 0.481853  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034163\n",
      "f1 macro averaged score: 0.779114\n",
      "Accuracy               : 77.8%\n",
      "Confusion matrix       :\n",
      "tensor([[173,  22,   0,   5],\n",
      "        [ 15, 135,  24,  26],\n",
      "        [  0,  29, 166,   5],\n",
      "        [  5,  31,  16, 148]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2605e-03.\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 0.203712  [    0/ 3200]\n",
      "loss: 0.385945  [   16/ 3200]\n",
      "loss: 0.262137  [   32/ 3200]\n",
      "loss: 0.718999  [   48/ 3200]\n",
      "loss: 0.724516  [   64/ 3200]\n",
      "loss: 0.356016  [   80/ 3200]\n",
      "loss: 0.285232  [   96/ 3200]\n",
      "loss: 0.343453  [  112/ 3200]\n",
      "loss: 0.114812  [  128/ 3200]\n",
      "loss: 0.289881  [  144/ 3200]\n",
      "loss: 0.362590  [  160/ 3200]\n",
      "loss: 0.296671  [  176/ 3200]\n",
      "loss: 0.363810  [  192/ 3200]\n",
      "loss: 0.392210  [  208/ 3200]\n",
      "loss: 0.586676  [  224/ 3200]\n",
      "loss: 0.312938  [  240/ 3200]\n",
      "loss: 0.503688  [  256/ 3200]\n",
      "loss: 0.650621  [  272/ 3200]\n",
      "loss: 0.342451  [  288/ 3200]\n",
      "loss: 0.421243  [  304/ 3200]\n",
      "loss: 0.259828  [  320/ 3200]\n",
      "loss: 0.264427  [  336/ 3200]\n",
      "loss: 0.648197  [  352/ 3200]\n",
      "loss: 0.135229  [  368/ 3200]\n",
      "loss: 0.177174  [  384/ 3200]\n",
      "loss: 0.224861  [  400/ 3200]\n",
      "loss: 0.226837  [  416/ 3200]\n",
      "loss: 0.367800  [  432/ 3200]\n",
      "loss: 0.267651  [  448/ 3200]\n",
      "loss: 0.340272  [  464/ 3200]\n",
      "loss: 0.269045  [  480/ 3200]\n",
      "loss: 0.455472  [  496/ 3200]\n",
      "loss: 0.235798  [  512/ 3200]\n",
      "loss: 0.571513  [  528/ 3200]\n",
      "loss: 0.331494  [  544/ 3200]\n",
      "loss: 0.230178  [  560/ 3200]\n",
      "loss: 0.512954  [  576/ 3200]\n",
      "loss: 0.188837  [  592/ 3200]\n",
      "loss: 0.409945  [  608/ 3200]\n",
      "loss: 0.132696  [  624/ 3200]\n",
      "loss: 0.219206  [  640/ 3200]\n",
      "loss: 0.282786  [  656/ 3200]\n",
      "loss: 0.248917  [  672/ 3200]\n",
      "loss: 0.239604  [  688/ 3200]\n",
      "loss: 0.538977  [  704/ 3200]\n",
      "loss: 0.314653  [  720/ 3200]\n",
      "loss: 0.328241  [  736/ 3200]\n",
      "loss: 0.337935  [  752/ 3200]\n",
      "loss: 0.783186  [  768/ 3200]\n",
      "loss: 0.253909  [  784/ 3200]\n",
      "loss: 0.569608  [  800/ 3200]\n",
      "loss: 0.731610  [  816/ 3200]\n",
      "loss: 0.413470  [  832/ 3200]\n",
      "loss: 0.402797  [  848/ 3200]\n",
      "loss: 0.326947  [  864/ 3200]\n",
      "loss: 0.490822  [  880/ 3200]\n",
      "loss: 0.243494  [  896/ 3200]\n",
      "loss: 0.389800  [  912/ 3200]\n",
      "loss: 0.168487  [  928/ 3200]\n",
      "loss: 0.269048  [  944/ 3200]\n",
      "loss: 0.313411  [  960/ 3200]\n",
      "loss: 0.132364  [  976/ 3200]\n",
      "loss: 0.780838  [  992/ 3200]\n",
      "loss: 0.192237  [ 1008/ 3200]\n",
      "loss: 0.151411  [ 1024/ 3200]\n",
      "loss: 0.446433  [ 1040/ 3200]\n",
      "loss: 0.112176  [ 1056/ 3200]\n",
      "loss: 0.354076  [ 1072/ 3200]\n",
      "loss: 0.768246  [ 1088/ 3200]\n",
      "loss: 0.166168  [ 1104/ 3200]\n",
      "loss: 0.322386  [ 1120/ 3200]\n",
      "loss: 0.294841  [ 1136/ 3200]\n",
      "loss: 0.318419  [ 1152/ 3200]\n",
      "loss: 0.307906  [ 1168/ 3200]\n",
      "loss: 0.338891  [ 1184/ 3200]\n",
      "loss: 0.219749  [ 1200/ 3200]\n",
      "loss: 0.268769  [ 1216/ 3200]\n",
      "loss: 0.335338  [ 1232/ 3200]\n",
      "loss: 0.391461  [ 1248/ 3200]\n",
      "loss: 0.353457  [ 1264/ 3200]\n",
      "loss: 0.308538  [ 1280/ 3200]\n",
      "loss: 0.228824  [ 1296/ 3200]\n",
      "loss: 0.262807  [ 1312/ 3200]\n",
      "loss: 0.116429  [ 1328/ 3200]\n",
      "loss: 0.216680  [ 1344/ 3200]\n",
      "loss: 0.263761  [ 1360/ 3200]\n",
      "loss: 0.133882  [ 1376/ 3200]\n",
      "loss: 0.288106  [ 1392/ 3200]\n",
      "loss: 0.233005  [ 1408/ 3200]\n",
      "loss: 0.231954  [ 1424/ 3200]\n",
      "loss: 0.100614  [ 1440/ 3200]\n",
      "loss: 0.241394  [ 1456/ 3200]\n",
      "loss: 0.262840  [ 1472/ 3200]\n",
      "loss: 0.442931  [ 1488/ 3200]\n",
      "loss: 0.089099  [ 1504/ 3200]\n",
      "loss: 0.221544  [ 1520/ 3200]\n",
      "loss: 0.479383  [ 1536/ 3200]\n",
      "loss: 0.415514  [ 1552/ 3200]\n",
      "loss: 0.417669  [ 1568/ 3200]\n",
      "loss: 0.251354  [ 1584/ 3200]\n",
      "loss: 0.294525  [ 1600/ 3200]\n",
      "loss: 0.281962  [ 1616/ 3200]\n",
      "loss: 0.272947  [ 1632/ 3200]\n",
      "loss: 0.262082  [ 1648/ 3200]\n",
      "loss: 0.295932  [ 1664/ 3200]\n",
      "loss: 0.798214  [ 1680/ 3200]\n",
      "loss: 0.231561  [ 1696/ 3200]\n",
      "loss: 0.206812  [ 1712/ 3200]\n",
      "loss: 0.331386  [ 1728/ 3200]\n",
      "loss: 0.747768  [ 1744/ 3200]\n",
      "loss: 0.356599  [ 1760/ 3200]\n",
      "loss: 0.653604  [ 1776/ 3200]\n",
      "loss: 0.109598  [ 1792/ 3200]\n",
      "loss: 0.510981  [ 1808/ 3200]\n",
      "loss: 0.195688  [ 1824/ 3200]\n",
      "loss: 0.249635  [ 1840/ 3200]\n",
      "loss: 0.165838  [ 1856/ 3200]\n",
      "loss: 0.255558  [ 1872/ 3200]\n",
      "loss: 0.165842  [ 1888/ 3200]\n",
      "loss: 0.436343  [ 1904/ 3200]\n",
      "loss: 0.393909  [ 1920/ 3200]\n",
      "loss: 0.486901  [ 1936/ 3200]\n",
      "loss: 0.345171  [ 1952/ 3200]\n",
      "loss: 0.243571  [ 1968/ 3200]\n",
      "loss: 0.596776  [ 1984/ 3200]\n",
      "loss: 0.138746  [ 2000/ 3200]\n",
      "loss: 0.355964  [ 2016/ 3200]\n",
      "loss: 0.274857  [ 2032/ 3200]\n",
      "loss: 0.193502  [ 2048/ 3200]\n",
      "loss: 0.105783  [ 2064/ 3200]\n",
      "loss: 0.572780  [ 2080/ 3200]\n",
      "loss: 0.125543  [ 2096/ 3200]\n",
      "loss: 0.323830  [ 2112/ 3200]\n",
      "loss: 0.380390  [ 2128/ 3200]\n",
      "loss: 0.192141  [ 2144/ 3200]\n",
      "loss: 0.351605  [ 2160/ 3200]\n",
      "loss: 0.224085  [ 2176/ 3200]\n",
      "loss: 0.143094  [ 2192/ 3200]\n",
      "loss: 0.235737  [ 2208/ 3200]\n",
      "loss: 0.507981  [ 2224/ 3200]\n",
      "loss: 0.239864  [ 2240/ 3200]\n",
      "loss: 0.654792  [ 2256/ 3200]\n",
      "loss: 0.078087  [ 2272/ 3200]\n",
      "loss: 0.453706  [ 2288/ 3200]\n",
      "loss: 0.628836  [ 2304/ 3200]\n",
      "loss: 0.446259  [ 2320/ 3200]\n",
      "loss: 0.323662  [ 2336/ 3200]\n",
      "loss: 0.337856  [ 2352/ 3200]\n",
      "loss: 0.150722  [ 2368/ 3200]\n",
      "loss: 0.107831  [ 2384/ 3200]\n",
      "loss: 0.261651  [ 2400/ 3200]\n",
      "loss: 0.167930  [ 2416/ 3200]\n",
      "loss: 0.190251  [ 2432/ 3200]\n",
      "loss: 0.556721  [ 2448/ 3200]\n",
      "loss: 0.599583  [ 2464/ 3200]\n",
      "loss: 0.530440  [ 2480/ 3200]\n",
      "loss: 0.319999  [ 2496/ 3200]\n",
      "loss: 0.386716  [ 2512/ 3200]\n",
      "loss: 0.233125  [ 2528/ 3200]\n",
      "loss: 0.290051  [ 2544/ 3200]\n",
      "loss: 0.551257  [ 2560/ 3200]\n",
      "loss: 0.234150  [ 2576/ 3200]\n",
      "loss: 0.160528  [ 2592/ 3200]\n",
      "loss: 0.314533  [ 2608/ 3200]\n",
      "loss: 0.288093  [ 2624/ 3200]\n",
      "loss: 0.322451  [ 2640/ 3200]\n",
      "loss: 0.265477  [ 2656/ 3200]\n",
      "loss: 0.328818  [ 2672/ 3200]\n",
      "loss: 0.495591  [ 2688/ 3200]\n",
      "loss: 0.174187  [ 2704/ 3200]\n",
      "loss: 0.180742  [ 2720/ 3200]\n",
      "loss: 0.570772  [ 2736/ 3200]\n",
      "loss: 0.272166  [ 2752/ 3200]\n",
      "loss: 0.310383  [ 2768/ 3200]\n",
      "loss: 0.472899  [ 2784/ 3200]\n",
      "loss: 0.139724  [ 2800/ 3200]\n",
      "loss: 0.394377  [ 2816/ 3200]\n",
      "loss: 0.273472  [ 2832/ 3200]\n",
      "loss: 0.098547  [ 2848/ 3200]\n",
      "loss: 0.218433  [ 2864/ 3200]\n",
      "loss: 0.384222  [ 2880/ 3200]\n",
      "loss: 0.235162  [ 2896/ 3200]\n",
      "loss: 0.278554  [ 2912/ 3200]\n",
      "loss: 0.419156  [ 2928/ 3200]\n",
      "loss: 0.284094  [ 2944/ 3200]\n",
      "loss: 0.327992  [ 2960/ 3200]\n",
      "loss: 0.202954  [ 2976/ 3200]\n",
      "loss: 0.284330  [ 2992/ 3200]\n",
      "loss: 0.257553  [ 3008/ 3200]\n",
      "loss: 0.180356  [ 3024/ 3200]\n",
      "loss: 0.354586  [ 3040/ 3200]\n",
      "loss: 0.169909  [ 3056/ 3200]\n",
      "loss: 0.123787  [ 3072/ 3200]\n",
      "loss: 0.651018  [ 3088/ 3200]\n",
      "loss: 0.541827  [ 3104/ 3200]\n",
      "loss: 0.709919  [ 3120/ 3200]\n",
      "loss: 0.149018  [ 3136/ 3200]\n",
      "loss: 0.248007  [ 3152/ 3200]\n",
      "loss: 0.156951  [ 3168/ 3200]\n",
      "loss: 0.537103  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036234\n",
      "f1 macro averaged score: 0.777820\n",
      "Accuracy               : 77.5%\n",
      "Confusion matrix       :\n",
      "tensor([[165,  28,   0,   7],\n",
      "        [ 10, 140,  22,  28],\n",
      "        [  0,  32, 157,  11],\n",
      "        [  6,  23,  13, 158]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1975e-03.\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 0.270436  [    0/ 3200]\n",
      "loss: 0.464376  [   16/ 3200]\n",
      "loss: 0.177800  [   32/ 3200]\n",
      "loss: 0.596098  [   48/ 3200]\n",
      "loss: 0.577002  [   64/ 3200]\n",
      "loss: 0.388583  [   80/ 3200]\n",
      "loss: 0.279732  [   96/ 3200]\n",
      "loss: 0.308173  [  112/ 3200]\n",
      "loss: 0.202358  [  128/ 3200]\n",
      "loss: 0.209824  [  144/ 3200]\n",
      "loss: 0.277680  [  160/ 3200]\n",
      "loss: 0.199280  [  176/ 3200]\n",
      "loss: 0.326788  [  192/ 3200]\n",
      "loss: 0.144668  [  208/ 3200]\n",
      "loss: 0.337496  [  224/ 3200]\n",
      "loss: 0.241382  [  240/ 3200]\n",
      "loss: 0.119472  [  256/ 3200]\n",
      "loss: 0.262537  [  272/ 3200]\n",
      "loss: 0.200553  [  288/ 3200]\n",
      "loss: 0.580544  [  304/ 3200]\n",
      "loss: 0.759209  [  320/ 3200]\n",
      "loss: 0.656016  [  336/ 3200]\n",
      "loss: 0.294758  [  352/ 3200]\n",
      "loss: 0.503421  [  368/ 3200]\n",
      "loss: 0.334731  [  384/ 3200]\n",
      "loss: 0.134068  [  400/ 3200]\n",
      "loss: 0.582196  [  416/ 3200]\n",
      "loss: 0.153046  [  432/ 3200]\n",
      "loss: 0.405330  [  448/ 3200]\n",
      "loss: 0.377811  [  464/ 3200]\n",
      "loss: 0.082915  [  480/ 3200]\n",
      "loss: 0.251085  [  496/ 3200]\n",
      "loss: 0.532873  [  512/ 3200]\n",
      "loss: 0.348256  [  528/ 3200]\n",
      "loss: 0.294128  [  544/ 3200]\n",
      "loss: 0.238935  [  560/ 3200]\n",
      "loss: 0.083418  [  576/ 3200]\n",
      "loss: 0.160025  [  592/ 3200]\n",
      "loss: 0.186481  [  608/ 3200]\n",
      "loss: 0.496152  [  624/ 3200]\n",
      "loss: 0.651675  [  640/ 3200]\n",
      "loss: 0.546387  [  656/ 3200]\n",
      "loss: 0.311190  [  672/ 3200]\n",
      "loss: 0.390958  [  688/ 3200]\n",
      "loss: 0.262503  [  704/ 3200]\n",
      "loss: 0.472318  [  720/ 3200]\n",
      "loss: 0.437263  [  736/ 3200]\n",
      "loss: 0.283581  [  752/ 3200]\n",
      "loss: 0.124060  [  768/ 3200]\n",
      "loss: 0.117293  [  784/ 3200]\n",
      "loss: 0.232129  [  800/ 3200]\n",
      "loss: 0.417389  [  816/ 3200]\n",
      "loss: 0.397077  [  832/ 3200]\n",
      "loss: 0.378643  [  848/ 3200]\n",
      "loss: 0.343565  [  864/ 3200]\n",
      "loss: 0.325462  [  880/ 3200]\n",
      "loss: 0.206310  [  896/ 3200]\n",
      "loss: 0.442909  [  912/ 3200]\n",
      "loss: 0.219902  [  928/ 3200]\n",
      "loss: 0.271245  [  944/ 3200]\n",
      "loss: 0.194915  [  960/ 3200]\n",
      "loss: 0.207618  [  976/ 3200]\n",
      "loss: 0.232732  [  992/ 3200]\n",
      "loss: 0.186488  [ 1008/ 3200]\n",
      "loss: 0.163102  [ 1024/ 3200]\n",
      "loss: 0.380033  [ 1040/ 3200]\n",
      "loss: 0.173759  [ 1056/ 3200]\n",
      "loss: 0.288206  [ 1072/ 3200]\n",
      "loss: 0.150186  [ 1088/ 3200]\n",
      "loss: 0.462761  [ 1104/ 3200]\n",
      "loss: 0.520452  [ 1120/ 3200]\n",
      "loss: 0.063937  [ 1136/ 3200]\n",
      "loss: 0.387499  [ 1152/ 3200]\n",
      "loss: 0.183091  [ 1168/ 3200]\n",
      "loss: 0.205494  [ 1184/ 3200]\n",
      "loss: 0.470957  [ 1200/ 3200]\n",
      "loss: 0.185332  [ 1216/ 3200]\n",
      "loss: 0.172361  [ 1232/ 3200]\n",
      "loss: 0.190972  [ 1248/ 3200]\n",
      "loss: 0.251904  [ 1264/ 3200]\n",
      "loss: 0.149068  [ 1280/ 3200]\n",
      "loss: 0.146923  [ 1296/ 3200]\n",
      "loss: 0.182461  [ 1312/ 3200]\n",
      "loss: 0.258231  [ 1328/ 3200]\n",
      "loss: 0.081213  [ 1344/ 3200]\n",
      "loss: 0.543778  [ 1360/ 3200]\n",
      "loss: 0.461811  [ 1376/ 3200]\n",
      "loss: 0.482673  [ 1392/ 3200]\n",
      "loss: 0.494966  [ 1408/ 3200]\n",
      "loss: 0.221943  [ 1424/ 3200]\n",
      "loss: 0.264705  [ 1440/ 3200]\n",
      "loss: 0.539480  [ 1456/ 3200]\n",
      "loss: 0.139867  [ 1472/ 3200]\n",
      "loss: 0.192246  [ 1488/ 3200]\n",
      "loss: 0.213007  [ 1504/ 3200]\n",
      "loss: 0.204258  [ 1520/ 3200]\n",
      "loss: 0.370746  [ 1536/ 3200]\n",
      "loss: 0.131759  [ 1552/ 3200]\n",
      "loss: 0.182011  [ 1568/ 3200]\n",
      "loss: 0.178534  [ 1584/ 3200]\n",
      "loss: 0.454388  [ 1600/ 3200]\n",
      "loss: 0.729689  [ 1616/ 3200]\n",
      "loss: 0.317375  [ 1632/ 3200]\n",
      "loss: 0.173285  [ 1648/ 3200]\n",
      "loss: 0.414695  [ 1664/ 3200]\n",
      "loss: 0.260634  [ 1680/ 3200]\n",
      "loss: 0.309036  [ 1696/ 3200]\n",
      "loss: 0.199713  [ 1712/ 3200]\n",
      "loss: 0.498095  [ 1728/ 3200]\n",
      "loss: 0.447840  [ 1744/ 3200]\n",
      "loss: 0.221805  [ 1760/ 3200]\n",
      "loss: 0.505027  [ 1776/ 3200]\n",
      "loss: 0.123123  [ 1792/ 3200]\n",
      "loss: 0.155843  [ 1808/ 3200]\n",
      "loss: 0.160098  [ 1824/ 3200]\n",
      "loss: 0.418221  [ 1840/ 3200]\n",
      "loss: 0.059893  [ 1856/ 3200]\n",
      "loss: 0.185514  [ 1872/ 3200]\n",
      "loss: 0.349399  [ 1888/ 3200]\n",
      "loss: 0.658201  [ 1904/ 3200]\n",
      "loss: 0.261735  [ 1920/ 3200]\n",
      "loss: 0.410744  [ 1936/ 3200]\n",
      "loss: 0.296685  [ 1952/ 3200]\n",
      "loss: 0.074760  [ 1968/ 3200]\n",
      "loss: 0.109529  [ 1984/ 3200]\n",
      "loss: 0.568319  [ 2000/ 3200]\n",
      "loss: 0.166907  [ 2016/ 3200]\n",
      "loss: 0.243641  [ 2032/ 3200]\n",
      "loss: 0.438589  [ 2048/ 3200]\n",
      "loss: 0.608228  [ 2064/ 3200]\n",
      "loss: 0.466866  [ 2080/ 3200]\n",
      "loss: 0.393434  [ 2096/ 3200]\n",
      "loss: 0.427135  [ 2112/ 3200]\n",
      "loss: 0.111194  [ 2128/ 3200]\n",
      "loss: 0.148733  [ 2144/ 3200]\n",
      "loss: 0.660794  [ 2160/ 3200]\n",
      "loss: 0.585366  [ 2176/ 3200]\n",
      "loss: 0.601212  [ 2192/ 3200]\n",
      "loss: 0.269129  [ 2208/ 3200]\n",
      "loss: 0.164835  [ 2224/ 3200]\n",
      "loss: 0.188924  [ 2240/ 3200]\n",
      "loss: 0.268724  [ 2256/ 3200]\n",
      "loss: 0.750477  [ 2272/ 3200]\n",
      "loss: 0.133097  [ 2288/ 3200]\n",
      "loss: 0.185729  [ 2304/ 3200]\n",
      "loss: 0.138671  [ 2320/ 3200]\n",
      "loss: 0.293807  [ 2336/ 3200]\n",
      "loss: 0.362733  [ 2352/ 3200]\n",
      "loss: 0.205686  [ 2368/ 3200]\n",
      "loss: 0.292444  [ 2384/ 3200]\n",
      "loss: 0.303617  [ 2400/ 3200]\n",
      "loss: 0.408345  [ 2416/ 3200]\n",
      "loss: 0.476738  [ 2432/ 3200]\n",
      "loss: 0.192315  [ 2448/ 3200]\n",
      "loss: 0.355217  [ 2464/ 3200]\n",
      "loss: 0.092211  [ 2480/ 3200]\n",
      "loss: 0.256421  [ 2496/ 3200]\n",
      "loss: 0.303400  [ 2512/ 3200]\n",
      "loss: 0.172576  [ 2528/ 3200]\n",
      "loss: 0.104656  [ 2544/ 3200]\n",
      "loss: 0.175384  [ 2560/ 3200]\n",
      "loss: 0.356810  [ 2576/ 3200]\n",
      "loss: 0.338771  [ 2592/ 3200]\n",
      "loss: 0.253089  [ 2608/ 3200]\n",
      "loss: 0.382239  [ 2624/ 3200]\n",
      "loss: 0.260120  [ 2640/ 3200]\n",
      "loss: 0.445287  [ 2656/ 3200]\n",
      "loss: 0.637555  [ 2672/ 3200]\n",
      "loss: 0.250083  [ 2688/ 3200]\n",
      "loss: 0.272296  [ 2704/ 3200]\n",
      "loss: 0.089367  [ 2720/ 3200]\n",
      "loss: 0.341647  [ 2736/ 3200]\n",
      "loss: 0.329044  [ 2752/ 3200]\n",
      "loss: 0.284158  [ 2768/ 3200]\n",
      "loss: 0.125914  [ 2784/ 3200]\n",
      "loss: 0.222477  [ 2800/ 3200]\n",
      "loss: 0.181795  [ 2816/ 3200]\n",
      "loss: 0.336106  [ 2832/ 3200]\n",
      "loss: 0.275548  [ 2848/ 3200]\n",
      "loss: 0.278083  [ 2864/ 3200]\n",
      "loss: 0.333753  [ 2880/ 3200]\n",
      "loss: 0.190571  [ 2896/ 3200]\n",
      "loss: 0.151380  [ 2912/ 3200]\n",
      "loss: 0.240433  [ 2928/ 3200]\n",
      "loss: 0.131715  [ 2944/ 3200]\n",
      "loss: 0.406410  [ 2960/ 3200]\n",
      "loss: 0.368490  [ 2976/ 3200]\n",
      "loss: 0.186092  [ 2992/ 3200]\n",
      "loss: 0.212855  [ 3008/ 3200]\n",
      "loss: 0.244037  [ 3024/ 3200]\n",
      "loss: 0.073043  [ 3040/ 3200]\n",
      "loss: 0.165134  [ 3056/ 3200]\n",
      "loss: 0.470279  [ 3072/ 3200]\n",
      "loss: 0.369302  [ 3088/ 3200]\n",
      "loss: 0.417276  [ 3104/ 3200]\n",
      "loss: 0.291924  [ 3120/ 3200]\n",
      "loss: 0.211595  [ 3136/ 3200]\n",
      "loss: 0.531548  [ 3152/ 3200]\n",
      "loss: 0.440149  [ 3168/ 3200]\n",
      "loss: 0.359238  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036304\n",
      "f1 macro averaged score: 0.780409\n",
      "Accuracy               : 78.8%\n",
      "Confusion matrix       :\n",
      "tensor([[189,   5,   0,   6],\n",
      "        [ 19, 108,  31,  42],\n",
      "        [  0,  21, 169,  10],\n",
      "        [ 11,   9,  16, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1376e-03.\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 0.313748  [    0/ 3200]\n",
      "loss: 0.426986  [   16/ 3200]\n",
      "loss: 0.318688  [   32/ 3200]\n",
      "loss: 0.112699  [   48/ 3200]\n",
      "loss: 0.215099  [   64/ 3200]\n",
      "loss: 0.627576  [   80/ 3200]\n",
      "loss: 0.341741  [   96/ 3200]\n",
      "loss: 0.257935  [  112/ 3200]\n",
      "loss: 0.239829  [  128/ 3200]\n",
      "loss: 0.369401  [  144/ 3200]\n",
      "loss: 0.499453  [  160/ 3200]\n",
      "loss: 0.103706  [  176/ 3200]\n",
      "loss: 0.261096  [  192/ 3200]\n",
      "loss: 0.448281  [  208/ 3200]\n",
      "loss: 0.116089  [  224/ 3200]\n",
      "loss: 0.274494  [  240/ 3200]\n",
      "loss: 0.582244  [  256/ 3200]\n",
      "loss: 0.383258  [  272/ 3200]\n",
      "loss: 0.368534  [  288/ 3200]\n",
      "loss: 0.376586  [  304/ 3200]\n",
      "loss: 0.464256  [  320/ 3200]\n",
      "loss: 0.234293  [  336/ 3200]\n",
      "loss: 0.199116  [  352/ 3200]\n",
      "loss: 0.309075  [  368/ 3200]\n",
      "loss: 0.155809  [  384/ 3200]\n",
      "loss: 0.184812  [  400/ 3200]\n",
      "loss: 0.128203  [  416/ 3200]\n",
      "loss: 0.113154  [  432/ 3200]\n",
      "loss: 0.142639  [  448/ 3200]\n",
      "loss: 0.101735  [  464/ 3200]\n",
      "loss: 0.957702  [  480/ 3200]\n",
      "loss: 0.153090  [  496/ 3200]\n",
      "loss: 0.545822  [  512/ 3200]\n",
      "loss: 0.135495  [  528/ 3200]\n",
      "loss: 0.147856  [  544/ 3200]\n",
      "loss: 0.279857  [  560/ 3200]\n",
      "loss: 0.293464  [  576/ 3200]\n",
      "loss: 0.256385  [  592/ 3200]\n",
      "loss: 0.200992  [  608/ 3200]\n",
      "loss: 0.446825  [  624/ 3200]\n",
      "loss: 0.091743  [  640/ 3200]\n",
      "loss: 0.523086  [  656/ 3200]\n",
      "loss: 0.290017  [  672/ 3200]\n",
      "loss: 0.323369  [  688/ 3200]\n",
      "loss: 0.119852  [  704/ 3200]\n",
      "loss: 0.281402  [  720/ 3200]\n",
      "loss: 0.343548  [  736/ 3200]\n",
      "loss: 0.353149  [  752/ 3200]\n",
      "loss: 0.303647  [  768/ 3200]\n",
      "loss: 0.369780  [  784/ 3200]\n",
      "loss: 0.137268  [  800/ 3200]\n",
      "loss: 0.257065  [  816/ 3200]\n",
      "loss: 0.224571  [  832/ 3200]\n",
      "loss: 0.120254  [  848/ 3200]\n",
      "loss: 0.519958  [  864/ 3200]\n",
      "loss: 0.509760  [  880/ 3200]\n",
      "loss: 0.140171  [  896/ 3200]\n",
      "loss: 0.171689  [  912/ 3200]\n",
      "loss: 0.202178  [  928/ 3200]\n",
      "loss: 0.225967  [  944/ 3200]\n",
      "loss: 0.280755  [  960/ 3200]\n",
      "loss: 0.254559  [  976/ 3200]\n",
      "loss: 0.467429  [  992/ 3200]\n",
      "loss: 0.538985  [ 1008/ 3200]\n",
      "loss: 0.366016  [ 1024/ 3200]\n",
      "loss: 0.161471  [ 1040/ 3200]\n",
      "loss: 0.258284  [ 1056/ 3200]\n",
      "loss: 0.217126  [ 1072/ 3200]\n",
      "loss: 0.238544  [ 1088/ 3200]\n",
      "loss: 0.566676  [ 1104/ 3200]\n",
      "loss: 0.119846  [ 1120/ 3200]\n",
      "loss: 0.245436  [ 1136/ 3200]\n",
      "loss: 0.213092  [ 1152/ 3200]\n",
      "loss: 0.159784  [ 1168/ 3200]\n",
      "loss: 0.441292  [ 1184/ 3200]\n",
      "loss: 0.243502  [ 1200/ 3200]\n",
      "loss: 0.561918  [ 1216/ 3200]\n",
      "loss: 0.188416  [ 1232/ 3200]\n",
      "loss: 0.203781  [ 1248/ 3200]\n",
      "loss: 0.178035  [ 1264/ 3200]\n",
      "loss: 0.197493  [ 1280/ 3200]\n",
      "loss: 0.075947  [ 1296/ 3200]\n",
      "loss: 0.347463  [ 1312/ 3200]\n",
      "loss: 0.123905  [ 1328/ 3200]\n",
      "loss: 0.215225  [ 1344/ 3200]\n",
      "loss: 0.079312  [ 1360/ 3200]\n",
      "loss: 0.283828  [ 1376/ 3200]\n",
      "loss: 0.356691  [ 1392/ 3200]\n",
      "loss: 0.333625  [ 1408/ 3200]\n",
      "loss: 0.217877  [ 1424/ 3200]\n",
      "loss: 0.289057  [ 1440/ 3200]\n",
      "loss: 0.231041  [ 1456/ 3200]\n",
      "loss: 0.163996  [ 1472/ 3200]\n",
      "loss: 0.332166  [ 1488/ 3200]\n",
      "loss: 0.499911  [ 1504/ 3200]\n",
      "loss: 0.316734  [ 1520/ 3200]\n",
      "loss: 0.336809  [ 1536/ 3200]\n",
      "loss: 0.177663  [ 1552/ 3200]\n",
      "loss: 0.588999  [ 1568/ 3200]\n",
      "loss: 0.199533  [ 1584/ 3200]\n",
      "loss: 0.426083  [ 1600/ 3200]\n",
      "loss: 0.224433  [ 1616/ 3200]\n",
      "loss: 0.329875  [ 1632/ 3200]\n",
      "loss: 0.331322  [ 1648/ 3200]\n",
      "loss: 0.189873  [ 1664/ 3200]\n",
      "loss: 0.445882  [ 1680/ 3200]\n",
      "loss: 0.426814  [ 1696/ 3200]\n",
      "loss: 0.349216  [ 1712/ 3200]\n",
      "loss: 0.400940  [ 1728/ 3200]\n",
      "loss: 0.098596  [ 1744/ 3200]\n",
      "loss: 0.155765  [ 1760/ 3200]\n",
      "loss: 0.087864  [ 1776/ 3200]\n",
      "loss: 0.152018  [ 1792/ 3200]\n",
      "loss: 0.227001  [ 1808/ 3200]\n",
      "loss: 0.233875  [ 1824/ 3200]\n",
      "loss: 0.143808  [ 1840/ 3200]\n",
      "loss: 0.385285  [ 1856/ 3200]\n",
      "loss: 0.079585  [ 1872/ 3200]\n",
      "loss: 0.264320  [ 1888/ 3200]\n",
      "loss: 0.287161  [ 1904/ 3200]\n",
      "loss: 0.181125  [ 1920/ 3200]\n",
      "loss: 0.149938  [ 1936/ 3200]\n",
      "loss: 0.240814  [ 1952/ 3200]\n",
      "loss: 0.242526  [ 1968/ 3200]\n",
      "loss: 0.226729  [ 1984/ 3200]\n",
      "loss: 0.170676  [ 2000/ 3200]\n",
      "loss: 0.269929  [ 2016/ 3200]\n",
      "loss: 0.223103  [ 2032/ 3200]\n",
      "loss: 0.326446  [ 2048/ 3200]\n",
      "loss: 0.228328  [ 2064/ 3200]\n",
      "loss: 0.196336  [ 2080/ 3200]\n",
      "loss: 0.202180  [ 2096/ 3200]\n",
      "loss: 0.191670  [ 2112/ 3200]\n",
      "loss: 0.410093  [ 2128/ 3200]\n",
      "loss: 0.446607  [ 2144/ 3200]\n",
      "loss: 0.171633  [ 2160/ 3200]\n",
      "loss: 0.143630  [ 2176/ 3200]\n",
      "loss: 0.254844  [ 2192/ 3200]\n",
      "loss: 0.137248  [ 2208/ 3200]\n",
      "loss: 0.189900  [ 2224/ 3200]\n",
      "loss: 0.414072  [ 2240/ 3200]\n",
      "loss: 0.453467  [ 2256/ 3200]\n",
      "loss: 0.218032  [ 2272/ 3200]\n",
      "loss: 0.105886  [ 2288/ 3200]\n",
      "loss: 0.592765  [ 2304/ 3200]\n",
      "loss: 0.257284  [ 2320/ 3200]\n",
      "loss: 0.261655  [ 2336/ 3200]\n",
      "loss: 0.064610  [ 2352/ 3200]\n",
      "loss: 0.270386  [ 2368/ 3200]\n",
      "loss: 0.208206  [ 2384/ 3200]\n",
      "loss: 0.335977  [ 2400/ 3200]\n",
      "loss: 0.271290  [ 2416/ 3200]\n",
      "loss: 0.267251  [ 2432/ 3200]\n",
      "loss: 0.561330  [ 2448/ 3200]\n",
      "loss: 0.068016  [ 2464/ 3200]\n",
      "loss: 0.135155  [ 2480/ 3200]\n",
      "loss: 0.338280  [ 2496/ 3200]\n",
      "loss: 0.237481  [ 2512/ 3200]\n",
      "loss: 0.127562  [ 2528/ 3200]\n",
      "loss: 0.279904  [ 2544/ 3200]\n",
      "loss: 0.148699  [ 2560/ 3200]\n",
      "loss: 0.223174  [ 2576/ 3200]\n",
      "loss: 0.186429  [ 2592/ 3200]\n",
      "loss: 0.308302  [ 2608/ 3200]\n",
      "loss: 0.125950  [ 2624/ 3200]\n",
      "loss: 0.108757  [ 2640/ 3200]\n",
      "loss: 0.362332  [ 2656/ 3200]\n",
      "loss: 0.129484  [ 2672/ 3200]\n",
      "loss: 0.228180  [ 2688/ 3200]\n",
      "loss: 0.555403  [ 2704/ 3200]\n",
      "loss: 0.244616  [ 2720/ 3200]\n",
      "loss: 0.051304  [ 2736/ 3200]\n",
      "loss: 0.335027  [ 2752/ 3200]\n",
      "loss: 0.043129  [ 2768/ 3200]\n",
      "loss: 0.160469  [ 2784/ 3200]\n",
      "loss: 0.257167  [ 2800/ 3200]\n",
      "loss: 0.380773  [ 2816/ 3200]\n",
      "loss: 0.235504  [ 2832/ 3200]\n",
      "loss: 0.452867  [ 2848/ 3200]\n",
      "loss: 0.319216  [ 2864/ 3200]\n",
      "loss: 0.255038  [ 2880/ 3200]\n",
      "loss: 0.164963  [ 2896/ 3200]\n",
      "loss: 0.087276  [ 2912/ 3200]\n",
      "loss: 0.426077  [ 2928/ 3200]\n",
      "loss: 0.342664  [ 2944/ 3200]\n",
      "loss: 0.421864  [ 2960/ 3200]\n",
      "loss: 0.259598  [ 2976/ 3200]\n",
      "loss: 0.281228  [ 2992/ 3200]\n",
      "loss: 0.416771  [ 3008/ 3200]\n",
      "loss: 0.277337  [ 3024/ 3200]\n",
      "loss: 0.277589  [ 3040/ 3200]\n",
      "loss: 0.201618  [ 3056/ 3200]\n",
      "loss: 0.205109  [ 3072/ 3200]\n",
      "loss: 0.288025  [ 3088/ 3200]\n",
      "loss: 0.323940  [ 3104/ 3200]\n",
      "loss: 0.359645  [ 3120/ 3200]\n",
      "loss: 0.307152  [ 3136/ 3200]\n",
      "loss: 0.112883  [ 3152/ 3200]\n",
      "loss: 0.246852  [ 3168/ 3200]\n",
      "loss: 0.227350  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038772\n",
      "f1 macro averaged score: 0.765571\n",
      "Accuracy               : 77.4%\n",
      "Confusion matrix       :\n",
      "tensor([[188,   4,   0,   8],\n",
      "        [ 22, 101,  24,  53],\n",
      "        [  1,  20, 163,  16],\n",
      "        [ 13,   6,  14, 167]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0807e-03.\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 0.236430  [    0/ 3200]\n",
      "loss: 0.219933  [   16/ 3200]\n",
      "loss: 0.086759  [   32/ 3200]\n",
      "loss: 0.260654  [   48/ 3200]\n",
      "loss: 0.295449  [   64/ 3200]\n",
      "loss: 0.198143  [   80/ 3200]\n",
      "loss: 0.275878  [   96/ 3200]\n",
      "loss: 0.224335  [  112/ 3200]\n",
      "loss: 0.460235  [  128/ 3200]\n",
      "loss: 0.175690  [  144/ 3200]\n",
      "loss: 0.244863  [  160/ 3200]\n",
      "loss: 0.349497  [  176/ 3200]\n",
      "loss: 0.175956  [  192/ 3200]\n",
      "loss: 0.201487  [  208/ 3200]\n",
      "loss: 0.133733  [  224/ 3200]\n",
      "loss: 0.224651  [  240/ 3200]\n",
      "loss: 0.409599  [  256/ 3200]\n",
      "loss: 0.064376  [  272/ 3200]\n",
      "loss: 0.083429  [  288/ 3200]\n",
      "loss: 0.158239  [  304/ 3200]\n",
      "loss: 0.361063  [  320/ 3200]\n",
      "loss: 0.391424  [  336/ 3200]\n",
      "loss: 0.194129  [  352/ 3200]\n",
      "loss: 0.272088  [  368/ 3200]\n",
      "loss: 0.307061  [  384/ 3200]\n",
      "loss: 0.448997  [  400/ 3200]\n",
      "loss: 0.165953  [  416/ 3200]\n",
      "loss: 0.393923  [  432/ 3200]\n",
      "loss: 0.174249  [  448/ 3200]\n",
      "loss: 0.141328  [  464/ 3200]\n",
      "loss: 0.115318  [  480/ 3200]\n",
      "loss: 0.290658  [  496/ 3200]\n",
      "loss: 0.264897  [  512/ 3200]\n",
      "loss: 0.317068  [  528/ 3200]\n",
      "loss: 0.246739  [  544/ 3200]\n",
      "loss: 0.271678  [  560/ 3200]\n",
      "loss: 0.171716  [  576/ 3200]\n",
      "loss: 0.252413  [  592/ 3200]\n",
      "loss: 0.283484  [  608/ 3200]\n",
      "loss: 0.312994  [  624/ 3200]\n",
      "loss: 0.413542  [  640/ 3200]\n",
      "loss: 0.253373  [  656/ 3200]\n",
      "loss: 0.198061  [  672/ 3200]\n",
      "loss: 0.446602  [  688/ 3200]\n",
      "loss: 0.335507  [  704/ 3200]\n",
      "loss: 0.321141  [  720/ 3200]\n",
      "loss: 0.374251  [  736/ 3200]\n",
      "loss: 0.123629  [  752/ 3200]\n",
      "loss: 0.340299  [  768/ 3200]\n",
      "loss: 0.223095  [  784/ 3200]\n",
      "loss: 0.109520  [  800/ 3200]\n",
      "loss: 0.148275  [  816/ 3200]\n",
      "loss: 0.308530  [  832/ 3200]\n",
      "loss: 0.204628  [  848/ 3200]\n",
      "loss: 0.052566  [  864/ 3200]\n",
      "loss: 0.335784  [  880/ 3200]\n",
      "loss: 0.146941  [  896/ 3200]\n",
      "loss: 0.106315  [  912/ 3200]\n",
      "loss: 0.388775  [  928/ 3200]\n",
      "loss: 0.312914  [  944/ 3200]\n",
      "loss: 0.374767  [  960/ 3200]\n",
      "loss: 0.189036  [  976/ 3200]\n",
      "loss: 0.234694  [  992/ 3200]\n",
      "loss: 0.184502  [ 1008/ 3200]\n",
      "loss: 0.380219  [ 1024/ 3200]\n",
      "loss: 0.200496  [ 1040/ 3200]\n",
      "loss: 0.224717  [ 1056/ 3200]\n",
      "loss: 0.383347  [ 1072/ 3200]\n",
      "loss: 0.393150  [ 1088/ 3200]\n",
      "loss: 0.394006  [ 1104/ 3200]\n",
      "loss: 0.255112  [ 1120/ 3200]\n",
      "loss: 0.184537  [ 1136/ 3200]\n",
      "loss: 0.356103  [ 1152/ 3200]\n",
      "loss: 0.292972  [ 1168/ 3200]\n",
      "loss: 0.307912  [ 1184/ 3200]\n",
      "loss: 0.388711  [ 1200/ 3200]\n",
      "loss: 0.290724  [ 1216/ 3200]\n",
      "loss: 0.213953  [ 1232/ 3200]\n",
      "loss: 0.210714  [ 1248/ 3200]\n",
      "loss: 0.126082  [ 1264/ 3200]\n",
      "loss: 0.340180  [ 1280/ 3200]\n",
      "loss: 0.193833  [ 1296/ 3200]\n",
      "loss: 0.504730  [ 1312/ 3200]\n",
      "loss: 0.226473  [ 1328/ 3200]\n",
      "loss: 0.059778  [ 1344/ 3200]\n",
      "loss: 0.095314  [ 1360/ 3200]\n",
      "loss: 0.256706  [ 1376/ 3200]\n",
      "loss: 0.183707  [ 1392/ 3200]\n",
      "loss: 0.408072  [ 1408/ 3200]\n",
      "loss: 0.266299  [ 1424/ 3200]\n",
      "loss: 0.233361  [ 1440/ 3200]\n",
      "loss: 0.045921  [ 1456/ 3200]\n",
      "loss: 0.091195  [ 1472/ 3200]\n",
      "loss: 0.308910  [ 1488/ 3200]\n",
      "loss: 0.319567  [ 1504/ 3200]\n",
      "loss: 0.207134  [ 1520/ 3200]\n",
      "loss: 0.128790  [ 1536/ 3200]\n",
      "loss: 0.264614  [ 1552/ 3200]\n",
      "loss: 0.648715  [ 1568/ 3200]\n",
      "loss: 0.207976  [ 1584/ 3200]\n",
      "loss: 0.123096  [ 1600/ 3200]\n",
      "loss: 0.337099  [ 1616/ 3200]\n",
      "loss: 0.110942  [ 1632/ 3200]\n",
      "loss: 0.039289  [ 1648/ 3200]\n",
      "loss: 0.130236  [ 1664/ 3200]\n",
      "loss: 0.281140  [ 1680/ 3200]\n",
      "loss: 0.268851  [ 1696/ 3200]\n",
      "loss: 0.051933  [ 1712/ 3200]\n",
      "loss: 0.184523  [ 1728/ 3200]\n",
      "loss: 0.339746  [ 1744/ 3200]\n",
      "loss: 0.169448  [ 1760/ 3200]\n",
      "loss: 0.311141  [ 1776/ 3200]\n",
      "loss: 0.183266  [ 1792/ 3200]\n",
      "loss: 0.146925  [ 1808/ 3200]\n",
      "loss: 0.361202  [ 1824/ 3200]\n",
      "loss: 0.337087  [ 1840/ 3200]\n",
      "loss: 0.256995  [ 1856/ 3200]\n",
      "loss: 0.093656  [ 1872/ 3200]\n",
      "loss: 0.104985  [ 1888/ 3200]\n",
      "loss: 0.117062  [ 1904/ 3200]\n",
      "loss: 0.207812  [ 1920/ 3200]\n",
      "loss: 0.158430  [ 1936/ 3200]\n",
      "loss: 0.175660  [ 1952/ 3200]\n",
      "loss: 0.171581  [ 1968/ 3200]\n",
      "loss: 0.448316  [ 1984/ 3200]\n",
      "loss: 0.282970  [ 2000/ 3200]\n",
      "loss: 0.123597  [ 2016/ 3200]\n",
      "loss: 0.073590  [ 2032/ 3200]\n",
      "loss: 0.090458  [ 2048/ 3200]\n",
      "loss: 0.255833  [ 2064/ 3200]\n",
      "loss: 0.407367  [ 2080/ 3200]\n",
      "loss: 0.788834  [ 2096/ 3200]\n",
      "loss: 0.171949  [ 2112/ 3200]\n",
      "loss: 0.162008  [ 2128/ 3200]\n",
      "loss: 0.070483  [ 2144/ 3200]\n",
      "loss: 0.437554  [ 2160/ 3200]\n",
      "loss: 0.295404  [ 2176/ 3200]\n",
      "loss: 0.175013  [ 2192/ 3200]\n",
      "loss: 0.147978  [ 2208/ 3200]\n",
      "loss: 0.265079  [ 2224/ 3200]\n",
      "loss: 0.423287  [ 2240/ 3200]\n",
      "loss: 0.212110  [ 2256/ 3200]\n",
      "loss: 0.121387  [ 2272/ 3200]\n",
      "loss: 0.102590  [ 2288/ 3200]\n",
      "loss: 0.122789  [ 2304/ 3200]\n",
      "loss: 0.476015  [ 2320/ 3200]\n",
      "loss: 0.159133  [ 2336/ 3200]\n",
      "loss: 0.212505  [ 2352/ 3200]\n",
      "loss: 0.199000  [ 2368/ 3200]\n",
      "loss: 0.423481  [ 2384/ 3200]\n",
      "loss: 0.431068  [ 2400/ 3200]\n",
      "loss: 0.193995  [ 2416/ 3200]\n",
      "loss: 0.214953  [ 2432/ 3200]\n",
      "loss: 0.288943  [ 2448/ 3200]\n",
      "loss: 0.131702  [ 2464/ 3200]\n",
      "loss: 0.393456  [ 2480/ 3200]\n",
      "loss: 0.135360  [ 2496/ 3200]\n",
      "loss: 0.115056  [ 2512/ 3200]\n",
      "loss: 0.072694  [ 2528/ 3200]\n",
      "loss: 0.172041  [ 2544/ 3200]\n",
      "loss: 0.173221  [ 2560/ 3200]\n",
      "loss: 0.139396  [ 2576/ 3200]\n",
      "loss: 0.160079  [ 2592/ 3200]\n",
      "loss: 0.217868  [ 2608/ 3200]\n",
      "loss: 0.406341  [ 2624/ 3200]\n",
      "loss: 0.645603  [ 2640/ 3200]\n",
      "loss: 0.240112  [ 2656/ 3200]\n",
      "loss: 0.220806  [ 2672/ 3200]\n",
      "loss: 0.574745  [ 2688/ 3200]\n",
      "loss: 0.346218  [ 2704/ 3200]\n",
      "loss: 0.215739  [ 2720/ 3200]\n",
      "loss: 0.453239  [ 2736/ 3200]\n",
      "loss: 0.434761  [ 2752/ 3200]\n",
      "loss: 0.335273  [ 2768/ 3200]\n",
      "loss: 0.103935  [ 2784/ 3200]\n",
      "loss: 0.308006  [ 2800/ 3200]\n",
      "loss: 0.173929  [ 2816/ 3200]\n",
      "loss: 0.242506  [ 2832/ 3200]\n",
      "loss: 0.120585  [ 2848/ 3200]\n",
      "loss: 0.248109  [ 2864/ 3200]\n",
      "loss: 0.218154  [ 2880/ 3200]\n",
      "loss: 0.246960  [ 2896/ 3200]\n",
      "loss: 0.089793  [ 2912/ 3200]\n",
      "loss: 0.111111  [ 2928/ 3200]\n",
      "loss: 0.485725  [ 2944/ 3200]\n",
      "loss: 0.426231  [ 2960/ 3200]\n",
      "loss: 0.129193  [ 2976/ 3200]\n",
      "loss: 0.409357  [ 2992/ 3200]\n",
      "loss: 0.316758  [ 3008/ 3200]\n",
      "loss: 0.267439  [ 3024/ 3200]\n",
      "loss: 0.216841  [ 3040/ 3200]\n",
      "loss: 0.137839  [ 3056/ 3200]\n",
      "loss: 0.212993  [ 3072/ 3200]\n",
      "loss: 0.584428  [ 3088/ 3200]\n",
      "loss: 0.195544  [ 3104/ 3200]\n",
      "loss: 0.197244  [ 3120/ 3200]\n",
      "loss: 0.155091  [ 3136/ 3200]\n",
      "loss: 0.162848  [ 3152/ 3200]\n",
      "loss: 0.466290  [ 3168/ 3200]\n",
      "loss: 0.653463  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038631\n",
      "f1 macro averaged score: 0.770575\n",
      "Accuracy               : 77.5%\n",
      "Confusion matrix       :\n",
      "tensor([[185,   7,   0,   8],\n",
      "        [ 17, 112,  20,  51],\n",
      "        [  0,  24, 157,  19],\n",
      "        [ 14,   7,  13, 166]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0267e-03.\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 0.206989  [    0/ 3200]\n",
      "loss: 0.288471  [   16/ 3200]\n",
      "loss: 0.108480  [   32/ 3200]\n",
      "loss: 0.302989  [   48/ 3200]\n",
      "loss: 0.184482  [   64/ 3200]\n",
      "loss: 0.123780  [   80/ 3200]\n",
      "loss: 0.127959  [   96/ 3200]\n",
      "loss: 0.140127  [  112/ 3200]\n",
      "loss: 0.043562  [  128/ 3200]\n",
      "loss: 0.216736  [  144/ 3200]\n",
      "loss: 0.113862  [  160/ 3200]\n",
      "loss: 0.091060  [  176/ 3200]\n",
      "loss: 0.074980  [  192/ 3200]\n",
      "loss: 0.144296  [  208/ 3200]\n",
      "loss: 0.315018  [  224/ 3200]\n",
      "loss: 0.172500  [  240/ 3200]\n",
      "loss: 0.179929  [  256/ 3200]\n",
      "loss: 0.232273  [  272/ 3200]\n",
      "loss: 0.262038  [  288/ 3200]\n",
      "loss: 0.251420  [  304/ 3200]\n",
      "loss: 0.348201  [  320/ 3200]\n",
      "loss: 0.165241  [  336/ 3200]\n",
      "loss: 0.252375  [  352/ 3200]\n",
      "loss: 0.137132  [  368/ 3200]\n",
      "loss: 0.211059  [  384/ 3200]\n",
      "loss: 0.401705  [  400/ 3200]\n",
      "loss: 0.280445  [  416/ 3200]\n",
      "loss: 0.203447  [  432/ 3200]\n",
      "loss: 0.054491  [  448/ 3200]\n",
      "loss: 0.288422  [  464/ 3200]\n",
      "loss: 0.336422  [  480/ 3200]\n",
      "loss: 0.043490  [  496/ 3200]\n",
      "loss: 0.331113  [  512/ 3200]\n",
      "loss: 0.039603  [  528/ 3200]\n",
      "loss: 0.048861  [  544/ 3200]\n",
      "loss: 0.539565  [  560/ 3200]\n",
      "loss: 0.130582  [  576/ 3200]\n",
      "loss: 0.301419  [  592/ 3200]\n",
      "loss: 0.509396  [  608/ 3200]\n",
      "loss: 0.168768  [  624/ 3200]\n",
      "loss: 0.107894  [  640/ 3200]\n",
      "loss: 0.192648  [  656/ 3200]\n",
      "loss: 0.321852  [  672/ 3200]\n",
      "loss: 0.228542  [  688/ 3200]\n",
      "loss: 0.277959  [  704/ 3200]\n",
      "loss: 0.242339  [  720/ 3200]\n",
      "loss: 0.275058  [  736/ 3200]\n",
      "loss: 0.367470  [  752/ 3200]\n",
      "loss: 0.137141  [  768/ 3200]\n",
      "loss: 0.065639  [  784/ 3200]\n",
      "loss: 0.103964  [  800/ 3200]\n",
      "loss: 0.376575  [  816/ 3200]\n",
      "loss: 0.286549  [  832/ 3200]\n",
      "loss: 0.188154  [  848/ 3200]\n",
      "loss: 0.186735  [  864/ 3200]\n",
      "loss: 0.404242  [  880/ 3200]\n",
      "loss: 0.149362  [  896/ 3200]\n",
      "loss: 0.453618  [  912/ 3200]\n",
      "loss: 0.158515  [  928/ 3200]\n",
      "loss: 0.423800  [  944/ 3200]\n",
      "loss: 0.144573  [  960/ 3200]\n",
      "loss: 0.057394  [  976/ 3200]\n",
      "loss: 0.359916  [  992/ 3200]\n",
      "loss: 0.176058  [ 1008/ 3200]\n",
      "loss: 0.287626  [ 1024/ 3200]\n",
      "loss: 0.222539  [ 1040/ 3200]\n",
      "loss: 0.363340  [ 1056/ 3200]\n",
      "loss: 0.121987  [ 1072/ 3200]\n",
      "loss: 0.309984  [ 1088/ 3200]\n",
      "loss: 0.107479  [ 1104/ 3200]\n",
      "loss: 0.170216  [ 1120/ 3200]\n",
      "loss: 0.054573  [ 1136/ 3200]\n",
      "loss: 0.210381  [ 1152/ 3200]\n",
      "loss: 0.483441  [ 1168/ 3200]\n",
      "loss: 0.387137  [ 1184/ 3200]\n",
      "loss: 0.264095  [ 1200/ 3200]\n",
      "loss: 0.088635  [ 1216/ 3200]\n",
      "loss: 0.287510  [ 1232/ 3200]\n",
      "loss: 0.573331  [ 1248/ 3200]\n",
      "loss: 0.129230  [ 1264/ 3200]\n",
      "loss: 0.132184  [ 1280/ 3200]\n",
      "loss: 0.275024  [ 1296/ 3200]\n",
      "loss: 0.121295  [ 1312/ 3200]\n",
      "loss: 0.380878  [ 1328/ 3200]\n",
      "loss: 0.217803  [ 1344/ 3200]\n",
      "loss: 0.125142  [ 1360/ 3200]\n",
      "loss: 0.132041  [ 1376/ 3200]\n",
      "loss: 0.034086  [ 1392/ 3200]\n",
      "loss: 0.161614  [ 1408/ 3200]\n",
      "loss: 0.113943  [ 1424/ 3200]\n",
      "loss: 0.213398  [ 1440/ 3200]\n",
      "loss: 0.098529  [ 1456/ 3200]\n",
      "loss: 0.400069  [ 1472/ 3200]\n",
      "loss: 0.341454  [ 1488/ 3200]\n",
      "loss: 0.098109  [ 1504/ 3200]\n",
      "loss: 0.254831  [ 1520/ 3200]\n",
      "loss: 0.202726  [ 1536/ 3200]\n",
      "loss: 0.348174  [ 1552/ 3200]\n",
      "loss: 0.230499  [ 1568/ 3200]\n",
      "loss: 0.287649  [ 1584/ 3200]\n",
      "loss: 0.270887  [ 1600/ 3200]\n",
      "loss: 0.174630  [ 1616/ 3200]\n",
      "loss: 0.497288  [ 1632/ 3200]\n",
      "loss: 0.121644  [ 1648/ 3200]\n",
      "loss: 0.220861  [ 1664/ 3200]\n",
      "loss: 0.302247  [ 1680/ 3200]\n",
      "loss: 0.234754  [ 1696/ 3200]\n",
      "loss: 0.220855  [ 1712/ 3200]\n",
      "loss: 0.286782  [ 1728/ 3200]\n",
      "loss: 0.108138  [ 1744/ 3200]\n",
      "loss: 0.250389  [ 1760/ 3200]\n",
      "loss: 0.392425  [ 1776/ 3200]\n",
      "loss: 0.139013  [ 1792/ 3200]\n",
      "loss: 0.481724  [ 1808/ 3200]\n",
      "loss: 0.113325  [ 1824/ 3200]\n",
      "loss: 0.185262  [ 1840/ 3200]\n",
      "loss: 0.167939  [ 1856/ 3200]\n",
      "loss: 0.127425  [ 1872/ 3200]\n",
      "loss: 0.078449  [ 1888/ 3200]\n",
      "loss: 0.219228  [ 1904/ 3200]\n",
      "loss: 0.121831  [ 1920/ 3200]\n",
      "loss: 0.135491  [ 1936/ 3200]\n",
      "loss: 0.302343  [ 1952/ 3200]\n",
      "loss: 0.132689  [ 1968/ 3200]\n",
      "loss: 0.026460  [ 1984/ 3200]\n",
      "loss: 0.234988  [ 2000/ 3200]\n",
      "loss: 0.088073  [ 2016/ 3200]\n",
      "loss: 0.545932  [ 2032/ 3200]\n",
      "loss: 0.341579  [ 2048/ 3200]\n",
      "loss: 0.245015  [ 2064/ 3200]\n",
      "loss: 0.352359  [ 2080/ 3200]\n",
      "loss: 0.427600  [ 2096/ 3200]\n",
      "loss: 0.275589  [ 2112/ 3200]\n",
      "loss: 0.239441  [ 2128/ 3200]\n",
      "loss: 0.313910  [ 2144/ 3200]\n",
      "loss: 0.113481  [ 2160/ 3200]\n",
      "loss: 0.108651  [ 2176/ 3200]\n",
      "loss: 0.194609  [ 2192/ 3200]\n",
      "loss: 0.162397  [ 2208/ 3200]\n",
      "loss: 0.332398  [ 2224/ 3200]\n",
      "loss: 0.159684  [ 2240/ 3200]\n",
      "loss: 0.204372  [ 2256/ 3200]\n",
      "loss: 0.451707  [ 2272/ 3200]\n",
      "loss: 0.135048  [ 2288/ 3200]\n",
      "loss: 0.107805  [ 2304/ 3200]\n",
      "loss: 0.098748  [ 2320/ 3200]\n",
      "loss: 0.268635  [ 2336/ 3200]\n",
      "loss: 0.144551  [ 2352/ 3200]\n",
      "loss: 0.179910  [ 2368/ 3200]\n",
      "loss: 0.306986  [ 2384/ 3200]\n",
      "loss: 0.277958  [ 2400/ 3200]\n",
      "loss: 0.302507  [ 2416/ 3200]\n",
      "loss: 0.175750  [ 2432/ 3200]\n",
      "loss: 0.451578  [ 2448/ 3200]\n",
      "loss: 0.346067  [ 2464/ 3200]\n",
      "loss: 0.107815  [ 2480/ 3200]\n",
      "loss: 0.108835  [ 2496/ 3200]\n",
      "loss: 0.367481  [ 2512/ 3200]\n",
      "loss: 0.102349  [ 2528/ 3200]\n",
      "loss: 0.196103  [ 2544/ 3200]\n",
      "loss: 0.043321  [ 2560/ 3200]\n",
      "loss: 0.315201  [ 2576/ 3200]\n",
      "loss: 0.184924  [ 2592/ 3200]\n",
      "loss: 0.357544  [ 2608/ 3200]\n",
      "loss: 0.178353  [ 2624/ 3200]\n",
      "loss: 0.079584  [ 2640/ 3200]\n",
      "loss: 0.239330  [ 2656/ 3200]\n",
      "loss: 0.124143  [ 2672/ 3200]\n",
      "loss: 0.192350  [ 2688/ 3200]\n",
      "loss: 0.212658  [ 2704/ 3200]\n",
      "loss: 0.176414  [ 2720/ 3200]\n",
      "loss: 0.282273  [ 2736/ 3200]\n",
      "loss: 0.432618  [ 2752/ 3200]\n",
      "loss: 0.346502  [ 2768/ 3200]\n",
      "loss: 0.259897  [ 2784/ 3200]\n",
      "loss: 0.243547  [ 2800/ 3200]\n",
      "loss: 0.165630  [ 2816/ 3200]\n",
      "loss: 0.317248  [ 2832/ 3200]\n",
      "loss: 0.225710  [ 2848/ 3200]\n",
      "loss: 0.209560  [ 2864/ 3200]\n",
      "loss: 0.153886  [ 2880/ 3200]\n",
      "loss: 0.208176  [ 2896/ 3200]\n",
      "loss: 0.162348  [ 2912/ 3200]\n",
      "loss: 0.132847  [ 2928/ 3200]\n",
      "loss: 0.049607  [ 2944/ 3200]\n",
      "loss: 0.235663  [ 2960/ 3200]\n",
      "loss: 0.143471  [ 2976/ 3200]\n",
      "loss: 0.052515  [ 2992/ 3200]\n",
      "loss: 0.069444  [ 3008/ 3200]\n",
      "loss: 0.097445  [ 3024/ 3200]\n",
      "loss: 0.724452  [ 3040/ 3200]\n",
      "loss: 0.323753  [ 3056/ 3200]\n",
      "loss: 0.240925  [ 3072/ 3200]\n",
      "loss: 0.061535  [ 3088/ 3200]\n",
      "loss: 0.329982  [ 3104/ 3200]\n",
      "loss: 0.193379  [ 3120/ 3200]\n",
      "loss: 0.052783  [ 3136/ 3200]\n",
      "loss: 0.379005  [ 3152/ 3200]\n",
      "loss: 0.133639  [ 3168/ 3200]\n",
      "loss: 0.161833  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038360\n",
      "f1 macro averaged score: 0.780449\n",
      "Accuracy               : 78.6%\n",
      "Confusion matrix       :\n",
      "tensor([[186,   4,   0,  10],\n",
      "        [ 17, 111,  30,  42],\n",
      "        [  0,  16, 170,  14],\n",
      "        [ 12,  11,  15, 162]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.7535e-04.\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.191459  [    0/ 3200]\n",
      "loss: 0.403886  [   16/ 3200]\n",
      "loss: 0.186185  [   32/ 3200]\n",
      "loss: 0.179523  [   48/ 3200]\n",
      "loss: 0.164208  [   64/ 3200]\n",
      "loss: 0.365714  [   80/ 3200]\n",
      "loss: 0.109968  [   96/ 3200]\n",
      "loss: 0.376517  [  112/ 3200]\n",
      "loss: 0.139246  [  128/ 3200]\n",
      "loss: 0.063413  [  144/ 3200]\n",
      "loss: 0.258719  [  160/ 3200]\n",
      "loss: 0.158208  [  176/ 3200]\n",
      "loss: 0.228817  [  192/ 3200]\n",
      "loss: 0.284726  [  208/ 3200]\n",
      "loss: 0.204224  [  224/ 3200]\n",
      "loss: 0.349816  [  240/ 3200]\n",
      "loss: 0.157348  [  256/ 3200]\n",
      "loss: 0.131447  [  272/ 3200]\n",
      "loss: 0.130810  [  288/ 3200]\n",
      "loss: 0.079279  [  304/ 3200]\n",
      "loss: 0.150065  [  320/ 3200]\n",
      "loss: 0.267370  [  336/ 3200]\n",
      "loss: 0.073490  [  352/ 3200]\n",
      "loss: 0.173667  [  368/ 3200]\n",
      "loss: 0.204451  [  384/ 3200]\n",
      "loss: 0.143913  [  400/ 3200]\n",
      "loss: 0.107466  [  416/ 3200]\n",
      "loss: 0.218685  [  432/ 3200]\n",
      "loss: 0.342176  [  448/ 3200]\n",
      "loss: 0.118293  [  464/ 3200]\n",
      "loss: 0.310380  [  480/ 3200]\n",
      "loss: 0.313367  [  496/ 3200]\n",
      "loss: 0.314836  [  512/ 3200]\n",
      "loss: 0.137443  [  528/ 3200]\n",
      "loss: 0.318148  [  544/ 3200]\n",
      "loss: 0.321539  [  560/ 3200]\n",
      "loss: 0.243482  [  576/ 3200]\n",
      "loss: 0.491955  [  592/ 3200]\n",
      "loss: 0.494870  [  608/ 3200]\n",
      "loss: 0.180312  [  624/ 3200]\n",
      "loss: 0.094241  [  640/ 3200]\n",
      "loss: 0.072292  [  656/ 3200]\n",
      "loss: 0.189179  [  672/ 3200]\n",
      "loss: 0.124592  [  688/ 3200]\n",
      "loss: 0.334306  [  704/ 3200]\n",
      "loss: 0.193286  [  720/ 3200]\n",
      "loss: 0.050547  [  736/ 3200]\n",
      "loss: 0.121732  [  752/ 3200]\n",
      "loss: 0.124838  [  768/ 3200]\n",
      "loss: 0.191982  [  784/ 3200]\n",
      "loss: 0.387052  [  800/ 3200]\n",
      "loss: 0.192821  [  816/ 3200]\n",
      "loss: 0.122365  [  832/ 3200]\n",
      "loss: 0.159766  [  848/ 3200]\n",
      "loss: 0.061431  [  864/ 3200]\n",
      "loss: 0.208515  [  880/ 3200]\n",
      "loss: 0.189185  [  896/ 3200]\n",
      "loss: 0.116955  [  912/ 3200]\n",
      "loss: 0.213031  [  928/ 3200]\n",
      "loss: 0.070794  [  944/ 3200]\n",
      "loss: 0.206267  [  960/ 3200]\n",
      "loss: 0.141905  [  976/ 3200]\n",
      "loss: 0.066725  [  992/ 3200]\n",
      "loss: 0.210557  [ 1008/ 3200]\n",
      "loss: 0.314271  [ 1024/ 3200]\n",
      "loss: 0.122359  [ 1040/ 3200]\n",
      "loss: 0.143129  [ 1056/ 3200]\n",
      "loss: 0.177282  [ 1072/ 3200]\n",
      "loss: 0.143866  [ 1088/ 3200]\n",
      "loss: 0.140139  [ 1104/ 3200]\n",
      "loss: 0.164512  [ 1120/ 3200]\n",
      "loss: 0.070089  [ 1136/ 3200]\n",
      "loss: 0.168379  [ 1152/ 3200]\n",
      "loss: 0.149842  [ 1168/ 3200]\n",
      "loss: 0.102619  [ 1184/ 3200]\n",
      "loss: 0.083906  [ 1200/ 3200]\n",
      "loss: 0.149795  [ 1216/ 3200]\n",
      "loss: 0.140725  [ 1232/ 3200]\n",
      "loss: 0.115881  [ 1248/ 3200]\n",
      "loss: 0.185384  [ 1264/ 3200]\n",
      "loss: 0.057366  [ 1280/ 3200]\n",
      "loss: 0.064801  [ 1296/ 3200]\n",
      "loss: 0.120495  [ 1312/ 3200]\n",
      "loss: 0.173698  [ 1328/ 3200]\n",
      "loss: 0.353632  [ 1344/ 3200]\n",
      "loss: 0.361337  [ 1360/ 3200]\n",
      "loss: 0.083814  [ 1376/ 3200]\n",
      "loss: 0.081467  [ 1392/ 3200]\n",
      "loss: 0.295025  [ 1408/ 3200]\n",
      "loss: 0.098903  [ 1424/ 3200]\n",
      "loss: 0.398176  [ 1440/ 3200]\n",
      "loss: 0.084634  [ 1456/ 3200]\n",
      "loss: 0.128194  [ 1472/ 3200]\n",
      "loss: 0.402574  [ 1488/ 3200]\n",
      "loss: 0.437623  [ 1504/ 3200]\n",
      "loss: 0.052230  [ 1520/ 3200]\n",
      "loss: 0.204977  [ 1536/ 3200]\n",
      "loss: 0.115263  [ 1552/ 3200]\n",
      "loss: 0.198991  [ 1568/ 3200]\n",
      "loss: 0.145646  [ 1584/ 3200]\n",
      "loss: 0.198542  [ 1600/ 3200]\n",
      "loss: 0.097765  [ 1616/ 3200]\n",
      "loss: 0.105691  [ 1632/ 3200]\n",
      "loss: 0.155167  [ 1648/ 3200]\n",
      "loss: 0.383843  [ 1664/ 3200]\n",
      "loss: 0.393130  [ 1680/ 3200]\n",
      "loss: 0.171995  [ 1696/ 3200]\n",
      "loss: 0.223887  [ 1712/ 3200]\n",
      "loss: 0.095093  [ 1728/ 3200]\n",
      "loss: 0.568499  [ 1744/ 3200]\n",
      "loss: 0.177232  [ 1760/ 3200]\n",
      "loss: 0.196052  [ 1776/ 3200]\n",
      "loss: 0.331733  [ 1792/ 3200]\n",
      "loss: 0.229267  [ 1808/ 3200]\n",
      "loss: 0.289092  [ 1824/ 3200]\n",
      "loss: 0.119275  [ 1840/ 3200]\n",
      "loss: 0.347744  [ 1856/ 3200]\n",
      "loss: 0.119776  [ 1872/ 3200]\n",
      "loss: 0.189288  [ 1888/ 3200]\n",
      "loss: 0.732835  [ 1904/ 3200]\n",
      "loss: 0.221285  [ 1920/ 3200]\n",
      "loss: 0.318423  [ 1936/ 3200]\n",
      "loss: 0.174602  [ 1952/ 3200]\n",
      "loss: 0.112076  [ 1968/ 3200]\n",
      "loss: 0.134279  [ 1984/ 3200]\n",
      "loss: 0.092869  [ 2000/ 3200]\n",
      "loss: 0.153109  [ 2016/ 3200]\n",
      "loss: 0.097837  [ 2032/ 3200]\n",
      "loss: 0.192215  [ 2048/ 3200]\n",
      "loss: 0.134357  [ 2064/ 3200]\n",
      "loss: 0.263231  [ 2080/ 3200]\n",
      "loss: 0.233209  [ 2096/ 3200]\n",
      "loss: 0.214314  [ 2112/ 3200]\n",
      "loss: 0.090779  [ 2128/ 3200]\n",
      "loss: 0.232463  [ 2144/ 3200]\n",
      "loss: 0.265135  [ 2160/ 3200]\n",
      "loss: 0.201731  [ 2176/ 3200]\n",
      "loss: 0.278053  [ 2192/ 3200]\n",
      "loss: 0.223157  [ 2208/ 3200]\n",
      "loss: 0.223277  [ 2224/ 3200]\n",
      "loss: 0.311723  [ 2240/ 3200]\n",
      "loss: 0.179603  [ 2256/ 3200]\n",
      "loss: 0.287781  [ 2272/ 3200]\n",
      "loss: 0.207990  [ 2288/ 3200]\n",
      "loss: 0.244074  [ 2304/ 3200]\n",
      "loss: 0.120169  [ 2320/ 3200]\n",
      "loss: 0.287325  [ 2336/ 3200]\n",
      "loss: 0.204217  [ 2352/ 3200]\n",
      "loss: 0.073551  [ 2368/ 3200]\n",
      "loss: 0.147606  [ 2384/ 3200]\n",
      "loss: 0.056362  [ 2400/ 3200]\n",
      "loss: 0.203266  [ 2416/ 3200]\n",
      "loss: 0.349617  [ 2432/ 3200]\n",
      "loss: 0.120428  [ 2448/ 3200]\n",
      "loss: 0.313262  [ 2464/ 3200]\n",
      "loss: 0.752820  [ 2480/ 3200]\n",
      "loss: 0.055364  [ 2496/ 3200]\n",
      "loss: 0.084009  [ 2512/ 3200]\n",
      "loss: 0.123981  [ 2528/ 3200]\n",
      "loss: 0.199214  [ 2544/ 3200]\n",
      "loss: 0.109097  [ 2560/ 3200]\n",
      "loss: 0.186592  [ 2576/ 3200]\n",
      "loss: 0.162383  [ 2592/ 3200]\n",
      "loss: 0.134439  [ 2608/ 3200]\n",
      "loss: 0.058849  [ 2624/ 3200]\n",
      "loss: 0.136089  [ 2640/ 3200]\n",
      "loss: 0.285063  [ 2656/ 3200]\n",
      "loss: 0.309639  [ 2672/ 3200]\n",
      "loss: 0.147526  [ 2688/ 3200]\n",
      "loss: 0.105017  [ 2704/ 3200]\n",
      "loss: 0.143823  [ 2720/ 3200]\n",
      "loss: 0.191222  [ 2736/ 3200]\n",
      "loss: 0.139729  [ 2752/ 3200]\n",
      "loss: 0.258669  [ 2768/ 3200]\n",
      "loss: 0.171062  [ 2784/ 3200]\n",
      "loss: 0.205981  [ 2800/ 3200]\n",
      "loss: 0.210330  [ 2816/ 3200]\n",
      "loss: 0.098800  [ 2832/ 3200]\n",
      "loss: 0.347137  [ 2848/ 3200]\n",
      "loss: 0.182836  [ 2864/ 3200]\n",
      "loss: 0.101169  [ 2880/ 3200]\n",
      "loss: 0.124574  [ 2896/ 3200]\n",
      "loss: 0.066097  [ 2912/ 3200]\n",
      "loss: 0.099641  [ 2928/ 3200]\n",
      "loss: 0.119688  [ 2944/ 3200]\n",
      "loss: 0.060616  [ 2960/ 3200]\n",
      "loss: 0.327785  [ 2976/ 3200]\n",
      "loss: 0.204020  [ 2992/ 3200]\n",
      "loss: 0.436429  [ 3008/ 3200]\n",
      "loss: 0.323006  [ 3024/ 3200]\n",
      "loss: 0.117387  [ 3040/ 3200]\n",
      "loss: 0.281558  [ 3056/ 3200]\n",
      "loss: 0.281528  [ 3072/ 3200]\n",
      "loss: 0.294306  [ 3088/ 3200]\n",
      "loss: 0.600222  [ 3104/ 3200]\n",
      "loss: 0.063090  [ 3120/ 3200]\n",
      "loss: 0.193357  [ 3136/ 3200]\n",
      "loss: 0.181395  [ 3152/ 3200]\n",
      "loss: 0.124854  [ 3168/ 3200]\n",
      "loss: 0.138204  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.041427\n",
      "f1 macro averaged score: 0.760280\n",
      "Accuracy               : 75.9%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  13,   0,  15],\n",
      "        [ 15, 125,   9,  51],\n",
      "        [  0,  34, 141,  25],\n",
      "        [  4,  18,   9, 169]], device='cuda:0')\n",
      "Test Error:\n",
      "Avg loss               : 0.041531\n",
      "f1 macro averaged score: 0.781964\n",
      "Accuracy               : 77.6%\n",
      "Confusion matrix       :\n",
      "tensor([[270,  18,   1,   8],\n",
      "        [ 10, 216,  16,  82],\n",
      "        [  3,  46, 268,  39],\n",
      "        [  9,  55,  21, 314]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "  Patience: 10\n",
      "Epoch: 1\n",
      "-----------------------------\n",
      "loss: 1.410910  [    0/ 3200]\n",
      "loss: 3.237996  [   16/ 3200]\n",
      "loss: 1.761683  [   32/ 3200]\n",
      "loss: 2.279021  [   48/ 3200]\n",
      "loss: 1.921307  [   64/ 3200]\n",
      "loss: 1.200242  [   80/ 3200]\n",
      "loss: 1.876231  [   96/ 3200]\n",
      "loss: 2.124714  [  112/ 3200]\n",
      "loss: 1.392274  [  128/ 3200]\n",
      "loss: 1.407979  [  144/ 3200]\n",
      "loss: 1.399582  [  160/ 3200]\n",
      "loss: 1.316472  [  176/ 3200]\n",
      "loss: 1.561095  [  192/ 3200]\n",
      "loss: 1.394648  [  208/ 3200]\n",
      "loss: 1.735171  [  224/ 3200]\n",
      "loss: 1.403881  [  240/ 3200]\n",
      "loss: 1.417257  [  256/ 3200]\n",
      "loss: 1.428851  [  272/ 3200]\n",
      "loss: 1.478123  [  288/ 3200]\n",
      "loss: 1.375917  [  304/ 3200]\n",
      "loss: 1.415263  [  320/ 3200]\n",
      "loss: 1.387382  [  336/ 3200]\n",
      "loss: 1.356661  [  352/ 3200]\n",
      "loss: 1.346226  [  368/ 3200]\n",
      "loss: 1.377258  [  384/ 3200]\n",
      "loss: 1.358207  [  400/ 3200]\n",
      "loss: 1.485980  [  416/ 3200]\n",
      "loss: 1.352431  [  432/ 3200]\n",
      "loss: 1.390954  [  448/ 3200]\n",
      "loss: 1.354195  [  464/ 3200]\n",
      "loss: 1.364135  [  480/ 3200]\n",
      "loss: 1.372646  [  496/ 3200]\n",
      "loss: 1.437072  [  512/ 3200]\n",
      "loss: 1.378598  [  528/ 3200]\n",
      "loss: 1.402828  [  544/ 3200]\n",
      "loss: 1.382319  [  560/ 3200]\n",
      "loss: 1.372618  [  576/ 3200]\n",
      "loss: 1.367445  [  592/ 3200]\n",
      "loss: 1.277654  [  608/ 3200]\n",
      "loss: 1.557926  [  624/ 3200]\n",
      "loss: 1.383562  [  640/ 3200]\n",
      "loss: 1.433604  [  656/ 3200]\n",
      "loss: 1.351838  [  672/ 3200]\n",
      "loss: 1.324393  [  688/ 3200]\n",
      "loss: 1.407170  [  704/ 3200]\n",
      "loss: 1.362709  [  720/ 3200]\n",
      "loss: 1.408955  [  736/ 3200]\n",
      "loss: 1.328783  [  752/ 3200]\n",
      "loss: 1.442364  [  768/ 3200]\n",
      "loss: 1.318637  [  784/ 3200]\n",
      "loss: 1.373539  [  800/ 3200]\n",
      "loss: 1.347563  [  816/ 3200]\n",
      "loss: 1.326365  [  832/ 3200]\n",
      "loss: 1.350469  [  848/ 3200]\n",
      "loss: 1.375745  [  864/ 3200]\n",
      "loss: 1.471138  [  880/ 3200]\n",
      "loss: 1.425859  [  896/ 3200]\n",
      "loss: 1.341562  [  912/ 3200]\n",
      "loss: 1.393549  [  928/ 3200]\n",
      "loss: 1.377002  [  944/ 3200]\n",
      "loss: 1.344931  [  960/ 3200]\n",
      "loss: 1.358740  [  976/ 3200]\n",
      "loss: 1.274884  [  992/ 3200]\n",
      "loss: 1.501865  [ 1008/ 3200]\n",
      "loss: 1.356203  [ 1024/ 3200]\n",
      "loss: 1.346581  [ 1040/ 3200]\n",
      "loss: 1.375161  [ 1056/ 3200]\n",
      "loss: 1.247763  [ 1072/ 3200]\n",
      "loss: 1.460935  [ 1088/ 3200]\n",
      "loss: 1.424220  [ 1104/ 3200]\n",
      "loss: 1.280300  [ 1120/ 3200]\n",
      "loss: 1.497151  [ 1136/ 3200]\n",
      "loss: 1.372274  [ 1152/ 3200]\n",
      "loss: 1.356639  [ 1168/ 3200]\n",
      "loss: 1.335284  [ 1184/ 3200]\n",
      "loss: 1.362513  [ 1200/ 3200]\n",
      "loss: 1.310205  [ 1216/ 3200]\n",
      "loss: 1.412251  [ 1232/ 3200]\n",
      "loss: 1.344212  [ 1248/ 3200]\n",
      "loss: 1.357317  [ 1264/ 3200]\n",
      "loss: 1.324237  [ 1280/ 3200]\n",
      "loss: 1.305694  [ 1296/ 3200]\n",
      "loss: 1.263274  [ 1312/ 3200]\n",
      "loss: 1.327924  [ 1328/ 3200]\n",
      "loss: 1.350901  [ 1344/ 3200]\n",
      "loss: 1.201255  [ 1360/ 3200]\n",
      "loss: 1.242456  [ 1376/ 3200]\n",
      "loss: 1.243505  [ 1392/ 3200]\n",
      "loss: 1.292077  [ 1408/ 3200]\n",
      "loss: 1.187207  [ 1424/ 3200]\n",
      "loss: 1.393437  [ 1440/ 3200]\n",
      "loss: 1.233559  [ 1456/ 3200]\n",
      "loss: 1.354319  [ 1472/ 3200]\n",
      "loss: 1.410908  [ 1488/ 3200]\n",
      "loss: 1.371239  [ 1504/ 3200]\n",
      "loss: 1.279593  [ 1520/ 3200]\n",
      "loss: 1.482910  [ 1536/ 3200]\n",
      "loss: 1.454768  [ 1552/ 3200]\n",
      "loss: 1.297748  [ 1568/ 3200]\n",
      "loss: 1.380850  [ 1584/ 3200]\n",
      "loss: 1.351535  [ 1600/ 3200]\n",
      "loss: 1.274776  [ 1616/ 3200]\n",
      "loss: 1.261349  [ 1632/ 3200]\n",
      "loss: 1.243427  [ 1648/ 3200]\n",
      "loss: 1.205095  [ 1664/ 3200]\n",
      "loss: 1.176320  [ 1680/ 3200]\n",
      "loss: 1.242011  [ 1696/ 3200]\n",
      "loss: 1.302608  [ 1712/ 3200]\n",
      "loss: 1.168667  [ 1728/ 3200]\n",
      "loss: 1.144248  [ 1744/ 3200]\n",
      "loss: 1.192140  [ 1760/ 3200]\n",
      "loss: 1.282210  [ 1776/ 3200]\n",
      "loss: 1.170475  [ 1792/ 3200]\n",
      "loss: 1.315410  [ 1808/ 3200]\n",
      "loss: 1.393888  [ 1824/ 3200]\n",
      "loss: 1.225441  [ 1840/ 3200]\n",
      "loss: 1.149162  [ 1856/ 3200]\n",
      "loss: 1.162421  [ 1872/ 3200]\n",
      "loss: 1.095147  [ 1888/ 3200]\n",
      "loss: 0.984955  [ 1904/ 3200]\n",
      "loss: 1.008549  [ 1920/ 3200]\n",
      "loss: 1.074374  [ 1936/ 3200]\n",
      "loss: 1.035900  [ 1952/ 3200]\n",
      "loss: 1.220665  [ 1968/ 3200]\n",
      "loss: 0.892805  [ 1984/ 3200]\n",
      "loss: 1.249532  [ 2000/ 3200]\n",
      "loss: 1.223053  [ 2016/ 3200]\n",
      "loss: 0.950492  [ 2032/ 3200]\n",
      "loss: 0.977578  [ 2048/ 3200]\n",
      "loss: 1.023320  [ 2064/ 3200]\n",
      "loss: 1.215460  [ 2080/ 3200]\n",
      "loss: 0.765752  [ 2096/ 3200]\n",
      "loss: 1.302442  [ 2112/ 3200]\n",
      "loss: 0.931897  [ 2128/ 3200]\n",
      "loss: 0.968982  [ 2144/ 3200]\n",
      "loss: 0.895442  [ 2160/ 3200]\n",
      "loss: 0.898550  [ 2176/ 3200]\n",
      "loss: 0.765467  [ 2192/ 3200]\n",
      "loss: 1.231336  [ 2208/ 3200]\n",
      "loss: 1.081660  [ 2224/ 3200]\n",
      "loss: 0.869434  [ 2240/ 3200]\n",
      "loss: 1.016471  [ 2256/ 3200]\n",
      "loss: 0.893583  [ 2272/ 3200]\n",
      "loss: 0.670889  [ 2288/ 3200]\n",
      "loss: 1.040921  [ 2304/ 3200]\n",
      "loss: 1.011965  [ 2320/ 3200]\n",
      "loss: 1.173289  [ 2336/ 3200]\n",
      "loss: 1.150086  [ 2352/ 3200]\n",
      "loss: 0.919981  [ 2368/ 3200]\n",
      "loss: 1.080639  [ 2384/ 3200]\n",
      "loss: 1.053823  [ 2400/ 3200]\n",
      "loss: 1.091752  [ 2416/ 3200]\n",
      "loss: 1.049327  [ 2432/ 3200]\n",
      "loss: 1.103229  [ 2448/ 3200]\n",
      "loss: 1.015974  [ 2464/ 3200]\n",
      "loss: 0.951440  [ 2480/ 3200]\n",
      "loss: 1.155181  [ 2496/ 3200]\n",
      "loss: 1.206409  [ 2512/ 3200]\n",
      "loss: 0.975272  [ 2528/ 3200]\n",
      "loss: 1.213280  [ 2544/ 3200]\n",
      "loss: 0.957542  [ 2560/ 3200]\n",
      "loss: 1.186295  [ 2576/ 3200]\n",
      "loss: 0.792137  [ 2592/ 3200]\n",
      "loss: 0.935471  [ 2608/ 3200]\n",
      "loss: 0.811332  [ 2624/ 3200]\n",
      "loss: 0.878924  [ 2640/ 3200]\n",
      "loss: 1.051904  [ 2656/ 3200]\n",
      "loss: 0.845907  [ 2672/ 3200]\n",
      "loss: 0.748981  [ 2688/ 3200]\n",
      "loss: 1.180606  [ 2704/ 3200]\n",
      "loss: 1.634045  [ 2720/ 3200]\n",
      "loss: 1.106671  [ 2736/ 3200]\n",
      "loss: 0.918415  [ 2752/ 3200]\n",
      "loss: 0.479737  [ 2768/ 3200]\n",
      "loss: 0.718988  [ 2784/ 3200]\n",
      "loss: 0.919570  [ 2800/ 3200]\n",
      "loss: 1.106435  [ 2816/ 3200]\n",
      "loss: 1.179065  [ 2832/ 3200]\n",
      "loss: 0.878817  [ 2848/ 3200]\n",
      "loss: 0.911060  [ 2864/ 3200]\n",
      "loss: 0.871488  [ 2880/ 3200]\n",
      "loss: 0.626611  [ 2896/ 3200]\n",
      "loss: 0.774416  [ 2912/ 3200]\n",
      "loss: 0.760370  [ 2928/ 3200]\n",
      "loss: 1.043720  [ 2944/ 3200]\n",
      "loss: 1.133876  [ 2960/ 3200]\n",
      "loss: 0.901248  [ 2976/ 3200]\n",
      "loss: 0.802699  [ 2992/ 3200]\n",
      "loss: 1.188794  [ 3008/ 3200]\n",
      "loss: 1.037094  [ 3024/ 3200]\n",
      "loss: 0.895362  [ 3040/ 3200]\n",
      "loss: 0.912736  [ 3056/ 3200]\n",
      "loss: 1.044601  [ 3072/ 3200]\n",
      "loss: 1.375351  [ 3088/ 3200]\n",
      "loss: 1.187680  [ 3104/ 3200]\n",
      "loss: 0.797051  [ 3120/ 3200]\n",
      "loss: 0.757145  [ 3136/ 3200]\n",
      "loss: 0.770593  [ 3152/ 3200]\n",
      "loss: 1.047720  [ 3168/ 3200]\n",
      "loss: 0.947308  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.051065\n",
      "f1 macro averaged score: 0.668321\n",
      "Accuracy               : 68.0%\n",
      "Confusion matrix       :\n",
      "tensor([[151,  38,   8,   3],\n",
      "        [ 36,  69,  64,  31],\n",
      "        [  3,  14, 175,   8],\n",
      "        [  2,  22,  27, 149]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.9000e-03.\n",
      "\n",
      "Epoch: 2\n",
      "-----------------------------\n",
      "loss: 0.745421  [    0/ 3200]\n",
      "loss: 0.592592  [   16/ 3200]\n",
      "loss: 0.770925  [   32/ 3200]\n",
      "loss: 1.088932  [   48/ 3200]\n",
      "loss: 0.677652  [   64/ 3200]\n",
      "loss: 0.399759  [   80/ 3200]\n",
      "loss: 1.131780  [   96/ 3200]\n",
      "loss: 0.856927  [  112/ 3200]\n",
      "loss: 0.621050  [  128/ 3200]\n",
      "loss: 0.748158  [  144/ 3200]\n",
      "loss: 0.831988  [  160/ 3200]\n",
      "loss: 0.882077  [  176/ 3200]\n",
      "loss: 0.994400  [  192/ 3200]\n",
      "loss: 1.159344  [  208/ 3200]\n",
      "loss: 0.909528  [  224/ 3200]\n",
      "loss: 0.881077  [  240/ 3200]\n",
      "loss: 0.482194  [  256/ 3200]\n",
      "loss: 0.728558  [  272/ 3200]\n",
      "loss: 0.835431  [  288/ 3200]\n",
      "loss: 0.659835  [  304/ 3200]\n",
      "loss: 0.894425  [  320/ 3200]\n",
      "loss: 0.875363  [  336/ 3200]\n",
      "loss: 0.882921  [  352/ 3200]\n",
      "loss: 0.810471  [  368/ 3200]\n",
      "loss: 0.952751  [  384/ 3200]\n",
      "loss: 0.831148  [  400/ 3200]\n",
      "loss: 0.452168  [  416/ 3200]\n",
      "loss: 1.025100  [  432/ 3200]\n",
      "loss: 1.086994  [  448/ 3200]\n",
      "loss: 0.642055  [  464/ 3200]\n",
      "loss: 0.911369  [  480/ 3200]\n",
      "loss: 0.857555  [  496/ 3200]\n",
      "loss: 0.578914  [  512/ 3200]\n",
      "loss: 0.802145  [  528/ 3200]\n",
      "loss: 0.867986  [  544/ 3200]\n",
      "loss: 1.507051  [  560/ 3200]\n",
      "loss: 1.008260  [  576/ 3200]\n",
      "loss: 0.556560  [  592/ 3200]\n",
      "loss: 0.769125  [  608/ 3200]\n",
      "loss: 0.722398  [  624/ 3200]\n",
      "loss: 0.871671  [  640/ 3200]\n",
      "loss: 0.597137  [  656/ 3200]\n",
      "loss: 0.841244  [  672/ 3200]\n",
      "loss: 0.781293  [  688/ 3200]\n",
      "loss: 0.552208  [  704/ 3200]\n",
      "loss: 0.845912  [  720/ 3200]\n",
      "loss: 1.152134  [  736/ 3200]\n",
      "loss: 0.679598  [  752/ 3200]\n",
      "loss: 1.429614  [  768/ 3200]\n",
      "loss: 0.904102  [  784/ 3200]\n",
      "loss: 0.851108  [  800/ 3200]\n",
      "loss: 1.100116  [  816/ 3200]\n",
      "loss: 0.658370  [  832/ 3200]\n",
      "loss: 0.695180  [  848/ 3200]\n",
      "loss: 0.540143  [  864/ 3200]\n",
      "loss: 0.531651  [  880/ 3200]\n",
      "loss: 0.570274  [  896/ 3200]\n",
      "loss: 0.716865  [  912/ 3200]\n",
      "loss: 0.880766  [  928/ 3200]\n",
      "loss: 0.930890  [  944/ 3200]\n",
      "loss: 0.707669  [  960/ 3200]\n",
      "loss: 0.594930  [  976/ 3200]\n",
      "loss: 0.901291  [  992/ 3200]\n",
      "loss: 1.150514  [ 1008/ 3200]\n",
      "loss: 0.774270  [ 1024/ 3200]\n",
      "loss: 0.759510  [ 1040/ 3200]\n",
      "loss: 0.907056  [ 1056/ 3200]\n",
      "loss: 0.741947  [ 1072/ 3200]\n",
      "loss: 0.596179  [ 1088/ 3200]\n",
      "loss: 0.609271  [ 1104/ 3200]\n",
      "loss: 1.103022  [ 1120/ 3200]\n",
      "loss: 0.680507  [ 1136/ 3200]\n",
      "loss: 0.960114  [ 1152/ 3200]\n",
      "loss: 0.560965  [ 1168/ 3200]\n",
      "loss: 0.847374  [ 1184/ 3200]\n",
      "loss: 0.772863  [ 1200/ 3200]\n",
      "loss: 0.594821  [ 1216/ 3200]\n",
      "loss: 0.668416  [ 1232/ 3200]\n",
      "loss: 0.714307  [ 1248/ 3200]\n",
      "loss: 1.077868  [ 1264/ 3200]\n",
      "loss: 0.840320  [ 1280/ 3200]\n",
      "loss: 0.710452  [ 1296/ 3200]\n",
      "loss: 0.501608  [ 1312/ 3200]\n",
      "loss: 0.828446  [ 1328/ 3200]\n",
      "loss: 0.574517  [ 1344/ 3200]\n",
      "loss: 0.659265  [ 1360/ 3200]\n",
      "loss: 0.603128  [ 1376/ 3200]\n",
      "loss: 0.740473  [ 1392/ 3200]\n",
      "loss: 0.663691  [ 1408/ 3200]\n",
      "loss: 0.880327  [ 1424/ 3200]\n",
      "loss: 0.606430  [ 1440/ 3200]\n",
      "loss: 1.189873  [ 1456/ 3200]\n",
      "loss: 0.314012  [ 1472/ 3200]\n",
      "loss: 0.673008  [ 1488/ 3200]\n",
      "loss: 0.518656  [ 1504/ 3200]\n",
      "loss: 0.703478  [ 1520/ 3200]\n",
      "loss: 0.537053  [ 1536/ 3200]\n",
      "loss: 0.873124  [ 1552/ 3200]\n",
      "loss: 1.181213  [ 1568/ 3200]\n",
      "loss: 0.656950  [ 1584/ 3200]\n",
      "loss: 0.729770  [ 1600/ 3200]\n",
      "loss: 0.802364  [ 1616/ 3200]\n",
      "loss: 0.832582  [ 1632/ 3200]\n",
      "loss: 0.703019  [ 1648/ 3200]\n",
      "loss: 0.902849  [ 1664/ 3200]\n",
      "loss: 0.675493  [ 1680/ 3200]\n",
      "loss: 0.670816  [ 1696/ 3200]\n",
      "loss: 0.676706  [ 1712/ 3200]\n",
      "loss: 0.523268  [ 1728/ 3200]\n",
      "loss: 0.636139  [ 1744/ 3200]\n",
      "loss: 0.550394  [ 1760/ 3200]\n",
      "loss: 0.656663  [ 1776/ 3200]\n",
      "loss: 0.654541  [ 1792/ 3200]\n",
      "loss: 1.423261  [ 1808/ 3200]\n",
      "loss: 0.854100  [ 1824/ 3200]\n",
      "loss: 0.565638  [ 1840/ 3200]\n",
      "loss: 0.830118  [ 1856/ 3200]\n",
      "loss: 0.623809  [ 1872/ 3200]\n",
      "loss: 1.092868  [ 1888/ 3200]\n",
      "loss: 0.401273  [ 1904/ 3200]\n",
      "loss: 0.645965  [ 1920/ 3200]\n",
      "loss: 0.604483  [ 1936/ 3200]\n",
      "loss: 0.571875  [ 1952/ 3200]\n",
      "loss: 0.635325  [ 1968/ 3200]\n",
      "loss: 0.811785  [ 1984/ 3200]\n",
      "loss: 0.785581  [ 2000/ 3200]\n",
      "loss: 0.877149  [ 2016/ 3200]\n",
      "loss: 0.450625  [ 2032/ 3200]\n",
      "loss: 0.606677  [ 2048/ 3200]\n",
      "loss: 0.604897  [ 2064/ 3200]\n",
      "loss: 0.840436  [ 2080/ 3200]\n",
      "loss: 0.478850  [ 2096/ 3200]\n",
      "loss: 0.769158  [ 2112/ 3200]\n",
      "loss: 0.574581  [ 2128/ 3200]\n",
      "loss: 0.716668  [ 2144/ 3200]\n",
      "loss: 0.354661  [ 2160/ 3200]\n",
      "loss: 0.857160  [ 2176/ 3200]\n",
      "loss: 0.406726  [ 2192/ 3200]\n",
      "loss: 1.043795  [ 2208/ 3200]\n",
      "loss: 0.572679  [ 2224/ 3200]\n",
      "loss: 0.569612  [ 2240/ 3200]\n",
      "loss: 0.429262  [ 2256/ 3200]\n",
      "loss: 0.637392  [ 2272/ 3200]\n",
      "loss: 0.462862  [ 2288/ 3200]\n",
      "loss: 0.626482  [ 2304/ 3200]\n",
      "loss: 0.641126  [ 2320/ 3200]\n",
      "loss: 1.315785  [ 2336/ 3200]\n",
      "loss: 0.734954  [ 2352/ 3200]\n",
      "loss: 1.130962  [ 2368/ 3200]\n",
      "loss: 0.848886  [ 2384/ 3200]\n",
      "loss: 1.035524  [ 2400/ 3200]\n",
      "loss: 0.669124  [ 2416/ 3200]\n",
      "loss: 0.697568  [ 2432/ 3200]\n",
      "loss: 0.783070  [ 2448/ 3200]\n",
      "loss: 0.802575  [ 2464/ 3200]\n",
      "loss: 0.589580  [ 2480/ 3200]\n",
      "loss: 0.731498  [ 2496/ 3200]\n",
      "loss: 0.851669  [ 2512/ 3200]\n",
      "loss: 0.475675  [ 2528/ 3200]\n",
      "loss: 1.293366  [ 2544/ 3200]\n",
      "loss: 0.818662  [ 2560/ 3200]\n",
      "loss: 0.651988  [ 2576/ 3200]\n",
      "loss: 0.511378  [ 2592/ 3200]\n",
      "loss: 0.797845  [ 2608/ 3200]\n",
      "loss: 1.401595  [ 2624/ 3200]\n",
      "loss: 0.644689  [ 2640/ 3200]\n",
      "loss: 0.498786  [ 2656/ 3200]\n",
      "loss: 1.047861  [ 2672/ 3200]\n",
      "loss: 0.846738  [ 2688/ 3200]\n",
      "loss: 0.728300  [ 2704/ 3200]\n",
      "loss: 0.769139  [ 2720/ 3200]\n",
      "loss: 0.527656  [ 2736/ 3200]\n",
      "loss: 0.702308  [ 2752/ 3200]\n",
      "loss: 0.652245  [ 2768/ 3200]\n",
      "loss: 0.999207  [ 2784/ 3200]\n",
      "loss: 0.481004  [ 2800/ 3200]\n",
      "loss: 0.895350  [ 2816/ 3200]\n",
      "loss: 0.669662  [ 2832/ 3200]\n",
      "loss: 0.874461  [ 2848/ 3200]\n",
      "loss: 0.544366  [ 2864/ 3200]\n",
      "loss: 0.627028  [ 2880/ 3200]\n",
      "loss: 0.657244  [ 2896/ 3200]\n",
      "loss: 0.529167  [ 2912/ 3200]\n",
      "loss: 0.976379  [ 2928/ 3200]\n",
      "loss: 0.676638  [ 2944/ 3200]\n",
      "loss: 1.029680  [ 2960/ 3200]\n",
      "loss: 0.740984  [ 2976/ 3200]\n",
      "loss: 0.667743  [ 2992/ 3200]\n",
      "loss: 0.602100  [ 3008/ 3200]\n",
      "loss: 0.839451  [ 3024/ 3200]\n",
      "loss: 0.879067  [ 3040/ 3200]\n",
      "loss: 0.948473  [ 3056/ 3200]\n",
      "loss: 0.460511  [ 3072/ 3200]\n",
      "loss: 0.455229  [ 3088/ 3200]\n",
      "loss: 0.653022  [ 3104/ 3200]\n",
      "loss: 1.028723  [ 3120/ 3200]\n",
      "loss: 0.817626  [ 3136/ 3200]\n",
      "loss: 0.955092  [ 3152/ 3200]\n",
      "loss: 0.607266  [ 3168/ 3200]\n",
      "loss: 0.233367  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.042572\n",
      "f1 macro averaged score: 0.703676\n",
      "Accuracy               : 71.6%\n",
      "Confusion matrix       :\n",
      "tensor([[162,  30,   4,   4],\n",
      "        [ 27,  76,  63,  34],\n",
      "        [  0,  10, 183,   7],\n",
      "        [  4,  20,  24, 152]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.8050e-03.\n",
      "\n",
      "Epoch: 3\n",
      "-----------------------------\n",
      "loss: 0.937350  [    0/ 3200]\n",
      "loss: 0.712012  [   16/ 3200]\n",
      "loss: 0.535174  [   32/ 3200]\n",
      "loss: 0.606345  [   48/ 3200]\n",
      "loss: 0.501856  [   64/ 3200]\n",
      "loss: 0.569772  [   80/ 3200]\n",
      "loss: 0.728129  [   96/ 3200]\n",
      "loss: 0.400126  [  112/ 3200]\n",
      "loss: 0.865349  [  128/ 3200]\n",
      "loss: 0.495852  [  144/ 3200]\n",
      "loss: 0.473384  [  160/ 3200]\n",
      "loss: 0.417406  [  176/ 3200]\n",
      "loss: 0.621214  [  192/ 3200]\n",
      "loss: 0.575680  [  208/ 3200]\n",
      "loss: 0.848200  [  224/ 3200]\n",
      "loss: 1.214820  [  240/ 3200]\n",
      "loss: 0.686542  [  256/ 3200]\n",
      "loss: 0.467967  [  272/ 3200]\n",
      "loss: 0.996739  [  288/ 3200]\n",
      "loss: 0.296521  [  304/ 3200]\n",
      "loss: 0.517706  [  320/ 3200]\n",
      "loss: 0.668425  [  336/ 3200]\n",
      "loss: 0.589937  [  352/ 3200]\n",
      "loss: 0.489416  [  368/ 3200]\n",
      "loss: 0.518228  [  384/ 3200]\n",
      "loss: 0.503500  [  400/ 3200]\n",
      "loss: 0.511066  [  416/ 3200]\n",
      "loss: 0.607121  [  432/ 3200]\n",
      "loss: 0.646236  [  448/ 3200]\n",
      "loss: 0.765106  [  464/ 3200]\n",
      "loss: 0.995600  [  480/ 3200]\n",
      "loss: 0.901221  [  496/ 3200]\n",
      "loss: 0.695032  [  512/ 3200]\n",
      "loss: 0.392974  [  528/ 3200]\n",
      "loss: 0.673422  [  544/ 3200]\n",
      "loss: 0.869387  [  560/ 3200]\n",
      "loss: 0.688507  [  576/ 3200]\n",
      "loss: 0.609186  [  592/ 3200]\n",
      "loss: 1.089401  [  608/ 3200]\n",
      "loss: 0.859290  [  624/ 3200]\n",
      "loss: 0.596950  [  640/ 3200]\n",
      "loss: 0.548519  [  656/ 3200]\n",
      "loss: 0.932729  [  672/ 3200]\n",
      "loss: 0.286025  [  688/ 3200]\n",
      "loss: 0.637848  [  704/ 3200]\n",
      "loss: 0.508278  [  720/ 3200]\n",
      "loss: 0.433157  [  736/ 3200]\n",
      "loss: 0.552146  [  752/ 3200]\n",
      "loss: 0.665940  [  768/ 3200]\n",
      "loss: 0.567607  [  784/ 3200]\n",
      "loss: 1.022174  [  800/ 3200]\n",
      "loss: 0.761345  [  816/ 3200]\n",
      "loss: 0.426122  [  832/ 3200]\n",
      "loss: 0.485390  [  848/ 3200]\n",
      "loss: 0.639030  [  864/ 3200]\n",
      "loss: 0.422105  [  880/ 3200]\n",
      "loss: 0.639015  [  896/ 3200]\n",
      "loss: 0.525104  [  912/ 3200]\n",
      "loss: 0.760283  [  928/ 3200]\n",
      "loss: 0.849222  [  944/ 3200]\n",
      "loss: 0.554902  [  960/ 3200]\n",
      "loss: 1.355582  [  976/ 3200]\n",
      "loss: 0.928803  [  992/ 3200]\n",
      "loss: 1.049025  [ 1008/ 3200]\n",
      "loss: 0.559835  [ 1024/ 3200]\n",
      "loss: 1.275927  [ 1040/ 3200]\n",
      "loss: 0.854420  [ 1056/ 3200]\n",
      "loss: 0.887832  [ 1072/ 3200]\n",
      "loss: 0.611438  [ 1088/ 3200]\n",
      "loss: 0.693360  [ 1104/ 3200]\n",
      "loss: 0.524611  [ 1120/ 3200]\n",
      "loss: 0.403737  [ 1136/ 3200]\n",
      "loss: 0.516072  [ 1152/ 3200]\n",
      "loss: 0.551644  [ 1168/ 3200]\n",
      "loss: 1.005678  [ 1184/ 3200]\n",
      "loss: 0.857839  [ 1200/ 3200]\n",
      "loss: 0.736660  [ 1216/ 3200]\n",
      "loss: 0.546488  [ 1232/ 3200]\n",
      "loss: 0.621752  [ 1248/ 3200]\n",
      "loss: 0.544391  [ 1264/ 3200]\n",
      "loss: 0.510229  [ 1280/ 3200]\n",
      "loss: 0.441633  [ 1296/ 3200]\n",
      "loss: 0.701955  [ 1312/ 3200]\n",
      "loss: 0.691683  [ 1328/ 3200]\n",
      "loss: 0.504308  [ 1344/ 3200]\n",
      "loss: 0.508029  [ 1360/ 3200]\n",
      "loss: 0.383366  [ 1376/ 3200]\n",
      "loss: 0.456922  [ 1392/ 3200]\n",
      "loss: 0.293425  [ 1408/ 3200]\n",
      "loss: 0.891600  [ 1424/ 3200]\n",
      "loss: 0.779202  [ 1440/ 3200]\n",
      "loss: 0.521196  [ 1456/ 3200]\n",
      "loss: 0.628138  [ 1472/ 3200]\n",
      "loss: 0.848676  [ 1488/ 3200]\n",
      "loss: 0.530706  [ 1504/ 3200]\n",
      "loss: 1.045203  [ 1520/ 3200]\n",
      "loss: 0.692508  [ 1536/ 3200]\n",
      "loss: 0.540780  [ 1552/ 3200]\n",
      "loss: 1.054243  [ 1568/ 3200]\n",
      "loss: 0.834506  [ 1584/ 3200]\n",
      "loss: 0.678613  [ 1600/ 3200]\n",
      "loss: 0.252795  [ 1616/ 3200]\n",
      "loss: 0.643803  [ 1632/ 3200]\n",
      "loss: 0.360650  [ 1648/ 3200]\n",
      "loss: 0.414178  [ 1664/ 3200]\n",
      "loss: 0.364658  [ 1680/ 3200]\n",
      "loss: 0.425988  [ 1696/ 3200]\n",
      "loss: 0.519477  [ 1712/ 3200]\n",
      "loss: 0.469571  [ 1728/ 3200]\n",
      "loss: 0.321446  [ 1744/ 3200]\n",
      "loss: 0.446891  [ 1760/ 3200]\n",
      "loss: 0.751293  [ 1776/ 3200]\n",
      "loss: 0.689848  [ 1792/ 3200]\n",
      "loss: 0.652930  [ 1808/ 3200]\n",
      "loss: 0.513901  [ 1824/ 3200]\n",
      "loss: 0.619448  [ 1840/ 3200]\n",
      "loss: 1.193837  [ 1856/ 3200]\n",
      "loss: 0.658516  [ 1872/ 3200]\n",
      "loss: 0.695445  [ 1888/ 3200]\n",
      "loss: 0.892478  [ 1904/ 3200]\n",
      "loss: 0.486431  [ 1920/ 3200]\n",
      "loss: 0.977717  [ 1936/ 3200]\n",
      "loss: 1.119469  [ 1952/ 3200]\n",
      "loss: 0.488350  [ 1968/ 3200]\n",
      "loss: 0.746698  [ 1984/ 3200]\n",
      "loss: 0.690460  [ 2000/ 3200]\n",
      "loss: 0.515404  [ 2016/ 3200]\n",
      "loss: 0.522067  [ 2032/ 3200]\n",
      "loss: 0.556506  [ 2048/ 3200]\n",
      "loss: 0.584810  [ 2064/ 3200]\n",
      "loss: 0.765717  [ 2080/ 3200]\n",
      "loss: 0.656147  [ 2096/ 3200]\n",
      "loss: 0.833844  [ 2112/ 3200]\n",
      "loss: 0.436149  [ 2128/ 3200]\n",
      "loss: 0.786612  [ 2144/ 3200]\n",
      "loss: 0.716407  [ 2160/ 3200]\n",
      "loss: 0.717509  [ 2176/ 3200]\n",
      "loss: 0.907276  [ 2192/ 3200]\n",
      "loss: 0.526083  [ 2208/ 3200]\n",
      "loss: 0.595335  [ 2224/ 3200]\n",
      "loss: 0.831064  [ 2240/ 3200]\n",
      "loss: 0.812886  [ 2256/ 3200]\n",
      "loss: 0.901226  [ 2272/ 3200]\n",
      "loss: 0.719045  [ 2288/ 3200]\n",
      "loss: 0.537579  [ 2304/ 3200]\n",
      "loss: 1.048148  [ 2320/ 3200]\n",
      "loss: 0.410602  [ 2336/ 3200]\n",
      "loss: 0.699859  [ 2352/ 3200]\n",
      "loss: 0.408985  [ 2368/ 3200]\n",
      "loss: 0.440865  [ 2384/ 3200]\n",
      "loss: 0.573154  [ 2400/ 3200]\n",
      "loss: 0.455790  [ 2416/ 3200]\n",
      "loss: 0.352011  [ 2432/ 3200]\n",
      "loss: 0.652967  [ 2448/ 3200]\n",
      "loss: 0.558894  [ 2464/ 3200]\n",
      "loss: 0.623909  [ 2480/ 3200]\n",
      "loss: 0.625288  [ 2496/ 3200]\n",
      "loss: 0.627041  [ 2512/ 3200]\n",
      "loss: 0.434620  [ 2528/ 3200]\n",
      "loss: 0.488083  [ 2544/ 3200]\n",
      "loss: 0.659983  [ 2560/ 3200]\n",
      "loss: 0.552979  [ 2576/ 3200]\n",
      "loss: 0.630941  [ 2592/ 3200]\n",
      "loss: 0.419258  [ 2608/ 3200]\n",
      "loss: 0.588603  [ 2624/ 3200]\n",
      "loss: 0.282993  [ 2640/ 3200]\n",
      "loss: 0.460003  [ 2656/ 3200]\n",
      "loss: 0.496968  [ 2672/ 3200]\n",
      "loss: 0.706782  [ 2688/ 3200]\n",
      "loss: 0.599080  [ 2704/ 3200]\n",
      "loss: 0.879004  [ 2720/ 3200]\n",
      "loss: 0.530038  [ 2736/ 3200]\n",
      "loss: 0.419642  [ 2752/ 3200]\n",
      "loss: 0.326884  [ 2768/ 3200]\n",
      "loss: 0.500307  [ 2784/ 3200]\n",
      "loss: 0.891897  [ 2800/ 3200]\n",
      "loss: 0.616908  [ 2816/ 3200]\n",
      "loss: 0.675051  [ 2832/ 3200]\n",
      "loss: 0.327238  [ 2848/ 3200]\n",
      "loss: 0.876278  [ 2864/ 3200]\n",
      "loss: 0.511702  [ 2880/ 3200]\n",
      "loss: 0.686673  [ 2896/ 3200]\n",
      "loss: 0.439713  [ 2912/ 3200]\n",
      "loss: 0.416453  [ 2928/ 3200]\n",
      "loss: 0.330133  [ 2944/ 3200]\n",
      "loss: 0.917455  [ 2960/ 3200]\n",
      "loss: 0.339378  [ 2976/ 3200]\n",
      "loss: 0.499058  [ 2992/ 3200]\n",
      "loss: 0.973403  [ 3008/ 3200]\n",
      "loss: 0.556897  [ 3024/ 3200]\n",
      "loss: 0.451835  [ 3040/ 3200]\n",
      "loss: 0.501120  [ 3056/ 3200]\n",
      "loss: 0.610968  [ 3072/ 3200]\n",
      "loss: 0.508075  [ 3088/ 3200]\n",
      "loss: 0.757783  [ 3104/ 3200]\n",
      "loss: 0.591668  [ 3120/ 3200]\n",
      "loss: 0.705410  [ 3136/ 3200]\n",
      "loss: 0.740203  [ 3152/ 3200]\n",
      "loss: 0.748329  [ 3168/ 3200]\n",
      "loss: 0.443580  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038450\n",
      "f1 macro averaged score: 0.731820\n",
      "Accuracy               : 74.0%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  17,   1,   8],\n",
      "        [ 25,  88,  19,  68],\n",
      "        [  0,  20, 161,  19],\n",
      "        [  5,  12,  14, 169]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.7147e-03.\n",
      "\n",
      "Epoch: 4\n",
      "-----------------------------\n",
      "loss: 0.408353  [    0/ 3200]\n",
      "loss: 0.528313  [   16/ 3200]\n",
      "loss: 0.391939  [   32/ 3200]\n",
      "loss: 0.414910  [   48/ 3200]\n",
      "loss: 0.707329  [   64/ 3200]\n",
      "loss: 0.932741  [   80/ 3200]\n",
      "loss: 0.594949  [   96/ 3200]\n",
      "loss: 0.387042  [  112/ 3200]\n",
      "loss: 0.901245  [  128/ 3200]\n",
      "loss: 0.365850  [  144/ 3200]\n",
      "loss: 0.362546  [  160/ 3200]\n",
      "loss: 0.786929  [  176/ 3200]\n",
      "loss: 0.624083  [  192/ 3200]\n",
      "loss: 0.398025  [  208/ 3200]\n",
      "loss: 0.732591  [  224/ 3200]\n",
      "loss: 0.578099  [  240/ 3200]\n",
      "loss: 0.881598  [  256/ 3200]\n",
      "loss: 0.623573  [  272/ 3200]\n",
      "loss: 0.532858  [  288/ 3200]\n",
      "loss: 0.168122  [  304/ 3200]\n",
      "loss: 0.635501  [  320/ 3200]\n",
      "loss: 0.296458  [  336/ 3200]\n",
      "loss: 0.637070  [  352/ 3200]\n",
      "loss: 0.755999  [  368/ 3200]\n",
      "loss: 0.765659  [  384/ 3200]\n",
      "loss: 0.679968  [  400/ 3200]\n",
      "loss: 0.439360  [  416/ 3200]\n",
      "loss: 0.520774  [  432/ 3200]\n",
      "loss: 0.585699  [  448/ 3200]\n",
      "loss: 0.633514  [  464/ 3200]\n",
      "loss: 0.582409  [  480/ 3200]\n",
      "loss: 0.809816  [  496/ 3200]\n",
      "loss: 0.680225  [  512/ 3200]\n",
      "loss: 0.431794  [  528/ 3200]\n",
      "loss: 0.367732  [  544/ 3200]\n",
      "loss: 0.609017  [  560/ 3200]\n",
      "loss: 0.699062  [  576/ 3200]\n",
      "loss: 0.629145  [  592/ 3200]\n",
      "loss: 0.701677  [  608/ 3200]\n",
      "loss: 0.680136  [  624/ 3200]\n",
      "loss: 0.331295  [  640/ 3200]\n",
      "loss: 0.489161  [  656/ 3200]\n",
      "loss: 0.339754  [  672/ 3200]\n",
      "loss: 0.428692  [  688/ 3200]\n",
      "loss: 0.795925  [  704/ 3200]\n",
      "loss: 0.967146  [  720/ 3200]\n",
      "loss: 0.398417  [  736/ 3200]\n",
      "loss: 0.471533  [  752/ 3200]\n",
      "loss: 0.323080  [  768/ 3200]\n",
      "loss: 0.615943  [  784/ 3200]\n",
      "loss: 0.341576  [  800/ 3200]\n",
      "loss: 0.444530  [  816/ 3200]\n",
      "loss: 0.482375  [  832/ 3200]\n",
      "loss: 0.410128  [  848/ 3200]\n",
      "loss: 0.361201  [  864/ 3200]\n",
      "loss: 0.498933  [  880/ 3200]\n",
      "loss: 0.267423  [  896/ 3200]\n",
      "loss: 0.272789  [  912/ 3200]\n",
      "loss: 0.740775  [  928/ 3200]\n",
      "loss: 0.693978  [  944/ 3200]\n",
      "loss: 0.538352  [  960/ 3200]\n",
      "loss: 0.607679  [  976/ 3200]\n",
      "loss: 0.364935  [  992/ 3200]\n",
      "loss: 0.415317  [ 1008/ 3200]\n",
      "loss: 0.748311  [ 1024/ 3200]\n",
      "loss: 0.528280  [ 1040/ 3200]\n",
      "loss: 0.440308  [ 1056/ 3200]\n",
      "loss: 0.554856  [ 1072/ 3200]\n",
      "loss: 0.513721  [ 1088/ 3200]\n",
      "loss: 0.301853  [ 1104/ 3200]\n",
      "loss: 1.108994  [ 1120/ 3200]\n",
      "loss: 0.585854  [ 1136/ 3200]\n",
      "loss: 0.598238  [ 1152/ 3200]\n",
      "loss: 0.672768  [ 1168/ 3200]\n",
      "loss: 0.679010  [ 1184/ 3200]\n",
      "loss: 0.802637  [ 1200/ 3200]\n",
      "loss: 0.623895  [ 1216/ 3200]\n",
      "loss: 0.517274  [ 1232/ 3200]\n",
      "loss: 0.348941  [ 1248/ 3200]\n",
      "loss: 0.887310  [ 1264/ 3200]\n",
      "loss: 0.502634  [ 1280/ 3200]\n",
      "loss: 0.441587  [ 1296/ 3200]\n",
      "loss: 0.330338  [ 1312/ 3200]\n",
      "loss: 0.620797  [ 1328/ 3200]\n",
      "loss: 0.672011  [ 1344/ 3200]\n",
      "loss: 0.452630  [ 1360/ 3200]\n",
      "loss: 0.745331  [ 1376/ 3200]\n",
      "loss: 0.506237  [ 1392/ 3200]\n",
      "loss: 0.308960  [ 1408/ 3200]\n",
      "loss: 0.387018  [ 1424/ 3200]\n",
      "loss: 0.573266  [ 1440/ 3200]\n",
      "loss: 0.792714  [ 1456/ 3200]\n",
      "loss: 0.401498  [ 1472/ 3200]\n",
      "loss: 0.292498  [ 1488/ 3200]\n",
      "loss: 0.579359  [ 1504/ 3200]\n",
      "loss: 0.668562  [ 1520/ 3200]\n",
      "loss: 0.813807  [ 1536/ 3200]\n",
      "loss: 0.500325  [ 1552/ 3200]\n",
      "loss: 0.268294  [ 1568/ 3200]\n",
      "loss: 0.497284  [ 1584/ 3200]\n",
      "loss: 0.545544  [ 1600/ 3200]\n",
      "loss: 0.345492  [ 1616/ 3200]\n",
      "loss: 0.735898  [ 1632/ 3200]\n",
      "loss: 0.656638  [ 1648/ 3200]\n",
      "loss: 0.979994  [ 1664/ 3200]\n",
      "loss: 0.643415  [ 1680/ 3200]\n",
      "loss: 0.580164  [ 1696/ 3200]\n",
      "loss: 0.754934  [ 1712/ 3200]\n",
      "loss: 0.398097  [ 1728/ 3200]\n",
      "loss: 0.532555  [ 1744/ 3200]\n",
      "loss: 0.816051  [ 1760/ 3200]\n",
      "loss: 0.768364  [ 1776/ 3200]\n",
      "loss: 0.638495  [ 1792/ 3200]\n",
      "loss: 0.860370  [ 1808/ 3200]\n",
      "loss: 0.341926  [ 1824/ 3200]\n",
      "loss: 0.440051  [ 1840/ 3200]\n",
      "loss: 0.132528  [ 1856/ 3200]\n",
      "loss: 0.543076  [ 1872/ 3200]\n",
      "loss: 0.751701  [ 1888/ 3200]\n",
      "loss: 0.677376  [ 1904/ 3200]\n",
      "loss: 0.384274  [ 1920/ 3200]\n",
      "loss: 0.252346  [ 1936/ 3200]\n",
      "loss: 0.479076  [ 1952/ 3200]\n",
      "loss: 0.448060  [ 1968/ 3200]\n",
      "loss: 0.553960  [ 1984/ 3200]\n",
      "loss: 0.327870  [ 2000/ 3200]\n",
      "loss: 0.294667  [ 2016/ 3200]\n",
      "loss: 0.724826  [ 2032/ 3200]\n",
      "loss: 0.322447  [ 2048/ 3200]\n",
      "loss: 0.630806  [ 2064/ 3200]\n",
      "loss: 0.501753  [ 2080/ 3200]\n",
      "loss: 0.600368  [ 2096/ 3200]\n",
      "loss: 0.454383  [ 2112/ 3200]\n",
      "loss: 0.846925  [ 2128/ 3200]\n",
      "loss: 0.543180  [ 2144/ 3200]\n",
      "loss: 0.542010  [ 2160/ 3200]\n",
      "loss: 0.631526  [ 2176/ 3200]\n",
      "loss: 0.675047  [ 2192/ 3200]\n",
      "loss: 0.484734  [ 2208/ 3200]\n",
      "loss: 0.500141  [ 2224/ 3200]\n",
      "loss: 0.355586  [ 2240/ 3200]\n",
      "loss: 0.486318  [ 2256/ 3200]\n",
      "loss: 0.807242  [ 2272/ 3200]\n",
      "loss: 0.490185  [ 2288/ 3200]\n",
      "loss: 0.499428  [ 2304/ 3200]\n",
      "loss: 0.846785  [ 2320/ 3200]\n",
      "loss: 0.414862  [ 2336/ 3200]\n",
      "loss: 0.531729  [ 2352/ 3200]\n",
      "loss: 0.263228  [ 2368/ 3200]\n",
      "loss: 0.516292  [ 2384/ 3200]\n",
      "loss: 0.420426  [ 2400/ 3200]\n",
      "loss: 0.526157  [ 2416/ 3200]\n",
      "loss: 0.349385  [ 2432/ 3200]\n",
      "loss: 0.296880  [ 2448/ 3200]\n",
      "loss: 0.242929  [ 2464/ 3200]\n",
      "loss: 0.530092  [ 2480/ 3200]\n",
      "loss: 0.382557  [ 2496/ 3200]\n",
      "loss: 0.502887  [ 2512/ 3200]\n",
      "loss: 0.937203  [ 2528/ 3200]\n",
      "loss: 0.594939  [ 2544/ 3200]\n",
      "loss: 0.381770  [ 2560/ 3200]\n",
      "loss: 0.805600  [ 2576/ 3200]\n",
      "loss: 0.495980  [ 2592/ 3200]\n",
      "loss: 0.534003  [ 2608/ 3200]\n",
      "loss: 0.474530  [ 2624/ 3200]\n",
      "loss: 0.692849  [ 2640/ 3200]\n",
      "loss: 0.922391  [ 2656/ 3200]\n",
      "loss: 0.563209  [ 2672/ 3200]\n",
      "loss: 0.882430  [ 2688/ 3200]\n",
      "loss: 0.301250  [ 2704/ 3200]\n",
      "loss: 0.962716  [ 2720/ 3200]\n",
      "loss: 0.731143  [ 2736/ 3200]\n",
      "loss: 0.397050  [ 2752/ 3200]\n",
      "loss: 0.732930  [ 2768/ 3200]\n",
      "loss: 0.474244  [ 2784/ 3200]\n",
      "loss: 0.469600  [ 2800/ 3200]\n",
      "loss: 0.833070  [ 2816/ 3200]\n",
      "loss: 0.737598  [ 2832/ 3200]\n",
      "loss: 0.632621  [ 2848/ 3200]\n",
      "loss: 0.710240  [ 2864/ 3200]\n",
      "loss: 0.416394  [ 2880/ 3200]\n",
      "loss: 0.485677  [ 2896/ 3200]\n",
      "loss: 0.448742  [ 2912/ 3200]\n",
      "loss: 0.249051  [ 2928/ 3200]\n",
      "loss: 0.373177  [ 2944/ 3200]\n",
      "loss: 0.950123  [ 2960/ 3200]\n",
      "loss: 0.679966  [ 2976/ 3200]\n",
      "loss: 0.475212  [ 2992/ 3200]\n",
      "loss: 0.528759  [ 3008/ 3200]\n",
      "loss: 0.414507  [ 3024/ 3200]\n",
      "loss: 0.496763  [ 3040/ 3200]\n",
      "loss: 0.665452  [ 3056/ 3200]\n",
      "loss: 0.462658  [ 3072/ 3200]\n",
      "loss: 0.633717  [ 3088/ 3200]\n",
      "loss: 0.506209  [ 3104/ 3200]\n",
      "loss: 0.591174  [ 3120/ 3200]\n",
      "loss: 0.779780  [ 3136/ 3200]\n",
      "loss: 0.516293  [ 3152/ 3200]\n",
      "loss: 0.552106  [ 3168/ 3200]\n",
      "loss: 0.628747  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.046130\n",
      "f1 macro averaged score: 0.709985\n",
      "Accuracy               : 70.8%\n",
      "Confusion matrix       :\n",
      "tensor([[103,  90,   3,   4],\n",
      "        [  4, 137,  41,  18],\n",
      "        [  0,  18, 178,   4],\n",
      "        [  0,  28,  24, 148]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.6290e-03.\n",
      "\n",
      "Epoch: 5\n",
      "-----------------------------\n",
      "loss: 0.503645  [    0/ 3200]\n",
      "loss: 0.501167  [   16/ 3200]\n",
      "loss: 0.363205  [   32/ 3200]\n",
      "loss: 0.485636  [   48/ 3200]\n",
      "loss: 0.532636  [   64/ 3200]\n",
      "loss: 0.495442  [   80/ 3200]\n",
      "loss: 0.482843  [   96/ 3200]\n",
      "loss: 0.659184  [  112/ 3200]\n",
      "loss: 0.503856  [  128/ 3200]\n",
      "loss: 0.303228  [  144/ 3200]\n",
      "loss: 0.480008  [  160/ 3200]\n",
      "loss: 0.376874  [  176/ 3200]\n",
      "loss: 0.351197  [  192/ 3200]\n",
      "loss: 0.556805  [  208/ 3200]\n",
      "loss: 0.546465  [  224/ 3200]\n",
      "loss: 0.358228  [  240/ 3200]\n",
      "loss: 0.292871  [  256/ 3200]\n",
      "loss: 0.717495  [  272/ 3200]\n",
      "loss: 0.350318  [  288/ 3200]\n",
      "loss: 0.424803  [  304/ 3200]\n",
      "loss: 0.416106  [  320/ 3200]\n",
      "loss: 0.571713  [  336/ 3200]\n",
      "loss: 0.558803  [  352/ 3200]\n",
      "loss: 0.479680  [  368/ 3200]\n",
      "loss: 0.172574  [  384/ 3200]\n",
      "loss: 0.349312  [  400/ 3200]\n",
      "loss: 0.859441  [  416/ 3200]\n",
      "loss: 0.309720  [  432/ 3200]\n",
      "loss: 0.701263  [  448/ 3200]\n",
      "loss: 0.563570  [  464/ 3200]\n",
      "loss: 0.557355  [  480/ 3200]\n",
      "loss: 0.569150  [  496/ 3200]\n",
      "loss: 0.683912  [  512/ 3200]\n",
      "loss: 0.333699  [  528/ 3200]\n",
      "loss: 0.650122  [  544/ 3200]\n",
      "loss: 0.439245  [  560/ 3200]\n",
      "loss: 1.148058  [  576/ 3200]\n",
      "loss: 0.573107  [  592/ 3200]\n",
      "loss: 0.637149  [  608/ 3200]\n",
      "loss: 0.719334  [  624/ 3200]\n",
      "loss: 0.904715  [  640/ 3200]\n",
      "loss: 0.754003  [  656/ 3200]\n",
      "loss: 0.451082  [  672/ 3200]\n",
      "loss: 0.605462  [  688/ 3200]\n",
      "loss: 0.492682  [  704/ 3200]\n",
      "loss: 0.291064  [  720/ 3200]\n",
      "loss: 0.470743  [  736/ 3200]\n",
      "loss: 0.637486  [  752/ 3200]\n",
      "loss: 0.323994  [  768/ 3200]\n",
      "loss: 0.458188  [  784/ 3200]\n",
      "loss: 0.556765  [  800/ 3200]\n",
      "loss: 0.295144  [  816/ 3200]\n",
      "loss: 0.364703  [  832/ 3200]\n",
      "loss: 0.498854  [  848/ 3200]\n",
      "loss: 0.394198  [  864/ 3200]\n",
      "loss: 0.411116  [  880/ 3200]\n",
      "loss: 0.548989  [  896/ 3200]\n",
      "loss: 0.820969  [  912/ 3200]\n",
      "loss: 0.906768  [  928/ 3200]\n",
      "loss: 0.636594  [  944/ 3200]\n",
      "loss: 0.401950  [  960/ 3200]\n",
      "loss: 1.226438  [  976/ 3200]\n",
      "loss: 0.404585  [  992/ 3200]\n",
      "loss: 0.504500  [ 1008/ 3200]\n",
      "loss: 0.255762  [ 1024/ 3200]\n",
      "loss: 0.518408  [ 1040/ 3200]\n",
      "loss: 0.415186  [ 1056/ 3200]\n",
      "loss: 0.739152  [ 1072/ 3200]\n",
      "loss: 0.258288  [ 1088/ 3200]\n",
      "loss: 0.507008  [ 1104/ 3200]\n",
      "loss: 0.658035  [ 1120/ 3200]\n",
      "loss: 0.795793  [ 1136/ 3200]\n",
      "loss: 0.517201  [ 1152/ 3200]\n",
      "loss: 0.310626  [ 1168/ 3200]\n",
      "loss: 0.382642  [ 1184/ 3200]\n",
      "loss: 0.682422  [ 1200/ 3200]\n",
      "loss: 0.959692  [ 1216/ 3200]\n",
      "loss: 0.729656  [ 1232/ 3200]\n",
      "loss: 0.759834  [ 1248/ 3200]\n",
      "loss: 0.519417  [ 1264/ 3200]\n",
      "loss: 0.470746  [ 1280/ 3200]\n",
      "loss: 0.398598  [ 1296/ 3200]\n",
      "loss: 0.315511  [ 1312/ 3200]\n",
      "loss: 0.850925  [ 1328/ 3200]\n",
      "loss: 0.420344  [ 1344/ 3200]\n",
      "loss: 0.514321  [ 1360/ 3200]\n",
      "loss: 0.375105  [ 1376/ 3200]\n",
      "loss: 0.470728  [ 1392/ 3200]\n",
      "loss: 0.537348  [ 1408/ 3200]\n",
      "loss: 0.513877  [ 1424/ 3200]\n",
      "loss: 0.759334  [ 1440/ 3200]\n",
      "loss: 0.645859  [ 1456/ 3200]\n",
      "loss: 0.282292  [ 1472/ 3200]\n",
      "loss: 0.648961  [ 1488/ 3200]\n",
      "loss: 0.424978  [ 1504/ 3200]\n",
      "loss: 0.218431  [ 1520/ 3200]\n",
      "loss: 0.587171  [ 1536/ 3200]\n",
      "loss: 0.304632  [ 1552/ 3200]\n",
      "loss: 0.622113  [ 1568/ 3200]\n",
      "loss: 0.475541  [ 1584/ 3200]\n",
      "loss: 0.495915  [ 1600/ 3200]\n",
      "loss: 0.309808  [ 1616/ 3200]\n",
      "loss: 0.480132  [ 1632/ 3200]\n",
      "loss: 0.337596  [ 1648/ 3200]\n",
      "loss: 0.550881  [ 1664/ 3200]\n",
      "loss: 0.538814  [ 1680/ 3200]\n",
      "loss: 0.565615  [ 1696/ 3200]\n",
      "loss: 0.357351  [ 1712/ 3200]\n",
      "loss: 0.652450  [ 1728/ 3200]\n",
      "loss: 0.759723  [ 1744/ 3200]\n",
      "loss: 0.767904  [ 1760/ 3200]\n",
      "loss: 0.583390  [ 1776/ 3200]\n",
      "loss: 0.413121  [ 1792/ 3200]\n",
      "loss: 0.565036  [ 1808/ 3200]\n",
      "loss: 0.489352  [ 1824/ 3200]\n",
      "loss: 0.656545  [ 1840/ 3200]\n",
      "loss: 0.549174  [ 1856/ 3200]\n",
      "loss: 0.614716  [ 1872/ 3200]\n",
      "loss: 0.516893  [ 1888/ 3200]\n",
      "loss: 0.775776  [ 1904/ 3200]\n",
      "loss: 0.625485  [ 1920/ 3200]\n",
      "loss: 0.345071  [ 1936/ 3200]\n",
      "loss: 0.660367  [ 1952/ 3200]\n",
      "loss: 0.326759  [ 1968/ 3200]\n",
      "loss: 0.640163  [ 1984/ 3200]\n",
      "loss: 0.474292  [ 2000/ 3200]\n",
      "loss: 0.311088  [ 2016/ 3200]\n",
      "loss: 0.270272  [ 2032/ 3200]\n",
      "loss: 0.623683  [ 2048/ 3200]\n",
      "loss: 0.612830  [ 2064/ 3200]\n",
      "loss: 0.392614  [ 2080/ 3200]\n",
      "loss: 0.516787  [ 2096/ 3200]\n",
      "loss: 0.502426  [ 2112/ 3200]\n",
      "loss: 0.329258  [ 2128/ 3200]\n",
      "loss: 0.359806  [ 2144/ 3200]\n",
      "loss: 0.455354  [ 2160/ 3200]\n",
      "loss: 0.631021  [ 2176/ 3200]\n",
      "loss: 0.444792  [ 2192/ 3200]\n",
      "loss: 0.590708  [ 2208/ 3200]\n",
      "loss: 0.335841  [ 2224/ 3200]\n",
      "loss: 0.362547  [ 2240/ 3200]\n",
      "loss: 0.503397  [ 2256/ 3200]\n",
      "loss: 0.633960  [ 2272/ 3200]\n",
      "loss: 0.532172  [ 2288/ 3200]\n",
      "loss: 0.514716  [ 2304/ 3200]\n",
      "loss: 0.285658  [ 2320/ 3200]\n",
      "loss: 0.563891  [ 2336/ 3200]\n",
      "loss: 0.895153  [ 2352/ 3200]\n",
      "loss: 0.357884  [ 2368/ 3200]\n",
      "loss: 0.392013  [ 2384/ 3200]\n",
      "loss: 0.895042  [ 2400/ 3200]\n",
      "loss: 0.480567  [ 2416/ 3200]\n",
      "loss: 0.360262  [ 2432/ 3200]\n",
      "loss: 0.604964  [ 2448/ 3200]\n",
      "loss: 0.391716  [ 2464/ 3200]\n",
      "loss: 0.868000  [ 2480/ 3200]\n",
      "loss: 0.543630  [ 2496/ 3200]\n",
      "loss: 0.485835  [ 2512/ 3200]\n",
      "loss: 0.390554  [ 2528/ 3200]\n",
      "loss: 0.630528  [ 2544/ 3200]\n",
      "loss: 0.822653  [ 2560/ 3200]\n",
      "loss: 0.266990  [ 2576/ 3200]\n",
      "loss: 0.672643  [ 2592/ 3200]\n",
      "loss: 0.479657  [ 2608/ 3200]\n",
      "loss: 0.692456  [ 2624/ 3200]\n",
      "loss: 0.588912  [ 2640/ 3200]\n",
      "loss: 0.389579  [ 2656/ 3200]\n",
      "loss: 0.515155  [ 2672/ 3200]\n",
      "loss: 0.420830  [ 2688/ 3200]\n",
      "loss: 0.702003  [ 2704/ 3200]\n",
      "loss: 0.351689  [ 2720/ 3200]\n",
      "loss: 0.564765  [ 2736/ 3200]\n",
      "loss: 0.637810  [ 2752/ 3200]\n",
      "loss: 0.324642  [ 2768/ 3200]\n",
      "loss: 0.317917  [ 2784/ 3200]\n",
      "loss: 0.512995  [ 2800/ 3200]\n",
      "loss: 0.624402  [ 2816/ 3200]\n",
      "loss: 0.704324  [ 2832/ 3200]\n",
      "loss: 0.377116  [ 2848/ 3200]\n",
      "loss: 0.455772  [ 2864/ 3200]\n",
      "loss: 0.495292  [ 2880/ 3200]\n",
      "loss: 0.605559  [ 2896/ 3200]\n",
      "loss: 0.190840  [ 2912/ 3200]\n",
      "loss: 0.378129  [ 2928/ 3200]\n",
      "loss: 0.642489  [ 2944/ 3200]\n",
      "loss: 0.506829  [ 2960/ 3200]\n",
      "loss: 0.297628  [ 2976/ 3200]\n",
      "loss: 0.415289  [ 2992/ 3200]\n",
      "loss: 0.542974  [ 3008/ 3200]\n",
      "loss: 0.307122  [ 3024/ 3200]\n",
      "loss: 0.313409  [ 3040/ 3200]\n",
      "loss: 0.413852  [ 3056/ 3200]\n",
      "loss: 0.173197  [ 3072/ 3200]\n",
      "loss: 0.418080  [ 3088/ 3200]\n",
      "loss: 0.408237  [ 3104/ 3200]\n",
      "loss: 0.214003  [ 3120/ 3200]\n",
      "loss: 0.477352  [ 3136/ 3200]\n",
      "loss: 0.406126  [ 3152/ 3200]\n",
      "loss: 0.331261  [ 3168/ 3200]\n",
      "loss: 0.501531  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038940\n",
      "f1 macro averaged score: 0.763682\n",
      "Accuracy               : 75.6%\n",
      "Confusion matrix       :\n",
      "tensor([[144,  53,   1,   2],\n",
      "        [  8, 158,  24,  10],\n",
      "        [  0,  28, 169,   3],\n",
      "        [  2,  48,  16, 134]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.5476e-03.\n",
      "\n",
      "Epoch: 6\n",
      "-----------------------------\n",
      "loss: 0.337835  [    0/ 3200]\n",
      "loss: 0.450199  [   16/ 3200]\n",
      "loss: 0.296942  [   32/ 3200]\n",
      "loss: 0.334460  [   48/ 3200]\n",
      "loss: 0.358923  [   64/ 3200]\n",
      "loss: 0.517312  [   80/ 3200]\n",
      "loss: 0.478558  [   96/ 3200]\n",
      "loss: 0.364598  [  112/ 3200]\n",
      "loss: 0.530290  [  128/ 3200]\n",
      "loss: 0.353908  [  144/ 3200]\n",
      "loss: 0.637004  [  160/ 3200]\n",
      "loss: 0.543431  [  176/ 3200]\n",
      "loss: 0.265739  [  192/ 3200]\n",
      "loss: 0.809027  [  208/ 3200]\n",
      "loss: 0.271515  [  224/ 3200]\n",
      "loss: 0.341961  [  240/ 3200]\n",
      "loss: 0.397909  [  256/ 3200]\n",
      "loss: 0.505629  [  272/ 3200]\n",
      "loss: 0.170576  [  288/ 3200]\n",
      "loss: 0.159185  [  304/ 3200]\n",
      "loss: 0.374791  [  320/ 3200]\n",
      "loss: 0.276780  [  336/ 3200]\n",
      "loss: 0.634128  [  352/ 3200]\n",
      "loss: 0.477633  [  368/ 3200]\n",
      "loss: 0.469675  [  384/ 3200]\n",
      "loss: 0.330984  [  400/ 3200]\n",
      "loss: 0.199990  [  416/ 3200]\n",
      "loss: 0.304612  [  432/ 3200]\n",
      "loss: 0.492410  [  448/ 3200]\n",
      "loss: 0.349458  [  464/ 3200]\n",
      "loss: 0.251117  [  480/ 3200]\n",
      "loss: 0.699128  [  496/ 3200]\n",
      "loss: 0.439343  [  512/ 3200]\n",
      "loss: 0.386087  [  528/ 3200]\n",
      "loss: 0.864214  [  544/ 3200]\n",
      "loss: 0.637303  [  560/ 3200]\n",
      "loss: 0.389613  [  576/ 3200]\n",
      "loss: 0.756319  [  592/ 3200]\n",
      "loss: 0.683457  [  608/ 3200]\n",
      "loss: 0.725430  [  624/ 3200]\n",
      "loss: 0.994591  [  640/ 3200]\n",
      "loss: 0.860790  [  656/ 3200]\n",
      "loss: 0.274190  [  672/ 3200]\n",
      "loss: 0.264526  [  688/ 3200]\n",
      "loss: 0.317862  [  704/ 3200]\n",
      "loss: 0.523282  [  720/ 3200]\n",
      "loss: 0.524642  [  736/ 3200]\n",
      "loss: 0.329139  [  752/ 3200]\n",
      "loss: 0.656415  [  768/ 3200]\n",
      "loss: 0.379911  [  784/ 3200]\n",
      "loss: 0.413999  [  800/ 3200]\n",
      "loss: 0.679198  [  816/ 3200]\n",
      "loss: 0.567020  [  832/ 3200]\n",
      "loss: 0.753079  [  848/ 3200]\n",
      "loss: 0.594797  [  864/ 3200]\n",
      "loss: 0.504056  [  880/ 3200]\n",
      "loss: 0.328187  [  896/ 3200]\n",
      "loss: 0.593665  [  912/ 3200]\n",
      "loss: 0.515910  [  928/ 3200]\n",
      "loss: 0.512570  [  944/ 3200]\n",
      "loss: 0.554739  [  960/ 3200]\n",
      "loss: 0.555893  [  976/ 3200]\n",
      "loss: 0.392874  [  992/ 3200]\n",
      "loss: 0.392746  [ 1008/ 3200]\n",
      "loss: 0.489779  [ 1024/ 3200]\n",
      "loss: 0.261060  [ 1040/ 3200]\n",
      "loss: 0.611446  [ 1056/ 3200]\n",
      "loss: 0.731587  [ 1072/ 3200]\n",
      "loss: 0.279866  [ 1088/ 3200]\n",
      "loss: 0.319320  [ 1104/ 3200]\n",
      "loss: 0.617223  [ 1120/ 3200]\n",
      "loss: 0.163546  [ 1136/ 3200]\n",
      "loss: 0.354992  [ 1152/ 3200]\n",
      "loss: 0.380188  [ 1168/ 3200]\n",
      "loss: 0.571625  [ 1184/ 3200]\n",
      "loss: 0.291725  [ 1200/ 3200]\n",
      "loss: 0.425864  [ 1216/ 3200]\n",
      "loss: 0.513630  [ 1232/ 3200]\n",
      "loss: 0.394982  [ 1248/ 3200]\n",
      "loss: 0.518387  [ 1264/ 3200]\n",
      "loss: 0.380477  [ 1280/ 3200]\n",
      "loss: 0.394371  [ 1296/ 3200]\n",
      "loss: 0.623100  [ 1312/ 3200]\n",
      "loss: 0.557626  [ 1328/ 3200]\n",
      "loss: 0.415230  [ 1344/ 3200]\n",
      "loss: 0.346378  [ 1360/ 3200]\n",
      "loss: 0.655070  [ 1376/ 3200]\n",
      "loss: 0.402464  [ 1392/ 3200]\n",
      "loss: 0.258811  [ 1408/ 3200]\n",
      "loss: 0.337043  [ 1424/ 3200]\n",
      "loss: 0.712518  [ 1440/ 3200]\n",
      "loss: 0.726090  [ 1456/ 3200]\n",
      "loss: 0.554668  [ 1472/ 3200]\n",
      "loss: 0.971915  [ 1488/ 3200]\n",
      "loss: 0.435903  [ 1504/ 3200]\n",
      "loss: 0.327710  [ 1520/ 3200]\n",
      "loss: 0.661815  [ 1536/ 3200]\n",
      "loss: 0.241060  [ 1552/ 3200]\n",
      "loss: 0.569907  [ 1568/ 3200]\n",
      "loss: 0.458678  [ 1584/ 3200]\n",
      "loss: 0.524529  [ 1600/ 3200]\n",
      "loss: 0.399671  [ 1616/ 3200]\n",
      "loss: 0.188741  [ 1632/ 3200]\n",
      "loss: 0.139172  [ 1648/ 3200]\n",
      "loss: 0.454026  [ 1664/ 3200]\n",
      "loss: 0.264229  [ 1680/ 3200]\n",
      "loss: 0.586652  [ 1696/ 3200]\n",
      "loss: 0.427728  [ 1712/ 3200]\n",
      "loss: 0.289764  [ 1728/ 3200]\n",
      "loss: 0.465805  [ 1744/ 3200]\n",
      "loss: 0.510066  [ 1760/ 3200]\n",
      "loss: 0.221292  [ 1776/ 3200]\n",
      "loss: 0.513266  [ 1792/ 3200]\n",
      "loss: 0.639481  [ 1808/ 3200]\n",
      "loss: 0.363262  [ 1824/ 3200]\n",
      "loss: 0.476958  [ 1840/ 3200]\n",
      "loss: 0.447243  [ 1856/ 3200]\n",
      "loss: 0.623769  [ 1872/ 3200]\n",
      "loss: 0.222441  [ 1888/ 3200]\n",
      "loss: 0.145610  [ 1904/ 3200]\n",
      "loss: 0.366430  [ 1920/ 3200]\n",
      "loss: 0.456003  [ 1936/ 3200]\n",
      "loss: 0.427675  [ 1952/ 3200]\n",
      "loss: 1.267732  [ 1968/ 3200]\n",
      "loss: 0.357352  [ 1984/ 3200]\n",
      "loss: 0.351664  [ 2000/ 3200]\n",
      "loss: 0.423781  [ 2016/ 3200]\n",
      "loss: 0.443632  [ 2032/ 3200]\n",
      "loss: 0.314216  [ 2048/ 3200]\n",
      "loss: 0.246133  [ 2064/ 3200]\n",
      "loss: 0.599292  [ 2080/ 3200]\n",
      "loss: 0.590475  [ 2096/ 3200]\n",
      "loss: 0.316409  [ 2112/ 3200]\n",
      "loss: 0.169749  [ 2128/ 3200]\n",
      "loss: 0.848335  [ 2144/ 3200]\n",
      "loss: 0.934890  [ 2160/ 3200]\n",
      "loss: 0.719313  [ 2176/ 3200]\n",
      "loss: 0.676601  [ 2192/ 3200]\n",
      "loss: 0.471782  [ 2208/ 3200]\n",
      "loss: 0.707365  [ 2224/ 3200]\n",
      "loss: 0.732652  [ 2240/ 3200]\n",
      "loss: 0.671130  [ 2256/ 3200]\n",
      "loss: 0.865565  [ 2272/ 3200]\n",
      "loss: 0.410259  [ 2288/ 3200]\n",
      "loss: 0.716263  [ 2304/ 3200]\n",
      "loss: 0.288077  [ 2320/ 3200]\n",
      "loss: 0.482793  [ 2336/ 3200]\n",
      "loss: 0.946022  [ 2352/ 3200]\n",
      "loss: 0.655587  [ 2368/ 3200]\n",
      "loss: 0.655181  [ 2384/ 3200]\n",
      "loss: 0.520988  [ 2400/ 3200]\n",
      "loss: 0.639553  [ 2416/ 3200]\n",
      "loss: 0.347932  [ 2432/ 3200]\n",
      "loss: 0.649391  [ 2448/ 3200]\n",
      "loss: 0.203071  [ 2464/ 3200]\n",
      "loss: 0.724246  [ 2480/ 3200]\n",
      "loss: 0.469370  [ 2496/ 3200]\n",
      "loss: 0.515446  [ 2512/ 3200]\n",
      "loss: 0.270842  [ 2528/ 3200]\n",
      "loss: 0.668750  [ 2544/ 3200]\n",
      "loss: 0.313220  [ 2560/ 3200]\n",
      "loss: 0.453678  [ 2576/ 3200]\n",
      "loss: 0.371609  [ 2592/ 3200]\n",
      "loss: 0.279982  [ 2608/ 3200]\n",
      "loss: 0.546917  [ 2624/ 3200]\n",
      "loss: 0.457412  [ 2640/ 3200]\n",
      "loss: 0.494938  [ 2656/ 3200]\n",
      "loss: 0.230281  [ 2672/ 3200]\n",
      "loss: 0.227193  [ 2688/ 3200]\n",
      "loss: 0.552163  [ 2704/ 3200]\n",
      "loss: 0.731155  [ 2720/ 3200]\n",
      "loss: 0.358101  [ 2736/ 3200]\n",
      "loss: 0.580145  [ 2752/ 3200]\n",
      "loss: 0.218570  [ 2768/ 3200]\n",
      "loss: 0.372503  [ 2784/ 3200]\n",
      "loss: 0.196025  [ 2800/ 3200]\n",
      "loss: 0.669985  [ 2816/ 3200]\n",
      "loss: 0.169363  [ 2832/ 3200]\n",
      "loss: 0.537863  [ 2848/ 3200]\n",
      "loss: 0.251480  [ 2864/ 3200]\n",
      "loss: 0.212935  [ 2880/ 3200]\n",
      "loss: 0.616777  [ 2896/ 3200]\n",
      "loss: 0.701508  [ 2912/ 3200]\n",
      "loss: 0.230930  [ 2928/ 3200]\n",
      "loss: 0.490565  [ 2944/ 3200]\n",
      "loss: 0.280030  [ 2960/ 3200]\n",
      "loss: 0.334351  [ 2976/ 3200]\n",
      "loss: 0.353466  [ 2992/ 3200]\n",
      "loss: 0.492495  [ 3008/ 3200]\n",
      "loss: 0.591518  [ 3024/ 3200]\n",
      "loss: 0.754083  [ 3040/ 3200]\n",
      "loss: 0.140407  [ 3056/ 3200]\n",
      "loss: 0.340572  [ 3072/ 3200]\n",
      "loss: 0.828315  [ 3088/ 3200]\n",
      "loss: 0.370601  [ 3104/ 3200]\n",
      "loss: 0.314592  [ 3120/ 3200]\n",
      "loss: 0.480248  [ 3136/ 3200]\n",
      "loss: 0.698874  [ 3152/ 3200]\n",
      "loss: 0.352877  [ 3168/ 3200]\n",
      "loss: 0.790908  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038178\n",
      "f1 macro averaged score: 0.762675\n",
      "Accuracy               : 75.9%\n",
      "Confusion matrix       :\n",
      "tensor([[150,  39,   0,  11],\n",
      "        [  9, 143,  19,  29],\n",
      "        [  0,  26, 160,  14],\n",
      "        [  3,  26,  17, 154]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.4702e-03.\n",
      "\n",
      "Epoch: 7\n",
      "-----------------------------\n",
      "loss: 0.473749  [    0/ 3200]\n",
      "loss: 0.833673  [   16/ 3200]\n",
      "loss: 0.417291  [   32/ 3200]\n",
      "loss: 0.402276  [   48/ 3200]\n",
      "loss: 0.468444  [   64/ 3200]\n",
      "loss: 0.688320  [   80/ 3200]\n",
      "loss: 0.590838  [   96/ 3200]\n",
      "loss: 0.502029  [  112/ 3200]\n",
      "loss: 0.457652  [  128/ 3200]\n",
      "loss: 0.933270  [  144/ 3200]\n",
      "loss: 0.478698  [  160/ 3200]\n",
      "loss: 0.348215  [  176/ 3200]\n",
      "loss: 0.568960  [  192/ 3200]\n",
      "loss: 0.226550  [  208/ 3200]\n",
      "loss: 0.403866  [  224/ 3200]\n",
      "loss: 0.414953  [  240/ 3200]\n",
      "loss: 0.450844  [  256/ 3200]\n",
      "loss: 0.402181  [  272/ 3200]\n",
      "loss: 0.374058  [  288/ 3200]\n",
      "loss: 0.441698  [  304/ 3200]\n",
      "loss: 0.313939  [  320/ 3200]\n",
      "loss: 0.539885  [  336/ 3200]\n",
      "loss: 0.232147  [  352/ 3200]\n",
      "loss: 0.406361  [  368/ 3200]\n",
      "loss: 0.255558  [  384/ 3200]\n",
      "loss: 0.575850  [  400/ 3200]\n",
      "loss: 0.586322  [  416/ 3200]\n",
      "loss: 0.444838  [  432/ 3200]\n",
      "loss: 0.543135  [  448/ 3200]\n",
      "loss: 0.644227  [  464/ 3200]\n",
      "loss: 0.281083  [  480/ 3200]\n",
      "loss: 0.469403  [  496/ 3200]\n",
      "loss: 0.353649  [  512/ 3200]\n",
      "loss: 0.635112  [  528/ 3200]\n",
      "loss: 0.528672  [  544/ 3200]\n",
      "loss: 0.350366  [  560/ 3200]\n",
      "loss: 0.496760  [  576/ 3200]\n",
      "loss: 0.471500  [  592/ 3200]\n",
      "loss: 0.266036  [  608/ 3200]\n",
      "loss: 0.491702  [  624/ 3200]\n",
      "loss: 0.186038  [  640/ 3200]\n",
      "loss: 0.622026  [  656/ 3200]\n",
      "loss: 0.547763  [  672/ 3200]\n",
      "loss: 0.654130  [  688/ 3200]\n",
      "loss: 0.262607  [  704/ 3200]\n",
      "loss: 0.572583  [  720/ 3200]\n",
      "loss: 0.275187  [  736/ 3200]\n",
      "loss: 0.366180  [  752/ 3200]\n",
      "loss: 0.432502  [  768/ 3200]\n",
      "loss: 0.334491  [  784/ 3200]\n",
      "loss: 0.426203  [  800/ 3200]\n",
      "loss: 0.595248  [  816/ 3200]\n",
      "loss: 0.323688  [  832/ 3200]\n",
      "loss: 0.255223  [  848/ 3200]\n",
      "loss: 0.544356  [  864/ 3200]\n",
      "loss: 0.410493  [  880/ 3200]\n",
      "loss: 0.376527  [  896/ 3200]\n",
      "loss: 0.623349  [  912/ 3200]\n",
      "loss: 0.596946  [  928/ 3200]\n",
      "loss: 0.327799  [  944/ 3200]\n",
      "loss: 0.420510  [  960/ 3200]\n",
      "loss: 0.488473  [  976/ 3200]\n",
      "loss: 0.080485  [  992/ 3200]\n",
      "loss: 0.276206  [ 1008/ 3200]\n",
      "loss: 0.291506  [ 1024/ 3200]\n",
      "loss: 0.353436  [ 1040/ 3200]\n",
      "loss: 0.450805  [ 1056/ 3200]\n",
      "loss: 0.301146  [ 1072/ 3200]\n",
      "loss: 0.342515  [ 1088/ 3200]\n",
      "loss: 0.234540  [ 1104/ 3200]\n",
      "loss: 0.283383  [ 1120/ 3200]\n",
      "loss: 0.785883  [ 1136/ 3200]\n",
      "loss: 0.426098  [ 1152/ 3200]\n",
      "loss: 0.450868  [ 1168/ 3200]\n",
      "loss: 0.603777  [ 1184/ 3200]\n",
      "loss: 0.988389  [ 1200/ 3200]\n",
      "loss: 0.670496  [ 1216/ 3200]\n",
      "loss: 0.290988  [ 1232/ 3200]\n",
      "loss: 0.357046  [ 1248/ 3200]\n",
      "loss: 0.154376  [ 1264/ 3200]\n",
      "loss: 0.389178  [ 1280/ 3200]\n",
      "loss: 0.127534  [ 1296/ 3200]\n",
      "loss: 0.310833  [ 1312/ 3200]\n",
      "loss: 0.228649  [ 1328/ 3200]\n",
      "loss: 0.407356  [ 1344/ 3200]\n",
      "loss: 0.474096  [ 1360/ 3200]\n",
      "loss: 1.038290  [ 1376/ 3200]\n",
      "loss: 0.698644  [ 1392/ 3200]\n",
      "loss: 0.281285  [ 1408/ 3200]\n",
      "loss: 0.457865  [ 1424/ 3200]\n",
      "loss: 0.456138  [ 1440/ 3200]\n",
      "loss: 0.370751  [ 1456/ 3200]\n",
      "loss: 0.451395  [ 1472/ 3200]\n",
      "loss: 0.618346  [ 1488/ 3200]\n",
      "loss: 0.661192  [ 1504/ 3200]\n",
      "loss: 0.406704  [ 1520/ 3200]\n",
      "loss: 0.467940  [ 1536/ 3200]\n",
      "loss: 0.383847  [ 1552/ 3200]\n",
      "loss: 0.403150  [ 1568/ 3200]\n",
      "loss: 0.417288  [ 1584/ 3200]\n",
      "loss: 0.230310  [ 1600/ 3200]\n",
      "loss: 0.470754  [ 1616/ 3200]\n",
      "loss: 0.206303  [ 1632/ 3200]\n",
      "loss: 1.051916  [ 1648/ 3200]\n",
      "loss: 0.475220  [ 1664/ 3200]\n",
      "loss: 0.568583  [ 1680/ 3200]\n",
      "loss: 0.404659  [ 1696/ 3200]\n",
      "loss: 0.565499  [ 1712/ 3200]\n",
      "loss: 0.468601  [ 1728/ 3200]\n",
      "loss: 0.504921  [ 1744/ 3200]\n",
      "loss: 0.264511  [ 1760/ 3200]\n",
      "loss: 0.301423  [ 1776/ 3200]\n",
      "loss: 0.159089  [ 1792/ 3200]\n",
      "loss: 0.294608  [ 1808/ 3200]\n",
      "loss: 0.844855  [ 1824/ 3200]\n",
      "loss: 0.182770  [ 1840/ 3200]\n",
      "loss: 0.424440  [ 1856/ 3200]\n",
      "loss: 0.578670  [ 1872/ 3200]\n",
      "loss: 0.445234  [ 1888/ 3200]\n",
      "loss: 0.459404  [ 1904/ 3200]\n",
      "loss: 0.236022  [ 1920/ 3200]\n",
      "loss: 0.174291  [ 1936/ 3200]\n",
      "loss: 0.273049  [ 1952/ 3200]\n",
      "loss: 0.390864  [ 1968/ 3200]\n",
      "loss: 0.203185  [ 1984/ 3200]\n",
      "loss: 0.264839  [ 2000/ 3200]\n",
      "loss: 0.285106  [ 2016/ 3200]\n",
      "loss: 0.492556  [ 2032/ 3200]\n",
      "loss: 0.313746  [ 2048/ 3200]\n",
      "loss: 0.265838  [ 2064/ 3200]\n",
      "loss: 0.150261  [ 2080/ 3200]\n",
      "loss: 0.461560  [ 2096/ 3200]\n",
      "loss: 0.267711  [ 2112/ 3200]\n",
      "loss: 0.385817  [ 2128/ 3200]\n",
      "loss: 0.586709  [ 2144/ 3200]\n",
      "loss: 0.249289  [ 2160/ 3200]\n",
      "loss: 0.312473  [ 2176/ 3200]\n",
      "loss: 0.235886  [ 2192/ 3200]\n",
      "loss: 0.408655  [ 2208/ 3200]\n",
      "loss: 0.213867  [ 2224/ 3200]\n",
      "loss: 0.485417  [ 2240/ 3200]\n",
      "loss: 0.793733  [ 2256/ 3200]\n",
      "loss: 1.178935  [ 2272/ 3200]\n",
      "loss: 0.546151  [ 2288/ 3200]\n",
      "loss: 0.318037  [ 2304/ 3200]\n",
      "loss: 0.498426  [ 2320/ 3200]\n",
      "loss: 0.411667  [ 2336/ 3200]\n",
      "loss: 0.450565  [ 2352/ 3200]\n",
      "loss: 0.426429  [ 2368/ 3200]\n",
      "loss: 0.288943  [ 2384/ 3200]\n",
      "loss: 0.829687  [ 2400/ 3200]\n",
      "loss: 0.419583  [ 2416/ 3200]\n",
      "loss: 0.264257  [ 2432/ 3200]\n",
      "loss: 0.414830  [ 2448/ 3200]\n",
      "loss: 0.228314  [ 2464/ 3200]\n",
      "loss: 0.417509  [ 2480/ 3200]\n",
      "loss: 0.411148  [ 2496/ 3200]\n",
      "loss: 0.385890  [ 2512/ 3200]\n",
      "loss: 0.342760  [ 2528/ 3200]\n",
      "loss: 0.317699  [ 2544/ 3200]\n",
      "loss: 0.552306  [ 2560/ 3200]\n",
      "loss: 0.829515  [ 2576/ 3200]\n",
      "loss: 0.577318  [ 2592/ 3200]\n",
      "loss: 0.232861  [ 2608/ 3200]\n",
      "loss: 0.441497  [ 2624/ 3200]\n",
      "loss: 0.149651  [ 2640/ 3200]\n",
      "loss: 0.518892  [ 2656/ 3200]\n",
      "loss: 0.305096  [ 2672/ 3200]\n",
      "loss: 0.504217  [ 2688/ 3200]\n",
      "loss: 0.359586  [ 2704/ 3200]\n",
      "loss: 0.203213  [ 2720/ 3200]\n",
      "loss: 0.285921  [ 2736/ 3200]\n",
      "loss: 0.581766  [ 2752/ 3200]\n",
      "loss: 0.636225  [ 2768/ 3200]\n",
      "loss: 0.147685  [ 2784/ 3200]\n",
      "loss: 0.487472  [ 2800/ 3200]\n",
      "loss: 0.290951  [ 2816/ 3200]\n",
      "loss: 0.203815  [ 2832/ 3200]\n",
      "loss: 0.278253  [ 2848/ 3200]\n",
      "loss: 0.943583  [ 2864/ 3200]\n",
      "loss: 0.889826  [ 2880/ 3200]\n",
      "loss: 0.247941  [ 2896/ 3200]\n",
      "loss: 0.429018  [ 2912/ 3200]\n",
      "loss: 0.455302  [ 2928/ 3200]\n",
      "loss: 0.148181  [ 2944/ 3200]\n",
      "loss: 0.327788  [ 2960/ 3200]\n",
      "loss: 0.522843  [ 2976/ 3200]\n",
      "loss: 0.283933  [ 2992/ 3200]\n",
      "loss: 0.688349  [ 3008/ 3200]\n",
      "loss: 0.848309  [ 3024/ 3200]\n",
      "loss: 0.689017  [ 3040/ 3200]\n",
      "loss: 0.494857  [ 3056/ 3200]\n",
      "loss: 0.505716  [ 3072/ 3200]\n",
      "loss: 0.313143  [ 3088/ 3200]\n",
      "loss: 0.091239  [ 3104/ 3200]\n",
      "loss: 0.526228  [ 3120/ 3200]\n",
      "loss: 0.349981  [ 3136/ 3200]\n",
      "loss: 0.411142  [ 3152/ 3200]\n",
      "loss: 0.403561  [ 3168/ 3200]\n",
      "loss: 0.463384  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034482\n",
      "f1 macro averaged score: 0.790279\n",
      "Accuracy               : 79.0%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  20,   0,   5],\n",
      "        [ 14, 137,  28,  21],\n",
      "        [  0,  21, 171,   8],\n",
      "        [  5,  27,  19, 149]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3967e-03.\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 0.722626  [    0/ 3200]\n",
      "loss: 0.345807  [   16/ 3200]\n",
      "loss: 0.405905  [   32/ 3200]\n",
      "loss: 0.382185  [   48/ 3200]\n",
      "loss: 0.292689  [   64/ 3200]\n",
      "loss: 0.615524  [   80/ 3200]\n",
      "loss: 0.332033  [   96/ 3200]\n",
      "loss: 0.233436  [  112/ 3200]\n",
      "loss: 0.340839  [  128/ 3200]\n",
      "loss: 0.537426  [  144/ 3200]\n",
      "loss: 0.231684  [  160/ 3200]\n",
      "loss: 0.364387  [  176/ 3200]\n",
      "loss: 0.668983  [  192/ 3200]\n",
      "loss: 0.401259  [  208/ 3200]\n",
      "loss: 0.333334  [  224/ 3200]\n",
      "loss: 0.265595  [  240/ 3200]\n",
      "loss: 0.744242  [  256/ 3200]\n",
      "loss: 0.315843  [  272/ 3200]\n",
      "loss: 0.351039  [  288/ 3200]\n",
      "loss: 0.253337  [  304/ 3200]\n",
      "loss: 0.756704  [  320/ 3200]\n",
      "loss: 0.358875  [  336/ 3200]\n",
      "loss: 0.667139  [  352/ 3200]\n",
      "loss: 0.318722  [  368/ 3200]\n",
      "loss: 0.362918  [  384/ 3200]\n",
      "loss: 0.653803  [  400/ 3200]\n",
      "loss: 0.347193  [  416/ 3200]\n",
      "loss: 0.806887  [  432/ 3200]\n",
      "loss: 0.364402  [  448/ 3200]\n",
      "loss: 0.480157  [  464/ 3200]\n",
      "loss: 0.190588  [  480/ 3200]\n",
      "loss: 0.257062  [  496/ 3200]\n",
      "loss: 0.695572  [  512/ 3200]\n",
      "loss: 0.329582  [  528/ 3200]\n",
      "loss: 0.814550  [  544/ 3200]\n",
      "loss: 0.578294  [  560/ 3200]\n",
      "loss: 0.474079  [  576/ 3200]\n",
      "loss: 0.333563  [  592/ 3200]\n",
      "loss: 0.309042  [  608/ 3200]\n",
      "loss: 0.243152  [  624/ 3200]\n",
      "loss: 0.297518  [  640/ 3200]\n",
      "loss: 0.713605  [  656/ 3200]\n",
      "loss: 0.477687  [  672/ 3200]\n",
      "loss: 0.429909  [  688/ 3200]\n",
      "loss: 0.309330  [  704/ 3200]\n",
      "loss: 0.382032  [  720/ 3200]\n",
      "loss: 0.224526  [  736/ 3200]\n",
      "loss: 0.387950  [  752/ 3200]\n",
      "loss: 0.461846  [  768/ 3200]\n",
      "loss: 0.538864  [  784/ 3200]\n",
      "loss: 0.211462  [  800/ 3200]\n",
      "loss: 0.481722  [  816/ 3200]\n",
      "loss: 0.231272  [  832/ 3200]\n",
      "loss: 0.253180  [  848/ 3200]\n",
      "loss: 0.678171  [  864/ 3200]\n",
      "loss: 0.417572  [  880/ 3200]\n",
      "loss: 0.294559  [  896/ 3200]\n",
      "loss: 0.166171  [  912/ 3200]\n",
      "loss: 0.594097  [  928/ 3200]\n",
      "loss: 0.326055  [  944/ 3200]\n",
      "loss: 0.371613  [  960/ 3200]\n",
      "loss: 0.230570  [  976/ 3200]\n",
      "loss: 0.268375  [  992/ 3200]\n",
      "loss: 0.472625  [ 1008/ 3200]\n",
      "loss: 0.699860  [ 1024/ 3200]\n",
      "loss: 0.488638  [ 1040/ 3200]\n",
      "loss: 0.293000  [ 1056/ 3200]\n",
      "loss: 0.500300  [ 1072/ 3200]\n",
      "loss: 0.422985  [ 1088/ 3200]\n",
      "loss: 0.610486  [ 1104/ 3200]\n",
      "loss: 0.156949  [ 1120/ 3200]\n",
      "loss: 0.231346  [ 1136/ 3200]\n",
      "loss: 0.358521  [ 1152/ 3200]\n",
      "loss: 0.634192  [ 1168/ 3200]\n",
      "loss: 0.572526  [ 1184/ 3200]\n",
      "loss: 0.524115  [ 1200/ 3200]\n",
      "loss: 0.464204  [ 1216/ 3200]\n",
      "loss: 0.182070  [ 1232/ 3200]\n",
      "loss: 0.285385  [ 1248/ 3200]\n",
      "loss: 0.245276  [ 1264/ 3200]\n",
      "loss: 0.489475  [ 1280/ 3200]\n",
      "loss: 0.342785  [ 1296/ 3200]\n",
      "loss: 0.395709  [ 1312/ 3200]\n",
      "loss: 0.405147  [ 1328/ 3200]\n",
      "loss: 0.312548  [ 1344/ 3200]\n",
      "loss: 0.271835  [ 1360/ 3200]\n",
      "loss: 0.335847  [ 1376/ 3200]\n",
      "loss: 0.210596  [ 1392/ 3200]\n",
      "loss: 0.465798  [ 1408/ 3200]\n",
      "loss: 0.513131  [ 1424/ 3200]\n",
      "loss: 0.694038  [ 1440/ 3200]\n",
      "loss: 0.482798  [ 1456/ 3200]\n",
      "loss: 0.121615  [ 1472/ 3200]\n",
      "loss: 0.304815  [ 1488/ 3200]\n",
      "loss: 0.231532  [ 1504/ 3200]\n",
      "loss: 0.627718  [ 1520/ 3200]\n",
      "loss: 0.577494  [ 1536/ 3200]\n",
      "loss: 0.329413  [ 1552/ 3200]\n",
      "loss: 0.148028  [ 1568/ 3200]\n",
      "loss: 0.409571  [ 1584/ 3200]\n",
      "loss: 0.568524  [ 1600/ 3200]\n",
      "loss: 0.176837  [ 1616/ 3200]\n",
      "loss: 0.196965  [ 1632/ 3200]\n",
      "loss: 0.277437  [ 1648/ 3200]\n",
      "loss: 0.875362  [ 1664/ 3200]\n",
      "loss: 0.450626  [ 1680/ 3200]\n",
      "loss: 0.695380  [ 1696/ 3200]\n",
      "loss: 0.297884  [ 1712/ 3200]\n",
      "loss: 0.248667  [ 1728/ 3200]\n",
      "loss: 0.147117  [ 1744/ 3200]\n",
      "loss: 0.226490  [ 1760/ 3200]\n",
      "loss: 0.605389  [ 1776/ 3200]\n",
      "loss: 0.276510  [ 1792/ 3200]\n",
      "loss: 0.326850  [ 1808/ 3200]\n",
      "loss: 0.546987  [ 1824/ 3200]\n",
      "loss: 0.324208  [ 1840/ 3200]\n",
      "loss: 0.448353  [ 1856/ 3200]\n",
      "loss: 0.344595  [ 1872/ 3200]\n",
      "loss: 0.222335  [ 1888/ 3200]\n",
      "loss: 0.349382  [ 1904/ 3200]\n",
      "loss: 0.230836  [ 1920/ 3200]\n",
      "loss: 0.375062  [ 1936/ 3200]\n",
      "loss: 0.167615  [ 1952/ 3200]\n",
      "loss: 0.190974  [ 1968/ 3200]\n",
      "loss: 0.315448  [ 1984/ 3200]\n",
      "loss: 0.267881  [ 2000/ 3200]\n",
      "loss: 0.423506  [ 2016/ 3200]\n",
      "loss: 0.423203  [ 2032/ 3200]\n",
      "loss: 0.609608  [ 2048/ 3200]\n",
      "loss: 0.298387  [ 2064/ 3200]\n",
      "loss: 0.449180  [ 2080/ 3200]\n",
      "loss: 0.693562  [ 2096/ 3200]\n",
      "loss: 0.436794  [ 2112/ 3200]\n",
      "loss: 0.550257  [ 2128/ 3200]\n",
      "loss: 0.543186  [ 2144/ 3200]\n",
      "loss: 0.696054  [ 2160/ 3200]\n",
      "loss: 0.272787  [ 2176/ 3200]\n",
      "loss: 0.551335  [ 2192/ 3200]\n",
      "loss: 0.223390  [ 2208/ 3200]\n",
      "loss: 0.441858  [ 2224/ 3200]\n",
      "loss: 0.503308  [ 2240/ 3200]\n",
      "loss: 0.260238  [ 2256/ 3200]\n",
      "loss: 0.401740  [ 2272/ 3200]\n",
      "loss: 0.696788  [ 2288/ 3200]\n",
      "loss: 0.435091  [ 2304/ 3200]\n",
      "loss: 0.500743  [ 2320/ 3200]\n",
      "loss: 0.553019  [ 2336/ 3200]\n",
      "loss: 0.415142  [ 2352/ 3200]\n",
      "loss: 0.577425  [ 2368/ 3200]\n",
      "loss: 0.418053  [ 2384/ 3200]\n",
      "loss: 0.400122  [ 2400/ 3200]\n",
      "loss: 0.287996  [ 2416/ 3200]\n",
      "loss: 0.790627  [ 2432/ 3200]\n",
      "loss: 0.425696  [ 2448/ 3200]\n",
      "loss: 0.557977  [ 2464/ 3200]\n",
      "loss: 0.218739  [ 2480/ 3200]\n",
      "loss: 0.185185  [ 2496/ 3200]\n",
      "loss: 0.507042  [ 2512/ 3200]\n",
      "loss: 0.228793  [ 2528/ 3200]\n",
      "loss: 0.346087  [ 2544/ 3200]\n",
      "loss: 0.215706  [ 2560/ 3200]\n",
      "loss: 0.496995  [ 2576/ 3200]\n",
      "loss: 0.329096  [ 2592/ 3200]\n",
      "loss: 0.460788  [ 2608/ 3200]\n",
      "loss: 0.296008  [ 2624/ 3200]\n",
      "loss: 0.331492  [ 2640/ 3200]\n",
      "loss: 0.210430  [ 2656/ 3200]\n",
      "loss: 0.246874  [ 2672/ 3200]\n",
      "loss: 0.417587  [ 2688/ 3200]\n",
      "loss: 0.568795  [ 2704/ 3200]\n",
      "loss: 0.758683  [ 2720/ 3200]\n",
      "loss: 0.209591  [ 2736/ 3200]\n",
      "loss: 0.293696  [ 2752/ 3200]\n",
      "loss: 0.221161  [ 2768/ 3200]\n",
      "loss: 0.383369  [ 2784/ 3200]\n",
      "loss: 0.135280  [ 2800/ 3200]\n",
      "loss: 0.389327  [ 2816/ 3200]\n",
      "loss: 0.304076  [ 2832/ 3200]\n",
      "loss: 0.788973  [ 2848/ 3200]\n",
      "loss: 0.199706  [ 2864/ 3200]\n",
      "loss: 0.564730  [ 2880/ 3200]\n",
      "loss: 0.397487  [ 2896/ 3200]\n",
      "loss: 0.724751  [ 2912/ 3200]\n",
      "loss: 0.509243  [ 2928/ 3200]\n",
      "loss: 0.457347  [ 2944/ 3200]\n",
      "loss: 0.520009  [ 2960/ 3200]\n",
      "loss: 0.237501  [ 2976/ 3200]\n",
      "loss: 0.265768  [ 2992/ 3200]\n",
      "loss: 0.224200  [ 3008/ 3200]\n",
      "loss: 0.084592  [ 3024/ 3200]\n",
      "loss: 0.946365  [ 3040/ 3200]\n",
      "loss: 0.259146  [ 3056/ 3200]\n",
      "loss: 0.197989  [ 3072/ 3200]\n",
      "loss: 0.100926  [ 3088/ 3200]\n",
      "loss: 0.304528  [ 3104/ 3200]\n",
      "loss: 0.626523  [ 3120/ 3200]\n",
      "loss: 0.321436  [ 3136/ 3200]\n",
      "loss: 0.582522  [ 3152/ 3200]\n",
      "loss: 0.363273  [ 3168/ 3200]\n",
      "loss: 0.436696  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.035892\n",
      "f1 macro averaged score: 0.765332\n",
      "Accuracy               : 77.1%\n",
      "Confusion matrix       :\n",
      "tensor([[180,   6,   0,  14],\n",
      "        [ 18, 103,  25,  54],\n",
      "        [  0,  18, 164,  18],\n",
      "        [  8,   9,  13, 170]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3268e-03.\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 0.282086  [    0/ 3200]\n",
      "loss: 0.420427  [   16/ 3200]\n",
      "loss: 0.450545  [   32/ 3200]\n",
      "loss: 0.359270  [   48/ 3200]\n",
      "loss: 0.274293  [   64/ 3200]\n",
      "loss: 0.373022  [   80/ 3200]\n",
      "loss: 0.366253  [   96/ 3200]\n",
      "loss: 0.148968  [  112/ 3200]\n",
      "loss: 0.297991  [  128/ 3200]\n",
      "loss: 0.597421  [  144/ 3200]\n",
      "loss: 0.591509  [  160/ 3200]\n",
      "loss: 0.376630  [  176/ 3200]\n",
      "loss: 0.448975  [  192/ 3200]\n",
      "loss: 0.183361  [  208/ 3200]\n",
      "loss: 0.434079  [  224/ 3200]\n",
      "loss: 0.307609  [  240/ 3200]\n",
      "loss: 0.230578  [  256/ 3200]\n",
      "loss: 0.294995  [  272/ 3200]\n",
      "loss: 0.565902  [  288/ 3200]\n",
      "loss: 0.227452  [  304/ 3200]\n",
      "loss: 0.626587  [  320/ 3200]\n",
      "loss: 0.368665  [  336/ 3200]\n",
      "loss: 0.432608  [  352/ 3200]\n",
      "loss: 0.462458  [  368/ 3200]\n",
      "loss: 0.093482  [  384/ 3200]\n",
      "loss: 0.386988  [  400/ 3200]\n",
      "loss: 0.445908  [  416/ 3200]\n",
      "loss: 0.078382  [  432/ 3200]\n",
      "loss: 0.274780  [  448/ 3200]\n",
      "loss: 0.197692  [  464/ 3200]\n",
      "loss: 0.411595  [  480/ 3200]\n",
      "loss: 0.187335  [  496/ 3200]\n",
      "loss: 0.294994  [  512/ 3200]\n",
      "loss: 0.330018  [  528/ 3200]\n",
      "loss: 0.347933  [  544/ 3200]\n",
      "loss: 0.455340  [  560/ 3200]\n",
      "loss: 0.355590  [  576/ 3200]\n",
      "loss: 0.327525  [  592/ 3200]\n",
      "loss: 0.341575  [  608/ 3200]\n",
      "loss: 0.568943  [  624/ 3200]\n",
      "loss: 0.446090  [  640/ 3200]\n",
      "loss: 0.230266  [  656/ 3200]\n",
      "loss: 0.399971  [  672/ 3200]\n",
      "loss: 0.150004  [  688/ 3200]\n",
      "loss: 0.578657  [  704/ 3200]\n",
      "loss: 0.666262  [  720/ 3200]\n",
      "loss: 0.389547  [  736/ 3200]\n",
      "loss: 0.347071  [  752/ 3200]\n",
      "loss: 0.310134  [  768/ 3200]\n",
      "loss: 0.393648  [  784/ 3200]\n",
      "loss: 0.388479  [  800/ 3200]\n",
      "loss: 0.399557  [  816/ 3200]\n",
      "loss: 0.237430  [  832/ 3200]\n",
      "loss: 0.148910  [  848/ 3200]\n",
      "loss: 0.509575  [  864/ 3200]\n",
      "loss: 0.255856  [  880/ 3200]\n",
      "loss: 0.199251  [  896/ 3200]\n",
      "loss: 0.444741  [  912/ 3200]\n",
      "loss: 0.198809  [  928/ 3200]\n",
      "loss: 0.296737  [  944/ 3200]\n",
      "loss: 0.538546  [  960/ 3200]\n",
      "loss: 0.310363  [  976/ 3200]\n",
      "loss: 0.692516  [  992/ 3200]\n",
      "loss: 0.413360  [ 1008/ 3200]\n",
      "loss: 0.506782  [ 1024/ 3200]\n",
      "loss: 0.429556  [ 1040/ 3200]\n",
      "loss: 0.350586  [ 1056/ 3200]\n",
      "loss: 0.585835  [ 1072/ 3200]\n",
      "loss: 0.760585  [ 1088/ 3200]\n",
      "loss: 0.242767  [ 1104/ 3200]\n",
      "loss: 0.404011  [ 1120/ 3200]\n",
      "loss: 0.317672  [ 1136/ 3200]\n",
      "loss: 0.305030  [ 1152/ 3200]\n",
      "loss: 0.345707  [ 1168/ 3200]\n",
      "loss: 0.367834  [ 1184/ 3200]\n",
      "loss: 0.265413  [ 1200/ 3200]\n",
      "loss: 0.643402  [ 1216/ 3200]\n",
      "loss: 0.475199  [ 1232/ 3200]\n",
      "loss: 0.338463  [ 1248/ 3200]\n",
      "loss: 0.472013  [ 1264/ 3200]\n",
      "loss: 0.327664  [ 1280/ 3200]\n",
      "loss: 0.429654  [ 1296/ 3200]\n",
      "loss: 0.357437  [ 1312/ 3200]\n",
      "loss: 0.448934  [ 1328/ 3200]\n",
      "loss: 0.461075  [ 1344/ 3200]\n",
      "loss: 0.326795  [ 1360/ 3200]\n",
      "loss: 0.334395  [ 1376/ 3200]\n",
      "loss: 0.251338  [ 1392/ 3200]\n",
      "loss: 0.616517  [ 1408/ 3200]\n",
      "loss: 0.281684  [ 1424/ 3200]\n",
      "loss: 0.861475  [ 1440/ 3200]\n",
      "loss: 0.443447  [ 1456/ 3200]\n",
      "loss: 0.629516  [ 1472/ 3200]\n",
      "loss: 0.690555  [ 1488/ 3200]\n",
      "loss: 0.163329  [ 1504/ 3200]\n",
      "loss: 0.128532  [ 1520/ 3200]\n",
      "loss: 0.271841  [ 1536/ 3200]\n",
      "loss: 0.448573  [ 1552/ 3200]\n",
      "loss: 0.210040  [ 1568/ 3200]\n",
      "loss: 0.087724  [ 1584/ 3200]\n",
      "loss: 0.361007  [ 1600/ 3200]\n",
      "loss: 0.323061  [ 1616/ 3200]\n",
      "loss: 0.345564  [ 1632/ 3200]\n",
      "loss: 0.346124  [ 1648/ 3200]\n",
      "loss: 0.379152  [ 1664/ 3200]\n",
      "loss: 0.304482  [ 1680/ 3200]\n",
      "loss: 0.207771  [ 1696/ 3200]\n",
      "loss: 0.309980  [ 1712/ 3200]\n",
      "loss: 0.416897  [ 1728/ 3200]\n",
      "loss: 0.544835  [ 1744/ 3200]\n",
      "loss: 0.283280  [ 1760/ 3200]\n",
      "loss: 0.313717  [ 1776/ 3200]\n",
      "loss: 0.720401  [ 1792/ 3200]\n",
      "loss: 0.396924  [ 1808/ 3200]\n",
      "loss: 0.184580  [ 1824/ 3200]\n",
      "loss: 0.380963  [ 1840/ 3200]\n",
      "loss: 0.537112  [ 1856/ 3200]\n",
      "loss: 0.311721  [ 1872/ 3200]\n",
      "loss: 0.167252  [ 1888/ 3200]\n",
      "loss: 0.338658  [ 1904/ 3200]\n",
      "loss: 0.298263  [ 1920/ 3200]\n",
      "loss: 0.294663  [ 1936/ 3200]\n",
      "loss: 0.593865  [ 1952/ 3200]\n",
      "loss: 0.235662  [ 1968/ 3200]\n",
      "loss: 0.271829  [ 1984/ 3200]\n",
      "loss: 0.356518  [ 2000/ 3200]\n",
      "loss: 0.263480  [ 2016/ 3200]\n",
      "loss: 0.310933  [ 2032/ 3200]\n",
      "loss: 0.318392  [ 2048/ 3200]\n",
      "loss: 0.266504  [ 2064/ 3200]\n",
      "loss: 0.281108  [ 2080/ 3200]\n",
      "loss: 0.103047  [ 2096/ 3200]\n",
      "loss: 0.270487  [ 2112/ 3200]\n",
      "loss: 0.305065  [ 2128/ 3200]\n",
      "loss: 0.209137  [ 2144/ 3200]\n",
      "loss: 0.689895  [ 2160/ 3200]\n",
      "loss: 0.263024  [ 2176/ 3200]\n",
      "loss: 0.163188  [ 2192/ 3200]\n",
      "loss: 0.190254  [ 2208/ 3200]\n",
      "loss: 0.640272  [ 2224/ 3200]\n",
      "loss: 0.214706  [ 2240/ 3200]\n",
      "loss: 0.374340  [ 2256/ 3200]\n",
      "loss: 0.306432  [ 2272/ 3200]\n",
      "loss: 0.508167  [ 2288/ 3200]\n",
      "loss: 0.455308  [ 2304/ 3200]\n",
      "loss: 0.541949  [ 2320/ 3200]\n",
      "loss: 0.329828  [ 2336/ 3200]\n",
      "loss: 0.482659  [ 2352/ 3200]\n",
      "loss: 0.705185  [ 2368/ 3200]\n",
      "loss: 0.282469  [ 2384/ 3200]\n",
      "loss: 0.247311  [ 2400/ 3200]\n",
      "loss: 0.250864  [ 2416/ 3200]\n",
      "loss: 0.214831  [ 2432/ 3200]\n",
      "loss: 0.195428  [ 2448/ 3200]\n",
      "loss: 0.181829  [ 2464/ 3200]\n",
      "loss: 0.278436  [ 2480/ 3200]\n",
      "loss: 0.187814  [ 2496/ 3200]\n",
      "loss: 0.343801  [ 2512/ 3200]\n",
      "loss: 0.432514  [ 2528/ 3200]\n",
      "loss: 0.256411  [ 2544/ 3200]\n",
      "loss: 0.264427  [ 2560/ 3200]\n",
      "loss: 0.341889  [ 2576/ 3200]\n",
      "loss: 0.444512  [ 2592/ 3200]\n",
      "loss: 0.455383  [ 2608/ 3200]\n",
      "loss: 0.339764  [ 2624/ 3200]\n",
      "loss: 0.493646  [ 2640/ 3200]\n",
      "loss: 0.246953  [ 2656/ 3200]\n",
      "loss: 0.346781  [ 2672/ 3200]\n",
      "loss: 0.520762  [ 2688/ 3200]\n",
      "loss: 0.169824  [ 2704/ 3200]\n",
      "loss: 0.561909  [ 2720/ 3200]\n",
      "loss: 0.407449  [ 2736/ 3200]\n",
      "loss: 0.335330  [ 2752/ 3200]\n",
      "loss: 0.878565  [ 2768/ 3200]\n",
      "loss: 0.549145  [ 2784/ 3200]\n",
      "loss: 0.145075  [ 2800/ 3200]\n",
      "loss: 0.162517  [ 2816/ 3200]\n",
      "loss: 0.455696  [ 2832/ 3200]\n",
      "loss: 0.618392  [ 2848/ 3200]\n",
      "loss: 0.347112  [ 2864/ 3200]\n",
      "loss: 0.375558  [ 2880/ 3200]\n",
      "loss: 0.482112  [ 2896/ 3200]\n",
      "loss: 0.420248  [ 2912/ 3200]\n",
      "loss: 0.207195  [ 2928/ 3200]\n",
      "loss: 0.465149  [ 2944/ 3200]\n",
      "loss: 0.240517  [ 2960/ 3200]\n",
      "loss: 0.744863  [ 2976/ 3200]\n",
      "loss: 0.541458  [ 2992/ 3200]\n",
      "loss: 0.296060  [ 3008/ 3200]\n",
      "loss: 0.213908  [ 3024/ 3200]\n",
      "loss: 0.245373  [ 3040/ 3200]\n",
      "loss: 0.373721  [ 3056/ 3200]\n",
      "loss: 0.419714  [ 3072/ 3200]\n",
      "loss: 0.219925  [ 3088/ 3200]\n",
      "loss: 0.118860  [ 3104/ 3200]\n",
      "loss: 0.330247  [ 3120/ 3200]\n",
      "loss: 0.590511  [ 3136/ 3200]\n",
      "loss: 0.405296  [ 3152/ 3200]\n",
      "loss: 0.220691  [ 3168/ 3200]\n",
      "loss: 0.481853  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.033933\n",
      "f1 macro averaged score: 0.783617\n",
      "Accuracy               : 78.2%\n",
      "Confusion matrix       :\n",
      "tensor([[173,  21,   0,   6],\n",
      "        [ 14, 136,  23,  27],\n",
      "        [  0,  25, 168,   7],\n",
      "        [  5,  30,  16, 149]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2605e-03.\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 0.203712  [    0/ 3200]\n",
      "loss: 0.385945  [   16/ 3200]\n",
      "loss: 0.262137  [   32/ 3200]\n",
      "loss: 0.718999  [   48/ 3200]\n",
      "loss: 0.724516  [   64/ 3200]\n",
      "loss: 0.356016  [   80/ 3200]\n",
      "loss: 0.285232  [   96/ 3200]\n",
      "loss: 0.343453  [  112/ 3200]\n",
      "loss: 0.114812  [  128/ 3200]\n",
      "loss: 0.289881  [  144/ 3200]\n",
      "loss: 0.362590  [  160/ 3200]\n",
      "loss: 0.296671  [  176/ 3200]\n",
      "loss: 0.363810  [  192/ 3200]\n",
      "loss: 0.392210  [  208/ 3200]\n",
      "loss: 0.586676  [  224/ 3200]\n",
      "loss: 0.312938  [  240/ 3200]\n",
      "loss: 0.503688  [  256/ 3200]\n",
      "loss: 0.650621  [  272/ 3200]\n",
      "loss: 0.342451  [  288/ 3200]\n",
      "loss: 0.421243  [  304/ 3200]\n",
      "loss: 0.259828  [  320/ 3200]\n",
      "loss: 0.264427  [  336/ 3200]\n",
      "loss: 0.648197  [  352/ 3200]\n",
      "loss: 0.135229  [  368/ 3200]\n",
      "loss: 0.177174  [  384/ 3200]\n",
      "loss: 0.224861  [  400/ 3200]\n",
      "loss: 0.226837  [  416/ 3200]\n",
      "loss: 0.367800  [  432/ 3200]\n",
      "loss: 0.267651  [  448/ 3200]\n",
      "loss: 0.340272  [  464/ 3200]\n",
      "loss: 0.269045  [  480/ 3200]\n",
      "loss: 0.455472  [  496/ 3200]\n",
      "loss: 0.235798  [  512/ 3200]\n",
      "loss: 0.571513  [  528/ 3200]\n",
      "loss: 0.331494  [  544/ 3200]\n",
      "loss: 0.230178  [  560/ 3200]\n",
      "loss: 0.512954  [  576/ 3200]\n",
      "loss: 0.188837  [  592/ 3200]\n",
      "loss: 0.409945  [  608/ 3200]\n",
      "loss: 0.132696  [  624/ 3200]\n",
      "loss: 0.219206  [  640/ 3200]\n",
      "loss: 0.282786  [  656/ 3200]\n",
      "loss: 0.248917  [  672/ 3200]\n",
      "loss: 0.239604  [  688/ 3200]\n",
      "loss: 0.538977  [  704/ 3200]\n",
      "loss: 0.314653  [  720/ 3200]\n",
      "loss: 0.328241  [  736/ 3200]\n",
      "loss: 0.337935  [  752/ 3200]\n",
      "loss: 0.783186  [  768/ 3200]\n",
      "loss: 0.253909  [  784/ 3200]\n",
      "loss: 0.569608  [  800/ 3200]\n",
      "loss: 0.731610  [  816/ 3200]\n",
      "loss: 0.413470  [  832/ 3200]\n",
      "loss: 0.402797  [  848/ 3200]\n",
      "loss: 0.326947  [  864/ 3200]\n",
      "loss: 0.490822  [  880/ 3200]\n",
      "loss: 0.243494  [  896/ 3200]\n",
      "loss: 0.389800  [  912/ 3200]\n",
      "loss: 0.168487  [  928/ 3200]\n",
      "loss: 0.269048  [  944/ 3200]\n",
      "loss: 0.313411  [  960/ 3200]\n",
      "loss: 0.132364  [  976/ 3200]\n",
      "loss: 0.780838  [  992/ 3200]\n",
      "loss: 0.192237  [ 1008/ 3200]\n",
      "loss: 0.151411  [ 1024/ 3200]\n",
      "loss: 0.446433  [ 1040/ 3200]\n",
      "loss: 0.112176  [ 1056/ 3200]\n",
      "loss: 0.354076  [ 1072/ 3200]\n",
      "loss: 0.768246  [ 1088/ 3200]\n",
      "loss: 0.166168  [ 1104/ 3200]\n",
      "loss: 0.322386  [ 1120/ 3200]\n",
      "loss: 0.294841  [ 1136/ 3200]\n",
      "loss: 0.318419  [ 1152/ 3200]\n",
      "loss: 0.307906  [ 1168/ 3200]\n",
      "loss: 0.338891  [ 1184/ 3200]\n",
      "loss: 0.219749  [ 1200/ 3200]\n",
      "loss: 0.268769  [ 1216/ 3200]\n",
      "loss: 0.335338  [ 1232/ 3200]\n",
      "loss: 0.391461  [ 1248/ 3200]\n",
      "loss: 0.353457  [ 1264/ 3200]\n",
      "loss: 0.308538  [ 1280/ 3200]\n",
      "loss: 0.228824  [ 1296/ 3200]\n",
      "loss: 0.262807  [ 1312/ 3200]\n",
      "loss: 0.116429  [ 1328/ 3200]\n",
      "loss: 0.216680  [ 1344/ 3200]\n",
      "loss: 0.263761  [ 1360/ 3200]\n",
      "loss: 0.133882  [ 1376/ 3200]\n",
      "loss: 0.288106  [ 1392/ 3200]\n",
      "loss: 0.233005  [ 1408/ 3200]\n",
      "loss: 0.231954  [ 1424/ 3200]\n",
      "loss: 0.100614  [ 1440/ 3200]\n",
      "loss: 0.241394  [ 1456/ 3200]\n",
      "loss: 0.262840  [ 1472/ 3200]\n",
      "loss: 0.442931  [ 1488/ 3200]\n",
      "loss: 0.089099  [ 1504/ 3200]\n",
      "loss: 0.221544  [ 1520/ 3200]\n",
      "loss: 0.479383  [ 1536/ 3200]\n",
      "loss: 0.415514  [ 1552/ 3200]\n",
      "loss: 0.417669  [ 1568/ 3200]\n",
      "loss: 0.251354  [ 1584/ 3200]\n",
      "loss: 0.294525  [ 1600/ 3200]\n",
      "loss: 0.281962  [ 1616/ 3200]\n",
      "loss: 0.272947  [ 1632/ 3200]\n",
      "loss: 0.262082  [ 1648/ 3200]\n",
      "loss: 0.295932  [ 1664/ 3200]\n",
      "loss: 0.798214  [ 1680/ 3200]\n",
      "loss: 0.231561  [ 1696/ 3200]\n",
      "loss: 0.206812  [ 1712/ 3200]\n",
      "loss: 0.331386  [ 1728/ 3200]\n",
      "loss: 0.747768  [ 1744/ 3200]\n",
      "loss: 0.356599  [ 1760/ 3200]\n",
      "loss: 0.653604  [ 1776/ 3200]\n",
      "loss: 0.109598  [ 1792/ 3200]\n",
      "loss: 0.510981  [ 1808/ 3200]\n",
      "loss: 0.195688  [ 1824/ 3200]\n",
      "loss: 0.249635  [ 1840/ 3200]\n",
      "loss: 0.165838  [ 1856/ 3200]\n",
      "loss: 0.255558  [ 1872/ 3200]\n",
      "loss: 0.165842  [ 1888/ 3200]\n",
      "loss: 0.436343  [ 1904/ 3200]\n",
      "loss: 0.393909  [ 1920/ 3200]\n",
      "loss: 0.486901  [ 1936/ 3200]\n",
      "loss: 0.345171  [ 1952/ 3200]\n",
      "loss: 0.243571  [ 1968/ 3200]\n",
      "loss: 0.596776  [ 1984/ 3200]\n",
      "loss: 0.138746  [ 2000/ 3200]\n",
      "loss: 0.355964  [ 2016/ 3200]\n",
      "loss: 0.274857  [ 2032/ 3200]\n",
      "loss: 0.193502  [ 2048/ 3200]\n",
      "loss: 0.105783  [ 2064/ 3200]\n",
      "loss: 0.572780  [ 2080/ 3200]\n",
      "loss: 0.125543  [ 2096/ 3200]\n",
      "loss: 0.323830  [ 2112/ 3200]\n",
      "loss: 0.380390  [ 2128/ 3200]\n",
      "loss: 0.192141  [ 2144/ 3200]\n",
      "loss: 0.351605  [ 2160/ 3200]\n",
      "loss: 0.224085  [ 2176/ 3200]\n",
      "loss: 0.143094  [ 2192/ 3200]\n",
      "loss: 0.235737  [ 2208/ 3200]\n",
      "loss: 0.507981  [ 2224/ 3200]\n",
      "loss: 0.239864  [ 2240/ 3200]\n",
      "loss: 0.654792  [ 2256/ 3200]\n",
      "loss: 0.078087  [ 2272/ 3200]\n",
      "loss: 0.453706  [ 2288/ 3200]\n",
      "loss: 0.628836  [ 2304/ 3200]\n",
      "loss: 0.446259  [ 2320/ 3200]\n",
      "loss: 0.323662  [ 2336/ 3200]\n",
      "loss: 0.337856  [ 2352/ 3200]\n",
      "loss: 0.150722  [ 2368/ 3200]\n",
      "loss: 0.107831  [ 2384/ 3200]\n",
      "loss: 0.261651  [ 2400/ 3200]\n",
      "loss: 0.167930  [ 2416/ 3200]\n",
      "loss: 0.190251  [ 2432/ 3200]\n",
      "loss: 0.556721  [ 2448/ 3200]\n",
      "loss: 0.599583  [ 2464/ 3200]\n",
      "loss: 0.530440  [ 2480/ 3200]\n",
      "loss: 0.319999  [ 2496/ 3200]\n",
      "loss: 0.386716  [ 2512/ 3200]\n",
      "loss: 0.233125  [ 2528/ 3200]\n",
      "loss: 0.290051  [ 2544/ 3200]\n",
      "loss: 0.551257  [ 2560/ 3200]\n",
      "loss: 0.234150  [ 2576/ 3200]\n",
      "loss: 0.160528  [ 2592/ 3200]\n",
      "loss: 0.314533  [ 2608/ 3200]\n",
      "loss: 0.288093  [ 2624/ 3200]\n",
      "loss: 0.322451  [ 2640/ 3200]\n",
      "loss: 0.265477  [ 2656/ 3200]\n",
      "loss: 0.328818  [ 2672/ 3200]\n",
      "loss: 0.495591  [ 2688/ 3200]\n",
      "loss: 0.174187  [ 2704/ 3200]\n",
      "loss: 0.180742  [ 2720/ 3200]\n",
      "loss: 0.570772  [ 2736/ 3200]\n",
      "loss: 0.272166  [ 2752/ 3200]\n",
      "loss: 0.310383  [ 2768/ 3200]\n",
      "loss: 0.472899  [ 2784/ 3200]\n",
      "loss: 0.139724  [ 2800/ 3200]\n",
      "loss: 0.394377  [ 2816/ 3200]\n",
      "loss: 0.273472  [ 2832/ 3200]\n",
      "loss: 0.098547  [ 2848/ 3200]\n",
      "loss: 0.218433  [ 2864/ 3200]\n",
      "loss: 0.384222  [ 2880/ 3200]\n",
      "loss: 0.235162  [ 2896/ 3200]\n",
      "loss: 0.278554  [ 2912/ 3200]\n",
      "loss: 0.419156  [ 2928/ 3200]\n",
      "loss: 0.284094  [ 2944/ 3200]\n",
      "loss: 0.327992  [ 2960/ 3200]\n",
      "loss: 0.202954  [ 2976/ 3200]\n",
      "loss: 0.284330  [ 2992/ 3200]\n",
      "loss: 0.257553  [ 3008/ 3200]\n",
      "loss: 0.180356  [ 3024/ 3200]\n",
      "loss: 0.354586  [ 3040/ 3200]\n",
      "loss: 0.169909  [ 3056/ 3200]\n",
      "loss: 0.123787  [ 3072/ 3200]\n",
      "loss: 0.651018  [ 3088/ 3200]\n",
      "loss: 0.541827  [ 3104/ 3200]\n",
      "loss: 0.709919  [ 3120/ 3200]\n",
      "loss: 0.149018  [ 3136/ 3200]\n",
      "loss: 0.248007  [ 3152/ 3200]\n",
      "loss: 0.156951  [ 3168/ 3200]\n",
      "loss: 0.537103  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036388\n",
      "f1 macro averaged score: 0.771996\n",
      "Accuracy               : 76.9%\n",
      "Confusion matrix       :\n",
      "tensor([[164,  29,   0,   7],\n",
      "        [ 10, 139,  19,  32],\n",
      "        [  0,  31, 155,  14],\n",
      "        [  5,  25,  13, 157]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1975e-03.\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 0.270436  [    0/ 3200]\n",
      "loss: 0.464376  [   16/ 3200]\n",
      "loss: 0.177800  [   32/ 3200]\n",
      "loss: 0.596098  [   48/ 3200]\n",
      "loss: 0.577002  [   64/ 3200]\n",
      "loss: 0.388583  [   80/ 3200]\n",
      "loss: 0.279732  [   96/ 3200]\n",
      "loss: 0.308173  [  112/ 3200]\n",
      "loss: 0.202358  [  128/ 3200]\n",
      "loss: 0.209824  [  144/ 3200]\n",
      "loss: 0.277680  [  160/ 3200]\n",
      "loss: 0.199280  [  176/ 3200]\n",
      "loss: 0.326788  [  192/ 3200]\n",
      "loss: 0.144668  [  208/ 3200]\n",
      "loss: 0.337496  [  224/ 3200]\n",
      "loss: 0.241382  [  240/ 3200]\n",
      "loss: 0.119472  [  256/ 3200]\n",
      "loss: 0.262537  [  272/ 3200]\n",
      "loss: 0.200553  [  288/ 3200]\n",
      "loss: 0.580544  [  304/ 3200]\n",
      "loss: 0.759209  [  320/ 3200]\n",
      "loss: 0.656016  [  336/ 3200]\n",
      "loss: 0.294758  [  352/ 3200]\n",
      "loss: 0.503421  [  368/ 3200]\n",
      "loss: 0.334731  [  384/ 3200]\n",
      "loss: 0.134068  [  400/ 3200]\n",
      "loss: 0.582196  [  416/ 3200]\n",
      "loss: 0.153046  [  432/ 3200]\n",
      "loss: 0.405330  [  448/ 3200]\n",
      "loss: 0.377811  [  464/ 3200]\n",
      "loss: 0.082915  [  480/ 3200]\n",
      "loss: 0.251085  [  496/ 3200]\n",
      "loss: 0.532873  [  512/ 3200]\n",
      "loss: 0.348256  [  528/ 3200]\n",
      "loss: 0.294128  [  544/ 3200]\n",
      "loss: 0.238935  [  560/ 3200]\n",
      "loss: 0.083418  [  576/ 3200]\n",
      "loss: 0.160025  [  592/ 3200]\n",
      "loss: 0.186481  [  608/ 3200]\n",
      "loss: 0.496152  [  624/ 3200]\n",
      "loss: 0.651675  [  640/ 3200]\n",
      "loss: 0.546387  [  656/ 3200]\n",
      "loss: 0.311190  [  672/ 3200]\n",
      "loss: 0.390958  [  688/ 3200]\n",
      "loss: 0.262503  [  704/ 3200]\n",
      "loss: 0.472318  [  720/ 3200]\n",
      "loss: 0.437263  [  736/ 3200]\n",
      "loss: 0.283581  [  752/ 3200]\n",
      "loss: 0.124060  [  768/ 3200]\n",
      "loss: 0.117293  [  784/ 3200]\n",
      "loss: 0.232129  [  800/ 3200]\n",
      "loss: 0.417389  [  816/ 3200]\n",
      "loss: 0.397077  [  832/ 3200]\n",
      "loss: 0.378643  [  848/ 3200]\n",
      "loss: 0.343565  [  864/ 3200]\n",
      "loss: 0.325462  [  880/ 3200]\n",
      "loss: 0.206310  [  896/ 3200]\n",
      "loss: 0.442909  [  912/ 3200]\n",
      "loss: 0.219902  [  928/ 3200]\n",
      "loss: 0.271245  [  944/ 3200]\n",
      "loss: 0.194915  [  960/ 3200]\n",
      "loss: 0.207618  [  976/ 3200]\n",
      "loss: 0.232732  [  992/ 3200]\n",
      "loss: 0.186488  [ 1008/ 3200]\n",
      "loss: 0.163102  [ 1024/ 3200]\n",
      "loss: 0.380033  [ 1040/ 3200]\n",
      "loss: 0.173759  [ 1056/ 3200]\n",
      "loss: 0.288206  [ 1072/ 3200]\n",
      "loss: 0.150186  [ 1088/ 3200]\n",
      "loss: 0.462761  [ 1104/ 3200]\n",
      "loss: 0.520452  [ 1120/ 3200]\n",
      "loss: 0.063937  [ 1136/ 3200]\n",
      "loss: 0.387499  [ 1152/ 3200]\n",
      "loss: 0.183091  [ 1168/ 3200]\n",
      "loss: 0.205494  [ 1184/ 3200]\n",
      "loss: 0.470957  [ 1200/ 3200]\n",
      "loss: 0.185332  [ 1216/ 3200]\n",
      "loss: 0.172361  [ 1232/ 3200]\n",
      "loss: 0.190972  [ 1248/ 3200]\n",
      "loss: 0.251904  [ 1264/ 3200]\n",
      "loss: 0.149068  [ 1280/ 3200]\n",
      "loss: 0.146923  [ 1296/ 3200]\n",
      "loss: 0.182461  [ 1312/ 3200]\n",
      "loss: 0.258231  [ 1328/ 3200]\n",
      "loss: 0.081213  [ 1344/ 3200]\n",
      "loss: 0.543778  [ 1360/ 3200]\n",
      "loss: 0.461811  [ 1376/ 3200]\n",
      "loss: 0.482673  [ 1392/ 3200]\n",
      "loss: 0.494966  [ 1408/ 3200]\n",
      "loss: 0.221943  [ 1424/ 3200]\n",
      "loss: 0.264705  [ 1440/ 3200]\n",
      "loss: 0.539480  [ 1456/ 3200]\n",
      "loss: 0.139867  [ 1472/ 3200]\n",
      "loss: 0.192246  [ 1488/ 3200]\n",
      "loss: 0.213007  [ 1504/ 3200]\n",
      "loss: 0.204258  [ 1520/ 3200]\n",
      "loss: 0.370746  [ 1536/ 3200]\n",
      "loss: 0.131759  [ 1552/ 3200]\n",
      "loss: 0.182011  [ 1568/ 3200]\n",
      "loss: 0.178534  [ 1584/ 3200]\n",
      "loss: 0.454388  [ 1600/ 3200]\n",
      "loss: 0.729689  [ 1616/ 3200]\n",
      "loss: 0.317375  [ 1632/ 3200]\n",
      "loss: 0.173285  [ 1648/ 3200]\n",
      "loss: 0.414695  [ 1664/ 3200]\n",
      "loss: 0.260634  [ 1680/ 3200]\n",
      "loss: 0.309036  [ 1696/ 3200]\n",
      "loss: 0.199713  [ 1712/ 3200]\n",
      "loss: 0.498095  [ 1728/ 3200]\n",
      "loss: 0.447840  [ 1744/ 3200]\n",
      "loss: 0.221805  [ 1760/ 3200]\n",
      "loss: 0.505027  [ 1776/ 3200]\n",
      "loss: 0.123123  [ 1792/ 3200]\n",
      "loss: 0.155843  [ 1808/ 3200]\n",
      "loss: 0.160098  [ 1824/ 3200]\n",
      "loss: 0.418221  [ 1840/ 3200]\n",
      "loss: 0.059893  [ 1856/ 3200]\n",
      "loss: 0.185514  [ 1872/ 3200]\n",
      "loss: 0.349399  [ 1888/ 3200]\n",
      "loss: 0.658201  [ 1904/ 3200]\n",
      "loss: 0.261735  [ 1920/ 3200]\n",
      "loss: 0.410744  [ 1936/ 3200]\n",
      "loss: 0.296685  [ 1952/ 3200]\n",
      "loss: 0.074760  [ 1968/ 3200]\n",
      "loss: 0.109529  [ 1984/ 3200]\n",
      "loss: 0.568319  [ 2000/ 3200]\n",
      "loss: 0.166907  [ 2016/ 3200]\n",
      "loss: 0.243641  [ 2032/ 3200]\n",
      "loss: 0.438589  [ 2048/ 3200]\n",
      "loss: 0.608228  [ 2064/ 3200]\n",
      "loss: 0.466866  [ 2080/ 3200]\n",
      "loss: 0.393434  [ 2096/ 3200]\n",
      "loss: 0.427135  [ 2112/ 3200]\n",
      "loss: 0.111194  [ 2128/ 3200]\n",
      "loss: 0.148733  [ 2144/ 3200]\n",
      "loss: 0.660794  [ 2160/ 3200]\n",
      "loss: 0.585366  [ 2176/ 3200]\n",
      "loss: 0.601212  [ 2192/ 3200]\n",
      "loss: 0.269129  [ 2208/ 3200]\n",
      "loss: 0.164835  [ 2224/ 3200]\n",
      "loss: 0.188924  [ 2240/ 3200]\n",
      "loss: 0.268724  [ 2256/ 3200]\n",
      "loss: 0.750477  [ 2272/ 3200]\n",
      "loss: 0.133097  [ 2288/ 3200]\n",
      "loss: 0.185729  [ 2304/ 3200]\n",
      "loss: 0.138671  [ 2320/ 3200]\n",
      "loss: 0.293807  [ 2336/ 3200]\n",
      "loss: 0.362733  [ 2352/ 3200]\n",
      "loss: 0.205686  [ 2368/ 3200]\n",
      "loss: 0.292444  [ 2384/ 3200]\n",
      "loss: 0.303617  [ 2400/ 3200]\n",
      "loss: 0.408345  [ 2416/ 3200]\n",
      "loss: 0.476738  [ 2432/ 3200]\n",
      "loss: 0.192315  [ 2448/ 3200]\n",
      "loss: 0.355217  [ 2464/ 3200]\n",
      "loss: 0.092211  [ 2480/ 3200]\n",
      "loss: 0.256421  [ 2496/ 3200]\n",
      "loss: 0.303400  [ 2512/ 3200]\n",
      "loss: 0.172576  [ 2528/ 3200]\n",
      "loss: 0.104656  [ 2544/ 3200]\n",
      "loss: 0.175384  [ 2560/ 3200]\n",
      "loss: 0.356810  [ 2576/ 3200]\n",
      "loss: 0.338771  [ 2592/ 3200]\n",
      "loss: 0.253089  [ 2608/ 3200]\n",
      "loss: 0.382239  [ 2624/ 3200]\n",
      "loss: 0.260120  [ 2640/ 3200]\n",
      "loss: 0.445287  [ 2656/ 3200]\n",
      "loss: 0.637555  [ 2672/ 3200]\n",
      "loss: 0.250083  [ 2688/ 3200]\n",
      "loss: 0.272296  [ 2704/ 3200]\n",
      "loss: 0.089367  [ 2720/ 3200]\n",
      "loss: 0.341647  [ 2736/ 3200]\n",
      "loss: 0.329044  [ 2752/ 3200]\n",
      "loss: 0.284158  [ 2768/ 3200]\n",
      "loss: 0.125914  [ 2784/ 3200]\n",
      "loss: 0.222477  [ 2800/ 3200]\n",
      "loss: 0.181795  [ 2816/ 3200]\n",
      "loss: 0.336106  [ 2832/ 3200]\n",
      "loss: 0.275548  [ 2848/ 3200]\n",
      "loss: 0.278083  [ 2864/ 3200]\n",
      "loss: 0.333753  [ 2880/ 3200]\n",
      "loss: 0.190571  [ 2896/ 3200]\n",
      "loss: 0.151380  [ 2912/ 3200]\n",
      "loss: 0.240433  [ 2928/ 3200]\n",
      "loss: 0.131715  [ 2944/ 3200]\n",
      "loss: 0.406410  [ 2960/ 3200]\n",
      "loss: 0.368490  [ 2976/ 3200]\n",
      "loss: 0.186092  [ 2992/ 3200]\n",
      "loss: 0.212855  [ 3008/ 3200]\n",
      "loss: 0.244037  [ 3024/ 3200]\n",
      "loss: 0.073043  [ 3040/ 3200]\n",
      "loss: 0.165134  [ 3056/ 3200]\n",
      "loss: 0.470279  [ 3072/ 3200]\n",
      "loss: 0.369302  [ 3088/ 3200]\n",
      "loss: 0.417276  [ 3104/ 3200]\n",
      "loss: 0.291924  [ 3120/ 3200]\n",
      "loss: 0.211595  [ 3136/ 3200]\n",
      "loss: 0.531548  [ 3152/ 3200]\n",
      "loss: 0.440149  [ 3168/ 3200]\n",
      "loss: 0.359238  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036674\n",
      "f1 macro averaged score: 0.775109\n",
      "Accuracy               : 78.1%\n",
      "Confusion matrix       :\n",
      "tensor([[187,   4,   0,   9],\n",
      "        [ 20, 110,  29,  41],\n",
      "        [  0,  20, 168,  12],\n",
      "        [ 12,  13,  15, 160]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1376e-03.\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 0.313748  [    0/ 3200]\n",
      "loss: 0.426986  [   16/ 3200]\n",
      "loss: 0.318688  [   32/ 3200]\n",
      "loss: 0.112699  [   48/ 3200]\n",
      "loss: 0.215099  [   64/ 3200]\n",
      "loss: 0.627576  [   80/ 3200]\n",
      "loss: 0.341741  [   96/ 3200]\n",
      "loss: 0.257935  [  112/ 3200]\n",
      "loss: 0.239829  [  128/ 3200]\n",
      "loss: 0.369401  [  144/ 3200]\n",
      "loss: 0.499453  [  160/ 3200]\n",
      "loss: 0.103706  [  176/ 3200]\n",
      "loss: 0.261096  [  192/ 3200]\n",
      "loss: 0.448281  [  208/ 3200]\n",
      "loss: 0.116089  [  224/ 3200]\n",
      "loss: 0.274494  [  240/ 3200]\n",
      "loss: 0.582244  [  256/ 3200]\n",
      "loss: 0.383258  [  272/ 3200]\n",
      "loss: 0.368534  [  288/ 3200]\n",
      "loss: 0.376586  [  304/ 3200]\n",
      "loss: 0.464256  [  320/ 3200]\n",
      "loss: 0.234293  [  336/ 3200]\n",
      "loss: 0.199116  [  352/ 3200]\n",
      "loss: 0.309075  [  368/ 3200]\n",
      "loss: 0.155809  [  384/ 3200]\n",
      "loss: 0.184812  [  400/ 3200]\n",
      "loss: 0.128203  [  416/ 3200]\n",
      "loss: 0.113154  [  432/ 3200]\n",
      "loss: 0.142639  [  448/ 3200]\n",
      "loss: 0.101735  [  464/ 3200]\n",
      "loss: 0.957702  [  480/ 3200]\n",
      "loss: 0.153090  [  496/ 3200]\n",
      "loss: 0.545822  [  512/ 3200]\n",
      "loss: 0.135495  [  528/ 3200]\n",
      "loss: 0.147856  [  544/ 3200]\n",
      "loss: 0.279857  [  560/ 3200]\n",
      "loss: 0.293464  [  576/ 3200]\n",
      "loss: 0.256385  [  592/ 3200]\n",
      "loss: 0.200992  [  608/ 3200]\n",
      "loss: 0.446825  [  624/ 3200]\n",
      "loss: 0.091743  [  640/ 3200]\n",
      "loss: 0.523086  [  656/ 3200]\n",
      "loss: 0.290017  [  672/ 3200]\n",
      "loss: 0.323369  [  688/ 3200]\n",
      "loss: 0.119852  [  704/ 3200]\n",
      "loss: 0.281402  [  720/ 3200]\n",
      "loss: 0.343548  [  736/ 3200]\n",
      "loss: 0.353149  [  752/ 3200]\n",
      "loss: 0.303647  [  768/ 3200]\n",
      "loss: 0.369780  [  784/ 3200]\n",
      "loss: 0.137268  [  800/ 3200]\n",
      "loss: 0.257065  [  816/ 3200]\n",
      "loss: 0.224571  [  832/ 3200]\n",
      "loss: 0.120254  [  848/ 3200]\n",
      "loss: 0.519958  [  864/ 3200]\n",
      "loss: 0.509760  [  880/ 3200]\n",
      "loss: 0.140171  [  896/ 3200]\n",
      "loss: 0.171689  [  912/ 3200]\n",
      "loss: 0.202178  [  928/ 3200]\n",
      "loss: 0.225967  [  944/ 3200]\n",
      "loss: 0.280755  [  960/ 3200]\n",
      "loss: 0.254559  [  976/ 3200]\n",
      "loss: 0.467429  [  992/ 3200]\n",
      "loss: 0.538985  [ 1008/ 3200]\n",
      "loss: 0.366016  [ 1024/ 3200]\n",
      "loss: 0.161471  [ 1040/ 3200]\n",
      "loss: 0.258284  [ 1056/ 3200]\n",
      "loss: 0.217126  [ 1072/ 3200]\n",
      "loss: 0.238544  [ 1088/ 3200]\n",
      "loss: 0.566676  [ 1104/ 3200]\n",
      "loss: 0.119846  [ 1120/ 3200]\n",
      "loss: 0.245436  [ 1136/ 3200]\n",
      "loss: 0.213092  [ 1152/ 3200]\n",
      "loss: 0.159784  [ 1168/ 3200]\n",
      "loss: 0.441292  [ 1184/ 3200]\n",
      "loss: 0.243502  [ 1200/ 3200]\n",
      "loss: 0.561918  [ 1216/ 3200]\n",
      "loss: 0.188416  [ 1232/ 3200]\n",
      "loss: 0.203781  [ 1248/ 3200]\n",
      "loss: 0.178035  [ 1264/ 3200]\n",
      "loss: 0.197493  [ 1280/ 3200]\n",
      "loss: 0.075947  [ 1296/ 3200]\n",
      "loss: 0.347463  [ 1312/ 3200]\n",
      "loss: 0.123905  [ 1328/ 3200]\n",
      "loss: 0.215225  [ 1344/ 3200]\n",
      "loss: 0.079312  [ 1360/ 3200]\n",
      "loss: 0.283828  [ 1376/ 3200]\n",
      "loss: 0.356691  [ 1392/ 3200]\n",
      "loss: 0.333625  [ 1408/ 3200]\n",
      "loss: 0.217877  [ 1424/ 3200]\n",
      "loss: 0.289057  [ 1440/ 3200]\n",
      "loss: 0.231041  [ 1456/ 3200]\n",
      "loss: 0.163996  [ 1472/ 3200]\n",
      "loss: 0.332166  [ 1488/ 3200]\n",
      "loss: 0.499911  [ 1504/ 3200]\n",
      "loss: 0.316734  [ 1520/ 3200]\n",
      "loss: 0.336809  [ 1536/ 3200]\n",
      "loss: 0.177663  [ 1552/ 3200]\n",
      "loss: 0.588999  [ 1568/ 3200]\n",
      "loss: 0.199533  [ 1584/ 3200]\n",
      "loss: 0.426083  [ 1600/ 3200]\n",
      "loss: 0.224433  [ 1616/ 3200]\n",
      "loss: 0.329875  [ 1632/ 3200]\n",
      "loss: 0.331322  [ 1648/ 3200]\n",
      "loss: 0.189873  [ 1664/ 3200]\n",
      "loss: 0.445882  [ 1680/ 3200]\n",
      "loss: 0.426814  [ 1696/ 3200]\n",
      "loss: 0.349216  [ 1712/ 3200]\n",
      "loss: 0.400940  [ 1728/ 3200]\n",
      "loss: 0.098596  [ 1744/ 3200]\n",
      "loss: 0.155765  [ 1760/ 3200]\n",
      "loss: 0.087864  [ 1776/ 3200]\n",
      "loss: 0.152018  [ 1792/ 3200]\n",
      "loss: 0.227001  [ 1808/ 3200]\n",
      "loss: 0.233875  [ 1824/ 3200]\n",
      "loss: 0.143808  [ 1840/ 3200]\n",
      "loss: 0.385285  [ 1856/ 3200]\n",
      "loss: 0.079585  [ 1872/ 3200]\n",
      "loss: 0.264320  [ 1888/ 3200]\n",
      "loss: 0.287161  [ 1904/ 3200]\n",
      "loss: 0.181125  [ 1920/ 3200]\n",
      "loss: 0.149938  [ 1936/ 3200]\n",
      "loss: 0.240814  [ 1952/ 3200]\n",
      "loss: 0.242526  [ 1968/ 3200]\n",
      "loss: 0.226729  [ 1984/ 3200]\n",
      "loss: 0.170676  [ 2000/ 3200]\n",
      "loss: 0.269929  [ 2016/ 3200]\n",
      "loss: 0.223103  [ 2032/ 3200]\n",
      "loss: 0.326446  [ 2048/ 3200]\n",
      "loss: 0.228328  [ 2064/ 3200]\n",
      "loss: 0.196336  [ 2080/ 3200]\n",
      "loss: 0.202180  [ 2096/ 3200]\n",
      "loss: 0.191670  [ 2112/ 3200]\n",
      "loss: 0.410093  [ 2128/ 3200]\n",
      "loss: 0.446607  [ 2144/ 3200]\n",
      "loss: 0.171633  [ 2160/ 3200]\n",
      "loss: 0.143630  [ 2176/ 3200]\n",
      "loss: 0.254844  [ 2192/ 3200]\n",
      "loss: 0.137248  [ 2208/ 3200]\n",
      "loss: 0.189900  [ 2224/ 3200]\n",
      "loss: 0.414072  [ 2240/ 3200]\n",
      "loss: 0.453467  [ 2256/ 3200]\n",
      "loss: 0.218032  [ 2272/ 3200]\n",
      "loss: 0.105886  [ 2288/ 3200]\n",
      "loss: 0.592765  [ 2304/ 3200]\n",
      "loss: 0.257284  [ 2320/ 3200]\n",
      "loss: 0.261655  [ 2336/ 3200]\n",
      "loss: 0.064610  [ 2352/ 3200]\n",
      "loss: 0.270386  [ 2368/ 3200]\n",
      "loss: 0.208206  [ 2384/ 3200]\n",
      "loss: 0.335977  [ 2400/ 3200]\n",
      "loss: 0.271290  [ 2416/ 3200]\n",
      "loss: 0.267251  [ 2432/ 3200]\n",
      "loss: 0.561330  [ 2448/ 3200]\n",
      "loss: 0.068016  [ 2464/ 3200]\n",
      "loss: 0.135155  [ 2480/ 3200]\n",
      "loss: 0.338280  [ 2496/ 3200]\n",
      "loss: 0.237481  [ 2512/ 3200]\n",
      "loss: 0.127562  [ 2528/ 3200]\n",
      "loss: 0.279904  [ 2544/ 3200]\n",
      "loss: 0.148699  [ 2560/ 3200]\n",
      "loss: 0.223174  [ 2576/ 3200]\n",
      "loss: 0.186429  [ 2592/ 3200]\n",
      "loss: 0.308302  [ 2608/ 3200]\n",
      "loss: 0.125950  [ 2624/ 3200]\n",
      "loss: 0.108757  [ 2640/ 3200]\n",
      "loss: 0.362332  [ 2656/ 3200]\n",
      "loss: 0.129484  [ 2672/ 3200]\n",
      "loss: 0.228180  [ 2688/ 3200]\n",
      "loss: 0.555403  [ 2704/ 3200]\n",
      "loss: 0.244616  [ 2720/ 3200]\n",
      "loss: 0.051304  [ 2736/ 3200]\n",
      "loss: 0.335027  [ 2752/ 3200]\n",
      "loss: 0.043129  [ 2768/ 3200]\n",
      "loss: 0.160469  [ 2784/ 3200]\n",
      "loss: 0.257167  [ 2800/ 3200]\n",
      "loss: 0.380773  [ 2816/ 3200]\n",
      "loss: 0.235504  [ 2832/ 3200]\n",
      "loss: 0.452867  [ 2848/ 3200]\n",
      "loss: 0.319216  [ 2864/ 3200]\n",
      "loss: 0.255038  [ 2880/ 3200]\n",
      "loss: 0.164963  [ 2896/ 3200]\n",
      "loss: 0.087276  [ 2912/ 3200]\n",
      "loss: 0.426077  [ 2928/ 3200]\n",
      "loss: 0.342664  [ 2944/ 3200]\n",
      "loss: 0.421864  [ 2960/ 3200]\n",
      "loss: 0.259598  [ 2976/ 3200]\n",
      "loss: 0.281228  [ 2992/ 3200]\n",
      "loss: 0.416771  [ 3008/ 3200]\n",
      "loss: 0.277337  [ 3024/ 3200]\n",
      "loss: 0.277589  [ 3040/ 3200]\n",
      "loss: 0.201618  [ 3056/ 3200]\n",
      "loss: 0.205109  [ 3072/ 3200]\n",
      "loss: 0.288025  [ 3088/ 3200]\n",
      "loss: 0.323940  [ 3104/ 3200]\n",
      "loss: 0.359645  [ 3120/ 3200]\n",
      "loss: 0.307152  [ 3136/ 3200]\n",
      "loss: 0.112883  [ 3152/ 3200]\n",
      "loss: 0.246852  [ 3168/ 3200]\n",
      "loss: 0.227350  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038538\n",
      "f1 macro averaged score: 0.758042\n",
      "Accuracy               : 76.6%\n",
      "Confusion matrix       :\n",
      "tensor([[187,   3,   0,  10],\n",
      "        [ 24,  99,  22,  55],\n",
      "        [  1,  21, 162,  16],\n",
      "        [ 13,   8,  14, 165]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0807e-03.\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 0.236430  [    0/ 3200]\n",
      "loss: 0.219933  [   16/ 3200]\n",
      "loss: 0.086759  [   32/ 3200]\n",
      "loss: 0.260654  [   48/ 3200]\n",
      "loss: 0.295449  [   64/ 3200]\n",
      "loss: 0.198143  [   80/ 3200]\n",
      "loss: 0.275878  [   96/ 3200]\n",
      "loss: 0.224335  [  112/ 3200]\n",
      "loss: 0.460235  [  128/ 3200]\n",
      "loss: 0.175690  [  144/ 3200]\n",
      "loss: 0.244863  [  160/ 3200]\n",
      "loss: 0.349497  [  176/ 3200]\n",
      "loss: 0.175956  [  192/ 3200]\n",
      "loss: 0.201487  [  208/ 3200]\n",
      "loss: 0.133733  [  224/ 3200]\n",
      "loss: 0.224651  [  240/ 3200]\n",
      "loss: 0.409599  [  256/ 3200]\n",
      "loss: 0.064376  [  272/ 3200]\n",
      "loss: 0.083429  [  288/ 3200]\n",
      "loss: 0.158239  [  304/ 3200]\n",
      "loss: 0.361063  [  320/ 3200]\n",
      "loss: 0.391424  [  336/ 3200]\n",
      "loss: 0.194129  [  352/ 3200]\n",
      "loss: 0.272088  [  368/ 3200]\n",
      "loss: 0.307061  [  384/ 3200]\n",
      "loss: 0.448997  [  400/ 3200]\n",
      "loss: 0.165953  [  416/ 3200]\n",
      "loss: 0.393923  [  432/ 3200]\n",
      "loss: 0.174249  [  448/ 3200]\n",
      "loss: 0.141328  [  464/ 3200]\n",
      "loss: 0.115318  [  480/ 3200]\n",
      "loss: 0.290658  [  496/ 3200]\n",
      "loss: 0.264897  [  512/ 3200]\n",
      "loss: 0.317068  [  528/ 3200]\n",
      "loss: 0.246739  [  544/ 3200]\n",
      "loss: 0.271678  [  560/ 3200]\n",
      "loss: 0.171716  [  576/ 3200]\n",
      "loss: 0.252413  [  592/ 3200]\n",
      "loss: 0.283484  [  608/ 3200]\n",
      "loss: 0.312994  [  624/ 3200]\n",
      "loss: 0.413542  [  640/ 3200]\n",
      "loss: 0.253373  [  656/ 3200]\n",
      "loss: 0.198061  [  672/ 3200]\n",
      "loss: 0.446602  [  688/ 3200]\n",
      "loss: 0.335507  [  704/ 3200]\n",
      "loss: 0.321141  [  720/ 3200]\n",
      "loss: 0.374251  [  736/ 3200]\n",
      "loss: 0.123629  [  752/ 3200]\n",
      "loss: 0.340299  [  768/ 3200]\n",
      "loss: 0.223095  [  784/ 3200]\n",
      "loss: 0.109520  [  800/ 3200]\n",
      "loss: 0.148275  [  816/ 3200]\n",
      "loss: 0.308530  [  832/ 3200]\n",
      "loss: 0.204628  [  848/ 3200]\n",
      "loss: 0.052566  [  864/ 3200]\n",
      "loss: 0.335784  [  880/ 3200]\n",
      "loss: 0.146941  [  896/ 3200]\n",
      "loss: 0.106315  [  912/ 3200]\n",
      "loss: 0.388775  [  928/ 3200]\n",
      "loss: 0.312914  [  944/ 3200]\n",
      "loss: 0.374767  [  960/ 3200]\n",
      "loss: 0.189036  [  976/ 3200]\n",
      "loss: 0.234694  [  992/ 3200]\n",
      "loss: 0.184502  [ 1008/ 3200]\n",
      "loss: 0.380219  [ 1024/ 3200]\n",
      "loss: 0.200496  [ 1040/ 3200]\n",
      "loss: 0.224717  [ 1056/ 3200]\n",
      "loss: 0.383347  [ 1072/ 3200]\n",
      "loss: 0.393150  [ 1088/ 3200]\n",
      "loss: 0.394006  [ 1104/ 3200]\n",
      "loss: 0.255112  [ 1120/ 3200]\n",
      "loss: 0.184537  [ 1136/ 3200]\n",
      "loss: 0.356103  [ 1152/ 3200]\n",
      "loss: 0.292972  [ 1168/ 3200]\n",
      "loss: 0.307912  [ 1184/ 3200]\n",
      "loss: 0.388711  [ 1200/ 3200]\n",
      "loss: 0.290724  [ 1216/ 3200]\n",
      "loss: 0.213953  [ 1232/ 3200]\n",
      "loss: 0.210714  [ 1248/ 3200]\n",
      "loss: 0.126082  [ 1264/ 3200]\n",
      "loss: 0.340180  [ 1280/ 3200]\n",
      "loss: 0.193833  [ 1296/ 3200]\n",
      "loss: 0.504730  [ 1312/ 3200]\n",
      "loss: 0.226473  [ 1328/ 3200]\n",
      "loss: 0.059778  [ 1344/ 3200]\n",
      "loss: 0.095314  [ 1360/ 3200]\n",
      "loss: 0.256706  [ 1376/ 3200]\n",
      "loss: 0.183707  [ 1392/ 3200]\n",
      "loss: 0.408072  [ 1408/ 3200]\n",
      "loss: 0.266299  [ 1424/ 3200]\n",
      "loss: 0.233361  [ 1440/ 3200]\n",
      "loss: 0.045921  [ 1456/ 3200]\n",
      "loss: 0.091195  [ 1472/ 3200]\n",
      "loss: 0.308910  [ 1488/ 3200]\n",
      "loss: 0.319567  [ 1504/ 3200]\n",
      "loss: 0.207134  [ 1520/ 3200]\n",
      "loss: 0.128790  [ 1536/ 3200]\n",
      "loss: 0.264614  [ 1552/ 3200]\n",
      "loss: 0.648715  [ 1568/ 3200]\n",
      "loss: 0.207976  [ 1584/ 3200]\n",
      "loss: 0.123096  [ 1600/ 3200]\n",
      "loss: 0.337099  [ 1616/ 3200]\n",
      "loss: 0.110942  [ 1632/ 3200]\n",
      "loss: 0.039289  [ 1648/ 3200]\n",
      "loss: 0.130236  [ 1664/ 3200]\n",
      "loss: 0.281140  [ 1680/ 3200]\n",
      "loss: 0.268851  [ 1696/ 3200]\n",
      "loss: 0.051933  [ 1712/ 3200]\n",
      "loss: 0.184523  [ 1728/ 3200]\n",
      "loss: 0.339746  [ 1744/ 3200]\n",
      "loss: 0.169448  [ 1760/ 3200]\n",
      "loss: 0.311141  [ 1776/ 3200]\n",
      "loss: 0.183266  [ 1792/ 3200]\n",
      "loss: 0.146925  [ 1808/ 3200]\n",
      "loss: 0.361202  [ 1824/ 3200]\n",
      "loss: 0.337087  [ 1840/ 3200]\n",
      "loss: 0.256995  [ 1856/ 3200]\n",
      "loss: 0.093656  [ 1872/ 3200]\n",
      "loss: 0.104985  [ 1888/ 3200]\n",
      "loss: 0.117062  [ 1904/ 3200]\n",
      "loss: 0.207812  [ 1920/ 3200]\n",
      "loss: 0.158430  [ 1936/ 3200]\n",
      "loss: 0.175660  [ 1952/ 3200]\n",
      "loss: 0.171581  [ 1968/ 3200]\n",
      "loss: 0.448316  [ 1984/ 3200]\n",
      "loss: 0.282970  [ 2000/ 3200]\n",
      "loss: 0.123597  [ 2016/ 3200]\n",
      "loss: 0.073590  [ 2032/ 3200]\n",
      "loss: 0.090458  [ 2048/ 3200]\n",
      "loss: 0.255833  [ 2064/ 3200]\n",
      "loss: 0.407367  [ 2080/ 3200]\n",
      "loss: 0.788834  [ 2096/ 3200]\n",
      "loss: 0.171949  [ 2112/ 3200]\n",
      "loss: 0.162008  [ 2128/ 3200]\n",
      "loss: 0.070483  [ 2144/ 3200]\n",
      "loss: 0.437554  [ 2160/ 3200]\n",
      "loss: 0.295404  [ 2176/ 3200]\n",
      "loss: 0.175013  [ 2192/ 3200]\n",
      "loss: 0.147978  [ 2208/ 3200]\n",
      "loss: 0.265079  [ 2224/ 3200]\n",
      "loss: 0.423287  [ 2240/ 3200]\n",
      "loss: 0.212110  [ 2256/ 3200]\n",
      "loss: 0.121387  [ 2272/ 3200]\n",
      "loss: 0.102590  [ 2288/ 3200]\n",
      "loss: 0.122789  [ 2304/ 3200]\n",
      "loss: 0.476015  [ 2320/ 3200]\n",
      "loss: 0.159133  [ 2336/ 3200]\n",
      "loss: 0.212505  [ 2352/ 3200]\n",
      "loss: 0.199000  [ 2368/ 3200]\n",
      "loss: 0.423481  [ 2384/ 3200]\n",
      "loss: 0.431068  [ 2400/ 3200]\n",
      "loss: 0.193995  [ 2416/ 3200]\n",
      "loss: 0.214953  [ 2432/ 3200]\n",
      "loss: 0.288943  [ 2448/ 3200]\n",
      "loss: 0.131702  [ 2464/ 3200]\n",
      "loss: 0.393456  [ 2480/ 3200]\n",
      "loss: 0.135360  [ 2496/ 3200]\n",
      "loss: 0.115056  [ 2512/ 3200]\n",
      "loss: 0.072694  [ 2528/ 3200]\n",
      "loss: 0.172041  [ 2544/ 3200]\n",
      "loss: 0.173221  [ 2560/ 3200]\n",
      "loss: 0.139396  [ 2576/ 3200]\n",
      "loss: 0.160079  [ 2592/ 3200]\n",
      "loss: 0.217868  [ 2608/ 3200]\n",
      "loss: 0.406341  [ 2624/ 3200]\n",
      "loss: 0.645603  [ 2640/ 3200]\n",
      "loss: 0.240112  [ 2656/ 3200]\n",
      "loss: 0.220806  [ 2672/ 3200]\n",
      "loss: 0.574745  [ 2688/ 3200]\n",
      "loss: 0.346218  [ 2704/ 3200]\n",
      "loss: 0.215739  [ 2720/ 3200]\n",
      "loss: 0.453239  [ 2736/ 3200]\n",
      "loss: 0.434761  [ 2752/ 3200]\n",
      "loss: 0.335273  [ 2768/ 3200]\n",
      "loss: 0.103935  [ 2784/ 3200]\n",
      "loss: 0.308006  [ 2800/ 3200]\n",
      "loss: 0.173929  [ 2816/ 3200]\n",
      "loss: 0.242506  [ 2832/ 3200]\n",
      "loss: 0.120585  [ 2848/ 3200]\n",
      "loss: 0.248109  [ 2864/ 3200]\n",
      "loss: 0.218154  [ 2880/ 3200]\n",
      "loss: 0.246960  [ 2896/ 3200]\n",
      "loss: 0.089793  [ 2912/ 3200]\n",
      "loss: 0.111111  [ 2928/ 3200]\n",
      "loss: 0.485725  [ 2944/ 3200]\n",
      "loss: 0.426231  [ 2960/ 3200]\n",
      "loss: 0.129193  [ 2976/ 3200]\n",
      "loss: 0.409357  [ 2992/ 3200]\n",
      "loss: 0.316758  [ 3008/ 3200]\n",
      "loss: 0.267439  [ 3024/ 3200]\n",
      "loss: 0.216841  [ 3040/ 3200]\n",
      "loss: 0.137839  [ 3056/ 3200]\n",
      "loss: 0.212993  [ 3072/ 3200]\n",
      "loss: 0.584428  [ 3088/ 3200]\n",
      "loss: 0.195544  [ 3104/ 3200]\n",
      "loss: 0.197244  [ 3120/ 3200]\n",
      "loss: 0.155091  [ 3136/ 3200]\n",
      "loss: 0.162848  [ 3152/ 3200]\n",
      "loss: 0.466290  [ 3168/ 3200]\n",
      "loss: 0.653463  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038081\n",
      "f1 macro averaged score: 0.769108\n",
      "Accuracy               : 77.5%\n",
      "Confusion matrix       :\n",
      "tensor([[188,   5,   0,   7],\n",
      "        [ 19, 109,  26,  46],\n",
      "        [  0,  24, 159,  17],\n",
      "        [ 13,  10,  13, 164]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0267e-03.\n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------\n",
      "loss: 0.206989  [    0/ 3200]\n",
      "loss: 0.288471  [   16/ 3200]\n",
      "loss: 0.108480  [   32/ 3200]\n",
      "loss: 0.302989  [   48/ 3200]\n",
      "loss: 0.184482  [   64/ 3200]\n",
      "loss: 0.123780  [   80/ 3200]\n",
      "loss: 0.127959  [   96/ 3200]\n",
      "loss: 0.140127  [  112/ 3200]\n",
      "loss: 0.043562  [  128/ 3200]\n",
      "loss: 0.216736  [  144/ 3200]\n",
      "loss: 0.113862  [  160/ 3200]\n",
      "loss: 0.091060  [  176/ 3200]\n",
      "loss: 0.074980  [  192/ 3200]\n",
      "loss: 0.144296  [  208/ 3200]\n",
      "loss: 0.315018  [  224/ 3200]\n",
      "loss: 0.172500  [  240/ 3200]\n",
      "loss: 0.179929  [  256/ 3200]\n",
      "loss: 0.232273  [  272/ 3200]\n",
      "loss: 0.262038  [  288/ 3200]\n",
      "loss: 0.251420  [  304/ 3200]\n",
      "loss: 0.348201  [  320/ 3200]\n",
      "loss: 0.165241  [  336/ 3200]\n",
      "loss: 0.252375  [  352/ 3200]\n",
      "loss: 0.137132  [  368/ 3200]\n",
      "loss: 0.211059  [  384/ 3200]\n",
      "loss: 0.401705  [  400/ 3200]\n",
      "loss: 0.280445  [  416/ 3200]\n",
      "loss: 0.203447  [  432/ 3200]\n",
      "loss: 0.054491  [  448/ 3200]\n",
      "loss: 0.288422  [  464/ 3200]\n",
      "loss: 0.336422  [  480/ 3200]\n",
      "loss: 0.043490  [  496/ 3200]\n",
      "loss: 0.331113  [  512/ 3200]\n",
      "loss: 0.039603  [  528/ 3200]\n",
      "loss: 0.048861  [  544/ 3200]\n",
      "loss: 0.539565  [  560/ 3200]\n",
      "loss: 0.130582  [  576/ 3200]\n",
      "loss: 0.301419  [  592/ 3200]\n",
      "loss: 0.509396  [  608/ 3200]\n",
      "loss: 0.168768  [  624/ 3200]\n",
      "loss: 0.107894  [  640/ 3200]\n",
      "loss: 0.192648  [  656/ 3200]\n",
      "loss: 0.321852  [  672/ 3200]\n",
      "loss: 0.228542  [  688/ 3200]\n",
      "loss: 0.277959  [  704/ 3200]\n",
      "loss: 0.242339  [  720/ 3200]\n",
      "loss: 0.275058  [  736/ 3200]\n",
      "loss: 0.367470  [  752/ 3200]\n",
      "loss: 0.137141  [  768/ 3200]\n",
      "loss: 0.065639  [  784/ 3200]\n",
      "loss: 0.103964  [  800/ 3200]\n",
      "loss: 0.376575  [  816/ 3200]\n",
      "loss: 0.286549  [  832/ 3200]\n",
      "loss: 0.188154  [  848/ 3200]\n",
      "loss: 0.186735  [  864/ 3200]\n",
      "loss: 0.404242  [  880/ 3200]\n",
      "loss: 0.149362  [  896/ 3200]\n",
      "loss: 0.453618  [  912/ 3200]\n",
      "loss: 0.158515  [  928/ 3200]\n",
      "loss: 0.423800  [  944/ 3200]\n",
      "loss: 0.144573  [  960/ 3200]\n",
      "loss: 0.057394  [  976/ 3200]\n",
      "loss: 0.359916  [  992/ 3200]\n",
      "loss: 0.176058  [ 1008/ 3200]\n",
      "loss: 0.287626  [ 1024/ 3200]\n",
      "loss: 0.222539  [ 1040/ 3200]\n",
      "loss: 0.363340  [ 1056/ 3200]\n",
      "loss: 0.121987  [ 1072/ 3200]\n",
      "loss: 0.309984  [ 1088/ 3200]\n",
      "loss: 0.107479  [ 1104/ 3200]\n",
      "loss: 0.170216  [ 1120/ 3200]\n",
      "loss: 0.054573  [ 1136/ 3200]\n",
      "loss: 0.210381  [ 1152/ 3200]\n",
      "loss: 0.483441  [ 1168/ 3200]\n",
      "loss: 0.387137  [ 1184/ 3200]\n",
      "loss: 0.264095  [ 1200/ 3200]\n",
      "loss: 0.088635  [ 1216/ 3200]\n",
      "loss: 0.287510  [ 1232/ 3200]\n",
      "loss: 0.573331  [ 1248/ 3200]\n",
      "loss: 0.129230  [ 1264/ 3200]\n",
      "loss: 0.132184  [ 1280/ 3200]\n",
      "loss: 0.275024  [ 1296/ 3200]\n",
      "loss: 0.121295  [ 1312/ 3200]\n",
      "loss: 0.380878  [ 1328/ 3200]\n",
      "loss: 0.217803  [ 1344/ 3200]\n",
      "loss: 0.125142  [ 1360/ 3200]\n",
      "loss: 0.132041  [ 1376/ 3200]\n",
      "loss: 0.034086  [ 1392/ 3200]\n",
      "loss: 0.161614  [ 1408/ 3200]\n",
      "loss: 0.113943  [ 1424/ 3200]\n",
      "loss: 0.213398  [ 1440/ 3200]\n",
      "loss: 0.098529  [ 1456/ 3200]\n",
      "loss: 0.400069  [ 1472/ 3200]\n",
      "loss: 0.341454  [ 1488/ 3200]\n",
      "loss: 0.098109  [ 1504/ 3200]\n",
      "loss: 0.254831  [ 1520/ 3200]\n",
      "loss: 0.202726  [ 1536/ 3200]\n",
      "loss: 0.348174  [ 1552/ 3200]\n",
      "loss: 0.230499  [ 1568/ 3200]\n",
      "loss: 0.287649  [ 1584/ 3200]\n",
      "loss: 0.270887  [ 1600/ 3200]\n",
      "loss: 0.174630  [ 1616/ 3200]\n",
      "loss: 0.497288  [ 1632/ 3200]\n",
      "loss: 0.121644  [ 1648/ 3200]\n",
      "loss: 0.220861  [ 1664/ 3200]\n",
      "loss: 0.302247  [ 1680/ 3200]\n",
      "loss: 0.234754  [ 1696/ 3200]\n",
      "loss: 0.220855  [ 1712/ 3200]\n",
      "loss: 0.286782  [ 1728/ 3200]\n",
      "loss: 0.108138  [ 1744/ 3200]\n",
      "loss: 0.250389  [ 1760/ 3200]\n",
      "loss: 0.392425  [ 1776/ 3200]\n",
      "loss: 0.139013  [ 1792/ 3200]\n",
      "loss: 0.481724  [ 1808/ 3200]\n",
      "loss: 0.113325  [ 1824/ 3200]\n",
      "loss: 0.185262  [ 1840/ 3200]\n",
      "loss: 0.167939  [ 1856/ 3200]\n",
      "loss: 0.127425  [ 1872/ 3200]\n",
      "loss: 0.078449  [ 1888/ 3200]\n",
      "loss: 0.219228  [ 1904/ 3200]\n",
      "loss: 0.121831  [ 1920/ 3200]\n",
      "loss: 0.135491  [ 1936/ 3200]\n",
      "loss: 0.302343  [ 1952/ 3200]\n",
      "loss: 0.132689  [ 1968/ 3200]\n",
      "loss: 0.026460  [ 1984/ 3200]\n",
      "loss: 0.234988  [ 2000/ 3200]\n",
      "loss: 0.088073  [ 2016/ 3200]\n",
      "loss: 0.545932  [ 2032/ 3200]\n",
      "loss: 0.341579  [ 2048/ 3200]\n",
      "loss: 0.245015  [ 2064/ 3200]\n",
      "loss: 0.352359  [ 2080/ 3200]\n",
      "loss: 0.427600  [ 2096/ 3200]\n",
      "loss: 0.275589  [ 2112/ 3200]\n",
      "loss: 0.239441  [ 2128/ 3200]\n",
      "loss: 0.313910  [ 2144/ 3200]\n",
      "loss: 0.113481  [ 2160/ 3200]\n",
      "loss: 0.108651  [ 2176/ 3200]\n",
      "loss: 0.194609  [ 2192/ 3200]\n",
      "loss: 0.162397  [ 2208/ 3200]\n",
      "loss: 0.332398  [ 2224/ 3200]\n",
      "loss: 0.159684  [ 2240/ 3200]\n",
      "loss: 0.204372  [ 2256/ 3200]\n",
      "loss: 0.451707  [ 2272/ 3200]\n",
      "loss: 0.135048  [ 2288/ 3200]\n",
      "loss: 0.107805  [ 2304/ 3200]\n",
      "loss: 0.098748  [ 2320/ 3200]\n",
      "loss: 0.268635  [ 2336/ 3200]\n",
      "loss: 0.144551  [ 2352/ 3200]\n",
      "loss: 0.179910  [ 2368/ 3200]\n",
      "loss: 0.306986  [ 2384/ 3200]\n",
      "loss: 0.277958  [ 2400/ 3200]\n",
      "loss: 0.302507  [ 2416/ 3200]\n",
      "loss: 0.175750  [ 2432/ 3200]\n",
      "loss: 0.451578  [ 2448/ 3200]\n",
      "loss: 0.346067  [ 2464/ 3200]\n",
      "loss: 0.107815  [ 2480/ 3200]\n",
      "loss: 0.108835  [ 2496/ 3200]\n",
      "loss: 0.367481  [ 2512/ 3200]\n",
      "loss: 0.102349  [ 2528/ 3200]\n",
      "loss: 0.196103  [ 2544/ 3200]\n",
      "loss: 0.043321  [ 2560/ 3200]\n",
      "loss: 0.315201  [ 2576/ 3200]\n",
      "loss: 0.184924  [ 2592/ 3200]\n",
      "loss: 0.357544  [ 2608/ 3200]\n",
      "loss: 0.178353  [ 2624/ 3200]\n",
      "loss: 0.079584  [ 2640/ 3200]\n",
      "loss: 0.239330  [ 2656/ 3200]\n",
      "loss: 0.124143  [ 2672/ 3200]\n",
      "loss: 0.192350  [ 2688/ 3200]\n",
      "loss: 0.212658  [ 2704/ 3200]\n",
      "loss: 0.176414  [ 2720/ 3200]\n",
      "loss: 0.282273  [ 2736/ 3200]\n",
      "loss: 0.432618  [ 2752/ 3200]\n",
      "loss: 0.346502  [ 2768/ 3200]\n",
      "loss: 0.259897  [ 2784/ 3200]\n",
      "loss: 0.243547  [ 2800/ 3200]\n",
      "loss: 0.165630  [ 2816/ 3200]\n",
      "loss: 0.317248  [ 2832/ 3200]\n",
      "loss: 0.225710  [ 2848/ 3200]\n",
      "loss: 0.209560  [ 2864/ 3200]\n",
      "loss: 0.153886  [ 2880/ 3200]\n",
      "loss: 0.208176  [ 2896/ 3200]\n",
      "loss: 0.162348  [ 2912/ 3200]\n",
      "loss: 0.132847  [ 2928/ 3200]\n",
      "loss: 0.049607  [ 2944/ 3200]\n",
      "loss: 0.235663  [ 2960/ 3200]\n",
      "loss: 0.143471  [ 2976/ 3200]\n",
      "loss: 0.052515  [ 2992/ 3200]\n",
      "loss: 0.069444  [ 3008/ 3200]\n",
      "loss: 0.097445  [ 3024/ 3200]\n",
      "loss: 0.724452  [ 3040/ 3200]\n",
      "loss: 0.323753  [ 3056/ 3200]\n",
      "loss: 0.240925  [ 3072/ 3200]\n",
      "loss: 0.061535  [ 3088/ 3200]\n",
      "loss: 0.329982  [ 3104/ 3200]\n",
      "loss: 0.193379  [ 3120/ 3200]\n",
      "loss: 0.052783  [ 3136/ 3200]\n",
      "loss: 0.379005  [ 3152/ 3200]\n",
      "loss: 0.133639  [ 3168/ 3200]\n",
      "loss: 0.161833  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038372\n",
      "f1 macro averaged score: 0.759881\n",
      "Accuracy               : 76.6%\n",
      "Confusion matrix       :\n",
      "tensor([[185,   6,   0,   9],\n",
      "        [ 18, 104,  30,  48],\n",
      "        [  0,  21, 164,  15],\n",
      "        [ 10,  14,  16, 160]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.7535e-04.\n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------\n",
      "loss: 0.191459  [    0/ 3200]\n",
      "loss: 0.403886  [   16/ 3200]\n",
      "loss: 0.186185  [   32/ 3200]\n",
      "loss: 0.179523  [   48/ 3200]\n",
      "loss: 0.164208  [   64/ 3200]\n",
      "loss: 0.365714  [   80/ 3200]\n",
      "loss: 0.109968  [   96/ 3200]\n",
      "loss: 0.376517  [  112/ 3200]\n",
      "loss: 0.139246  [  128/ 3200]\n",
      "loss: 0.063413  [  144/ 3200]\n",
      "loss: 0.258719  [  160/ 3200]\n",
      "loss: 0.158208  [  176/ 3200]\n",
      "loss: 0.228817  [  192/ 3200]\n",
      "loss: 0.284726  [  208/ 3200]\n",
      "loss: 0.204224  [  224/ 3200]\n",
      "loss: 0.349816  [  240/ 3200]\n",
      "loss: 0.157348  [  256/ 3200]\n",
      "loss: 0.131447  [  272/ 3200]\n",
      "loss: 0.130810  [  288/ 3200]\n",
      "loss: 0.079279  [  304/ 3200]\n",
      "loss: 0.150065  [  320/ 3200]\n",
      "loss: 0.267370  [  336/ 3200]\n",
      "loss: 0.073490  [  352/ 3200]\n",
      "loss: 0.173667  [  368/ 3200]\n",
      "loss: 0.204451  [  384/ 3200]\n",
      "loss: 0.143913  [  400/ 3200]\n",
      "loss: 0.107466  [  416/ 3200]\n",
      "loss: 0.218685  [  432/ 3200]\n",
      "loss: 0.342176  [  448/ 3200]\n",
      "loss: 0.118293  [  464/ 3200]\n",
      "loss: 0.310380  [  480/ 3200]\n",
      "loss: 0.313367  [  496/ 3200]\n",
      "loss: 0.314836  [  512/ 3200]\n",
      "loss: 0.137443  [  528/ 3200]\n",
      "loss: 0.318148  [  544/ 3200]\n",
      "loss: 0.321539  [  560/ 3200]\n",
      "loss: 0.243482  [  576/ 3200]\n",
      "loss: 0.491955  [  592/ 3200]\n",
      "loss: 0.494870  [  608/ 3200]\n",
      "loss: 0.180312  [  624/ 3200]\n",
      "loss: 0.094241  [  640/ 3200]\n",
      "loss: 0.072292  [  656/ 3200]\n",
      "loss: 0.189179  [  672/ 3200]\n",
      "loss: 0.124592  [  688/ 3200]\n",
      "loss: 0.334306  [  704/ 3200]\n",
      "loss: 0.193286  [  720/ 3200]\n",
      "loss: 0.050547  [  736/ 3200]\n",
      "loss: 0.121732  [  752/ 3200]\n",
      "loss: 0.124838  [  768/ 3200]\n",
      "loss: 0.191982  [  784/ 3200]\n",
      "loss: 0.387052  [  800/ 3200]\n",
      "loss: 0.192821  [  816/ 3200]\n",
      "loss: 0.122365  [  832/ 3200]\n",
      "loss: 0.159766  [  848/ 3200]\n",
      "loss: 0.061431  [  864/ 3200]\n",
      "loss: 0.208515  [  880/ 3200]\n",
      "loss: 0.189185  [  896/ 3200]\n",
      "loss: 0.116955  [  912/ 3200]\n",
      "loss: 0.213031  [  928/ 3200]\n",
      "loss: 0.070794  [  944/ 3200]\n",
      "loss: 0.206267  [  960/ 3200]\n",
      "loss: 0.141905  [  976/ 3200]\n",
      "loss: 0.066725  [  992/ 3200]\n",
      "loss: 0.210557  [ 1008/ 3200]\n",
      "loss: 0.314271  [ 1024/ 3200]\n",
      "loss: 0.122359  [ 1040/ 3200]\n",
      "loss: 0.143129  [ 1056/ 3200]\n",
      "loss: 0.177282  [ 1072/ 3200]\n",
      "loss: 0.143866  [ 1088/ 3200]\n",
      "loss: 0.140139  [ 1104/ 3200]\n",
      "loss: 0.164512  [ 1120/ 3200]\n",
      "loss: 0.070089  [ 1136/ 3200]\n",
      "loss: 0.168379  [ 1152/ 3200]\n",
      "loss: 0.149842  [ 1168/ 3200]\n",
      "loss: 0.102619  [ 1184/ 3200]\n",
      "loss: 0.083906  [ 1200/ 3200]\n",
      "loss: 0.149795  [ 1216/ 3200]\n",
      "loss: 0.140725  [ 1232/ 3200]\n",
      "loss: 0.115881  [ 1248/ 3200]\n",
      "loss: 0.185384  [ 1264/ 3200]\n",
      "loss: 0.057366  [ 1280/ 3200]\n",
      "loss: 0.064801  [ 1296/ 3200]\n",
      "loss: 0.120495  [ 1312/ 3200]\n",
      "loss: 0.173698  [ 1328/ 3200]\n",
      "loss: 0.353632  [ 1344/ 3200]\n",
      "loss: 0.361337  [ 1360/ 3200]\n",
      "loss: 0.083814  [ 1376/ 3200]\n",
      "loss: 0.081467  [ 1392/ 3200]\n",
      "loss: 0.295025  [ 1408/ 3200]\n",
      "loss: 0.098903  [ 1424/ 3200]\n",
      "loss: 0.398176  [ 1440/ 3200]\n",
      "loss: 0.084634  [ 1456/ 3200]\n",
      "loss: 0.128194  [ 1472/ 3200]\n",
      "loss: 0.402574  [ 1488/ 3200]\n",
      "loss: 0.437623  [ 1504/ 3200]\n",
      "loss: 0.052230  [ 1520/ 3200]\n",
      "loss: 0.204977  [ 1536/ 3200]\n",
      "loss: 0.115263  [ 1552/ 3200]\n",
      "loss: 0.198991  [ 1568/ 3200]\n",
      "loss: 0.145646  [ 1584/ 3200]\n",
      "loss: 0.198542  [ 1600/ 3200]\n",
      "loss: 0.097765  [ 1616/ 3200]\n",
      "loss: 0.105691  [ 1632/ 3200]\n",
      "loss: 0.155167  [ 1648/ 3200]\n",
      "loss: 0.383843  [ 1664/ 3200]\n",
      "loss: 0.393130  [ 1680/ 3200]\n",
      "loss: 0.171995  [ 1696/ 3200]\n",
      "loss: 0.223887  [ 1712/ 3200]\n",
      "loss: 0.095093  [ 1728/ 3200]\n",
      "loss: 0.568499  [ 1744/ 3200]\n",
      "loss: 0.177232  [ 1760/ 3200]\n",
      "loss: 0.196052  [ 1776/ 3200]\n",
      "loss: 0.331733  [ 1792/ 3200]\n",
      "loss: 0.229267  [ 1808/ 3200]\n",
      "loss: 0.289092  [ 1824/ 3200]\n",
      "loss: 0.119275  [ 1840/ 3200]\n",
      "loss: 0.347744  [ 1856/ 3200]\n",
      "loss: 0.119776  [ 1872/ 3200]\n",
      "loss: 0.189288  [ 1888/ 3200]\n",
      "loss: 0.732835  [ 1904/ 3200]\n",
      "loss: 0.221285  [ 1920/ 3200]\n",
      "loss: 0.318423  [ 1936/ 3200]\n",
      "loss: 0.174602  [ 1952/ 3200]\n",
      "loss: 0.112076  [ 1968/ 3200]\n",
      "loss: 0.134279  [ 1984/ 3200]\n",
      "loss: 0.092869  [ 2000/ 3200]\n",
      "loss: 0.153109  [ 2016/ 3200]\n",
      "loss: 0.097837  [ 2032/ 3200]\n",
      "loss: 0.192215  [ 2048/ 3200]\n",
      "loss: 0.134357  [ 2064/ 3200]\n",
      "loss: 0.263231  [ 2080/ 3200]\n",
      "loss: 0.233209  [ 2096/ 3200]\n",
      "loss: 0.214314  [ 2112/ 3200]\n",
      "loss: 0.090779  [ 2128/ 3200]\n",
      "loss: 0.232463  [ 2144/ 3200]\n",
      "loss: 0.265135  [ 2160/ 3200]\n",
      "loss: 0.201731  [ 2176/ 3200]\n",
      "loss: 0.278053  [ 2192/ 3200]\n",
      "loss: 0.223157  [ 2208/ 3200]\n",
      "loss: 0.223277  [ 2224/ 3200]\n",
      "loss: 0.311723  [ 2240/ 3200]\n",
      "loss: 0.179603  [ 2256/ 3200]\n",
      "loss: 0.287781  [ 2272/ 3200]\n",
      "loss: 0.207990  [ 2288/ 3200]\n",
      "loss: 0.244074  [ 2304/ 3200]\n",
      "loss: 0.120169  [ 2320/ 3200]\n",
      "loss: 0.287325  [ 2336/ 3200]\n",
      "loss: 0.204217  [ 2352/ 3200]\n",
      "loss: 0.073551  [ 2368/ 3200]\n",
      "loss: 0.147606  [ 2384/ 3200]\n",
      "loss: 0.056362  [ 2400/ 3200]\n",
      "loss: 0.203266  [ 2416/ 3200]\n",
      "loss: 0.349617  [ 2432/ 3200]\n",
      "loss: 0.120428  [ 2448/ 3200]\n",
      "loss: 0.313262  [ 2464/ 3200]\n",
      "loss: 0.752820  [ 2480/ 3200]\n",
      "loss: 0.055364  [ 2496/ 3200]\n",
      "loss: 0.084009  [ 2512/ 3200]\n",
      "loss: 0.123981  [ 2528/ 3200]\n",
      "loss: 0.199214  [ 2544/ 3200]\n",
      "loss: 0.109097  [ 2560/ 3200]\n",
      "loss: 0.186592  [ 2576/ 3200]\n",
      "loss: 0.162383  [ 2592/ 3200]\n",
      "loss: 0.134439  [ 2608/ 3200]\n",
      "loss: 0.058849  [ 2624/ 3200]\n",
      "loss: 0.136089  [ 2640/ 3200]\n",
      "loss: 0.285063  [ 2656/ 3200]\n",
      "loss: 0.309639  [ 2672/ 3200]\n",
      "loss: 0.147526  [ 2688/ 3200]\n",
      "loss: 0.105017  [ 2704/ 3200]\n",
      "loss: 0.143823  [ 2720/ 3200]\n",
      "loss: 0.191222  [ 2736/ 3200]\n",
      "loss: 0.139729  [ 2752/ 3200]\n",
      "loss: 0.258669  [ 2768/ 3200]\n",
      "loss: 0.171062  [ 2784/ 3200]\n",
      "loss: 0.205981  [ 2800/ 3200]\n",
      "loss: 0.210330  [ 2816/ 3200]\n",
      "loss: 0.098800  [ 2832/ 3200]\n",
      "loss: 0.347137  [ 2848/ 3200]\n",
      "loss: 0.182836  [ 2864/ 3200]\n",
      "loss: 0.101169  [ 2880/ 3200]\n",
      "loss: 0.124574  [ 2896/ 3200]\n",
      "loss: 0.066097  [ 2912/ 3200]\n",
      "loss: 0.099641  [ 2928/ 3200]\n",
      "loss: 0.119688  [ 2944/ 3200]\n",
      "loss: 0.060616  [ 2960/ 3200]\n",
      "loss: 0.327785  [ 2976/ 3200]\n",
      "loss: 0.204020  [ 2992/ 3200]\n",
      "loss: 0.436429  [ 3008/ 3200]\n",
      "loss: 0.323006  [ 3024/ 3200]\n",
      "loss: 0.117387  [ 3040/ 3200]\n",
      "loss: 0.281558  [ 3056/ 3200]\n",
      "loss: 0.281528  [ 3072/ 3200]\n",
      "loss: 0.294306  [ 3088/ 3200]\n",
      "loss: 0.600222  [ 3104/ 3200]\n",
      "loss: 0.063090  [ 3120/ 3200]\n",
      "loss: 0.193357  [ 3136/ 3200]\n",
      "loss: 0.181395  [ 3152/ 3200]\n",
      "loss: 0.124854  [ 3168/ 3200]\n",
      "loss: 0.138204  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.042361\n",
      "f1 macro averaged score: 0.759973\n",
      "Accuracy               : 75.9%\n",
      "Confusion matrix       :\n",
      "tensor([[172,  12,   0,  16],\n",
      "        [ 13, 124,  11,  52],\n",
      "        [  0,  35, 140,  25],\n",
      "        [  5,  16,   8, 171]], device='cuda:0')\n",
      "Test Error:\n",
      "Avg loss               : 0.040541\n",
      "f1 macro averaged score: 0.787609\n",
      "Accuracy               : 78.2%\n",
      "Confusion matrix       :\n",
      "tensor([[273,  16,   1,   7],\n",
      "        [ 10, 220,  15,  79],\n",
      "        [  3,  47, 267,  39],\n",
      "        [  9,  53,  21, 316]], device='cuda:0')\n",
      "CPU times: user 1min 59s, sys: 2.56 s, total: 2min 2s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "patience_list = range(1, 11)\n",
    "\n",
    "epochs = 30\n",
    "learning_rate_0 = 0.002\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(list(zip(melgram_x_train, melgram_y_train)), batch_size=16, shuffle=True, generator=torch.Generator(device='cpu'))\n",
    "train_dataloader.set_epoch = set_epoch\n",
    "\n",
    "f1_accuracy = []\n",
    "for patience in patience_list:\n",
    "  torch_seed(0)\n",
    "  cnn_model = Net(ELU, 0.1).to(device)\n",
    "  optimizer = Adagrad(params=cnn_model.parameters(), lr=learning_rate_0, weight_decay=0.0001)\n",
    "  scheduler = MultiplicativeLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95, verbose=True)\n",
    "  print(\"  Patience:\", patience)\n",
    "  t_0 = timer()\n",
    "  best_model, f1_per_epoch, epoch = validate_convolutional_neural_network(\n",
    "      epochs, optimizer, train_dataloader, val_dataloader, loss_function, cnn_model, True, scheduler, patience\n",
    "      )\n",
    "  t_1 = timer()\n",
    "  results = test_convolutional_neural_network(test_dataloader, loss_function, best_model)\n",
    "  validation_time = t_1 - t_0\n",
    "  f1_accuracy.append((patience, results[1], results[2], validation_time, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CKGZLAM4i29",
    "outputId": "9d9a0477-9837-4fbb-be6b-c68aed88887b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 epochs\n",
      "Patience\tf1_macro_avg\tAccuracy\tVal time\tEpochs\n",
      "7\t\t0.788396\t78.779070\t14.822313\t13\n",
      "10\t\t0.787609\t78.197674\t17.459485\t15\n",
      "5\t\t0.784092\t77.543605\t11.460107\t10\n",
      "9\t\t0.781964\t77.616279\t16.766858\t15\n",
      "8\t\t0.779790\t77.325581\t17.085610\t15\n",
      "4\t\t0.779634\t77.325581\t10.597540\t9\n",
      "6\t\t0.767612\t77.034884\t13.469538\t12\n",
      "3\t\t0.759468\t76.526163\t9.399402\t8\n",
      "2\t\t0.755669\t74.781977\t8.871604\t6\n",
      "1\t\t0.683036\t67.877907\t5.052789\t4\n"
     ]
    }
   ],
   "source": [
    "f1_accuracy = sorted(f1_accuracy, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"30 epochs\")\n",
    "print(\"Patience\\tf1_macro_avg\\tAccuracy\\tVal time\\tEpochs\")\n",
    "for (patience, f1_macro_avg, accuracy, validation_time, epoch) in f1_accuracy:\n",
    "  print(f\"{patience}\\t\\t{f1_macro_avg:>5f}\\t{accuracy:>5f}\\t{validation_time:>2f}\\t{epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljpCk9nGmW2Z"
   },
   "source": [
    "From the table shown above, what we can observe that a model with higher patience needs more epochs for training.\n",
    "\n",
    "The highest accuracy and f1 score is achieved for $patience = 7$.\n",
    "\n",
    "This choice leads to a small drop in accuracy from $79\\%$ to $78.77\\%$.\n",
    "\n",
    "This small compromise shouldn't be an issue, because the accuracy achieved is really close to our goal and the training-validation has become much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv9Znio_uFKV"
   },
   "source": [
    "# Question 4 - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POKOdzh0qLT6"
   },
   "source": [
    "### Step 1 - Inference\n",
    "\n",
    "This function returns the predictions of the given model for the given dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "GGDpsOR7uKud"
   },
   "outputs": [],
   "source": [
    "def inference(dataloader, model):\n",
    "  predictions = []\n",
    "  with torch.no_grad():\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "      # move data to GPU if GPU is in use\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # predict\n",
    "      prediction = model(torch.unsqueeze(x, 1)).argmax(1).item()\n",
    "      predictions.append(labels_int_to_str[prediction]) # invert label to use in plot\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEe8SJSnu-xJ"
   },
   "source": [
    "### Step 2 - Download music file\n",
    "\n",
    "<code>yt-dl</code> package lead to errors that couldn't be solved, so I decided to use the <code>yt-dlp</code> package instead.\n",
    "\n",
    "Install <code>yt-dlp</code> package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64ntOiQ0vCIP",
    "outputId": "4df51f44-1bce-4940-f2d6-8636a97f7e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "0% [Working]\r\n",
      "            \r\n",
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "\r\n",
      "0% [Connecting to archive.ubuntu.com (185.125.190.36)] [1 InRelease 14.2 kB/114\r\n",
      "                                                                               \r\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
      "\r\n",
      "0% [Connecting to archive.ubuntu.com (185.125.190.36)] [1 InRelease 43.1 kB/114\r\n",
      "0% [Connecting to archive.ubuntu.com (185.125.190.36)] [1 InRelease 60.5 kB/114\r\n",
      "                                                                               \r\n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n",
      "\r\n",
      "0% [Waiting for headers] [1 InRelease 69.2 kB/114 kB 61%] [Connecting to ppa.la\r\n",
      "0% [Waiting for headers] [1 InRelease 69.2 kB/114 kB 61%] [Connecting to ppa.la\r\n",
      "                                                                               \r\n",
      "0% [Waiting for headers] [Connecting to ppa.launchpad.net (185.125.190.52)]\r\n",
      "                                                                           \r\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:7 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Packages [81.0 kB]\n",
      "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1,079 kB]\n",
      "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Get:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease [24.3 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,636 kB]\n",
      "Hit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
      "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,604 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [39.5 kB]\n",
      "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,229 kB]\n",
      "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal/main amd64 Packages [44.1 kB]\n",
      "Fetched 8,096 kB in 3s (2,623 kB/s)\n",
      "Reading package lists... Done\n",
      "Collecting yt-dlp\n",
      "  Downloading yt_dlp-2023.6.22-py2.py3-none-any.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mutagen (from yt-dlp)\n",
      "  Downloading mutagen-1.46.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
      "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets (from yt-dlp)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.5.7)\n",
      "Collecting brotli (from yt-dlp)\n",
      "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
      "Successfully installed brotli-1.0.9 mutagen-1.46.0 pycryptodomex-3.18.0 websockets-11.0.3 yt-dlp-2023.6.22\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get update\n",
    "!python3 -m pip install -U yt-dlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1O92vcY0qT4i"
   },
   "source": [
    "Import <code>youtube</code> module that was provided in the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "DBMNmbKgvZn1"
   },
   "outputs": [],
   "source": [
    "sys.path.append('/content/gdrive/MyDrive/')\n",
    "import youtube\n",
    "sys.path.append('/content/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuS8qtOsqZaC"
   },
   "source": [
    "Redefine <code>download_youtube()</code>, so that it uses the <code>yt-dlp</code> package instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ADTxsM3pVsuw"
   },
   "outputs": [],
   "source": [
    "def download_youtube(url):\n",
    "  command = f'yt-dlp -x --audio-format wav --audio-quality 0 --output temp.wav --postprocessor-args \"-ar 8000 -ac 1\" ' + url + \" --quiet\"\n",
    "  os.system(command)\n",
    "\n",
    "def youtube_to_melgram(url):\n",
    "    download_youtube(url)\n",
    "    melgrams = youtube.get_melgrams(\"/content/temp.wav\")\n",
    "    np.save(\"youtube_melgrams.npy\", melgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FovaCGMUqnfn"
   },
   "source": [
    "This function returns a dataloader in which the samples are taken from the melgram of each second of the song with the given url and the labels are the given labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "NOtJXaPCvawv"
   },
   "outputs": [],
   "source": [
    "def get_youtube_dataloader(youtube_url, label):\n",
    "  youtube_to_melgram(youtube_url)\n",
    "  melgram_x = np.load('/content/youtube_melgrams.npy')\n",
    "  melgram_y = [label for melgram_sample in melgram_x]\n",
    "  melgram_y = np.array(melgram_y)\n",
    "  youtube_dataloader = DataLoader(list(zip(melgram_x, melgram_y)), batch_size=1, shuffle=True)\n",
    "  os.remove('youtube_melgrams.npy')\n",
    "  os.remove('temp.wav')\n",
    "  return youtube_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxVVr-HvMoEj"
   },
   "source": [
    "### Step 3 - Predictions\n",
    "\n",
    "We validate our Convolutional Neural Network for $30$ epochs and test it.\n",
    "\n",
    "Our model uses:\n",
    "+ the Adagrad optimizer\n",
    "+ the ELU activation function\n",
    "+ the MultiplicativeLR scheduler\n",
    "+ batch normalization\n",
    "+ weight decay $0.0001$\n",
    "+ dropout $0.1$\n",
    "+ batch size $16$\n",
    "+ patience $7$\n",
    "\n",
    "as stated in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QiT5INrEMubw",
    "outputId": "4b231ca2-0318-44c9-b2e7-1b125ae1cbbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-03.\n",
      "Epoch: 1\n",
      "-----------------------------\n",
      "loss: 1.410910  [    0/ 3200]\n",
      "loss: 3.237996  [   16/ 3200]\n",
      "loss: 1.761683  [   32/ 3200]\n",
      "loss: 2.279021  [   48/ 3200]\n",
      "loss: 1.921307  [   64/ 3200]\n",
      "loss: 1.200242  [   80/ 3200]\n",
      "loss: 1.876231  [   96/ 3200]\n",
      "loss: 2.124714  [  112/ 3200]\n",
      "loss: 1.392274  [  128/ 3200]\n",
      "loss: 1.407979  [  144/ 3200]\n",
      "loss: 1.399582  [  160/ 3200]\n",
      "loss: 1.316472  [  176/ 3200]\n",
      "loss: 1.561095  [  192/ 3200]\n",
      "loss: 1.394648  [  208/ 3200]\n",
      "loss: 1.735171  [  224/ 3200]\n",
      "loss: 1.403881  [  240/ 3200]\n",
      "loss: 1.417257  [  256/ 3200]\n",
      "loss: 1.428851  [  272/ 3200]\n",
      "loss: 1.478123  [  288/ 3200]\n",
      "loss: 1.375917  [  304/ 3200]\n",
      "loss: 1.415263  [  320/ 3200]\n",
      "loss: 1.387382  [  336/ 3200]\n",
      "loss: 1.356661  [  352/ 3200]\n",
      "loss: 1.346226  [  368/ 3200]\n",
      "loss: 1.377258  [  384/ 3200]\n",
      "loss: 1.358207  [  400/ 3200]\n",
      "loss: 1.485980  [  416/ 3200]\n",
      "loss: 1.352431  [  432/ 3200]\n",
      "loss: 1.390954  [  448/ 3200]\n",
      "loss: 1.354195  [  464/ 3200]\n",
      "loss: 1.364135  [  480/ 3200]\n",
      "loss: 1.372646  [  496/ 3200]\n",
      "loss: 1.437072  [  512/ 3200]\n",
      "loss: 1.378598  [  528/ 3200]\n",
      "loss: 1.402828  [  544/ 3200]\n",
      "loss: 1.382319  [  560/ 3200]\n",
      "loss: 1.372618  [  576/ 3200]\n",
      "loss: 1.367445  [  592/ 3200]\n",
      "loss: 1.277654  [  608/ 3200]\n",
      "loss: 1.557926  [  624/ 3200]\n",
      "loss: 1.383562  [  640/ 3200]\n",
      "loss: 1.433604  [  656/ 3200]\n",
      "loss: 1.351838  [  672/ 3200]\n",
      "loss: 1.324393  [  688/ 3200]\n",
      "loss: 1.407170  [  704/ 3200]\n",
      "loss: 1.362709  [  720/ 3200]\n",
      "loss: 1.408955  [  736/ 3200]\n",
      "loss: 1.328783  [  752/ 3200]\n",
      "loss: 1.442364  [  768/ 3200]\n",
      "loss: 1.318637  [  784/ 3200]\n",
      "loss: 1.373539  [  800/ 3200]\n",
      "loss: 1.347563  [  816/ 3200]\n",
      "loss: 1.326365  [  832/ 3200]\n",
      "loss: 1.350469  [  848/ 3200]\n",
      "loss: 1.375745  [  864/ 3200]\n",
      "loss: 1.471138  [  880/ 3200]\n",
      "loss: 1.425859  [  896/ 3200]\n",
      "loss: 1.341562  [  912/ 3200]\n",
      "loss: 1.393549  [  928/ 3200]\n",
      "loss: 1.377002  [  944/ 3200]\n",
      "loss: 1.344931  [  960/ 3200]\n",
      "loss: 1.358740  [  976/ 3200]\n",
      "loss: 1.274884  [  992/ 3200]\n",
      "loss: 1.501865  [ 1008/ 3200]\n",
      "loss: 1.356203  [ 1024/ 3200]\n",
      "loss: 1.346581  [ 1040/ 3200]\n",
      "loss: 1.375161  [ 1056/ 3200]\n",
      "loss: 1.247763  [ 1072/ 3200]\n",
      "loss: 1.460935  [ 1088/ 3200]\n",
      "loss: 1.424220  [ 1104/ 3200]\n",
      "loss: 1.280300  [ 1120/ 3200]\n",
      "loss: 1.497151  [ 1136/ 3200]\n",
      "loss: 1.372274  [ 1152/ 3200]\n",
      "loss: 1.356639  [ 1168/ 3200]\n",
      "loss: 1.335284  [ 1184/ 3200]\n",
      "loss: 1.362513  [ 1200/ 3200]\n",
      "loss: 1.310205  [ 1216/ 3200]\n",
      "loss: 1.412251  [ 1232/ 3200]\n",
      "loss: 1.344212  [ 1248/ 3200]\n",
      "loss: 1.357317  [ 1264/ 3200]\n",
      "loss: 1.324237  [ 1280/ 3200]\n",
      "loss: 1.305694  [ 1296/ 3200]\n",
      "loss: 1.263274  [ 1312/ 3200]\n",
      "loss: 1.327924  [ 1328/ 3200]\n",
      "loss: 1.350901  [ 1344/ 3200]\n",
      "loss: 1.201255  [ 1360/ 3200]\n",
      "loss: 1.242456  [ 1376/ 3200]\n",
      "loss: 1.243505  [ 1392/ 3200]\n",
      "loss: 1.292077  [ 1408/ 3200]\n",
      "loss: 1.187207  [ 1424/ 3200]\n",
      "loss: 1.393437  [ 1440/ 3200]\n",
      "loss: 1.233559  [ 1456/ 3200]\n",
      "loss: 1.354319  [ 1472/ 3200]\n",
      "loss: 1.410908  [ 1488/ 3200]\n",
      "loss: 1.371239  [ 1504/ 3200]\n",
      "loss: 1.279593  [ 1520/ 3200]\n",
      "loss: 1.482910  [ 1536/ 3200]\n",
      "loss: 1.454768  [ 1552/ 3200]\n",
      "loss: 1.297748  [ 1568/ 3200]\n",
      "loss: 1.380850  [ 1584/ 3200]\n",
      "loss: 1.351535  [ 1600/ 3200]\n",
      "loss: 1.274776  [ 1616/ 3200]\n",
      "loss: 1.261349  [ 1632/ 3200]\n",
      "loss: 1.243427  [ 1648/ 3200]\n",
      "loss: 1.205095  [ 1664/ 3200]\n",
      "loss: 1.176320  [ 1680/ 3200]\n",
      "loss: 1.242011  [ 1696/ 3200]\n",
      "loss: 1.302608  [ 1712/ 3200]\n",
      "loss: 1.168667  [ 1728/ 3200]\n",
      "loss: 1.144248  [ 1744/ 3200]\n",
      "loss: 1.192140  [ 1760/ 3200]\n",
      "loss: 1.282210  [ 1776/ 3200]\n",
      "loss: 1.170475  [ 1792/ 3200]\n",
      "loss: 1.315410  [ 1808/ 3200]\n",
      "loss: 1.393888  [ 1824/ 3200]\n",
      "loss: 1.225441  [ 1840/ 3200]\n",
      "loss: 1.149162  [ 1856/ 3200]\n",
      "loss: 1.162421  [ 1872/ 3200]\n",
      "loss: 1.095147  [ 1888/ 3200]\n",
      "loss: 0.984955  [ 1904/ 3200]\n",
      "loss: 1.008549  [ 1920/ 3200]\n",
      "loss: 1.074374  [ 1936/ 3200]\n",
      "loss: 1.035900  [ 1952/ 3200]\n",
      "loss: 1.220665  [ 1968/ 3200]\n",
      "loss: 0.892805  [ 1984/ 3200]\n",
      "loss: 1.249532  [ 2000/ 3200]\n",
      "loss: 1.223053  [ 2016/ 3200]\n",
      "loss: 0.950492  [ 2032/ 3200]\n",
      "loss: 0.977578  [ 2048/ 3200]\n",
      "loss: 1.023320  [ 2064/ 3200]\n",
      "loss: 1.215460  [ 2080/ 3200]\n",
      "loss: 0.765752  [ 2096/ 3200]\n",
      "loss: 1.302442  [ 2112/ 3200]\n",
      "loss: 0.931897  [ 2128/ 3200]\n",
      "loss: 0.968982  [ 2144/ 3200]\n",
      "loss: 0.895442  [ 2160/ 3200]\n",
      "loss: 0.898550  [ 2176/ 3200]\n",
      "loss: 0.765467  [ 2192/ 3200]\n",
      "loss: 1.231336  [ 2208/ 3200]\n",
      "loss: 1.081660  [ 2224/ 3200]\n",
      "loss: 0.869434  [ 2240/ 3200]\n",
      "loss: 1.016471  [ 2256/ 3200]\n",
      "loss: 0.893583  [ 2272/ 3200]\n",
      "loss: 0.670889  [ 2288/ 3200]\n",
      "loss: 1.040921  [ 2304/ 3200]\n",
      "loss: 1.011965  [ 2320/ 3200]\n",
      "loss: 1.173289  [ 2336/ 3200]\n",
      "loss: 1.150086  [ 2352/ 3200]\n",
      "loss: 0.919981  [ 2368/ 3200]\n",
      "loss: 1.080639  [ 2384/ 3200]\n",
      "loss: 1.053823  [ 2400/ 3200]\n",
      "loss: 1.091752  [ 2416/ 3200]\n",
      "loss: 1.049327  [ 2432/ 3200]\n",
      "loss: 1.103229  [ 2448/ 3200]\n",
      "loss: 1.015974  [ 2464/ 3200]\n",
      "loss: 0.951440  [ 2480/ 3200]\n",
      "loss: 1.155181  [ 2496/ 3200]\n",
      "loss: 1.206409  [ 2512/ 3200]\n",
      "loss: 0.975272  [ 2528/ 3200]\n",
      "loss: 1.213280  [ 2544/ 3200]\n",
      "loss: 0.957542  [ 2560/ 3200]\n",
      "loss: 1.186295  [ 2576/ 3200]\n",
      "loss: 0.792137  [ 2592/ 3200]\n",
      "loss: 0.935471  [ 2608/ 3200]\n",
      "loss: 0.811332  [ 2624/ 3200]\n",
      "loss: 0.878924  [ 2640/ 3200]\n",
      "loss: 1.051904  [ 2656/ 3200]\n",
      "loss: 0.845907  [ 2672/ 3200]\n",
      "loss: 0.748981  [ 2688/ 3200]\n",
      "loss: 1.180606  [ 2704/ 3200]\n",
      "loss: 1.634045  [ 2720/ 3200]\n",
      "loss: 1.106671  [ 2736/ 3200]\n",
      "loss: 0.918415  [ 2752/ 3200]\n",
      "loss: 0.479737  [ 2768/ 3200]\n",
      "loss: 0.718988  [ 2784/ 3200]\n",
      "loss: 0.919570  [ 2800/ 3200]\n",
      "loss: 1.106435  [ 2816/ 3200]\n",
      "loss: 1.179065  [ 2832/ 3200]\n",
      "loss: 0.878817  [ 2848/ 3200]\n",
      "loss: 0.911060  [ 2864/ 3200]\n",
      "loss: 0.871488  [ 2880/ 3200]\n",
      "loss: 0.626611  [ 2896/ 3200]\n",
      "loss: 0.774416  [ 2912/ 3200]\n",
      "loss: 0.760370  [ 2928/ 3200]\n",
      "loss: 1.043720  [ 2944/ 3200]\n",
      "loss: 1.133876  [ 2960/ 3200]\n",
      "loss: 0.901248  [ 2976/ 3200]\n",
      "loss: 0.802699  [ 2992/ 3200]\n",
      "loss: 1.188794  [ 3008/ 3200]\n",
      "loss: 1.037094  [ 3024/ 3200]\n",
      "loss: 0.895362  [ 3040/ 3200]\n",
      "loss: 0.912736  [ 3056/ 3200]\n",
      "loss: 1.044601  [ 3072/ 3200]\n",
      "loss: 1.375351  [ 3088/ 3200]\n",
      "loss: 1.187680  [ 3104/ 3200]\n",
      "loss: 0.797051  [ 3120/ 3200]\n",
      "loss: 0.757145  [ 3136/ 3200]\n",
      "loss: 0.770593  [ 3152/ 3200]\n",
      "loss: 1.047720  [ 3168/ 3200]\n",
      "loss: 0.947308  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.051372\n",
      "f1 macro averaged score: 0.664901\n",
      "Accuracy               : 67.9%\n",
      "Confusion matrix       :\n",
      "tensor([[152,  35,  10,   3],\n",
      "        [ 38,  66,  57,  39],\n",
      "        [  2,  13, 176,   9],\n",
      "        [  1,  24,  26, 149]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.9000e-03.\n",
      "\n",
      "Epoch: 2\n",
      "-----------------------------\n",
      "loss: 0.745421  [    0/ 3200]\n",
      "loss: 0.592592  [   16/ 3200]\n",
      "loss: 0.770925  [   32/ 3200]\n",
      "loss: 1.088932  [   48/ 3200]\n",
      "loss: 0.677652  [   64/ 3200]\n",
      "loss: 0.399759  [   80/ 3200]\n",
      "loss: 1.131780  [   96/ 3200]\n",
      "loss: 0.856927  [  112/ 3200]\n",
      "loss: 0.621050  [  128/ 3200]\n",
      "loss: 0.748158  [  144/ 3200]\n",
      "loss: 0.831988  [  160/ 3200]\n",
      "loss: 0.882077  [  176/ 3200]\n",
      "loss: 0.994400  [  192/ 3200]\n",
      "loss: 1.159344  [  208/ 3200]\n",
      "loss: 0.909528  [  224/ 3200]\n",
      "loss: 0.881077  [  240/ 3200]\n",
      "loss: 0.482194  [  256/ 3200]\n",
      "loss: 0.728558  [  272/ 3200]\n",
      "loss: 0.835431  [  288/ 3200]\n",
      "loss: 0.659835  [  304/ 3200]\n",
      "loss: 0.894425  [  320/ 3200]\n",
      "loss: 0.875363  [  336/ 3200]\n",
      "loss: 0.882921  [  352/ 3200]\n",
      "loss: 0.810471  [  368/ 3200]\n",
      "loss: 0.952751  [  384/ 3200]\n",
      "loss: 0.831148  [  400/ 3200]\n",
      "loss: 0.452168  [  416/ 3200]\n",
      "loss: 1.025100  [  432/ 3200]\n",
      "loss: 1.086994  [  448/ 3200]\n",
      "loss: 0.642055  [  464/ 3200]\n",
      "loss: 0.911369  [  480/ 3200]\n",
      "loss: 0.857555  [  496/ 3200]\n",
      "loss: 0.578914  [  512/ 3200]\n",
      "loss: 0.802145  [  528/ 3200]\n",
      "loss: 0.867986  [  544/ 3200]\n",
      "loss: 1.507051  [  560/ 3200]\n",
      "loss: 1.008260  [  576/ 3200]\n",
      "loss: 0.556560  [  592/ 3200]\n",
      "loss: 0.769125  [  608/ 3200]\n",
      "loss: 0.722398  [  624/ 3200]\n",
      "loss: 0.871671  [  640/ 3200]\n",
      "loss: 0.597137  [  656/ 3200]\n",
      "loss: 0.841244  [  672/ 3200]\n",
      "loss: 0.781293  [  688/ 3200]\n",
      "loss: 0.552208  [  704/ 3200]\n",
      "loss: 0.845912  [  720/ 3200]\n",
      "loss: 1.152134  [  736/ 3200]\n",
      "loss: 0.679598  [  752/ 3200]\n",
      "loss: 1.429614  [  768/ 3200]\n",
      "loss: 0.904102  [  784/ 3200]\n",
      "loss: 0.851108  [  800/ 3200]\n",
      "loss: 1.100116  [  816/ 3200]\n",
      "loss: 0.658370  [  832/ 3200]\n",
      "loss: 0.695180  [  848/ 3200]\n",
      "loss: 0.540143  [  864/ 3200]\n",
      "loss: 0.531651  [  880/ 3200]\n",
      "loss: 0.570274  [  896/ 3200]\n",
      "loss: 0.716865  [  912/ 3200]\n",
      "loss: 0.880766  [  928/ 3200]\n",
      "loss: 0.930890  [  944/ 3200]\n",
      "loss: 0.707669  [  960/ 3200]\n",
      "loss: 0.594930  [  976/ 3200]\n",
      "loss: 0.901291  [  992/ 3200]\n",
      "loss: 1.150514  [ 1008/ 3200]\n",
      "loss: 0.774270  [ 1024/ 3200]\n",
      "loss: 0.759510  [ 1040/ 3200]\n",
      "loss: 0.907056  [ 1056/ 3200]\n",
      "loss: 0.741947  [ 1072/ 3200]\n",
      "loss: 0.596179  [ 1088/ 3200]\n",
      "loss: 0.609271  [ 1104/ 3200]\n",
      "loss: 1.103022  [ 1120/ 3200]\n",
      "loss: 0.680507  [ 1136/ 3200]\n",
      "loss: 0.960114  [ 1152/ 3200]\n",
      "loss: 0.560965  [ 1168/ 3200]\n",
      "loss: 0.847374  [ 1184/ 3200]\n",
      "loss: 0.772863  [ 1200/ 3200]\n",
      "loss: 0.594821  [ 1216/ 3200]\n",
      "loss: 0.668416  [ 1232/ 3200]\n",
      "loss: 0.714307  [ 1248/ 3200]\n",
      "loss: 1.077868  [ 1264/ 3200]\n",
      "loss: 0.840320  [ 1280/ 3200]\n",
      "loss: 0.710452  [ 1296/ 3200]\n",
      "loss: 0.501608  [ 1312/ 3200]\n",
      "loss: 0.828446  [ 1328/ 3200]\n",
      "loss: 0.574517  [ 1344/ 3200]\n",
      "loss: 0.659265  [ 1360/ 3200]\n",
      "loss: 0.603128  [ 1376/ 3200]\n",
      "loss: 0.740473  [ 1392/ 3200]\n",
      "loss: 0.663691  [ 1408/ 3200]\n",
      "loss: 0.880327  [ 1424/ 3200]\n",
      "loss: 0.606430  [ 1440/ 3200]\n",
      "loss: 1.189873  [ 1456/ 3200]\n",
      "loss: 0.314012  [ 1472/ 3200]\n",
      "loss: 0.673008  [ 1488/ 3200]\n",
      "loss: 0.518656  [ 1504/ 3200]\n",
      "loss: 0.703478  [ 1520/ 3200]\n",
      "loss: 0.537053  [ 1536/ 3200]\n",
      "loss: 0.873124  [ 1552/ 3200]\n",
      "loss: 1.181213  [ 1568/ 3200]\n",
      "loss: 0.656950  [ 1584/ 3200]\n",
      "loss: 0.729770  [ 1600/ 3200]\n",
      "loss: 0.802364  [ 1616/ 3200]\n",
      "loss: 0.832582  [ 1632/ 3200]\n",
      "loss: 0.703019  [ 1648/ 3200]\n",
      "loss: 0.902849  [ 1664/ 3200]\n",
      "loss: 0.675493  [ 1680/ 3200]\n",
      "loss: 0.670816  [ 1696/ 3200]\n",
      "loss: 0.676706  [ 1712/ 3200]\n",
      "loss: 0.523268  [ 1728/ 3200]\n",
      "loss: 0.636139  [ 1744/ 3200]\n",
      "loss: 0.550394  [ 1760/ 3200]\n",
      "loss: 0.656663  [ 1776/ 3200]\n",
      "loss: 0.654541  [ 1792/ 3200]\n",
      "loss: 1.423261  [ 1808/ 3200]\n",
      "loss: 0.854100  [ 1824/ 3200]\n",
      "loss: 0.565638  [ 1840/ 3200]\n",
      "loss: 0.830118  [ 1856/ 3200]\n",
      "loss: 0.623809  [ 1872/ 3200]\n",
      "loss: 1.092868  [ 1888/ 3200]\n",
      "loss: 0.401273  [ 1904/ 3200]\n",
      "loss: 0.645965  [ 1920/ 3200]\n",
      "loss: 0.604483  [ 1936/ 3200]\n",
      "loss: 0.571875  [ 1952/ 3200]\n",
      "loss: 0.635325  [ 1968/ 3200]\n",
      "loss: 0.811785  [ 1984/ 3200]\n",
      "loss: 0.785581  [ 2000/ 3200]\n",
      "loss: 0.877149  [ 2016/ 3200]\n",
      "loss: 0.450625  [ 2032/ 3200]\n",
      "loss: 0.606677  [ 2048/ 3200]\n",
      "loss: 0.604897  [ 2064/ 3200]\n",
      "loss: 0.840436  [ 2080/ 3200]\n",
      "loss: 0.478850  [ 2096/ 3200]\n",
      "loss: 0.769158  [ 2112/ 3200]\n",
      "loss: 0.574581  [ 2128/ 3200]\n",
      "loss: 0.716668  [ 2144/ 3200]\n",
      "loss: 0.354661  [ 2160/ 3200]\n",
      "loss: 0.857160  [ 2176/ 3200]\n",
      "loss: 0.406726  [ 2192/ 3200]\n",
      "loss: 1.043795  [ 2208/ 3200]\n",
      "loss: 0.572679  [ 2224/ 3200]\n",
      "loss: 0.569612  [ 2240/ 3200]\n",
      "loss: 0.429262  [ 2256/ 3200]\n",
      "loss: 0.637392  [ 2272/ 3200]\n",
      "loss: 0.462862  [ 2288/ 3200]\n",
      "loss: 0.626482  [ 2304/ 3200]\n",
      "loss: 0.641126  [ 2320/ 3200]\n",
      "loss: 1.315785  [ 2336/ 3200]\n",
      "loss: 0.734954  [ 2352/ 3200]\n",
      "loss: 1.130962  [ 2368/ 3200]\n",
      "loss: 0.848886  [ 2384/ 3200]\n",
      "loss: 1.035524  [ 2400/ 3200]\n",
      "loss: 0.669124  [ 2416/ 3200]\n",
      "loss: 0.697568  [ 2432/ 3200]\n",
      "loss: 0.783070  [ 2448/ 3200]\n",
      "loss: 0.802575  [ 2464/ 3200]\n",
      "loss: 0.589580  [ 2480/ 3200]\n",
      "loss: 0.731498  [ 2496/ 3200]\n",
      "loss: 0.851669  [ 2512/ 3200]\n",
      "loss: 0.475675  [ 2528/ 3200]\n",
      "loss: 1.293366  [ 2544/ 3200]\n",
      "loss: 0.818662  [ 2560/ 3200]\n",
      "loss: 0.651988  [ 2576/ 3200]\n",
      "loss: 0.511378  [ 2592/ 3200]\n",
      "loss: 0.797845  [ 2608/ 3200]\n",
      "loss: 1.401595  [ 2624/ 3200]\n",
      "loss: 0.644689  [ 2640/ 3200]\n",
      "loss: 0.498786  [ 2656/ 3200]\n",
      "loss: 1.047861  [ 2672/ 3200]\n",
      "loss: 0.846738  [ 2688/ 3200]\n",
      "loss: 0.728300  [ 2704/ 3200]\n",
      "loss: 0.769139  [ 2720/ 3200]\n",
      "loss: 0.527656  [ 2736/ 3200]\n",
      "loss: 0.702308  [ 2752/ 3200]\n",
      "loss: 0.652245  [ 2768/ 3200]\n",
      "loss: 0.999207  [ 2784/ 3200]\n",
      "loss: 0.481004  [ 2800/ 3200]\n",
      "loss: 0.895350  [ 2816/ 3200]\n",
      "loss: 0.669662  [ 2832/ 3200]\n",
      "loss: 0.874461  [ 2848/ 3200]\n",
      "loss: 0.544366  [ 2864/ 3200]\n",
      "loss: 0.627028  [ 2880/ 3200]\n",
      "loss: 0.657244  [ 2896/ 3200]\n",
      "loss: 0.529167  [ 2912/ 3200]\n",
      "loss: 0.976379  [ 2928/ 3200]\n",
      "loss: 0.676638  [ 2944/ 3200]\n",
      "loss: 1.029680  [ 2960/ 3200]\n",
      "loss: 0.740984  [ 2976/ 3200]\n",
      "loss: 0.667743  [ 2992/ 3200]\n",
      "loss: 0.602100  [ 3008/ 3200]\n",
      "loss: 0.839451  [ 3024/ 3200]\n",
      "loss: 0.879067  [ 3040/ 3200]\n",
      "loss: 0.948473  [ 3056/ 3200]\n",
      "loss: 0.460511  [ 3072/ 3200]\n",
      "loss: 0.455229  [ 3088/ 3200]\n",
      "loss: 0.653022  [ 3104/ 3200]\n",
      "loss: 1.028723  [ 3120/ 3200]\n",
      "loss: 0.817626  [ 3136/ 3200]\n",
      "loss: 0.955092  [ 3152/ 3200]\n",
      "loss: 0.607266  [ 3168/ 3200]\n",
      "loss: 0.233367  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.042757\n",
      "f1 macro averaged score: 0.704015\n",
      "Accuracy               : 71.8%\n",
      "Confusion matrix       :\n",
      "tensor([[164,  27,   4,   5],\n",
      "        [ 29,  75,  62,  34],\n",
      "        [  0,  10, 183,   7],\n",
      "        [  3,  20,  25, 152]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.8050e-03.\n",
      "\n",
      "Epoch: 3\n",
      "-----------------------------\n",
      "loss: 0.937350  [    0/ 3200]\n",
      "loss: 0.712012  [   16/ 3200]\n",
      "loss: 0.535174  [   32/ 3200]\n",
      "loss: 0.606345  [   48/ 3200]\n",
      "loss: 0.501856  [   64/ 3200]\n",
      "loss: 0.569772  [   80/ 3200]\n",
      "loss: 0.728129  [   96/ 3200]\n",
      "loss: 0.400126  [  112/ 3200]\n",
      "loss: 0.865349  [  128/ 3200]\n",
      "loss: 0.495852  [  144/ 3200]\n",
      "loss: 0.473384  [  160/ 3200]\n",
      "loss: 0.417406  [  176/ 3200]\n",
      "loss: 0.621214  [  192/ 3200]\n",
      "loss: 0.575680  [  208/ 3200]\n",
      "loss: 0.848200  [  224/ 3200]\n",
      "loss: 1.214820  [  240/ 3200]\n",
      "loss: 0.686542  [  256/ 3200]\n",
      "loss: 0.467967  [  272/ 3200]\n",
      "loss: 0.996739  [  288/ 3200]\n",
      "loss: 0.296521  [  304/ 3200]\n",
      "loss: 0.517706  [  320/ 3200]\n",
      "loss: 0.668425  [  336/ 3200]\n",
      "loss: 0.589937  [  352/ 3200]\n",
      "loss: 0.489416  [  368/ 3200]\n",
      "loss: 0.518228  [  384/ 3200]\n",
      "loss: 0.503500  [  400/ 3200]\n",
      "loss: 0.511066  [  416/ 3200]\n",
      "loss: 0.607121  [  432/ 3200]\n",
      "loss: 0.646236  [  448/ 3200]\n",
      "loss: 0.765106  [  464/ 3200]\n",
      "loss: 0.995600  [  480/ 3200]\n",
      "loss: 0.901221  [  496/ 3200]\n",
      "loss: 0.695032  [  512/ 3200]\n",
      "loss: 0.392974  [  528/ 3200]\n",
      "loss: 0.673422  [  544/ 3200]\n",
      "loss: 0.869387  [  560/ 3200]\n",
      "loss: 0.688507  [  576/ 3200]\n",
      "loss: 0.609186  [  592/ 3200]\n",
      "loss: 1.089401  [  608/ 3200]\n",
      "loss: 0.859290  [  624/ 3200]\n",
      "loss: 0.596950  [  640/ 3200]\n",
      "loss: 0.548519  [  656/ 3200]\n",
      "loss: 0.932729  [  672/ 3200]\n",
      "loss: 0.286025  [  688/ 3200]\n",
      "loss: 0.637848  [  704/ 3200]\n",
      "loss: 0.508278  [  720/ 3200]\n",
      "loss: 0.433157  [  736/ 3200]\n",
      "loss: 0.552146  [  752/ 3200]\n",
      "loss: 0.665940  [  768/ 3200]\n",
      "loss: 0.567607  [  784/ 3200]\n",
      "loss: 1.022174  [  800/ 3200]\n",
      "loss: 0.761345  [  816/ 3200]\n",
      "loss: 0.426122  [  832/ 3200]\n",
      "loss: 0.485390  [  848/ 3200]\n",
      "loss: 0.639030  [  864/ 3200]\n",
      "loss: 0.422105  [  880/ 3200]\n",
      "loss: 0.639015  [  896/ 3200]\n",
      "loss: 0.525104  [  912/ 3200]\n",
      "loss: 0.760283  [  928/ 3200]\n",
      "loss: 0.849222  [  944/ 3200]\n",
      "loss: 0.554902  [  960/ 3200]\n",
      "loss: 1.355582  [  976/ 3200]\n",
      "loss: 0.928803  [  992/ 3200]\n",
      "loss: 1.049025  [ 1008/ 3200]\n",
      "loss: 0.559835  [ 1024/ 3200]\n",
      "loss: 1.275927  [ 1040/ 3200]\n",
      "loss: 0.854420  [ 1056/ 3200]\n",
      "loss: 0.887832  [ 1072/ 3200]\n",
      "loss: 0.611438  [ 1088/ 3200]\n",
      "loss: 0.693360  [ 1104/ 3200]\n",
      "loss: 0.524611  [ 1120/ 3200]\n",
      "loss: 0.403737  [ 1136/ 3200]\n",
      "loss: 0.516072  [ 1152/ 3200]\n",
      "loss: 0.551644  [ 1168/ 3200]\n",
      "loss: 1.005678  [ 1184/ 3200]\n",
      "loss: 0.857839  [ 1200/ 3200]\n",
      "loss: 0.736660  [ 1216/ 3200]\n",
      "loss: 0.546488  [ 1232/ 3200]\n",
      "loss: 0.621752  [ 1248/ 3200]\n",
      "loss: 0.544391  [ 1264/ 3200]\n",
      "loss: 0.510229  [ 1280/ 3200]\n",
      "loss: 0.441633  [ 1296/ 3200]\n",
      "loss: 0.701955  [ 1312/ 3200]\n",
      "loss: 0.691683  [ 1328/ 3200]\n",
      "loss: 0.504308  [ 1344/ 3200]\n",
      "loss: 0.508029  [ 1360/ 3200]\n",
      "loss: 0.383366  [ 1376/ 3200]\n",
      "loss: 0.456922  [ 1392/ 3200]\n",
      "loss: 0.293425  [ 1408/ 3200]\n",
      "loss: 0.891600  [ 1424/ 3200]\n",
      "loss: 0.779202  [ 1440/ 3200]\n",
      "loss: 0.521196  [ 1456/ 3200]\n",
      "loss: 0.628138  [ 1472/ 3200]\n",
      "loss: 0.848676  [ 1488/ 3200]\n",
      "loss: 0.530706  [ 1504/ 3200]\n",
      "loss: 1.045203  [ 1520/ 3200]\n",
      "loss: 0.692508  [ 1536/ 3200]\n",
      "loss: 0.540780  [ 1552/ 3200]\n",
      "loss: 1.054243  [ 1568/ 3200]\n",
      "loss: 0.834506  [ 1584/ 3200]\n",
      "loss: 0.678613  [ 1600/ 3200]\n",
      "loss: 0.252795  [ 1616/ 3200]\n",
      "loss: 0.643803  [ 1632/ 3200]\n",
      "loss: 0.360650  [ 1648/ 3200]\n",
      "loss: 0.414178  [ 1664/ 3200]\n",
      "loss: 0.364658  [ 1680/ 3200]\n",
      "loss: 0.425988  [ 1696/ 3200]\n",
      "loss: 0.519477  [ 1712/ 3200]\n",
      "loss: 0.469571  [ 1728/ 3200]\n",
      "loss: 0.321446  [ 1744/ 3200]\n",
      "loss: 0.446891  [ 1760/ 3200]\n",
      "loss: 0.751293  [ 1776/ 3200]\n",
      "loss: 0.689848  [ 1792/ 3200]\n",
      "loss: 0.652930  [ 1808/ 3200]\n",
      "loss: 0.513901  [ 1824/ 3200]\n",
      "loss: 0.619448  [ 1840/ 3200]\n",
      "loss: 1.193837  [ 1856/ 3200]\n",
      "loss: 0.658516  [ 1872/ 3200]\n",
      "loss: 0.695445  [ 1888/ 3200]\n",
      "loss: 0.892478  [ 1904/ 3200]\n",
      "loss: 0.486431  [ 1920/ 3200]\n",
      "loss: 0.977717  [ 1936/ 3200]\n",
      "loss: 1.119469  [ 1952/ 3200]\n",
      "loss: 0.488350  [ 1968/ 3200]\n",
      "loss: 0.746698  [ 1984/ 3200]\n",
      "loss: 0.690460  [ 2000/ 3200]\n",
      "loss: 0.515404  [ 2016/ 3200]\n",
      "loss: 0.522067  [ 2032/ 3200]\n",
      "loss: 0.556506  [ 2048/ 3200]\n",
      "loss: 0.584810  [ 2064/ 3200]\n",
      "loss: 0.765717  [ 2080/ 3200]\n",
      "loss: 0.656147  [ 2096/ 3200]\n",
      "loss: 0.833844  [ 2112/ 3200]\n",
      "loss: 0.436149  [ 2128/ 3200]\n",
      "loss: 0.786612  [ 2144/ 3200]\n",
      "loss: 0.716407  [ 2160/ 3200]\n",
      "loss: 0.717509  [ 2176/ 3200]\n",
      "loss: 0.907276  [ 2192/ 3200]\n",
      "loss: 0.526083  [ 2208/ 3200]\n",
      "loss: 0.595335  [ 2224/ 3200]\n",
      "loss: 0.831064  [ 2240/ 3200]\n",
      "loss: 0.812886  [ 2256/ 3200]\n",
      "loss: 0.901226  [ 2272/ 3200]\n",
      "loss: 0.719045  [ 2288/ 3200]\n",
      "loss: 0.537579  [ 2304/ 3200]\n",
      "loss: 1.048148  [ 2320/ 3200]\n",
      "loss: 0.410602  [ 2336/ 3200]\n",
      "loss: 0.699859  [ 2352/ 3200]\n",
      "loss: 0.408985  [ 2368/ 3200]\n",
      "loss: 0.440865  [ 2384/ 3200]\n",
      "loss: 0.573154  [ 2400/ 3200]\n",
      "loss: 0.455790  [ 2416/ 3200]\n",
      "loss: 0.352011  [ 2432/ 3200]\n",
      "loss: 0.652967  [ 2448/ 3200]\n",
      "loss: 0.558894  [ 2464/ 3200]\n",
      "loss: 0.623909  [ 2480/ 3200]\n",
      "loss: 0.625288  [ 2496/ 3200]\n",
      "loss: 0.627041  [ 2512/ 3200]\n",
      "loss: 0.434620  [ 2528/ 3200]\n",
      "loss: 0.488083  [ 2544/ 3200]\n",
      "loss: 0.659983  [ 2560/ 3200]\n",
      "loss: 0.552979  [ 2576/ 3200]\n",
      "loss: 0.630941  [ 2592/ 3200]\n",
      "loss: 0.419258  [ 2608/ 3200]\n",
      "loss: 0.588603  [ 2624/ 3200]\n",
      "loss: 0.282993  [ 2640/ 3200]\n",
      "loss: 0.460003  [ 2656/ 3200]\n",
      "loss: 0.496968  [ 2672/ 3200]\n",
      "loss: 0.706782  [ 2688/ 3200]\n",
      "loss: 0.599080  [ 2704/ 3200]\n",
      "loss: 0.879004  [ 2720/ 3200]\n",
      "loss: 0.530038  [ 2736/ 3200]\n",
      "loss: 0.419642  [ 2752/ 3200]\n",
      "loss: 0.326884  [ 2768/ 3200]\n",
      "loss: 0.500307  [ 2784/ 3200]\n",
      "loss: 0.891897  [ 2800/ 3200]\n",
      "loss: 0.616908  [ 2816/ 3200]\n",
      "loss: 0.675051  [ 2832/ 3200]\n",
      "loss: 0.327238  [ 2848/ 3200]\n",
      "loss: 0.876278  [ 2864/ 3200]\n",
      "loss: 0.511702  [ 2880/ 3200]\n",
      "loss: 0.686673  [ 2896/ 3200]\n",
      "loss: 0.439713  [ 2912/ 3200]\n",
      "loss: 0.416453  [ 2928/ 3200]\n",
      "loss: 0.330133  [ 2944/ 3200]\n",
      "loss: 0.917455  [ 2960/ 3200]\n",
      "loss: 0.339378  [ 2976/ 3200]\n",
      "loss: 0.499058  [ 2992/ 3200]\n",
      "loss: 0.973403  [ 3008/ 3200]\n",
      "loss: 0.556897  [ 3024/ 3200]\n",
      "loss: 0.451835  [ 3040/ 3200]\n",
      "loss: 0.501120  [ 3056/ 3200]\n",
      "loss: 0.610968  [ 3072/ 3200]\n",
      "loss: 0.508075  [ 3088/ 3200]\n",
      "loss: 0.757783  [ 3104/ 3200]\n",
      "loss: 0.591668  [ 3120/ 3200]\n",
      "loss: 0.705410  [ 3136/ 3200]\n",
      "loss: 0.740203  [ 3152/ 3200]\n",
      "loss: 0.748329  [ 3168/ 3200]\n",
      "loss: 0.443580  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038638\n",
      "f1 macro averaged score: 0.726521\n",
      "Accuracy               : 73.4%\n",
      "Confusion matrix       :\n",
      "tensor([[171,  18,   0,  11],\n",
      "        [ 25,  89,  21,  65],\n",
      "        [  0,  20, 159,  21],\n",
      "        [  5,  13,  14, 168]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.7147e-03.\n",
      "\n",
      "Epoch: 4\n",
      "-----------------------------\n",
      "loss: 0.408353  [    0/ 3200]\n",
      "loss: 0.528313  [   16/ 3200]\n",
      "loss: 0.391939  [   32/ 3200]\n",
      "loss: 0.414910  [   48/ 3200]\n",
      "loss: 0.707329  [   64/ 3200]\n",
      "loss: 0.932741  [   80/ 3200]\n",
      "loss: 0.594949  [   96/ 3200]\n",
      "loss: 0.387042  [  112/ 3200]\n",
      "loss: 0.901245  [  128/ 3200]\n",
      "loss: 0.365850  [  144/ 3200]\n",
      "loss: 0.362546  [  160/ 3200]\n",
      "loss: 0.786929  [  176/ 3200]\n",
      "loss: 0.624083  [  192/ 3200]\n",
      "loss: 0.398025  [  208/ 3200]\n",
      "loss: 0.732591  [  224/ 3200]\n",
      "loss: 0.578099  [  240/ 3200]\n",
      "loss: 0.881598  [  256/ 3200]\n",
      "loss: 0.623573  [  272/ 3200]\n",
      "loss: 0.532858  [  288/ 3200]\n",
      "loss: 0.168122  [  304/ 3200]\n",
      "loss: 0.635501  [  320/ 3200]\n",
      "loss: 0.296458  [  336/ 3200]\n",
      "loss: 0.637070  [  352/ 3200]\n",
      "loss: 0.755999  [  368/ 3200]\n",
      "loss: 0.765659  [  384/ 3200]\n",
      "loss: 0.679968  [  400/ 3200]\n",
      "loss: 0.439360  [  416/ 3200]\n",
      "loss: 0.520774  [  432/ 3200]\n",
      "loss: 0.585699  [  448/ 3200]\n",
      "loss: 0.633514  [  464/ 3200]\n",
      "loss: 0.582409  [  480/ 3200]\n",
      "loss: 0.809816  [  496/ 3200]\n",
      "loss: 0.680225  [  512/ 3200]\n",
      "loss: 0.431794  [  528/ 3200]\n",
      "loss: 0.367732  [  544/ 3200]\n",
      "loss: 0.609017  [  560/ 3200]\n",
      "loss: 0.699062  [  576/ 3200]\n",
      "loss: 0.629145  [  592/ 3200]\n",
      "loss: 0.701677  [  608/ 3200]\n",
      "loss: 0.680136  [  624/ 3200]\n",
      "loss: 0.331295  [  640/ 3200]\n",
      "loss: 0.489161  [  656/ 3200]\n",
      "loss: 0.339754  [  672/ 3200]\n",
      "loss: 0.428692  [  688/ 3200]\n",
      "loss: 0.795925  [  704/ 3200]\n",
      "loss: 0.967146  [  720/ 3200]\n",
      "loss: 0.398417  [  736/ 3200]\n",
      "loss: 0.471533  [  752/ 3200]\n",
      "loss: 0.323080  [  768/ 3200]\n",
      "loss: 0.615943  [  784/ 3200]\n",
      "loss: 0.341576  [  800/ 3200]\n",
      "loss: 0.444530  [  816/ 3200]\n",
      "loss: 0.482375  [  832/ 3200]\n",
      "loss: 0.410128  [  848/ 3200]\n",
      "loss: 0.361201  [  864/ 3200]\n",
      "loss: 0.498933  [  880/ 3200]\n",
      "loss: 0.267423  [  896/ 3200]\n",
      "loss: 0.272789  [  912/ 3200]\n",
      "loss: 0.740775  [  928/ 3200]\n",
      "loss: 0.693978  [  944/ 3200]\n",
      "loss: 0.538352  [  960/ 3200]\n",
      "loss: 0.607679  [  976/ 3200]\n",
      "loss: 0.364935  [  992/ 3200]\n",
      "loss: 0.415317  [ 1008/ 3200]\n",
      "loss: 0.748311  [ 1024/ 3200]\n",
      "loss: 0.528280  [ 1040/ 3200]\n",
      "loss: 0.440308  [ 1056/ 3200]\n",
      "loss: 0.554856  [ 1072/ 3200]\n",
      "loss: 0.513721  [ 1088/ 3200]\n",
      "loss: 0.301853  [ 1104/ 3200]\n",
      "loss: 1.108994  [ 1120/ 3200]\n",
      "loss: 0.585854  [ 1136/ 3200]\n",
      "loss: 0.598238  [ 1152/ 3200]\n",
      "loss: 0.672768  [ 1168/ 3200]\n",
      "loss: 0.679010  [ 1184/ 3200]\n",
      "loss: 0.802637  [ 1200/ 3200]\n",
      "loss: 0.623895  [ 1216/ 3200]\n",
      "loss: 0.517274  [ 1232/ 3200]\n",
      "loss: 0.348941  [ 1248/ 3200]\n",
      "loss: 0.887310  [ 1264/ 3200]\n",
      "loss: 0.502634  [ 1280/ 3200]\n",
      "loss: 0.441587  [ 1296/ 3200]\n",
      "loss: 0.330338  [ 1312/ 3200]\n",
      "loss: 0.620797  [ 1328/ 3200]\n",
      "loss: 0.672011  [ 1344/ 3200]\n",
      "loss: 0.452630  [ 1360/ 3200]\n",
      "loss: 0.745331  [ 1376/ 3200]\n",
      "loss: 0.506237  [ 1392/ 3200]\n",
      "loss: 0.308960  [ 1408/ 3200]\n",
      "loss: 0.387018  [ 1424/ 3200]\n",
      "loss: 0.573266  [ 1440/ 3200]\n",
      "loss: 0.792714  [ 1456/ 3200]\n",
      "loss: 0.401498  [ 1472/ 3200]\n",
      "loss: 0.292498  [ 1488/ 3200]\n",
      "loss: 0.579359  [ 1504/ 3200]\n",
      "loss: 0.668562  [ 1520/ 3200]\n",
      "loss: 0.813807  [ 1536/ 3200]\n",
      "loss: 0.500325  [ 1552/ 3200]\n",
      "loss: 0.268294  [ 1568/ 3200]\n",
      "loss: 0.497284  [ 1584/ 3200]\n",
      "loss: 0.545544  [ 1600/ 3200]\n",
      "loss: 0.345492  [ 1616/ 3200]\n",
      "loss: 0.735898  [ 1632/ 3200]\n",
      "loss: 0.656638  [ 1648/ 3200]\n",
      "loss: 0.979994  [ 1664/ 3200]\n",
      "loss: 0.643415  [ 1680/ 3200]\n",
      "loss: 0.580164  [ 1696/ 3200]\n",
      "loss: 0.754934  [ 1712/ 3200]\n",
      "loss: 0.398097  [ 1728/ 3200]\n",
      "loss: 0.532555  [ 1744/ 3200]\n",
      "loss: 0.816051  [ 1760/ 3200]\n",
      "loss: 0.768364  [ 1776/ 3200]\n",
      "loss: 0.638495  [ 1792/ 3200]\n",
      "loss: 0.860370  [ 1808/ 3200]\n",
      "loss: 0.341926  [ 1824/ 3200]\n",
      "loss: 0.440051  [ 1840/ 3200]\n",
      "loss: 0.132528  [ 1856/ 3200]\n",
      "loss: 0.543076  [ 1872/ 3200]\n",
      "loss: 0.751701  [ 1888/ 3200]\n",
      "loss: 0.677376  [ 1904/ 3200]\n",
      "loss: 0.384274  [ 1920/ 3200]\n",
      "loss: 0.252346  [ 1936/ 3200]\n",
      "loss: 0.479076  [ 1952/ 3200]\n",
      "loss: 0.448060  [ 1968/ 3200]\n",
      "loss: 0.553960  [ 1984/ 3200]\n",
      "loss: 0.327870  [ 2000/ 3200]\n",
      "loss: 0.294667  [ 2016/ 3200]\n",
      "loss: 0.724826  [ 2032/ 3200]\n",
      "loss: 0.322447  [ 2048/ 3200]\n",
      "loss: 0.630806  [ 2064/ 3200]\n",
      "loss: 0.501753  [ 2080/ 3200]\n",
      "loss: 0.600368  [ 2096/ 3200]\n",
      "loss: 0.454383  [ 2112/ 3200]\n",
      "loss: 0.846925  [ 2128/ 3200]\n",
      "loss: 0.543180  [ 2144/ 3200]\n",
      "loss: 0.542010  [ 2160/ 3200]\n",
      "loss: 0.631526  [ 2176/ 3200]\n",
      "loss: 0.675047  [ 2192/ 3200]\n",
      "loss: 0.484734  [ 2208/ 3200]\n",
      "loss: 0.500141  [ 2224/ 3200]\n",
      "loss: 0.355586  [ 2240/ 3200]\n",
      "loss: 0.486318  [ 2256/ 3200]\n",
      "loss: 0.807242  [ 2272/ 3200]\n",
      "loss: 0.490185  [ 2288/ 3200]\n",
      "loss: 0.499428  [ 2304/ 3200]\n",
      "loss: 0.846785  [ 2320/ 3200]\n",
      "loss: 0.414862  [ 2336/ 3200]\n",
      "loss: 0.531729  [ 2352/ 3200]\n",
      "loss: 0.263228  [ 2368/ 3200]\n",
      "loss: 0.516292  [ 2384/ 3200]\n",
      "loss: 0.420426  [ 2400/ 3200]\n",
      "loss: 0.526157  [ 2416/ 3200]\n",
      "loss: 0.349385  [ 2432/ 3200]\n",
      "loss: 0.296880  [ 2448/ 3200]\n",
      "loss: 0.242929  [ 2464/ 3200]\n",
      "loss: 0.530092  [ 2480/ 3200]\n",
      "loss: 0.382557  [ 2496/ 3200]\n",
      "loss: 0.502887  [ 2512/ 3200]\n",
      "loss: 0.937203  [ 2528/ 3200]\n",
      "loss: 0.594939  [ 2544/ 3200]\n",
      "loss: 0.381770  [ 2560/ 3200]\n",
      "loss: 0.805600  [ 2576/ 3200]\n",
      "loss: 0.495980  [ 2592/ 3200]\n",
      "loss: 0.534003  [ 2608/ 3200]\n",
      "loss: 0.474530  [ 2624/ 3200]\n",
      "loss: 0.692849  [ 2640/ 3200]\n",
      "loss: 0.922391  [ 2656/ 3200]\n",
      "loss: 0.563209  [ 2672/ 3200]\n",
      "loss: 0.882430  [ 2688/ 3200]\n",
      "loss: 0.301250  [ 2704/ 3200]\n",
      "loss: 0.962716  [ 2720/ 3200]\n",
      "loss: 0.731143  [ 2736/ 3200]\n",
      "loss: 0.397050  [ 2752/ 3200]\n",
      "loss: 0.732930  [ 2768/ 3200]\n",
      "loss: 0.474244  [ 2784/ 3200]\n",
      "loss: 0.469600  [ 2800/ 3200]\n",
      "loss: 0.833070  [ 2816/ 3200]\n",
      "loss: 0.737598  [ 2832/ 3200]\n",
      "loss: 0.632621  [ 2848/ 3200]\n",
      "loss: 0.710240  [ 2864/ 3200]\n",
      "loss: 0.416394  [ 2880/ 3200]\n",
      "loss: 0.485677  [ 2896/ 3200]\n",
      "loss: 0.448742  [ 2912/ 3200]\n",
      "loss: 0.249051  [ 2928/ 3200]\n",
      "loss: 0.373177  [ 2944/ 3200]\n",
      "loss: 0.950123  [ 2960/ 3200]\n",
      "loss: 0.679966  [ 2976/ 3200]\n",
      "loss: 0.475212  [ 2992/ 3200]\n",
      "loss: 0.528759  [ 3008/ 3200]\n",
      "loss: 0.414507  [ 3024/ 3200]\n",
      "loss: 0.496763  [ 3040/ 3200]\n",
      "loss: 0.665452  [ 3056/ 3200]\n",
      "loss: 0.462658  [ 3072/ 3200]\n",
      "loss: 0.633717  [ 3088/ 3200]\n",
      "loss: 0.506209  [ 3104/ 3200]\n",
      "loss: 0.591174  [ 3120/ 3200]\n",
      "loss: 0.779780  [ 3136/ 3200]\n",
      "loss: 0.516293  [ 3152/ 3200]\n",
      "loss: 0.552106  [ 3168/ 3200]\n",
      "loss: 0.628747  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.046979\n",
      "f1 macro averaged score: 0.695529\n",
      "Accuracy               : 69.4%\n",
      "Confusion matrix       :\n",
      "tensor([[100,  93,   3,   4],\n",
      "        [  6, 129,  43,  22],\n",
      "        [  0,  15, 179,   6],\n",
      "        [  0,  32,  21, 147]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.6290e-03.\n",
      "\n",
      "Epoch: 5\n",
      "-----------------------------\n",
      "loss: 0.503645  [    0/ 3200]\n",
      "loss: 0.501167  [   16/ 3200]\n",
      "loss: 0.363205  [   32/ 3200]\n",
      "loss: 0.485636  [   48/ 3200]\n",
      "loss: 0.532636  [   64/ 3200]\n",
      "loss: 0.495442  [   80/ 3200]\n",
      "loss: 0.482843  [   96/ 3200]\n",
      "loss: 0.659184  [  112/ 3200]\n",
      "loss: 0.503856  [  128/ 3200]\n",
      "loss: 0.303228  [  144/ 3200]\n",
      "loss: 0.480008  [  160/ 3200]\n",
      "loss: 0.376874  [  176/ 3200]\n",
      "loss: 0.351197  [  192/ 3200]\n",
      "loss: 0.556805  [  208/ 3200]\n",
      "loss: 0.546465  [  224/ 3200]\n",
      "loss: 0.358228  [  240/ 3200]\n",
      "loss: 0.292871  [  256/ 3200]\n",
      "loss: 0.717495  [  272/ 3200]\n",
      "loss: 0.350318  [  288/ 3200]\n",
      "loss: 0.424803  [  304/ 3200]\n",
      "loss: 0.416106  [  320/ 3200]\n",
      "loss: 0.571713  [  336/ 3200]\n",
      "loss: 0.558803  [  352/ 3200]\n",
      "loss: 0.479680  [  368/ 3200]\n",
      "loss: 0.172574  [  384/ 3200]\n",
      "loss: 0.349312  [  400/ 3200]\n",
      "loss: 0.859441  [  416/ 3200]\n",
      "loss: 0.309720  [  432/ 3200]\n",
      "loss: 0.701263  [  448/ 3200]\n",
      "loss: 0.563570  [  464/ 3200]\n",
      "loss: 0.557355  [  480/ 3200]\n",
      "loss: 0.569150  [  496/ 3200]\n",
      "loss: 0.683912  [  512/ 3200]\n",
      "loss: 0.333699  [  528/ 3200]\n",
      "loss: 0.650122  [  544/ 3200]\n",
      "loss: 0.439245  [  560/ 3200]\n",
      "loss: 1.148058  [  576/ 3200]\n",
      "loss: 0.573107  [  592/ 3200]\n",
      "loss: 0.637149  [  608/ 3200]\n",
      "loss: 0.719334  [  624/ 3200]\n",
      "loss: 0.904715  [  640/ 3200]\n",
      "loss: 0.754003  [  656/ 3200]\n",
      "loss: 0.451082  [  672/ 3200]\n",
      "loss: 0.605462  [  688/ 3200]\n",
      "loss: 0.492682  [  704/ 3200]\n",
      "loss: 0.291064  [  720/ 3200]\n",
      "loss: 0.470743  [  736/ 3200]\n",
      "loss: 0.637486  [  752/ 3200]\n",
      "loss: 0.323994  [  768/ 3200]\n",
      "loss: 0.458188  [  784/ 3200]\n",
      "loss: 0.556765  [  800/ 3200]\n",
      "loss: 0.295144  [  816/ 3200]\n",
      "loss: 0.364703  [  832/ 3200]\n",
      "loss: 0.498854  [  848/ 3200]\n",
      "loss: 0.394198  [  864/ 3200]\n",
      "loss: 0.411116  [  880/ 3200]\n",
      "loss: 0.548989  [  896/ 3200]\n",
      "loss: 0.820969  [  912/ 3200]\n",
      "loss: 0.906768  [  928/ 3200]\n",
      "loss: 0.636594  [  944/ 3200]\n",
      "loss: 0.401950  [  960/ 3200]\n",
      "loss: 1.226438  [  976/ 3200]\n",
      "loss: 0.404585  [  992/ 3200]\n",
      "loss: 0.504500  [ 1008/ 3200]\n",
      "loss: 0.255762  [ 1024/ 3200]\n",
      "loss: 0.518408  [ 1040/ 3200]\n",
      "loss: 0.415186  [ 1056/ 3200]\n",
      "loss: 0.739152  [ 1072/ 3200]\n",
      "loss: 0.258288  [ 1088/ 3200]\n",
      "loss: 0.507008  [ 1104/ 3200]\n",
      "loss: 0.658035  [ 1120/ 3200]\n",
      "loss: 0.795793  [ 1136/ 3200]\n",
      "loss: 0.517201  [ 1152/ 3200]\n",
      "loss: 0.310626  [ 1168/ 3200]\n",
      "loss: 0.382642  [ 1184/ 3200]\n",
      "loss: 0.682422  [ 1200/ 3200]\n",
      "loss: 0.959692  [ 1216/ 3200]\n",
      "loss: 0.729656  [ 1232/ 3200]\n",
      "loss: 0.759834  [ 1248/ 3200]\n",
      "loss: 0.519417  [ 1264/ 3200]\n",
      "loss: 0.470746  [ 1280/ 3200]\n",
      "loss: 0.398598  [ 1296/ 3200]\n",
      "loss: 0.315511  [ 1312/ 3200]\n",
      "loss: 0.850925  [ 1328/ 3200]\n",
      "loss: 0.420344  [ 1344/ 3200]\n",
      "loss: 0.514321  [ 1360/ 3200]\n",
      "loss: 0.375105  [ 1376/ 3200]\n",
      "loss: 0.470728  [ 1392/ 3200]\n",
      "loss: 0.537348  [ 1408/ 3200]\n",
      "loss: 0.513877  [ 1424/ 3200]\n",
      "loss: 0.759334  [ 1440/ 3200]\n",
      "loss: 0.645859  [ 1456/ 3200]\n",
      "loss: 0.282292  [ 1472/ 3200]\n",
      "loss: 0.648961  [ 1488/ 3200]\n",
      "loss: 0.424978  [ 1504/ 3200]\n",
      "loss: 0.218431  [ 1520/ 3200]\n",
      "loss: 0.587171  [ 1536/ 3200]\n",
      "loss: 0.304632  [ 1552/ 3200]\n",
      "loss: 0.622113  [ 1568/ 3200]\n",
      "loss: 0.475541  [ 1584/ 3200]\n",
      "loss: 0.495915  [ 1600/ 3200]\n",
      "loss: 0.309808  [ 1616/ 3200]\n",
      "loss: 0.480132  [ 1632/ 3200]\n",
      "loss: 0.337596  [ 1648/ 3200]\n",
      "loss: 0.550881  [ 1664/ 3200]\n",
      "loss: 0.538814  [ 1680/ 3200]\n",
      "loss: 0.565615  [ 1696/ 3200]\n",
      "loss: 0.357351  [ 1712/ 3200]\n",
      "loss: 0.652450  [ 1728/ 3200]\n",
      "loss: 0.759723  [ 1744/ 3200]\n",
      "loss: 0.767904  [ 1760/ 3200]\n",
      "loss: 0.583390  [ 1776/ 3200]\n",
      "loss: 0.413121  [ 1792/ 3200]\n",
      "loss: 0.565036  [ 1808/ 3200]\n",
      "loss: 0.489352  [ 1824/ 3200]\n",
      "loss: 0.656545  [ 1840/ 3200]\n",
      "loss: 0.549174  [ 1856/ 3200]\n",
      "loss: 0.614716  [ 1872/ 3200]\n",
      "loss: 0.516893  [ 1888/ 3200]\n",
      "loss: 0.775776  [ 1904/ 3200]\n",
      "loss: 0.625485  [ 1920/ 3200]\n",
      "loss: 0.345071  [ 1936/ 3200]\n",
      "loss: 0.660367  [ 1952/ 3200]\n",
      "loss: 0.326759  [ 1968/ 3200]\n",
      "loss: 0.640163  [ 1984/ 3200]\n",
      "loss: 0.474292  [ 2000/ 3200]\n",
      "loss: 0.311088  [ 2016/ 3200]\n",
      "loss: 0.270272  [ 2032/ 3200]\n",
      "loss: 0.623683  [ 2048/ 3200]\n",
      "loss: 0.612830  [ 2064/ 3200]\n",
      "loss: 0.392614  [ 2080/ 3200]\n",
      "loss: 0.516787  [ 2096/ 3200]\n",
      "loss: 0.502426  [ 2112/ 3200]\n",
      "loss: 0.329258  [ 2128/ 3200]\n",
      "loss: 0.359806  [ 2144/ 3200]\n",
      "loss: 0.455354  [ 2160/ 3200]\n",
      "loss: 0.631021  [ 2176/ 3200]\n",
      "loss: 0.444792  [ 2192/ 3200]\n",
      "loss: 0.590708  [ 2208/ 3200]\n",
      "loss: 0.335841  [ 2224/ 3200]\n",
      "loss: 0.362547  [ 2240/ 3200]\n",
      "loss: 0.503397  [ 2256/ 3200]\n",
      "loss: 0.633960  [ 2272/ 3200]\n",
      "loss: 0.532172  [ 2288/ 3200]\n",
      "loss: 0.514716  [ 2304/ 3200]\n",
      "loss: 0.285658  [ 2320/ 3200]\n",
      "loss: 0.563891  [ 2336/ 3200]\n",
      "loss: 0.895153  [ 2352/ 3200]\n",
      "loss: 0.357884  [ 2368/ 3200]\n",
      "loss: 0.392013  [ 2384/ 3200]\n",
      "loss: 0.895042  [ 2400/ 3200]\n",
      "loss: 0.480567  [ 2416/ 3200]\n",
      "loss: 0.360262  [ 2432/ 3200]\n",
      "loss: 0.604964  [ 2448/ 3200]\n",
      "loss: 0.391716  [ 2464/ 3200]\n",
      "loss: 0.868000  [ 2480/ 3200]\n",
      "loss: 0.543630  [ 2496/ 3200]\n",
      "loss: 0.485835  [ 2512/ 3200]\n",
      "loss: 0.390554  [ 2528/ 3200]\n",
      "loss: 0.630528  [ 2544/ 3200]\n",
      "loss: 0.822653  [ 2560/ 3200]\n",
      "loss: 0.266990  [ 2576/ 3200]\n",
      "loss: 0.672643  [ 2592/ 3200]\n",
      "loss: 0.479657  [ 2608/ 3200]\n",
      "loss: 0.692456  [ 2624/ 3200]\n",
      "loss: 0.588912  [ 2640/ 3200]\n",
      "loss: 0.389579  [ 2656/ 3200]\n",
      "loss: 0.515155  [ 2672/ 3200]\n",
      "loss: 0.420830  [ 2688/ 3200]\n",
      "loss: 0.702003  [ 2704/ 3200]\n",
      "loss: 0.351689  [ 2720/ 3200]\n",
      "loss: 0.564765  [ 2736/ 3200]\n",
      "loss: 0.637810  [ 2752/ 3200]\n",
      "loss: 0.324642  [ 2768/ 3200]\n",
      "loss: 0.317917  [ 2784/ 3200]\n",
      "loss: 0.512995  [ 2800/ 3200]\n",
      "loss: 0.624402  [ 2816/ 3200]\n",
      "loss: 0.704324  [ 2832/ 3200]\n",
      "loss: 0.377116  [ 2848/ 3200]\n",
      "loss: 0.455772  [ 2864/ 3200]\n",
      "loss: 0.495292  [ 2880/ 3200]\n",
      "loss: 0.605559  [ 2896/ 3200]\n",
      "loss: 0.190840  [ 2912/ 3200]\n",
      "loss: 0.378129  [ 2928/ 3200]\n",
      "loss: 0.642489  [ 2944/ 3200]\n",
      "loss: 0.506829  [ 2960/ 3200]\n",
      "loss: 0.297628  [ 2976/ 3200]\n",
      "loss: 0.415289  [ 2992/ 3200]\n",
      "loss: 0.542974  [ 3008/ 3200]\n",
      "loss: 0.307122  [ 3024/ 3200]\n",
      "loss: 0.313409  [ 3040/ 3200]\n",
      "loss: 0.413852  [ 3056/ 3200]\n",
      "loss: 0.173197  [ 3072/ 3200]\n",
      "loss: 0.418080  [ 3088/ 3200]\n",
      "loss: 0.408237  [ 3104/ 3200]\n",
      "loss: 0.214003  [ 3120/ 3200]\n",
      "loss: 0.477352  [ 3136/ 3200]\n",
      "loss: 0.406126  [ 3152/ 3200]\n",
      "loss: 0.331261  [ 3168/ 3200]\n",
      "loss: 0.501531  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.039451\n",
      "f1 macro averaged score: 0.758081\n",
      "Accuracy               : 75.0%\n",
      "Confusion matrix       :\n",
      "tensor([[145,  55,   0,   0],\n",
      "        [  8, 158,  24,  10],\n",
      "        [  0,  30, 166,   4],\n",
      "        [  2,  49,  18, 131]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.5476e-03.\n",
      "\n",
      "Epoch: 6\n",
      "-----------------------------\n",
      "loss: 0.337835  [    0/ 3200]\n",
      "loss: 0.450199  [   16/ 3200]\n",
      "loss: 0.296942  [   32/ 3200]\n",
      "loss: 0.334460  [   48/ 3200]\n",
      "loss: 0.358923  [   64/ 3200]\n",
      "loss: 0.517312  [   80/ 3200]\n",
      "loss: 0.478558  [   96/ 3200]\n",
      "loss: 0.364598  [  112/ 3200]\n",
      "loss: 0.530290  [  128/ 3200]\n",
      "loss: 0.353908  [  144/ 3200]\n",
      "loss: 0.637004  [  160/ 3200]\n",
      "loss: 0.543431  [  176/ 3200]\n",
      "loss: 0.265739  [  192/ 3200]\n",
      "loss: 0.809027  [  208/ 3200]\n",
      "loss: 0.271515  [  224/ 3200]\n",
      "loss: 0.341961  [  240/ 3200]\n",
      "loss: 0.397909  [  256/ 3200]\n",
      "loss: 0.505629  [  272/ 3200]\n",
      "loss: 0.170576  [  288/ 3200]\n",
      "loss: 0.159185  [  304/ 3200]\n",
      "loss: 0.374791  [  320/ 3200]\n",
      "loss: 0.276780  [  336/ 3200]\n",
      "loss: 0.634128  [  352/ 3200]\n",
      "loss: 0.477633  [  368/ 3200]\n",
      "loss: 0.469675  [  384/ 3200]\n",
      "loss: 0.330984  [  400/ 3200]\n",
      "loss: 0.199990  [  416/ 3200]\n",
      "loss: 0.304612  [  432/ 3200]\n",
      "loss: 0.492410  [  448/ 3200]\n",
      "loss: 0.349458  [  464/ 3200]\n",
      "loss: 0.251117  [  480/ 3200]\n",
      "loss: 0.699128  [  496/ 3200]\n",
      "loss: 0.439343  [  512/ 3200]\n",
      "loss: 0.386087  [  528/ 3200]\n",
      "loss: 0.864214  [  544/ 3200]\n",
      "loss: 0.637303  [  560/ 3200]\n",
      "loss: 0.389613  [  576/ 3200]\n",
      "loss: 0.756319  [  592/ 3200]\n",
      "loss: 0.683457  [  608/ 3200]\n",
      "loss: 0.725430  [  624/ 3200]\n",
      "loss: 0.994591  [  640/ 3200]\n",
      "loss: 0.860790  [  656/ 3200]\n",
      "loss: 0.274190  [  672/ 3200]\n",
      "loss: 0.264526  [  688/ 3200]\n",
      "loss: 0.317862  [  704/ 3200]\n",
      "loss: 0.523282  [  720/ 3200]\n",
      "loss: 0.524642  [  736/ 3200]\n",
      "loss: 0.329139  [  752/ 3200]\n",
      "loss: 0.656415  [  768/ 3200]\n",
      "loss: 0.379911  [  784/ 3200]\n",
      "loss: 0.413999  [  800/ 3200]\n",
      "loss: 0.679198  [  816/ 3200]\n",
      "loss: 0.567020  [  832/ 3200]\n",
      "loss: 0.753079  [  848/ 3200]\n",
      "loss: 0.594797  [  864/ 3200]\n",
      "loss: 0.504056  [  880/ 3200]\n",
      "loss: 0.328187  [  896/ 3200]\n",
      "loss: 0.593665  [  912/ 3200]\n",
      "loss: 0.515910  [  928/ 3200]\n",
      "loss: 0.512570  [  944/ 3200]\n",
      "loss: 0.554739  [  960/ 3200]\n",
      "loss: 0.555893  [  976/ 3200]\n",
      "loss: 0.392874  [  992/ 3200]\n",
      "loss: 0.392746  [ 1008/ 3200]\n",
      "loss: 0.489779  [ 1024/ 3200]\n",
      "loss: 0.261060  [ 1040/ 3200]\n",
      "loss: 0.611446  [ 1056/ 3200]\n",
      "loss: 0.731587  [ 1072/ 3200]\n",
      "loss: 0.279866  [ 1088/ 3200]\n",
      "loss: 0.319320  [ 1104/ 3200]\n",
      "loss: 0.617223  [ 1120/ 3200]\n",
      "loss: 0.163546  [ 1136/ 3200]\n",
      "loss: 0.354992  [ 1152/ 3200]\n",
      "loss: 0.380188  [ 1168/ 3200]\n",
      "loss: 0.571625  [ 1184/ 3200]\n",
      "loss: 0.291725  [ 1200/ 3200]\n",
      "loss: 0.425864  [ 1216/ 3200]\n",
      "loss: 0.513630  [ 1232/ 3200]\n",
      "loss: 0.394982  [ 1248/ 3200]\n",
      "loss: 0.518387  [ 1264/ 3200]\n",
      "loss: 0.380477  [ 1280/ 3200]\n",
      "loss: 0.394371  [ 1296/ 3200]\n",
      "loss: 0.623100  [ 1312/ 3200]\n",
      "loss: 0.557626  [ 1328/ 3200]\n",
      "loss: 0.415230  [ 1344/ 3200]\n",
      "loss: 0.346378  [ 1360/ 3200]\n",
      "loss: 0.655070  [ 1376/ 3200]\n",
      "loss: 0.402464  [ 1392/ 3200]\n",
      "loss: 0.258811  [ 1408/ 3200]\n",
      "loss: 0.337043  [ 1424/ 3200]\n",
      "loss: 0.712518  [ 1440/ 3200]\n",
      "loss: 0.726090  [ 1456/ 3200]\n",
      "loss: 0.554668  [ 1472/ 3200]\n",
      "loss: 0.971915  [ 1488/ 3200]\n",
      "loss: 0.435903  [ 1504/ 3200]\n",
      "loss: 0.327710  [ 1520/ 3200]\n",
      "loss: 0.661815  [ 1536/ 3200]\n",
      "loss: 0.241060  [ 1552/ 3200]\n",
      "loss: 0.569907  [ 1568/ 3200]\n",
      "loss: 0.458678  [ 1584/ 3200]\n",
      "loss: 0.524529  [ 1600/ 3200]\n",
      "loss: 0.399671  [ 1616/ 3200]\n",
      "loss: 0.188741  [ 1632/ 3200]\n",
      "loss: 0.139172  [ 1648/ 3200]\n",
      "loss: 0.454026  [ 1664/ 3200]\n",
      "loss: 0.264229  [ 1680/ 3200]\n",
      "loss: 0.586652  [ 1696/ 3200]\n",
      "loss: 0.427728  [ 1712/ 3200]\n",
      "loss: 0.289764  [ 1728/ 3200]\n",
      "loss: 0.465805  [ 1744/ 3200]\n",
      "loss: 0.510066  [ 1760/ 3200]\n",
      "loss: 0.221292  [ 1776/ 3200]\n",
      "loss: 0.513266  [ 1792/ 3200]\n",
      "loss: 0.639481  [ 1808/ 3200]\n",
      "loss: 0.363262  [ 1824/ 3200]\n",
      "loss: 0.476958  [ 1840/ 3200]\n",
      "loss: 0.447243  [ 1856/ 3200]\n",
      "loss: 0.623769  [ 1872/ 3200]\n",
      "loss: 0.222441  [ 1888/ 3200]\n",
      "loss: 0.145610  [ 1904/ 3200]\n",
      "loss: 0.366430  [ 1920/ 3200]\n",
      "loss: 0.456003  [ 1936/ 3200]\n",
      "loss: 0.427675  [ 1952/ 3200]\n",
      "loss: 1.267732  [ 1968/ 3200]\n",
      "loss: 0.357352  [ 1984/ 3200]\n",
      "loss: 0.351664  [ 2000/ 3200]\n",
      "loss: 0.423781  [ 2016/ 3200]\n",
      "loss: 0.443632  [ 2032/ 3200]\n",
      "loss: 0.314216  [ 2048/ 3200]\n",
      "loss: 0.246133  [ 2064/ 3200]\n",
      "loss: 0.599292  [ 2080/ 3200]\n",
      "loss: 0.590475  [ 2096/ 3200]\n",
      "loss: 0.316409  [ 2112/ 3200]\n",
      "loss: 0.169749  [ 2128/ 3200]\n",
      "loss: 0.848335  [ 2144/ 3200]\n",
      "loss: 0.934890  [ 2160/ 3200]\n",
      "loss: 0.719313  [ 2176/ 3200]\n",
      "loss: 0.676601  [ 2192/ 3200]\n",
      "loss: 0.471782  [ 2208/ 3200]\n",
      "loss: 0.707365  [ 2224/ 3200]\n",
      "loss: 0.732652  [ 2240/ 3200]\n",
      "loss: 0.671130  [ 2256/ 3200]\n",
      "loss: 0.865565  [ 2272/ 3200]\n",
      "loss: 0.410259  [ 2288/ 3200]\n",
      "loss: 0.716263  [ 2304/ 3200]\n",
      "loss: 0.288077  [ 2320/ 3200]\n",
      "loss: 0.482793  [ 2336/ 3200]\n",
      "loss: 0.946022  [ 2352/ 3200]\n",
      "loss: 0.655587  [ 2368/ 3200]\n",
      "loss: 0.655181  [ 2384/ 3200]\n",
      "loss: 0.520988  [ 2400/ 3200]\n",
      "loss: 0.639553  [ 2416/ 3200]\n",
      "loss: 0.347932  [ 2432/ 3200]\n",
      "loss: 0.649391  [ 2448/ 3200]\n",
      "loss: 0.203071  [ 2464/ 3200]\n",
      "loss: 0.724246  [ 2480/ 3200]\n",
      "loss: 0.469370  [ 2496/ 3200]\n",
      "loss: 0.515446  [ 2512/ 3200]\n",
      "loss: 0.270842  [ 2528/ 3200]\n",
      "loss: 0.668750  [ 2544/ 3200]\n",
      "loss: 0.313220  [ 2560/ 3200]\n",
      "loss: 0.453678  [ 2576/ 3200]\n",
      "loss: 0.371609  [ 2592/ 3200]\n",
      "loss: 0.279982  [ 2608/ 3200]\n",
      "loss: 0.546917  [ 2624/ 3200]\n",
      "loss: 0.457412  [ 2640/ 3200]\n",
      "loss: 0.494938  [ 2656/ 3200]\n",
      "loss: 0.230281  [ 2672/ 3200]\n",
      "loss: 0.227193  [ 2688/ 3200]\n",
      "loss: 0.552163  [ 2704/ 3200]\n",
      "loss: 0.731155  [ 2720/ 3200]\n",
      "loss: 0.358101  [ 2736/ 3200]\n",
      "loss: 0.580145  [ 2752/ 3200]\n",
      "loss: 0.218570  [ 2768/ 3200]\n",
      "loss: 0.372503  [ 2784/ 3200]\n",
      "loss: 0.196025  [ 2800/ 3200]\n",
      "loss: 0.669985  [ 2816/ 3200]\n",
      "loss: 0.169363  [ 2832/ 3200]\n",
      "loss: 0.537863  [ 2848/ 3200]\n",
      "loss: 0.251480  [ 2864/ 3200]\n",
      "loss: 0.212935  [ 2880/ 3200]\n",
      "loss: 0.616777  [ 2896/ 3200]\n",
      "loss: 0.701508  [ 2912/ 3200]\n",
      "loss: 0.230930  [ 2928/ 3200]\n",
      "loss: 0.490565  [ 2944/ 3200]\n",
      "loss: 0.280030  [ 2960/ 3200]\n",
      "loss: 0.334351  [ 2976/ 3200]\n",
      "loss: 0.353466  [ 2992/ 3200]\n",
      "loss: 0.492495  [ 3008/ 3200]\n",
      "loss: 0.591518  [ 3024/ 3200]\n",
      "loss: 0.754083  [ 3040/ 3200]\n",
      "loss: 0.140407  [ 3056/ 3200]\n",
      "loss: 0.340572  [ 3072/ 3200]\n",
      "loss: 0.828315  [ 3088/ 3200]\n",
      "loss: 0.370601  [ 3104/ 3200]\n",
      "loss: 0.314592  [ 3120/ 3200]\n",
      "loss: 0.480248  [ 3136/ 3200]\n",
      "loss: 0.698874  [ 3152/ 3200]\n",
      "loss: 0.352877  [ 3168/ 3200]\n",
      "loss: 0.790908  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.037674\n",
      "f1 macro averaged score: 0.762662\n",
      "Accuracy               : 75.9%\n",
      "Confusion matrix       :\n",
      "tensor([[148,  41,   0,  11],\n",
      "        [  7, 140,  22,  31],\n",
      "        [  0,  26, 161,  13],\n",
      "        [  3,  24,  15, 158]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.4702e-03.\n",
      "\n",
      "Epoch: 7\n",
      "-----------------------------\n",
      "loss: 0.473749  [    0/ 3200]\n",
      "loss: 0.833673  [   16/ 3200]\n",
      "loss: 0.417291  [   32/ 3200]\n",
      "loss: 0.402276  [   48/ 3200]\n",
      "loss: 0.468444  [   64/ 3200]\n",
      "loss: 0.688320  [   80/ 3200]\n",
      "loss: 0.590838  [   96/ 3200]\n",
      "loss: 0.502029  [  112/ 3200]\n",
      "loss: 0.457652  [  128/ 3200]\n",
      "loss: 0.933270  [  144/ 3200]\n",
      "loss: 0.478698  [  160/ 3200]\n",
      "loss: 0.348215  [  176/ 3200]\n",
      "loss: 0.568960  [  192/ 3200]\n",
      "loss: 0.226550  [  208/ 3200]\n",
      "loss: 0.403866  [  224/ 3200]\n",
      "loss: 0.414953  [  240/ 3200]\n",
      "loss: 0.450844  [  256/ 3200]\n",
      "loss: 0.402181  [  272/ 3200]\n",
      "loss: 0.374058  [  288/ 3200]\n",
      "loss: 0.441698  [  304/ 3200]\n",
      "loss: 0.313939  [  320/ 3200]\n",
      "loss: 0.539885  [  336/ 3200]\n",
      "loss: 0.232147  [  352/ 3200]\n",
      "loss: 0.406361  [  368/ 3200]\n",
      "loss: 0.255558  [  384/ 3200]\n",
      "loss: 0.575850  [  400/ 3200]\n",
      "loss: 0.586322  [  416/ 3200]\n",
      "loss: 0.444838  [  432/ 3200]\n",
      "loss: 0.543135  [  448/ 3200]\n",
      "loss: 0.644227  [  464/ 3200]\n",
      "loss: 0.281083  [  480/ 3200]\n",
      "loss: 0.469403  [  496/ 3200]\n",
      "loss: 0.353649  [  512/ 3200]\n",
      "loss: 0.635112  [  528/ 3200]\n",
      "loss: 0.528672  [  544/ 3200]\n",
      "loss: 0.350366  [  560/ 3200]\n",
      "loss: 0.496760  [  576/ 3200]\n",
      "loss: 0.471500  [  592/ 3200]\n",
      "loss: 0.266036  [  608/ 3200]\n",
      "loss: 0.491702  [  624/ 3200]\n",
      "loss: 0.186038  [  640/ 3200]\n",
      "loss: 0.622026  [  656/ 3200]\n",
      "loss: 0.547763  [  672/ 3200]\n",
      "loss: 0.654130  [  688/ 3200]\n",
      "loss: 0.262607  [  704/ 3200]\n",
      "loss: 0.572583  [  720/ 3200]\n",
      "loss: 0.275187  [  736/ 3200]\n",
      "loss: 0.366180  [  752/ 3200]\n",
      "loss: 0.432502  [  768/ 3200]\n",
      "loss: 0.334491  [  784/ 3200]\n",
      "loss: 0.426203  [  800/ 3200]\n",
      "loss: 0.595248  [  816/ 3200]\n",
      "loss: 0.323688  [  832/ 3200]\n",
      "loss: 0.255223  [  848/ 3200]\n",
      "loss: 0.544356  [  864/ 3200]\n",
      "loss: 0.410493  [  880/ 3200]\n",
      "loss: 0.376527  [  896/ 3200]\n",
      "loss: 0.623349  [  912/ 3200]\n",
      "loss: 0.596946  [  928/ 3200]\n",
      "loss: 0.327799  [  944/ 3200]\n",
      "loss: 0.420510  [  960/ 3200]\n",
      "loss: 0.488473  [  976/ 3200]\n",
      "loss: 0.080485  [  992/ 3200]\n",
      "loss: 0.276206  [ 1008/ 3200]\n",
      "loss: 0.291506  [ 1024/ 3200]\n",
      "loss: 0.353436  [ 1040/ 3200]\n",
      "loss: 0.450805  [ 1056/ 3200]\n",
      "loss: 0.301146  [ 1072/ 3200]\n",
      "loss: 0.342515  [ 1088/ 3200]\n",
      "loss: 0.234540  [ 1104/ 3200]\n",
      "loss: 0.283383  [ 1120/ 3200]\n",
      "loss: 0.785883  [ 1136/ 3200]\n",
      "loss: 0.426098  [ 1152/ 3200]\n",
      "loss: 0.450868  [ 1168/ 3200]\n",
      "loss: 0.603777  [ 1184/ 3200]\n",
      "loss: 0.988389  [ 1200/ 3200]\n",
      "loss: 0.670496  [ 1216/ 3200]\n",
      "loss: 0.290988  [ 1232/ 3200]\n",
      "loss: 0.357046  [ 1248/ 3200]\n",
      "loss: 0.154376  [ 1264/ 3200]\n",
      "loss: 0.389178  [ 1280/ 3200]\n",
      "loss: 0.127534  [ 1296/ 3200]\n",
      "loss: 0.310833  [ 1312/ 3200]\n",
      "loss: 0.228649  [ 1328/ 3200]\n",
      "loss: 0.407356  [ 1344/ 3200]\n",
      "loss: 0.474096  [ 1360/ 3200]\n",
      "loss: 1.038290  [ 1376/ 3200]\n",
      "loss: 0.698644  [ 1392/ 3200]\n",
      "loss: 0.281285  [ 1408/ 3200]\n",
      "loss: 0.457865  [ 1424/ 3200]\n",
      "loss: 0.456138  [ 1440/ 3200]\n",
      "loss: 0.370751  [ 1456/ 3200]\n",
      "loss: 0.451395  [ 1472/ 3200]\n",
      "loss: 0.618346  [ 1488/ 3200]\n",
      "loss: 0.661192  [ 1504/ 3200]\n",
      "loss: 0.406704  [ 1520/ 3200]\n",
      "loss: 0.467940  [ 1536/ 3200]\n",
      "loss: 0.383847  [ 1552/ 3200]\n",
      "loss: 0.403150  [ 1568/ 3200]\n",
      "loss: 0.417288  [ 1584/ 3200]\n",
      "loss: 0.230310  [ 1600/ 3200]\n",
      "loss: 0.470754  [ 1616/ 3200]\n",
      "loss: 0.206303  [ 1632/ 3200]\n",
      "loss: 1.051916  [ 1648/ 3200]\n",
      "loss: 0.475220  [ 1664/ 3200]\n",
      "loss: 0.568583  [ 1680/ 3200]\n",
      "loss: 0.404659  [ 1696/ 3200]\n",
      "loss: 0.565499  [ 1712/ 3200]\n",
      "loss: 0.468601  [ 1728/ 3200]\n",
      "loss: 0.504921  [ 1744/ 3200]\n",
      "loss: 0.264511  [ 1760/ 3200]\n",
      "loss: 0.301423  [ 1776/ 3200]\n",
      "loss: 0.159089  [ 1792/ 3200]\n",
      "loss: 0.294608  [ 1808/ 3200]\n",
      "loss: 0.844855  [ 1824/ 3200]\n",
      "loss: 0.182770  [ 1840/ 3200]\n",
      "loss: 0.424440  [ 1856/ 3200]\n",
      "loss: 0.578670  [ 1872/ 3200]\n",
      "loss: 0.445234  [ 1888/ 3200]\n",
      "loss: 0.459404  [ 1904/ 3200]\n",
      "loss: 0.236022  [ 1920/ 3200]\n",
      "loss: 0.174291  [ 1936/ 3200]\n",
      "loss: 0.273049  [ 1952/ 3200]\n",
      "loss: 0.390864  [ 1968/ 3200]\n",
      "loss: 0.203185  [ 1984/ 3200]\n",
      "loss: 0.264839  [ 2000/ 3200]\n",
      "loss: 0.285106  [ 2016/ 3200]\n",
      "loss: 0.492556  [ 2032/ 3200]\n",
      "loss: 0.313746  [ 2048/ 3200]\n",
      "loss: 0.265838  [ 2064/ 3200]\n",
      "loss: 0.150261  [ 2080/ 3200]\n",
      "loss: 0.461560  [ 2096/ 3200]\n",
      "loss: 0.267711  [ 2112/ 3200]\n",
      "loss: 0.385817  [ 2128/ 3200]\n",
      "loss: 0.586709  [ 2144/ 3200]\n",
      "loss: 0.249289  [ 2160/ 3200]\n",
      "loss: 0.312473  [ 2176/ 3200]\n",
      "loss: 0.235886  [ 2192/ 3200]\n",
      "loss: 0.408655  [ 2208/ 3200]\n",
      "loss: 0.213867  [ 2224/ 3200]\n",
      "loss: 0.485417  [ 2240/ 3200]\n",
      "loss: 0.793733  [ 2256/ 3200]\n",
      "loss: 1.178935  [ 2272/ 3200]\n",
      "loss: 0.546151  [ 2288/ 3200]\n",
      "loss: 0.318037  [ 2304/ 3200]\n",
      "loss: 0.498426  [ 2320/ 3200]\n",
      "loss: 0.411667  [ 2336/ 3200]\n",
      "loss: 0.450565  [ 2352/ 3200]\n",
      "loss: 0.426429  [ 2368/ 3200]\n",
      "loss: 0.288943  [ 2384/ 3200]\n",
      "loss: 0.829687  [ 2400/ 3200]\n",
      "loss: 0.419583  [ 2416/ 3200]\n",
      "loss: 0.264257  [ 2432/ 3200]\n",
      "loss: 0.414830  [ 2448/ 3200]\n",
      "loss: 0.228314  [ 2464/ 3200]\n",
      "loss: 0.417509  [ 2480/ 3200]\n",
      "loss: 0.411148  [ 2496/ 3200]\n",
      "loss: 0.385890  [ 2512/ 3200]\n",
      "loss: 0.342760  [ 2528/ 3200]\n",
      "loss: 0.317699  [ 2544/ 3200]\n",
      "loss: 0.552306  [ 2560/ 3200]\n",
      "loss: 0.829515  [ 2576/ 3200]\n",
      "loss: 0.577318  [ 2592/ 3200]\n",
      "loss: 0.232861  [ 2608/ 3200]\n",
      "loss: 0.441497  [ 2624/ 3200]\n",
      "loss: 0.149651  [ 2640/ 3200]\n",
      "loss: 0.518892  [ 2656/ 3200]\n",
      "loss: 0.305096  [ 2672/ 3200]\n",
      "loss: 0.504217  [ 2688/ 3200]\n",
      "loss: 0.359586  [ 2704/ 3200]\n",
      "loss: 0.203213  [ 2720/ 3200]\n",
      "loss: 0.285921  [ 2736/ 3200]\n",
      "loss: 0.581766  [ 2752/ 3200]\n",
      "loss: 0.636225  [ 2768/ 3200]\n",
      "loss: 0.147685  [ 2784/ 3200]\n",
      "loss: 0.487472  [ 2800/ 3200]\n",
      "loss: 0.290951  [ 2816/ 3200]\n",
      "loss: 0.203815  [ 2832/ 3200]\n",
      "loss: 0.278253  [ 2848/ 3200]\n",
      "loss: 0.943583  [ 2864/ 3200]\n",
      "loss: 0.889826  [ 2880/ 3200]\n",
      "loss: 0.247941  [ 2896/ 3200]\n",
      "loss: 0.429018  [ 2912/ 3200]\n",
      "loss: 0.455302  [ 2928/ 3200]\n",
      "loss: 0.148181  [ 2944/ 3200]\n",
      "loss: 0.327788  [ 2960/ 3200]\n",
      "loss: 0.522843  [ 2976/ 3200]\n",
      "loss: 0.283933  [ 2992/ 3200]\n",
      "loss: 0.688349  [ 3008/ 3200]\n",
      "loss: 0.848309  [ 3024/ 3200]\n",
      "loss: 0.689017  [ 3040/ 3200]\n",
      "loss: 0.494857  [ 3056/ 3200]\n",
      "loss: 0.505716  [ 3072/ 3200]\n",
      "loss: 0.313143  [ 3088/ 3200]\n",
      "loss: 0.091239  [ 3104/ 3200]\n",
      "loss: 0.526228  [ 3120/ 3200]\n",
      "loss: 0.349981  [ 3136/ 3200]\n",
      "loss: 0.411142  [ 3152/ 3200]\n",
      "loss: 0.403561  [ 3168/ 3200]\n",
      "loss: 0.463384  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034575\n",
      "f1 macro averaged score: 0.790397\n",
      "Accuracy               : 79.0%\n",
      "Confusion matrix       :\n",
      "tensor([[175,  21,   0,   4],\n",
      "        [ 13, 137,  29,  21],\n",
      "        [  0,  22, 173,   5],\n",
      "        [  6,  28,  19, 147]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3967e-03.\n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------\n",
      "loss: 0.722626  [    0/ 3200]\n",
      "loss: 0.345807  [   16/ 3200]\n",
      "loss: 0.405905  [   32/ 3200]\n",
      "loss: 0.382185  [   48/ 3200]\n",
      "loss: 0.292689  [   64/ 3200]\n",
      "loss: 0.615524  [   80/ 3200]\n",
      "loss: 0.332033  [   96/ 3200]\n",
      "loss: 0.233436  [  112/ 3200]\n",
      "loss: 0.340839  [  128/ 3200]\n",
      "loss: 0.537426  [  144/ 3200]\n",
      "loss: 0.231684  [  160/ 3200]\n",
      "loss: 0.364387  [  176/ 3200]\n",
      "loss: 0.668983  [  192/ 3200]\n",
      "loss: 0.401259  [  208/ 3200]\n",
      "loss: 0.333334  [  224/ 3200]\n",
      "loss: 0.265595  [  240/ 3200]\n",
      "loss: 0.744242  [  256/ 3200]\n",
      "loss: 0.315843  [  272/ 3200]\n",
      "loss: 0.351039  [  288/ 3200]\n",
      "loss: 0.253337  [  304/ 3200]\n",
      "loss: 0.756704  [  320/ 3200]\n",
      "loss: 0.358875  [  336/ 3200]\n",
      "loss: 0.667139  [  352/ 3200]\n",
      "loss: 0.318722  [  368/ 3200]\n",
      "loss: 0.362918  [  384/ 3200]\n",
      "loss: 0.653803  [  400/ 3200]\n",
      "loss: 0.347193  [  416/ 3200]\n",
      "loss: 0.806887  [  432/ 3200]\n",
      "loss: 0.364402  [  448/ 3200]\n",
      "loss: 0.480157  [  464/ 3200]\n",
      "loss: 0.190588  [  480/ 3200]\n",
      "loss: 0.257062  [  496/ 3200]\n",
      "loss: 0.695572  [  512/ 3200]\n",
      "loss: 0.329582  [  528/ 3200]\n",
      "loss: 0.814550  [  544/ 3200]\n",
      "loss: 0.578294  [  560/ 3200]\n",
      "loss: 0.474079  [  576/ 3200]\n",
      "loss: 0.333563  [  592/ 3200]\n",
      "loss: 0.309042  [  608/ 3200]\n",
      "loss: 0.243152  [  624/ 3200]\n",
      "loss: 0.297518  [  640/ 3200]\n",
      "loss: 0.713605  [  656/ 3200]\n",
      "loss: 0.477687  [  672/ 3200]\n",
      "loss: 0.429909  [  688/ 3200]\n",
      "loss: 0.309330  [  704/ 3200]\n",
      "loss: 0.382032  [  720/ 3200]\n",
      "loss: 0.224526  [  736/ 3200]\n",
      "loss: 0.387950  [  752/ 3200]\n",
      "loss: 0.461846  [  768/ 3200]\n",
      "loss: 0.538864  [  784/ 3200]\n",
      "loss: 0.211462  [  800/ 3200]\n",
      "loss: 0.481722  [  816/ 3200]\n",
      "loss: 0.231272  [  832/ 3200]\n",
      "loss: 0.253180  [  848/ 3200]\n",
      "loss: 0.678171  [  864/ 3200]\n",
      "loss: 0.417572  [  880/ 3200]\n",
      "loss: 0.294559  [  896/ 3200]\n",
      "loss: 0.166171  [  912/ 3200]\n",
      "loss: 0.594097  [  928/ 3200]\n",
      "loss: 0.326055  [  944/ 3200]\n",
      "loss: 0.371613  [  960/ 3200]\n",
      "loss: 0.230570  [  976/ 3200]\n",
      "loss: 0.268375  [  992/ 3200]\n",
      "loss: 0.472625  [ 1008/ 3200]\n",
      "loss: 0.699860  [ 1024/ 3200]\n",
      "loss: 0.488638  [ 1040/ 3200]\n",
      "loss: 0.293000  [ 1056/ 3200]\n",
      "loss: 0.500300  [ 1072/ 3200]\n",
      "loss: 0.422985  [ 1088/ 3200]\n",
      "loss: 0.610486  [ 1104/ 3200]\n",
      "loss: 0.156949  [ 1120/ 3200]\n",
      "loss: 0.231346  [ 1136/ 3200]\n",
      "loss: 0.358521  [ 1152/ 3200]\n",
      "loss: 0.634192  [ 1168/ 3200]\n",
      "loss: 0.572526  [ 1184/ 3200]\n",
      "loss: 0.524115  [ 1200/ 3200]\n",
      "loss: 0.464204  [ 1216/ 3200]\n",
      "loss: 0.182070  [ 1232/ 3200]\n",
      "loss: 0.285385  [ 1248/ 3200]\n",
      "loss: 0.245276  [ 1264/ 3200]\n",
      "loss: 0.489475  [ 1280/ 3200]\n",
      "loss: 0.342785  [ 1296/ 3200]\n",
      "loss: 0.395709  [ 1312/ 3200]\n",
      "loss: 0.405147  [ 1328/ 3200]\n",
      "loss: 0.312548  [ 1344/ 3200]\n",
      "loss: 0.271835  [ 1360/ 3200]\n",
      "loss: 0.335847  [ 1376/ 3200]\n",
      "loss: 0.210596  [ 1392/ 3200]\n",
      "loss: 0.465798  [ 1408/ 3200]\n",
      "loss: 0.513131  [ 1424/ 3200]\n",
      "loss: 0.694038  [ 1440/ 3200]\n",
      "loss: 0.482798  [ 1456/ 3200]\n",
      "loss: 0.121615  [ 1472/ 3200]\n",
      "loss: 0.304815  [ 1488/ 3200]\n",
      "loss: 0.231532  [ 1504/ 3200]\n",
      "loss: 0.627718  [ 1520/ 3200]\n",
      "loss: 0.577494  [ 1536/ 3200]\n",
      "loss: 0.329413  [ 1552/ 3200]\n",
      "loss: 0.148028  [ 1568/ 3200]\n",
      "loss: 0.409571  [ 1584/ 3200]\n",
      "loss: 0.568524  [ 1600/ 3200]\n",
      "loss: 0.176837  [ 1616/ 3200]\n",
      "loss: 0.196965  [ 1632/ 3200]\n",
      "loss: 0.277437  [ 1648/ 3200]\n",
      "loss: 0.875362  [ 1664/ 3200]\n",
      "loss: 0.450626  [ 1680/ 3200]\n",
      "loss: 0.695380  [ 1696/ 3200]\n",
      "loss: 0.297884  [ 1712/ 3200]\n",
      "loss: 0.248667  [ 1728/ 3200]\n",
      "loss: 0.147117  [ 1744/ 3200]\n",
      "loss: 0.226490  [ 1760/ 3200]\n",
      "loss: 0.605389  [ 1776/ 3200]\n",
      "loss: 0.276510  [ 1792/ 3200]\n",
      "loss: 0.326850  [ 1808/ 3200]\n",
      "loss: 0.546987  [ 1824/ 3200]\n",
      "loss: 0.324208  [ 1840/ 3200]\n",
      "loss: 0.448353  [ 1856/ 3200]\n",
      "loss: 0.344595  [ 1872/ 3200]\n",
      "loss: 0.222335  [ 1888/ 3200]\n",
      "loss: 0.349382  [ 1904/ 3200]\n",
      "loss: 0.230836  [ 1920/ 3200]\n",
      "loss: 0.375062  [ 1936/ 3200]\n",
      "loss: 0.167615  [ 1952/ 3200]\n",
      "loss: 0.190974  [ 1968/ 3200]\n",
      "loss: 0.315448  [ 1984/ 3200]\n",
      "loss: 0.267881  [ 2000/ 3200]\n",
      "loss: 0.423506  [ 2016/ 3200]\n",
      "loss: 0.423203  [ 2032/ 3200]\n",
      "loss: 0.609608  [ 2048/ 3200]\n",
      "loss: 0.298387  [ 2064/ 3200]\n",
      "loss: 0.449180  [ 2080/ 3200]\n",
      "loss: 0.693562  [ 2096/ 3200]\n",
      "loss: 0.436794  [ 2112/ 3200]\n",
      "loss: 0.550257  [ 2128/ 3200]\n",
      "loss: 0.543186  [ 2144/ 3200]\n",
      "loss: 0.696054  [ 2160/ 3200]\n",
      "loss: 0.272787  [ 2176/ 3200]\n",
      "loss: 0.551335  [ 2192/ 3200]\n",
      "loss: 0.223390  [ 2208/ 3200]\n",
      "loss: 0.441858  [ 2224/ 3200]\n",
      "loss: 0.503308  [ 2240/ 3200]\n",
      "loss: 0.260238  [ 2256/ 3200]\n",
      "loss: 0.401740  [ 2272/ 3200]\n",
      "loss: 0.696788  [ 2288/ 3200]\n",
      "loss: 0.435091  [ 2304/ 3200]\n",
      "loss: 0.500743  [ 2320/ 3200]\n",
      "loss: 0.553019  [ 2336/ 3200]\n",
      "loss: 0.415142  [ 2352/ 3200]\n",
      "loss: 0.577425  [ 2368/ 3200]\n",
      "loss: 0.418053  [ 2384/ 3200]\n",
      "loss: 0.400122  [ 2400/ 3200]\n",
      "loss: 0.287996  [ 2416/ 3200]\n",
      "loss: 0.790627  [ 2432/ 3200]\n",
      "loss: 0.425696  [ 2448/ 3200]\n",
      "loss: 0.557977  [ 2464/ 3200]\n",
      "loss: 0.218739  [ 2480/ 3200]\n",
      "loss: 0.185185  [ 2496/ 3200]\n",
      "loss: 0.507042  [ 2512/ 3200]\n",
      "loss: 0.228793  [ 2528/ 3200]\n",
      "loss: 0.346087  [ 2544/ 3200]\n",
      "loss: 0.215706  [ 2560/ 3200]\n",
      "loss: 0.496995  [ 2576/ 3200]\n",
      "loss: 0.329096  [ 2592/ 3200]\n",
      "loss: 0.460788  [ 2608/ 3200]\n",
      "loss: 0.296008  [ 2624/ 3200]\n",
      "loss: 0.331492  [ 2640/ 3200]\n",
      "loss: 0.210430  [ 2656/ 3200]\n",
      "loss: 0.246874  [ 2672/ 3200]\n",
      "loss: 0.417587  [ 2688/ 3200]\n",
      "loss: 0.568795  [ 2704/ 3200]\n",
      "loss: 0.758683  [ 2720/ 3200]\n",
      "loss: 0.209591  [ 2736/ 3200]\n",
      "loss: 0.293696  [ 2752/ 3200]\n",
      "loss: 0.221161  [ 2768/ 3200]\n",
      "loss: 0.383369  [ 2784/ 3200]\n",
      "loss: 0.135280  [ 2800/ 3200]\n",
      "loss: 0.389327  [ 2816/ 3200]\n",
      "loss: 0.304076  [ 2832/ 3200]\n",
      "loss: 0.788973  [ 2848/ 3200]\n",
      "loss: 0.199706  [ 2864/ 3200]\n",
      "loss: 0.564730  [ 2880/ 3200]\n",
      "loss: 0.397487  [ 2896/ 3200]\n",
      "loss: 0.724751  [ 2912/ 3200]\n",
      "loss: 0.509243  [ 2928/ 3200]\n",
      "loss: 0.457347  [ 2944/ 3200]\n",
      "loss: 0.520009  [ 2960/ 3200]\n",
      "loss: 0.237501  [ 2976/ 3200]\n",
      "loss: 0.265768  [ 2992/ 3200]\n",
      "loss: 0.224200  [ 3008/ 3200]\n",
      "loss: 0.084592  [ 3024/ 3200]\n",
      "loss: 0.946365  [ 3040/ 3200]\n",
      "loss: 0.259146  [ 3056/ 3200]\n",
      "loss: 0.197989  [ 3072/ 3200]\n",
      "loss: 0.100926  [ 3088/ 3200]\n",
      "loss: 0.304528  [ 3104/ 3200]\n",
      "loss: 0.626523  [ 3120/ 3200]\n",
      "loss: 0.321436  [ 3136/ 3200]\n",
      "loss: 0.582522  [ 3152/ 3200]\n",
      "loss: 0.363273  [ 3168/ 3200]\n",
      "loss: 0.436696  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036007\n",
      "f1 macro averaged score: 0.771031\n",
      "Accuracy               : 77.6%\n",
      "Confusion matrix       :\n",
      "tensor([[177,   8,   0,  15],\n",
      "        [ 18, 105,  26,  51],\n",
      "        [  0,  11, 166,  23],\n",
      "        [  6,   9,  12, 173]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.3268e-03.\n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------\n",
      "loss: 0.282086  [    0/ 3200]\n",
      "loss: 0.420427  [   16/ 3200]\n",
      "loss: 0.450545  [   32/ 3200]\n",
      "loss: 0.359270  [   48/ 3200]\n",
      "loss: 0.274293  [   64/ 3200]\n",
      "loss: 0.373022  [   80/ 3200]\n",
      "loss: 0.366253  [   96/ 3200]\n",
      "loss: 0.148968  [  112/ 3200]\n",
      "loss: 0.297991  [  128/ 3200]\n",
      "loss: 0.597421  [  144/ 3200]\n",
      "loss: 0.591509  [  160/ 3200]\n",
      "loss: 0.376630  [  176/ 3200]\n",
      "loss: 0.448975  [  192/ 3200]\n",
      "loss: 0.183361  [  208/ 3200]\n",
      "loss: 0.434079  [  224/ 3200]\n",
      "loss: 0.307609  [  240/ 3200]\n",
      "loss: 0.230578  [  256/ 3200]\n",
      "loss: 0.294995  [  272/ 3200]\n",
      "loss: 0.565902  [  288/ 3200]\n",
      "loss: 0.227452  [  304/ 3200]\n",
      "loss: 0.626587  [  320/ 3200]\n",
      "loss: 0.368665  [  336/ 3200]\n",
      "loss: 0.432608  [  352/ 3200]\n",
      "loss: 0.462458  [  368/ 3200]\n",
      "loss: 0.093482  [  384/ 3200]\n",
      "loss: 0.386988  [  400/ 3200]\n",
      "loss: 0.445908  [  416/ 3200]\n",
      "loss: 0.078382  [  432/ 3200]\n",
      "loss: 0.274780  [  448/ 3200]\n",
      "loss: 0.197692  [  464/ 3200]\n",
      "loss: 0.411595  [  480/ 3200]\n",
      "loss: 0.187335  [  496/ 3200]\n",
      "loss: 0.294994  [  512/ 3200]\n",
      "loss: 0.330018  [  528/ 3200]\n",
      "loss: 0.347933  [  544/ 3200]\n",
      "loss: 0.455340  [  560/ 3200]\n",
      "loss: 0.355590  [  576/ 3200]\n",
      "loss: 0.327525  [  592/ 3200]\n",
      "loss: 0.341575  [  608/ 3200]\n",
      "loss: 0.568943  [  624/ 3200]\n",
      "loss: 0.446090  [  640/ 3200]\n",
      "loss: 0.230266  [  656/ 3200]\n",
      "loss: 0.399971  [  672/ 3200]\n",
      "loss: 0.150004  [  688/ 3200]\n",
      "loss: 0.578657  [  704/ 3200]\n",
      "loss: 0.666262  [  720/ 3200]\n",
      "loss: 0.389547  [  736/ 3200]\n",
      "loss: 0.347071  [  752/ 3200]\n",
      "loss: 0.310134  [  768/ 3200]\n",
      "loss: 0.393648  [  784/ 3200]\n",
      "loss: 0.388479  [  800/ 3200]\n",
      "loss: 0.399557  [  816/ 3200]\n",
      "loss: 0.237430  [  832/ 3200]\n",
      "loss: 0.148910  [  848/ 3200]\n",
      "loss: 0.509575  [  864/ 3200]\n",
      "loss: 0.255856  [  880/ 3200]\n",
      "loss: 0.199251  [  896/ 3200]\n",
      "loss: 0.444741  [  912/ 3200]\n",
      "loss: 0.198809  [  928/ 3200]\n",
      "loss: 0.296737  [  944/ 3200]\n",
      "loss: 0.538546  [  960/ 3200]\n",
      "loss: 0.310363  [  976/ 3200]\n",
      "loss: 0.692516  [  992/ 3200]\n",
      "loss: 0.413360  [ 1008/ 3200]\n",
      "loss: 0.506782  [ 1024/ 3200]\n",
      "loss: 0.429556  [ 1040/ 3200]\n",
      "loss: 0.350586  [ 1056/ 3200]\n",
      "loss: 0.585835  [ 1072/ 3200]\n",
      "loss: 0.760585  [ 1088/ 3200]\n",
      "loss: 0.242767  [ 1104/ 3200]\n",
      "loss: 0.404011  [ 1120/ 3200]\n",
      "loss: 0.317672  [ 1136/ 3200]\n",
      "loss: 0.305030  [ 1152/ 3200]\n",
      "loss: 0.345707  [ 1168/ 3200]\n",
      "loss: 0.367834  [ 1184/ 3200]\n",
      "loss: 0.265413  [ 1200/ 3200]\n",
      "loss: 0.643402  [ 1216/ 3200]\n",
      "loss: 0.475199  [ 1232/ 3200]\n",
      "loss: 0.338463  [ 1248/ 3200]\n",
      "loss: 0.472013  [ 1264/ 3200]\n",
      "loss: 0.327664  [ 1280/ 3200]\n",
      "loss: 0.429654  [ 1296/ 3200]\n",
      "loss: 0.357437  [ 1312/ 3200]\n",
      "loss: 0.448934  [ 1328/ 3200]\n",
      "loss: 0.461075  [ 1344/ 3200]\n",
      "loss: 0.326795  [ 1360/ 3200]\n",
      "loss: 0.334395  [ 1376/ 3200]\n",
      "loss: 0.251338  [ 1392/ 3200]\n",
      "loss: 0.616517  [ 1408/ 3200]\n",
      "loss: 0.281684  [ 1424/ 3200]\n",
      "loss: 0.861475  [ 1440/ 3200]\n",
      "loss: 0.443447  [ 1456/ 3200]\n",
      "loss: 0.629516  [ 1472/ 3200]\n",
      "loss: 0.690555  [ 1488/ 3200]\n",
      "loss: 0.163329  [ 1504/ 3200]\n",
      "loss: 0.128532  [ 1520/ 3200]\n",
      "loss: 0.271841  [ 1536/ 3200]\n",
      "loss: 0.448573  [ 1552/ 3200]\n",
      "loss: 0.210040  [ 1568/ 3200]\n",
      "loss: 0.087724  [ 1584/ 3200]\n",
      "loss: 0.361007  [ 1600/ 3200]\n",
      "loss: 0.323061  [ 1616/ 3200]\n",
      "loss: 0.345564  [ 1632/ 3200]\n",
      "loss: 0.346124  [ 1648/ 3200]\n",
      "loss: 0.379152  [ 1664/ 3200]\n",
      "loss: 0.304482  [ 1680/ 3200]\n",
      "loss: 0.207771  [ 1696/ 3200]\n",
      "loss: 0.309980  [ 1712/ 3200]\n",
      "loss: 0.416897  [ 1728/ 3200]\n",
      "loss: 0.544835  [ 1744/ 3200]\n",
      "loss: 0.283280  [ 1760/ 3200]\n",
      "loss: 0.313717  [ 1776/ 3200]\n",
      "loss: 0.720401  [ 1792/ 3200]\n",
      "loss: 0.396924  [ 1808/ 3200]\n",
      "loss: 0.184580  [ 1824/ 3200]\n",
      "loss: 0.380963  [ 1840/ 3200]\n",
      "loss: 0.537112  [ 1856/ 3200]\n",
      "loss: 0.311721  [ 1872/ 3200]\n",
      "loss: 0.167252  [ 1888/ 3200]\n",
      "loss: 0.338658  [ 1904/ 3200]\n",
      "loss: 0.298263  [ 1920/ 3200]\n",
      "loss: 0.294663  [ 1936/ 3200]\n",
      "loss: 0.593865  [ 1952/ 3200]\n",
      "loss: 0.235662  [ 1968/ 3200]\n",
      "loss: 0.271829  [ 1984/ 3200]\n",
      "loss: 0.356518  [ 2000/ 3200]\n",
      "loss: 0.263480  [ 2016/ 3200]\n",
      "loss: 0.310933  [ 2032/ 3200]\n",
      "loss: 0.318392  [ 2048/ 3200]\n",
      "loss: 0.266504  [ 2064/ 3200]\n",
      "loss: 0.281108  [ 2080/ 3200]\n",
      "loss: 0.103047  [ 2096/ 3200]\n",
      "loss: 0.270487  [ 2112/ 3200]\n",
      "loss: 0.305065  [ 2128/ 3200]\n",
      "loss: 0.209137  [ 2144/ 3200]\n",
      "loss: 0.689895  [ 2160/ 3200]\n",
      "loss: 0.263024  [ 2176/ 3200]\n",
      "loss: 0.163188  [ 2192/ 3200]\n",
      "loss: 0.190254  [ 2208/ 3200]\n",
      "loss: 0.640272  [ 2224/ 3200]\n",
      "loss: 0.214706  [ 2240/ 3200]\n",
      "loss: 0.374340  [ 2256/ 3200]\n",
      "loss: 0.306432  [ 2272/ 3200]\n",
      "loss: 0.508167  [ 2288/ 3200]\n",
      "loss: 0.455308  [ 2304/ 3200]\n",
      "loss: 0.541949  [ 2320/ 3200]\n",
      "loss: 0.329828  [ 2336/ 3200]\n",
      "loss: 0.482659  [ 2352/ 3200]\n",
      "loss: 0.705185  [ 2368/ 3200]\n",
      "loss: 0.282469  [ 2384/ 3200]\n",
      "loss: 0.247311  [ 2400/ 3200]\n",
      "loss: 0.250864  [ 2416/ 3200]\n",
      "loss: 0.214831  [ 2432/ 3200]\n",
      "loss: 0.195428  [ 2448/ 3200]\n",
      "loss: 0.181829  [ 2464/ 3200]\n",
      "loss: 0.278436  [ 2480/ 3200]\n",
      "loss: 0.187814  [ 2496/ 3200]\n",
      "loss: 0.343801  [ 2512/ 3200]\n",
      "loss: 0.432514  [ 2528/ 3200]\n",
      "loss: 0.256411  [ 2544/ 3200]\n",
      "loss: 0.264427  [ 2560/ 3200]\n",
      "loss: 0.341889  [ 2576/ 3200]\n",
      "loss: 0.444512  [ 2592/ 3200]\n",
      "loss: 0.455383  [ 2608/ 3200]\n",
      "loss: 0.339764  [ 2624/ 3200]\n",
      "loss: 0.493646  [ 2640/ 3200]\n",
      "loss: 0.246953  [ 2656/ 3200]\n",
      "loss: 0.346781  [ 2672/ 3200]\n",
      "loss: 0.520762  [ 2688/ 3200]\n",
      "loss: 0.169824  [ 2704/ 3200]\n",
      "loss: 0.561909  [ 2720/ 3200]\n",
      "loss: 0.407449  [ 2736/ 3200]\n",
      "loss: 0.335330  [ 2752/ 3200]\n",
      "loss: 0.878565  [ 2768/ 3200]\n",
      "loss: 0.549145  [ 2784/ 3200]\n",
      "loss: 0.145075  [ 2800/ 3200]\n",
      "loss: 0.162517  [ 2816/ 3200]\n",
      "loss: 0.455696  [ 2832/ 3200]\n",
      "loss: 0.618392  [ 2848/ 3200]\n",
      "loss: 0.347112  [ 2864/ 3200]\n",
      "loss: 0.375558  [ 2880/ 3200]\n",
      "loss: 0.482112  [ 2896/ 3200]\n",
      "loss: 0.420248  [ 2912/ 3200]\n",
      "loss: 0.207195  [ 2928/ 3200]\n",
      "loss: 0.465149  [ 2944/ 3200]\n",
      "loss: 0.240517  [ 2960/ 3200]\n",
      "loss: 0.744863  [ 2976/ 3200]\n",
      "loss: 0.541458  [ 2992/ 3200]\n",
      "loss: 0.296060  [ 3008/ 3200]\n",
      "loss: 0.213908  [ 3024/ 3200]\n",
      "loss: 0.245373  [ 3040/ 3200]\n",
      "loss: 0.373721  [ 3056/ 3200]\n",
      "loss: 0.419714  [ 3072/ 3200]\n",
      "loss: 0.219925  [ 3088/ 3200]\n",
      "loss: 0.118860  [ 3104/ 3200]\n",
      "loss: 0.330247  [ 3120/ 3200]\n",
      "loss: 0.590511  [ 3136/ 3200]\n",
      "loss: 0.405296  [ 3152/ 3200]\n",
      "loss: 0.220691  [ 3168/ 3200]\n",
      "loss: 0.481853  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.034430\n",
      "f1 macro averaged score: 0.787223\n",
      "Accuracy               : 78.6%\n",
      "Confusion matrix       :\n",
      "tensor([[174,  21,   0,   5],\n",
      "        [ 13, 135,  25,  27],\n",
      "        [  0,  27, 167,   6],\n",
      "        [  6,  26,  15, 153]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2605e-03.\n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------\n",
      "loss: 0.203712  [    0/ 3200]\n",
      "loss: 0.385945  [   16/ 3200]\n",
      "loss: 0.262137  [   32/ 3200]\n",
      "loss: 0.718999  [   48/ 3200]\n",
      "loss: 0.724516  [   64/ 3200]\n",
      "loss: 0.356016  [   80/ 3200]\n",
      "loss: 0.285232  [   96/ 3200]\n",
      "loss: 0.343453  [  112/ 3200]\n",
      "loss: 0.114812  [  128/ 3200]\n",
      "loss: 0.289881  [  144/ 3200]\n",
      "loss: 0.362590  [  160/ 3200]\n",
      "loss: 0.296671  [  176/ 3200]\n",
      "loss: 0.363810  [  192/ 3200]\n",
      "loss: 0.392210  [  208/ 3200]\n",
      "loss: 0.586676  [  224/ 3200]\n",
      "loss: 0.312938  [  240/ 3200]\n",
      "loss: 0.503688  [  256/ 3200]\n",
      "loss: 0.650621  [  272/ 3200]\n",
      "loss: 0.342451  [  288/ 3200]\n",
      "loss: 0.421243  [  304/ 3200]\n",
      "loss: 0.259828  [  320/ 3200]\n",
      "loss: 0.264427  [  336/ 3200]\n",
      "loss: 0.648197  [  352/ 3200]\n",
      "loss: 0.135229  [  368/ 3200]\n",
      "loss: 0.177174  [  384/ 3200]\n",
      "loss: 0.224861  [  400/ 3200]\n",
      "loss: 0.226837  [  416/ 3200]\n",
      "loss: 0.367800  [  432/ 3200]\n",
      "loss: 0.267651  [  448/ 3200]\n",
      "loss: 0.340272  [  464/ 3200]\n",
      "loss: 0.269045  [  480/ 3200]\n",
      "loss: 0.455472  [  496/ 3200]\n",
      "loss: 0.235798  [  512/ 3200]\n",
      "loss: 0.571513  [  528/ 3200]\n",
      "loss: 0.331494  [  544/ 3200]\n",
      "loss: 0.230178  [  560/ 3200]\n",
      "loss: 0.512954  [  576/ 3200]\n",
      "loss: 0.188837  [  592/ 3200]\n",
      "loss: 0.409945  [  608/ 3200]\n",
      "loss: 0.132696  [  624/ 3200]\n",
      "loss: 0.219206  [  640/ 3200]\n",
      "loss: 0.282786  [  656/ 3200]\n",
      "loss: 0.248917  [  672/ 3200]\n",
      "loss: 0.239604  [  688/ 3200]\n",
      "loss: 0.538977  [  704/ 3200]\n",
      "loss: 0.314653  [  720/ 3200]\n",
      "loss: 0.328241  [  736/ 3200]\n",
      "loss: 0.337935  [  752/ 3200]\n",
      "loss: 0.783186  [  768/ 3200]\n",
      "loss: 0.253909  [  784/ 3200]\n",
      "loss: 0.569608  [  800/ 3200]\n",
      "loss: 0.731610  [  816/ 3200]\n",
      "loss: 0.413470  [  832/ 3200]\n",
      "loss: 0.402797  [  848/ 3200]\n",
      "loss: 0.326947  [  864/ 3200]\n",
      "loss: 0.490822  [  880/ 3200]\n",
      "loss: 0.243494  [  896/ 3200]\n",
      "loss: 0.389800  [  912/ 3200]\n",
      "loss: 0.168487  [  928/ 3200]\n",
      "loss: 0.269048  [  944/ 3200]\n",
      "loss: 0.313411  [  960/ 3200]\n",
      "loss: 0.132364  [  976/ 3200]\n",
      "loss: 0.780838  [  992/ 3200]\n",
      "loss: 0.192237  [ 1008/ 3200]\n",
      "loss: 0.151411  [ 1024/ 3200]\n",
      "loss: 0.446433  [ 1040/ 3200]\n",
      "loss: 0.112176  [ 1056/ 3200]\n",
      "loss: 0.354076  [ 1072/ 3200]\n",
      "loss: 0.768246  [ 1088/ 3200]\n",
      "loss: 0.166168  [ 1104/ 3200]\n",
      "loss: 0.322386  [ 1120/ 3200]\n",
      "loss: 0.294841  [ 1136/ 3200]\n",
      "loss: 0.318419  [ 1152/ 3200]\n",
      "loss: 0.307906  [ 1168/ 3200]\n",
      "loss: 0.338891  [ 1184/ 3200]\n",
      "loss: 0.219749  [ 1200/ 3200]\n",
      "loss: 0.268769  [ 1216/ 3200]\n",
      "loss: 0.335338  [ 1232/ 3200]\n",
      "loss: 0.391461  [ 1248/ 3200]\n",
      "loss: 0.353457  [ 1264/ 3200]\n",
      "loss: 0.308538  [ 1280/ 3200]\n",
      "loss: 0.228824  [ 1296/ 3200]\n",
      "loss: 0.262807  [ 1312/ 3200]\n",
      "loss: 0.116429  [ 1328/ 3200]\n",
      "loss: 0.216680  [ 1344/ 3200]\n",
      "loss: 0.263761  [ 1360/ 3200]\n",
      "loss: 0.133882  [ 1376/ 3200]\n",
      "loss: 0.288106  [ 1392/ 3200]\n",
      "loss: 0.233005  [ 1408/ 3200]\n",
      "loss: 0.231954  [ 1424/ 3200]\n",
      "loss: 0.100614  [ 1440/ 3200]\n",
      "loss: 0.241394  [ 1456/ 3200]\n",
      "loss: 0.262840  [ 1472/ 3200]\n",
      "loss: 0.442931  [ 1488/ 3200]\n",
      "loss: 0.089099  [ 1504/ 3200]\n",
      "loss: 0.221544  [ 1520/ 3200]\n",
      "loss: 0.479383  [ 1536/ 3200]\n",
      "loss: 0.415514  [ 1552/ 3200]\n",
      "loss: 0.417669  [ 1568/ 3200]\n",
      "loss: 0.251354  [ 1584/ 3200]\n",
      "loss: 0.294525  [ 1600/ 3200]\n",
      "loss: 0.281962  [ 1616/ 3200]\n",
      "loss: 0.272947  [ 1632/ 3200]\n",
      "loss: 0.262082  [ 1648/ 3200]\n",
      "loss: 0.295932  [ 1664/ 3200]\n",
      "loss: 0.798214  [ 1680/ 3200]\n",
      "loss: 0.231561  [ 1696/ 3200]\n",
      "loss: 0.206812  [ 1712/ 3200]\n",
      "loss: 0.331386  [ 1728/ 3200]\n",
      "loss: 0.747768  [ 1744/ 3200]\n",
      "loss: 0.356599  [ 1760/ 3200]\n",
      "loss: 0.653604  [ 1776/ 3200]\n",
      "loss: 0.109598  [ 1792/ 3200]\n",
      "loss: 0.510981  [ 1808/ 3200]\n",
      "loss: 0.195688  [ 1824/ 3200]\n",
      "loss: 0.249635  [ 1840/ 3200]\n",
      "loss: 0.165838  [ 1856/ 3200]\n",
      "loss: 0.255558  [ 1872/ 3200]\n",
      "loss: 0.165842  [ 1888/ 3200]\n",
      "loss: 0.436343  [ 1904/ 3200]\n",
      "loss: 0.393909  [ 1920/ 3200]\n",
      "loss: 0.486901  [ 1936/ 3200]\n",
      "loss: 0.345171  [ 1952/ 3200]\n",
      "loss: 0.243571  [ 1968/ 3200]\n",
      "loss: 0.596776  [ 1984/ 3200]\n",
      "loss: 0.138746  [ 2000/ 3200]\n",
      "loss: 0.355964  [ 2016/ 3200]\n",
      "loss: 0.274857  [ 2032/ 3200]\n",
      "loss: 0.193502  [ 2048/ 3200]\n",
      "loss: 0.105783  [ 2064/ 3200]\n",
      "loss: 0.572780  [ 2080/ 3200]\n",
      "loss: 0.125543  [ 2096/ 3200]\n",
      "loss: 0.323830  [ 2112/ 3200]\n",
      "loss: 0.380390  [ 2128/ 3200]\n",
      "loss: 0.192141  [ 2144/ 3200]\n",
      "loss: 0.351605  [ 2160/ 3200]\n",
      "loss: 0.224085  [ 2176/ 3200]\n",
      "loss: 0.143094  [ 2192/ 3200]\n",
      "loss: 0.235737  [ 2208/ 3200]\n",
      "loss: 0.507981  [ 2224/ 3200]\n",
      "loss: 0.239864  [ 2240/ 3200]\n",
      "loss: 0.654792  [ 2256/ 3200]\n",
      "loss: 0.078087  [ 2272/ 3200]\n",
      "loss: 0.453706  [ 2288/ 3200]\n",
      "loss: 0.628836  [ 2304/ 3200]\n",
      "loss: 0.446259  [ 2320/ 3200]\n",
      "loss: 0.323662  [ 2336/ 3200]\n",
      "loss: 0.337856  [ 2352/ 3200]\n",
      "loss: 0.150722  [ 2368/ 3200]\n",
      "loss: 0.107831  [ 2384/ 3200]\n",
      "loss: 0.261651  [ 2400/ 3200]\n",
      "loss: 0.167930  [ 2416/ 3200]\n",
      "loss: 0.190251  [ 2432/ 3200]\n",
      "loss: 0.556721  [ 2448/ 3200]\n",
      "loss: 0.599583  [ 2464/ 3200]\n",
      "loss: 0.530440  [ 2480/ 3200]\n",
      "loss: 0.319999  [ 2496/ 3200]\n",
      "loss: 0.386716  [ 2512/ 3200]\n",
      "loss: 0.233125  [ 2528/ 3200]\n",
      "loss: 0.290051  [ 2544/ 3200]\n",
      "loss: 0.551257  [ 2560/ 3200]\n",
      "loss: 0.234150  [ 2576/ 3200]\n",
      "loss: 0.160528  [ 2592/ 3200]\n",
      "loss: 0.314533  [ 2608/ 3200]\n",
      "loss: 0.288093  [ 2624/ 3200]\n",
      "loss: 0.322451  [ 2640/ 3200]\n",
      "loss: 0.265477  [ 2656/ 3200]\n",
      "loss: 0.328818  [ 2672/ 3200]\n",
      "loss: 0.495591  [ 2688/ 3200]\n",
      "loss: 0.174187  [ 2704/ 3200]\n",
      "loss: 0.180742  [ 2720/ 3200]\n",
      "loss: 0.570772  [ 2736/ 3200]\n",
      "loss: 0.272166  [ 2752/ 3200]\n",
      "loss: 0.310383  [ 2768/ 3200]\n",
      "loss: 0.472899  [ 2784/ 3200]\n",
      "loss: 0.139724  [ 2800/ 3200]\n",
      "loss: 0.394377  [ 2816/ 3200]\n",
      "loss: 0.273472  [ 2832/ 3200]\n",
      "loss: 0.098547  [ 2848/ 3200]\n",
      "loss: 0.218433  [ 2864/ 3200]\n",
      "loss: 0.384222  [ 2880/ 3200]\n",
      "loss: 0.235162  [ 2896/ 3200]\n",
      "loss: 0.278554  [ 2912/ 3200]\n",
      "loss: 0.419156  [ 2928/ 3200]\n",
      "loss: 0.284094  [ 2944/ 3200]\n",
      "loss: 0.327992  [ 2960/ 3200]\n",
      "loss: 0.202954  [ 2976/ 3200]\n",
      "loss: 0.284330  [ 2992/ 3200]\n",
      "loss: 0.257553  [ 3008/ 3200]\n",
      "loss: 0.180356  [ 3024/ 3200]\n",
      "loss: 0.354586  [ 3040/ 3200]\n",
      "loss: 0.169909  [ 3056/ 3200]\n",
      "loss: 0.123787  [ 3072/ 3200]\n",
      "loss: 0.651018  [ 3088/ 3200]\n",
      "loss: 0.541827  [ 3104/ 3200]\n",
      "loss: 0.709919  [ 3120/ 3200]\n",
      "loss: 0.149018  [ 3136/ 3200]\n",
      "loss: 0.248007  [ 3152/ 3200]\n",
      "loss: 0.156951  [ 3168/ 3200]\n",
      "loss: 0.537103  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036523\n",
      "f1 macro averaged score: 0.766849\n",
      "Accuracy               : 76.4%\n",
      "Confusion matrix       :\n",
      "tensor([[168,  26,   0,   6],\n",
      "        [ 13, 135,  20,  32],\n",
      "        [  0,  30, 157,  13],\n",
      "        [  4,  32,  13, 151]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1975e-03.\n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------\n",
      "loss: 0.270436  [    0/ 3200]\n",
      "loss: 0.464376  [   16/ 3200]\n",
      "loss: 0.177800  [   32/ 3200]\n",
      "loss: 0.596098  [   48/ 3200]\n",
      "loss: 0.577002  [   64/ 3200]\n",
      "loss: 0.388583  [   80/ 3200]\n",
      "loss: 0.279732  [   96/ 3200]\n",
      "loss: 0.308173  [  112/ 3200]\n",
      "loss: 0.202358  [  128/ 3200]\n",
      "loss: 0.209824  [  144/ 3200]\n",
      "loss: 0.277680  [  160/ 3200]\n",
      "loss: 0.199280  [  176/ 3200]\n",
      "loss: 0.326788  [  192/ 3200]\n",
      "loss: 0.144668  [  208/ 3200]\n",
      "loss: 0.337496  [  224/ 3200]\n",
      "loss: 0.241382  [  240/ 3200]\n",
      "loss: 0.119472  [  256/ 3200]\n",
      "loss: 0.262537  [  272/ 3200]\n",
      "loss: 0.200553  [  288/ 3200]\n",
      "loss: 0.580544  [  304/ 3200]\n",
      "loss: 0.759209  [  320/ 3200]\n",
      "loss: 0.656016  [  336/ 3200]\n",
      "loss: 0.294758  [  352/ 3200]\n",
      "loss: 0.503421  [  368/ 3200]\n",
      "loss: 0.334731  [  384/ 3200]\n",
      "loss: 0.134068  [  400/ 3200]\n",
      "loss: 0.582196  [  416/ 3200]\n",
      "loss: 0.153046  [  432/ 3200]\n",
      "loss: 0.405330  [  448/ 3200]\n",
      "loss: 0.377811  [  464/ 3200]\n",
      "loss: 0.082915  [  480/ 3200]\n",
      "loss: 0.251085  [  496/ 3200]\n",
      "loss: 0.532873  [  512/ 3200]\n",
      "loss: 0.348256  [  528/ 3200]\n",
      "loss: 0.294128  [  544/ 3200]\n",
      "loss: 0.238935  [  560/ 3200]\n",
      "loss: 0.083418  [  576/ 3200]\n",
      "loss: 0.160025  [  592/ 3200]\n",
      "loss: 0.186481  [  608/ 3200]\n",
      "loss: 0.496152  [  624/ 3200]\n",
      "loss: 0.651675  [  640/ 3200]\n",
      "loss: 0.546387  [  656/ 3200]\n",
      "loss: 0.311190  [  672/ 3200]\n",
      "loss: 0.390958  [  688/ 3200]\n",
      "loss: 0.262503  [  704/ 3200]\n",
      "loss: 0.472318  [  720/ 3200]\n",
      "loss: 0.437263  [  736/ 3200]\n",
      "loss: 0.283581  [  752/ 3200]\n",
      "loss: 0.124060  [  768/ 3200]\n",
      "loss: 0.117293  [  784/ 3200]\n",
      "loss: 0.232129  [  800/ 3200]\n",
      "loss: 0.417389  [  816/ 3200]\n",
      "loss: 0.397077  [  832/ 3200]\n",
      "loss: 0.378643  [  848/ 3200]\n",
      "loss: 0.343565  [  864/ 3200]\n",
      "loss: 0.325462  [  880/ 3200]\n",
      "loss: 0.206310  [  896/ 3200]\n",
      "loss: 0.442909  [  912/ 3200]\n",
      "loss: 0.219902  [  928/ 3200]\n",
      "loss: 0.271245  [  944/ 3200]\n",
      "loss: 0.194915  [  960/ 3200]\n",
      "loss: 0.207618  [  976/ 3200]\n",
      "loss: 0.232732  [  992/ 3200]\n",
      "loss: 0.186488  [ 1008/ 3200]\n",
      "loss: 0.163102  [ 1024/ 3200]\n",
      "loss: 0.380033  [ 1040/ 3200]\n",
      "loss: 0.173759  [ 1056/ 3200]\n",
      "loss: 0.288206  [ 1072/ 3200]\n",
      "loss: 0.150186  [ 1088/ 3200]\n",
      "loss: 0.462761  [ 1104/ 3200]\n",
      "loss: 0.520452  [ 1120/ 3200]\n",
      "loss: 0.063937  [ 1136/ 3200]\n",
      "loss: 0.387499  [ 1152/ 3200]\n",
      "loss: 0.183091  [ 1168/ 3200]\n",
      "loss: 0.205494  [ 1184/ 3200]\n",
      "loss: 0.470957  [ 1200/ 3200]\n",
      "loss: 0.185332  [ 1216/ 3200]\n",
      "loss: 0.172361  [ 1232/ 3200]\n",
      "loss: 0.190972  [ 1248/ 3200]\n",
      "loss: 0.251904  [ 1264/ 3200]\n",
      "loss: 0.149068  [ 1280/ 3200]\n",
      "loss: 0.146923  [ 1296/ 3200]\n",
      "loss: 0.182461  [ 1312/ 3200]\n",
      "loss: 0.258231  [ 1328/ 3200]\n",
      "loss: 0.081213  [ 1344/ 3200]\n",
      "loss: 0.543778  [ 1360/ 3200]\n",
      "loss: 0.461811  [ 1376/ 3200]\n",
      "loss: 0.482673  [ 1392/ 3200]\n",
      "loss: 0.494966  [ 1408/ 3200]\n",
      "loss: 0.221943  [ 1424/ 3200]\n",
      "loss: 0.264705  [ 1440/ 3200]\n",
      "loss: 0.539480  [ 1456/ 3200]\n",
      "loss: 0.139867  [ 1472/ 3200]\n",
      "loss: 0.192246  [ 1488/ 3200]\n",
      "loss: 0.213007  [ 1504/ 3200]\n",
      "loss: 0.204258  [ 1520/ 3200]\n",
      "loss: 0.370746  [ 1536/ 3200]\n",
      "loss: 0.131759  [ 1552/ 3200]\n",
      "loss: 0.182011  [ 1568/ 3200]\n",
      "loss: 0.178534  [ 1584/ 3200]\n",
      "loss: 0.454388  [ 1600/ 3200]\n",
      "loss: 0.729689  [ 1616/ 3200]\n",
      "loss: 0.317375  [ 1632/ 3200]\n",
      "loss: 0.173285  [ 1648/ 3200]\n",
      "loss: 0.414695  [ 1664/ 3200]\n",
      "loss: 0.260634  [ 1680/ 3200]\n",
      "loss: 0.309036  [ 1696/ 3200]\n",
      "loss: 0.199713  [ 1712/ 3200]\n",
      "loss: 0.498095  [ 1728/ 3200]\n",
      "loss: 0.447840  [ 1744/ 3200]\n",
      "loss: 0.221805  [ 1760/ 3200]\n",
      "loss: 0.505027  [ 1776/ 3200]\n",
      "loss: 0.123123  [ 1792/ 3200]\n",
      "loss: 0.155843  [ 1808/ 3200]\n",
      "loss: 0.160098  [ 1824/ 3200]\n",
      "loss: 0.418221  [ 1840/ 3200]\n",
      "loss: 0.059893  [ 1856/ 3200]\n",
      "loss: 0.185514  [ 1872/ 3200]\n",
      "loss: 0.349399  [ 1888/ 3200]\n",
      "loss: 0.658201  [ 1904/ 3200]\n",
      "loss: 0.261735  [ 1920/ 3200]\n",
      "loss: 0.410744  [ 1936/ 3200]\n",
      "loss: 0.296685  [ 1952/ 3200]\n",
      "loss: 0.074760  [ 1968/ 3200]\n",
      "loss: 0.109529  [ 1984/ 3200]\n",
      "loss: 0.568319  [ 2000/ 3200]\n",
      "loss: 0.166907  [ 2016/ 3200]\n",
      "loss: 0.243641  [ 2032/ 3200]\n",
      "loss: 0.438589  [ 2048/ 3200]\n",
      "loss: 0.608228  [ 2064/ 3200]\n",
      "loss: 0.466866  [ 2080/ 3200]\n",
      "loss: 0.393434  [ 2096/ 3200]\n",
      "loss: 0.427135  [ 2112/ 3200]\n",
      "loss: 0.111194  [ 2128/ 3200]\n",
      "loss: 0.148733  [ 2144/ 3200]\n",
      "loss: 0.660794  [ 2160/ 3200]\n",
      "loss: 0.585366  [ 2176/ 3200]\n",
      "loss: 0.601212  [ 2192/ 3200]\n",
      "loss: 0.269129  [ 2208/ 3200]\n",
      "loss: 0.164835  [ 2224/ 3200]\n",
      "loss: 0.188924  [ 2240/ 3200]\n",
      "loss: 0.268724  [ 2256/ 3200]\n",
      "loss: 0.750477  [ 2272/ 3200]\n",
      "loss: 0.133097  [ 2288/ 3200]\n",
      "loss: 0.185729  [ 2304/ 3200]\n",
      "loss: 0.138671  [ 2320/ 3200]\n",
      "loss: 0.293807  [ 2336/ 3200]\n",
      "loss: 0.362733  [ 2352/ 3200]\n",
      "loss: 0.205686  [ 2368/ 3200]\n",
      "loss: 0.292444  [ 2384/ 3200]\n",
      "loss: 0.303617  [ 2400/ 3200]\n",
      "loss: 0.408345  [ 2416/ 3200]\n",
      "loss: 0.476738  [ 2432/ 3200]\n",
      "loss: 0.192315  [ 2448/ 3200]\n",
      "loss: 0.355217  [ 2464/ 3200]\n",
      "loss: 0.092211  [ 2480/ 3200]\n",
      "loss: 0.256421  [ 2496/ 3200]\n",
      "loss: 0.303400  [ 2512/ 3200]\n",
      "loss: 0.172576  [ 2528/ 3200]\n",
      "loss: 0.104656  [ 2544/ 3200]\n",
      "loss: 0.175384  [ 2560/ 3200]\n",
      "loss: 0.356810  [ 2576/ 3200]\n",
      "loss: 0.338771  [ 2592/ 3200]\n",
      "loss: 0.253089  [ 2608/ 3200]\n",
      "loss: 0.382239  [ 2624/ 3200]\n",
      "loss: 0.260120  [ 2640/ 3200]\n",
      "loss: 0.445287  [ 2656/ 3200]\n",
      "loss: 0.637555  [ 2672/ 3200]\n",
      "loss: 0.250083  [ 2688/ 3200]\n",
      "loss: 0.272296  [ 2704/ 3200]\n",
      "loss: 0.089367  [ 2720/ 3200]\n",
      "loss: 0.341647  [ 2736/ 3200]\n",
      "loss: 0.329044  [ 2752/ 3200]\n",
      "loss: 0.284158  [ 2768/ 3200]\n",
      "loss: 0.125914  [ 2784/ 3200]\n",
      "loss: 0.222477  [ 2800/ 3200]\n",
      "loss: 0.181795  [ 2816/ 3200]\n",
      "loss: 0.336106  [ 2832/ 3200]\n",
      "loss: 0.275548  [ 2848/ 3200]\n",
      "loss: 0.278083  [ 2864/ 3200]\n",
      "loss: 0.333753  [ 2880/ 3200]\n",
      "loss: 0.190571  [ 2896/ 3200]\n",
      "loss: 0.151380  [ 2912/ 3200]\n",
      "loss: 0.240433  [ 2928/ 3200]\n",
      "loss: 0.131715  [ 2944/ 3200]\n",
      "loss: 0.406410  [ 2960/ 3200]\n",
      "loss: 0.368490  [ 2976/ 3200]\n",
      "loss: 0.186092  [ 2992/ 3200]\n",
      "loss: 0.212855  [ 3008/ 3200]\n",
      "loss: 0.244037  [ 3024/ 3200]\n",
      "loss: 0.073043  [ 3040/ 3200]\n",
      "loss: 0.165134  [ 3056/ 3200]\n",
      "loss: 0.470279  [ 3072/ 3200]\n",
      "loss: 0.369302  [ 3088/ 3200]\n",
      "loss: 0.417276  [ 3104/ 3200]\n",
      "loss: 0.291924  [ 3120/ 3200]\n",
      "loss: 0.211595  [ 3136/ 3200]\n",
      "loss: 0.531548  [ 3152/ 3200]\n",
      "loss: 0.440149  [ 3168/ 3200]\n",
      "loss: 0.359238  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.036838\n",
      "f1 macro averaged score: 0.772355\n",
      "Accuracy               : 77.9%\n",
      "Confusion matrix       :\n",
      "tensor([[187,   4,   0,   9],\n",
      "        [ 19, 109,  30,  42],\n",
      "        [  0,  19, 170,  11],\n",
      "        [ 12,  13,  18, 157]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.1376e-03.\n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------\n",
      "loss: 0.313748  [    0/ 3200]\n",
      "loss: 0.426986  [   16/ 3200]\n",
      "loss: 0.318688  [   32/ 3200]\n",
      "loss: 0.112699  [   48/ 3200]\n",
      "loss: 0.215099  [   64/ 3200]\n",
      "loss: 0.627576  [   80/ 3200]\n",
      "loss: 0.341741  [   96/ 3200]\n",
      "loss: 0.257935  [  112/ 3200]\n",
      "loss: 0.239829  [  128/ 3200]\n",
      "loss: 0.369401  [  144/ 3200]\n",
      "loss: 0.499453  [  160/ 3200]\n",
      "loss: 0.103706  [  176/ 3200]\n",
      "loss: 0.261096  [  192/ 3200]\n",
      "loss: 0.448281  [  208/ 3200]\n",
      "loss: 0.116089  [  224/ 3200]\n",
      "loss: 0.274494  [  240/ 3200]\n",
      "loss: 0.582244  [  256/ 3200]\n",
      "loss: 0.383258  [  272/ 3200]\n",
      "loss: 0.368534  [  288/ 3200]\n",
      "loss: 0.376586  [  304/ 3200]\n",
      "loss: 0.464256  [  320/ 3200]\n",
      "loss: 0.234293  [  336/ 3200]\n",
      "loss: 0.199116  [  352/ 3200]\n",
      "loss: 0.309075  [  368/ 3200]\n",
      "loss: 0.155809  [  384/ 3200]\n",
      "loss: 0.184812  [  400/ 3200]\n",
      "loss: 0.128203  [  416/ 3200]\n",
      "loss: 0.113154  [  432/ 3200]\n",
      "loss: 0.142639  [  448/ 3200]\n",
      "loss: 0.101735  [  464/ 3200]\n",
      "loss: 0.957702  [  480/ 3200]\n",
      "loss: 0.153090  [  496/ 3200]\n",
      "loss: 0.545822  [  512/ 3200]\n",
      "loss: 0.135495  [  528/ 3200]\n",
      "loss: 0.147856  [  544/ 3200]\n",
      "loss: 0.279857  [  560/ 3200]\n",
      "loss: 0.293464  [  576/ 3200]\n",
      "loss: 0.256385  [  592/ 3200]\n",
      "loss: 0.200992  [  608/ 3200]\n",
      "loss: 0.446825  [  624/ 3200]\n",
      "loss: 0.091743  [  640/ 3200]\n",
      "loss: 0.523086  [  656/ 3200]\n",
      "loss: 0.290017  [  672/ 3200]\n",
      "loss: 0.323369  [  688/ 3200]\n",
      "loss: 0.119852  [  704/ 3200]\n",
      "loss: 0.281402  [  720/ 3200]\n",
      "loss: 0.343548  [  736/ 3200]\n",
      "loss: 0.353149  [  752/ 3200]\n",
      "loss: 0.303647  [  768/ 3200]\n",
      "loss: 0.369780  [  784/ 3200]\n",
      "loss: 0.137268  [  800/ 3200]\n",
      "loss: 0.257065  [  816/ 3200]\n",
      "loss: 0.224571  [  832/ 3200]\n",
      "loss: 0.120254  [  848/ 3200]\n",
      "loss: 0.519958  [  864/ 3200]\n",
      "loss: 0.509760  [  880/ 3200]\n",
      "loss: 0.140171  [  896/ 3200]\n",
      "loss: 0.171689  [  912/ 3200]\n",
      "loss: 0.202178  [  928/ 3200]\n",
      "loss: 0.225967  [  944/ 3200]\n",
      "loss: 0.280755  [  960/ 3200]\n",
      "loss: 0.254559  [  976/ 3200]\n",
      "loss: 0.467429  [  992/ 3200]\n",
      "loss: 0.538985  [ 1008/ 3200]\n",
      "loss: 0.366016  [ 1024/ 3200]\n",
      "loss: 0.161471  [ 1040/ 3200]\n",
      "loss: 0.258284  [ 1056/ 3200]\n",
      "loss: 0.217126  [ 1072/ 3200]\n",
      "loss: 0.238544  [ 1088/ 3200]\n",
      "loss: 0.566676  [ 1104/ 3200]\n",
      "loss: 0.119846  [ 1120/ 3200]\n",
      "loss: 0.245436  [ 1136/ 3200]\n",
      "loss: 0.213092  [ 1152/ 3200]\n",
      "loss: 0.159784  [ 1168/ 3200]\n",
      "loss: 0.441292  [ 1184/ 3200]\n",
      "loss: 0.243502  [ 1200/ 3200]\n",
      "loss: 0.561918  [ 1216/ 3200]\n",
      "loss: 0.188416  [ 1232/ 3200]\n",
      "loss: 0.203781  [ 1248/ 3200]\n",
      "loss: 0.178035  [ 1264/ 3200]\n",
      "loss: 0.197493  [ 1280/ 3200]\n",
      "loss: 0.075947  [ 1296/ 3200]\n",
      "loss: 0.347463  [ 1312/ 3200]\n",
      "loss: 0.123905  [ 1328/ 3200]\n",
      "loss: 0.215225  [ 1344/ 3200]\n",
      "loss: 0.079312  [ 1360/ 3200]\n",
      "loss: 0.283828  [ 1376/ 3200]\n",
      "loss: 0.356691  [ 1392/ 3200]\n",
      "loss: 0.333625  [ 1408/ 3200]\n",
      "loss: 0.217877  [ 1424/ 3200]\n",
      "loss: 0.289057  [ 1440/ 3200]\n",
      "loss: 0.231041  [ 1456/ 3200]\n",
      "loss: 0.163996  [ 1472/ 3200]\n",
      "loss: 0.332166  [ 1488/ 3200]\n",
      "loss: 0.499911  [ 1504/ 3200]\n",
      "loss: 0.316734  [ 1520/ 3200]\n",
      "loss: 0.336809  [ 1536/ 3200]\n",
      "loss: 0.177663  [ 1552/ 3200]\n",
      "loss: 0.588999  [ 1568/ 3200]\n",
      "loss: 0.199533  [ 1584/ 3200]\n",
      "loss: 0.426083  [ 1600/ 3200]\n",
      "loss: 0.224433  [ 1616/ 3200]\n",
      "loss: 0.329875  [ 1632/ 3200]\n",
      "loss: 0.331322  [ 1648/ 3200]\n",
      "loss: 0.189873  [ 1664/ 3200]\n",
      "loss: 0.445882  [ 1680/ 3200]\n",
      "loss: 0.426814  [ 1696/ 3200]\n",
      "loss: 0.349216  [ 1712/ 3200]\n",
      "loss: 0.400940  [ 1728/ 3200]\n",
      "loss: 0.098596  [ 1744/ 3200]\n",
      "loss: 0.155765  [ 1760/ 3200]\n",
      "loss: 0.087864  [ 1776/ 3200]\n",
      "loss: 0.152018  [ 1792/ 3200]\n",
      "loss: 0.227001  [ 1808/ 3200]\n",
      "loss: 0.233875  [ 1824/ 3200]\n",
      "loss: 0.143808  [ 1840/ 3200]\n",
      "loss: 0.385285  [ 1856/ 3200]\n",
      "loss: 0.079585  [ 1872/ 3200]\n",
      "loss: 0.264320  [ 1888/ 3200]\n",
      "loss: 0.287161  [ 1904/ 3200]\n",
      "loss: 0.181125  [ 1920/ 3200]\n",
      "loss: 0.149938  [ 1936/ 3200]\n",
      "loss: 0.240814  [ 1952/ 3200]\n",
      "loss: 0.242526  [ 1968/ 3200]\n",
      "loss: 0.226729  [ 1984/ 3200]\n",
      "loss: 0.170676  [ 2000/ 3200]\n",
      "loss: 0.269929  [ 2016/ 3200]\n",
      "loss: 0.223103  [ 2032/ 3200]\n",
      "loss: 0.326446  [ 2048/ 3200]\n",
      "loss: 0.228328  [ 2064/ 3200]\n",
      "loss: 0.196336  [ 2080/ 3200]\n",
      "loss: 0.202180  [ 2096/ 3200]\n",
      "loss: 0.191670  [ 2112/ 3200]\n",
      "loss: 0.410093  [ 2128/ 3200]\n",
      "loss: 0.446607  [ 2144/ 3200]\n",
      "loss: 0.171633  [ 2160/ 3200]\n",
      "loss: 0.143630  [ 2176/ 3200]\n",
      "loss: 0.254844  [ 2192/ 3200]\n",
      "loss: 0.137248  [ 2208/ 3200]\n",
      "loss: 0.189900  [ 2224/ 3200]\n",
      "loss: 0.414072  [ 2240/ 3200]\n",
      "loss: 0.453467  [ 2256/ 3200]\n",
      "loss: 0.218032  [ 2272/ 3200]\n",
      "loss: 0.105886  [ 2288/ 3200]\n",
      "loss: 0.592765  [ 2304/ 3200]\n",
      "loss: 0.257284  [ 2320/ 3200]\n",
      "loss: 0.261655  [ 2336/ 3200]\n",
      "loss: 0.064610  [ 2352/ 3200]\n",
      "loss: 0.270386  [ 2368/ 3200]\n",
      "loss: 0.208206  [ 2384/ 3200]\n",
      "loss: 0.335977  [ 2400/ 3200]\n",
      "loss: 0.271290  [ 2416/ 3200]\n",
      "loss: 0.267251  [ 2432/ 3200]\n",
      "loss: 0.561330  [ 2448/ 3200]\n",
      "loss: 0.068016  [ 2464/ 3200]\n",
      "loss: 0.135155  [ 2480/ 3200]\n",
      "loss: 0.338280  [ 2496/ 3200]\n",
      "loss: 0.237481  [ 2512/ 3200]\n",
      "loss: 0.127562  [ 2528/ 3200]\n",
      "loss: 0.279904  [ 2544/ 3200]\n",
      "loss: 0.148699  [ 2560/ 3200]\n",
      "loss: 0.223174  [ 2576/ 3200]\n",
      "loss: 0.186429  [ 2592/ 3200]\n",
      "loss: 0.308302  [ 2608/ 3200]\n",
      "loss: 0.125950  [ 2624/ 3200]\n",
      "loss: 0.108757  [ 2640/ 3200]\n",
      "loss: 0.362332  [ 2656/ 3200]\n",
      "loss: 0.129484  [ 2672/ 3200]\n",
      "loss: 0.228180  [ 2688/ 3200]\n",
      "loss: 0.555403  [ 2704/ 3200]\n",
      "loss: 0.244616  [ 2720/ 3200]\n",
      "loss: 0.051304  [ 2736/ 3200]\n",
      "loss: 0.335027  [ 2752/ 3200]\n",
      "loss: 0.043129  [ 2768/ 3200]\n",
      "loss: 0.160469  [ 2784/ 3200]\n",
      "loss: 0.257167  [ 2800/ 3200]\n",
      "loss: 0.380773  [ 2816/ 3200]\n",
      "loss: 0.235504  [ 2832/ 3200]\n",
      "loss: 0.452867  [ 2848/ 3200]\n",
      "loss: 0.319216  [ 2864/ 3200]\n",
      "loss: 0.255038  [ 2880/ 3200]\n",
      "loss: 0.164963  [ 2896/ 3200]\n",
      "loss: 0.087276  [ 2912/ 3200]\n",
      "loss: 0.426077  [ 2928/ 3200]\n",
      "loss: 0.342664  [ 2944/ 3200]\n",
      "loss: 0.421864  [ 2960/ 3200]\n",
      "loss: 0.259598  [ 2976/ 3200]\n",
      "loss: 0.281228  [ 2992/ 3200]\n",
      "loss: 0.416771  [ 3008/ 3200]\n",
      "loss: 0.277337  [ 3024/ 3200]\n",
      "loss: 0.277589  [ 3040/ 3200]\n",
      "loss: 0.201618  [ 3056/ 3200]\n",
      "loss: 0.205109  [ 3072/ 3200]\n",
      "loss: 0.288025  [ 3088/ 3200]\n",
      "loss: 0.323940  [ 3104/ 3200]\n",
      "loss: 0.359645  [ 3120/ 3200]\n",
      "loss: 0.307152  [ 3136/ 3200]\n",
      "loss: 0.112883  [ 3152/ 3200]\n",
      "loss: 0.246852  [ 3168/ 3200]\n",
      "loss: 0.227350  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038606\n",
      "f1 macro averaged score: 0.758923\n",
      "Accuracy               : 76.6%\n",
      "Confusion matrix       :\n",
      "tensor([[187,   4,   0,   9],\n",
      "        [ 26, 102,  22,  50],\n",
      "        [  0,  21, 159,  20],\n",
      "        [ 12,  10,  13, 165]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0807e-03.\n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------\n",
      "loss: 0.236430  [    0/ 3200]\n",
      "loss: 0.219933  [   16/ 3200]\n",
      "loss: 0.086759  [   32/ 3200]\n",
      "loss: 0.260654  [   48/ 3200]\n",
      "loss: 0.295449  [   64/ 3200]\n",
      "loss: 0.198143  [   80/ 3200]\n",
      "loss: 0.275878  [   96/ 3200]\n",
      "loss: 0.224335  [  112/ 3200]\n",
      "loss: 0.460235  [  128/ 3200]\n",
      "loss: 0.175690  [  144/ 3200]\n",
      "loss: 0.244863  [  160/ 3200]\n",
      "loss: 0.349497  [  176/ 3200]\n",
      "loss: 0.175956  [  192/ 3200]\n",
      "loss: 0.201487  [  208/ 3200]\n",
      "loss: 0.133733  [  224/ 3200]\n",
      "loss: 0.224651  [  240/ 3200]\n",
      "loss: 0.409599  [  256/ 3200]\n",
      "loss: 0.064376  [  272/ 3200]\n",
      "loss: 0.083429  [  288/ 3200]\n",
      "loss: 0.158239  [  304/ 3200]\n",
      "loss: 0.361063  [  320/ 3200]\n",
      "loss: 0.391424  [  336/ 3200]\n",
      "loss: 0.194129  [  352/ 3200]\n",
      "loss: 0.272088  [  368/ 3200]\n",
      "loss: 0.307061  [  384/ 3200]\n",
      "loss: 0.448997  [  400/ 3200]\n",
      "loss: 0.165953  [  416/ 3200]\n",
      "loss: 0.393923  [  432/ 3200]\n",
      "loss: 0.174249  [  448/ 3200]\n",
      "loss: 0.141328  [  464/ 3200]\n",
      "loss: 0.115318  [  480/ 3200]\n",
      "loss: 0.290658  [  496/ 3200]\n",
      "loss: 0.264897  [  512/ 3200]\n",
      "loss: 0.317068  [  528/ 3200]\n",
      "loss: 0.246739  [  544/ 3200]\n",
      "loss: 0.271678  [  560/ 3200]\n",
      "loss: 0.171716  [  576/ 3200]\n",
      "loss: 0.252413  [  592/ 3200]\n",
      "loss: 0.283484  [  608/ 3200]\n",
      "loss: 0.312994  [  624/ 3200]\n",
      "loss: 0.413542  [  640/ 3200]\n",
      "loss: 0.253373  [  656/ 3200]\n",
      "loss: 0.198061  [  672/ 3200]\n",
      "loss: 0.446602  [  688/ 3200]\n",
      "loss: 0.335507  [  704/ 3200]\n",
      "loss: 0.321141  [  720/ 3200]\n",
      "loss: 0.374251  [  736/ 3200]\n",
      "loss: 0.123629  [  752/ 3200]\n",
      "loss: 0.340299  [  768/ 3200]\n",
      "loss: 0.223095  [  784/ 3200]\n",
      "loss: 0.109520  [  800/ 3200]\n",
      "loss: 0.148275  [  816/ 3200]\n",
      "loss: 0.308530  [  832/ 3200]\n",
      "loss: 0.204628  [  848/ 3200]\n",
      "loss: 0.052566  [  864/ 3200]\n",
      "loss: 0.335784  [  880/ 3200]\n",
      "loss: 0.146941  [  896/ 3200]\n",
      "loss: 0.106315  [  912/ 3200]\n",
      "loss: 0.388775  [  928/ 3200]\n",
      "loss: 0.312914  [  944/ 3200]\n",
      "loss: 0.374767  [  960/ 3200]\n",
      "loss: 0.189036  [  976/ 3200]\n",
      "loss: 0.234694  [  992/ 3200]\n",
      "loss: 0.184502  [ 1008/ 3200]\n",
      "loss: 0.380219  [ 1024/ 3200]\n",
      "loss: 0.200496  [ 1040/ 3200]\n",
      "loss: 0.224717  [ 1056/ 3200]\n",
      "loss: 0.383347  [ 1072/ 3200]\n",
      "loss: 0.393150  [ 1088/ 3200]\n",
      "loss: 0.394006  [ 1104/ 3200]\n",
      "loss: 0.255112  [ 1120/ 3200]\n",
      "loss: 0.184537  [ 1136/ 3200]\n",
      "loss: 0.356103  [ 1152/ 3200]\n",
      "loss: 0.292972  [ 1168/ 3200]\n",
      "loss: 0.307912  [ 1184/ 3200]\n",
      "loss: 0.388711  [ 1200/ 3200]\n",
      "loss: 0.290724  [ 1216/ 3200]\n",
      "loss: 0.213953  [ 1232/ 3200]\n",
      "loss: 0.210714  [ 1248/ 3200]\n",
      "loss: 0.126082  [ 1264/ 3200]\n",
      "loss: 0.340180  [ 1280/ 3200]\n",
      "loss: 0.193833  [ 1296/ 3200]\n",
      "loss: 0.504730  [ 1312/ 3200]\n",
      "loss: 0.226473  [ 1328/ 3200]\n",
      "loss: 0.059778  [ 1344/ 3200]\n",
      "loss: 0.095314  [ 1360/ 3200]\n",
      "loss: 0.256706  [ 1376/ 3200]\n",
      "loss: 0.183707  [ 1392/ 3200]\n",
      "loss: 0.408072  [ 1408/ 3200]\n",
      "loss: 0.266299  [ 1424/ 3200]\n",
      "loss: 0.233361  [ 1440/ 3200]\n",
      "loss: 0.045921  [ 1456/ 3200]\n",
      "loss: 0.091195  [ 1472/ 3200]\n",
      "loss: 0.308910  [ 1488/ 3200]\n",
      "loss: 0.319567  [ 1504/ 3200]\n",
      "loss: 0.207134  [ 1520/ 3200]\n",
      "loss: 0.128790  [ 1536/ 3200]\n",
      "loss: 0.264614  [ 1552/ 3200]\n",
      "loss: 0.648715  [ 1568/ 3200]\n",
      "loss: 0.207976  [ 1584/ 3200]\n",
      "loss: 0.123096  [ 1600/ 3200]\n",
      "loss: 0.337099  [ 1616/ 3200]\n",
      "loss: 0.110942  [ 1632/ 3200]\n",
      "loss: 0.039289  [ 1648/ 3200]\n",
      "loss: 0.130236  [ 1664/ 3200]\n",
      "loss: 0.281140  [ 1680/ 3200]\n",
      "loss: 0.268851  [ 1696/ 3200]\n",
      "loss: 0.051933  [ 1712/ 3200]\n",
      "loss: 0.184523  [ 1728/ 3200]\n",
      "loss: 0.339746  [ 1744/ 3200]\n",
      "loss: 0.169448  [ 1760/ 3200]\n",
      "loss: 0.311141  [ 1776/ 3200]\n",
      "loss: 0.183266  [ 1792/ 3200]\n",
      "loss: 0.146925  [ 1808/ 3200]\n",
      "loss: 0.361202  [ 1824/ 3200]\n",
      "loss: 0.337087  [ 1840/ 3200]\n",
      "loss: 0.256995  [ 1856/ 3200]\n",
      "loss: 0.093656  [ 1872/ 3200]\n",
      "loss: 0.104985  [ 1888/ 3200]\n",
      "loss: 0.117062  [ 1904/ 3200]\n",
      "loss: 0.207812  [ 1920/ 3200]\n",
      "loss: 0.158430  [ 1936/ 3200]\n",
      "loss: 0.175660  [ 1952/ 3200]\n",
      "loss: 0.171581  [ 1968/ 3200]\n",
      "loss: 0.448316  [ 1984/ 3200]\n",
      "loss: 0.282970  [ 2000/ 3200]\n",
      "loss: 0.123597  [ 2016/ 3200]\n",
      "loss: 0.073590  [ 2032/ 3200]\n",
      "loss: 0.090458  [ 2048/ 3200]\n",
      "loss: 0.255833  [ 2064/ 3200]\n",
      "loss: 0.407367  [ 2080/ 3200]\n",
      "loss: 0.788834  [ 2096/ 3200]\n",
      "loss: 0.171949  [ 2112/ 3200]\n",
      "loss: 0.162008  [ 2128/ 3200]\n",
      "loss: 0.070483  [ 2144/ 3200]\n",
      "loss: 0.437554  [ 2160/ 3200]\n",
      "loss: 0.295404  [ 2176/ 3200]\n",
      "loss: 0.175013  [ 2192/ 3200]\n",
      "loss: 0.147978  [ 2208/ 3200]\n",
      "loss: 0.265079  [ 2224/ 3200]\n",
      "loss: 0.423287  [ 2240/ 3200]\n",
      "loss: 0.212110  [ 2256/ 3200]\n",
      "loss: 0.121387  [ 2272/ 3200]\n",
      "loss: 0.102590  [ 2288/ 3200]\n",
      "loss: 0.122789  [ 2304/ 3200]\n",
      "loss: 0.476015  [ 2320/ 3200]\n",
      "loss: 0.159133  [ 2336/ 3200]\n",
      "loss: 0.212505  [ 2352/ 3200]\n",
      "loss: 0.199000  [ 2368/ 3200]\n",
      "loss: 0.423481  [ 2384/ 3200]\n",
      "loss: 0.431068  [ 2400/ 3200]\n",
      "loss: 0.193995  [ 2416/ 3200]\n",
      "loss: 0.214953  [ 2432/ 3200]\n",
      "loss: 0.288943  [ 2448/ 3200]\n",
      "loss: 0.131702  [ 2464/ 3200]\n",
      "loss: 0.393456  [ 2480/ 3200]\n",
      "loss: 0.135360  [ 2496/ 3200]\n",
      "loss: 0.115056  [ 2512/ 3200]\n",
      "loss: 0.072694  [ 2528/ 3200]\n",
      "loss: 0.172041  [ 2544/ 3200]\n",
      "loss: 0.173221  [ 2560/ 3200]\n",
      "loss: 0.139396  [ 2576/ 3200]\n",
      "loss: 0.160079  [ 2592/ 3200]\n",
      "loss: 0.217868  [ 2608/ 3200]\n",
      "loss: 0.406341  [ 2624/ 3200]\n",
      "loss: 0.645603  [ 2640/ 3200]\n",
      "loss: 0.240112  [ 2656/ 3200]\n",
      "loss: 0.220806  [ 2672/ 3200]\n",
      "loss: 0.574745  [ 2688/ 3200]\n",
      "loss: 0.346218  [ 2704/ 3200]\n",
      "loss: 0.215739  [ 2720/ 3200]\n",
      "loss: 0.453239  [ 2736/ 3200]\n",
      "loss: 0.434761  [ 2752/ 3200]\n",
      "loss: 0.335273  [ 2768/ 3200]\n",
      "loss: 0.103935  [ 2784/ 3200]\n",
      "loss: 0.308006  [ 2800/ 3200]\n",
      "loss: 0.173929  [ 2816/ 3200]\n",
      "loss: 0.242506  [ 2832/ 3200]\n",
      "loss: 0.120585  [ 2848/ 3200]\n",
      "loss: 0.248109  [ 2864/ 3200]\n",
      "loss: 0.218154  [ 2880/ 3200]\n",
      "loss: 0.246960  [ 2896/ 3200]\n",
      "loss: 0.089793  [ 2912/ 3200]\n",
      "loss: 0.111111  [ 2928/ 3200]\n",
      "loss: 0.485725  [ 2944/ 3200]\n",
      "loss: 0.426231  [ 2960/ 3200]\n",
      "loss: 0.129193  [ 2976/ 3200]\n",
      "loss: 0.409357  [ 2992/ 3200]\n",
      "loss: 0.316758  [ 3008/ 3200]\n",
      "loss: 0.267439  [ 3024/ 3200]\n",
      "loss: 0.216841  [ 3040/ 3200]\n",
      "loss: 0.137839  [ 3056/ 3200]\n",
      "loss: 0.212993  [ 3072/ 3200]\n",
      "loss: 0.584428  [ 3088/ 3200]\n",
      "loss: 0.195544  [ 3104/ 3200]\n",
      "loss: 0.197244  [ 3120/ 3200]\n",
      "loss: 0.155091  [ 3136/ 3200]\n",
      "loss: 0.162848  [ 3152/ 3200]\n",
      "loss: 0.466290  [ 3168/ 3200]\n",
      "loss: 0.653463  [ 3184/ 3200]\n",
      "Test Error:\n",
      "Avg loss               : 0.038475\n",
      "f1 macro averaged score: 0.771089\n",
      "Accuracy               : 77.8%\n",
      "Confusion matrix       :\n",
      "tensor([[190,   4,   0,   6],\n",
      "        [ 20, 108,  26,  46],\n",
      "        [  0,  21, 159,  20],\n",
      "        [ 12,  10,  13, 165]], device='cuda:0')\n",
      "Test Error:\n",
      "Avg loss               : 0.038027\n",
      "f1 macro averaged score: 0.789856\n",
      "Accuracy               : 78.9%\n",
      "Confusion matrix       :\n",
      "tensor([[286,   5,   1,   5],\n",
      "        [ 18, 198,  26,  82],\n",
      "        [  4,  32, 291,  29],\n",
      "        [ 17,  42,  29, 311]], device='cuda:0')\n",
      "CPU times: user 14.2 s, sys: 314 ms, total: 14.5 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 30\n",
    "learning_rate_0 = 0.002\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(list(zip(melgram_x_train, melgram_y_train)), batch_size=16, shuffle=True, generator=torch.Generator(device='cpu'))\n",
    "train_dataloader.set_epoch = set_epoch\n",
    "\n",
    "torch_seed(0)\n",
    "cnn_model = Net(ELU, 0.1).to(device)\n",
    "optimizer = Adagrad(params=cnn_model.parameters(), lr=learning_rate_0, weight_decay=0.0001)\n",
    "scheduler = MultiplicativeLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95, verbose=True)\n",
    "cnn_model, f1_per_epoch, epoch = validate_convolutional_neural_network(\n",
    "      epochs, optimizer, train_dataloader, val_dataloader, loss_function, cnn_model, True, scheduler, 7\n",
    "      )\n",
    "results = test_convolutional_neural_network(test_dataloader, loss_function, cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bShmSmpeq6Gs"
   },
   "source": [
    "Invert labels so that the plots are easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0b4tNWREZOg2"
   },
   "outputs": [],
   "source": [
    "# invert label dict\n",
    "labels_int_to_str = dict(map(reversed, labels_str_to_int.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76i7Sk6FrA3f"
   },
   "source": [
    "### Test 1: Chopin - Nocturne (classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "sdH_P9twzeJ2",
    "outputId": "8fb88c02-5142-41c6-f8e2-1a18167d713d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;33mWARNING:\u001b[0m Post-Processor arguments given without specifying name. The arguments will be given to all post-processors\n",
      "Test Error:\n",
      "Avg loss               : 0.019865\n",
      "f1 macro averaged score: 0.249071\n",
      "Accuracy               : 99.3%\n",
      "Confusion matrix       :\n",
      "tensor([[268,   0,   1,   1],\n",
      "        [  0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0]], device='cuda:0')\n",
      "Chopin - Nocturne: accuracy = 99.25925925925925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5735f66ec0>]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "youtube_dataloader = get_youtube_dataloader(\"https://www.youtube.com/watch?v=9E6b3swbnWg\", labels_str_to_int[\"classical\"])\n",
    "results = test_convolutional_neural_network(youtube_dataloader, loss_function, cnn_model)\n",
    "print(f\"Chopin - Nocturne: accuracy = {results[2]}\")\n",
    "plt.plot(inference(youtube_dataloader, cnn_model), linestyle=None, marker='o', markersize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikI4TcvCgPtG"
   },
   "source": [
    "From the confusion matrix, we can see that\n",
    "+ $268 / 270 \\approx 99.3\\%$ of samples were classified as <code>'classical'</code> (accuracy)\n",
    "+ $1 / 270 \\approx 0.3\\%$ of samples was classified as <code>'hiphop'</code>\n",
    "+ $1 / 270 \\approx 0.3\\%$ of samples were classified as <code>'rock_metal_hardrock'</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XUbtI2L4B7X"
   },
   "source": [
    "### Test 2: Madonna - Hung Up (pop)\n",
    "\n",
    "There is no pop class in our dataset, so we classify this song as hiphop, which is the closest genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "6nNLRhHO1dH-",
    "outputId": "ffa398b8-13ec-41e4-fd78-3d6274349df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;33mWARNING:\u001b[0m Post-Processor arguments given without specifying name. The arguments will be given to all post-processors\n",
      "Test Error:\n",
      "Avg loss               : 2.516693\n",
      "f1 macro averaged score: 0.063969\n",
      "Accuracy               : 14.7%\n",
      "Confusion matrix       :\n",
      "tensor([[  0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0],\n",
      "        [ 32,  35,  49, 218],\n",
      "        [  0,   0,   0,   0]], device='cuda:0')\n",
      "Madonna - Hung Up: accuracy = 14.67065868263473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5735a88d00>]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "youtube_dataloader = get_youtube_dataloader(\"https://www.youtube.com/watch?v=EDwb9jOVRtU\", labels_str_to_int[\"hiphop\"])\n",
    "results = test_convolutional_neural_network(youtube_dataloader, loss_function, cnn_model)\n",
    "print(f\"Madonna - Hung Up: accuracy = {results[2]}\")\n",
    "plt.plot(inference(youtube_dataloader, cnn_model), linestyle=None, marker='o', markersize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nI1qplRemHq"
   },
   "source": [
    "From the confusion matrix, we can see that\n",
    "+ $218 / 334 \\approx 65.2\\%$ of samples were classified as <code>'rock_metal_hardrock'</code>\n",
    "+ $49 / 334 \\approx 14.6\\%$ of samples were classified as <code>'hiphop'</code> (accuracy)\n",
    "+ $35 / 334 \\approx 10\\%$ of samples were classified as <code>'blues'</code>\n",
    "+ $32 / 334 \\approx 9.5\\%$ of samples were classified as <code>'classical'</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuOAJumW4SE3"
   },
   "source": [
    "### Test 3: Placebo - Every You Every Me (rock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "eUQkBer_4Sup",
    "outputId": "cacae548-6e32-40b3-d3ec-dd3fb2297b4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;33mWARNING:\u001b[0m Post-Processor arguments given without specifying name. The arguments will be given to all post-processors\n",
      "Test Error:\n",
      "Avg loss               : 0.337196\n",
      "f1 macro averaged score: 0.235294\n",
      "Accuracy               : 88.9%\n",
      "Confusion matrix       :\n",
      "tensor([[  0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0],\n",
      "        [ 16,   5,   3, 192]], device='cuda:0')\n",
      "Placebo - Every You Every Me: accuracy = 88.88888888888889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5735c54580>]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "youtube_dataloader = get_youtube_dataloader(\"https://www.youtube.com/watch?v=OMaycNcPsHI\", labels_str_to_int[\"rock_metal_hardrock\"])\n",
    "results = test_convolutional_neural_network(youtube_dataloader, loss_function, cnn_model)\n",
    "print(f\"Placebo - Every You Every Me: accuracy = {results[2]}\")\n",
    "plt.plot(inference(youtube_dataloader, cnn_model), marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7lnPIuQdgZP"
   },
   "source": [
    "From the confusion matrix, we can see that\n",
    "+ $192 / 216 \\approx 88.8\\%$ of samples were classified as <code>'rock_metal_hardrock'</code> (accuracy)\n",
    "+ $16 / 216 \\approx 7.4\\%$ of samples were classified as <code>'classical'</code>\n",
    "+ $5 / 216 \\approx 2.3\\%$ of samples were classified as <code>'blues'</code>\n",
    "+ $3 / 216 \\approx 1.3\\%$ of samples were classified as <code>'hiphop'</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VYSEFgg4YRF"
   },
   "source": [
    "### Test 4: B.B. King - How Blue Can You Get (blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "RnMmKzcr4kod",
    "outputId": "cc2dc562-9a9e-4373-fdc5-8a046b721651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;33mWARNING:\u001b[0m Post-Processor arguments given without specifying name. The arguments will be given to all post-processors\n",
      "Test Error:\n",
      "Avg loss               : 1.594254\n",
      "f1 macro averaged score: 0.140230\n",
      "Accuracy               : 39.0%\n",
      "Confusion matrix       :\n",
      "tensor([[  0,   0,   0,   0],\n",
      "        [ 34, 122,  52, 105],\n",
      "        [  0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0]], device='cuda:0')\n",
      "B.B. King - How Blue Can You Get: accuracy = 38.977635782747605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5735ab6aa0>]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "youtube_dataloader = get_youtube_dataloader(\"https://www.youtube.com/watch?v=l45f28PzfCI\", labels_str_to_int[\"blues\"])\n",
    "results = test_convolutional_neural_network(youtube_dataloader, loss_function, cnn_model)\n",
    "print(f\"B.B. King - How Blue Can You Get: accuracy = {results[2]}\")\n",
    "plt.plot(inference(youtube_dataloader, cnn_model), marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txUIpAMzfWL4"
   },
   "source": [
    "From the confusion matrix, we can see that\n",
    "+ $122 / 313 \\approx 39\\%$ of samples were classified as <code>'blues'</code> (accuracy)\n",
    "+ $105 / 313 \\approx 33.5\\%$ of samples were classified as <code>'rock_metal_hardrock'</code>\n",
    "+ $52 / 313 \\approx 16.6\\%$ of samples were classified as <code>'hiphop'</code>\n",
    "+ $34 / 313 \\approx 10.8\\%$ of samples were classified as <code>'classical'</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_BES1Y2gffb"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Our model classified $3$ out of $4$ songs successfully, which means that the model has $75\\%$ accuracy, which is close to the $79\\%$ that was achieved during training-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnWXcnFiq0rZ"
   },
   "source": [
    "# Reference\n",
    "\n",
    "[1] SaturnCloud, *TensorFlow Neural Network Faster on CPU Than GPU Exploring the Reasons*, June 2023, https://saturncloud.io/blog/tensorflow-neural-network-faster-on-cpu-than-gpu-exploring-the-reasons/\n",
    "\n",
    "[2] DeepAI, *Max Pooling Definition*, https://deepai.org/machine-learning-glossary-and-terms/max-pooling\n",
    "\n",
    "[3] DeepAI, *Padding Definition*, https://deepai.org/machine-learning-glossary-and-terms/padding\n",
    "\n",
    "[4] S. L. Smith, P.-J. Kindermans, C. Ying, Q. V. Le, *Don't Decay the Learning Rate, Increase the Batch Size*, 2017, https://arxiv.org/abs/1711.00489"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
